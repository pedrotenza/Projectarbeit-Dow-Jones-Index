{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    # Initialize lists to store training and temporary sets\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    # Iterate through the data with the specified window size\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i+split_window_size+1]\n",
    "        y_out1_out2 = y_in.iloc[i:i+split_window_size+1]\n",
    "\n",
    "        # Separate the last row for the temporary set\n",
    "        # [ :-1]: all elements except the last one\n",
    "        # [-1:]:  selects only the last element\n",
    "        # (:) is used to indicate slicing of a sequence\n",
    "        # sequence[start : end : step]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    # Concatenate the lists into pandas DataFrames\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "# Split Data to train and temp\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 3\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split temp into val and test\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 1\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob=0.5):  # Added dropout_prob\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)  # Included dropout in LSTM\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Added dropout layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.01, window_size=20, dropout_prob=0, weight_decay=0\n",
      "Epoch 1/150000, Loss: 29889.6171875, Validation Loss: 30381.451171875, Train Gradient: 223.41114807128906\n",
      "Epoch 101/150000, Loss: 28942.427734375, Validation Loss: 29417.9140625, Train Gradient: 214.7916717529297\n",
      "Epoch 201/150000, Loss: 27956.173828125, Validation Loss: 28422.931640625, Train Gradient: 205.37060546875\n",
      "Epoch 301/150000, Loss: 27119.40625, Validation Loss: 27576.611328125, Train Gradient: 197.04705810546875\n",
      "Epoch 401/150000, Loss: 26354.35546875, Validation Loss: 26802.232421875, Train Gradient: 189.11952209472656\n",
      "Epoch 501/150000, Loss: 25644.25390625, Validation Loss: 26083.021484375, Train Gradient: 181.4528350830078\n",
      "Epoch 601/150000, Loss: 24981.658203125, Validation Loss: 25411.521484375, Train Gradient: 173.99517822265625\n",
      "Epoch 701/150000, Loss: 24362.10546875, Validation Loss: 24783.2578125, Train Gradient: 166.72084045410156\n",
      "Epoch 801/150000, Loss: 23782.435546875, Validation Loss: 24195.0625, Train Gradient: 159.6150665283203\n",
      "Epoch 901/150000, Loss: 23240.158203125, Validation Loss: 23644.43359375, Train Gradient: 152.66867065429688\n",
      "Epoch 1001/150000, Loss: 22733.173828125, Validation Loss: 23129.26953125, Train Gradient: 145.87559509277344\n",
      "Epoch 1101/150000, Loss: 22259.62890625, Validation Loss: 22647.71875, Train Gradient: 139.2317352294922\n",
      "Epoch 1201/150000, Loss: 21646.974609375, Validation Loss: 22047.0546875, Train Gradient: 148.51083374023438\n",
      "Epoch 1301/150000, Loss: 21122.53515625, Validation Loss: 21520.171875, Train Gradient: 144.31822204589844\n",
      "Epoch 1401/150000, Loss: 20620.75390625, Validation Loss: 21013.181640625, Train Gradient: 140.8124237060547\n",
      "Epoch 1501/150000, Loss: 20134.99609375, Validation Loss: 20518.9375, Train Gradient: 137.5883026123047\n",
      "Epoch 1601/150000, Loss: 19664.248046875, Validation Loss: 20040.05078125, Train Gradient: 134.3055419921875\n",
      "Epoch 1701/150000, Loss: 19207.837890625, Validation Loss: 19575.6484375, Train Gradient: 131.20126342773438\n",
      "Epoch 1801/150000, Loss: 18764.994140625, Validation Loss: 19125.298828125, Train Gradient: 128.20651245117188\n",
      "Epoch 1901/150000, Loss: 18334.900390625, Validation Loss: 18688.142578125, Train Gradient: 125.31965637207031\n",
      "Epoch 2001/150000, Loss: 17916.8984375, Validation Loss: 18263.41796875, Train Gradient: 122.5112533569336\n",
      "Epoch 2101/150000, Loss: 17510.544921875, Validation Loss: 17850.595703125, Train Gradient: 119.74919128417969\n",
      "Epoch 2201/150000, Loss: 17115.671875, Validation Loss: 17449.552734375, Train Gradient: 117.00967407226562\n",
      "Epoch 2301/150000, Loss: 16732.103515625, Validation Loss: 17060.029296875, Train Gradient: 114.30693054199219\n",
      "Epoch 2401/150000, Loss: 16359.5634765625, Validation Loss: 16681.740234375, Train Gradient: 111.64897155761719\n",
      "Epoch 2501/150000, Loss: 15997.765625, Validation Loss: 16314.4951171875, Train Gradient: 109.03756713867188\n",
      "Epoch 2601/150000, Loss: 15646.4091796875, Validation Loss: 15958.08203125, Train Gradient: 106.47567749023438\n",
      "Epoch 2701/150000, Loss: 15305.177734375, Validation Loss: 15612.3017578125, Train Gradient: 103.962646484375\n",
      "Epoch 2801/150000, Loss: 14973.7861328125, Validation Loss: 15276.7509765625, Train Gradient: 101.48684692382812\n",
      "Epoch 2901/150000, Loss: 14652.029296875, Validation Loss: 14950.5908203125, Train Gradient: 99.05160522460938\n",
      "Epoch 3001/150000, Loss: 14338.302734375, Validation Loss: 14632.02734375, Train Gradient: 96.9299087524414\n",
      "Epoch 3101/150000, Loss: 14032.9521484375, Validation Loss: 14321.72265625, Train Gradient: 94.70370483398438\n",
      "Epoch 3201/150000, Loss: 13735.7509765625, Validation Loss: 14019.568359375, Train Gradient: 92.53089904785156\n",
      "Epoch 3301/150000, Loss: 13446.3837890625, Validation Loss: 13726.513671875, Train Gradient: 90.42633056640625\n",
      "Epoch 3401/150000, Loss: 13164.4931640625, Validation Loss: 13441.625, Train Gradient: 88.38311767578125\n",
      "Epoch 3501/150000, Loss: 12889.724609375, Validation Loss: 13164.357421875, Train Gradient: 86.40028381347656\n",
      "Epoch 3601/150000, Loss: 12621.8408203125, Validation Loss: 12893.5439453125, Train Gradient: 84.46797943115234\n",
      "Epoch 3701/150000, Loss: 12360.2998046875, Validation Loss: 12628.193359375, Train Gradient: 82.6427230834961\n",
      "Epoch 3801/150000, Loss: 12104.1904296875, Validation Loss: 12367.0888671875, Train Gradient: 80.92716979980469\n",
      "Epoch 3901/150000, Loss: 11853.400390625, Validation Loss: 12109.490234375, Train Gradient: 79.22219848632812\n",
      "Epoch 4001/150000, Loss: 11608.6123046875, Validation Loss: 11861.7802734375, Train Gradient: 77.57987213134766\n",
      "Epoch 4101/150000, Loss: 11369.2939453125, Validation Loss: 11619.64453125, Train Gradient: 75.94937133789062\n",
      "Epoch 4201/150000, Loss: 11135.3486328125, Validation Loss: 11383.0107421875, Train Gradient: 74.35618591308594\n",
      "Epoch 4301/150000, Loss: 10906.4462890625, Validation Loss: 11150.892578125, Train Gradient: 72.82176971435547\n",
      "Epoch 4401/150000, Loss: 10682.3046875, Validation Loss: 10923.5595703125, Train Gradient: 71.34335327148438\n",
      "Epoch 4501/150000, Loss: 10462.6376953125, Validation Loss: 10701.302734375, Train Gradient: 69.91503143310547\n",
      "Epoch 4601/150000, Loss: 10247.447265625, Validation Loss: 10483.4736328125, Train Gradient: 68.51993560791016\n",
      "Epoch 4701/150000, Loss: 10036.5087890625, Validation Loss: 10269.72265625, Train Gradient: 67.37547302246094\n",
      "Epoch 4801/150000, Loss: 9829.5537109375, Validation Loss: 10059.880859375, Train Gradient: 65.84464263916016\n",
      "Epoch 4901/150000, Loss: 9626.8349609375, Validation Loss: 9853.7392578125, Train Gradient: 64.56588745117188\n",
      "Epoch 5001/150000, Loss: 9427.5244140625, Validation Loss: 9650.970703125, Train Gradient: 63.35307693481445\n",
      "Epoch 5101/150000, Loss: 9231.8515625, Validation Loss: 9451.4189453125, Train Gradient: 62.187347412109375\n",
      "Epoch 5201/150000, Loss: 9039.658203125, Validation Loss: 9256.1650390625, Train Gradient: 61.07274627685547\n",
      "Epoch 5301/150000, Loss: 8850.4541015625, Validation Loss: 9065.3759765625, Train Gradient: 59.98353576660156\n",
      "Epoch 5401/150000, Loss: 8674.2841796875, Validation Loss: 8877.25390625, Train Gradient: 59.04275894165039\n",
      "Epoch 5501/150000, Loss: 8481.150390625, Validation Loss: 8689.8349609375, Train Gradient: 57.90808868408203\n",
      "Epoch 5601/150000, Loss: 8300.625, Validation Loss: 8506.904296875, Train Gradient: 56.914913177490234\n",
      "Epoch 5701/150000, Loss: 8122.869140625, Validation Loss: 8327.091796875, Train Gradient: 55.94779968261719\n",
      "Epoch 5801/150000, Loss: 7947.60888671875, Validation Loss: 8149.919921875, Train Gradient: 54.98604202270508\n",
      "Epoch 5901/150000, Loss: 7775.0048828125, Validation Loss: 7975.20263671875, Train Gradient: 54.089874267578125\n",
      "Epoch 6001/150000, Loss: 7605.0029296875, Validation Loss: 7802.9794921875, Train Gradient: 53.178489685058594\n",
      "Epoch 6101/150000, Loss: 7437.482421875, Validation Loss: 7633.17919921875, Train Gradient: 52.308265686035156\n",
      "Epoch 6201/150000, Loss: 7272.3662109375, Validation Loss: 7465.8291015625, Train Gradient: 51.45093536376953\n",
      "Epoch 6301/150000, Loss: 7109.6279296875, Validation Loss: 7301.2119140625, Train Gradient: 50.60582733154297\n",
      "Epoch 6401/150000, Loss: 6949.3193359375, Validation Loss: 7139.20068359375, Train Gradient: 49.75330352783203\n",
      "Epoch 6501/150000, Loss: 6791.4150390625, Validation Loss: 6979.63134765625, Train Gradient: 48.908042907714844\n",
      "Epoch 6601/150000, Loss: 6635.90234375, Validation Loss: 6822.29541015625, Train Gradient: 48.14508819580078\n",
      "Epoch 6701/150000, Loss: 6482.77734375, Validation Loss: 6667.34521484375, Train Gradient: 47.6191291809082\n",
      "Epoch 6801/150000, Loss: 6331.96044921875, Validation Loss: 6514.64501953125, Train Gradient: 46.56084442138672\n",
      "Epoch 6901/150000, Loss: 6183.51708984375, Validation Loss: 6364.44677734375, Train Gradient: 45.80886459350586\n",
      "Epoch 7001/150000, Loss: 6037.515625, Validation Loss: 6217.029296875, Train Gradient: 44.52574157714844\n",
      "Epoch 7101/150000, Loss: 5893.76416015625, Validation Loss: 6071.6572265625, Train Gradient: 44.23698425292969\n",
      "Epoch 7201/150000, Loss: 5752.49951171875, Validation Loss: 5928.96044921875, Train Gradient: 43.50375747680664\n",
      "Epoch 7301/150000, Loss: 5613.6630859375, Validation Loss: 5788.7802734375, Train Gradient: 42.705604553222656\n",
      "Epoch 7401/150000, Loss: 5477.2939453125, Validation Loss: 5651.01171875, Train Gradient: 41.91214370727539\n",
      "Epoch 7501/150000, Loss: 5343.39697265625, Validation Loss: 5515.8583984375, Train Gradient: 41.19533157348633\n",
      "Epoch 7601/150000, Loss: 5212.025390625, Validation Loss: 5383.20947265625, Train Gradient: 40.101139068603516\n",
      "Epoch 7701/150000, Loss: 5083.04345703125, Validation Loss: 5252.9638671875, Train Gradient: 39.711910247802734\n",
      "Epoch 7801/150000, Loss: 4956.39892578125, Validation Loss: 5125.21630859375, Train Gradient: 39.01673889160156\n",
      "Epoch 7901/150000, Loss: 4831.9228515625, Validation Loss: 5000.1923828125, Train Gradient: 38.34012985229492\n",
      "Epoch 8001/150000, Loss: 4709.70849609375, Validation Loss: 4877.34912109375, Train Gradient: 37.646114349365234\n",
      "Epoch 8101/150000, Loss: 4589.8095703125, Validation Loss: 4756.775390625, Train Gradient: 37.46456527709961\n",
      "Epoch 8201/150000, Loss: 4471.8583984375, Validation Loss: 4639.095703125, Train Gradient: 36.3315544128418\n",
      "Epoch 8301/150000, Loss: 4356.072265625, Validation Loss: 4523.03955078125, Train Gradient: 35.632259368896484\n",
      "Epoch 8401/150000, Loss: 4242.5126953125, Validation Loss: 4409.0625, Train Gradient: 35.00518798828125\n",
      "Epoch 8501/150000, Loss: 4131.29052734375, Validation Loss: 4297.1396484375, Train Gradient: 35.01982879638672\n",
      "Epoch 8601/150000, Loss: 4021.985595703125, Validation Loss: 4187.7958984375, Train Gradient: 33.71123123168945\n",
      "Epoch 8701/150000, Loss: 3914.986083984375, Validation Loss: 4080.50390625, Train Gradient: 33.06923294067383\n",
      "Epoch 8801/150000, Loss: 3810.157470703125, Validation Loss: 3975.357666015625, Train Gradient: 32.459495544433594\n",
      "Epoch 8901/150000, Loss: 3707.484130859375, Validation Loss: 3872.546142578125, Train Gradient: 31.802257537841797\n",
      "Epoch 9001/150000, Loss: 3606.977294921875, Validation Loss: 3771.904541015625, Train Gradient: 31.100341796875\n",
      "Epoch 9101/150000, Loss: 3508.61181640625, Validation Loss: 3673.45166015625, Train Gradient: 30.55624771118164\n",
      "Epoch 9201/150000, Loss: 3412.400146484375, Validation Loss: 3577.2353515625, Train Gradient: 29.745662689208984\n",
      "Epoch 9301/150000, Loss: 3318.295654296875, Validation Loss: 3483.172607421875, Train Gradient: 29.328392028808594\n",
      "Epoch 9401/150000, Loss: 3226.328857421875, Validation Loss: 3391.22509765625, Train Gradient: 28.816532135009766\n",
      "Epoch 9501/150000, Loss: 3136.229248046875, Validation Loss: 3300.283203125, Train Gradient: 28.148374557495117\n",
      "Epoch 9601/150000, Loss: 3048.1728515625, Validation Loss: 3211.800048828125, Train Gradient: 27.571949005126953\n",
      "Epoch 9701/150000, Loss: 2962.123046875, Validation Loss: 3125.5751953125, Train Gradient: 26.998048782348633\n",
      "Epoch 9801/150000, Loss: 2878.113525390625, Validation Loss: 3041.26416015625, Train Gradient: 26.914583206176758\n",
      "Epoch 9901/150000, Loss: 2795.895751953125, Validation Loss: 2959.49755859375, Train Gradient: 25.875144958496094\n",
      "Epoch 10001/150000, Loss: 2715.76416015625, Validation Loss: 2879.16748046875, Train Gradient: 26.009429931640625\n",
      "Epoch 10101/150000, Loss: 2637.216552734375, Validation Loss: 2801.513916015625, Train Gradient: 24.78324317932129\n",
      "Epoch 10201/150000, Loss: 2560.633056640625, Validation Loss: 2725.7490234375, Train Gradient: 24.311140060424805\n",
      "Epoch 10301/150000, Loss: 2485.833984375, Validation Loss: 2651.412353515625, Train Gradient: 23.73890495300293\n",
      "Epoch 10401/150000, Loss: 2412.8466796875, Validation Loss: 2579.25390625, Train Gradient: 22.709280014038086\n",
      "Epoch 10501/150000, Loss: 2341.366455078125, Validation Loss: 2508.876220703125, Train Gradient: 22.742633819580078\n",
      "Epoch 10601/150000, Loss: 2271.54931640625, Validation Loss: 2439.876708984375, Train Gradient: 22.288726806640625\n",
      "Epoch 10701/150000, Loss: 2203.291748046875, Validation Loss: 2372.21142578125, Train Gradient: 21.778085708618164\n",
      "Epoch 10801/150000, Loss: 2136.5712890625, Validation Loss: 2306.05419921875, Train Gradient: 21.320343017578125\n",
      "Epoch 10901/150000, Loss: 2071.357421875, Validation Loss: 2241.381103515625, Train Gradient: 20.86861801147461\n",
      "Epoch 11001/150000, Loss: 2007.644287109375, Validation Loss: 2178.30615234375, Train Gradient: 20.41316032409668\n",
      "Epoch 11101/150000, Loss: 1945.431396484375, Validation Loss: 2116.757080078125, Train Gradient: 19.954021453857422\n",
      "Epoch 11201/150000, Loss: 1884.7119140625, Validation Loss: 2056.7900390625, Train Gradient: 19.512950897216797\n",
      "Epoch 11301/150000, Loss: 1825.4901123046875, Validation Loss: 1998.2823486328125, Train Gradient: 19.121232986450195\n",
      "Epoch 11401/150000, Loss: 1767.7568359375, Validation Loss: 1941.3714599609375, Train Gradient: 18.628047943115234\n",
      "Epoch 11501/150000, Loss: 1711.514404296875, Validation Loss: 1885.9556884765625, Train Gradient: 18.2161922454834\n",
      "Epoch 11601/150000, Loss: 1656.73583984375, Validation Loss: 1831.8348388671875, Train Gradient: 17.760446548461914\n",
      "Epoch 11701/150000, Loss: 1603.4071044921875, Validation Loss: 1779.2586669921875, Train Gradient: 17.3489933013916\n",
      "Epoch 11801/150000, Loss: 1551.50244140625, Validation Loss: 1728.1199951171875, Train Gradient: 16.926437377929688\n",
      "Epoch 11901/150000, Loss: 1500.9908447265625, Validation Loss: 1678.4315185546875, Train Gradient: 16.51032066345215\n",
      "Epoch 12001/150000, Loss: 1451.771484375, Validation Loss: 1630.146484375, Train Gradient: 16.13346290588379\n",
      "Epoch 12101/150000, Loss: 1403.6209716796875, Validation Loss: 1583.8514404296875, Train Gradient: 15.7351655960083\n",
      "Epoch 12201/150000, Loss: 1356.769287109375, Validation Loss: 1538.5035400390625, Train Gradient: 15.38850212097168\n",
      "Epoch 12301/150000, Loss: 1311.1563720703125, Validation Loss: 1494.44970703125, Train Gradient: 15.015986442565918\n",
      "Epoch 12401/150000, Loss: 1266.776123046875, Validation Loss: 1451.726318359375, Train Gradient: 14.410018920898438\n",
      "Epoch 12501/150000, Loss: 1223.5498046875, Validation Loss: 1410.074951171875, Train Gradient: 14.291306495666504\n",
      "Epoch 12601/150000, Loss: 1181.52685546875, Validation Loss: 1369.431396484375, Train Gradient: 14.168193817138672\n",
      "Epoch 12701/150000, Loss: 1140.6246337890625, Validation Loss: 1330.622314453125, Train Gradient: 13.5875244140625\n",
      "Epoch 12801/150000, Loss: 1100.8878173828125, Validation Loss: 1292.6319580078125, Train Gradient: 12.97408676147461\n",
      "Epoch 12901/150000, Loss: 1062.2064208984375, Validation Loss: 1255.98876953125, Train Gradient: 12.911943435668945\n",
      "Epoch 13001/150000, Loss: 1024.6837158203125, Validation Loss: 1220.07275390625, Train Gradient: 12.89875602722168\n",
      "Epoch 13101/150000, Loss: 988.2099609375, Validation Loss: 1185.7919921875, Train Gradient: 12.251165390014648\n",
      "Epoch 13201/150000, Loss: 952.8619995117188, Validation Loss: 1152.4349365234375, Train Gradient: 11.91168212890625\n",
      "Epoch 13301/150000, Loss: 918.6044921875, Validation Loss: 1120.19091796875, Train Gradient: 11.545446395874023\n",
      "Epoch 13401/150000, Loss: 885.4082641601562, Validation Loss: 1089.029296875, Train Gradient: 11.268891334533691\n",
      "Epoch 13501/150000, Loss: 853.234619140625, Validation Loss: 1059.0023193359375, Train Gradient: 10.962939262390137\n",
      "Epoch 13601/150000, Loss: 821.8577270507812, Validation Loss: 1028.034423828125, Train Gradient: 10.589706420898438\n",
      "Epoch 13701/150000, Loss: 791.537353515625, Validation Loss: 1000.46923828125, Train Gradient: 10.3878812789917\n",
      "Epoch 13801/150000, Loss: 762.16943359375, Validation Loss: 973.3741455078125, Train Gradient: 10.102330207824707\n",
      "Epoch 13901/150000, Loss: 733.71728515625, Validation Loss: 947.0270385742188, Train Gradient: 9.69675064086914\n",
      "Epoch 14001/150000, Loss: 706.1524658203125, Validation Loss: 921.3690185546875, Train Gradient: 9.544694900512695\n",
      "Epoch 14101/150000, Loss: 679.6382446289062, Validation Loss: 896.361572265625, Train Gradient: 10.038142204284668\n",
      "Epoch 14201/150000, Loss: 653.6664428710938, Validation Loss: 872.7413940429688, Train Gradient: 9.00743293762207\n",
      "Epoch 14301/150000, Loss: 628.7015991210938, Validation Loss: 849.660400390625, Train Gradient: 8.74556827545166\n",
      "Epoch 14401/150000, Loss: 604.559326171875, Validation Loss: 827.3268432617188, Train Gradient: 8.44736099243164\n",
      "Epoch 14501/150000, Loss: 581.2086791992188, Validation Loss: 805.733642578125, Train Gradient: 8.240898132324219\n",
      "Epoch 14601/150000, Loss: 558.6331787109375, Validation Loss: 784.8464965820312, Train Gradient: 8.12094497680664\n",
      "Epoch 14701/150000, Loss: 536.796875, Validation Loss: 764.7931518554688, Train Gradient: 7.759687423706055\n",
      "Epoch 14801/150000, Loss: 515.8587646484375, Validation Loss: 745.1163330078125, Train Gradient: 8.308670997619629\n",
      "Epoch 14901/150000, Loss: 495.33251953125, Validation Loss: 727.032958984375, Train Gradient: 7.292403697967529\n",
      "Epoch 15001/150000, Loss: 475.71771240234375, Validation Loss: 709.3489990234375, Train Gradient: 7.060861587524414\n",
      "Epoch 15101/150000, Loss: 456.8344421386719, Validation Loss: 692.2335205078125, Train Gradient: 6.880721569061279\n",
      "Epoch 15201/150000, Loss: 438.6658020019531, Validation Loss: 675.8359375, Train Gradient: 6.610001087188721\n",
      "Epoch 15301/150000, Loss: 421.2115783691406, Validation Loss: 659.7907104492188, Train Gradient: 6.591565132141113\n",
      "Epoch 15401/150000, Loss: 404.307373046875, Validation Loss: 643.9100952148438, Train Gradient: 6.172280311584473\n",
      "Epoch 15501/150000, Loss: 387.70361328125, Validation Loss: 627.20654296875, Train Gradient: 6.259304523468018\n",
      "Epoch 15601/150000, Loss: 371.75048828125, Validation Loss: 611.4281616210938, Train Gradient: 5.513386249542236\n",
      "Epoch 15701/150000, Loss: 356.51470947265625, Validation Loss: 596.7030029296875, Train Gradient: 4.946338176727295\n",
      "Epoch 15801/150000, Loss: 341.67291259765625, Validation Loss: 581.5474243164062, Train Gradient: 5.454586029052734\n",
      "Epoch 15901/150000, Loss: 327.60003662109375, Validation Loss: 568.3935546875, Train Gradient: 5.244046211242676\n",
      "Epoch 16001/150000, Loss: 314.1036071777344, Validation Loss: 555.8441772460938, Train Gradient: 5.068960189819336\n",
      "Epoch 16101/150000, Loss: 301.3559875488281, Validation Loss: 544.2532348632812, Train Gradient: 4.152054786682129\n",
      "Epoch 16201/150000, Loss: 288.9568786621094, Validation Loss: 532.271240234375, Train Gradient: 4.672486782073975\n",
      "Epoch 16301/150000, Loss: 277.3177185058594, Validation Loss: 521.6566772460938, Train Gradient: 4.51500940322876\n",
      "Epoch 16401/150000, Loss: 266.2590026855469, Validation Loss: 511.9463806152344, Train Gradient: 4.294369220733643\n",
      "Epoch 16501/150000, Loss: 255.74691772460938, Validation Loss: 502.99072265625, Train Gradient: 4.137214660644531\n",
      "Epoch 16601/150000, Loss: 245.7562255859375, Validation Loss: 494.6170654296875, Train Gradient: 3.9651503562927246\n",
      "Epoch 16701/150000, Loss: 236.2606658935547, Validation Loss: 486.886962890625, Train Gradient: 3.8068134784698486\n",
      "Epoch 16801/150000, Loss: 227.25904846191406, Validation Loss: 479.81817626953125, Train Gradient: 3.3831820487976074\n",
      "Epoch 16901/150000, Loss: 218.67276000976562, Validation Loss: 472.808349609375, Train Gradient: 3.4904563426971436\n",
      "Epoch 17001/150000, Loss: 210.53904724121094, Validation Loss: 466.32049560546875, Train Gradient: 3.345363140106201\n",
      "Epoch 17101/150000, Loss: 202.8215789794922, Validation Loss: 460.05487060546875, Train Gradient: 3.2462997436523438\n",
      "Epoch 17201/150000, Loss: 195.49215698242188, Validation Loss: 454.1834716796875, Train Gradient: 3.0674679279327393\n",
      "Epoch 17301/150000, Loss: 188.52320861816406, Validation Loss: 448.6015319824219, Train Gradient: 2.9591784477233887\n",
      "Epoch 17401/150000, Loss: 181.89002990722656, Validation Loss: 443.27838134765625, Train Gradient: 2.8147966861724854\n",
      "Epoch 17501/150000, Loss: 175.6761932373047, Validation Loss: 438.19366455078125, Train Gradient: 2.154475688934326\n",
      "Epoch 17601/150000, Loss: 169.5698699951172, Validation Loss: 433.4825744628906, Train Gradient: 2.578890800476074\n",
      "Early stopping at epoch 17696 with validation loss 429.44091796875.\n",
      "Test Loss: 416.6976623535156\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.01]\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0] \n",
    "weight_decays = [0]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "                # Monitor gradients, print(\"Gradients:\"), print(f\"{name}: {param.grad.norm().item()}\")\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Train Gradient: {param.grad.norm().item()}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.01, window_size=20, dropout_prob=0, weight_decay=0.01\n",
      "Epoch 1/150000, Loss: 29889.6171875, Validation Loss: 30381.451171875, Train Gradient: 223.41114807128906\n",
      "Epoch 101/150000, Loss: 28940.951171875, Validation Loss: 29416.482421875, Train Gradient: 214.77740478515625\n",
      "Epoch 201/150000, Loss: 27956.419921875, Validation Loss: 28423.181640625, Train Gradient: 205.37313842773438\n",
      "Epoch 301/150000, Loss: 27120.056640625, Validation Loss: 27577.27734375, Train Gradient: 197.05389404296875\n",
      "Epoch 401/150000, Loss: 26355.2265625, Validation Loss: 26803.115234375, Train Gradient: 189.12904357910156\n",
      "Epoch 501/150000, Loss: 25645.28125, Validation Loss: 26084.064453125, Train Gradient: 181.46456909179688\n",
      "Epoch 601/150000, Loss: 24982.80859375, Validation Loss: 25412.6875, Train Gradient: 174.00897216796875\n",
      "Epoch 701/150000, Loss: 24363.365234375, Validation Loss: 24784.5390625, Train Gradient: 166.73666381835938\n",
      "Epoch 801/150000, Loss: 23783.794921875, Validation Loss: 24196.44140625, Train Gradient: 159.63308715820312\n",
      "Epoch 901/150000, Loss: 23241.603515625, Validation Loss: 23645.90625, Train Gradient: 152.69000244140625\n",
      "Epoch 1001/150000, Loss: 22667.181640625, Validation Loss: 23069.30859375, Train Gradient: 157.456787109375\n",
      "Epoch 1101/150000, Loss: 22105.259765625, Validation Loss: 22522.001953125, Train Gradient: 151.29446411132812\n",
      "Epoch 1201/150000, Loss: 21579.060546875, Validation Loss: 21989.734375, Train Gradient: 147.47781372070312\n",
      "Epoch 1301/150000, Loss: 21069.58984375, Validation Loss: 21472.5078125, Train Gradient: 143.9430389404297\n",
      "Epoch 1401/150000, Loss: 20575.76171875, Validation Loss: 20969.2109375, Train Gradient: 140.72512817382812\n",
      "Epoch 1501/150000, Loss: 20095.421875, Validation Loss: 20475.322265625, Train Gradient: 137.45741271972656\n",
      "Epoch 1601/150000, Loss: 19631.041015625, Validation Loss: 20002.83203125, Train Gradient: 134.0892333984375\n",
      "Epoch 1701/150000, Loss: 19180.15625, Validation Loss: 19544.0234375, Train Gradient: 131.0203094482422\n",
      "Epoch 1801/150000, Loss: 18742.025390625, Validation Loss: 19098.330078125, Train Gradient: 128.05648803710938\n",
      "Epoch 1901/150000, Loss: 18316.072265625, Validation Loss: 18665.21875, Train Gradient: 125.18084716796875\n",
      "Epoch 2001/150000, Loss: 17901.857421875, Validation Loss: 18244.21484375, Train Gradient: 122.37263488769531\n",
      "Epoch 2101/150000, Loss: 17499.033203125, Validation Loss: 17834.939453125, Train Gradient: 119.6151351928711\n",
      "Epoch 2201/150000, Loss: 17107.345703125, Validation Loss: 17437.115234375, Train Gradient: 116.89959716796875\n",
      "Epoch 2301/150000, Loss: 16726.546875, Validation Loss: 17050.494140625, Train Gradient: 114.22270965576172\n",
      "Epoch 2401/150000, Loss: 16356.4208984375, Validation Loss: 16674.828125, Train Gradient: 111.58680725097656\n",
      "Epoch 2501/150000, Loss: 15996.70703125, Validation Loss: 16309.8505859375, Train Gradient: 108.99864959716797\n",
      "Epoch 2601/150000, Loss: 15647.1181640625, Validation Loss: 15955.2734375, Train Gradient: 106.46156311035156\n",
      "Epoch 2701/150000, Loss: 15307.3681640625, Validation Loss: 15610.6953125, Train Gradient: 103.97242736816406\n",
      "Epoch 2801/150000, Loss: 14977.14453125, Validation Loss: 15275.3837890625, Train Gradient: 101.55846405029297\n",
      "Epoch 2901/150000, Loss: 14655.798828125, Validation Loss: 14949.67578125, Train Gradient: 99.25337982177734\n",
      "Epoch 3001/150000, Loss: 14343.0078125, Validation Loss: 14632.7177734375, Train Gradient: 96.98258209228516\n",
      "Epoch 3101/150000, Loss: 14038.55078125, Validation Loss: 14324.0078125, Train Gradient: 94.76165771484375\n",
      "Epoch 3201/150000, Loss: 13742.1279296875, Validation Loss: 14023.29296875, Train Gradient: 92.60581970214844\n",
      "Epoch 3301/150000, Loss: 13453.3681640625, Validation Loss: 13730.388671875, Train Gradient: 90.51829528808594\n",
      "Epoch 3401/150000, Loss: 13171.90625, Validation Loss: 13444.900390625, Train Gradient: 88.5025405883789\n",
      "Epoch 3501/150000, Loss: 12897.15234375, Validation Loss: 13166.0888671875, Train Gradient: 86.56672668457031\n",
      "Epoch 3601/150000, Loss: 12629.2138671875, Validation Loss: 12894.6748046875, Train Gradient: 84.6655502319336\n",
      "Epoch 3701/150000, Loss: 12367.666015625, Validation Loss: 12629.3427734375, Train Gradient: 82.83905029296875\n",
      "Epoch 3801/150000, Loss: 12112.1728515625, Validation Loss: 12370.3330078125, Train Gradient: 81.06979370117188\n",
      "Epoch 3901/150000, Loss: 11862.482421875, Validation Loss: 12117.509765625, Train Gradient: 79.34921264648438\n",
      "Epoch 4001/150000, Loss: 11618.39453125, Validation Loss: 11870.6171875, Train Gradient: 77.67017364501953\n",
      "Epoch 4101/150000, Loss: 11379.767578125, Validation Loss: 11629.423828125, Train Gradient: 76.02989196777344\n",
      "Epoch 4201/150000, Loss: 11146.353515625, Validation Loss: 11393.52734375, Train Gradient: 74.4422607421875\n",
      "Epoch 4301/150000, Loss: 10918.0224609375, Validation Loss: 11163.142578125, Train Gradient: 72.90331268310547\n",
      "Epoch 4401/150000, Loss: 10694.5205078125, Validation Loss: 10937.7822265625, Train Gradient: 71.41902160644531\n",
      "Epoch 4501/150000, Loss: 10475.5146484375, Validation Loss: 10716.90625, Train Gradient: 69.98663330078125\n",
      "Epoch 4601/150000, Loss: 10260.89453125, Validation Loss: 10500.3076171875, Train Gradient: 68.59369659423828\n",
      "Epoch 4701/150000, Loss: 10050.529296875, Validation Loss: 10287.978515625, Train Gradient: 67.2319107055664\n",
      "Epoch 4801/150000, Loss: 9844.2978515625, Validation Loss: 10079.66796875, Train Gradient: 66.04051971435547\n",
      "Epoch 4901/150000, Loss: 9642.01953125, Validation Loss: 9875.265625, Train Gradient: 64.67289733886719\n",
      "Epoch 5001/150000, Loss: 9443.4541015625, Validation Loss: 9674.1796875, Train Gradient: 63.406272888183594\n",
      "Epoch 5101/150000, Loss: 9248.3603515625, Validation Loss: 9476.458984375, Train Gradient: 62.381614685058594\n",
      "Epoch 5201/150000, Loss: 9056.5576171875, Validation Loss: 9282.3369140625, Train Gradient: 61.22677230834961\n",
      "Epoch 5301/150000, Loss: 8867.84765625, Validation Loss: 9091.6943359375, Train Gradient: 60.067203521728516\n",
      "Epoch 5401/150000, Loss: 8682.01953125, Validation Loss: 8903.7216796875, Train Gradient: 59.011329650878906\n",
      "Epoch 5501/150000, Loss: 8499.1591796875, Validation Loss: 8719.119140625, Train Gradient: 57.99372100830078\n",
      "Epoch 5601/150000, Loss: 8319.142578125, Validation Loss: 8537.412109375, Train Gradient: 56.9749641418457\n",
      "Epoch 5701/150000, Loss: 8141.94482421875, Validation Loss: 8358.2802734375, Train Gradient: 55.956729888916016\n",
      "Epoch 5801/150000, Loss: 7967.302734375, Validation Loss: 8182.423828125, Train Gradient: 55.05183410644531\n",
      "Epoch 5901/150000, Loss: 7795.22509765625, Validation Loss: 8009.16357421875, Train Gradient: 54.14976119995117\n",
      "Epoch 6001/150000, Loss: 7624.341796875, Validation Loss: 7831.38134765625, Train Gradient: 52.97926330566406\n",
      "Epoch 6101/150000, Loss: 7456.7734375, Validation Loss: 7660.93798828125, Train Gradient: 52.42366027832031\n",
      "Epoch 6201/150000, Loss: 7291.95556640625, Validation Loss: 7493.361328125, Train Gradient: 51.32810974121094\n",
      "Epoch 6301/150000, Loss: 7129.48876953125, Validation Loss: 7327.5693359375, Train Gradient: 50.70655059814453\n",
      "Epoch 6401/150000, Loss: 6969.3837890625, Validation Loss: 7165.12255859375, Train Gradient: 49.879249572753906\n",
      "Epoch 6501/150000, Loss: 6811.85595703125, Validation Loss: 7005.494140625, Train Gradient: 49.0595703125\n",
      "Epoch 6601/150000, Loss: 6656.59326171875, Validation Loss: 6848.08349609375, Train Gradient: 48.23969650268555\n",
      "Epoch 6701/150000, Loss: 6503.68701171875, Validation Loss: 6693.19921875, Train Gradient: 47.53695297241211\n",
      "Epoch 6801/150000, Loss: 6353.1142578125, Validation Loss: 6540.89111328125, Train Gradient: 46.702205657958984\n",
      "Epoch 6901/150000, Loss: 6204.89990234375, Validation Loss: 6391.1845703125, Train Gradient: 45.88271713256836\n",
      "Epoch 7001/150000, Loss: 6059.06787109375, Validation Loss: 6243.966796875, Train Gradient: 45.10670471191406\n",
      "Epoch 7101/150000, Loss: 5915.63916015625, Validation Loss: 6099.5009765625, Train Gradient: 44.15038299560547\n",
      "Epoch 7201/150000, Loss: 5774.6298828125, Validation Loss: 5957.23095703125, Train Gradient: 43.553619384765625\n",
      "Epoch 7301/150000, Loss: 5636.08154296875, Validation Loss: 5817.7626953125, Train Gradient: 42.67282485961914\n",
      "Epoch 7401/150000, Loss: 5500.09033203125, Validation Loss: 5680.7431640625, Train Gradient: 42.605125427246094\n",
      "Epoch 7501/150000, Loss: 5366.3935546875, Validation Loss: 5546.40234375, Train Gradient: 41.298133850097656\n",
      "Epoch 7601/150000, Loss: 5235.275390625, Validation Loss: 5414.4619140625, Train Gradient: 40.51988983154297\n",
      "Epoch 7701/150000, Loss: 5106.57080078125, Validation Loss: 5284.796875, Train Gradient: 39.82176208496094\n",
      "Epoch 7801/150000, Loss: 4980.13037109375, Validation Loss: 5157.1630859375, Train Gradient: 39.37256622314453\n",
      "Epoch 7901/150000, Loss: 4855.77197265625, Validation Loss: 5032.7734375, Train Gradient: 38.462162017822266\n",
      "Epoch 8001/150000, Loss: 4733.64111328125, Validation Loss: 4910.3916015625, Train Gradient: 37.78108596801758\n",
      "Epoch 8101/150000, Loss: 4613.7099609375, Validation Loss: 4790.37109375, Train Gradient: 36.89281463623047\n",
      "Epoch 8201/150000, Loss: 4495.94873046875, Validation Loss: 4672.24853515625, Train Gradient: 36.440223693847656\n",
      "Epoch 8301/150000, Loss: 4380.416015625, Validation Loss: 4556.603515625, Train Gradient: 35.89562225341797\n",
      "Epoch 8401/150000, Loss: 4267.099609375, Validation Loss: 4443.283203125, Train Gradient: 35.13706588745117\n",
      "Epoch 8501/150000, Loss: 4155.9833984375, Validation Loss: 4332.04052734375, Train Gradient: 34.46576690673828\n",
      "Epoch 8601/150000, Loss: 4047.05615234375, Validation Loss: 4222.98046875, Train Gradient: 33.82331848144531\n",
      "Epoch 8701/150000, Loss: 3940.2890625, Validation Loss: 4116.12548828125, Train Gradient: 33.16727828979492\n",
      "Epoch 8801/150000, Loss: 3835.715576171875, Validation Loss: 4011.892578125, Train Gradient: 32.19709014892578\n",
      "Epoch 8901/150000, Loss: 3733.241943359375, Validation Loss: 3908.688232421875, Train Gradient: 32.217872619628906\n",
      "Epoch 9001/150000, Loss: 3632.902099609375, Validation Loss: 3808.75732421875, Train Gradient: 31.357139587402344\n",
      "Epoch 9101/150000, Loss: 3534.717041015625, Validation Loss: 3710.80517578125, Train Gradient: 30.695865631103516\n",
      "Epoch 9201/150000, Loss: 3438.661865234375, Validation Loss: 3615.06201171875, Train Gradient: 30.019458770751953\n",
      "Epoch 9301/150000, Loss: 3344.781982421875, Validation Loss: 3520.937255859375, Train Gradient: 29.924327850341797\n",
      "Epoch 9401/150000, Loss: 3252.896728515625, Validation Loss: 3429.828857421875, Train Gradient: 28.841527938842773\n",
      "Epoch 9501/150000, Loss: 3163.11962890625, Validation Loss: 3340.57275390625, Train Gradient: 28.083213806152344\n",
      "Epoch 9601/150000, Loss: 3075.23095703125, Validation Loss: 3252.46337890625, Train Gradient: 27.72237205505371\n",
      "Epoch 9701/150000, Loss: 2989.33203125, Validation Loss: 3166.78125, Train Gradient: 27.147855758666992\n",
      "Epoch 9801/150000, Loss: 2905.37939453125, Validation Loss: 3083.158447265625, Train Gradient: 26.521343231201172\n",
      "Epoch 9901/150000, Loss: 2823.395751953125, Validation Loss: 3001.3701171875, Train Gradient: 26.39338493347168\n",
      "Epoch 10001/150000, Loss: 2743.22314453125, Validation Loss: 2921.662841796875, Train Gradient: 25.487415313720703\n",
      "Epoch 10101/150000, Loss: 2664.9658203125, Validation Loss: 2843.964111328125, Train Gradient: 24.89385223388672\n",
      "Epoch 10201/150000, Loss: 2588.555419921875, Validation Loss: 2768.041259765625, Train Gradient: 24.520294189453125\n",
      "Epoch 10301/150000, Loss: 2513.9453125, Validation Loss: 2694.310546875, Train Gradient: 23.8940486907959\n",
      "Epoch 10401/150000, Loss: 2441.09619140625, Validation Loss: 2622.27978515625, Train Gradient: 23.426729202270508\n",
      "Epoch 10501/150000, Loss: 2369.9404296875, Validation Loss: 2551.94189453125, Train Gradient: 23.029586791992188\n",
      "Epoch 10601/150000, Loss: 2300.381591796875, Validation Loss: 2483.59716796875, Train Gradient: 22.408323287963867\n",
      "Epoch 10701/150000, Loss: 2232.387451171875, Validation Loss: 2416.673583984375, Train Gradient: 21.747331619262695\n",
      "Epoch 10801/150000, Loss: 2165.98388671875, Validation Loss: 2350.542236328125, Train Gradient: 22.088642120361328\n",
      "Epoch 10901/150000, Loss: 2100.912109375, Validation Loss: 2286.134521484375, Train Gradient: 21.052778244018555\n",
      "Epoch 11001/150000, Loss: 2037.4046630859375, Validation Loss: 2224.113037109375, Train Gradient: 20.760013580322266\n",
      "Epoch 11101/150000, Loss: 1975.3892822265625, Validation Loss: 2163.035888671875, Train Gradient: 20.22439193725586\n",
      "Epoch 11201/150000, Loss: 1914.8851318359375, Validation Loss: 2103.272216796875, Train Gradient: 19.525312423706055\n",
      "Epoch 11301/150000, Loss: 1855.883544921875, Validation Loss: 2044.411376953125, Train Gradient: 18.99189567565918\n",
      "Early stopping at epoch 11329 with validation loss 2041.08349609375.\n",
      "Test Loss: 1999.36865234375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.01]\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0] \n",
    "weight_decays = [1e-2]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "                # Monitor gradients, print(\"Gradients:\"), print(f\"{name}: {param.grad.norm().item()}\")\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Train Gradient: {param.grad.norm().item()}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.01, window_size=20, dropout_prob=0, weight_decay=1e-05\n",
      "Epoch 1/150000, Loss: 29889.6171875, Validation Loss: 30381.451171875, Train Gradient: 223.41114807128906\n",
      "Epoch 101/150000, Loss: 28942.42578125, Validation Loss: 29417.9140625, Train Gradient: 214.79165649414062\n",
      "Epoch 201/150000, Loss: 27956.173828125, Validation Loss: 28422.931640625, Train Gradient: 205.37060546875\n",
      "Epoch 301/150000, Loss: 27119.40625, Validation Loss: 27576.615234375, Train Gradient: 197.0470733642578\n",
      "Epoch 401/150000, Loss: 26354.357421875, Validation Loss: 26802.234375, Train Gradient: 189.11953735351562\n",
      "Epoch 501/150000, Loss: 25644.255859375, Validation Loss: 26083.021484375, Train Gradient: 181.4528350830078\n",
      "Epoch 601/150000, Loss: 24981.658203125, Validation Loss: 25411.521484375, Train Gradient: 173.99520874023438\n",
      "Epoch 701/150000, Loss: 24362.10546875, Validation Loss: 24783.259765625, Train Gradient: 166.7208709716797\n",
      "Epoch 801/150000, Loss: 23782.439453125, Validation Loss: 24195.0625, Train Gradient: 159.61509704589844\n",
      "Epoch 901/150000, Loss: 23240.16015625, Validation Loss: 23644.435546875, Train Gradient: 152.668701171875\n",
      "Epoch 1001/150000, Loss: 22733.173828125, Validation Loss: 23129.26953125, Train Gradient: 145.87562561035156\n",
      "Epoch 1101/150000, Loss: 22259.6328125, Validation Loss: 22647.720703125, Train Gradient: 139.23178100585938\n",
      "Epoch 1201/150000, Loss: 21645.744140625, Validation Loss: 22046.07421875, Train Gradient: 148.96539306640625\n",
      "Epoch 1301/150000, Loss: 21121.232421875, Validation Loss: 21517.69140625, Train Gradient: 144.31399536132812\n",
      "Epoch 1401/150000, Loss: 20619.18359375, Validation Loss: 21008.232421875, Train Gradient: 140.8512725830078\n",
      "Epoch 1501/150000, Loss: 20133.65625, Validation Loss: 20515.275390625, Train Gradient: 137.51426696777344\n",
      "Epoch 1601/150000, Loss: 19663.236328125, Validation Loss: 20037.34765625, Train Gradient: 134.3053741455078\n",
      "Epoch 1701/150000, Loss: 19207.06640625, Validation Loss: 19573.958984375, Train Gradient: 131.2064666748047\n",
      "Epoch 1801/150000, Loss: 18764.349609375, Validation Loss: 19124.23828125, Train Gradient: 128.21153259277344\n",
      "Epoch 1901/150000, Loss: 18334.376953125, Validation Loss: 18687.482421875, Train Gradient: 125.32232666015625\n",
      "Epoch 2001/150000, Loss: 17916.458984375, Validation Loss: 18262.84375, Train Gradient: 122.51752471923828\n",
      "Epoch 2101/150000, Loss: 17510.171875, Validation Loss: 17850.00390625, Train Gradient: 119.75624084472656\n",
      "Epoch 2201/150000, Loss: 17115.3515625, Validation Loss: 17448.955078125, Train Gradient: 117.01824951171875\n",
      "Epoch 2301/150000, Loss: 16731.814453125, Validation Loss: 17059.419921875, Train Gradient: 114.31605529785156\n",
      "Epoch 2401/150000, Loss: 16359.3095703125, Validation Loss: 16681.095703125, Train Gradient: 111.65606689453125\n",
      "Epoch 2501/150000, Loss: 15997.5576171875, Validation Loss: 16313.8564453125, Train Gradient: 109.04275512695312\n",
      "Epoch 2601/150000, Loss: 15646.2509765625, Validation Loss: 15957.625, Train Gradient: 106.47942352294922\n",
      "Epoch 2701/150000, Loss: 15305.0576171875, Validation Loss: 15612.1064453125, Train Gradient: 103.93756103515625\n",
      "Epoch 2801/150000, Loss: 14973.6904296875, Validation Loss: 15276.6513671875, Train Gradient: 101.49523162841797\n",
      "Epoch 2901/150000, Loss: 14651.97265625, Validation Loss: 14950.7509765625, Train Gradient: 99.02365112304688\n",
      "Epoch 3001/150000, Loss: 14339.58203125, Validation Loss: 14632.2099609375, Train Gradient: 96.71006774902344\n",
      "Epoch 3101/150000, Loss: 14033.451171875, Validation Loss: 14321.2353515625, Train Gradient: 94.71550750732422\n",
      "Epoch 3201/150000, Loss: 13736.1787109375, Validation Loss: 14019.1884765625, Train Gradient: 92.54122161865234\n",
      "Epoch 3301/150000, Loss: 13446.744140625, Validation Loss: 13726.01953125, Train Gradient: 90.43709564208984\n",
      "Epoch 3401/150000, Loss: 13164.7626953125, Validation Loss: 13440.6484375, Train Gradient: 88.39188385009766\n",
      "Epoch 3501/150000, Loss: 12889.765625, Validation Loss: 13162.0908203125, Train Gradient: 86.43304443359375\n",
      "Epoch 3601/150000, Loss: 12621.3408203125, Validation Loss: 12889.744140625, Train Gradient: 84.54249572753906\n",
      "Epoch 3701/150000, Loss: 12358.5595703125, Validation Loss: 12619.810546875, Train Gradient: 82.40773010253906\n",
      "Epoch 3801/150000, Loss: 12102.1689453125, Validation Loss: 12359.3837890625, Train Gradient: 81.00444793701172\n",
      "Epoch 3901/150000, Loss: 11851.833984375, Validation Loss: 12105.3505859375, Train Gradient: 79.37396240234375\n",
      "Epoch 4001/150000, Loss: 11607.2587890625, Validation Loss: 11857.4365234375, Train Gradient: 77.6087875366211\n",
      "Epoch 4101/150000, Loss: 11367.982421875, Validation Loss: 11614.984375, Train Gradient: 76.17671203613281\n",
      "Epoch 4201/150000, Loss: 11134.15625, Validation Loss: 11377.943359375, Train Gradient: 74.02030944824219\n",
      "Epoch 4301/150000, Loss: 10905.2412109375, Validation Loss: 11146.7001953125, Train Gradient: 72.86602020263672\n",
      "Epoch 4401/150000, Loss: 10681.109375, Validation Loss: 10920.6142578125, Train Gradient: 71.35796356201172\n",
      "Epoch 4501/150000, Loss: 10461.671875, Validation Loss: 10699.09375, Train Gradient: 69.84722137451172\n",
      "Epoch 4601/150000, Loss: 10246.7646484375, Validation Loss: 10481.197265625, Train Gradient: 68.04202270507812\n",
      "Epoch 4701/150000, Loss: 10035.869140625, Validation Loss: 10267.5380859375, Train Gradient: 67.170166015625\n",
      "Epoch 4801/150000, Loss: 9829.1025390625, Validation Loss: 10057.3623046875, Train Gradient: 65.87147521972656\n",
      "Epoch 4901/150000, Loss: 9626.6123046875, Validation Loss: 9851.2939453125, Train Gradient: 65.00438690185547\n",
      "Epoch 5001/150000, Loss: 9427.1259765625, Validation Loss: 9649.4736328125, Train Gradient: 63.283905029296875\n",
      "Epoch 5101/150000, Loss: 9231.982421875, Validation Loss: 9451.2138671875, Train Gradient: 61.348026275634766\n",
      "Epoch 5201/150000, Loss: 9039.11328125, Validation Loss: 9256.2255859375, Train Gradient: 61.07636260986328\n",
      "Epoch 5301/150000, Loss: 8849.919921875, Validation Loss: 9064.84375, Train Gradient: 59.99632263183594\n",
      "Epoch 5401/150000, Loss: 8663.7421875, Validation Loss: 8876.6767578125, Train Gradient: 58.9392204284668\n",
      "Epoch 5501/150000, Loss: 8480.685546875, Validation Loss: 8693.2392578125, Train Gradient: 58.079551696777344\n",
      "Epoch 5601/150000, Loss: 8300.0361328125, Validation Loss: 8509.88671875, Train Gradient: 56.92034149169922\n",
      "Epoch 5701/150000, Loss: 8122.28369140625, Validation Loss: 8329.16015625, Train Gradient: 55.95402145385742\n",
      "Epoch 5801/150000, Loss: 7947.193359375, Validation Loss: 8151.3330078125, Train Gradient: 55.009849548339844\n",
      "Epoch 5901/150000, Loss: 7774.69970703125, Validation Loss: 7976.86083984375, Train Gradient: 54.088260650634766\n",
      "Epoch 6001/150000, Loss: 7604.751953125, Validation Loss: 7805.0576171875, Train Gradient: 53.186920166015625\n",
      "Epoch 6101/150000, Loss: 7437.3056640625, Validation Loss: 7635.7646484375, Train Gradient: 52.30511474609375\n",
      "Epoch 6201/150000, Loss: 7272.37109375, Validation Loss: 7468.66748046875, Train Gradient: 51.22475051879883\n",
      "Epoch 6301/150000, Loss: 7109.74560546875, Validation Loss: 7303.9287109375, Train Gradient: 50.646575927734375\n",
      "Epoch 6401/150000, Loss: 6949.52685546875, Validation Loss: 7141.1787109375, Train Gradient: 49.72145080566406\n",
      "Epoch 6501/150000, Loss: 6791.662109375, Validation Loss: 6979.59033203125, Train Gradient: 48.98487854003906\n",
      "Epoch 6601/150000, Loss: 6636.11572265625, Validation Loss: 6823.82666015625, Train Gradient: 48.13865280151367\n",
      "Epoch 6701/150000, Loss: 6483.01513671875, Validation Loss: 6669.296875, Train Gradient: 47.33808898925781\n",
      "Epoch 6801/150000, Loss: 6332.28466796875, Validation Loss: 6517.19921875, Train Gradient: 46.547420501708984\n",
      "Epoch 6901/150000, Loss: 6183.9228515625, Validation Loss: 6366.97802734375, Train Gradient: 45.76325988769531\n",
      "Epoch 7001/150000, Loss: 6037.80029296875, Validation Loss: 6218.7470703125, Train Gradient: 45.008644104003906\n",
      "Epoch 7101/150000, Loss: 5894.107421875, Validation Loss: 6073.60205078125, Train Gradient: 44.23640441894531\n",
      "Epoch 7201/150000, Loss: 5752.84423828125, Validation Loss: 5931.0712890625, Train Gradient: 43.46893310546875\n",
      "Epoch 7301/150000, Loss: 5614.0302734375, Validation Loss: 5791.087890625, Train Gradient: 42.704776763916016\n",
      "Epoch 7401/150000, Loss: 5477.68896484375, Validation Loss: 5653.63720703125, Train Gradient: 41.94367218017578\n",
      "Epoch 7501/150000, Loss: 5343.8359375, Validation Loss: 5518.697265625, Train Gradient: 41.18565368652344\n",
      "Epoch 7601/150000, Loss: 5212.4775390625, Validation Loss: 5386.232421875, Train Gradient: 40.43446350097656\n",
      "Epoch 7701/150000, Loss: 5083.52978515625, Validation Loss: 5256.14111328125, Train Gradient: 39.726234436035156\n",
      "Epoch 7801/150000, Loss: 4956.791015625, Validation Loss: 5128.7021484375, Train Gradient: 39.02333068847656\n",
      "Epoch 7901/150000, Loss: 4832.30810546875, Validation Loss: 5003.73828125, Train Gradient: 38.32734298706055\n",
      "Epoch 8001/150000, Loss: 4710.03759765625, Validation Loss: 4881.28857421875, Train Gradient: 37.64732360839844\n",
      "Epoch 8101/150000, Loss: 4590.00244140625, Validation Loss: 4761.390625, Train Gradient: 36.69717025756836\n",
      "Epoch 8201/150000, Loss: 4472.14990234375, Validation Loss: 4643.16015625, Train Gradient: 36.304325103759766\n",
      "Epoch 8301/150000, Loss: 4356.57373046875, Validation Loss: 4527.8291015625, Train Gradient: 35.237239837646484\n",
      "Epoch 8401/150000, Loss: 4243.091796875, Validation Loss: 4413.8564453125, Train Gradient: 34.98442840576172\n",
      "Epoch 8501/150000, Loss: 4131.84716796875, Validation Loss: 4302.54443359375, Train Gradient: 34.26683044433594\n",
      "Epoch 8601/150000, Loss: 4022.763916015625, Validation Loss: 4193.30615234375, Train Gradient: 33.69369888305664\n",
      "Epoch 8701/150000, Loss: 3915.9521484375, Validation Loss: 4086.92626953125, Train Gradient: 32.444740295410156\n",
      "Epoch 8801/150000, Loss: 3811.075439453125, Validation Loss: 3981.65869140625, Train Gradient: 32.41587448120117\n",
      "Epoch 8901/150000, Loss: 3708.4580078125, Validation Loss: 3879.26806640625, Train Gradient: 31.658504486083984\n",
      "Epoch 9001/150000, Loss: 3607.988037109375, Validation Loss: 3779.042724609375, Train Gradient: 30.98318099975586\n",
      "Epoch 9101/150000, Loss: 3509.651611328125, Validation Loss: 3680.846435546875, Train Gradient: 30.497812271118164\n",
      "Epoch 9201/150000, Loss: 3413.498046875, Validation Loss: 3584.992431640625, Train Gradient: 29.78415870666504\n",
      "Epoch 9301/150000, Loss: 3319.55517578125, Validation Loss: 3490.925048828125, Train Gradient: 29.7373104095459\n",
      "Epoch 9401/150000, Loss: 3227.59521484375, Validation Loss: 3399.4814453125, Train Gradient: 28.617351531982422\n",
      "Epoch 9501/150000, Loss: 3137.91015625, Validation Loss: 3310.8623046875, Train Gradient: 28.28723907470703\n",
      "Epoch 9601/150000, Loss: 3050.1611328125, Validation Loss: 3225.602294921875, Train Gradient: 27.614957809448242\n",
      "Epoch 9701/150000, Loss: 2964.532470703125, Validation Loss: 3137.79248046875, Train Gradient: 26.833255767822266\n",
      "Epoch 9801/150000, Loss: 2880.97705078125, Validation Loss: 3054.592041015625, Train Gradient: 26.17530059814453\n",
      "Epoch 9901/150000, Loss: 2799.25732421875, Validation Loss: 2973.270263671875, Train Gradient: 25.803739547729492\n",
      "Epoch 10001/150000, Loss: 2719.632080078125, Validation Loss: 2894.570068359375, Train Gradient: 25.025482177734375\n",
      "Epoch 10101/150000, Loss: 2641.339111328125, Validation Loss: 2818.282958984375, Train Gradient: 24.94207000732422\n",
      "Epoch 10201/150000, Loss: 2563.857421875, Validation Loss: 2741.8623046875, Train Gradient: 24.25090217590332\n",
      "Epoch 10301/150000, Loss: 2488.939697265625, Validation Loss: 2667.064453125, Train Gradient: 23.71471405029297\n",
      "Epoch 10401/150000, Loss: 2415.866455078125, Validation Loss: 2594.23291015625, Train Gradient: 23.19598388671875\n",
      "Epoch 10501/150000, Loss: 2344.47412109375, Validation Loss: 2523.059326171875, Train Gradient: 22.705102920532227\n",
      "Epoch 10601/150000, Loss: 2274.40478515625, Validation Loss: 2450.245849609375, Train Gradient: 22.240703582763672\n",
      "Early stopping at epoch 10637 with validation loss 2434.228515625.\n",
      "Test Loss: 2391.616455078125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.01]\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0] \n",
    "weight_decays = [1e-5]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "                # Monitor gradients, print(\"Gradients:\"), print(f\"{name}: {param.grad.norm().item()}\")\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Train Gradient: {param.grad.norm().item()}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
