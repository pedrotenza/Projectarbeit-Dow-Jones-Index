{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Training Loss: 0.3947, Validation Loss: 0.2523\n",
      "Epoch [200/1000], Training Loss: 0.2791, Validation Loss: 0.2480\n",
      "Epoch [300/1000], Training Loss: 0.2611, Validation Loss: 0.2481\n",
      "Epoch [400/1000], Training Loss: 0.2384, Validation Loss: 0.2478\n",
      "Epoch [500/1000], Training Loss: 0.2951, Validation Loss: 0.2482\n",
      "Epoch [600/1000], Training Loss: 0.2489, Validation Loss: 0.2478\n",
      "Epoch [700/1000], Training Loss: 0.2534, Validation Loss: 0.2479\n",
      "Epoch [800/1000], Training Loss: 0.2882, Validation Loss: 0.2478\n",
      "Epoch [900/1000], Training Loss: 0.2630, Validation Loss: 0.2478\n",
      "Epoch [1000/1000], Training Loss: 0.2441, Validation Loss: 0.2475\n",
      "Mean Squared Error on Training Data: 0.2451\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "# Convert the 'date' column to datetime\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.isnull().sum()\n",
    "data = data.fillna(0)  # Filling null values with zero\n",
    "\n",
    "# Specify the columns you want to standardize\n",
    "columns_to_standardize = [\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Loop through the columns and standardize each one\n",
    "for column in columns_to_standardize:\n",
    "    data[column] = scaler.fit_transform(data[[column]])\n",
    "\n",
    "# Extract day, month, and year from the date column\n",
    "data['day'] = data['date'].dt.day\n",
    "data['month'] = data['date'].dt.month\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "# Define the embedding dimensions\n",
    "embedding_dim = 5  # You can adjust this dimension as needed\n",
    "\n",
    "# Create embedding layers for day, month, and year\n",
    "day_embedding = nn.Embedding(32, embedding_dim)  # 0-31 days\n",
    "month_embedding = nn.Embedding(13, embedding_dim)  # 1-12 months\n",
    "year_embedding = nn.Embedding(44, embedding_dim)  # Embedding for years from 1980 to 2023\n",
    "\n",
    "# Convert day, month, and year to tensors with Long data type\n",
    "day_tensor = torch.LongTensor(data['day'].values)\n",
    "month_tensor = torch.LongTensor(data['month'].values)\n",
    "year_tensor = torch.LongTensor(data['year'].values - 1980)  # Convert years to an index from 0 to 43\n",
    "\n",
    "# Pass tensors through embedding layers to get embeddings\n",
    "day_embeddings = day_embedding(day_tensor)\n",
    "month_embeddings = month_embedding(month_tensor)\n",
    "year_embeddings = year_embedding(year_tensor)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "date_embeddings = torch.cat((day_embeddings, month_embeddings, year_embeddings), dim=1)\n",
    "\n",
    "# Combine date_embeddings with the standardized features\n",
    "input_data = torch.cat((torch.Tensor(data[columns_to_standardize].values), date_embeddings), dim=1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(input_data.detach().numpy(), test_size=0.2, random_state=42)\n",
    "train_data = torch.Tensor(train_data)\n",
    "val_data = torch.Tensor(val_data)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "class TemporalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(TemporalAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.encoder(x))\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Specify the input and hidden size for the autoencoder\n",
    "input_size = input_data.shape[1]\n",
    "hidden_size = 10  # You can adjust this size as needed\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = TemporalAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to PyTorch DataLoader\n",
    "train_dataset = TensorDataset(train_data, train_data)\n",
    "val_dataset = TensorDataset(val_data, val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Train the autoencoder\n",
    "num_epochs = 1000  # You can adjust this number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()  # Set the model to training mode\n",
    "    for data_batch, _ in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstructions = autoencoder(data_batch)\n",
    "        loss = criterion(reconstructions, data_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    autoencoder.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for val_batch, _ in val_loader:\n",
    "            val_reconstructions = autoencoder(val_batch)\n",
    "            val_loss += criterion(val_reconstructions, val_batch).item()\n",
    "\n",
    "    # Print training and validation loss at each epoch\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "    #print(f'Epoch [{epoch+100}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "# Evaluate the autoencoder on the training data\n",
    "with torch.no_grad():\n",
    "    autoencoder.eval()  # Set the model to evaluation mode\n",
    "    reconstructions = autoencoder(train_data)\n",
    "    mse = F.mse_loss(reconstructions, train_data)\n",
    "    print(f'Mean Squared Error on Training Data: {mse.item():.4f}')\n",
    "\n",
    "# Extract the learned temporal features from the encoder\n",
    "with torch.no_grad():\n",
    "    autoencoder.eval()\n",
    "    encoded_data = autoencoder.encoder(input_data).detach().numpy()\n",
    "\n",
    "# Now 'encoded_data' contains the learned temporal features\n",
    "# You can use these features for further analysis or downstream tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_8480\\3393339538.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_train_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Temporal Autoencoders.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Temporal%20Autoencoders.ipynb#W1sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Temporal%20Autoencoders.ipynb#W1sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m# Create a DataLoader for batch training\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Temporal%20Autoencoders.ipynb#W1sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m train_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTensorDataset(x_train_tensor, y_train_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Temporal%20Autoencoders.ipynb#W1sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(dataset\u001b[39m=\u001b[39mtrain_data, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Temporal%20Autoencoders.ipynb#W1sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# Create an instance of the LSTM model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming 'encoded_data' is the result from the first model\n",
    "x_train_encoded = torch.tensor(encoded_data, dtype=torch.float32)\n",
    "\n",
    "# Load the y_train data\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Concatenate the encoded data with the target variable\n",
    "x_train_combined = torch.cat((x_train_encoded, torch.tensor(y_train, dtype=torch.float32).view(-1, 1)), dim=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train_combined, x_test_combined, _, y_test = train_test_split(\n",
    "    x_train_combined, y_train, test_size=0.33, random_state=45\n",
    ")\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_combined_scaled = scaler.fit_transform(x_train_combined)\n",
    "x_test_combined_scaled = scaler.transform(x_test_combined)\n",
    "\n",
    "# Concatenate date embeddings with your feature vectors for training data\n",
    "x_train_feature_tensors = torch.tensor(x_train_combined_scaled[:, :-1], dtype=torch.float32)\n",
    "x_train_date_embeddings = date_embeddings[:len(x_train_combined_scaled)]\n",
    "x_train_combined = torch.cat((x_train_feature_tensors, x_train_date_embeddings), dim=1)\n",
    "\n",
    "# Convert the combined data to PyTorch tensors\n",
    "x_train_tensor = x_train_combined.clone().detach()\n",
    "x_train_tensor = x_train_tensor.to(torch.float32)\n",
    "\n",
    "# Concatenate date embeddings with your testing data\n",
    "x_test_feature_tensors = torch.tensor(x_test_combined_scaled[:, :-1], dtype=torch.float32)\n",
    "x_test_date_embeddings = date_embeddings[len(x_train_combined_scaled):]\n",
    "x_test_combined = torch.cat((x_test_feature_tensors, x_test_date_embeddings), dim=1)\n",
    "\n",
    "# Convert the combined testing data to PyTorch tensors\n",
    "x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Inside your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Define the LSTM layer as a class attribute\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Ensure input has the shape [batch_size, sequence_length, input_size]\n",
    "        x = x.view(batch_size, 1, -1)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = x_train_tensor.shape[1]\n",
    "hidden_dim = 32\n",
    "output_size = 1\n",
    "n_layers = 4\n",
    "learning_rate = 0.00015\n",
    "batch_size = 64\n",
    "\n",
    "# Create a DataLoader for batch training\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, hidden_dim, output_size, n_layers).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Hyperparameters: Learning Rate={learning_rate}, Hidden Dim={hidden_dim}, Layers={n_layers}')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Extract features on the testing set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "            val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# Print the predictions on the test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_predictions = model(x_test_tensor.view(x_test_tensor.size(0), 1, -1)).numpy()\n",
    "\n",
    "# Print or use 'test_predictions' for further analysis\n",
    "print(test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.- IMPORTING LIBRARIES: The script starts by importing the necessary libraries, including pandas, numpy, torch, and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.-DATA PREPROCESSING: The CSV file containing historical stock data is read using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjusted_close</th>\n",
       "      <th>change_percent</th>\n",
       "      <th>avg_vol_20d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-12-12</td>\n",
       "      <td>28.7392</td>\n",
       "      <td>28.8736</td>\n",
       "      <td>28.7392</td>\n",
       "      <td>28.7392</td>\n",
       "      <td>2093900</td>\n",
       "      <td>0.0994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     open     high      low    close   volume  adjusted_close  \\\n",
       "0  1980-12-12  28.7392  28.8736  28.7392  28.7392  2093900          0.0994   \n",
       "\n",
       "   change_percent  avg_vol_20d  \n",
       "0             NaN          NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "#data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between 'close' and 'adjusted_close': 0.28\n"
     ]
    }
   ],
   "source": [
    "correlation = data['close'].corr(data['adjusted_close'])\n",
    "print(f\"Correlation between 'close' and 'adjusted_close': {correlation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between 'volume' and 'adjusted_close': 0.05\n"
     ]
    }
   ],
   "source": [
    "correlation = data['volume'].corr(data['avg_vol_20d'])\n",
    "print(f\"Correlation between 'volume' and 'adjusted_close': {correlation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.- Data Info: to get a summary of the dataset, including the count of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.- describe() method to get summary statistics for the dataset, including the count, mean, and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.- BOX PLOT is a graphical representation of the distribution of a dataset. It provides a concise summary of key statistical properties, such as the median, quartiles, and potential outliers within the data. Box plots are particularly useful for visualizing the spread and skewness of data and for identifying potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\n\\n\\n# Initialize an empty list to store valid column names\\nvalid_columns = []\\n\\n# Iterate through columns and check data type\\nfor column_name in data.columns:\\n    if pd.api.types.is_numeric_dtype(data[column_name]):\\n        valid_columns.append(column_name)\\n\\n# Create box plots for valid columns\\nfor column_name in valid_columns:\\n    plt.figure()  # Create a new figure for each box plot\\n    plt.boxplot(data[column_name])\\n    plt.xlabel(\"X-axis label\")\\n    plt.ylabel(\"Y-axis label\")\\n    plt.title(f\"Box Plot for {column_name}\")\\n\\n# Show all the plots\\nplt.show()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Initialize an empty list to store valid column names\n",
    "valid_columns = []\n",
    "\n",
    "# Iterate through columns and check data type\n",
    "for column_name in data.columns:\n",
    "    if pd.api.types.is_numeric_dtype(data[column_name]):\n",
    "        valid_columns.append(column_name)\n",
    "\n",
    "# Create box plots for valid columns\n",
    "for column_name in valid_columns:\n",
    "    plt.figure()  # Create a new figure for each box plot\n",
    "    plt.boxplot(data[column_name])\n",
    "    plt.xlabel(\"X-axis label\")\n",
    "    plt.ylabel(\"Y-axis label\")\n",
    "    plt.title(f\"Box Plot for {column_name}\")\n",
    "\n",
    "# Show all the plots\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.- Heatmap to visualize CORRELATIONS between multiple variables in a dataset, making it easier to identify relationships and patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\nplt.figure(figsize=(8, 8))\\nheatmap = sns.heatmap(data.corr(), annot=True, fmt=\".4f\")  # Display two decimal places\\n\\n# Get the labels from the DataFrame columns\\nlabels = data.columns\\n\\n# Set the labels on the x-axis (bottom) and y-axis (left) to be the variable names\\nplt.xticks(rotation=45, ha=\\'right\\')\\nplt.yticks(rotation=0)\\n\\nplt.show()\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "heatmap = sns.heatmap(data.corr(), annot=True, fmt=\".4f\")  # Display two decimal places\n",
    "\n",
    "# Get the labels from the DataFrame columns\n",
    "labels = data.columns\n",
    "\n",
    "# Set the labels on the x-axis (bottom) and y-axis (left) to be the variable names\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.- PAIRPLOT: is a grid of scatterplots that shows the relationships between pairs of variables in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(data);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6.- Standardization (z-score normalization) transforms data to have a \n",
    "mean: 0 \n",
    "standard deviation: 1 \n",
    "making it comparable and suitable for machine learning; \n",
    "it helps with outlier handling and improves interpretability of model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns you want to standardize\n",
    "columns_to_standardize = [\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Loop through the columns and standardize each one\n",
    "for column in columns_to_standardize:\n",
    "    data[column] = scaler.fit_transform(data[[column]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7.-t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    " is a dimensionality reduction technique that is widely used for data visualization and exploration. It helps uncover patterns and relationships within data by projecting it into a lower-dimensional space while preserving pairwise similarities among data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nimport seaborn as sns\\n\\n# Function to load and preprocess data\\ndef load_and_preprocess_data():\\n\\n    # Create the feature matrix \\'x\\' and binarize the target \\'y\\'\\n    x = data[[\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]].to_numpy()\\n\\n    y = np.where(data[\"close\"] >= data[\"open\"], 1, 0)\\n\\n    return x, y\\n\\n# Function to perform t-SNE and plot the results in 3D\\ndef perform_tsne_and_plot_3d(x, y, feature_names):\\n    tsne = TSNE(n_iter=1000, n_components=3)  # Set n_components to 3 for 3D visualization\\n    embedding = tsne.fit_transform(x)\\n\\n    # Create a 3D scatter plot with feature names as labels, legends, and title\\n    fig = plt.figure(figsize=(12, 10))\\n    ax = fig.add_subplot(111, projection=\\'3d\\')\\n\\n    palette = sns.color_palette(\"husl\", 10)\\n\\n    for i in range(10):\\n        ax.scatter(\\n            embedding[np.where(y == i), 0],\\n            embedding[np.where(y == i), 1],\\n            embedding[np.where(y == i), 2],\\n            label=feature_names[i],  # Use feature names as labels\\n            alpha=0.7, s=50, edgecolor=\\'k\\', c=[palette[i]]\\n        )\\n\\n    ax.legend(title=\"Feature Names\", labels=feature_names)  # Set feature names as legend labels\\n    ax.set_title(\"Apple t-SNE 3D Embedding\")\\n    plt.show()\\n\\n# Main function\\ndef main():\\n    # Load and preprocess data\\n    x, y = load_and_preprocess_data()\\n\\n    # Define feature names corresponding to each class (0 to 9)\\n    feature_names = [\\n       \"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\",\\n    ]\\n\\n    # Perform t-SNE and plot the results in 3D\\n    perform_tsne_and_plot_3d(x, y, feature_names)\\n\\nif __name__ == \"__main__\":\\n    main()\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "\n",
    "    # Create the feature matrix 'x' and binarize the target 'y'\n",
    "    x = data[[\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]].to_numpy()\n",
    "\n",
    "    y = np.where(data[\"close\"] >= data[\"open\"], 1, 0)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Function to perform t-SNE and plot the results in 3D\n",
    "def perform_tsne_and_plot_3d(x, y, feature_names):\n",
    "    tsne = TSNE(n_iter=1000, n_components=3)  # Set n_components to 3 for 3D visualization\n",
    "    embedding = tsne.fit_transform(x)\n",
    "\n",
    "    # Create a 3D scatter plot with feature names as labels, legends, and title\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    palette = sns.color_palette(\"husl\", 10)\n",
    "\n",
    "    for i in range(10):\n",
    "        ax.scatter(\n",
    "            embedding[np.where(y == i), 0],\n",
    "            embedding[np.where(y == i), 1],\n",
    "            embedding[np.where(y == i), 2],\n",
    "            label=feature_names[i],  # Use feature names as labels\n",
    "            alpha=0.7, s=50, edgecolor='k', c=[palette[i]]\n",
    "        )\n",
    "\n",
    "    ax.legend(title=\"Feature Names\", labels=feature_names)  # Set feature names as legend labels\n",
    "    ax.set_title(\"Apple t-SNE 3D Embedding\")\n",
    "    plt.show()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    x, y = load_and_preprocess_data()\n",
    "\n",
    "    # Define feature names corresponding to each class (0 to 9)\n",
    "    feature_names = [\n",
    "       \"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\",\n",
    "    ]\n",
    "\n",
    "    # Perform t-SNE and plot the results in 3D\n",
    "    perform_tsne_and_plot_3d(x, y, feature_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8.-Hauptkomponentenanalyse (PCA) \n",
    "\n",
    " is a dimensionality reduction technique. PCA helps in transforming high-dimensional data into a lower-dimensional form while preserving as much of the original data's variance as possible. It does this by identifying the principal components, which are linear combinations of the original features that capture the most significant variations in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom sklearn.decomposition import PCA\\nfrom mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\\n\\n# Load your data into the \\'data\\' DataFrame\\n# Assuming you have a DataFrame named \\'data\\'\\n\\n# Create the feature matrix \\'x\\' and binarize the target \\'y\\'\\n\\nx = data[[\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]].to_numpy()\\n\\ny = np.where(data[\"close\"] >= data[\"open\"], 1, 0)\\n\\n# Define feature labels\\nfeature_labels = [\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]\\n\\n# Perform PCA with 3 components (for 3D visualization)\\npca = PCA(n_components=3)\\nembedding = pca.fit_transform(x)\\n\\n# Create a 3D scatter plot\\nfig = plt.figure(figsize=(12, 10))\\nax = fig.add_subplot(111, projection=\\'3d\\')\\n\\n# Define a color palette with distinct colors for each class\\npalette = plt.cm.Paired(np.linspace(0, 1, 10))\\n\\nfor i in range(10):\\n    ax.scatter(embedding[np.where(y == i), 0], embedding[np.where(y == i), 1], embedding[np.where(y == i), 2],\\n               label=feature_labels[i], alpha=0.7, marker=\\'o\\', s=50, edgecolor=\\'k\\', c=[palette[i]])\\n\\nax.legend(loc=\\'lower right\\')\\nax.set_title(\"Apple PCA Embedding (3D)\")\\n\\n# Set axis labels\\nax.set_xlabel(\"Principal Component 1\")\\nax.set_ylabel(\"Principal Component 2\")\\nax.set_zlabel(\"Principal Component 3\")\\n\\n# Annotate points with feature labels (optional)\\nfor i, label in enumerate(feature_labels):\\n    ax.text(embedding[:, 0][i], embedding[:, 1][i], embedding[:, 2][i], label, fontsize=8)\\n\\nplt.show()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
    "\n",
    "# Load your data into the 'data' DataFrame\n",
    "# Assuming you have a DataFrame named 'data'\n",
    "\n",
    "# Create the feature matrix 'x' and binarize the target 'y'\n",
    "\n",
    "x = data[[\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]].to_numpy()\n",
    "\n",
    "y = np.where(data[\"close\"] >= data[\"open\"], 1, 0)\n",
    "\n",
    "# Define feature labels\n",
    "feature_labels = [\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]\n",
    "\n",
    "# Perform PCA with 3 components (for 3D visualization)\n",
    "pca = PCA(n_components=3)\n",
    "embedding = pca.fit_transform(x)\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Define a color palette with distinct colors for each class\n",
    "palette = plt.cm.Paired(np.linspace(0, 1, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax.scatter(embedding[np.where(y == i), 0], embedding[np.where(y == i), 1], embedding[np.where(y == i), 2],\n",
    "               label=feature_labels[i], alpha=0.7, marker='o', s=50, edgecolor='k', c=[palette[i]])\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_title(\"Apple PCA Embedding (3D)\")\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel(\"Principal Component 1\")\n",
    "ax.set_ylabel(\"Principal Component 2\")\n",
    "ax.set_zlabel(\"Principal Component 3\")\n",
    "\n",
    "# Annotate points with feature labels (optional)\n",
    "for i, label in enumerate(feature_labels):\n",
    "    ax.text(embedding[:, 0][i], embedding[:, 1][i], embedding[:, 2][i], label, fontsize=8)\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0446,  1.3953,  1.4894],\n",
      "        [-0.7662,  1.3953,  1.4894],\n",
      "        [ 0.7816,  1.3953,  1.4894],\n",
      "        ...,\n",
      "        [-0.0917,  1.9948, -1.9794],\n",
      "        [-0.2234,  1.9948, -1.9794],\n",
      "        [-0.0446,  1.9948, -1.9794]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with a 'date' column containing dates from 1980-12-12 to 2023-09-12\n",
    "data2 = pd.DataFrame({'date': pd.date_range(start='1980-12-12', end='2023-09-12')})\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "data2['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Extract day, month, and year from the date column\n",
    "data['day'] = data2['date'].dt.day\n",
    "data['month'] = data2['date'].dt.month\n",
    "data['year'] = data2['date'].dt.year\n",
    "\n",
    "# Define the embedding dimensions\n",
    "embedding_dim = 1  # You can adjust this dimension as needed\n",
    "\n",
    "# Create embedding layers for day, month, and year\n",
    "day_embedding = nn.Embedding(32, embedding_dim)  # 0-31 days\n",
    "month_embedding = nn.Embedding(13, embedding_dim)  # 1-12 months\n",
    "year_embedding = nn.Embedding(44, embedding_dim)  # Embedding for years from 1980 to 2023\n",
    "\n",
    "# Convert day, month, and year to tensors with Long data type\n",
    "day_tensor = torch.LongTensor(data['day'].values)\n",
    "month_tensor = torch.LongTensor(data['month'].values)\n",
    "year_tensor = torch.LongTensor(data['year'].values - 1980)  # Convert years to an index from 0 to 43\n",
    "\n",
    "# Pass tensors through embedding layers to get embeddings\n",
    "day_embeddings = day_embedding(day_tensor)\n",
    "month_embeddings = month_embedding(month_tensor)\n",
    "year_embeddings = year_embedding(year_tensor)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "date_embeddings = torch.cat((day_embeddings, month_embeddings, year_embeddings), dim=1)\n",
    "\n",
    "# Print the resulting embeddings\n",
    "print(date_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ö' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ö\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ö' is not defined"
     ]
    }
   ],
   "source": [
    "ö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_9852\\3844195813.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: Learning Rate=0.00015, Sequence Length=75, Batch Size=64, Input Size=7, Date Embedding Dim=3, Hidden Dim=32,Layers=4\n",
      "Epoch [100/4000], Loss: 17092.7461, Val Loss: 21942.2578\n",
      "Epoch [200/4000], Loss: 14605.1865, Val Loss: 18084.5977\n",
      "Epoch [300/4000], Loss: 8754.7090, Val Loss: 12628.4258\n",
      "Epoch [400/4000], Loss: 12405.7861, Val Loss: 9659.0439\n",
      "Epoch [500/4000], Loss: 11266.8896, Val Loss: 7365.3594\n",
      "Epoch [600/4000], Loss: 4706.5659, Val Loss: 5510.0410\n",
      "Epoch [700/4000], Loss: 2607.5483, Val Loss: 4017.1169\n",
      "Epoch [800/4000], Loss: 5423.0728, Val Loss: 2837.3076\n",
      "Epoch [900/4000], Loss: 2836.4011, Val Loss: 1936.3269\n",
      "Epoch [1000/4000], Loss: 3388.9854, Val Loss: 1264.0769\n",
      "Epoch [1100/4000], Loss: 1677.0354, Val Loss: 778.7593\n",
      "Epoch [1200/4000], Loss: 1314.0870, Val Loss: 451.5746\n",
      "Epoch [1300/4000], Loss: 33.3315, Val Loss: 239.9185\n",
      "Epoch [1400/4000], Loss: 104.7675, Val Loss: 120.4431\n",
      "Epoch [1500/4000], Loss: 234.0286, Val Loss: 57.6882\n",
      "Epoch [1600/4000], Loss: 1.0510, Val Loss: 26.3653\n",
      "Epoch [1700/4000], Loss: 166.2930, Val Loss: 12.2397\n",
      "Epoch [1800/4000], Loss: 0.8262, Val Loss: 6.1795\n",
      "Epoch [1900/4000], Loss: 1.8544, Val Loss: 3.8463\n",
      "Epoch [2000/4000], Loss: 1.1260, Val Loss: 2.4217\n",
      "Epoch [2100/4000], Loss: 0.7698, Val Loss: 1.8143\n",
      "Epoch [2200/4000], Loss: 1.0157, Val Loss: 1.5902\n",
      "Epoch [2300/4000], Loss: 1.9705, Val Loss: 1.4449\n",
      "Epoch [2400/4000], Loss: 1.0450, Val Loss: 1.4137\n",
      "Epoch [2500/4000], Loss: 0.4812, Val Loss: 1.6081\n",
      "Epoch [2600/4000], Loss: 1.9455, Val Loss: 1.4678\n",
      "Epoch [2700/4000], Loss: 0.8740, Val Loss: 1.3995\n",
      "Epoch [2800/4000], Loss: 1.1430, Val Loss: 1.5447\n",
      "Epoch [2900/4000], Loss: 1.1945, Val Loss: 1.2983\n",
      "Epoch [3000/4000], Loss: 1.3892, Val Loss: 1.3642\n",
      "Epoch [3100/4000], Loss: 0.8035, Val Loss: 1.5373\n",
      "Epoch [3200/4000], Loss: 0.7107, Val Loss: 1.3638\n",
      "Epoch [3300/4000], Loss: 0.8714, Val Loss: 1.4458\n",
      "Epoch [3400/4000], Loss: 1.0430, Val Loss: 1.2294\n",
      "Epoch [3500/4000], Loss: 0.8734, Val Loss: 1.4675\n",
      "Epoch [3600/4000], Loss: 0.6290, Val Loss: 1.3511\n",
      "Epoch [3700/4000], Loss: 0.2241, Val Loss: 1.2254\n",
      "Epoch [3800/4000], Loss: 0.8713, Val Loss: 1.1610\n",
      "Epoch [3900/4000], Loss: 1.0486, Val Loss: 1.2524\n",
      "Epoch [4000/4000], Loss: 1.5718, Val Loss: 1.2294\n",
      "tensor([[[-0.0225,  0.0582, -0.0087,  ...,  0.0943,  0.0640, -0.0472]],\n",
      "\n",
      "        [[-0.0205, -0.0126,  0.3360,  ...,  0.4229,  0.0957, -0.0886]],\n",
      "\n",
      "        [[-0.0212,  0.0425,  0.1181,  ...,  0.2501,  0.0674, -0.0559]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0245,  0.0595,  0.0164,  ...,  0.1327,  0.0740, -0.0556]],\n",
      "\n",
      "        [[-0.0244,  0.0591,  0.0194,  ...,  0.1371,  0.0738, -0.0556]],\n",
      "\n",
      "        [[-0.0257,  0.0630,  0.0004,  ...,  0.1073,  0.0779, -0.0573]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the x_train and y_train data\n",
    "x_train = data[['open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].to_numpy()\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Concatenate date embeddings with your testing data\n",
    "x_test_feature_tensors = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "x_test_date_embeddings = date_embeddings[len(x_train_scaled):]  # Use the remaining embeddings for testing data\n",
    "x_test_combined = torch.cat((x_test_feature_tensors, x_test_date_embeddings), dim=1)\n",
    "\n",
    "# Convert the combined testing data to PyTorch tensors\n",
    "x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inside your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size + date_embedding_dim  # Updated input size to include date embeddings\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Define the LSTM layer as a class attribute\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Ensure input has the shape [batch_size, sequence_length, input_size]\n",
    "        x = x.view(batch_size, 1, self.input_size)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 7  # Adjust based on the number of input features (excluding change_percent)\n",
    "date_embedding_dim = 3  # Adjust based on the dimension of your date embeddings\n",
    "output_size = 1\n",
    "hidden_dim = 32\n",
    "n_layers = 4\n",
    "sequence_length = 75  # Keep this as 1 for your input data\n",
    "batch_size = 64\n",
    "\n",
    "# Concatenate date embeddings with your feature vectors\n",
    "x_train_feature_tensors = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_train_date_embeddings = date_embeddings[:len(x_train_scaled)]  # Use the same length as your training data\n",
    "x_train_combined = torch.cat((x_train_feature_tensors, x_train_date_embeddings), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the combined data to PyTorch tensors\n",
    "#x_train_tensor = torch.tensor(x_train_combined, dtype=torch.float32)\n",
    "#y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Copy x_train_combined to create x_train_tensor\n",
    "x_train_tensor = x_train_combined.clone().detach()\n",
    "x_train_tensor = x_train_tensor.to(torch.float32)\n",
    "\n",
    "# Copy x_test_combined to create x_test_tensor\n",
    "x_test_tensor = x_test_combined.clone().detach()\n",
    "x_test_tensor = x_test_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "# Create y_train_tensor directly from y_train\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader for batch training\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.00015\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Hyperparameters: Learning Rate={learning_rate}, Sequence Length={sequence_length}, Batch Size={batch_size}, Input Size={input_size}, Date Embedding Dim={date_embedding_dim}, Hidden Dim={hidden_dim},'\n",
    "              f'Layers={n_layers}')\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 4000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Extract features on the testing set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Extract features from the hidden states\n",
    "            hidden_states, _ = model.lstm(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "            val_outputs = model(x_test_tensor)\n",
    "            val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "        # You can now use 'hidden_states' as the feature representations of your sequences\n",
    "        # The shape of 'hidden_states' will be (batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "# Print hidden_states after training\n",
    "print(hidden_states)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the features extracted from the LSTM as input to a RANDOM FOREST model for a regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSE: 0.4796417605072094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Reshape the hidden states\n",
    "reshaped_features = hidden_states.view(x_test_tensor.size(0), -1).numpy()\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(reshaped_features, y_test)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.predict(reshaped_features)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error) or other evaluation metrics\n",
    "rmse = mean_squared_error(y_test, rf_predictions, squared=False)\n",
    "print(f'Random Forest RMSE: {rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSE: 0.6470004569099246\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Reshape the hidden states\n",
    "reshaped_features = hidden_states.view(x_test_tensor.size(0), -1).numpy()\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(reshaped_features, y_test)\n",
    "\n",
    "# Make predictions\n",
    "xgb_predictions = xgb_model.predict(reshaped_features)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error) or other evaluation metrics\n",
    "rmse = mean_squared_error(y_test, xgb_predictions, squared=False)\n",
    "print(f'XGBoost RMSE: {rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM .ipynb Cell 31\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_squared_error\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Reshape the hidden states\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m reshaped_features \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mview(x_test_tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# List of models to iterate over\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m models \u001b[39m=\u001b[39m [xgb\u001b[39m.\u001b[39mXGBRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m), RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hidden_states' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Reshape the hidden states\n",
    "reshaped_features = hidden_states.view(x_test_tensor.size(0), -1).numpy()\n",
    "\n",
    "# List of models to iterate over\n",
    "models = [xgb.XGBRegressor(n_estimators=100, random_state=42), RandomForestRegressor(n_estimators=100, random_state=42)]\n",
    "\n",
    "for model in models:\n",
    "    # Initialize and train the current model\n",
    "    model.fit(reshaped_features, y_test)\n",
    "\n",
    "    # Make predictions\n",
    "    model_predictions = model.predict(reshaped_features)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = mean_squared_error(y_test, model_predictions, squared=False)\n",
    "\n",
    "    # Print RMSE for the current model\n",
    "    print(f\"Model: {type(model).__name__}, RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_5220\\3494852810.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: Learning Rate=0.00015, Sequence Length=75, Batch Size=64, Input Size=7, Date Embedding Dim=3, Hidden Dim=32,Layers=4\n",
      "Epoch [100/8000], Loss: 17093.5117, Val Loss: 21943.0117\n",
      "Epoch [200/8000], Loss: 13513.0918, Val Loss: 16716.0137\n",
      "Epoch [300/8000], Loss: 8734.6221, Val Loss: 12607.3535\n",
      "Epoch [400/8000], Loss: 12387.0977, Val Loss: 9643.1445\n",
      "Epoch [500/8000], Loss: 11247.2510, Val Loss: 7352.6221\n",
      "Epoch [600/8000], Loss: 4698.8184, Val Loss: 5499.7446\n",
      "Epoch [700/8000], Loss: 2600.6853, Val Loss: 4008.8850\n",
      "Epoch [800/8000], Loss: 5414.8291, Val Loss: 2830.9409\n",
      "Epoch [900/8000], Loss: 2829.0701, Val Loss: 1931.5015\n",
      "Epoch [1000/8000], Loss: 3382.1975, Val Loss: 1260.4774\n",
      "Epoch [1100/8000], Loss: 1672.0790, Val Loss: 776.2928\n",
      "Epoch [1200/8000], Loss: 1310.7181, Val Loss: 449.7979\n",
      "Epoch [1300/8000], Loss: 32.9099, Val Loss: 238.8957\n",
      "Epoch [1400/8000], Loss: 104.1758, Val Loss: 119.7787\n",
      "Epoch [1500/8000], Loss: 233.3927, Val Loss: 57.3146\n",
      "Epoch [1600/8000], Loss: 1.1345, Val Loss: 26.2670\n",
      "Epoch [1700/8000], Loss: 165.6841, Val Loss: 12.0677\n",
      "Epoch [1800/8000], Loss: 0.6999, Val Loss: 6.1915\n",
      "Epoch [1900/8000], Loss: 1.7274, Val Loss: 3.9309\n",
      "Epoch [2000/8000], Loss: 0.8772, Val Loss: 2.2822\n",
      "Epoch [2100/8000], Loss: 0.7572, Val Loss: 1.7192\n",
      "Epoch [2200/8000], Loss: 1.1198, Val Loss: 1.6498\n",
      "Epoch [2300/8000], Loss: 1.8941, Val Loss: 1.5107\n",
      "Epoch [2400/8000], Loss: 0.8555, Val Loss: 1.3498\n",
      "Epoch [2500/8000], Loss: 0.5586, Val Loss: 1.7954\n",
      "Epoch [2600/8000], Loss: 2.9183, Val Loss: 1.3900\n",
      "Epoch [2700/8000], Loss: 0.8381, Val Loss: 1.3859\n",
      "Epoch [2800/8000], Loss: 0.7391, Val Loss: 1.3041\n",
      "Epoch [2900/8000], Loss: 1.2645, Val Loss: 1.2031\n",
      "Epoch [3000/8000], Loss: 1.5023, Val Loss: 1.2690\n",
      "Epoch [3100/8000], Loss: 0.8714, Val Loss: 1.4016\n",
      "Epoch [3200/8000], Loss: 0.6488, Val Loss: 1.2338\n",
      "Epoch [3300/8000], Loss: 1.0347, Val Loss: 1.2385\n",
      "Epoch [3400/8000], Loss: 0.9281, Val Loss: 1.2836\n",
      "Epoch [3500/8000], Loss: 0.2637, Val Loss: 1.2442\n",
      "Epoch [3600/8000], Loss: 0.4930, Val Loss: 1.3068\n",
      "Epoch [3700/8000], Loss: 0.2080, Val Loss: 1.1502\n",
      "Epoch [3800/8000], Loss: 0.8301, Val Loss: 1.1574\n",
      "Epoch [3900/8000], Loss: 1.1748, Val Loss: 1.1252\n",
      "Epoch [4000/8000], Loss: 1.1049, Val Loss: 1.1786\n",
      "Epoch [4100/8000], Loss: 0.5553, Val Loss: 1.2480\n",
      "Epoch [4200/8000], Loss: 1.0996, Val Loss: 1.2834\n",
      "Epoch [4300/8000], Loss: 1.1749, Val Loss: 1.2680\n",
      "Epoch [4400/8000], Loss: 0.2626, Val Loss: 1.1446\n",
      "Epoch [4500/8000], Loss: 1.6311, Val Loss: 1.4137\n",
      "Epoch [4600/8000], Loss: 0.6334, Val Loss: 1.3520\n",
      "Epoch [4700/8000], Loss: 0.9213, Val Loss: 1.4094\n",
      "Epoch [4800/8000], Loss: 0.8734, Val Loss: 1.1530\n",
      "Epoch [4900/8000], Loss: 0.5534, Val Loss: 1.1332\n",
      "Epoch [5000/8000], Loss: 1.8089, Val Loss: 1.1748\n",
      "Epoch [5100/8000], Loss: 0.6947, Val Loss: 1.1428\n",
      "Epoch [5200/8000], Loss: 0.8255, Val Loss: 1.2362\n",
      "Epoch [5300/8000], Loss: 0.5854, Val Loss: 1.3311\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X36sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X36sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X36sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(batch_x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X36sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X36sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs, batch_y\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))  \u001b[39m# Ensure batch_y has the right shape\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X36sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Ensure input has the shape [batch_size, sequence_length, input_size]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X36sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(batch_size, \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X36sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m out, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h0, c0))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X36sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X36sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the x_train and y_train data\n",
    "x_train = data[['open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].to_numpy()\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Concatenate date embeddings with your testing data\n",
    "x_test_feature_tensors = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "x_test_date_embeddings = date_embeddings[len(x_train_scaled):]  # Use the remaining embeddings for testing data\n",
    "x_test_combined = torch.cat((x_test_feature_tensors, x_test_date_embeddings), dim=1)\n",
    "\n",
    "# Convert the combined testing data to PyTorch tensors\n",
    "x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Inside your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size + date_embedding_dim  # Updated input size to include date embeddings\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Ensure input has the shape [batch_size, sequence_length, input_size]\n",
    "        x = x.view(batch_size, 1, self.input_size)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 7  # Adjust based on the number of input features (excluding change_percent)\n",
    "date_embedding_dim = 3  # Adjust based on the dimension of your date embeddings\n",
    "output_size = 1\n",
    "hidden_dim = 502\n",
    "n_layers = 4\n",
    "sequence_length = 75  # Keep this as 1 for your input data\n",
    "batch_size = 64\n",
    "\n",
    "# Concatenate date embeddings with your feature vectors\n",
    "x_train_feature_tensors = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_train_date_embeddings = date_embeddings[:len(x_train_scaled)]  # Use the same length as your training data\n",
    "x_train_combined = torch.cat((x_train_feature_tensors, x_train_date_embeddings), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the combined data to PyTorch tensors\n",
    "#x_train_tensor = torch.tensor(x_train_combined, dtype=torch.float32)\n",
    "#y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Copy x_train_combined to create x_train_tensor\n",
    "x_train_tensor = x_train_combined.clone().detach()\n",
    "x_train_tensor = x_train_tensor.to(torch.float32)\n",
    "\n",
    "# Copy x_test_combined to create x_test_tensor\n",
    "x_test_tensor = x_test_combined.clone().detach()\n",
    "x_test_tensor = x_test_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "# Create y_train_tensor directly from y_train\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader for batch training\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.00015\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Hyperparameters: Learning Rate={learning_rate}, Sequence Length={sequence_length}, Batch Size={batch_size}, Input Size={input_size}, Date Embedding Dim={date_embedding_dim}, Hidden Dim={hidden_dim},'\n",
    "              f'Layers={n_layers}')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 8000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Evaluate on testing set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(x_test_tensor)\n",
    "            val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the x_train and y_train data\n",
    "x_train = data[['open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].to_numpy()\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Concatenate date embeddings with your testing data\n",
    "x_test_feature_tensors = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "x_test_date_embeddings = date_embeddings[len(x_train_scaled):]  # Use the remaining embeddings for testing data\n",
    "x_test_combined = torch.cat((x_test_feature_tensors, x_test_date_embeddings), dim=1)\n",
    "\n",
    "# Convert the combined testing data to PyTorch tensors\n",
    "x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Inside your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size + date_embedding_dim  # Updated input size to include date embeddings\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Ensure input has the shape [batch_size, sequence_length, input_size]\n",
    "        x = x.view(batch_size, 1, self.input_size)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 7  # Adjust based on the number of input features (excluding change_percent)\n",
    "date_embedding_dim = 3  # Adjust based on the dimension of your date embeddings\n",
    "output_size = 1\n",
    "hidden_dim = 32\n",
    "n_layers = 4\n",
    "sequence_length = 75  # Keep this as 1 for your input data\n",
    "batch_size = 64\n",
    "\n",
    "# Concatenate date embeddings with your feature vectors\n",
    "x_train_feature_tensors = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_train_date_embeddings = date_embeddings[:len(x_train_scaled)]  # Use the same length as your training data\n",
    "x_train_combined = torch.cat((x_train_feature_tensors, x_train_date_embeddings), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the combined data to PyTorch tensors\n",
    "#x_train_tensor = torch.tensor(x_train_combined, dtype=torch.float32)\n",
    "#y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Copy x_train_combined to create x_train_tensor\n",
    "x_train_tensor = x_train_combined.clone().detach()\n",
    "x_train_tensor = x_train_tensor.to(torch.float32)\n",
    "\n",
    "# Copy x_test_combined to create x_test_tensor\n",
    "x_test_tensor = x_test_combined.clone().detach()\n",
    "x_test_tensor = x_test_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "# Create y_train_tensor directly from y_train\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader for batch training\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.00015\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Hyperparameters: Learning Rate={learning_rate}, Sequence Length={sequence_length}, Batch Size={batch_size}, Input Size={input_size}, Date Embedding Dim={date_embedding_dim}, Hidden Dim={hidden_dim},'\n",
    "              f'Layers={n_layers}')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 8000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Evaluate on testing set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(x_test_tensor)\n",
    "            val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LSTM model with hyperparameter optimization using Optuna\n",
    "\n",
    " Optuna is generally more computationally efficient than traditional grid search and evolutionary parameter optimization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2023-10-09 11:53:39,130] A new study created in memory with name: no-name-0003f577-e84d-47d5-b2ab-8207026650fe\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_4832\\1271520721.py:99: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
      "[I 2023-10-09 12:52:28,808] Trial 0 finished with value: 1587.290283203125 and parameters: {'learning_rate': 0.00011981449738351975, 'batch_size': 32, 'hidden_dim': 38, 'n_layers': 3, 'sequence_length': 1}. Best is trial 0 with value: 1587.290283203125.\n",
      "[I 2023-10-09 15:32:13,099] Trial 1 finished with value: 17463.806640625 and parameters: {'learning_rate': 0.006908733732558209, 'batch_size': 64, 'hidden_dim': 122, 'n_layers': 6, 'sequence_length': 6}. Best is trial 0 with value: 1587.290283203125.\n",
      "[I 2023-10-09 16:57:30,582] Trial 2 finished with value: 1.9595749378204346 and parameters: {'learning_rate': 0.001238948570141521, 'batch_size': 32, 'hidden_dim': 114, 'n_layers': 4, 'sequence_length': 6}. Best is trial 2 with value: 1.9595749378204346.\n",
      "[I 2023-10-09 18:05:04,291] Trial 3 finished with value: 1.4891705513000488 and parameters: {'learning_rate': 0.0001360296767874139, 'batch_size': 32, 'hidden_dim': 96, 'n_layers': 2, 'sequence_length': 7}. Best is trial 3 with value: 1.4891705513000488.\n",
      "[I 2023-10-09 19:37:31,507] Trial 4 finished with value: 2.944600820541382 and parameters: {'learning_rate': 0.0038547948979262754, 'batch_size': 64, 'hidden_dim': 78, 'n_layers': 4, 'sequence_length': 2}. Best is trial 3 with value: 1.4891705513000488.\n",
      "[I 2023-10-09 20:17:02,473] Trial 5 finished with value: 12934.869140625 and parameters: {'learning_rate': 4.6802275372796983e-05, 'batch_size': 128, 'hidden_dim': 29, 'n_layers': 3, 'sequence_length': 2}. Best is trial 3 with value: 1.4891705513000488.\n",
      "[I 2023-10-09 21:38:47,537] Trial 6 finished with value: 2.0269417762756348 and parameters: {'learning_rate': 0.0003366126376697788, 'batch_size': 32, 'hidden_dim': 48, 'n_layers': 4, 'sequence_length': 8}. Best is trial 3 with value: 1.4891705513000488.\n",
      "[I 2023-10-09 22:45:04,062] Trial 7 finished with value: 23397.626953125 and parameters: {'learning_rate': 1.607276084222012e-05, 'batch_size': 128, 'hidden_dim': 23, 'n_layers': 4, 'sequence_length': 5}. Best is trial 3 with value: 1.4891705513000488.\n",
      "[I 2023-10-10 00:09:21,148] Trial 8 finished with value: 2.1429076194763184 and parameters: {'learning_rate': 0.0011552256895482871, 'batch_size': 32, 'hidden_dim': 123, 'n_layers': 3, 'sequence_length': 4}. Best is trial 3 with value: 1.4891705513000488.\n",
      "[I 2023-10-10 00:46:53,477] Trial 9 finished with value: 1.4735058546066284 and parameters: {'learning_rate': 0.0006485551664676017, 'batch_size': 32, 'hidden_dim': 37, 'n_layers': 4, 'sequence_length': 7}. Best is trial 9 with value: 1.4735058546066284.\n",
      "[I 2023-10-10 01:43:50,807] Trial 10 finished with value: 3.05531907081604 and parameters: {'learning_rate': 0.0006107594631041428, 'batch_size': 64, 'hidden_dim': 57, 'n_layers': 6, 'sequence_length': 10}. Best is trial 9 with value: 1.4735058546066284.\n",
      "[I 2023-10-10 02:15:33,703] Trial 11 finished with value: 1.478348970413208 and parameters: {'learning_rate': 0.00015064208807449324, 'batch_size': 32, 'hidden_dim': 86, 'n_layers': 2, 'sequence_length': 8}. Best is trial 9 with value: 1.4735058546066284.\n",
      "[I 2023-10-10 02:45:25,639] Trial 12 finished with value: 1.4234168529510498 and parameters: {'learning_rate': 0.00017046938865544983, 'batch_size': 32, 'hidden_dim': 78, 'n_layers': 2, 'sequence_length': 9}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 03:37:23,047] Trial 13 finished with value: 1.5415962934494019 and parameters: {'learning_rate': 0.0004459079280445674, 'batch_size': 32, 'hidden_dim': 63, 'n_layers': 5, 'sequence_length': 10}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 04:51:33,962] Trial 14 finished with value: 1.8655378818511963 and parameters: {'learning_rate': 0.0011491322305171507, 'batch_size': 128, 'hidden_dim': 98, 'n_layers': 5, 'sequence_length': 9}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 05:16:46,744] Trial 15 finished with value: 1.4476672410964966 and parameters: {'learning_rate': 0.00021192375953872985, 'batch_size': 32, 'hidden_dim': 64, 'n_layers': 2, 'sequence_length': 8}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 05:44:49,811] Trial 16 finished with value: 3206.432861328125 and parameters: {'learning_rate': 5.261975220169083e-05, 'batch_size': 32, 'hidden_dim': 70, 'n_layers': 2, 'sequence_length': 9}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 06:14:38,768] Trial 17 finished with value: 1.4293239116668701 and parameters: {'learning_rate': 0.00022619638029511152, 'batch_size': 32, 'hidden_dim': 82, 'n_layers': 2, 'sequence_length': 8}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 07:01:41,745] Trial 18 finished with value: 395.5538330078125 and parameters: {'learning_rate': 6.0597865090312885e-05, 'batch_size': 64, 'hidden_dim': 99, 'n_layers': 3, 'sequence_length': 4}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 07:32:19,093] Trial 19 finished with value: 1.4922202825546265 and parameters: {'learning_rate': 0.00026743857723057903, 'batch_size': 128, 'hidden_dim': 83, 'n_layers': 2, 'sequence_length': 9}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 09:11:32,256] Trial 20 finished with value: 14625.4384765625 and parameters: {'learning_rate': 1.0643775619497625e-05, 'batch_size': 32, 'hidden_dim': 109, 'n_layers': 3, 'sequence_length': 7}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 10:03:40,532] Trial 21 finished with value: 1.5090068578720093 and parameters: {'learning_rate': 0.00022065637250388335, 'batch_size': 32, 'hidden_dim': 69, 'n_layers': 2, 'sequence_length': 8}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 10:41:17,945] Trial 22 finished with value: 646.7184448242188 and parameters: {'learning_rate': 0.00010007499454522434, 'batch_size': 32, 'hidden_dim': 55, 'n_layers': 2, 'sequence_length': 10}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 11:23:54,425] Trial 23 finished with value: 1.5330870151519775 and parameters: {'learning_rate': 0.00027200936182933, 'batch_size': 32, 'hidden_dim': 87, 'n_layers': 2, 'sequence_length': 8}. Best is trial 12 with value: 1.4234168529510498.\n",
      "[I 2023-10-10 12:21:37,033] Trial 24 finished with value: 1.4064759016036987 and parameters: {'learning_rate': 0.000193488782301083, 'batch_size': 32, 'hidden_dim': 75, 'n_layers': 3, 'sequence_length': 9}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 13:17:06,866] Trial 25 finished with value: 153.01454162597656 and parameters: {'learning_rate': 8.955510321310845e-05, 'batch_size': 32, 'hidden_dim': 75, 'n_layers': 3, 'sequence_length': 9}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 14:33:34,748] Trial 26 finished with value: 6389.2568359375 and parameters: {'learning_rate': 2.8099742893415407e-05, 'batch_size': 32, 'hidden_dim': 94, 'n_layers': 3, 'sequence_length': 10}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 16:21:48,670] Trial 27 finished with value: 1.8296173810958862 and parameters: {'learning_rate': 0.00016351999933184936, 'batch_size': 128, 'hidden_dim': 107, 'n_layers': 3, 'sequence_length': 9}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 17:02:49,424] Trial 28 finished with value: 476.2970886230469 and parameters: {'learning_rate': 7.355604854515388e-05, 'batch_size': 64, 'hidden_dim': 79, 'n_layers': 2, 'sequence_length': 7}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 17:42:03,591] Trial 29 finished with value: 434.8277587890625 and parameters: {'learning_rate': 0.00011979138634912953, 'batch_size': 32, 'hidden_dim': 49, 'n_layers': 3, 'sequence_length': 5}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 18:43:46,752] Trial 30 finished with value: 4097.72900390625 and parameters: {'learning_rate': 3.7207887941291944e-05, 'batch_size': 32, 'hidden_dim': 90, 'n_layers': 3, 'sequence_length': 6}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 19:17:02,900] Trial 31 finished with value: 1.4441128969192505 and parameters: {'learning_rate': 0.0002384763584041347, 'batch_size': 32, 'hidden_dim': 62, 'n_layers': 2, 'sequence_length': 8}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 19:50:50,112] Trial 32 finished with value: 1.503040075302124 and parameters: {'learning_rate': 0.00018160093437500678, 'batch_size': 32, 'hidden_dim': 63, 'n_layers': 2, 'sequence_length': 9}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 20:27:13,384] Trial 33 finished with value: 109.41523742675781 and parameters: {'learning_rate': 9.506032953443762e-05, 'batch_size': 32, 'hidden_dim': 73, 'n_layers': 2, 'sequence_length': 8}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 20:55:26,961] Trial 34 finished with value: 355.6753845214844 and parameters: {'learning_rate': 0.0001397054605574304, 'batch_size': 32, 'hidden_dim': 43, 'n_layers': 2, 'sequence_length': 7}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 21:29:44,299] Trial 35 finished with value: 1.527265191078186 and parameters: {'learning_rate': 0.0003622484276243008, 'batch_size': 32, 'hidden_dim': 81, 'n_layers': 2, 'sequence_length': 6}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 21:53:44,015] Trial 36 finished with value: 1.4764437675476074 and parameters: {'learning_rate': 0.00021694091206655248, 'batch_size': 64, 'hidden_dim': 58, 'n_layers': 2, 'sequence_length': 10}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 22:30:42,480] Trial 37 finished with value: 1.6656267642974854 and parameters: {'learning_rate': 0.0004032079627586677, 'batch_size': 32, 'hidden_dim': 72, 'n_layers': 3, 'sequence_length': 1}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 23:12:51,195] Trial 38 finished with value: 171.65939331054688 and parameters: {'learning_rate': 7.306627127156033e-05, 'batch_size': 32, 'hidden_dim': 91, 'n_layers': 3, 'sequence_length': 7}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-10 23:38:18,613] Trial 39 finished with value: 13.645126342773438 and parameters: {'learning_rate': 0.00012654510039307543, 'batch_size': 128, 'hidden_dim': 66, 'n_layers': 2, 'sequence_length': 9}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 00:39:08,043] Trial 40 finished with value: 1.8164994716644287 and parameters: {'learning_rate': 0.0006737311287530779, 'batch_size': 32, 'hidden_dim': 102, 'n_layers': 4, 'sequence_length': 8}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 01:01:29,528] Trial 41 finished with value: 1.7886477708816528 and parameters: {'learning_rate': 0.0002031901607253787, 'batch_size': 32, 'hidden_dim': 52, 'n_layers': 2, 'sequence_length': 8}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 01:25:59,699] Trial 42 finished with value: 1.5855193138122559 and parameters: {'learning_rate': 0.0002752612932111653, 'batch_size': 32, 'hidden_dim': 62, 'n_layers': 2, 'sequence_length': 8}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 01:54:03,865] Trial 43 finished with value: 1.5647921562194824 and parameters: {'learning_rate': 0.0001769151641224097, 'batch_size': 32, 'hidden_dim': 77, 'n_layers': 2, 'sequence_length': 9}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 02:21:13,117] Trial 44 finished with value: 1.8789466619491577 and parameters: {'learning_rate': 0.00047274359112779675, 'batch_size': 32, 'hidden_dim': 35, 'n_layers': 3, 'sequence_length': 5}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 02:50:45,569] Trial 45 finished with value: 1.4773600101470947 and parameters: {'learning_rate': 0.00032296279034822864, 'batch_size': 32, 'hidden_dim': 83, 'n_layers': 2, 'sequence_length': 7}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 03:43:53,975] Trial 46 finished with value: 855.5968627929688 and parameters: {'learning_rate': 0.00012239642786505563, 'batch_size': 64, 'hidden_dim': 43, 'n_layers': 5, 'sequence_length': 6}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 04:10:06,275] Trial 47 finished with value: 2909.83984375 and parameters: {'learning_rate': 0.00022744839147401748, 'batch_size': 32, 'hidden_dim': 16, 'n_layers': 2, 'sequence_length': 10}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 04:56:53,649] Trial 48 finished with value: 2.714103937149048 and parameters: {'learning_rate': 0.0005616417010621646, 'batch_size': 32, 'hidden_dim': 68, 'n_layers': 3, 'sequence_length': 8}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 05:20:42,062] Trial 49 finished with value: 1.5279295444488525 and parameters: {'learning_rate': 0.00033484834242491606, 'batch_size': 128, 'hidden_dim': 58, 'n_layers': 2, 'sequence_length': 9}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 06:08:58,297] Trial 50 finished with value: 1.6296228170394897 and parameters: {'learning_rate': 0.0008449239065963545, 'batch_size': 32, 'hidden_dim': 76, 'n_layers': 4, 'sequence_length': 3}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 06:40:03,732] Trial 51 finished with value: 2.435336112976074 and parameters: {'learning_rate': 0.00041979232408150365, 'batch_size': 32, 'hidden_dim': 34, 'n_layers': 4, 'sequence_length': 7}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 07:20:08,624] Trial 52 finished with value: 3.0891048908233643 and parameters: {'learning_rate': 0.002140403963341404, 'batch_size': 32, 'hidden_dim': 42, 'n_layers': 5, 'sequence_length': 8}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 07:49:32,451] Trial 53 finished with value: 57.92744064331055 and parameters: {'learning_rate': 0.00026891459778054835, 'batch_size': 32, 'hidden_dim': 27, 'n_layers': 4, 'sequence_length': 8}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 09:07:37,871] Trial 54 finished with value: 1.634503722190857 and parameters: {'learning_rate': 0.00015269028010869912, 'batch_size': 32, 'hidden_dim': 86, 'n_layers': 5, 'sequence_length': 7}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[I 2023-10-11 09:54:12,216] Trial 55 finished with value: 1.8108159303665161 and parameters: {'learning_rate': 0.000507718804763496, 'batch_size': 32, 'hidden_dim': 66, 'n_layers': 4, 'sequence_length': 9}. Best is trial 24 with value: 1.4064759016036987.\n",
      "[W 2023-10-11 11:40:47,455] Trial 56 failed with parameters: {'learning_rate': 0.0008078907144051324, 'batch_size': 64, 'hidden_dim': 118, 'n_layers': 6, 'sequence_length': 9} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_4832\\1271520721.py\", line 127, in objective\n",
      "    loss.backward()\n",
      "  File \"c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2023-10-11 11:40:47,876] Trial 56 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39m# Create a study for hyperparameter optimization\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Minimize validation loss\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)  \u001b[39m# You can adjust the number of trials\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39m# Get the best hyperparameters from the study\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     _optimize(\n\u001b[0;32m    443\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    444\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    448\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    452\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m         loss \u001b[39m=\u001b[39m loss_function(outputs, batch_y\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m         \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39m# Evaluate on testing set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the x_train and y_train data\n",
    "x_train = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']].to_numpy()\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Inside your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size + date_embedding_dim  # Updated input size to include date embeddings\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Ensure input has the shape [batch_size, sequence_length, input_size]\n",
    "        x = x.view(batch_size, 1, self.input_size)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 7  # Adjust based on the number of input features (excluding change_percent)\n",
    "date_embedding_dim = 3  # Adjust based on the dimension of your date embeddings\n",
    "output_size = 1\n",
    "hidden_dim = 32\n",
    "n_layers = 4\n",
    "sequence_length = 75  # Keep this as 1 for your input data\n",
    "batch_size = 64\n",
    "\n",
    "# Concatenate date embeddings with your feature vectors\n",
    "x_train_feature_tensors = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_train_date_embeddings = date_embeddings[:len(x_train_scaled)]  # Use the same length as your training data\n",
    "x_train_combined = torch.cat((x_train_feature_tensors, x_train_date_embeddings), dim=1)\n",
    "\n",
    "# Convert the combined data to PyTorch tensors\n",
    "x_train_tensor = x_train_combined.clone().detach()\n",
    "x_train_tensor = x_train_tensor.to(torch.float32)\n",
    "\n",
    "# Copy x_test_combined to create x_test_tensor\n",
    "x_test_feature_tensors = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "x_test_date_embeddings = date_embeddings[len(x_train_scaled):]  # Use the remaining embeddings for testing data\n",
    "x_test_combined = torch.cat((x_test_feature_tensors, x_test_date_embeddings), dim=1)\n",
    "x_test_tensor = x_test_combined.clone().detach()\n",
    "x_test_tensor = x_test_tensor.to(torch.float32)\n",
    "\n",
    "# Create y_train_tensor directly from y_train\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Create a DataLoader for batch training\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the objective function for hyperparameter optimization\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 16, 128)\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 6)\n",
    "    sequence_length = trial.suggest_int('sequence_length', 1, 10)\n",
    "\n",
    "    # Create an instance of the LSTM model with sampled hyperparameters\n",
    "    model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1000  # You can adjust the number of epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(outputs, batch_y.view(-1, 1))\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on testing set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_test_tensor)\n",
    "        val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))\n",
    "\n",
    "    return val_loss.item()\n",
    "\n",
    "# Create a study for hyperparameter optimization\n",
    "study = optuna.create_study(direction='minimize')  # Minimize validation loss\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters from the study\n",
    "best_params = study.best_params\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"Learning Rate:\", best_params['learning_rate'])\n",
    "print(\"Batch Size:\", best_params['batch_size'])\n",
    "print(\"Hidden Dim:\", best_params['hidden_dim'])\n",
    "print(\"Number of Layers:\", best_params['n_layers'])\n",
    "print(\"Sequence Length:\", best_params['sequence_length'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolutionary hyperparameter optimization:\n",
    "\n",
    "is a technique that uses evolutionary algorithms to search for optimal hyperparameters for machine learning models. One common evolutionary algorithm used for this purpose is Genetic Algorithm (GA).\n",
    "\n",
    "Certainly! Genetic Algorithms (GAs) are a type of optimization algorithm inspired by the process of natural selection and genetics. They are used to find approximate solutions to complex optimization and search problems. GAs work by evolving a population of candidate solutions over multiple generations to improve their fitness or objective function value. Here are the key components and steps of a typical GA:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "Create an initial population of candidate solutions (often random).\n",
    "Each candidate solution represents a possible solution to the problem, typically encoded as a set of parameters or variables.\n",
    "Evaluation:\n",
    "\n",
    "Calculate the fitness or objective function value for each candidate solution in the population.\n",
    "The fitness function quantifies how good each solution is with respect to the optimization problem. It guides the search for better solutions.\n",
    "Selection:\n",
    "\n",
    "Select a subset of candidate solutions (parents) from the current population to create the next generation.\n",
    "Solutions with higher fitness values are more likely to be selected.\n",
    "Common selection methods include roulette wheel selection, tournament selection, and rank-based selection.\n",
    "Crossover (Recombination):\n",
    "\n",
    "Combine pairs of selected parent solutions to produce offspring solutions.\n",
    "Crossover typically involves swapping or recombining parts of parent solutions to create one or more offspring solutions.\n",
    "The goal is to introduce diversity and combine the good features of different solutions.\n",
    "Mutation:\n",
    "\n",
    "Apply small random changes to some of the offspring solutions.\n",
    "Mutation helps to introduce new genetic material into the population and prevents premature convergence.\n",
    "Replacement:\n",
    "\n",
    "Create the next generation by combining the original population, offspring, and potentially some survivors from the previous generation.\n",
    "The new population replaces the old population, and the cycle repeats.\n",
    "Termination:\n",
    "\n",
    "Determine when to stop the GA. This can be based on the number of generations, convergence criteria, or other stopping conditions.\n",
    "Optionally, the best solution found during the optimization can be returned as the final result.\n",
    "Convergence:\n",
    "\n",
    "GAs tend to improve the quality of solutions over generations, and they converge toward optimal or near-optimal solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 30\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X53sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m mutpb \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m \u001b[39m# Mutation probability\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X53sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# Run the genetic algorithm\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X53sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m algorithms\u001b[39m.\u001b[39;49meaSimple(population, toolbox, cxpb, mutpb, ngen, stats, halloffame\u001b[39m=\u001b[39;49mhof, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X53sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m# Get the best individual (optimized hyperparameters)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X53sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m best_individual \u001b[39m=\u001b[39m hof[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\deap\\algorithms.py:151\u001b[0m, in \u001b[0;36meaSimple\u001b[1;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[0;32m    149\u001b[0m invalid_ind \u001b[39m=\u001b[39m [ind \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m population \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ind\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalid]\n\u001b[0;32m    150\u001b[0m fitnesses \u001b[39m=\u001b[39m toolbox\u001b[39m.\u001b[39mmap(toolbox\u001b[39m.\u001b[39mevaluate, invalid_ind)\n\u001b[1;32m--> 151\u001b[0m \u001b[39mfor\u001b[39;00m ind, fit \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(invalid_ind, fitnesses):\n\u001b[0;32m    152\u001b[0m     ind\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m fit\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m halloffame \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X53sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         outputs \u001b[39m=\u001b[39m model(batch_x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X53sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         loss \u001b[39m=\u001b[39m loss_function(outputs, batch_y\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X53sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X53sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X53sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Evaluate the model on the validation set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from deap import base, creator, tools, algorithms\n",
    "import torch\n",
    "\n",
    "# Define a function to evaluate the fitness of an individual\n",
    "def evaluate(individual):\n",
    "    # Extract hyperparameters from the individual\n",
    "    learning_rate, sequence_length, batch_size, hidden_dim, n_layers = individual\n",
    "\n",
    "    # Create and train the LSTM model (You need to define these functions)\n",
    "    model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop (You need to define train_loader, loss_function, num_epochs, x_test_tensor, y_test_tensor)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = loss_function(outputs, batch_y.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_test_tensor)\n",
    "        val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))\n",
    "\n",
    "    # Return the validation loss as the fitness value\n",
    "    return val_loss.item(),\n",
    "\n",
    "# Define the optimization problem\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Register the evaluate function in the toolbox\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "toolbox.register(\"attr_float\", np.random.uniform, 1e-5, 1e-3)  # Learning rate\n",
    "toolbox.register(\"attr_int_seq_length\", np.random.randint, 1, 200)         # Sequence length\n",
    "toolbox.register(\"attr_int_batch_size\", np.random.randint, 16, 128)        # Batch size\n",
    "toolbox.register(\"attr_int_hidden_dim\", np.random.randint, 16, 128)        # Hidden dimension\n",
    "toolbox.register(\"attr_int_n_layers\", np.random.randint, 1, 5)           # Number of layers\n",
    "\n",
    "# Define the individual and population size\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.attr_float, toolbox.attr_int_seq_length,\n",
    "                                                                     toolbox.attr_int_batch_size, toolbox.attr_int_hidden_dim,\n",
    "                                                                     toolbox.attr_int_n_layers), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define the genetic operators (crossover and mutation)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
    "\n",
    "# Define the selection operator\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Define the statistics to track during optimization\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"min\", np.min)\n",
    "\n",
    "# Create the initial population\n",
    "population = toolbox.population(n=10)\n",
    "\n",
    "# Create the Hall of Fame to store the best individuals\n",
    "hof = tools.HallOfFame(1)\n",
    "\n",
    "# Set the algorithm parameters\n",
    "ngen = 10   # Number of generations\n",
    "cxpb = 0.7  # Crossover probability\n",
    "mutpb = 0.2 # Mutation probability\n",
    "\n",
    "# Run the genetic algorithm\n",
    "algorithms.eaSimple(population, toolbox, cxpb, mutpb, ngen, stats, halloffame=hof, verbose=True)\n",
    "\n",
    "# Get the best individual (optimized hyperparameters)\n",
    "best_individual = hof[0]\n",
    "best_learning_rate, best_sequence_length, best_batch_size, best_hidden_dim, best_n_layers = best_individual\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"Learning Rate: {best_learning_rate}\")\n",
    "print(f\"Sequence Length: {best_sequence_length}\")\n",
    "print(f\"Batch Size: {best_batch_size}\")\n",
    "print(f\"Hidden Dimension: {best_hidden_dim}\")\n",
    "print(f\"Number of Layers: {best_n_layers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayessche Hyperparmeter optimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-05 23:18:08,179] A new study created in memory with name: no-name-1cd1821b-9136-450d-a09d-cebe054b0453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Epoch [100/4000], Loss: 0.4427, Val Loss: 286.9637\n",
      "Trial 0, Epoch [200/4000], Loss: 3.0529, Val Loss: 2.4061\n",
      "Trial 0, Epoch [300/4000], Loss: 3.6636, Val Loss: 1.9211\n",
      "Trial 0, Epoch [400/4000], Loss: 0.7943, Val Loss: 1.9691\n",
      "Trial 0, Epoch [500/4000], Loss: 1.9298, Val Loss: 2.7451\n",
      "Trial 0, Epoch [600/4000], Loss: 0.8580, Val Loss: 1.8333\n",
      "Trial 0, Epoch [700/4000], Loss: 3.1275, Val Loss: 1.8677\n",
      "Trial 0, Epoch [800/4000], Loss: 0.8538, Val Loss: 1.8187\n",
      "Trial 0, Epoch [900/4000], Loss: 1.1385, Val Loss: 1.6672\n",
      "Trial 0, Epoch [1000/4000], Loss: 1.1785, Val Loss: 1.7389\n",
      "Trial 0, Epoch [1100/4000], Loss: 0.8306, Val Loss: 1.7341\n",
      "Trial 0, Epoch [1200/4000], Loss: 0.7424, Val Loss: 1.7269\n",
      "Trial 0, Epoch [1300/4000], Loss: 0.5770, Val Loss: 2.0358\n",
      "Trial 0, Epoch [1400/4000], Loss: 0.5814, Val Loss: 1.6488\n",
      "Trial 0, Epoch [1500/4000], Loss: 0.7378, Val Loss: 1.7816\n",
      "Trial 0, Epoch [1600/4000], Loss: 1.2634, Val Loss: 1.9273\n",
      "Trial 0, Epoch [1700/4000], Loss: 0.5632, Val Loss: 2.0644\n",
      "Trial 0, Epoch [1800/4000], Loss: 1.3334, Val Loss: 2.4988\n",
      "Trial 0, Epoch [1900/4000], Loss: 0.1731, Val Loss: 1.8823\n",
      "Trial 0, Epoch [2000/4000], Loss: 0.2153, Val Loss: 1.9976\n",
      "Trial 0, Epoch [2100/4000], Loss: 0.4296, Val Loss: 1.8263\n",
      "Trial 0, Epoch [2200/4000], Loss: 0.3915, Val Loss: 1.7704\n",
      "Trial 0, Epoch [2300/4000], Loss: 0.4197, Val Loss: 1.9698\n",
      "Trial 0, Epoch [2400/4000], Loss: 0.5655, Val Loss: 2.2442\n",
      "Trial 0, Epoch [2500/4000], Loss: 0.8719, Val Loss: 2.2726\n",
      "Trial 0, Epoch [2600/4000], Loss: 0.3381, Val Loss: 1.9486\n",
      "Trial 0, Epoch [2700/4000], Loss: 1.0915, Val Loss: 2.0736\n",
      "Trial 0, Epoch [2800/4000], Loss: 0.3699, Val Loss: 1.9792\n",
      "Trial 0, Epoch [2900/4000], Loss: 0.2914, Val Loss: 2.0950\n",
      "Trial 0, Epoch [3000/4000], Loss: 0.6989, Val Loss: 2.0008\n",
      "Trial 0, Epoch [3100/4000], Loss: 0.6296, Val Loss: 2.1120\n",
      "Trial 0, Epoch [3200/4000], Loss: 0.5712, Val Loss: 2.1885\n",
      "Trial 0, Epoch [3300/4000], Loss: 0.9227, Val Loss: 2.1708\n",
      "Trial 0, Epoch [3400/4000], Loss: 0.9358, Val Loss: 2.1881\n",
      "Trial 0, Epoch [3500/4000], Loss: 0.5394, Val Loss: 2.0229\n",
      "Trial 0, Epoch [3600/4000], Loss: 0.4879, Val Loss: 2.1100\n",
      "Trial 0, Epoch [3700/4000], Loss: 0.4371, Val Loss: 2.3427\n",
      "Trial 0, Epoch [3800/4000], Loss: 0.3460, Val Loss: 2.2888\n",
      "Trial 0, Epoch [3900/4000], Loss: 0.3526, Val Loss: 2.3735\n",
      "Trial 0, Epoch [4000/4000], Loss: 0.4658, Val Loss: 2.5434\n",
      "Trial 0, Final Val Loss: 2.5434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-06 02:01:39,550] Trial 0 finished with value: 2.543423652648926 and parameters: {'hidden_dim': 76, 'sequence_length': 91, 'batch_size': 49, 'n_layers': 2, 'learning_rate': 0.001056769059573483}. Best is trial 0 with value: 2.543423652648926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Epoch [100/4000], Loss: 3.2198, Val Loss: 3.0797\n",
      "Trial 1, Epoch [200/4000], Loss: 2.1986, Val Loss: 4.0407\n",
      "Trial 1, Epoch [300/4000], Loss: 1.4162, Val Loss: 1.8597\n",
      "Trial 1, Epoch [400/4000], Loss: 1.3368, Val Loss: 2.0725\n",
      "Trial 1, Epoch [500/4000], Loss: 2.7763, Val Loss: 2.0355\n",
      "Trial 1, Epoch [600/4000], Loss: 1.0290, Val Loss: 2.0713\n",
      "Trial 1, Epoch [700/4000], Loss: 0.9509, Val Loss: 3.6938\n",
      "Trial 1, Epoch [800/4000], Loss: 1.6594, Val Loss: 2.5067\n",
      "Trial 1, Epoch [900/4000], Loss: 0.8237, Val Loss: 1.9579\n",
      "Trial 1, Epoch [1000/4000], Loss: 0.9927, Val Loss: 2.6719\n",
      "Trial 1, Epoch [1100/4000], Loss: 0.4332, Val Loss: 1.8620\n",
      "Trial 1, Epoch [1200/4000], Loss: 0.8185, Val Loss: 2.3024\n",
      "Trial 1, Epoch [1300/4000], Loss: 1.3519, Val Loss: 2.3728\n",
      "Trial 1, Epoch [1400/4000], Loss: 0.9412, Val Loss: 2.1581\n",
      "Trial 1, Epoch [1500/4000], Loss: 0.6877, Val Loss: 2.5076\n",
      "Trial 1, Epoch [1600/4000], Loss: 0.8318, Val Loss: 2.3031\n",
      "Trial 1, Epoch [1700/4000], Loss: 0.5837, Val Loss: 2.5397\n",
      "Trial 1, Epoch [1800/4000], Loss: 0.5213, Val Loss: 2.5878\n",
      "Trial 1, Epoch [1900/4000], Loss: 0.6060, Val Loss: 2.7582\n",
      "Trial 1, Epoch [2000/4000], Loss: 0.8971, Val Loss: 2.9890\n",
      "Trial 1, Epoch [2100/4000], Loss: 1.1022, Val Loss: 3.6904\n",
      "Trial 1, Epoch [2200/4000], Loss: 0.3830, Val Loss: 3.6829\n",
      "Trial 1, Epoch [2300/4000], Loss: 0.5397, Val Loss: 4.1780\n",
      "Trial 1, Epoch [2400/4000], Loss: 0.1487, Val Loss: 4.0418\n",
      "Trial 1, Epoch [2500/4000], Loss: 0.3014, Val Loss: 4.0107\n",
      "Trial 1, Epoch [2600/4000], Loss: 0.3116, Val Loss: 3.6647\n",
      "Trial 1, Epoch [2700/4000], Loss: 0.3500, Val Loss: 3.1504\n",
      "Trial 1, Epoch [2800/4000], Loss: 0.3881, Val Loss: 2.7095\n",
      "Trial 1, Epoch [2900/4000], Loss: 1.0427, Val Loss: 2.7696\n",
      "Trial 1, Epoch [3000/4000], Loss: 1.1315, Val Loss: 3.2273\n",
      "Trial 1, Epoch [3100/4000], Loss: 0.4800, Val Loss: 2.9599\n",
      "Trial 1, Epoch [3200/4000], Loss: 0.1466, Val Loss: 2.6921\n",
      "Trial 1, Epoch [3300/4000], Loss: 0.7902, Val Loss: 2.9673\n",
      "Trial 1, Epoch [3400/4000], Loss: 0.1701, Val Loss: 2.9551\n",
      "Trial 1, Epoch [3500/4000], Loss: 0.5535, Val Loss: 3.4272\n",
      "Trial 1, Epoch [3600/4000], Loss: 0.1759, Val Loss: 3.3099\n",
      "Trial 1, Epoch [3700/4000], Loss: 0.1450, Val Loss: 5.2188\n",
      "Trial 1, Epoch [3800/4000], Loss: 0.2744, Val Loss: 5.0641\n",
      "Trial 1, Epoch [3900/4000], Loss: 0.0838, Val Loss: 4.5734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-06 05:57:05,242] Trial 1 finished with value: 4.636506080627441 and parameters: {'hidden_dim': 84, 'sequence_length': 9, 'batch_size': 72, 'n_layers': 4, 'learning_rate': 0.0024304404022470315}. Best is trial 0 with value: 2.543423652648926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Epoch [4000/4000], Loss: 0.1518, Val Loss: 4.6365\n",
      "Trial 1, Final Val Loss: 4.6365\n",
      "Trial 2, Epoch [100/4000], Loss: 3.5483, Val Loss: 3.6360\n",
      "Trial 2, Epoch [200/4000], Loss: 1.8142, Val Loss: 2.8685\n",
      "Trial 2, Epoch [300/4000], Loss: 1.4716, Val Loss: 2.1728\n",
      "Trial 2, Epoch [400/4000], Loss: 0.6954, Val Loss: 2.4020\n",
      "Trial 2, Epoch [500/4000], Loss: 0.4653, Val Loss: 2.6970\n",
      "Trial 2, Epoch [600/4000], Loss: 0.8609, Val Loss: 2.4226\n",
      "Trial 2, Epoch [700/4000], Loss: 1.1875, Val Loss: 2.9687\n",
      "Trial 2, Epoch [800/4000], Loss: 0.5470, Val Loss: 2.7076\n",
      "Trial 2, Epoch [900/4000], Loss: 0.5839, Val Loss: 2.7061\n",
      "Trial 2, Epoch [1000/4000], Loss: 0.8411, Val Loss: 3.4528\n",
      "Trial 2, Epoch [1100/4000], Loss: 0.5588, Val Loss: 3.0443\n",
      "Trial 2, Epoch [1200/4000], Loss: 0.2837, Val Loss: 3.1611\n",
      "Trial 2, Epoch [1300/4000], Loss: 0.1896, Val Loss: 3.4276\n",
      "Trial 2, Epoch [1400/4000], Loss: 0.4600, Val Loss: 3.0120\n",
      "Trial 2, Epoch [1500/4000], Loss: 0.2590, Val Loss: 2.6288\n",
      "Trial 2, Epoch [1600/4000], Loss: 0.3496, Val Loss: 3.5831\n",
      "Trial 2, Epoch [1700/4000], Loss: 0.3262, Val Loss: 3.1503\n",
      "Trial 2, Epoch [1800/4000], Loss: 0.1954, Val Loss: 3.3039\n",
      "Trial 2, Epoch [1900/4000], Loss: 0.1495, Val Loss: 3.1686\n",
      "Trial 2, Epoch [2000/4000], Loss: 0.3684, Val Loss: 3.6324\n",
      "Trial 2, Epoch [2100/4000], Loss: 0.2470, Val Loss: 3.8372\n",
      "Trial 2, Epoch [2200/4000], Loss: 0.7712, Val Loss: 4.1188\n",
      "Trial 2, Epoch [2300/4000], Loss: 0.4868, Val Loss: 6.8923\n",
      "Trial 2, Epoch [2400/4000], Loss: 0.1669, Val Loss: 3.2799\n",
      "Trial 2, Epoch [2500/4000], Loss: 0.2537, Val Loss: 3.8031\n",
      "Trial 2, Epoch [2600/4000], Loss: 0.1292, Val Loss: 3.7743\n",
      "Trial 2, Epoch [2700/4000], Loss: 0.1661, Val Loss: 3.6849\n",
      "Trial 2, Epoch [2800/4000], Loss: 0.1537, Val Loss: 3.6840\n",
      "Trial 2, Epoch [2900/4000], Loss: 0.0772, Val Loss: 4.0086\n",
      "Trial 2, Epoch [3000/4000], Loss: 0.3409, Val Loss: 4.5233\n",
      "Trial 2, Epoch [3100/4000], Loss: 0.2290, Val Loss: 3.7556\n",
      "Trial 2, Epoch [3200/4000], Loss: 0.1499, Val Loss: 4.5847\n",
      "Trial 2, Epoch [3300/4000], Loss: 0.2231, Val Loss: 3.8664\n",
      "Trial 2, Epoch [3400/4000], Loss: 0.1312, Val Loss: 4.1582\n",
      "Trial 2, Epoch [3500/4000], Loss: 0.4643, Val Loss: 4.8927\n",
      "Trial 2, Epoch [3600/4000], Loss: 0.1943, Val Loss: 4.8619\n",
      "Trial 2, Epoch [3700/4000], Loss: 0.1196, Val Loss: 4.5263\n",
      "Trial 2, Epoch [3800/4000], Loss: 0.0943, Val Loss: 4.5890\n",
      "Trial 2, Epoch [3900/4000], Loss: 0.1299, Val Loss: 4.7351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-06 08:54:51,977] Trial 2 finished with value: 6.366947174072266 and parameters: {'hidden_dim': 105, 'sequence_length': 16, 'batch_size': 18, 'n_layers': 2, 'learning_rate': 0.007134936172000346}. Best is trial 0 with value: 2.543423652648926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Epoch [4000/4000], Loss: 0.4110, Val Loss: 6.3669\n",
      "Trial 2, Final Val Loss: 6.3669\n",
      "Trial 3, Epoch [100/4000], Loss: 10.9420, Val Loss: 151.2464\n",
      "Trial 3, Epoch [200/4000], Loss: 2.0384, Val Loss: 2.7555\n",
      "Trial 3, Epoch [300/4000], Loss: 0.8142, Val Loss: 2.2526\n",
      "Trial 3, Epoch [400/4000], Loss: 1.1392, Val Loss: 1.7454\n",
      "Trial 3, Epoch [500/4000], Loss: 2.5281, Val Loss: 3.3333\n",
      "Trial 3, Epoch [600/4000], Loss: 1.0609, Val Loss: 2.3283\n",
      "Trial 3, Epoch [700/4000], Loss: 1.0676, Val Loss: 2.6454\n",
      "Trial 3, Epoch [800/4000], Loss: 0.3901, Val Loss: 1.8150\n",
      "Trial 3, Epoch [900/4000], Loss: 0.8207, Val Loss: 2.4185\n",
      "Trial 3, Epoch [1000/4000], Loss: 0.8046, Val Loss: 2.0778\n",
      "Trial 3, Epoch [1100/4000], Loss: 1.2548, Val Loss: 2.0797\n",
      "Trial 3, Epoch [1200/4000], Loss: 0.6240, Val Loss: 2.0299\n",
      "Trial 3, Epoch [1300/4000], Loss: 0.9098, Val Loss: 1.9263\n",
      "Trial 3, Epoch [1400/4000], Loss: 0.4787, Val Loss: 1.8495\n",
      "Trial 3, Epoch [1500/4000], Loss: 0.8251, Val Loss: 2.0075\n",
      "Trial 3, Epoch [1600/4000], Loss: 0.5052, Val Loss: 2.0486\n",
      "Trial 3, Epoch [1700/4000], Loss: 0.8353, Val Loss: 1.7419\n",
      "Trial 3, Epoch [1800/4000], Loss: 0.4611, Val Loss: 1.9802\n",
      "Trial 3, Epoch [1900/4000], Loss: 0.6380, Val Loss: 2.1797\n",
      "Trial 3, Epoch [2000/4000], Loss: 0.8686, Val Loss: 2.1560\n",
      "Trial 3, Epoch [2100/4000], Loss: 0.2822, Val Loss: 1.9506\n",
      "Trial 3, Epoch [2200/4000], Loss: 1.1810, Val Loss: 2.2996\n",
      "Trial 3, Epoch [2300/4000], Loss: 0.2497, Val Loss: 2.3229\n",
      "Trial 3, Epoch [2400/4000], Loss: 0.3066, Val Loss: 2.5378\n",
      "Trial 3, Epoch [2500/4000], Loss: 0.4661, Val Loss: 2.7202\n",
      "Trial 3, Epoch [2600/4000], Loss: 0.5239, Val Loss: 3.0479\n",
      "Trial 3, Epoch [2700/4000], Loss: 0.3697, Val Loss: 3.0097\n",
      "Trial 3, Epoch [2800/4000], Loss: 0.8838, Val Loss: 3.2137\n",
      "Trial 3, Epoch [2900/4000], Loss: 0.7747, Val Loss: 3.7656\n",
      "Trial 3, Epoch [3000/4000], Loss: 0.3376, Val Loss: 3.1999\n",
      "Trial 3, Epoch [3100/4000], Loss: 0.2028, Val Loss: 3.3537\n",
      "Trial 3, Epoch [3200/4000], Loss: 0.7075, Val Loss: 3.1810\n",
      "Trial 3, Epoch [3300/4000], Loss: 0.3737, Val Loss: 3.2633\n",
      "Trial 3, Epoch [3400/4000], Loss: 0.4640, Val Loss: 4.1157\n",
      "Trial 3, Epoch [3500/4000], Loss: 0.4636, Val Loss: 3.7506\n",
      "Trial 3, Epoch [3600/4000], Loss: 0.4338, Val Loss: 3.3589\n",
      "Trial 3, Epoch [3700/4000], Loss: 0.4559, Val Loss: 3.4741\n",
      "Trial 3, Epoch [3800/4000], Loss: 0.3493, Val Loss: 3.3493\n",
      "Trial 3, Epoch [3900/4000], Loss: 0.3134, Val Loss: 3.3012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-06 11:00:59,198] Trial 3 finished with value: 3.4866154193878174 and parameters: {'hidden_dim': 31, 'sequence_length': 34, 'batch_size': 105, 'n_layers': 3, 'learning_rate': 0.0028864019632477023}. Best is trial 0 with value: 2.543423652648926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Epoch [4000/4000], Loss: 0.5775, Val Loss: 3.4866\n",
      "Trial 3, Final Val Loss: 3.4866\n",
      "Trial 4, Epoch [100/4000], Loss: 6.9716, Val Loss: 4.1858\n",
      "Trial 4, Epoch [200/4000], Loss: 1.8134, Val Loss: 2.8560\n",
      "Trial 4, Epoch [300/4000], Loss: 3.4881, Val Loss: 2.1255\n",
      "Trial 4, Epoch [400/4000], Loss: 4.1433, Val Loss: 2.8191\n",
      "Trial 4, Epoch [500/4000], Loss: 0.6446, Val Loss: 3.1666\n",
      "Trial 4, Epoch [600/4000], Loss: 1.3057, Val Loss: 2.0166\n",
      "Trial 4, Epoch [700/4000], Loss: 1.6527, Val Loss: 2.1053\n",
      "Trial 4, Epoch [800/4000], Loss: 0.7598, Val Loss: 2.2410\n",
      "Trial 4, Epoch [900/4000], Loss: 0.4332, Val Loss: 2.7010\n",
      "Trial 4, Epoch [1000/4000], Loss: 1.9196, Val Loss: 6.5419\n",
      "Trial 4, Epoch [1100/4000], Loss: 1.0546, Val Loss: 3.0876\n",
      "Trial 4, Epoch [1200/4000], Loss: 0.6131, Val Loss: 3.7622\n",
      "Trial 4, Epoch [1300/4000], Loss: 0.5953, Val Loss: 3.4032\n",
      "Trial 4, Epoch [1400/4000], Loss: 0.3829, Val Loss: 3.7827\n",
      "Trial 4, Epoch [1500/4000], Loss: 0.3244, Val Loss: 3.3692\n",
      "Trial 4, Epoch [1600/4000], Loss: 0.4360, Val Loss: 4.3073\n",
      "Trial 4, Epoch [1700/4000], Loss: 0.3224, Val Loss: 3.5431\n",
      "Trial 4, Epoch [1800/4000], Loss: 0.4269, Val Loss: 4.1904\n",
      "Trial 4, Epoch [1900/4000], Loss: 0.3687, Val Loss: 5.0835\n",
      "Trial 4, Epoch [2000/4000], Loss: 0.6077, Val Loss: 12.7729\n",
      "Trial 4, Epoch [2100/4000], Loss: 1.1529, Val Loss: 13.6102\n",
      "Trial 4, Epoch [2200/4000], Loss: 0.4019, Val Loss: 12.5253\n",
      "Trial 4, Epoch [2300/4000], Loss: 0.3448, Val Loss: 12.6791\n",
      "Trial 4, Epoch [2400/4000], Loss: 0.2332, Val Loss: 14.6361\n",
      "Trial 4, Epoch [2500/4000], Loss: 0.2086, Val Loss: 9.3839\n",
      "Trial 4, Epoch [2600/4000], Loss: 0.2056, Val Loss: 7.9910\n",
      "Trial 4, Epoch [2700/4000], Loss: 0.4195, Val Loss: 7.9968\n",
      "Trial 4, Epoch [2800/4000], Loss: 0.1655, Val Loss: 8.9002\n",
      "Trial 4, Epoch [2900/4000], Loss: 0.1170, Val Loss: 9.1151\n",
      "Trial 4, Epoch [3000/4000], Loss: 0.3292, Val Loss: 9.6552\n",
      "Trial 4, Epoch [3100/4000], Loss: 0.1811, Val Loss: 7.5965\n",
      "Trial 4, Epoch [3200/4000], Loss: 0.1340, Val Loss: 8.2150\n",
      "Trial 4, Epoch [3300/4000], Loss: 0.1019, Val Loss: 5.5084\n",
      "Trial 4, Epoch [3400/4000], Loss: 0.2444, Val Loss: 7.9075\n",
      "Trial 4, Epoch [3500/4000], Loss: 0.1335, Val Loss: 11.1748\n",
      "Trial 4, Epoch [3600/4000], Loss: 0.0858, Val Loss: 6.5708\n",
      "Trial 4, Epoch [3700/4000], Loss: 0.2027, Val Loss: 4.5039\n",
      "Trial 4, Epoch [3800/4000], Loss: 0.0851, Val Loss: 4.6108\n",
      "Trial 4, Epoch [3900/4000], Loss: 0.1503, Val Loss: 4.8571\n",
      "Trial 4, Epoch [4000/4000], Loss: 0.0772, Val Loss: 5.1046\n",
      "Trial 4, Final Val Loss: 5.1046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-06 17:12:16,608] Trial 4 finished with value: 5.104568004608154 and parameters: {'hidden_dim': 122, 'sequence_length': 92, 'batch_size': 61, 'n_layers': 3, 'learning_rate': 0.004984607340686313}. Best is trial 0 with value: 2.543423652648926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Epoch [100/4000], Loss: 9.6739, Val Loss: 2.7074\n",
      "Trial 5, Epoch [200/4000], Loss: 1.4473, Val Loss: 2.6943\n",
      "Trial 5, Epoch [300/4000], Loss: 2.5616, Val Loss: 2.8807\n",
      "Trial 5, Epoch [400/4000], Loss: 1.3412, Val Loss: 1.8569\n",
      "Trial 5, Epoch [500/4000], Loss: 0.9967, Val Loss: 1.9836\n",
      "Trial 5, Epoch [600/4000], Loss: 0.8428, Val Loss: 2.0122\n",
      "Trial 5, Epoch [700/4000], Loss: 1.2615, Val Loss: 1.9130\n",
      "Trial 5, Epoch [800/4000], Loss: 0.1753, Val Loss: 2.0015\n",
      "Trial 5, Epoch [900/4000], Loss: 0.9209, Val Loss: 3.0041\n",
      "Trial 5, Epoch [1000/4000], Loss: 0.7290, Val Loss: 2.1634\n",
      "Trial 5, Epoch [1100/4000], Loss: 0.6879, Val Loss: 2.2460\n",
      "Trial 5, Epoch [1200/4000], Loss: 0.8541, Val Loss: 3.3704\n",
      "Trial 5, Epoch [1300/4000], Loss: 0.5079, Val Loss: 2.2071\n",
      "Trial 5, Epoch [1400/4000], Loss: 0.4289, Val Loss: 3.0679\n",
      "Trial 5, Epoch [1500/4000], Loss: 0.6306, Val Loss: 2.8262\n",
      "Trial 5, Epoch [1600/4000], Loss: 0.3821, Val Loss: 2.8940\n",
      "Trial 5, Epoch [1700/4000], Loss: 0.2229, Val Loss: 3.1207\n",
      "Trial 5, Epoch [1800/4000], Loss: 0.1855, Val Loss: 3.6765\n",
      "Trial 5, Epoch [1900/4000], Loss: 0.7445, Val Loss: 3.6236\n",
      "Trial 5, Epoch [2000/4000], Loss: 0.3988, Val Loss: 3.6366\n",
      "Trial 5, Epoch [2100/4000], Loss: 0.2468, Val Loss: 3.7472\n",
      "Trial 5, Epoch [2200/4000], Loss: 0.1859, Val Loss: 3.6397\n",
      "Trial 5, Epoch [2300/4000], Loss: 0.1052, Val Loss: 3.6046\n",
      "Trial 5, Epoch [2400/4000], Loss: 0.6286, Val Loss: 4.0218\n",
      "Trial 5, Epoch [2500/4000], Loss: 0.2317, Val Loss: 3.9395\n",
      "Trial 5, Epoch [2600/4000], Loss: 0.1191, Val Loss: 3.8681\n",
      "Trial 5, Epoch [2700/4000], Loss: 0.0731, Val Loss: 4.0577\n",
      "Trial 5, Epoch [2800/4000], Loss: 0.1150, Val Loss: 4.4067\n",
      "Trial 5, Epoch [2900/4000], Loss: 0.2427, Val Loss: 4.4337\n",
      "Trial 5, Epoch [3000/4000], Loss: 0.1221, Val Loss: 4.2777\n",
      "Trial 5, Epoch [3100/4000], Loss: 0.0694, Val Loss: 4.2286\n",
      "Trial 5, Epoch [3200/4000], Loss: 0.1214, Val Loss: 4.0862\n",
      "Trial 5, Epoch [3300/4000], Loss: 0.0654, Val Loss: 4.4787\n",
      "Trial 5, Epoch [3400/4000], Loss: 0.1550, Val Loss: 4.6338\n",
      "Trial 5, Epoch [3500/4000], Loss: 0.0657, Val Loss: 4.4630\n",
      "Trial 5, Epoch [3600/4000], Loss: 0.1768, Val Loss: 4.9088\n",
      "Trial 5, Epoch [3700/4000], Loss: 0.1199, Val Loss: 4.8841\n",
      "Trial 5, Epoch [3800/4000], Loss: 0.1062, Val Loss: 4.7988\n",
      "Trial 5, Epoch [3900/4000], Loss: 0.2720, Val Loss: 5.7195\n",
      "Trial 5, Epoch [4000/4000], Loss: 0.0870, Val Loss: 5.1259\n",
      "Trial 5, Final Val Loss: 5.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-06 20:49:38,386] Trial 5 finished with value: 5.125919818878174 and parameters: {'hidden_dim': 107, 'sequence_length': 47, 'batch_size': 105, 'n_layers': 2, 'learning_rate': 0.0036289717809802357}. Best is trial 0 with value: 2.543423652648926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Epoch [100/4000], Loss: 7.1430, Val Loss: 14.8412\n",
      "Trial 6, Epoch [200/4000], Loss: 3.1704, Val Loss: 4.7144\n",
      "Trial 6, Epoch [300/4000], Loss: 2.1629, Val Loss: 10.3035\n",
      "Trial 6, Epoch [400/4000], Loss: 1.1330, Val Loss: 2.2157\n",
      "Trial 6, Epoch [500/4000], Loss: 2.1343, Val Loss: 3.4853\n",
      "Trial 6, Epoch [600/4000], Loss: 3.0523, Val Loss: 2.2263\n",
      "Trial 6, Epoch [700/4000], Loss: 0.6951, Val Loss: 2.4470\n",
      "Trial 6, Epoch [800/4000], Loss: 1.3536, Val Loss: 3.1215\n",
      "Trial 6, Epoch [900/4000], Loss: 0.6422, Val Loss: 2.5307\n",
      "Trial 6, Epoch [1000/4000], Loss: 0.9002, Val Loss: 3.5397\n",
      "Trial 6, Epoch [1100/4000], Loss: 1.0198, Val Loss: 2.4805\n",
      "Trial 6, Epoch [1200/4000], Loss: 0.7072, Val Loss: 2.9221\n",
      "Trial 6, Epoch [1300/4000], Loss: 0.1890, Val Loss: 2.9604\n",
      "Trial 6, Epoch [1400/4000], Loss: 0.8706, Val Loss: 3.1512\n",
      "Trial 6, Epoch [1500/4000], Loss: 0.3768, Val Loss: 2.7539\n",
      "Trial 6, Epoch [1600/4000], Loss: 0.3181, Val Loss: 2.3461\n",
      "Trial 6, Epoch [1700/4000], Loss: 0.4497, Val Loss: 2.9641\n",
      "Trial 6, Epoch [1800/4000], Loss: 0.5701, Val Loss: 2.9138\n",
      "Trial 6, Epoch [1900/4000], Loss: 0.3890, Val Loss: 2.8007\n",
      "Trial 6, Epoch [2000/4000], Loss: 0.4170, Val Loss: 2.9358\n",
      "Trial 6, Epoch [2100/4000], Loss: 1.7254, Val Loss: 3.4184\n",
      "Trial 6, Epoch [2200/4000], Loss: 0.4722, Val Loss: 2.8672\n",
      "Trial 6, Epoch [2300/4000], Loss: 0.4753, Val Loss: 3.3914\n",
      "Trial 6, Epoch [2400/4000], Loss: 0.4711, Val Loss: 2.9681\n",
      "Trial 6, Epoch [2500/4000], Loss: 0.1966, Val Loss: 2.9918\n",
      "Trial 6, Epoch [2600/4000], Loss: 0.2239, Val Loss: 3.1964\n",
      "Trial 6, Epoch [2700/4000], Loss: 0.2417, Val Loss: 3.3754\n",
      "Trial 6, Epoch [2800/4000], Loss: 0.5794, Val Loss: 3.3710\n",
      "Trial 6, Epoch [2900/4000], Loss: 1.0335, Val Loss: 3.5913\n",
      "Trial 6, Epoch [3000/4000], Loss: 0.3831, Val Loss: 2.8799\n",
      "Trial 6, Epoch [3100/4000], Loss: 0.4535, Val Loss: 4.2943\n",
      "Trial 6, Epoch [3200/4000], Loss: 0.3102, Val Loss: 3.2024\n",
      "Trial 6, Epoch [3300/4000], Loss: 0.1629, Val Loss: 4.3876\n",
      "Trial 6, Epoch [3400/4000], Loss: 0.4159, Val Loss: 5.2635\n",
      "Trial 6, Epoch [3500/4000], Loss: 0.2003, Val Loss: 4.5883\n",
      "Trial 6, Epoch [3600/4000], Loss: 0.2348, Val Loss: 5.1260\n",
      "Trial 6, Epoch [3700/4000], Loss: 0.0850, Val Loss: 5.0459\n",
      "Trial 6, Epoch [3800/4000], Loss: 0.5597, Val Loss: 5.3363\n",
      "Trial 6, Epoch [3900/4000], Loss: 0.0764, Val Loss: 4.3287\n",
      "Trial 6, Epoch [4000/4000], Loss: 0.7211, Val Loss: 4.3649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-07 10:39:03,810] Trial 6 finished with value: 4.364938735961914 and parameters: {'hidden_dim': 116, 'sequence_length': 84, 'batch_size': 19, 'n_layers': 4, 'learning_rate': 0.005572688280976959}. Best is trial 0 with value: 2.543423652648926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Final Val Loss: 4.3649\n",
      "Trial 7, Epoch [100/4000], Loss: 1.8124, Val Loss: 166.1222\n",
      "Trial 7, Epoch [200/4000], Loss: 0.5377, Val Loss: 2.4296\n",
      "Trial 7, Epoch [300/4000], Loss: 1.5132, Val Loss: 3.9726\n",
      "Trial 7, Epoch [400/4000], Loss: 1.8723, Val Loss: 2.3322\n",
      "Trial 7, Epoch [500/4000], Loss: 3.3542, Val Loss: 1.6805\n",
      "Trial 7, Epoch [600/4000], Loss: 1.0231, Val Loss: 2.2971\n",
      "Trial 7, Epoch [700/4000], Loss: 1.6742, Val Loss: 2.6160\n",
      "Trial 7, Epoch [800/4000], Loss: 1.2103, Val Loss: 2.0282\n",
      "Trial 7, Epoch [900/4000], Loss: 0.6615, Val Loss: 2.6192\n",
      "Trial 7, Epoch [1000/4000], Loss: 0.5896, Val Loss: 2.3975\n",
      "Trial 7, Epoch [1100/4000], Loss: 0.9781, Val Loss: 2.6643\n",
      "Trial 7, Epoch [1200/4000], Loss: 0.5520, Val Loss: 1.9241\n",
      "Trial 7, Epoch [1300/4000], Loss: 0.7229, Val Loss: 1.6537\n",
      "Trial 7, Epoch [1400/4000], Loss: 1.6998, Val Loss: 2.1297\n",
      "Trial 7, Epoch [1500/4000], Loss: 0.9615, Val Loss: 2.0264\n",
      "Trial 7, Epoch [1600/4000], Loss: 1.1251, Val Loss: 2.3696\n",
      "Trial 7, Epoch [1700/4000], Loss: 0.6772, Val Loss: 2.0100\n",
      "Trial 7, Epoch [1800/4000], Loss: 1.2731, Val Loss: 2.5996\n",
      "Trial 7, Epoch [1900/4000], Loss: 0.5910, Val Loss: 2.1426\n",
      "Trial 7, Epoch [2000/4000], Loss: 0.5145, Val Loss: 2.0152\n",
      "Trial 7, Epoch [2100/4000], Loss: 1.1603, Val Loss: 2.2147\n",
      "Trial 7, Epoch [2200/4000], Loss: 0.6552, Val Loss: 2.7276\n",
      "Trial 7, Epoch [2300/4000], Loss: 0.4979, Val Loss: 2.3711\n",
      "Trial 7, Epoch [2400/4000], Loss: 0.3737, Val Loss: 2.8713\n",
      "Trial 7, Epoch [2500/4000], Loss: 1.0003, Val Loss: 2.4822\n",
      "Trial 7, Epoch [2600/4000], Loss: 1.1673, Val Loss: 2.5857\n",
      "Trial 7, Epoch [2700/4000], Loss: 0.7039, Val Loss: 2.9765\n",
      "Trial 7, Epoch [2800/4000], Loss: 0.9916, Val Loss: 3.0521\n",
      "Trial 7, Epoch [2900/4000], Loss: 0.4941, Val Loss: 3.3943\n",
      "Trial 7, Epoch [3000/4000], Loss: 0.9535, Val Loss: 4.4197\n",
      "Trial 7, Epoch [3100/4000], Loss: 0.3020, Val Loss: 3.3300\n",
      "Trial 7, Epoch [3200/4000], Loss: 1.3155, Val Loss: 3.7663\n",
      "Trial 7, Epoch [3300/4000], Loss: 0.3559, Val Loss: 3.6703\n",
      "Trial 7, Epoch [3400/4000], Loss: 0.7907, Val Loss: 3.2279\n",
      "Trial 7, Epoch [3500/4000], Loss: 0.4031, Val Loss: 2.6015\n",
      "Trial 7, Epoch [3600/4000], Loss: 0.5875, Val Loss: 2.6834\n",
      "Trial 7, Epoch [3700/4000], Loss: 0.2737, Val Loss: 90.5680\n",
      "Trial 7, Epoch [3800/4000], Loss: 0.2350, Val Loss: 2.9675\n",
      "Trial 7, Epoch [3900/4000], Loss: 0.4625, Val Loss: 7.5704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-07 12:44:39,129] Trial 7 finished with value: 82.69794464111328 and parameters: {'hidden_dim': 28, 'sequence_length': 28, 'batch_size': 67, 'n_layers': 4, 'learning_rate': 0.003219681364016982}. Best is trial 0 with value: 2.543423652648926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Epoch [4000/4000], Loss: 0.3811, Val Loss: 82.6979\n",
      "Trial 7, Final Val Loss: 82.6979\n",
      "Trial 8, Epoch [100/4000], Loss: 2.5486, Val Loss: 2.1391\n",
      "Trial 8, Epoch [200/4000], Loss: 1.1248, Val Loss: 2.8298\n",
      "Trial 8, Epoch [300/4000], Loss: 1.3750, Val Loss: 2.5605\n",
      "Trial 8, Epoch [400/4000], Loss: 0.7264, Val Loss: 3.0075\n",
      "Trial 8, Epoch [500/4000], Loss: 0.7262, Val Loss: 2.7667\n",
      "Trial 8, Epoch [600/4000], Loss: 0.8390, Val Loss: 4.5630\n",
      "Trial 8, Epoch [700/4000], Loss: 1.2667, Val Loss: 3.5549\n",
      "Trial 8, Epoch [800/4000], Loss: 1.2346, Val Loss: 2.6030\n",
      "Trial 8, Epoch [900/4000], Loss: 0.8426, Val Loss: 2.4265\n",
      "Trial 8, Epoch [1000/4000], Loss: 1.1194, Val Loss: 1.9970\n",
      "Trial 8, Epoch [1100/4000], Loss: 2.9854, Val Loss: 2.0844\n",
      "Trial 8, Epoch [1200/4000], Loss: 0.7369, Val Loss: 2.1456\n",
      "Trial 8, Epoch [1300/4000], Loss: 1.1705, Val Loss: 2.5637\n",
      "Trial 8, Epoch [1400/4000], Loss: 0.8405, Val Loss: 4.4944\n",
      "Trial 8, Epoch [1500/4000], Loss: 0.9368, Val Loss: 6.2019\n",
      "Trial 8, Epoch [1600/4000], Loss: 1.4057, Val Loss: 5.2270\n",
      "Trial 8, Epoch [1700/4000], Loss: 0.8803, Val Loss: 4.8366\n",
      "Trial 8, Epoch [1800/4000], Loss: 0.6640, Val Loss: 4.1671\n",
      "Trial 8, Epoch [1900/4000], Loss: 0.6777, Val Loss: 3.2653\n",
      "Trial 8, Epoch [2000/4000], Loss: 0.7110, Val Loss: 2.6270\n",
      "Trial 8, Epoch [2100/4000], Loss: 1.2509, Val Loss: 2.5004\n",
      "Trial 8, Epoch [2200/4000], Loss: 0.8465, Val Loss: 2.1657\n",
      "Trial 8, Epoch [2300/4000], Loss: 0.6891, Val Loss: 2.4654\n",
      "Trial 8, Epoch [2400/4000], Loss: 1.5270, Val Loss: 2.4545\n",
      "Trial 8, Epoch [2500/4000], Loss: 0.7251, Val Loss: 2.4086\n",
      "Trial 8, Epoch [2600/4000], Loss: 1.0281, Val Loss: 2.6441\n",
      "Trial 8, Epoch [2700/4000], Loss: 1.0841, Val Loss: 2.6824\n",
      "Trial 8, Epoch [2800/4000], Loss: 1.7596, Val Loss: 2.4294\n",
      "Trial 8, Epoch [2900/4000], Loss: 1.2434, Val Loss: 2.2455\n",
      "Trial 8, Epoch [3000/4000], Loss: 0.6657, Val Loss: 2.1462\n",
      "Trial 8, Epoch [3100/4000], Loss: 1.5594, Val Loss: 2.9105\n",
      "Trial 8, Epoch [3200/4000], Loss: 0.6045, Val Loss: 2.8185\n",
      "Trial 8, Epoch [3300/4000], Loss: 1.9592, Val Loss: 3.2144\n",
      "Trial 8, Epoch [3400/4000], Loss: 1.0387, Val Loss: 2.4111\n",
      "Trial 8, Epoch [3500/4000], Loss: 0.3634, Val Loss: 2.5934\n",
      "Trial 8, Epoch [3600/4000], Loss: 0.6295, Val Loss: 2.8079\n",
      "Trial 8, Epoch [3700/4000], Loss: 1.4600, Val Loss: 2.9360\n",
      "Trial 8, Epoch [3800/4000], Loss: 0.9414, Val Loss: 2.7771\n",
      "Trial 8, Epoch [3900/4000], Loss: 0.4438, Val Loss: 3.0733\n",
      "Trial 8, Epoch [4000/4000], Loss: 1.2309, Val Loss: 2.9460\n",
      "Trial 8, Final Val Loss: 2.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-07 14:04:39,930] Trial 8 finished with value: 2.946018934249878 and parameters: {'hidden_dim': 75, 'sequence_length': 3, 'batch_size': 25, 'n_layers': 1, 'learning_rate': 0.009346372189458564}. Best is trial 0 with value: 2.543423652648926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Epoch [100/4000], Loss: 1.6712, Val Loss: 5.1696\n",
      "Trial 9, Epoch [200/4000], Loss: 1.4234, Val Loss: 2.7799\n",
      "Trial 9, Epoch [300/4000], Loss: 1.6638, Val Loss: 1.6209\n",
      "Trial 9, Epoch [400/4000], Loss: 1.4100, Val Loss: 1.7805\n",
      "Trial 9, Epoch [500/4000], Loss: 0.7871, Val Loss: 2.0007\n",
      "Trial 9, Epoch [600/4000], Loss: 1.0089, Val Loss: 2.1835\n",
      "Trial 9, Epoch [700/4000], Loss: 0.7824, Val Loss: 1.7645\n",
      "Trial 9, Epoch [800/4000], Loss: 0.7452, Val Loss: 1.6886\n",
      "Trial 9, Epoch [900/4000], Loss: 0.8755, Val Loss: 2.0400\n",
      "Trial 9, Epoch [1000/4000], Loss: 1.2282, Val Loss: 1.8729\n",
      "Trial 9, Epoch [1100/4000], Loss: 1.2586, Val Loss: 2.3768\n",
      "Trial 9, Epoch [1200/4000], Loss: 0.3321, Val Loss: 1.8090\n",
      "Trial 9, Epoch [1300/4000], Loss: 0.2516, Val Loss: 1.6202\n",
      "Trial 9, Epoch [1400/4000], Loss: 1.4296, Val Loss: 2.0018\n",
      "Trial 9, Epoch [1500/4000], Loss: 0.3880, Val Loss: 1.9152\n",
      "Trial 9, Epoch [1600/4000], Loss: 1.0906, Val Loss: 2.1916\n",
      "Trial 9, Epoch [1700/4000], Loss: 0.4642, Val Loss: 2.0720\n",
      "Trial 9, Epoch [1800/4000], Loss: 0.5435, Val Loss: 2.4818\n",
      "Trial 9, Epoch [1900/4000], Loss: 0.2732, Val Loss: 3.2384\n",
      "Trial 9, Epoch [2000/4000], Loss: 0.8219, Val Loss: 3.3906\n",
      "Trial 9, Epoch [2100/4000], Loss: 0.5505, Val Loss: 4.0676\n",
      "Trial 9, Epoch [2200/4000], Loss: 0.3316, Val Loss: 4.1581\n",
      "Trial 9, Epoch [2300/4000], Loss: 0.6272, Val Loss: 3.2842\n",
      "Trial 9, Epoch [2400/4000], Loss: 0.3236, Val Loss: 3.0180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-10-07 17:25:04,757] Trial 9 failed with parameters: {'hidden_dim': 126, 'sequence_length': 69, 'batch_size': 77, 'n_layers': 3, 'learning_rate': 0.0013957939123843718} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_5220\\3950734113.py\", line 33, in objective\n",
      "    outputs = model(batch_x)\n",
      "  File \"c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_5220\\3494852810.py\", line 61, in forward\n",
      "    out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
      "  File \"c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\", line 812, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "KeyboardInterrupt\n",
      "[W 2023-10-07 17:25:04,910] Trial 9 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 32\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Create an Optuna study and optimize hyperparameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# We want to minimize the validation loss\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)  \u001b[39m# You can adjust the number of trials as needed\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# Retrieve the best hyperparameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     _optimize(\n\u001b[0;32m    443\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    444\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    448\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    452\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 32\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(batch_x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs, batch_y\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))  \u001b[39m# Ensure batch_y has the right shape\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 32\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Ensure input has the shape [batch_size, sequence_length, input_size]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(batch_size, \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m out, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h0, c0))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Define the objective function for Optuna hyperparameter optimization\n",
    "def objective(trial):\n",
    "    # Define a search space for hyperparameters\n",
    "\n",
    "    input_size = 7\n",
    "    date_embedding_dim = 3\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 16, 128)\n",
    "    sequence_length = trial.suggest_int('sequence_length', 1, 100)\n",
    "    batch_size = trial.suggest_int('batch_size',16,128)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "\n",
    "\n",
    "    # Create an instance of the LSTM model with the suggested hyperparameters\n",
    "    model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 4000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            # Evaluate on testing set\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(x_test_tensor)\n",
    "                val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "            print(f'Trial {trial.number}, Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    # Calculate the final validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_test_tensor)\n",
    "        val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "\n",
    "    print(f'Trial {trial.number}, Final Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    return val_loss.item()\n",
    "\n",
    "# Create an Optuna study and optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize the validation loss\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials as needed\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 27\u001b[0m line \u001b[0;36m1\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39moptuna\u001b[39;00m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Define the objective function for Optuna hyperparameter optimization\u001b[39;00m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective\u001b[39m(trial):\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Define a search space for hyperparameters\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Define the objective function for Optuna hyperparameter optimization\n",
    "def objective(trial):\n",
    "    # Define a search space for hyperparameters\n",
    "    input_size = trial.suggest_int('input_size', 5, 20)\n",
    "    date_embedding_dim = trial.suggest_int('date_embedding_dim', 2, 10)\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 16, 128)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "    # Create an instance of the LSTM model with the suggested hyperparameters\n",
    "    model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 4000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            # Evaluate on testing set\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(x_test_tensor)\n",
    "                val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "            print(f'Trial {trial.number}, Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    # Calculate the final validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_test_tensor)\n",
    "        val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "\n",
    "    print(f'Trial {trial.number}, Final Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    return val_loss.item()\n",
    "\n",
    "# Create an Optuna study and optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize the validation loss\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials as needed\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_9648\\37430299.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: Learning Rate=0.00015, Sequence Length=75, Batch Size=64, Input Size=7, Date Embedding Dim=3, Hidden Dim=32,Layers=3\n",
      "Epoch [100/8000], Loss: 21872.4414, Val Loss: 21901.0703\n",
      "Epoch [200/8000], Loss: 6361.3525, Val Loss: 16445.4980\n",
      "Epoch [300/8000], Loss: 1646.8357, Val Loss: 12494.5293\n",
      "Epoch [400/8000], Loss: 4116.4194, Val Loss: 9560.4355\n",
      "Epoch [500/8000], Loss: 10061.7539, Val Loss: 7283.7065\n",
      "Epoch [600/8000], Loss: 4822.4604, Val Loss: 5441.8330\n",
      "Epoch [700/8000], Loss: 4601.1895, Val Loss: 3959.5132\n",
      "Epoch [800/8000], Loss: 2730.0044, Val Loss: 2792.3157\n",
      "Epoch [900/8000], Loss: 2838.2622, Val Loss: 1901.4434\n",
      "Epoch [1000/8000], Loss: 3543.3816, Val Loss: 1236.9226\n",
      "Epoch [1100/8000], Loss: 835.0153, Val Loss: 761.1047\n",
      "Epoch [1200/8000], Loss: 134.0529, Val Loss: 439.4286\n",
      "Epoch [1300/8000], Loss: 1.0507, Val Loss: 233.5591\n",
      "Epoch [1400/8000], Loss: 0.8516, Val Loss: 116.6809\n",
      "Epoch [1500/8000], Loss: 28.6091, Val Loss: 55.5436\n",
      "Epoch [1600/8000], Loss: 0.3527, Val Loss: 25.6186\n",
      "Epoch [1700/8000], Loss: 0.5226, Val Loss: 11.7919\n",
      "Epoch [1800/8000], Loss: 1.0172, Val Loss: 6.0297\n",
      "Epoch [1900/8000], Loss: 0.8218, Val Loss: 3.2568\n",
      "Epoch [2000/8000], Loss: 1.6560, Val Loss: 2.0912\n",
      "Epoch [2100/8000], Loss: 0.7444, Val Loss: 1.6925\n",
      "Epoch [2200/8000], Loss: 1.8920, Val Loss: 1.7384\n",
      "Epoch [2300/8000], Loss: 0.2725, Val Loss: 1.5264\n",
      "Epoch [2400/8000], Loss: 0.7662, Val Loss: 1.4050\n",
      "Epoch [2500/8000], Loss: 0.4672, Val Loss: 1.3241\n",
      "Epoch [2600/8000], Loss: 0.4366, Val Loss: 1.3706\n",
      "Epoch [2700/8000], Loss: 0.4019, Val Loss: 1.4125\n",
      "Epoch [2800/8000], Loss: 3.0611, Val Loss: 1.7612\n",
      "Epoch [2900/8000], Loss: 1.0886, Val Loss: 1.2867\n",
      "Epoch [3000/8000], Loss: 2.1469, Val Loss: 1.3748\n",
      "Epoch [3100/8000], Loss: 0.5407, Val Loss: 1.2627\n",
      "Epoch [3200/8000], Loss: 0.8001, Val Loss: 1.2591\n",
      "Epoch [3300/8000], Loss: 0.3748, Val Loss: 1.3376\n",
      "Epoch [3400/8000], Loss: 0.5124, Val Loss: 1.4920\n",
      "Epoch [3500/8000], Loss: 0.7958, Val Loss: 1.4415\n",
      "Epoch [3600/8000], Loss: 0.5430, Val Loss: 1.2751\n",
      "Epoch [3700/8000], Loss: 0.6568, Val Loss: 1.2669\n",
      "Epoch [3800/8000], Loss: 0.7289, Val Loss: 1.3516\n",
      "Epoch [3900/8000], Loss: 0.7448, Val Loss: 1.1907\n",
      "Epoch [4000/8000], Loss: 1.4368, Val Loss: 1.2432\n",
      "Epoch [4100/8000], Loss: 1.0244, Val Loss: 1.2682\n",
      "Epoch [4200/8000], Loss: 0.4825, Val Loss: 1.2967\n",
      "Epoch [4300/8000], Loss: 0.4363, Val Loss: 1.2599\n",
      "Epoch [4400/8000], Loss: 1.1683, Val Loss: 1.2310\n",
      "Epoch [4500/8000], Loss: 0.8497, Val Loss: 1.3167\n",
      "Epoch [4600/8000], Loss: 0.4127, Val Loss: 1.3977\n",
      "Epoch [4700/8000], Loss: 1.0069, Val Loss: 1.4070\n",
      "Epoch [4800/8000], Loss: 0.3395, Val Loss: 1.2375\n",
      "Epoch [4900/8000], Loss: 1.3151, Val Loss: 1.3345\n",
      "Epoch [5000/8000], Loss: 1.1274, Val Loss: 1.6664\n",
      "Epoch [5100/8000], Loss: 0.9202, Val Loss: 1.1759\n",
      "Epoch [5200/8000], Loss: 0.8518, Val Loss: 1.2342\n",
      "Epoch [5300/8000], Loss: 0.5309, Val Loss: 1.2739\n",
      "Epoch [5400/8000], Loss: 2.1077, Val Loss: 1.2196\n",
      "Epoch [5500/8000], Loss: 1.0391, Val Loss: 1.2098\n",
      "Epoch [5600/8000], Loss: 0.2451, Val Loss: 1.2317\n",
      "Epoch [5700/8000], Loss: 0.6729, Val Loss: 1.1954\n",
      "Epoch [5800/8000], Loss: 0.4028, Val Loss: 1.3131\n",
      "Epoch [5900/8000], Loss: 0.3001, Val Loss: 1.1583\n",
      "Epoch [6000/8000], Loss: 1.0655, Val Loss: 1.2031\n",
      "Epoch [6100/8000], Loss: 0.5068, Val Loss: 1.1928\n",
      "Epoch [6200/8000], Loss: 0.6256, Val Loss: 1.2480\n",
      "Epoch [6300/8000], Loss: 0.4264, Val Loss: 1.2668\n",
      "Epoch [6400/8000], Loss: 0.7368, Val Loss: 1.2170\n",
      "Epoch [6500/8000], Loss: 0.6330, Val Loss: 1.2356\n",
      "Epoch [6600/8000], Loss: 0.4719, Val Loss: 1.1893\n",
      "Epoch [6700/8000], Loss: 0.4484, Val Loss: 1.2399\n",
      "Epoch [6800/8000], Loss: 0.9190, Val Loss: 1.2244\n",
      "Epoch [6900/8000], Loss: 0.3673, Val Loss: 1.1759\n",
      "Epoch [7000/8000], Loss: 0.7805, Val Loss: 1.2731\n",
      "Epoch [7100/8000], Loss: 1.1545, Val Loss: 1.2511\n",
      "Epoch [7200/8000], Loss: 0.5860, Val Loss: 1.2240\n",
      "Epoch [7300/8000], Loss: 0.6458, Val Loss: 1.2697\n",
      "Epoch [7400/8000], Loss: 0.4110, Val Loss: 1.2572\n",
      "Epoch [7500/8000], Loss: 1.3050, Val Loss: 1.3474\n",
      "Epoch [7600/8000], Loss: 0.4106, Val Loss: 1.1874\n",
      "Epoch [7700/8000], Loss: 0.8588, Val Loss: 1.1751\n",
      "Epoch [7800/8000], Loss: 0.5360, Val Loss: 1.2435\n",
      "Epoch [7900/8000], Loss: 0.6222, Val Loss: 1.1720\n",
      "Epoch [8000/8000], Loss: 0.8323, Val Loss: 1.2820\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the x_train and y_train data\n",
    "x_train = data[['open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].to_numpy()\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Concatenate date embeddings with your testing data\n",
    "x_test_feature_tensors = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "x_test_date_embeddings = date_embeddings[len(x_train_scaled):]  # Use the remaining embeddings for testing data\n",
    "x_test_combined = torch.cat((x_test_feature_tensors, x_test_date_embeddings), dim=1)\n",
    "\n",
    "# Convert the combined testing data to PyTorch tensors\n",
    "x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Inside your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size + date_embedding_dim  # Updated input size to include date embeddings\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Ensure input has the shape [batch_size, sequence_length, input_size]\n",
    "        x = x.view(batch_size, 1, self.input_size)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 7  # Adjust based on the number of input features (excluding change_percent)\n",
    "date_embedding_dim = 3  # Adjust based on the dimension of your date embeddings\n",
    "output_size = 1\n",
    "hidden_dim = 32\n",
    "n_layers = 3\n",
    "sequence_length = 75  # Keep this as 1 for your input data\n",
    "batch_size = 64\n",
    "\n",
    "# Concatenate date embeddings with your feature vectors\n",
    "x_train_feature_tensors = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_train_date_embeddings = date_embeddings[:len(x_train_scaled)]  # Use the same length as your training data\n",
    "x_train_combined = torch.cat((x_train_feature_tensors, x_train_date_embeddings), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the combined data to PyTorch tensors\n",
    "#x_train_tensor = torch.tensor(x_train_combined, dtype=torch.float32)\n",
    "#y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Copy x_train_combined to create x_train_tensor\n",
    "x_train_tensor = x_train_combined.clone().detach()\n",
    "x_train_tensor = x_train_tensor.to(torch.float32)\n",
    "\n",
    "# Copy x_test_combined to create x_test_tensor\n",
    "x_test_tensor = x_test_combined.clone().detach()\n",
    "x_test_tensor = x_test_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "# Create y_train_tensor directly from y_train\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader for batch training\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.00015\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Hyperparameters: Learning Rate={learning_rate}, Sequence Length={sequence_length}, Batch Size={batch_size}, Input Size={input_size}, Date Embedding Dim={date_embedding_dim}, Hidden Dim={hidden_dim},'\n",
    "              f'Layers={n_layers}')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 8000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Evaluate on testing set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(x_test_tensor)\n",
    "            val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM (With time embeding. Preprocesing).ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39moptuna\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Define the objective function for Optuna hyperparameter optimization\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective\u001b[39m(trial):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20%28With%20time%20embeding.%20Preprocesing%29.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Define a search space for hyperparameters\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Define the objective function for Optuna hyperparameter optimization\n",
    "def objective(trial):\n",
    "    # Define a search space for hyperparameters\n",
    "    input_size = trial.suggest_int('input_size', 5, 20)\n",
    "    date_embedding_dim = trial.suggest_int('date_embedding_dim', 2, 10)\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 16, 128)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "    # Create an instance of the LSTM model with the suggested hyperparameters\n",
    "    model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 4000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            # Evaluate on testing set\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(x_test_tensor)\n",
    "                val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "            print(f'Trial {trial.number}, Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    # Calculate the final validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_test_tensor)\n",
    "        val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "\n",
    "    print(f'Trial {trial.number}, Final Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    return val_loss.item()\n",
    "\n",
    "# Create an Optuna study and optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize the validation loss\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials as needed\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_9648\\176290703.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: Learning Rate=1e-05, Sequence Length=75, Batch Size=64, Input Size=7, Date Embedding Dim=3, Hidden Dim=32,Layers=3\n",
      "Epoch [100/8000], Loss: 28950.9043, Val Loss: 29036.4414\n",
      "Epoch [200/8000], Loss: 15068.4043, Val Loss: 28256.5996\n",
      "Epoch [300/8000], Loss: 7812.2471, Val Loss: 27665.1289\n",
      "Epoch [400/8000], Loss: 16380.8115, Val Loss: 27101.8984\n",
      "Epoch [500/8000], Loss: 32320.1777, Val Loss: 26556.0469\n",
      "Epoch [600/8000], Loss: 24506.4375, Val Loss: 26026.7324\n",
      "Epoch [700/8000], Loss: 24143.5723, Val Loss: 25513.6387\n",
      "Epoch [800/8000], Loss: 35430.0117, Val Loss: 25016.9082\n",
      "Epoch [900/8000], Loss: 25550.9727, Val Loss: 24536.6582\n",
      "Epoch [1000/8000], Loss: 42544.4922, Val Loss: 24072.6230\n",
      "Epoch [1100/8000], Loss: 27106.0723, Val Loss: 23625.0059\n",
      "Epoch [1200/8000], Loss: 19985.8086, Val Loss: 23193.7754\n",
      "Epoch [1300/8000], Loss: 9424.4775, Val Loss: 22778.9414\n",
      "Epoch [1400/8000], Loss: 12648.0938, Val Loss: 22380.5137\n",
      "Epoch [1500/8000], Loss: 18165.0078, Val Loss: 21998.2930\n",
      "Epoch [1600/8000], Loss: 10779.2959, Val Loss: 21632.4258\n",
      "Epoch [1700/8000], Loss: 20154.1152, Val Loss: 21280.3203\n",
      "Epoch [1800/8000], Loss: 20796.7305, Val Loss: 20933.9316\n",
      "Epoch [1900/8000], Loss: 24749.4727, Val Loss: 20573.0000\n",
      "Epoch [2000/8000], Loss: 20184.7539, Val Loss: 20106.2832\n",
      "Epoch [2100/8000], Loss: 15306.4785, Val Loss: 19525.6270\n",
      "Epoch [2200/8000], Loss: 16876.8086, Val Loss: 19133.5254\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the x_train and y_train data\n",
    "x_train = data[['open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].to_numpy()\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Concatenate date embeddings with your testing data\n",
    "x_test_feature_tensors = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "x_test_date_embeddings = date_embeddings[len(x_train_scaled):]  # Use the remaining embeddings for testing data\n",
    "x_test_combined = torch.cat((x_test_feature_tensors, x_test_date_embeddings), dim=1)\n",
    "\n",
    "# Convert the combined testing data to PyTorch tensors\n",
    "x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Inside your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size + date_embedding_dim  # Updated input size to include date embeddings\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Ensure input has the shape [batch_size, sequence_length, input_size]\n",
    "        x = x.view(batch_size, 1, self.input_size)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 7  # Adjust based on the number of input features (excluding change_percent)\n",
    "date_embedding_dim = 3  # Adjust based on the dimension of your date embeddings\n",
    "output_size = 1\n",
    "hidden_dim = 32\n",
    "n_layers = 3\n",
    "sequence_length = 75  # Keep this as 1 for your input data\n",
    "batch_size = 64\n",
    "\n",
    "# Concatenate date embeddings with your feature vectors\n",
    "x_train_feature_tensors = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_train_date_embeddings = date_embeddings[:len(x_train_scaled)]  # Use the same length as your training data\n",
    "x_train_combined = torch.cat((x_train_feature_tensors, x_train_date_embeddings), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the combined data to PyTorch tensors\n",
    "#x_train_tensor = torch.tensor(x_train_combined, dtype=torch.float32)\n",
    "#y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Copy x_train_combined to create x_train_tensor\n",
    "x_train_tensor = x_train_combined.clone().detach()\n",
    "x_train_tensor = x_train_tensor.to(torch.float32)\n",
    "\n",
    "# Copy x_test_combined to create x_test_tensor\n",
    "x_test_tensor = x_test_combined.clone().detach()\n",
    "x_test_tensor = x_test_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "# Create y_train_tensor directly from y_train\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader for batch training\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.00001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Hyperparameters: Learning Rate={learning_rate}, Sequence Length={sequence_length}, Batch Size={batch_size}, Input Size={input_size}, Date Embedding Dim={date_embedding_dim}, Hidden Dim={hidden_dim},'\n",
    "              f'Layers={n_layers}')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 8000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Evaluate on testing set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(x_test_tensor)\n",
    "            val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
