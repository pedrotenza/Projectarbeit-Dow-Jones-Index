{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters: input_size=7, hidden_size=5, num_layers=2, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
    "\n",
    "Early stopping at epoch 25536 with validation loss 400.9601135253906.\n",
    "Test Loss: 386.94378662109375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    # Initialize lists to store training and temporary sets\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    # Iterate through the data with the specified window size\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i+split_window_size+1]\n",
    "        y_out1_out2 = y_in.iloc[i:i+split_window_size+1]\n",
    "\n",
    "        # Separate the last row for the temporary set\n",
    "        # [ :-1]: all elements except the last one\n",
    "        # [-1:]:  selects only the last element\n",
    "        # (:) is used to indicate slicing of a sequence\n",
    "        # sequence[start : end : step]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    # Concatenate the lists into pandas DataFrames\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "# Split Data to train and temp\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 3\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split temp into val and test\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 1\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob=0.5):  # Added dropout_prob\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)  # Included dropout in LSTM\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Added dropout layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    # Initialize lists to store training and temporary sets\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    # Iterate through the data with the specified window size\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i+split_window_size+1]\n",
    "        y_out1_out2 = y_in.iloc[i:i+split_window_size+1]\n",
    "\n",
    "        # Separate the last row for the temporary set\n",
    "        # [ :-1]: all elements except the last one\n",
    "        # [-1:]:  selects only the last element\n",
    "        # (:) is used to indicate slicing of a sequence\n",
    "        # sequence[start : end : step]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    # Concatenate the lists into pandas DataFrames\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "# Split Data to train and temp\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 3\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split temp into val and test\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 1\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob=0.5):  # Added dropout_prob\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)  # Included dropout in LSTM\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Added dropout layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=5, num_layers=2, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Define the loss function and optimizer\u001b[39;00m\n\u001b[0;32m     52\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m---> 53\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Initialize the scheduler after defining the optimizer\u001b[39;00m\n\u001b[0;32m     56\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m lr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39mnum_epochs)\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:278\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\_compile.py:22\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m code_context\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\_dynamo\\allowed_functions.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdeprecated_func\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_symbolic_trace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_fx_tracing\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_compiling\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashable, is_safe_constant, NP_SUPPORTED_MODULES\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\_dynamo\\config.py:288\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG_DIR_VAR_NAME \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    285\u001b[0m     debug_dir_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(  \u001b[38;5;66;03m# [@compile_ignored: debug]\u001b[39;00m\n\u001b[0;32m    286\u001b[0m         os\u001b[38;5;241m.\u001b[39menviron[DEBUG_DIR_VAR_NAME], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_compile_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[1;32m--> 288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_fbcode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    289\u001b[0m     debug_dir_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(  \u001b[38;5;66;03m# [@compile_ignored: debug]\u001b[39;00m\n\u001b[0;32m    290\u001b[0m         tempfile\u001b[38;5;241m.\u001b[39mgettempdir(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_compile_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\_dynamo\\config.py:279\u001b[0m, in \u001b[0;36mis_fbcode\u001b[1;34m()\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_fbcode\u001b[39m():\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit_version\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\__init__.py:1938\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   1935\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m   1936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m-> 1938\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np  # Added this line\n",
    "\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [5]\n",
    "num_layers_list = [2]\n",
    "\n",
    "learning_rates = [0.005]\n",
    "\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "factors = [0.5]\n",
    "patience_lr = [10]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "def set_random_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    set_random_seeds(42)\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Lear. Rate: {current_lr:.5f}, Train Grad.: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=5, num_layers=1, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150000, Train Loss: 30031, Val Loss: 30527,  Lear. Rate: 0.00500, Train Grad.: 224.6\n",
      "Epoch 101/150000, Train Loss: 29224, Val Loss: 29707,  Lear. Rate: 0.00453, Train Grad.: 217.4\n",
      "Epoch 201/150000, Train Loss: 28529, Val Loss: 29005,  Lear. Rate: 0.00455, Train Grad.: 210.9\n",
      "Epoch 301/150000, Train Loss: 27920, Val Loss: 28389,  Lear. Rate: 0.00457, Train Grad.: 205.0\n",
      "Epoch 401/150000, Train Loss: 27352, Val Loss: 27815,  Lear. Rate: 0.00459, Train Grad.: 199.4\n",
      "Epoch 501/150000, Train Loss: 26814, Val Loss: 27270,  Lear. Rate: 0.00460, Train Grad.: 193.9\n",
      "Epoch 601/150000, Train Loss: 26301, Val Loss: 26750,  Lear. Rate: 0.00462, Train Grad.: 188.6\n",
      "Epoch 701/150000, Train Loss: 25810, Val Loss: 26253,  Lear. Rate: 0.00463, Train Grad.: 183.3\n",
      "Epoch 801/150000, Train Loss: 25340, Val Loss: 25776,  Lear. Rate: 0.00464, Train Grad.: 178.1\n",
      "Epoch 901/150000, Train Loss: 24889, Val Loss: 25319,  Lear. Rate: 0.00466, Train Grad.: 172.9\n",
      "Epoch 1001/150000, Train Loss: 24457, Val Loss: 24881,  Lear. Rate: 0.00467, Train Grad.: 167.9\n",
      "Epoch 1101/150000, Train Loss: 24042, Val Loss: 24461,  Lear. Rate: 0.00468, Train Grad.: 162.8\n",
      "Epoch 1201/150000, Train Loss: 23645, Val Loss: 24058,  Lear. Rate: 0.00469, Train Grad.: 157.9\n",
      "Epoch 1301/150000, Train Loss: 23265, Val Loss: 23671,  Lear. Rate: 0.00470, Train Grad.: 153.0\n",
      "Epoch 1401/150000, Train Loss: 22901, Val Loss: 23301,  Lear. Rate: 0.00471, Train Grad.: 148.2\n",
      "Epoch 1501/150000, Train Loss: 22553, Val Loss: 22947,  Lear. Rate: 0.00472, Train Grad.: 143.4\n",
      "Epoch 1601/150000, Train Loss: 22220, Val Loss: 22609,  Lear. Rate: 0.00472, Train Grad.: 138.7\n",
      "Epoch 1701/150000, Train Loss: 21882, Val Loss: 22264,  Lear. Rate: 0.00473, Train Grad.: 136.9\n",
      "Epoch 1801/150000, Train Loss: 21375, Val Loss: 21795,  Lear. Rate: 0.00474, Train Grad.: 146.0\n",
      "Epoch 1901/150000, Train Loss: 21005, Val Loss: 21421,  Lear. Rate: 0.00475, Train Grad.: 143.7\n",
      "Epoch 2001/150000, Train Loss: 20642, Val Loss: 21049,  Lear. Rate: 0.00476, Train Grad.: 140.9\n",
      "Epoch 2101/150000, Train Loss: 20286, Val Loss: 20682,  Lear. Rate: 0.00477, Train Grad.: 138.5\n",
      "Epoch 2201/150000, Train Loss: 19939, Val Loss: 20330,  Lear. Rate: 0.00478, Train Grad.: 136.1\n",
      "Epoch 2301/150000, Train Loss: 19598, Val Loss: 19982,  Lear. Rate: 0.00478, Train Grad.: 134.7\n",
      "Epoch 2401/150000, Train Loss: 19264, Val Loss: 19642,  Lear. Rate: 0.00479, Train Grad.: 131.6\n",
      "Epoch 2501/150000, Train Loss: 18936, Val Loss: 19308,  Lear. Rate: 0.00480, Train Grad.: 129.5\n",
      "Epoch 2601/150000, Train Loss: 18614, Val Loss: 18980,  Lear. Rate: 0.00480, Train Grad.: 127.2\n",
      "Epoch 2701/150000, Train Loss: 18299, Val Loss: 18659,  Lear. Rate: 0.00481, Train Grad.: 125.0\n",
      "Epoch 2801/150000, Train Loss: 17989, Val Loss: 18343,  Lear. Rate: 0.00482, Train Grad.: 122.8\n",
      "Epoch 2901/150000, Train Loss: 17685, Val Loss: 18034,  Lear. Rate: 0.00482, Train Grad.: 121.0\n",
      "Epoch 3001/150000, Train Loss: 17387, Val Loss: 17730,  Lear. Rate: 0.00483, Train Grad.: 118.9\n",
      "Epoch 3101/150000, Train Loss: 17095, Val Loss: 17432,  Lear. Rate: 0.00484, Train Grad.: 116.8\n",
      "Epoch 3201/150000, Train Loss: 16808, Val Loss: 17140,  Lear. Rate: 0.00484, Train Grad.: 114.8\n",
      "Epoch 3301/150000, Train Loss: 16526, Val Loss: 16854,  Lear. Rate: 0.00485, Train Grad.: 112.8\n",
      "Epoch 3401/150000, Train Loss: 16250, Val Loss: 16573,  Lear. Rate: 0.00485, Train Grad.: 110.9\n",
      "Epoch 3501/150000, Train Loss: 15979, Val Loss: 16298,  Lear. Rate: 0.00486, Train Grad.: 108.9\n",
      "Epoch 3601/150000, Train Loss: 15714, Val Loss: 16028,  Lear. Rate: 0.00486, Train Grad.: 107.0\n",
      "Epoch 3701/150000, Train Loss: 15454, Val Loss: 15764,  Lear. Rate: 0.00486, Train Grad.: 105.1\n",
      "Epoch 3801/150000, Train Loss: 15199, Val Loss: 15505,  Lear. Rate: 0.00487, Train Grad.: 103.2\n",
      "Epoch 3901/150000, Train Loss: 14949, Val Loss: 15252,  Lear. Rate: 0.00487, Train Grad.: 101.3\n",
      "Epoch 4001/150000, Train Loss: 14704, Val Loss: 15003,  Lear. Rate: 0.00488, Train Grad.: 99.5\n",
      "Epoch 4101/150000, Train Loss: 14463, Val Loss: 14759,  Lear. Rate: 0.00488, Train Grad.: 97.8\n",
      "Epoch 4201/150000, Train Loss: 14227, Val Loss: 14520,  Lear. Rate: 0.00489, Train Grad.: 96.1\n",
      "Epoch 4301/150000, Train Loss: 13995, Val Loss: 14284,  Lear. Rate: 0.00489, Train Grad.: 94.4\n",
      "Epoch 4401/150000, Train Loss: 13768, Val Loss: 14053,  Lear. Rate: 0.00489, Train Grad.: 92.8\n",
      "Epoch 4501/150000, Train Loss: 13544, Val Loss: 13826,  Lear. Rate: 0.00490, Train Grad.: 91.2\n",
      "Epoch 4601/150000, Train Loss: 13325, Val Loss: 13602,  Lear. Rate: 0.00490, Train Grad.: 89.6\n",
      "Epoch 4701/150000, Train Loss: 13109, Val Loss: 13383,  Lear. Rate: 0.00490, Train Grad.: 88.0\n",
      "Epoch 4801/150000, Train Loss: 12897, Val Loss: 13168,  Lear. Rate: 0.00491, Train Grad.: 86.5\n",
      "Epoch 4901/150000, Train Loss: 12689, Val Loss: 12957,  Lear. Rate: 0.00491, Train Grad.: 85.1\n",
      "Epoch 5001/150000, Train Loss: 12484, Val Loss: 12749,  Lear. Rate: 0.00491, Train Grad.: 83.6\n",
      "Epoch 5101/150000, Train Loss: 12283, Val Loss: 12544,  Lear. Rate: 0.00491, Train Grad.: 82.2\n",
      "Epoch 5201/150000, Train Loss: 12085, Val Loss: 12344,  Lear. Rate: 0.00492, Train Grad.: 80.9\n",
      "Epoch 5301/150000, Train Loss: 11891, Val Loss: 12147,  Lear. Rate: 0.00492, Train Grad.: 79.5\n",
      "Epoch 5401/150000, Train Loss: 11699, Val Loss: 11953,  Lear. Rate: 0.00492, Train Grad.: 78.2\n",
      "Epoch 5501/150000, Train Loss: 11511, Val Loss: 11762,  Lear. Rate: 0.00492, Train Grad.: 76.9\n",
      "Epoch 5601/150000, Train Loss: 11326, Val Loss: 11575,  Lear. Rate: 0.00493, Train Grad.: 75.7\n",
      "Epoch 5701/150000, Train Loss: 11143, Val Loss: 11391,  Lear. Rate: 0.00493, Train Grad.: 74.4\n",
      "Epoch 5801/150000, Train Loss: 10964, Val Loss: 11209,  Lear. Rate: 0.00493, Train Grad.: 73.2\n",
      "Epoch 5901/150000, Train Loss: 10787, Val Loss: 11031,  Lear. Rate: 0.00493, Train Grad.: 72.0\n",
      "Epoch 6001/150000, Train Loss: 10614, Val Loss: 10856,  Lear. Rate: 0.00494, Train Grad.: 70.9\n",
      "Epoch 6101/150000, Train Loss: 10442, Val Loss: 10683,  Lear. Rate: 0.00494, Train Grad.: 69.8\n",
      "Epoch 6201/150000, Train Loss: 10274, Val Loss: 10513,  Lear. Rate: 0.00494, Train Grad.: 68.7\n",
      "Epoch 6301/150000, Train Loss: 10108, Val Loss: 10345,  Lear. Rate: 0.00494, Train Grad.: 67.6\n",
      "Epoch 6401/150000, Train Loss: 9944, Val Loss: 10180,  Lear. Rate: 0.00494, Train Grad.: 66.5\n",
      "Epoch 6501/150000, Train Loss: 9782, Val Loss: 10017,  Lear. Rate: 0.00495, Train Grad.: 65.5\n",
      "Epoch 6601/150000, Train Loss: 9623, Val Loss: 9856,  Lear. Rate: 0.00495, Train Grad.: 64.5\n",
      "Epoch 6701/150000, Train Loss: 9466, Val Loss: 9697,  Lear. Rate: 0.00495, Train Grad.: 63.5\n",
      "Epoch 6801/150000, Train Loss: 9312, Val Loss: 9541,  Lear. Rate: 0.00495, Train Grad.: 62.6\n",
      "Epoch 6901/150000, Train Loss: 9159, Val Loss: 9387,  Lear. Rate: 0.00495, Train Grad.: 61.7\n",
      "Epoch 7001/150000, Train Loss: 9008, Val Loss: 9234,  Lear. Rate: 0.00495, Train Grad.: 60.8\n",
      "Epoch 7101/150000, Train Loss: 8859, Val Loss: 9083,  Lear. Rate: 0.00495, Train Grad.: 60.1\n",
      "Epoch 7201/150000, Train Loss: 8712, Val Loss: 8934,  Lear. Rate: 0.00496, Train Grad.: 59.2\n",
      "Epoch 7301/150000, Train Loss: 8566, Val Loss: 8787,  Lear. Rate: 0.00496, Train Grad.: 58.3\n",
      "Epoch 7401/150000, Train Loss: 8422, Val Loss: 8641,  Lear. Rate: 0.00496, Train Grad.: 57.5\n",
      "Epoch 7501/150000, Train Loss: 8280, Val Loss: 8497,  Lear. Rate: 0.00496, Train Grad.: 56.8\n",
      "Epoch 7601/150000, Train Loss: 8139, Val Loss: 8353,  Lear. Rate: 0.00496, Train Grad.: 56.1\n",
      "Epoch 7701/150000, Train Loss: 8000, Val Loss: 8211,  Lear. Rate: 0.00496, Train Grad.: 55.3\n",
      "Epoch 7801/150000, Train Loss: 7863, Val Loss: 8072,  Lear. Rate: 0.00496, Train Grad.: 54.5\n",
      "Epoch 7901/150000, Train Loss: 7727, Val Loss: 7935,  Lear. Rate: 0.00497, Train Grad.: 53.8\n",
      "Epoch 8001/150000, Train Loss: 7593, Val Loss: 7800,  Lear. Rate: 0.00497, Train Grad.: 53.1\n",
      "Epoch 8101/150000, Train Loss: 7460, Val Loss: 7664,  Lear. Rate: 0.00497, Train Grad.: 52.5\n",
      "Epoch 8201/150000, Train Loss: 7329, Val Loss: 7531,  Lear. Rate: 0.00497, Train Grad.: 51.7\n",
      "Epoch 8301/150000, Train Loss: 7199, Val Loss: 7399,  Lear. Rate: 0.00497, Train Grad.: 51.1\n",
      "Epoch 8401/150000, Train Loss: 7071, Val Loss: 7269,  Lear. Rate: 0.00497, Train Grad.: 50.4\n",
      "Epoch 8501/150000, Train Loss: 6944, Val Loss: 7140,  Lear. Rate: 0.00497, Train Grad.: 49.7\n",
      "Epoch 8601/150000, Train Loss: 6819, Val Loss: 7013,  Lear. Rate: 0.00497, Train Grad.: 49.1\n",
      "Epoch 8701/150000, Train Loss: 6695, Val Loss: 6888,  Lear. Rate: 0.00497, Train Grad.: 48.4\n",
      "Epoch 8801/150000, Train Loss: 6573, Val Loss: 6764,  Lear. Rate: 0.00497, Train Grad.: 47.9\n",
      "Epoch 8901/150000, Train Loss: 6453, Val Loss: 6642,  Lear. Rate: 0.00498, Train Grad.: 47.5\n",
      "Epoch 9001/150000, Train Loss: 6334, Val Loss: 6522,  Lear. Rate: 0.00498, Train Grad.: 47.0\n",
      "Epoch 9101/150000, Train Loss: 6216, Val Loss: 6403,  Lear. Rate: 0.00498, Train Grad.: 45.9\n",
      "Epoch 9201/150000, Train Loss: 6100, Val Loss: 6286,  Lear. Rate: 0.00498, Train Grad.: 45.4\n",
      "Epoch 9301/150000, Train Loss: 5985, Val Loss: 6169,  Lear. Rate: 0.00498, Train Grad.: 44.7\n",
      "Epoch 9401/150000, Train Loss: 5872, Val Loss: 6056,  Lear. Rate: 0.00498, Train Grad.: 44.1\n",
      "Epoch 9501/150000, Train Loss: 5760, Val Loss: 5944,  Lear. Rate: 0.00498, Train Grad.: 43.5\n",
      "Epoch 9601/150000, Train Loss: 5650, Val Loss: 5831,  Lear. Rate: 0.00498, Train Grad.: 42.9\n",
      "Epoch 9701/150000, Train Loss: 5542, Val Loss: 5722,  Lear. Rate: 0.00498, Train Grad.: 42.3\n",
      "Epoch 9801/150000, Train Loss: 5435, Val Loss: 5615,  Lear. Rate: 0.00498, Train Grad.: 41.7\n",
      "Epoch 9901/150000, Train Loss: 5329, Val Loss: 5509,  Lear. Rate: 0.00498, Train Grad.: 41.1\n",
      "Epoch 10001/150000, Train Loss: 5225, Val Loss: 5405,  Lear. Rate: 0.00498, Train Grad.: 40.5\n",
      "Epoch 10101/150000, Train Loss: 5123, Val Loss: 5301,  Lear. Rate: 0.00498, Train Grad.: 39.9\n",
      "Epoch 10201/150000, Train Loss: 5022, Val Loss: 5197,  Lear. Rate: 0.00499, Train Grad.: 39.2\n",
      "Epoch 10301/150000, Train Loss: 4923, Val Loss: 5100,  Lear. Rate: 0.00499, Train Grad.: 38.9\n",
      "Epoch 10401/150000, Train Loss: 4825, Val Loss: 4999,  Lear. Rate: 0.00499, Train Grad.: 38.2\n",
      "Epoch 10501/150000, Train Loss: 4728, Val Loss: 4901,  Lear. Rate: 0.00499, Train Grad.: 37.8\n",
      "Epoch 10601/150000, Train Loss: 4632, Val Loss: 4804,  Lear. Rate: 0.00499, Train Grad.: 37.2\n",
      "Epoch 10701/150000, Train Loss: 4538, Val Loss: 4709,  Lear. Rate: 0.00499, Train Grad.: 36.7\n",
      "Epoch 10801/150000, Train Loss: 4446, Val Loss: 4616,  Lear. Rate: 0.00499, Train Grad.: 36.1\n",
      "Epoch 10901/150000, Train Loss: 4354, Val Loss: 4524,  Lear. Rate: 0.00499, Train Grad.: 35.6\n",
      "Epoch 11001/150000, Train Loss: 4265, Val Loss: 4435,  Lear. Rate: 0.00499, Train Grad.: 35.1\n",
      "Epoch 11101/150000, Train Loss: 4176, Val Loss: 4346,  Lear. Rate: 0.00499, Train Grad.: 34.6\n",
      "Epoch 11201/150000, Train Loss: 4089, Val Loss: 4258,  Lear. Rate: 0.00499, Train Grad.: 34.0\n",
      "Epoch 11301/150000, Train Loss: 4003, Val Loss: 4172,  Lear. Rate: 0.00499, Train Grad.: 33.6\n",
      "Epoch 11401/150000, Train Loss: 3918, Val Loss: 4087,  Lear. Rate: 0.00499, Train Grad.: 33.0\n",
      "Epoch 11501/150000, Train Loss: 3835, Val Loss: 4004,  Lear. Rate: 0.00499, Train Grad.: 32.5\n",
      "Epoch 11601/150000, Train Loss: 3753, Val Loss: 3922,  Lear. Rate: 0.00499, Train Grad.: 32.1\n",
      "Epoch 11701/150000, Train Loss: 3673, Val Loss: 3841,  Lear. Rate: 0.00499, Train Grad.: 31.6\n",
      "Epoch 11801/150000, Train Loss: 3594, Val Loss: 3762,  Lear. Rate: 0.00499, Train Grad.: 31.0\n",
      "Epoch 11901/150000, Train Loss: 3516, Val Loss: 3685,  Lear. Rate: 0.00499, Train Grad.: 30.6\n",
      "Epoch 12001/150000, Train Loss: 3440, Val Loss: 3608,  Lear. Rate: 0.00499, Train Grad.: 30.1\n",
      "Epoch 12101/150000, Train Loss: 3364, Val Loss: 3533,  Lear. Rate: 0.00499, Train Grad.: 29.5\n",
      "Epoch 12201/150000, Train Loss: 3291, Val Loss: 3459,  Lear. Rate: 0.00499, Train Grad.: 29.1\n",
      "Epoch 12301/150000, Train Loss: 3218, Val Loss: 3388,  Lear. Rate: 0.00499, Train Grad.: 28.6\n",
      "Epoch 12401/150000, Train Loss: 3147, Val Loss: 3317,  Lear. Rate: 0.00499, Train Grad.: 28.2\n",
      "Epoch 12501/150000, Train Loss: 3077, Val Loss: 3247,  Lear. Rate: 0.00499, Train Grad.: 27.8\n",
      "Epoch 12601/150000, Train Loss: 3009, Val Loss: 3176,  Lear. Rate: 0.00499, Train Grad.: 27.3\n",
      "Epoch 12701/150000, Train Loss: 2941, Val Loss: 3108,  Lear. Rate: 0.00499, Train Grad.: 26.9\n",
      "Epoch 12801/150000, Train Loss: 2874, Val Loss: 3041,  Lear. Rate: 0.00499, Train Grad.: 26.4\n",
      "Epoch 12901/150000, Train Loss: 2809, Val Loss: 2976,  Lear. Rate: 0.00500, Train Grad.: 26.0\n",
      "Epoch 13001/150000, Train Loss: 2745, Val Loss: 2912,  Lear. Rate: 0.00500, Train Grad.: 25.5\n",
      "Epoch 13101/150000, Train Loss: 2682, Val Loss: 2849,  Lear. Rate: 0.00500, Train Grad.: 25.1\n",
      "Epoch 13201/150000, Train Loss: 2620, Val Loss: 2787,  Lear. Rate: 0.00500, Train Grad.: 24.7\n",
      "Epoch 13301/150000, Train Loss: 2559, Val Loss: 2727,  Lear. Rate: 0.00500, Train Grad.: 24.2\n",
      "Epoch 13401/150000, Train Loss: 2500, Val Loss: 2668,  Lear. Rate: 0.00500, Train Grad.: 23.8\n",
      "Epoch 13501/150000, Train Loss: 2441, Val Loss: 2610,  Lear. Rate: 0.00500, Train Grad.: 23.4\n",
      "Epoch 13601/150000, Train Loss: 2384, Val Loss: 2553,  Lear. Rate: 0.00500, Train Grad.: 23.0\n",
      "Epoch 13701/150000, Train Loss: 2327, Val Loss: 2498,  Lear. Rate: 0.00500, Train Grad.: 22.6\n",
      "Epoch 13801/150000, Train Loss: 2272, Val Loss: 2442,  Lear. Rate: 0.00500, Train Grad.: 22.2\n",
      "Epoch 13901/150000, Train Loss: 2217, Val Loss: 2387,  Lear. Rate: 0.00500, Train Grad.: 21.8\n",
      "Epoch 14001/150000, Train Loss: 2164, Val Loss: 2333,  Lear. Rate: 0.00500, Train Grad.: 21.6\n",
      "Epoch 14101/150000, Train Loss: 2111, Val Loss: 2282,  Lear. Rate: 0.00500, Train Grad.: 21.2\n",
      "Epoch 14201/150000, Train Loss: 2059, Val Loss: 2230,  Lear. Rate: 0.00500, Train Grad.: 20.8\n",
      "Epoch 14301/150000, Train Loss: 2009, Val Loss: 2180,  Lear. Rate: 0.00500, Train Grad.: 20.5\n",
      "Epoch 14401/150000, Train Loss: 1959, Val Loss: 2131,  Lear. Rate: 0.00500, Train Grad.: 20.2\n",
      "Epoch 14501/150000, Train Loss: 1910, Val Loss: 2083,  Lear. Rate: 0.00500, Train Grad.: 19.6\n",
      "Epoch 14601/150000, Train Loss: 1862, Val Loss: 2036,  Lear. Rate: 0.00500, Train Grad.: 19.3\n",
      "Epoch 14701/150000, Train Loss: 1815, Val Loss: 1990,  Lear. Rate: 0.00500, Train Grad.: 18.9\n",
      "Epoch 14801/150000, Train Loss: 1769, Val Loss: 1944,  Lear. Rate: 0.00500, Train Grad.: 18.7\n",
      "Epoch 14901/150000, Train Loss: 1724, Val Loss: 1900,  Lear. Rate: 0.00500, Train Grad.: 18.2\n",
      "Epoch 15001/150000, Train Loss: 1680, Val Loss: 1857,  Lear. Rate: 0.00500, Train Grad.: 17.8\n",
      "Epoch 15101/150000, Train Loss: 1637, Val Loss: 1814,  Lear. Rate: 0.00500, Train Grad.: 17.7\n",
      "Epoch 15201/150000, Train Loss: 1595, Val Loss: 1772,  Lear. Rate: 0.00500, Train Grad.: 17.2\n",
      "Epoch 15301/150000, Train Loss: 1553, Val Loss: 1732,  Lear. Rate: 0.00500, Train Grad.: 16.9\n",
      "Epoch 15401/150000, Train Loss: 1513, Val Loss: 1692,  Lear. Rate: 0.00500, Train Grad.: 16.6\n",
      "Epoch 15501/150000, Train Loss: 1473, Val Loss: 1653,  Lear. Rate: 0.00500, Train Grad.: 16.3\n",
      "Epoch 15601/150000, Train Loss: 1434, Val Loss: 1615,  Lear. Rate: 0.00500, Train Grad.: 16.0\n",
      "Epoch 15701/150000, Train Loss: 1396, Val Loss: 1578,  Lear. Rate: 0.00500, Train Grad.: 15.7\n",
      "Epoch 15801/150000, Train Loss: 1358, Val Loss: 1542,  Lear. Rate: 0.00500, Train Grad.: 15.4\n",
      "Epoch 15901/150000, Train Loss: 1322, Val Loss: 1506,  Lear. Rate: 0.00500, Train Grad.: 15.1\n",
      "Epoch 16001/150000, Train Loss: 1286, Val Loss: 1472,  Lear. Rate: 0.00500, Train Grad.: 14.8\n",
      "Epoch 16101/150000, Train Loss: 1251, Val Loss: 1438,  Lear. Rate: 0.00500, Train Grad.: 14.5\n",
      "Epoch 16201/150000, Train Loss: 1216, Val Loss: 1405,  Lear. Rate: 0.00500, Train Grad.: 14.2\n",
      "Epoch 16301/150000, Train Loss: 1182, Val Loss: 1372,  Lear. Rate: 0.00500, Train Grad.: 14.0\n",
      "Epoch 16401/150000, Train Loss: 1150, Val Loss: 1341,  Lear. Rate: 0.00500, Train Grad.: 13.7\n",
      "Epoch 16501/150000, Train Loss: 1117, Val Loss: 1310,  Lear. Rate: 0.00500, Train Grad.: 13.4\n",
      "Epoch 16601/150000, Train Loss: 1086, Val Loss: 1280,  Lear. Rate: 0.00500, Train Grad.: 13.1\n",
      "Epoch 16701/150000, Train Loss: 1055, Val Loss: 1251,  Lear. Rate: 0.00500, Train Grad.: 12.8\n",
      "Epoch 16801/150000, Train Loss: 1025, Val Loss: 1222,  Lear. Rate: 0.00500, Train Grad.: 12.5\n",
      "Epoch 16901/150000, Train Loss: 996, Val Loss: 1194,  Lear. Rate: 0.00500, Train Grad.: 12.3\n",
      "Epoch 17001/150000, Train Loss: 968, Val Loss: 1166,  Lear. Rate: 0.00500, Train Grad.: 12.0\n",
      "Epoch 17101/150000, Train Loss: 940, Val Loss: 1140,  Lear. Rate: 0.00500, Train Grad.: 11.7\n",
      "Epoch 17201/150000, Train Loss: 912, Val Loss: 1114,  Lear. Rate: 0.00500, Train Grad.: 11.5\n",
      "Epoch 17301/150000, Train Loss: 886, Val Loss: 1090,  Lear. Rate: 0.00500, Train Grad.: 11.3\n",
      "Epoch 17401/150000, Train Loss: 859, Val Loss: 1065,  Lear. Rate: 0.00500, Train Grad.: 11.2\n",
      "Epoch 17501/150000, Train Loss: 834, Val Loss: 1042,  Lear. Rate: 0.00500, Train Grad.: 10.8\n",
      "Epoch 17601/150000, Train Loss: 809, Val Loss: 1020,  Lear. Rate: 0.00500, Train Grad.: 10.5\n",
      "Epoch 17701/150000, Train Loss: 785, Val Loss: 998,  Lear. Rate: 0.00500, Train Grad.: 10.3\n",
      "Epoch 17801/150000, Train Loss: 761, Val Loss: 978,  Lear. Rate: 0.00500, Train Grad.: 10.1\n",
      "Epoch 17901/150000, Train Loss: 738, Val Loss: 956,  Lear. Rate: 0.00500, Train Grad.: 9.9\n",
      "Epoch 18001/150000, Train Loss: 716, Val Loss: 934,  Lear. Rate: 0.00500, Train Grad.: 9.8\n",
      "Epoch 18101/150000, Train Loss: 694, Val Loss: 912,  Lear. Rate: 0.00500, Train Grad.: 9.4\n",
      "Epoch 18201/150000, Train Loss: 672, Val Loss: 891,  Lear. Rate: 0.00500, Train Grad.: 9.2\n",
      "Epoch 18301/150000, Train Loss: 652, Val Loss: 872,  Lear. Rate: 0.00500, Train Grad.: 8.9\n",
      "Epoch 18401/150000, Train Loss: 631, Val Loss: 853,  Lear. Rate: 0.00500, Train Grad.: 8.7\n",
      "Epoch 18501/150000, Train Loss: 612, Val Loss: 835,  Lear. Rate: 0.00500, Train Grad.: 8.5\n",
      "Epoch 18601/150000, Train Loss: 592, Val Loss: 818,  Lear. Rate: 0.00500, Train Grad.: 8.3\n",
      "Epoch 18701/150000, Train Loss: 574, Val Loss: 802,  Lear. Rate: 0.00500, Train Grad.: 8.0\n",
      "Epoch 18801/150000, Train Loss: 555, Val Loss: 787,  Lear. Rate: 0.00500, Train Grad.: 7.9\n",
      "Epoch 18901/150000, Train Loss: 538, Val Loss: 771,  Lear. Rate: 0.00500, Train Grad.: 7.7\n",
      "Epoch 19001/150000, Train Loss: 521, Val Loss: 756,  Lear. Rate: 0.00500, Train Grad.: 7.6\n",
      "Epoch 19101/150000, Train Loss: 504, Val Loss: 742,  Lear. Rate: 0.00500, Train Grad.: 7.4\n",
      "Epoch 19201/150000, Train Loss: 488, Val Loss: 728,  Lear. Rate: 0.00500, Train Grad.: 7.2\n",
      "Epoch 19301/150000, Train Loss: 472, Val Loss: 713,  Lear. Rate: 0.00500, Train Grad.: 7.0\n",
      "Epoch 19401/150000, Train Loss: 457, Val Loss: 700,  Lear. Rate: 0.00500, Train Grad.: 6.8\n",
      "Epoch 19501/150000, Train Loss: 442, Val Loss: 687,  Lear. Rate: 0.00500, Train Grad.: 6.5\n",
      "Epoch 19601/150000, Train Loss: 428, Val Loss: 674,  Lear. Rate: 0.00500, Train Grad.: 6.6\n",
      "Epoch 19701/150000, Train Loss: 414, Val Loss: 663,  Lear. Rate: 0.00500, Train Grad.: 6.2\n",
      "Epoch 19801/150000, Train Loss: 400, Val Loss: 651,  Lear. Rate: 0.00500, Train Grad.: 6.0\n",
      "Epoch 19901/150000, Train Loss: 387, Val Loss: 641,  Lear. Rate: 0.00500, Train Grad.: 5.8\n",
      "Epoch 20001/150000, Train Loss: 374, Val Loss: 630,  Lear. Rate: 0.00500, Train Grad.: 5.7\n",
      "Epoch 20101/150000, Train Loss: 362, Val Loss: 620,  Lear. Rate: 0.00500, Train Grad.: 5.4\n",
      "Epoch 20201/150000, Train Loss: 350, Val Loss: 609,  Lear. Rate: 0.00500, Train Grad.: 5.5\n",
      "Epoch 20301/150000, Train Loss: 338, Val Loss: 586,  Lear. Rate: 0.00500, Train Grad.: 5.3\n",
      "Epoch 20401/150000, Train Loss: 326, Val Loss: 571,  Lear. Rate: 0.00500, Train Grad.: 5.3\n",
      "Epoch 20501/150000, Train Loss: 314, Val Loss: 573,  Lear. Rate: 0.00500, Train Grad.: 5.1\n",
      "Epoch 20601/150000, Train Loss: 302, Val Loss: 555,  Lear. Rate: 0.00500, Train Grad.: 4.8\n",
      "Early stopping at epoch 20618 with validation loss 604.8871459960938.\n",
      "Test Loss: 616.4830322265625\n",
      "Hyperparameters: input_size=7, hidden_size=5, num_layers=2, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 30126, Val Loss: 30623,  Lear. Rate: 0.00500, Train Grad.: 225.5\n",
      "Epoch 101/150000, Train Loss: 29390, Val Loss: 29876,  Lear. Rate: 0.00453, Train Grad.: 218.9\n",
      "Epoch 201/150000, Train Loss: 28651, Val Loss: 29128,  Lear. Rate: 0.00455, Train Grad.: 212.0\n",
      "Epoch 301/150000, Train Loss: 28002, Val Loss: 28472,  Lear. Rate: 0.00457, Train Grad.: 205.8\n",
      "Epoch 401/150000, Train Loss: 27415, Val Loss: 27878,  Lear. Rate: 0.00459, Train Grad.: 200.0\n",
      "Epoch 501/150000, Train Loss: 26865, Val Loss: 27322,  Lear. Rate: 0.00460, Train Grad.: 194.4\n",
      "Epoch 601/150000, Train Loss: 26344, Val Loss: 26794,  Lear. Rate: 0.00462, Train Grad.: 189.0\n",
      "Epoch 701/150000, Train Loss: 25846, Val Loss: 26290,  Lear. Rate: 0.00463, Train Grad.: 183.7\n",
      "Epoch 801/150000, Train Loss: 25371, Val Loss: 25808,  Lear. Rate: 0.00464, Train Grad.: 178.4\n",
      "Epoch 901/150000, Train Loss: 24916, Val Loss: 25347,  Lear. Rate: 0.00466, Train Grad.: 173.2\n",
      "Epoch 1001/150000, Train Loss: 24481, Val Loss: 24906,  Lear. Rate: 0.00467, Train Grad.: 168.1\n",
      "Epoch 1101/150000, Train Loss: 24064, Val Loss: 24483,  Lear. Rate: 0.00468, Train Grad.: 163.1\n",
      "Epoch 1201/150000, Train Loss: 23665, Val Loss: 24077,  Lear. Rate: 0.00469, Train Grad.: 158.1\n",
      "Epoch 1301/150000, Train Loss: 23282, Val Loss: 23689,  Lear. Rate: 0.00470, Train Grad.: 153.2\n",
      "Epoch 1401/150000, Train Loss: 22917, Val Loss: 23317,  Lear. Rate: 0.00471, Train Grad.: 148.4\n",
      "Epoch 1501/150000, Train Loss: 22567, Val Loss: 22962,  Lear. Rate: 0.00472, Train Grad.: 143.6\n",
      "Epoch 1601/150000, Train Loss: 22233, Val Loss: 22622,  Lear. Rate: 0.00472, Train Grad.: 138.8\n",
      "Epoch 1701/150000, Train Loss: 21914, Val Loss: 22297,  Lear. Rate: 0.00473, Train Grad.: 134.2\n",
      "Epoch 1801/150000, Train Loss: 21609, Val Loss: 21987,  Lear. Rate: 0.00474, Train Grad.: 129.5\n",
      "Epoch 1901/150000, Train Loss: 21319, Val Loss: 21691,  Lear. Rate: 0.00475, Train Grad.: 125.0\n",
      "Epoch 2001/150000, Train Loss: 21043, Val Loss: 21409,  Lear. Rate: 0.00475, Train Grad.: 120.5\n",
      "Epoch 2101/150000, Train Loss: 20780, Val Loss: 21141,  Lear. Rate: 0.00476, Train Grad.: 116.0\n",
      "Epoch 2201/150000, Train Loss: 20531, Val Loss: 20886,  Lear. Rate: 0.00476, Train Grad.: 111.7\n",
      "Epoch 2301/150000, Train Loss: 20294, Val Loss: 20644,  Lear. Rate: 0.00477, Train Grad.: 107.3\n",
      "Epoch 2401/150000, Train Loss: 20070, Val Loss: 20415,  Lear. Rate: 0.00477, Train Grad.: 103.1\n",
      "Epoch 2501/150000, Train Loss: 19858, Val Loss: 20198,  Lear. Rate: 0.00478, Train Grad.: 98.9\n",
      "Epoch 2601/150000, Train Loss: 19658, Val Loss: 19993,  Lear. Rate: 0.00478, Train Grad.: 94.7\n",
      "Epoch 2701/150000, Train Loss: 19469, Val Loss: 19799,  Lear. Rate: 0.00479, Train Grad.: 90.7\n",
      "Epoch 2801/150000, Train Loss: 19291, Val Loss: 19616,  Lear. Rate: 0.00479, Train Grad.: 86.7\n",
      "Epoch 2901/150000, Train Loss: 19125, Val Loss: 19445,  Lear. Rate: 0.00480, Train Grad.: 82.7\n",
      "Epoch 3001/150000, Train Loss: 18968, Val Loss: 19284,  Lear. Rate: 0.00480, Train Grad.: 78.9\n",
      "Epoch 3101/150000, Train Loss: 18822, Val Loss: 19133,  Lear. Rate: 0.00480, Train Grad.: 75.1\n",
      "Epoch 3201/150000, Train Loss: 18686, Val Loss: 18992,  Lear. Rate: 0.00480, Train Grad.: 71.3\n",
      "Epoch 3301/150000, Train Loss: 18559, Val Loss: 18860,  Lear. Rate: 0.00481, Train Grad.: 67.7\n",
      "Epoch 3401/150000, Train Loss: 18441, Val Loss: 18738,  Lear. Rate: 0.00481, Train Grad.: 64.1\n",
      "Epoch 3501/150000, Train Loss: 18332, Val Loss: 18625,  Lear. Rate: 0.00481, Train Grad.: 60.6\n",
      "Epoch 3601/150000, Train Loss: 16642, Val Loss: 17025,  Lear. Rate: 0.00484, Train Grad.: 113.8\n",
      "Epoch 3701/150000, Train Loss: 16278, Val Loss: 16654,  Lear. Rate: 0.00485, Train Grad.: 110.9\n",
      "Epoch 3801/150000, Train Loss: 15934, Val Loss: 16302,  Lear. Rate: 0.00486, Train Grad.: 108.4\n",
      "Epoch 3901/150000, Train Loss: 15608, Val Loss: 15966,  Lear. Rate: 0.00486, Train Grad.: 106.0\n",
      "Epoch 4001/150000, Train Loss: 15295, Val Loss: 15644,  Lear. Rate: 0.00487, Train Grad.: 103.7\n",
      "Epoch 4101/150000, Train Loss: 14996, Val Loss: 15335,  Lear. Rate: 0.00487, Train Grad.: 101.5\n",
      "Epoch 4201/150000, Train Loss: 14708, Val Loss: 15038,  Lear. Rate: 0.00488, Train Grad.: 99.4\n",
      "Epoch 4301/150000, Train Loss: 14431, Val Loss: 14752,  Lear. Rate: 0.00488, Train Grad.: 97.5\n",
      "Epoch 4401/150000, Train Loss: 14163, Val Loss: 14479,  Lear. Rate: 0.00489, Train Grad.: 95.6\n",
      "Epoch 4501/150000, Train Loss: 13904, Val Loss: 14216,  Lear. Rate: 0.00489, Train Grad.: 93.7\n",
      "Epoch 4601/150000, Train Loss: 13654, Val Loss: 13961,  Lear. Rate: 0.00489, Train Grad.: 91.9\n",
      "Epoch 4701/150000, Train Loss: 13410, Val Loss: 13711,  Lear. Rate: 0.00490, Train Grad.: 90.1\n",
      "Epoch 4801/150000, Train Loss: 13173, Val Loss: 13469,  Lear. Rate: 0.00490, Train Grad.: 88.4\n",
      "Epoch 4901/150000, Train Loss: 12943, Val Loss: 13235,  Lear. Rate: 0.00490, Train Grad.: 86.7\n",
      "Epoch 5001/150000, Train Loss: 12719, Val Loss: 13007,  Lear. Rate: 0.00491, Train Grad.: 85.1\n",
      "Epoch 5101/150000, Train Loss: 12500, Val Loss: 12786,  Lear. Rate: 0.00491, Train Grad.: 83.6\n",
      "Epoch 5201/150000, Train Loss: 12286, Val Loss: 12569,  Lear. Rate: 0.00491, Train Grad.: 82.1\n",
      "Epoch 5301/150000, Train Loss: 12078, Val Loss: 12356,  Lear. Rate: 0.00492, Train Grad.: 80.7\n",
      "Epoch 5401/150000, Train Loss: 11873, Val Loss: 12148,  Lear. Rate: 0.00492, Train Grad.: 79.3\n",
      "Epoch 5501/150000, Train Loss: 11673, Val Loss: 11944,  Lear. Rate: 0.00492, Train Grad.: 77.8\n",
      "Epoch 5601/150000, Train Loss: 11477, Val Loss: 11744,  Lear. Rate: 0.00492, Train Grad.: 76.6\n",
      "Epoch 5701/150000, Train Loss: 11285, Val Loss: 11548,  Lear. Rate: 0.00493, Train Grad.: 75.5\n",
      "Epoch 5801/150000, Train Loss: 11096, Val Loss: 11356,  Lear. Rate: 0.00493, Train Grad.: 74.0\n",
      "Epoch 5901/150000, Train Loss: 10911, Val Loss: 11168,  Lear. Rate: 0.00493, Train Grad.: 72.8\n",
      "Epoch 6001/150000, Train Loss: 10730, Val Loss: 10984,  Lear. Rate: 0.00493, Train Grad.: 71.6\n",
      "Epoch 6101/150000, Train Loss: 10552, Val Loss: 10803,  Lear. Rate: 0.00494, Train Grad.: 70.4\n",
      "Epoch 6201/150000, Train Loss: 10377, Val Loss: 10625,  Lear. Rate: 0.00494, Train Grad.: 69.4\n",
      "Epoch 6301/150000, Train Loss: 10205, Val Loss: 10450,  Lear. Rate: 0.00494, Train Grad.: 68.2\n",
      "Epoch 6401/150000, Train Loss: 10036, Val Loss: 10279,  Lear. Rate: 0.00494, Train Grad.: 66.9\n",
      "Epoch 6501/150000, Train Loss: 9870, Val Loss: 10109,  Lear. Rate: 0.00494, Train Grad.: 66.1\n",
      "Epoch 6601/150000, Train Loss: 9706, Val Loss: 9943,  Lear. Rate: 0.00495, Train Grad.: 65.1\n",
      "Epoch 6701/150000, Train Loss: 9545, Val Loss: 9780,  Lear. Rate: 0.00495, Train Grad.: 64.1\n",
      "Epoch 6801/150000, Train Loss: 9387, Val Loss: 9619,  Lear. Rate: 0.00495, Train Grad.: 63.1\n",
      "Epoch 6901/150000, Train Loss: 9231, Val Loss: 9460,  Lear. Rate: 0.00495, Train Grad.: 62.3\n",
      "Epoch 7001/150000, Train Loss: 9077, Val Loss: 9304,  Lear. Rate: 0.00495, Train Grad.: 61.2\n",
      "Epoch 7101/150000, Train Loss: 8925, Val Loss: 9150,  Lear. Rate: 0.00495, Train Grad.: 60.4\n",
      "Epoch 7201/150000, Train Loss: 8775, Val Loss: 8997,  Lear. Rate: 0.00496, Train Grad.: 59.7\n",
      "Epoch 7301/150000, Train Loss: 8627, Val Loss: 8846,  Lear. Rate: 0.00496, Train Grad.: 58.7\n",
      "Epoch 7401/150000, Train Loss: 8482, Val Loss: 8698,  Lear. Rate: 0.00496, Train Grad.: 56.8\n",
      "Epoch 7501/150000, Train Loss: 8337, Val Loss: 8550,  Lear. Rate: 0.00496, Train Grad.: 57.1\n",
      "Epoch 7601/150000, Train Loss: 8195, Val Loss: 8406,  Lear. Rate: 0.00496, Train Grad.: 56.3\n",
      "Epoch 7701/150000, Train Loss: 8054, Val Loss: 8263,  Lear. Rate: 0.00496, Train Grad.: 55.6\n",
      "Epoch 7801/150000, Train Loss: 7915, Val Loss: 8121,  Lear. Rate: 0.00496, Train Grad.: 54.8\n",
      "Epoch 7901/150000, Train Loss: 7778, Val Loss: 7981,  Lear. Rate: 0.00497, Train Grad.: 54.1\n",
      "Epoch 8001/150000, Train Loss: 7642, Val Loss: 7843,  Lear. Rate: 0.00497, Train Grad.: 53.4\n",
      "Epoch 8101/150000, Train Loss: 7508, Val Loss: 7708,  Lear. Rate: 0.00497, Train Grad.: 52.2\n",
      "Epoch 8201/150000, Train Loss: 7376, Val Loss: 7573,  Lear. Rate: 0.00497, Train Grad.: 52.0\n",
      "Epoch 8301/150000, Train Loss: 7245, Val Loss: 7441,  Lear. Rate: 0.00497, Train Grad.: 51.3\n",
      "Epoch 8401/150000, Train Loss: 7116, Val Loss: 7310,  Lear. Rate: 0.00497, Train Grad.: 50.7\n",
      "Epoch 8501/150000, Train Loss: 6989, Val Loss: 7181,  Lear. Rate: 0.00497, Train Grad.: 50.0\n",
      "Epoch 8601/150000, Train Loss: 6863, Val Loss: 7054,  Lear. Rate: 0.00497, Train Grad.: 49.5\n",
      "Epoch 8701/150000, Train Loss: 6738, Val Loss: 6928,  Lear. Rate: 0.00497, Train Grad.: 48.7\n",
      "Epoch 8801/150000, Train Loss: 6617, Val Loss: 6804,  Lear. Rate: 0.00497, Train Grad.: 50.3\n",
      "Epoch 8901/150000, Train Loss: 6494, Val Loss: 6681,  Lear. Rate: 0.00498, Train Grad.: 47.4\n",
      "Epoch 9001/150000, Train Loss: 6374, Val Loss: 6560,  Lear. Rate: 0.00498, Train Grad.: 46.8\n",
      "Epoch 9101/150000, Train Loss: 6255, Val Loss: 6440,  Lear. Rate: 0.00498, Train Grad.: 46.0\n",
      "Epoch 9201/150000, Train Loss: 6139, Val Loss: 6322,  Lear. Rate: 0.00498, Train Grad.: 45.5\n",
      "Epoch 9301/150000, Train Loss: 6023, Val Loss: 6205,  Lear. Rate: 0.00498, Train Grad.: 44.8\n",
      "Epoch 9401/150000, Train Loss: 5909, Val Loss: 6090,  Lear. Rate: 0.00498, Train Grad.: 44.3\n",
      "Epoch 9501/150000, Train Loss: 5797, Val Loss: 5977,  Lear. Rate: 0.00498, Train Grad.: 43.7\n",
      "Epoch 9601/150000, Train Loss: 5686, Val Loss: 5865,  Lear. Rate: 0.00498, Train Grad.: 43.8\n",
      "Epoch 9701/150000, Train Loss: 5577, Val Loss: 5755,  Lear. Rate: 0.00498, Train Grad.: 42.5\n",
      "Epoch 9801/150000, Train Loss: 5469, Val Loss: 5646,  Lear. Rate: 0.00498, Train Grad.: 41.9\n",
      "Epoch 9901/150000, Train Loss: 5363, Val Loss: 5539,  Lear. Rate: 0.00498, Train Grad.: 41.3\n",
      "Epoch 10001/150000, Train Loss: 5259, Val Loss: 5433,  Lear. Rate: 0.00498, Train Grad.: 40.7\n",
      "Epoch 10101/150000, Train Loss: 5156, Val Loss: 5330,  Lear. Rate: 0.00498, Train Grad.: 40.1\n",
      "Epoch 10201/150000, Train Loss: 5055, Val Loss: 5227,  Lear. Rate: 0.00499, Train Grad.: 39.5\n",
      "Epoch 10301/150000, Train Loss: 4955, Val Loss: 5126,  Lear. Rate: 0.00499, Train Grad.: 39.1\n",
      "Epoch 10401/150000, Train Loss: 4856, Val Loss: 5027,  Lear. Rate: 0.00499, Train Grad.: 38.4\n",
      "Epoch 10501/150000, Train Loss: 4759, Val Loss: 4930,  Lear. Rate: 0.00499, Train Grad.: 37.8\n",
      "Epoch 10601/150000, Train Loss: 4663, Val Loss: 4833,  Lear. Rate: 0.00499, Train Grad.: 37.4\n",
      "Epoch 10701/150000, Train Loss: 4569, Val Loss: 4739,  Lear. Rate: 0.00499, Train Grad.: 36.7\n",
      "Epoch 10801/150000, Train Loss: 4476, Val Loss: 4645,  Lear. Rate: 0.00499, Train Grad.: 36.3\n",
      "Epoch 10901/150000, Train Loss: 4384, Val Loss: 4553,  Lear. Rate: 0.00499, Train Grad.: 35.6\n",
      "Epoch 11001/150000, Train Loss: 4294, Val Loss: 4463,  Lear. Rate: 0.00499, Train Grad.: 35.3\n",
      "Epoch 11101/150000, Train Loss: 4205, Val Loss: 4373,  Lear. Rate: 0.00499, Train Grad.: 34.9\n",
      "Epoch 11201/150000, Train Loss: 4117, Val Loss: 4284,  Lear. Rate: 0.00499, Train Grad.: 34.3\n",
      "Epoch 11301/150000, Train Loss: 4030, Val Loss: 4197,  Lear. Rate: 0.00499, Train Grad.: 33.8\n",
      "Epoch 11401/150000, Train Loss: 3945, Val Loss: 4112,  Lear. Rate: 0.00499, Train Grad.: 33.3\n",
      "Epoch 11501/150000, Train Loss: 3862, Val Loss: 4028,  Lear. Rate: 0.00499, Train Grad.: 32.8\n",
      "Epoch 11601/150000, Train Loss: 3779, Val Loss: 3945,  Lear. Rate: 0.00499, Train Grad.: 32.6\n",
      "Epoch 11701/150000, Train Loss: 3698, Val Loss: 3865,  Lear. Rate: 0.00499, Train Grad.: 31.7\n",
      "Epoch 11801/150000, Train Loss: 3619, Val Loss: 3785,  Lear. Rate: 0.00499, Train Grad.: 31.3\n",
      "Epoch 11901/150000, Train Loss: 3540, Val Loss: 3707,  Lear. Rate: 0.00499, Train Grad.: 30.9\n",
      "Epoch 12001/150000, Train Loss: 3463, Val Loss: 3630,  Lear. Rate: 0.00499, Train Grad.: 30.3\n",
      "Epoch 12101/150000, Train Loss: 3388, Val Loss: 3555,  Lear. Rate: 0.00499, Train Grad.: 29.4\n",
      "Epoch 12201/150000, Train Loss: 3313, Val Loss: 3481,  Lear. Rate: 0.00499, Train Grad.: 29.3\n",
      "Epoch 12301/150000, Train Loss: 3241, Val Loss: 3408,  Lear. Rate: 0.00499, Train Grad.: 29.2\n",
      "Epoch 12401/150000, Train Loss: 3169, Val Loss: 3336,  Lear. Rate: 0.00499, Train Grad.: 28.3\n",
      "Epoch 12501/150000, Train Loss: 3099, Val Loss: 3265,  Lear. Rate: 0.00499, Train Grad.: 28.2\n",
      "Epoch 12601/150000, Train Loss: 3029, Val Loss: 3195,  Lear. Rate: 0.00499, Train Grad.: 27.4\n",
      "Epoch 12701/150000, Train Loss: 2961, Val Loss: 3126,  Lear. Rate: 0.00499, Train Grad.: 27.5\n",
      "Epoch 12801/150000, Train Loss: 2894, Val Loss: 3059,  Lear. Rate: 0.00499, Train Grad.: 26.6\n",
      "Epoch 12901/150000, Train Loss: 2829, Val Loss: 2994,  Lear. Rate: 0.00500, Train Grad.: 25.8\n",
      "Epoch 13001/150000, Train Loss: 2764, Val Loss: 2929,  Lear. Rate: 0.00500, Train Grad.: 25.7\n",
      "Epoch 13101/150000, Train Loss: 2701, Val Loss: 2866,  Lear. Rate: 0.00500, Train Grad.: 25.2\n",
      "Epoch 13201/150000, Train Loss: 2639, Val Loss: 2804,  Lear. Rate: 0.00500, Train Grad.: 24.8\n",
      "Epoch 13301/150000, Train Loss: 2578, Val Loss: 2743,  Lear. Rate: 0.00500, Train Grad.: 24.4\n",
      "Epoch 13401/150000, Train Loss: 2518, Val Loss: 2684,  Lear. Rate: 0.00500, Train Grad.: 22.9\n",
      "Epoch 13501/150000, Train Loss: 2459, Val Loss: 2625,  Lear. Rate: 0.00500, Train Grad.: 23.6\n",
      "Epoch 13601/150000, Train Loss: 2402, Val Loss: 2567,  Lear. Rate: 0.00500, Train Grad.: 24.6\n",
      "Epoch 13701/150000, Train Loss: 2344, Val Loss: 2511,  Lear. Rate: 0.00500, Train Grad.: 22.8\n",
      "Epoch 13801/150000, Train Loss: 2289, Val Loss: 2456,  Lear. Rate: 0.00500, Train Grad.: 23.0\n",
      "Epoch 13901/150000, Train Loss: 2234, Val Loss: 2402,  Lear. Rate: 0.00500, Train Grad.: 22.0\n",
      "Epoch 14001/150000, Train Loss: 2180, Val Loss: 2349,  Lear. Rate: 0.00500, Train Grad.: 21.6\n",
      "Epoch 14101/150000, Train Loss: 2127, Val Loss: 2296,  Lear. Rate: 0.00500, Train Grad.: 21.2\n",
      "Epoch 14201/150000, Train Loss: 2075, Val Loss: 2245,  Lear. Rate: 0.00500, Train Grad.: 20.9\n",
      "Epoch 14301/150000, Train Loss: 2024, Val Loss: 2194,  Lear. Rate: 0.00500, Train Grad.: 20.7\n",
      "Epoch 14401/150000, Train Loss: 1974, Val Loss: 2145,  Lear. Rate: 0.00500, Train Grad.: 20.2\n",
      "Epoch 14501/150000, Train Loss: 1925, Val Loss: 2097,  Lear. Rate: 0.00500, Train Grad.: 19.6\n",
      "Epoch 14601/150000, Train Loss: 1877, Val Loss: 2049,  Lear. Rate: 0.00500, Train Grad.: 19.5\n",
      "Epoch 14701/150000, Train Loss: 1830, Val Loss: 2002,  Lear. Rate: 0.00500, Train Grad.: 19.1\n",
      "Epoch 14801/150000, Train Loss: 1784, Val Loss: 1956,  Lear. Rate: 0.00500, Train Grad.: 18.7\n",
      "Epoch 14901/150000, Train Loss: 1738, Val Loss: 1911,  Lear. Rate: 0.00500, Train Grad.: 18.4\n",
      "Epoch 15001/150000, Train Loss: 1694, Val Loss: 1868,  Lear. Rate: 0.00500, Train Grad.: 17.7\n",
      "Epoch 15101/150000, Train Loss: 1650, Val Loss: 1825,  Lear. Rate: 0.00500, Train Grad.: 17.7\n",
      "Epoch 15201/150000, Train Loss: 1608, Val Loss: 1783,  Lear. Rate: 0.00500, Train Grad.: 17.4\n",
      "Epoch 15301/150000, Train Loss: 1566, Val Loss: 1742,  Lear. Rate: 0.00500, Train Grad.: 17.0\n",
      "Epoch 15401/150000, Train Loss: 1525, Val Loss: 1702,  Lear. Rate: 0.00500, Train Grad.: 16.7\n",
      "Epoch 15501/150000, Train Loss: 1485, Val Loss: 1662,  Lear. Rate: 0.00500, Train Grad.: 16.7\n",
      "Epoch 15601/150000, Train Loss: 1446, Val Loss: 1625,  Lear. Rate: 0.00500, Train Grad.: 16.1\n",
      "Epoch 15701/150000, Train Loss: 1408, Val Loss: 1587,  Lear. Rate: 0.00500, Train Grad.: 15.8\n",
      "Epoch 15801/150000, Train Loss: 1370, Val Loss: 1551,  Lear. Rate: 0.00500, Train Grad.: 15.5\n",
      "Epoch 15901/150000, Train Loss: 1333, Val Loss: 1516,  Lear. Rate: 0.00500, Train Grad.: 15.2\n",
      "Epoch 16001/150000, Train Loss: 1297, Val Loss: 1480,  Lear. Rate: 0.00500, Train Grad.: 14.9\n",
      "Epoch 16101/150000, Train Loss: 1262, Val Loss: 1446,  Lear. Rate: 0.00500, Train Grad.: 14.6\n",
      "Epoch 16201/150000, Train Loss: 1227, Val Loss: 1413,  Lear. Rate: 0.00500, Train Grad.: 14.3\n",
      "Epoch 16301/150000, Train Loss: 1193, Val Loss: 1380,  Lear. Rate: 0.00500, Train Grad.: 14.1\n",
      "Epoch 16401/150000, Train Loss: 1160, Val Loss: 1348,  Lear. Rate: 0.00500, Train Grad.: 13.8\n",
      "Epoch 16501/150000, Train Loss: 1127, Val Loss: 1317,  Lear. Rate: 0.00500, Train Grad.: 13.3\n",
      "Epoch 16601/150000, Train Loss: 1096, Val Loss: 1286,  Lear. Rate: 0.00500, Train Grad.: 13.2\n",
      "Epoch 16701/150000, Train Loss: 1065, Val Loss: 1257,  Lear. Rate: 0.00500, Train Grad.: 12.7\n",
      "Epoch 16801/150000, Train Loss: 1034, Val Loss: 1228,  Lear. Rate: 0.00500, Train Grad.: 12.7\n",
      "Epoch 16901/150000, Train Loss: 1005, Val Loss: 1200,  Lear. Rate: 0.00500, Train Grad.: 12.4\n",
      "Epoch 17001/150000, Train Loss: 976, Val Loss: 1172,  Lear. Rate: 0.00500, Train Grad.: 12.1\n",
      "Epoch 17101/150000, Train Loss: 948, Val Loss: 1146,  Lear. Rate: 0.00500, Train Grad.: 12.2\n",
      "Epoch 17201/150000, Train Loss: 920, Val Loss: 1121,  Lear. Rate: 0.00500, Train Grad.: 11.6\n",
      "Epoch 17301/150000, Train Loss: 893, Val Loss: 1096,  Lear. Rate: 0.00500, Train Grad.: 11.1\n",
      "Epoch 17401/150000, Train Loss: 867, Val Loss: 1071,  Lear. Rate: 0.00500, Train Grad.: 11.1\n",
      "Epoch 17501/150000, Train Loss: 842, Val Loss: 1047,  Lear. Rate: 0.00500, Train Grad.: 12.5\n",
      "Epoch 17601/150000, Train Loss: 816, Val Loss: 1024,  Lear. Rate: 0.00500, Train Grad.: 10.6\n",
      "Epoch 17701/150000, Train Loss: 792, Val Loss: 1001,  Lear. Rate: 0.00500, Train Grad.: 10.4\n",
      "Epoch 17801/150000, Train Loss: 768, Val Loss: 979,  Lear. Rate: 0.00500, Train Grad.: 10.1\n",
      "Epoch 17901/150000, Train Loss: 745, Val Loss: 958,  Lear. Rate: 0.00500, Train Grad.: 9.9\n",
      "Epoch 18001/150000, Train Loss: 722, Val Loss: 938,  Lear. Rate: 0.00500, Train Grad.: 9.5\n",
      "Epoch 18101/150000, Train Loss: 700, Val Loss: 918,  Lear. Rate: 0.00500, Train Grad.: 9.5\n",
      "Epoch 18201/150000, Train Loss: 679, Val Loss: 898,  Lear. Rate: 0.00500, Train Grad.: 9.5\n",
      "Epoch 18301/150000, Train Loss: 658, Val Loss: 879,  Lear. Rate: 0.00500, Train Grad.: 9.0\n",
      "Epoch 18401/150000, Train Loss: 637, Val Loss: 861,  Lear. Rate: 0.00500, Train Grad.: 9.1\n",
      "Epoch 18501/150000, Train Loss: 617, Val Loss: 843,  Lear. Rate: 0.00500, Train Grad.: 8.6\n",
      "Epoch 18601/150000, Train Loss: 598, Val Loss: 826,  Lear. Rate: 0.00500, Train Grad.: 8.8\n",
      "Epoch 18701/150000, Train Loss: 579, Val Loss: 809,  Lear. Rate: 0.00500, Train Grad.: 8.2\n",
      "Epoch 18801/150000, Train Loss: 563, Val Loss: 792,  Lear. Rate: 0.00500, Train Grad.: 10.4\n",
      "Epoch 18901/150000, Train Loss: 543, Val Loss: 778,  Lear. Rate: 0.00500, Train Grad.: 7.8\n",
      "Epoch 19001/150000, Train Loss: 526, Val Loss: 761,  Lear. Rate: 0.00500, Train Grad.: 8.0\n",
      "Epoch 19101/150000, Train Loss: 509, Val Loss: 746,  Lear. Rate: 0.00500, Train Grad.: 7.4\n",
      "Epoch 19201/150000, Train Loss: 493, Val Loss: 731,  Lear. Rate: 0.00500, Train Grad.: 7.1\n",
      "Epoch 19301/150000, Train Loss: 478, Val Loss: 719,  Lear. Rate: 0.00500, Train Grad.: 5.2\n",
      "Epoch 19401/150000, Train Loss: 461, Val Loss: 702,  Lear. Rate: 0.00500, Train Grad.: 6.8\n",
      "Epoch 19501/150000, Train Loss: 446, Val Loss: 689,  Lear. Rate: 0.00500, Train Grad.: 6.6\n",
      "Epoch 19601/150000, Train Loss: 432, Val Loss: 678,  Lear. Rate: 0.00500, Train Grad.: 5.9\n",
      "Epoch 19701/150000, Train Loss: 418, Val Loss: 664,  Lear. Rate: 0.00500, Train Grad.: 6.3\n",
      "Epoch 19801/150000, Train Loss: 404, Val Loss: 652,  Lear. Rate: 0.00500, Train Grad.: 6.1\n",
      "Epoch 19901/150000, Train Loss: 391, Val Loss: 640,  Lear. Rate: 0.00500, Train Grad.: 6.1\n",
      "Epoch 20001/150000, Train Loss: 378, Val Loss: 629,  Lear. Rate: 0.00500, Train Grad.: 5.9\n",
      "Epoch 20101/150000, Train Loss: 366, Val Loss: 618,  Lear. Rate: 0.00500, Train Grad.: 5.8\n",
      "Epoch 20201/150000, Train Loss: 354, Val Loss: 608,  Lear. Rate: 0.00500, Train Grad.: 5.5\n",
      "Epoch 20301/150000, Train Loss: 343, Val Loss: 594,  Lear. Rate: 0.00500, Train Grad.: 7.7\n",
      "Epoch 20401/150000, Train Loss: 329, Val Loss: 582,  Lear. Rate: 0.00500, Train Grad.: 5.3\n",
      "Epoch 20501/150000, Train Loss: 318, Val Loss: 569,  Lear. Rate: 0.00500, Train Grad.: 4.6\n",
      "Epoch 20601/150000, Train Loss: 307, Val Loss: 558,  Lear. Rate: 0.00500, Train Grad.: 5.0\n",
      "Epoch 20701/150000, Train Loss: 297, Val Loss: 547,  Lear. Rate: 0.00500, Train Grad.: 4.9\n",
      "Epoch 20801/150000, Train Loss: 287, Val Loss: 538,  Lear. Rate: 0.00500, Train Grad.: 4.6\n",
      "Epoch 20901/150000, Train Loss: 277, Val Loss: 529,  Lear. Rate: 0.00500, Train Grad.: 4.5\n",
      "Epoch 21001/150000, Train Loss: 268, Val Loss: 522,  Lear. Rate: 0.00500, Train Grad.: 4.3\n",
      "Epoch 21101/150000, Train Loss: 259, Val Loss: 515,  Lear. Rate: 0.00500, Train Grad.: 4.6\n",
      "Epoch 21201/150000, Train Loss: 251, Val Loss: 508,  Lear. Rate: 0.00500, Train Grad.: 4.0\n",
      "Epoch 21301/150000, Train Loss: 242, Val Loss: 503,  Lear. Rate: 0.00500, Train Grad.: 3.9\n",
      "Epoch 21401/150000, Train Loss: 235, Val Loss: 497,  Lear. Rate: 0.00500, Train Grad.: 3.8\n",
      "Epoch 21501/150000, Train Loss: 227, Val Loss: 493,  Lear. Rate: 0.00500, Train Grad.: 3.6\n",
      "Epoch 21601/150000, Train Loss: 220, Val Loss: 489,  Lear. Rate: 0.00500, Train Grad.: 3.5\n",
      "Epoch 21701/150000, Train Loss: 214, Val Loss: 485,  Lear. Rate: 0.00500, Train Grad.: 3.3\n",
      "Epoch 21801/150000, Train Loss: 207, Val Loss: 482,  Lear. Rate: 0.00500, Train Grad.: 2.9\n",
      "Epoch 21901/150000, Train Loss: 201, Val Loss: 478,  Lear. Rate: 0.00500, Train Grad.: 3.1\n",
      "Epoch 22001/150000, Train Loss: 196, Val Loss: 474,  Lear. Rate: 0.00500, Train Grad.: 3.0\n",
      "Epoch 22101/150000, Train Loss: 190, Val Loss: 471,  Lear. Rate: 0.00500, Train Grad.: 1.7\n",
      "Epoch 22201/150000, Train Loss: 185, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 2.8\n",
      "Epoch 22301/150000, Train Loss: 179, Val Loss: 455,  Lear. Rate: 0.00500, Train Grad.: 2.7\n",
      "Epoch 22401/150000, Train Loss: 174, Val Loss: 446,  Lear. Rate: 0.00500, Train Grad.: 2.7\n",
      "Epoch 22501/150000, Train Loss: 169, Val Loss: 442,  Lear. Rate: 0.00500, Train Grad.: 2.6\n",
      "Epoch 22601/150000, Train Loss: 164, Val Loss: 438,  Lear. Rate: 0.00500, Train Grad.: 2.5\n",
      "Epoch 22701/150000, Train Loss: 160, Val Loss: 435,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 22801/150000, Train Loss: 155, Val Loss: 432,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 22901/150000, Train Loss: 151, Val Loss: 430,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 23001/150000, Train Loss: 147, Val Loss: 427,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 23101/150000, Train Loss: 143, Val Loss: 425,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 23201/150000, Train Loss: 140, Val Loss: 423,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 23301/150000, Train Loss: 136, Val Loss: 420,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 23401/150000, Train Loss: 134, Val Loss: 417,  Lear. Rate: 0.00500, Train Grad.: 3.7\n",
      "Epoch 23501/150000, Train Loss: 130, Val Loss: 416,  Lear. Rate: 0.00500, Train Grad.: 1.7\n",
      "Epoch 23601/150000, Train Loss: 127, Val Loss: 413,  Lear. Rate: 0.00500, Train Grad.: 1.7\n",
      "Epoch 23701/150000, Train Loss: 124, Val Loss: 411,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 23801/150000, Train Loss: 121, Val Loss: 409,  Lear. Rate: 0.00500, Train Grad.: 1.6\n",
      "Epoch 23901/150000, Train Loss: 118, Val Loss: 406,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 24001/150000, Train Loss: 115, Val Loss: 406,  Lear. Rate: 0.00500, Train Grad.: 1.5\n",
      "Epoch 24101/150000, Train Loss: 113, Val Loss: 405,  Lear. Rate: 0.00500, Train Grad.: 1.4\n",
      "Epoch 24201/150000, Train Loss: 111, Val Loss: 404,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 24301/150000, Train Loss: 108, Val Loss: 402,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
      "Epoch 24401/150000, Train Loss: 106, Val Loss: 400,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 24501/150000, Train Loss: 104, Val Loss: 400,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 24601/150000, Train Loss: 102, Val Loss: 399,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 24701/150000, Train Loss: 100, Val Loss: 399,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 24801/150000, Train Loss: 98, Val Loss: 398,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 24901/150000, Train Loss: 97, Val Loss: 397,  Lear. Rate: 0.00500, Train Grad.: 0.6\n",
      "Epoch 25001/150000, Train Loss: 95, Val Loss: 397,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 25101/150000, Train Loss: 94, Val Loss: 396,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 25201/150000, Train Loss: 92, Val Loss: 396,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 25301/150000, Train Loss: 90, Val Loss: 397,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 25401/150000, Train Loss: 90, Val Loss: 399,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 25501/150000, Train Loss: 87, Val Loss: 397,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
      "Early stopping at epoch 25536 with validation loss 400.9601135253906.\n",
      "Test Loss: 386.94378662109375\n",
      "Hyperparameters: input_size=7, hidden_size=5, num_layers=3, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 29981, Val Loss: 30474,  Lear. Rate: 0.00500, Train Grad.: 224.2\n",
      "Epoch 101/150000, Train Loss: 29103, Val Loss: 29584,  Lear. Rate: 0.00453, Train Grad.: 216.2\n",
      "Epoch 201/150000, Train Loss: 28394, Val Loss: 28869,  Lear. Rate: 0.00456, Train Grad.: 209.6\n",
      "Epoch 301/150000, Train Loss: 27784, Val Loss: 28252,  Lear. Rate: 0.00457, Train Grad.: 203.7\n",
      "Epoch 401/150000, Train Loss: 27218, Val Loss: 27679,  Lear. Rate: 0.00459, Train Grad.: 198.0\n",
      "Epoch 501/150000, Train Loss: 26681, Val Loss: 27136,  Lear. Rate: 0.00461, Train Grad.: 192.5\n",
      "Epoch 601/150000, Train Loss: 26171, Val Loss: 26618,  Lear. Rate: 0.00462, Train Grad.: 187.2\n",
      "Epoch 701/150000, Train Loss: 25683, Val Loss: 26124,  Lear. Rate: 0.00463, Train Grad.: 181.9\n",
      "Epoch 801/150000, Train Loss: 25215, Val Loss: 25650,  Lear. Rate: 0.00465, Train Grad.: 176.7\n",
      "Epoch 901/150000, Train Loss: 24768, Val Loss: 25196,  Lear. Rate: 0.00466, Train Grad.: 171.5\n",
      "Epoch 1001/150000, Train Loss: 24339, Val Loss: 24761,  Lear. Rate: 0.00467, Train Grad.: 166.4\n",
      "Epoch 1101/150000, Train Loss: 23928, Val Loss: 24344,  Lear. Rate: 0.00468, Train Grad.: 161.4\n",
      "Epoch 1201/150000, Train Loss: 23534, Val Loss: 23945,  Lear. Rate: 0.00469, Train Grad.: 156.5\n",
      "Epoch 1301/150000, Train Loss: 23157, Val Loss: 23562,  Lear. Rate: 0.00470, Train Grad.: 151.6\n",
      "Epoch 1401/150000, Train Loss: 22797, Val Loss: 23196,  Lear. Rate: 0.00471, Train Grad.: 146.7\n",
      "Epoch 1501/150000, Train Loss: 22452, Val Loss: 22845,  Lear. Rate: 0.00472, Train Grad.: 142.0\n",
      "Epoch 1601/150000, Train Loss: 22123, Val Loss: 22510,  Lear. Rate: 0.00473, Train Grad.: 137.2\n",
      "Epoch 1701/150000, Train Loss: 21809, Val Loss: 22190,  Lear. Rate: 0.00473, Train Grad.: 132.6\n",
      "Epoch 1801/150000, Train Loss: 21509, Val Loss: 21884,  Lear. Rate: 0.00474, Train Grad.: 128.0\n",
      "Epoch 1901/150000, Train Loss: 21223, Val Loss: 21593,  Lear. Rate: 0.00475, Train Grad.: 123.4\n",
      "Epoch 2001/150000, Train Loss: 20951, Val Loss: 21316,  Lear. Rate: 0.00475, Train Grad.: 119.0\n",
      "Epoch 2101/150000, Train Loss: 20693, Val Loss: 21052,  Lear. Rate: 0.00476, Train Grad.: 114.5\n",
      "Epoch 2201/150000, Train Loss: 20448, Val Loss: 20802,  Lear. Rate: 0.00477, Train Grad.: 110.2\n",
      "Epoch 2301/150000, Train Loss: 20215, Val Loss: 20564,  Lear. Rate: 0.00477, Train Grad.: 105.9\n",
      "Epoch 2401/150000, Train Loss: 19995, Val Loss: 20338,  Lear. Rate: 0.00478, Train Grad.: 101.6\n",
      "Epoch 2501/150000, Train Loss: 19787, Val Loss: 20125,  Lear. Rate: 0.00478, Train Grad.: 97.4\n",
      "Epoch 2601/150000, Train Loss: 19591, Val Loss: 19924,  Lear. Rate: 0.00479, Train Grad.: 93.3\n",
      "Epoch 2701/150000, Train Loss: 19406, Val Loss: 19734,  Lear. Rate: 0.00479, Train Grad.: 89.3\n",
      "Epoch 2801/150000, Train Loss: 19232, Val Loss: 19555,  Lear. Rate: 0.00479, Train Grad.: 85.3\n",
      "Epoch 2901/150000, Train Loss: 19068, Val Loss: 19387,  Lear. Rate: 0.00480, Train Grad.: 81.4\n",
      "Epoch 3001/150000, Train Loss: 18915, Val Loss: 19229,  Lear. Rate: 0.00480, Train Grad.: 77.5\n",
      "Epoch 3101/150000, Train Loss: 18773, Val Loss: 19082,  Lear. Rate: 0.00480, Train Grad.: 73.7\n",
      "Epoch 3201/150000, Train Loss: 18639, Val Loss: 18944,  Lear. Rate: 0.00481, Train Grad.: 70.0\n",
      "Epoch 3301/150000, Train Loss: 18516, Val Loss: 18816,  Lear. Rate: 0.00481, Train Grad.: 66.4\n",
      "Epoch 3401/150000, Train Loss: 18401, Val Loss: 18697,  Lear. Rate: 0.00481, Train Grad.: 62.8\n",
      "Epoch 3501/150000, Train Loss: 18295, Val Loss: 18586,  Lear. Rate: 0.00481, Train Grad.: 59.4\n",
      "Epoch 3601/150000, Train Loss: 18197, Val Loss: 18484,  Lear. Rate: 0.00481, Train Grad.: 56.0\n",
      "Epoch 3701/150000, Train Loss: 18107, Val Loss: 18390,  Lear. Rate: 0.00482, Train Grad.: 52.7\n",
      "Epoch 3801/150000, Train Loss: 18025, Val Loss: 18304,  Lear. Rate: 0.00482, Train Grad.: 49.4\n",
      "Epoch 3901/150000, Train Loss: 17950, Val Loss: 18225,  Lear. Rate: 0.00482, Train Grad.: 46.3\n",
      "Epoch 4001/150000, Train Loss: 17881, Val Loss: 18153,  Lear. Rate: 0.00482, Train Grad.: 43.3\n",
      "Epoch 4101/150000, Train Loss: 17820, Val Loss: 18088,  Lear. Rate: 0.00482, Train Grad.: 40.3\n",
      "Epoch 4201/150000, Train Loss: 17764, Val Loss: 18029,  Lear. Rate: 0.00482, Train Grad.: 37.5\n",
      "Epoch 4301/150000, Train Loss: 17715, Val Loss: 17976,  Lear. Rate: 0.00482, Train Grad.: 34.7\n",
      "Epoch 4401/150000, Train Loss: 17670, Val Loss: 17928,  Lear. Rate: 0.00483, Train Grad.: 32.1\n",
      "Epoch 4501/150000, Train Loss: 17631, Val Loss: 17886,  Lear. Rate: 0.00483, Train Grad.: 29.5\n",
      "Epoch 4601/150000, Train Loss: 17597, Val Loss: 17848,  Lear. Rate: 0.00483, Train Grad.: 27.1\n",
      "Epoch 4701/150000, Train Loss: 17566, Val Loss: 17815,  Lear. Rate: 0.00483, Train Grad.: 24.7\n",
      "Epoch 4801/150000, Train Loss: 17540, Val Loss: 17786,  Lear. Rate: 0.00483, Train Grad.: 22.5\n",
      "Epoch 4901/150000, Train Loss: 17518, Val Loss: 17761,  Lear. Rate: 0.00483, Train Grad.: 20.4\n",
      "Epoch 5001/150000, Train Loss: 17498, Val Loss: 17739,  Lear. Rate: 0.00483, Train Grad.: 18.4\n",
      "Epoch 5101/150000, Train Loss: 17482, Val Loss: 17720,  Lear. Rate: 0.00483, Train Grad.: 16.5\n",
      "Epoch 5201/150000, Train Loss: 17468, Val Loss: 17705,  Lear. Rate: 0.00483, Train Grad.: 14.8\n",
      "Epoch 5301/150000, Train Loss: 17457, Val Loss: 17691,  Lear. Rate: 0.00483, Train Grad.: 13.1\n",
      "Epoch 5401/150000, Train Loss: 17447, Val Loss: 17680,  Lear. Rate: 0.00483, Train Grad.: 11.6\n",
      "Epoch 5501/150000, Train Loss: 17439, Val Loss: 17670,  Lear. Rate: 0.00483, Train Grad.: 10.2\n",
      "Epoch 5601/150000, Train Loss: 17433, Val Loss: 17662,  Lear. Rate: 0.00483, Train Grad.: 8.9\n",
      "Epoch 5701/150000, Train Loss: 17428, Val Loss: 17656,  Lear. Rate: 0.00483, Train Grad.: 7.7\n",
      "Epoch 5801/150000, Train Loss: 17425, Val Loss: 17651,  Lear. Rate: 0.00483, Train Grad.: 6.6\n",
      "Epoch 5901/150000, Train Loss: 17422, Val Loss: 17647,  Lear. Rate: 0.00483, Train Grad.: 5.7\n",
      "Epoch 6001/150000, Train Loss: 17419, Val Loss: 17643,  Lear. Rate: 0.00483, Train Grad.: 4.8\n",
      "Epoch 6101/150000, Train Loss: 17418, Val Loss: 17641,  Lear. Rate: 0.00483, Train Grad.: 4.0\n",
      "Epoch 6201/150000, Train Loss: 17416, Val Loss: 17639,  Lear. Rate: 0.00483, Train Grad.: 3.4\n",
      "Epoch 6301/150000, Train Loss: 17416, Val Loss: 17637,  Lear. Rate: 0.00483, Train Grad.: 2.8\n",
      "Epoch 6401/150000, Train Loss: 17415, Val Loss: 17636,  Lear. Rate: 0.00483, Train Grad.: 2.3\n",
      "Epoch 6501/150000, Train Loss: 17414, Val Loss: 17635,  Lear. Rate: 0.00483, Train Grad.: 1.8\n",
      "Epoch 6601/150000, Train Loss: 17414, Val Loss: 17634,  Lear. Rate: 0.00483, Train Grad.: 1.5\n",
      "Epoch 6701/150000, Train Loss: 17414, Val Loss: 17633,  Lear. Rate: 0.00483, Train Grad.: 1.1\n",
      "Epoch 6801/150000, Train Loss: 17414, Val Loss: 17633,  Lear. Rate: 0.00483, Train Grad.: 0.9\n",
      "Epoch 6901/150000, Train Loss: 17414, Val Loss: 17633,  Lear. Rate: 0.00483, Train Grad.: 0.7\n",
      "Epoch 7001/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.5\n",
      "Epoch 7101/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.4\n",
      "Epoch 7201/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.3\n",
      "Epoch 7301/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.2\n",
      "Epoch 7401/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.1\n",
      "Epoch 7501/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.1\n",
      "Epoch 7601/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.1\n",
      "Epoch 7701/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 7801/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 7901/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8001/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8101/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8201/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8301/150000, Train Loss: 13975, Val Loss: 14388,  Lear. Rate: 0.00489, Train Grad.: 98.0\n",
      "Epoch 8401/150000, Train Loss: 12804, Val Loss: 13314,  Lear. Rate: 0.00490, Train Grad.: 83.0\n",
      "Early stopping at epoch 8491 with validation loss 12860.1796875.\n",
      "Test Loss: 12687.048828125\n",
      "Hyperparameters: input_size=7, hidden_size=5, num_layers=4, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 30069, Val Loss: 30565,  Lear. Rate: 0.00500, Train Grad.: 225.0\n",
      "Epoch 101/150000, Train Loss: 29299, Val Loss: 29783,  Lear. Rate: 0.00453, Train Grad.: 218.0\n",
      "Epoch 201/150000, Train Loss: 28590, Val Loss: 29066,  Lear. Rate: 0.00455, Train Grad.: 211.4\n",
      "Epoch 301/150000, Train Loss: 27978, Val Loss: 28448,  Lear. Rate: 0.00457, Train Grad.: 205.6\n",
      "Epoch 401/150000, Train Loss: 27408, Val Loss: 27872,  Lear. Rate: 0.00459, Train Grad.: 199.9\n",
      "Epoch 501/150000, Train Loss: 26869, Val Loss: 27326,  Lear. Rate: 0.00460, Train Grad.: 194.5\n",
      "Epoch 601/150000, Train Loss: 26354, Val Loss: 26805,  Lear. Rate: 0.00462, Train Grad.: 189.1\n",
      "Epoch 701/150000, Train Loss: 25862, Val Loss: 26306,  Lear. Rate: 0.00463, Train Grad.: 183.8\n",
      "Epoch 801/150000, Train Loss: 25390, Val Loss: 25828,  Lear. Rate: 0.00464, Train Grad.: 178.6\n",
      "Epoch 901/150000, Train Loss: 24938, Val Loss: 25369,  Lear. Rate: 0.00466, Train Grad.: 173.5\n",
      "Epoch 1001/150000, Train Loss: 24505, Val Loss: 24930,  Lear. Rate: 0.00467, Train Grad.: 168.4\n",
      "Epoch 1101/150000, Train Loss: 24089, Val Loss: 24508,  Lear. Rate: 0.00468, Train Grad.: 163.4\n",
      "Epoch 1201/150000, Train Loss: 23690, Val Loss: 24103,  Lear. Rate: 0.00469, Train Grad.: 158.5\n",
      "Epoch 1301/150000, Train Loss: 23309, Val Loss: 23716,  Lear. Rate: 0.00470, Train Grad.: 153.6\n",
      "Epoch 1401/150000, Train Loss: 22943, Val Loss: 23344,  Lear. Rate: 0.00471, Train Grad.: 148.7\n",
      "Epoch 1501/150000, Train Loss: 22594, Val Loss: 22989,  Lear. Rate: 0.00472, Train Grad.: 143.9\n",
      "Epoch 1601/150000, Train Loss: 22259, Val Loss: 22649,  Lear. Rate: 0.00472, Train Grad.: 139.2\n",
      "Epoch 1701/150000, Train Loss: 21940, Val Loss: 22323,  Lear. Rate: 0.00473, Train Grad.: 134.6\n",
      "Epoch 1801/150000, Train Loss: 21635, Val Loss: 22013,  Lear. Rate: 0.00474, Train Grad.: 129.9\n",
      "Epoch 1901/150000, Train Loss: 21344, Val Loss: 21717,  Lear. Rate: 0.00475, Train Grad.: 125.4\n",
      "Epoch 2001/150000, Train Loss: 21067, Val Loss: 21434,  Lear. Rate: 0.00475, Train Grad.: 120.9\n",
      "Epoch 2101/150000, Train Loss: 20804, Val Loss: 21166,  Lear. Rate: 0.00476, Train Grad.: 116.5\n",
      "Epoch 2201/150000, Train Loss: 20554, Val Loss: 20910,  Lear. Rate: 0.00476, Train Grad.: 112.1\n",
      "Epoch 2301/150000, Train Loss: 20316, Val Loss: 20667,  Lear. Rate: 0.00477, Train Grad.: 107.8\n",
      "Epoch 2401/150000, Train Loss: 20091, Val Loss: 20437,  Lear. Rate: 0.00477, Train Grad.: 103.5\n",
      "Epoch 2501/150000, Train Loss: 19879, Val Loss: 20219,  Lear. Rate: 0.00478, Train Grad.: 99.3\n",
      "Epoch 2601/150000, Train Loss: 19678, Val Loss: 20013,  Lear. Rate: 0.00478, Train Grad.: 95.2\n",
      "Epoch 2701/150000, Train Loss: 19488, Val Loss: 19818,  Lear. Rate: 0.00479, Train Grad.: 91.1\n",
      "Epoch 2801/150000, Train Loss: 19309, Val Loss: 19635,  Lear. Rate: 0.00479, Train Grad.: 87.1\n",
      "Epoch 2901/150000, Train Loss: 19142, Val Loss: 19462,  Lear. Rate: 0.00480, Train Grad.: 83.1\n",
      "Epoch 3001/150000, Train Loss: 18984, Val Loss: 19300,  Lear. Rate: 0.00480, Train Grad.: 79.3\n",
      "Epoch 3101/150000, Train Loss: 18837, Val Loss: 19149,  Lear. Rate: 0.00480, Train Grad.: 75.5\n",
      "Epoch 3201/150000, Train Loss: 18700, Val Loss: 19007,  Lear. Rate: 0.00480, Train Grad.: 71.7\n",
      "Epoch 3301/150000, Train Loss: 18572, Val Loss: 18874,  Lear. Rate: 0.00481, Train Grad.: 68.1\n",
      "Epoch 3401/150000, Train Loss: 18453, Val Loss: 18751,  Lear. Rate: 0.00481, Train Grad.: 64.5\n",
      "Epoch 3501/150000, Train Loss: 18343, Val Loss: 18637,  Lear. Rate: 0.00481, Train Grad.: 61.0\n",
      "Epoch 3601/150000, Train Loss: 18242, Val Loss: 18531,  Lear. Rate: 0.00481, Train Grad.: 57.6\n",
      "Epoch 3701/150000, Train Loss: 18148, Val Loss: 18434,  Lear. Rate: 0.00482, Train Grad.: 54.2\n",
      "Epoch 3801/150000, Train Loss: 18063, Val Loss: 18344,  Lear. Rate: 0.00482, Train Grad.: 51.0\n",
      "Epoch 3901/150000, Train Loss: 17984, Val Loss: 18262,  Lear. Rate: 0.00482, Train Grad.: 47.8\n",
      "Epoch 4001/150000, Train Loss: 17913, Val Loss: 18187,  Lear. Rate: 0.00482, Train Grad.: 44.7\n",
      "Epoch 4101/150000, Train Loss: 17848, Val Loss: 18118,  Lear. Rate: 0.00482, Train Grad.: 41.7\n",
      "Epoch 4201/150000, Train Loss: 17790, Val Loss: 18056,  Lear. Rate: 0.00482, Train Grad.: 38.8\n",
      "Epoch 4301/150000, Train Loss: 17738, Val Loss: 18001,  Lear. Rate: 0.00482, Train Grad.: 36.0\n",
      "Epoch 4401/150000, Train Loss: 17691, Val Loss: 17950,  Lear. Rate: 0.00483, Train Grad.: 33.3\n",
      "Epoch 4501/150000, Train Loss: 17650, Val Loss: 17906,  Lear. Rate: 0.00483, Train Grad.: 30.7\n",
      "Epoch 4601/150000, Train Loss: 17613, Val Loss: 17866,  Lear. Rate: 0.00483, Train Grad.: 28.2\n",
      "Epoch 4701/150000, Train Loss: 17581, Val Loss: 17831,  Lear. Rate: 0.00483, Train Grad.: 25.8\n",
      "Epoch 4801/150000, Train Loss: 17552, Val Loss: 17800,  Lear. Rate: 0.00483, Train Grad.: 23.6\n",
      "Epoch 4901/150000, Train Loss: 17528, Val Loss: 17773,  Lear. Rate: 0.00483, Train Grad.: 21.4\n",
      "Epoch 5001/150000, Train Loss: 17507, Val Loss: 17749,  Lear. Rate: 0.00483, Train Grad.: 19.4\n",
      "Epoch 5101/150000, Train Loss: 17490, Val Loss: 17729,  Lear. Rate: 0.00483, Train Grad.: 17.4\n",
      "Epoch 5201/150000, Train Loss: 17475, Val Loss: 17712,  Lear. Rate: 0.00483, Train Grad.: 15.6\n",
      "Epoch 5301/150000, Train Loss: 17462, Val Loss: 17697,  Lear. Rate: 0.00483, Train Grad.: 13.9\n",
      "Epoch 5401/150000, Train Loss: 17452, Val Loss: 17685,  Lear. Rate: 0.00483, Train Grad.: 12.3\n",
      "Epoch 5501/150000, Train Loss: 17443, Val Loss: 17675,  Lear. Rate: 0.00483, Train Grad.: 10.9\n",
      "Epoch 5601/150000, Train Loss: 17436, Val Loss: 17666,  Lear. Rate: 0.00483, Train Grad.: 9.5\n",
      "Epoch 5701/150000, Train Loss: 17431, Val Loss: 17659,  Lear. Rate: 0.00483, Train Grad.: 8.3\n",
      "Epoch 5801/150000, Train Loss: 17426, Val Loss: 17653,  Lear. Rate: 0.00483, Train Grad.: 7.1\n",
      "Epoch 5901/150000, Train Loss: 17423, Val Loss: 17649,  Lear. Rate: 0.00483, Train Grad.: 6.1\n",
      "Epoch 6001/150000, Train Loss: 17420, Val Loss: 17645,  Lear. Rate: 0.00483, Train Grad.: 5.2\n",
      "Epoch 6101/150000, Train Loss: 17418, Val Loss: 17642,  Lear. Rate: 0.00483, Train Grad.: 4.4\n",
      "Epoch 6201/150000, Train Loss: 17417, Val Loss: 17640,  Lear. Rate: 0.00483, Train Grad.: 3.7\n",
      "Epoch 6301/150000, Train Loss: 17416, Val Loss: 17638,  Lear. Rate: 0.00483, Train Grad.: 3.0\n",
      "Epoch 6401/150000, Train Loss: 17415, Val Loss: 17636,  Lear. Rate: 0.00483, Train Grad.: 2.5\n",
      "Epoch 6501/150000, Train Loss: 17415, Val Loss: 17635,  Lear. Rate: 0.00483, Train Grad.: 2.0\n",
      "Epoch 6601/150000, Train Loss: 17414, Val Loss: 17634,  Lear. Rate: 0.00483, Train Grad.: 1.6\n",
      "Epoch 6701/150000, Train Loss: 17414, Val Loss: 17634,  Lear. Rate: 0.00483, Train Grad.: 1.3\n",
      "Epoch 6801/150000, Train Loss: 17414, Val Loss: 17633,  Lear. Rate: 0.00483, Train Grad.: 1.0\n",
      "Epoch 6901/150000, Train Loss: 17414, Val Loss: 17633,  Lear. Rate: 0.00483, Train Grad.: 0.8\n",
      "Epoch 7001/150000, Train Loss: 17414, Val Loss: 17633,  Lear. Rate: 0.00483, Train Grad.: 0.6\n",
      "Epoch 7101/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.5\n",
      "Epoch 7201/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.3\n",
      "Epoch 7301/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.2\n",
      "Epoch 7401/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.2\n",
      "Epoch 7501/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.1\n",
      "Epoch 7601/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.1\n",
      "Epoch 7701/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.1\n",
      "Epoch 7801/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 7901/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8001/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8101/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8201/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8301/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8401/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8501/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8601/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8701/150000, Train Loss: 17414, Val Loss: 17632,  Lear. Rate: 0.00483, Train Grad.: 0.0\n",
      "Epoch 8801/150000, Train Loss: 13518, Val Loss: 14082,  Lear. Rate: 0.00489, Train Grad.: 86.3\n",
      "Epoch 8901/150000, Train Loss: 12516, Val Loss: 12992,  Lear. Rate: 0.00491, Train Grad.: 82.8\n",
      "Epoch 9001/150000, Train Loss: 11949, Val Loss: 12298,  Lear. Rate: 0.00492, Train Grad.: 79.7\n",
      "Epoch 9101/150000, Train Loss: 11513, Val Loss: 11822,  Lear. Rate: 0.00492, Train Grad.: 76.6\n",
      "Epoch 9201/150000, Train Loss: 11148, Val Loss: 11447,  Lear. Rate: 0.00493, Train Grad.: 73.7\n",
      "Epoch 9301/150000, Train Loss: 10842, Val Loss: 11157,  Lear. Rate: 0.00493, Train Grad.: 65.7\n",
      "Epoch 9401/150000, Train Loss: 10541, Val Loss: 10835,  Lear. Rate: 0.00494, Train Grad.: 69.8\n",
      "Epoch 9501/150000, Train Loss: 10278, Val Loss: 10573,  Lear. Rate: 0.00494, Train Grad.: 68.1\n",
      "Epoch 9601/150000, Train Loss: 10026, Val Loss: 10298,  Lear. Rate: 0.00494, Train Grad.: 66.6\n",
      "Epoch 9701/150000, Train Loss: 9792, Val Loss: 10047,  Lear. Rate: 0.00494, Train Grad.: 65.5\n",
      "Epoch 9801/150000, Train Loss: 9574, Val Loss: 9823,  Lear. Rate: 0.00495, Train Grad.: 63.9\n",
      "Epoch 9901/150000, Train Loss: 9367, Val Loss: 9607,  Lear. Rate: 0.00495, Train Grad.: 62.9\n",
      "Epoch 10001/150000, Train Loss: 9169, Val Loss: 9405,  Lear. Rate: 0.00495, Train Grad.: 61.7\n",
      "Epoch 10101/150000, Train Loss: 8979, Val Loss: 9209,  Lear. Rate: 0.00495, Train Grad.: 60.8\n",
      "Epoch 10201/150000, Train Loss: 8797, Val Loss: 9022,  Lear. Rate: 0.00496, Train Grad.: 59.6\n",
      "Epoch 10301/150000, Train Loss: 8621, Val Loss: 8844,  Lear. Rate: 0.00496, Train Grad.: 60.3\n",
      "Epoch 10401/150000, Train Loss: 8450, Val Loss: 8670,  Lear. Rate: 0.00496, Train Grad.: 57.7\n",
      "Epoch 10501/150000, Train Loss: 8284, Val Loss: 8502,  Lear. Rate: 0.00496, Train Grad.: 56.8\n",
      "Epoch 10601/150000, Train Loss: 8124, Val Loss: 8341,  Lear. Rate: 0.00496, Train Grad.: 54.5\n",
      "Epoch 10701/150000, Train Loss: 7966, Val Loss: 8181,  Lear. Rate: 0.00496, Train Grad.: 55.0\n",
      "Epoch 10801/150000, Train Loss: 7813, Val Loss: 8027,  Lear. Rate: 0.00496, Train Grad.: 54.2\n",
      "Epoch 10901/150000, Train Loss: 7663, Val Loss: 7876,  Lear. Rate: 0.00497, Train Grad.: 53.7\n",
      "Epoch 11001/150000, Train Loss: 7517, Val Loss: 7729,  Lear. Rate: 0.00497, Train Grad.: 52.6\n",
      "Epoch 11101/150000, Train Loss: 7374, Val Loss: 7584,  Lear. Rate: 0.00497, Train Grad.: 51.9\n",
      "Epoch 11201/150000, Train Loss: 7235, Val Loss: 7445,  Lear. Rate: 0.00497, Train Grad.: 48.9\n",
      "Epoch 11301/150000, Train Loss: 7093, Val Loss: 7287,  Lear. Rate: 0.00497, Train Grad.: 50.5\n",
      "Epoch 11401/150000, Train Loss: 6958, Val Loss: 7149,  Lear. Rate: 0.00497, Train Grad.: 49.8\n",
      "Epoch 11501/150000, Train Loss: 6825, Val Loss: 7015,  Lear. Rate: 0.00497, Train Grad.: 48.4\n",
      "Epoch 11601/150000, Train Loss: 6694, Val Loss: 6883,  Lear. Rate: 0.00497, Train Grad.: 48.4\n",
      "Epoch 11701/150000, Train Loss: 6566, Val Loss: 6753,  Lear. Rate: 0.00498, Train Grad.: 47.8\n",
      "Epoch 11801/150000, Train Loss: 6440, Val Loss: 6626,  Lear. Rate: 0.00498, Train Grad.: 48.3\n",
      "Epoch 11901/150000, Train Loss: 6316, Val Loss: 6500,  Lear. Rate: 0.00498, Train Grad.: 46.5\n",
      "Epoch 12001/150000, Train Loss: 6194, Val Loss: 6376,  Lear. Rate: 0.00498, Train Grad.: 45.8\n",
      "Epoch 12101/150000, Train Loss: 6075, Val Loss: 6255,  Lear. Rate: 0.00498, Train Grad.: 46.8\n",
      "Epoch 12201/150000, Train Loss: 5956, Val Loss: 6135,  Lear. Rate: 0.00498, Train Grad.: 44.6\n",
      "Epoch 12301/150000, Train Loss: 5840, Val Loss: 6018,  Lear. Rate: 0.00498, Train Grad.: 43.9\n",
      "Epoch 12401/150000, Train Loss: 5728, Val Loss: 5903,  Lear. Rate: 0.00498, Train Grad.: 46.1\n",
      "Epoch 12501/150000, Train Loss: 5613, Val Loss: 5789,  Lear. Rate: 0.00498, Train Grad.: 42.7\n",
      "Epoch 12601/150000, Train Loss: 5503, Val Loss: 5678,  Lear. Rate: 0.00498, Train Grad.: 42.1\n",
      "Epoch 12701/150000, Train Loss: 5395, Val Loss: 5568,  Lear. Rate: 0.00498, Train Grad.: 43.1\n",
      "Epoch 12801/150000, Train Loss: 5288, Val Loss: 5460,  Lear. Rate: 0.00498, Train Grad.: 40.9\n",
      "Epoch 12901/150000, Train Loss: 5183, Val Loss: 5354,  Lear. Rate: 0.00498, Train Grad.: 40.3\n",
      "Epoch 13001/150000, Train Loss: 5080, Val Loss: 5251,  Lear. Rate: 0.00498, Train Grad.: 41.5\n",
      "Epoch 13101/150000, Train Loss: 4978, Val Loss: 5148,  Lear. Rate: 0.00499, Train Grad.: 39.1\n",
      "Epoch 13201/150000, Train Loss: 4878, Val Loss: 5047,  Lear. Rate: 0.00499, Train Grad.: 38.6\n",
      "Epoch 13301/150000, Train Loss: 4779, Val Loss: 4954,  Lear. Rate: 0.00499, Train Grad.: 37.5\n",
      "Epoch 13401/150000, Train Loss: 4682, Val Loss: 4851,  Lear. Rate: 0.00499, Train Grad.: 37.5\n",
      "Epoch 13501/150000, Train Loss: 4586, Val Loss: 4755,  Lear. Rate: 0.00499, Train Grad.: 37.0\n",
      "Epoch 13601/150000, Train Loss: 4492, Val Loss: 4660,  Lear. Rate: 0.00499, Train Grad.: 36.4\n",
      "Epoch 13701/150000, Train Loss: 4400, Val Loss: 4568,  Lear. Rate: 0.00499, Train Grad.: 36.5\n",
      "Epoch 13801/150000, Train Loss: 4308, Val Loss: 4476,  Lear. Rate: 0.00499, Train Grad.: 35.4\n",
      "Epoch 13901/150000, Train Loss: 4218, Val Loss: 4386,  Lear. Rate: 0.00499, Train Grad.: 34.9\n",
      "Epoch 14001/150000, Train Loss: 4132, Val Loss: 4299,  Lear. Rate: 0.00499, Train Grad.: 36.7\n",
      "Epoch 14101/150000, Train Loss: 4043, Val Loss: 4210,  Lear. Rate: 0.00499, Train Grad.: 33.8\n",
      "Epoch 14201/150000, Train Loss: 3957, Val Loss: 4125,  Lear. Rate: 0.00499, Train Grad.: 33.3\n",
      "Epoch 14301/150000, Train Loss: 3874, Val Loss: 4040,  Lear. Rate: 0.00499, Train Grad.: 31.1\n",
      "Epoch 14401/150000, Train Loss: 3790, Val Loss: 3958,  Lear. Rate: 0.00499, Train Grad.: 32.3\n",
      "Epoch 14501/150000, Train Loss: 3709, Val Loss: 3876,  Lear. Rate: 0.00499, Train Grad.: 31.8\n",
      "Epoch 14601/150000, Train Loss: 3631, Val Loss: 3796,  Lear. Rate: 0.00499, Train Grad.: 34.0\n",
      "Epoch 14701/150000, Train Loss: 3550, Val Loss: 3718,  Lear. Rate: 0.00499, Train Grad.: 30.8\n",
      "Epoch 14801/150000, Train Loss: 3473, Val Loss: 3640,  Lear. Rate: 0.00499, Train Grad.: 30.3\n",
      "Epoch 14901/150000, Train Loss: 3399, Val Loss: 3567,  Lear. Rate: 0.00499, Train Grad.: 27.3\n",
      "Epoch 15001/150000, Train Loss: 3322, Val Loss: 3490,  Lear. Rate: 0.00499, Train Grad.: 29.4\n",
      "Epoch 15101/150000, Train Loss: 3249, Val Loss: 3416,  Lear. Rate: 0.00499, Train Grad.: 28.9\n",
      "Epoch 15201/150000, Train Loss: 3178, Val Loss: 3350,  Lear. Rate: 0.00499, Train Grad.: 26.4\n",
      "Epoch 15301/150000, Train Loss: 3106, Val Loss: 3274,  Lear. Rate: 0.00499, Train Grad.: 27.9\n",
      "Epoch 15401/150000, Train Loss: 3037, Val Loss: 3204,  Lear. Rate: 0.00499, Train Grad.: 27.5\n",
      "Epoch 15501/150000, Train Loss: 2974, Val Loss: 3139,  Lear. Rate: 0.00499, Train Grad.: 31.4\n",
      "Epoch 15601/150000, Train Loss: 2902, Val Loss: 3069,  Lear. Rate: 0.00499, Train Grad.: 26.5\n",
      "Epoch 15701/150000, Train Loss: 2836, Val Loss: 3002,  Lear. Rate: 0.00500, Train Grad.: 26.1\n",
      "Epoch 15801/150000, Train Loss: 2773, Val Loss: 2947,  Lear. Rate: 0.00500, Train Grad.: 23.4\n",
      "Epoch 15901/150000, Train Loss: 2708, Val Loss: 2874,  Lear. Rate: 0.00500, Train Grad.: 25.3\n",
      "Epoch 16001/150000, Train Loss: 2645, Val Loss: 2811,  Lear. Rate: 0.00500, Train Grad.: 24.9\n",
      "Epoch 16101/150000, Train Loss: 2584, Val Loss: 2749,  Lear. Rate: 0.00500, Train Grad.: 24.6\n",
      "Epoch 16201/150000, Train Loss: 2524, Val Loss: 2690,  Lear. Rate: 0.00500, Train Grad.: 24.0\n",
      "Epoch 16301/150000, Train Loss: 2465, Val Loss: 2631,  Lear. Rate: 0.00500, Train Grad.: 23.6\n",
      "Epoch 16401/150000, Train Loss: 2408, Val Loss: 2573,  Lear. Rate: 0.00500, Train Grad.: 24.5\n",
      "Epoch 16501/150000, Train Loss: 2350, Val Loss: 2517,  Lear. Rate: 0.00500, Train Grad.: 22.8\n",
      "Epoch 16601/150000, Train Loss: 2294, Val Loss: 2462,  Lear. Rate: 0.00500, Train Grad.: 22.4\n",
      "Epoch 16701/150000, Train Loss: 2239, Val Loss: 2408,  Lear. Rate: 0.00500, Train Grad.: 22.2\n",
      "Epoch 16801/150000, Train Loss: 2186, Val Loss: 2354,  Lear. Rate: 0.00500, Train Grad.: 21.7\n",
      "Epoch 16901/150000, Train Loss: 2133, Val Loss: 2302,  Lear. Rate: 0.00500, Train Grad.: 21.3\n",
      "Epoch 17001/150000, Train Loss: 2081, Val Loss: 2253,  Lear. Rate: 0.00500, Train Grad.: 20.2\n",
      "Epoch 17101/150000, Train Loss: 2029, Val Loss: 2199,  Lear. Rate: 0.00500, Train Grad.: 20.6\n",
      "Epoch 17201/150000, Train Loss: 1979, Val Loss: 2150,  Lear. Rate: 0.00500, Train Grad.: 20.2\n",
      "Epoch 17301/150000, Train Loss: 1930, Val Loss: 2101,  Lear. Rate: 0.00500, Train Grad.: 20.4\n",
      "Epoch 17401/150000, Train Loss: 1882, Val Loss: 2053,  Lear. Rate: 0.00500, Train Grad.: 19.5\n",
      "Epoch 17501/150000, Train Loss: 1834, Val Loss: 2007,  Lear. Rate: 0.00500, Train Grad.: 19.2\n",
      "Epoch 17601/150000, Train Loss: 1788, Val Loss: 1961,  Lear. Rate: 0.00500, Train Grad.: 18.9\n",
      "Epoch 17701/150000, Train Loss: 1743, Val Loss: 1916,  Lear. Rate: 0.00500, Train Grad.: 18.4\n",
      "Epoch 17801/150000, Train Loss: 1698, Val Loss: 1872,  Lear. Rate: 0.00500, Train Grad.: 18.1\n",
      "Epoch 17901/150000, Train Loss: 1655, Val Loss: 1829,  Lear. Rate: 0.00500, Train Grad.: 17.2\n",
      "Epoch 18001/150000, Train Loss: 1612, Val Loss: 1787,  Lear. Rate: 0.00500, Train Grad.: 17.4\n",
      "Epoch 18101/150000, Train Loss: 1570, Val Loss: 1746,  Lear. Rate: 0.00500, Train Grad.: 17.1\n",
      "Epoch 18201/150000, Train Loss: 1536, Val Loss: 1708,  Lear. Rate: 0.00500, Train Grad.: 21.5\n",
      "Epoch 18301/150000, Train Loss: 1489, Val Loss: 1667,  Lear. Rate: 0.00500, Train Grad.: 16.4\n",
      "Epoch 18401/150000, Train Loss: 1450, Val Loss: 1629,  Lear. Rate: 0.00500, Train Grad.: 16.1\n",
      "Epoch 18501/150000, Train Loss: 1412, Val Loss: 1591,  Lear. Rate: 0.00500, Train Grad.: 15.8\n",
      "Epoch 18601/150000, Train Loss: 1374, Val Loss: 1555,  Lear. Rate: 0.00500, Train Grad.: 15.6\n",
      "Epoch 18701/150000, Train Loss: 1337, Val Loss: 1520,  Lear. Rate: 0.00500, Train Grad.: 15.2\n",
      "Epoch 18801/150000, Train Loss: 1301, Val Loss: 1485,  Lear. Rate: 0.00500, Train Grad.: 14.9\n",
      "Epoch 18901/150000, Train Loss: 1266, Val Loss: 1451,  Lear. Rate: 0.00500, Train Grad.: 14.5\n",
      "Epoch 19001/150000, Train Loss: 1231, Val Loss: 1417,  Lear. Rate: 0.00500, Train Grad.: 14.3\n",
      "Epoch 19101/150000, Train Loss: 1197, Val Loss: 1385,  Lear. Rate: 0.00500, Train Grad.: 14.1\n",
      "Epoch 19201/150000, Train Loss: 1164, Val Loss: 1353,  Lear. Rate: 0.00500, Train Grad.: 13.7\n",
      "Epoch 19301/150000, Train Loss: 1131, Val Loss: 1322,  Lear. Rate: 0.00500, Train Grad.: 13.5\n",
      "Epoch 19401/150000, Train Loss: 1100, Val Loss: 1298,  Lear. Rate: 0.00500, Train Grad.: 11.3\n",
      "Epoch 19501/150000, Train Loss: 1068, Val Loss: 1262,  Lear. Rate: 0.00500, Train Grad.: 13.0\n",
      "Epoch 19601/150000, Train Loss: 1038, Val Loss: 1234,  Lear. Rate: 0.00500, Train Grad.: 12.7\n",
      "Epoch 19701/150000, Train Loss: 1008, Val Loss: 1205,  Lear. Rate: 0.00500, Train Grad.: 11.7\n",
      "Epoch 19801/150000, Train Loss: 979, Val Loss: 1178,  Lear. Rate: 0.00500, Train Grad.: 12.1\n",
      "Epoch 19901/150000, Train Loss: 951, Val Loss: 1152,  Lear. Rate: 0.00500, Train Grad.: 11.9\n",
      "Epoch 20001/150000, Train Loss: 924, Val Loss: 1128,  Lear. Rate: 0.00500, Train Grad.: 11.7\n",
      "Epoch 20101/150000, Train Loss: 897, Val Loss: 1101,  Lear. Rate: 0.00500, Train Grad.: 11.3\n",
      "Epoch 20201/150000, Train Loss: 871, Val Loss: 1077,  Lear. Rate: 0.00500, Train Grad.: 11.0\n",
      "Epoch 20301/150000, Train Loss: 846, Val Loss: 1054,  Lear. Rate: 0.00500, Train Grad.: 10.7\n",
      "Epoch 20401/150000, Train Loss: 821, Val Loss: 1030,  Lear. Rate: 0.00500, Train Grad.: 10.4\n",
      "Epoch 20501/150000, Train Loss: 797, Val Loss: 1008,  Lear. Rate: 0.00500, Train Grad.: 10.3\n",
      "Epoch 20601/150000, Train Loss: 775, Val Loss: 995,  Lear. Rate: 0.00500, Train Grad.: 7.3\n",
      "Epoch 20701/150000, Train Loss: 749, Val Loss: 966,  Lear. Rate: 0.00500, Train Grad.: 10.0\n",
      "Epoch 20801/150000, Train Loss: 726, Val Loss: 945,  Lear. Rate: 0.00500, Train Grad.: 9.8\n",
      "Epoch 20901/150000, Train Loss: 704, Val Loss: 925,  Lear. Rate: 0.00500, Train Grad.: 9.5\n",
      "Epoch 21001/150000, Train Loss: 683, Val Loss: 904,  Lear. Rate: 0.00500, Train Grad.: 10.3\n",
      "Epoch 21101/150000, Train Loss: 661, Val Loss: 886,  Lear. Rate: 0.00500, Train Grad.: 9.1\n",
      "Epoch 21201/150000, Train Loss: 642, Val Loss: 866,  Lear. Rate: 0.00500, Train Grad.: 10.8\n",
      "Epoch 21301/150000, Train Loss: 620, Val Loss: 848,  Lear. Rate: 0.00500, Train Grad.: 9.3\n",
      "Epoch 21401/150000, Train Loss: 601, Val Loss: 830,  Lear. Rate: 0.00500, Train Grad.: 9.6\n",
      "Epoch 21501/150000, Train Loss: 582, Val Loss: 817,  Lear. Rate: 0.00500, Train Grad.: 6.9\n",
      "Epoch 21601/150000, Train Loss: 564, Val Loss: 799,  Lear. Rate: 0.00500, Train Grad.: 6.6\n",
      "Epoch 21701/150000, Train Loss: 545, Val Loss: 779,  Lear. Rate: 0.00500, Train Grad.: 7.7\n",
      "Epoch 21801/150000, Train Loss: 528, Val Loss: 764,  Lear. Rate: 0.00500, Train Grad.: 7.4\n",
      "Epoch 21901/150000, Train Loss: 511, Val Loss: 748,  Lear. Rate: 0.00500, Train Grad.: 7.4\n",
      "Epoch 22001/150000, Train Loss: 494, Val Loss: 734,  Lear. Rate: 0.00500, Train Grad.: 7.3\n",
      "Epoch 22101/150000, Train Loss: 478, Val Loss: 722,  Lear. Rate: 0.00500, Train Grad.: 6.4\n",
      "Epoch 22201/150000, Train Loss: 463, Val Loss: 709,  Lear. Rate: 0.00500, Train Grad.: 6.9\n",
      "Epoch 22301/150000, Train Loss: 448, Val Loss: 698,  Lear. Rate: 0.00500, Train Grad.: 6.3\n",
      "Epoch 22401/150000, Train Loss: 435, Val Loss: 681,  Lear. Rate: 0.00500, Train Grad.: 9.1\n",
      "Epoch 22501/150000, Train Loss: 419, Val Loss: 669,  Lear. Rate: 0.00500, Train Grad.: 7.5\n",
      "Epoch 22601/150000, Train Loss: 405, Val Loss: 663,  Lear. Rate: 0.00500, Train Grad.: 6.1\n",
      "Early stopping at epoch 22616 with validation loss 660.6995849609375.\n",
      "Test Loss: 643.0474243164062\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np  # Added this line\n",
    "\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [5]\n",
    "num_layers_list = [1,2,3,4]\n",
    "\n",
    "learning_rates = [0.005]\n",
    "\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "factors = [0.5]\n",
    "patience_lr = [10]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "def set_random_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    set_random_seeds(42)\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Lear. Rate: {current_lr:.5f}, Train Grad.: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=15, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150000, Train Loss: 30070, Val Loss: 30473,  Lear. Rate: 0.00500, Train Grad.: 225.0\n",
      "Epoch 101/150000, Train Loss: 29383, Val Loss: 29776,  Lear. Rate: 0.00453, Train Grad.: 218.9\n",
      "Epoch 201/150000, Train Loss: 28777, Val Loss: 29166,  Lear. Rate: 0.00455, Train Grad.: 213.3\n",
      "Epoch 301/150000, Train Loss: 28254, Val Loss: 28639,  Lear. Rate: 0.00456, Train Grad.: 208.3\n",
      "Epoch 401/150000, Train Loss: 27765, Val Loss: 28146,  Lear. Rate: 0.00458, Train Grad.: 203.6\n",
      "Epoch 501/150000, Train Loss: 27300, Val Loss: 27676,  Lear. Rate: 0.00459, Train Grad.: 198.9\n",
      "Epoch 601/150000, Train Loss: 26855, Val Loss: 27226,  Lear. Rate: 0.00460, Train Grad.: 194.4\n",
      "Epoch 701/150000, Train Loss: 26426, Val Loss: 26793,  Lear. Rate: 0.00462, Train Grad.: 189.9\n",
      "Epoch 801/150000, Train Loss: 26012, Val Loss: 26375,  Lear. Rate: 0.00463, Train Grad.: 185.5\n",
      "Epoch 901/150000, Train Loss: 25612, Val Loss: 25971,  Lear. Rate: 0.00464, Train Grad.: 181.2\n",
      "Epoch 1001/150000, Train Loss: 25226, Val Loss: 25581,  Lear. Rate: 0.00465, Train Grad.: 176.9\n",
      "Epoch 1101/150000, Train Loss: 24853, Val Loss: 25204,  Lear. Rate: 0.00466, Train Grad.: 172.6\n",
      "Epoch 1201/150000, Train Loss: 24493, Val Loss: 24840,  Lear. Rate: 0.00467, Train Grad.: 168.4\n",
      "Epoch 1301/150000, Train Loss: 24144, Val Loss: 24488,  Lear. Rate: 0.00468, Train Grad.: 164.2\n",
      "Epoch 1401/150000, Train Loss: 23808, Val Loss: 24147,  Lear. Rate: 0.00469, Train Grad.: 160.0\n",
      "Epoch 1501/150000, Train Loss: 23483, Val Loss: 23818,  Lear. Rate: 0.00470, Train Grad.: 155.9\n",
      "Epoch 1601/150000, Train Loss: 23169, Val Loss: 23501,  Lear. Rate: 0.00470, Train Grad.: 151.8\n",
      "Epoch 1701/150000, Train Loss: 22866, Val Loss: 23194,  Lear. Rate: 0.00471, Train Grad.: 147.8\n",
      "Epoch 1801/150000, Train Loss: 22574, Val Loss: 22898,  Lear. Rate: 0.00472, Train Grad.: 143.8\n",
      "Epoch 1901/150000, Train Loss: 22293, Val Loss: 22613,  Lear. Rate: 0.00472, Train Grad.: 139.8\n",
      "Epoch 2001/150000, Train Loss: 22022, Val Loss: 22338,  Lear. Rate: 0.00473, Train Grad.: 135.9\n",
      "Epoch 2101/150000, Train Loss: 21761, Val Loss: 22073,  Lear. Rate: 0.00474, Train Grad.: 132.0\n",
      "Epoch 2201/150000, Train Loss: 21509, Val Loss: 21818,  Lear. Rate: 0.00474, Train Grad.: 128.1\n",
      "Epoch 2301/150000, Train Loss: 21083, Val Loss: 21409,  Lear. Rate: 0.00475, Train Grad.: 143.0\n",
      "Epoch 2401/150000, Train Loss: 20715, Val Loss: 21046,  Lear. Rate: 0.00476, Train Grad.: 141.5\n",
      "Epoch 2501/150000, Train Loss: 20406, Val Loss: 20731,  Lear. Rate: 0.00477, Train Grad.: 139.5\n",
      "Epoch 2601/150000, Train Loss: 20105, Val Loss: 20426,  Lear. Rate: 0.00477, Train Grad.: 137.4\n",
      "Epoch 2701/150000, Train Loss: 19809, Val Loss: 20127,  Lear. Rate: 0.00478, Train Grad.: 135.4\n",
      "Epoch 2801/150000, Train Loss: 19519, Val Loss: 19833,  Lear. Rate: 0.00479, Train Grad.: 133.4\n",
      "Epoch 2901/150000, Train Loss: 19234, Val Loss: 19545,  Lear. Rate: 0.00479, Train Grad.: 131.4\n",
      "Epoch 3001/150000, Train Loss: 18954, Val Loss: 19261,  Lear. Rate: 0.00480, Train Grad.: 129.5\n",
      "Epoch 3101/150000, Train Loss: 18679, Val Loss: 18982,  Lear. Rate: 0.00480, Train Grad.: 127.6\n",
      "Epoch 3201/150000, Train Loss: 18408, Val Loss: 18707,  Lear. Rate: 0.00481, Train Grad.: 125.8\n",
      "Epoch 3301/150000, Train Loss: 18142, Val Loss: 18436,  Lear. Rate: 0.00482, Train Grad.: 124.0\n",
      "Epoch 3401/150000, Train Loss: 17880, Val Loss: 18170,  Lear. Rate: 0.00482, Train Grad.: 122.3\n",
      "Epoch 3501/150000, Train Loss: 17622, Val Loss: 17908,  Lear. Rate: 0.00483, Train Grad.: 120.2\n",
      "Epoch 3601/150000, Train Loss: 17368, Val Loss: 17651,  Lear. Rate: 0.00483, Train Grad.: 118.8\n",
      "Epoch 3701/150000, Train Loss: 17119, Val Loss: 17398,  Lear. Rate: 0.00484, Train Grad.: 117.0\n",
      "Epoch 3801/150000, Train Loss: 16873, Val Loss: 17149,  Lear. Rate: 0.00484, Train Grad.: 115.3\n",
      "Epoch 3901/150000, Train Loss: 16632, Val Loss: 16904,  Lear. Rate: 0.00484, Train Grad.: 113.6\n",
      "Epoch 4001/150000, Train Loss: 16394, Val Loss: 16664,  Lear. Rate: 0.00485, Train Grad.: 111.9\n",
      "Epoch 4101/150000, Train Loss: 16160, Val Loss: 16427,  Lear. Rate: 0.00485, Train Grad.: 110.2\n",
      "Epoch 4201/150000, Train Loss: 15930, Val Loss: 16194,  Lear. Rate: 0.00486, Train Grad.: 108.6\n",
      "Epoch 4301/150000, Train Loss: 15704, Val Loss: 15965,  Lear. Rate: 0.00486, Train Grad.: 106.9\n",
      "Epoch 4401/150000, Train Loss: 15482, Val Loss: 15739,  Lear. Rate: 0.00487, Train Grad.: 105.3\n",
      "Epoch 4501/150000, Train Loss: 15263, Val Loss: 15518,  Lear. Rate: 0.00487, Train Grad.: 103.6\n",
      "Epoch 4601/150000, Train Loss: 15048, Val Loss: 15300,  Lear. Rate: 0.00487, Train Grad.: 102.1\n",
      "Epoch 4701/150000, Train Loss: 14837, Val Loss: 15085,  Lear. Rate: 0.00488, Train Grad.: 100.7\n",
      "Epoch 4801/150000, Train Loss: 14629, Val Loss: 14875,  Lear. Rate: 0.00488, Train Grad.: 99.1\n",
      "Epoch 4901/150000, Train Loss: 14423, Val Loss: 14667,  Lear. Rate: 0.00488, Train Grad.: 97.6\n",
      "Epoch 5001/150000, Train Loss: 14222, Val Loss: 14463,  Lear. Rate: 0.00489, Train Grad.: 96.1\n",
      "Epoch 5101/150000, Train Loss: 14023, Val Loss: 14261,  Lear. Rate: 0.00489, Train Grad.: 94.7\n",
      "Epoch 5201/150000, Train Loss: 13827, Val Loss: 14063,  Lear. Rate: 0.00489, Train Grad.: 93.3\n",
      "Epoch 5301/150000, Train Loss: 13635, Val Loss: 13868,  Lear. Rate: 0.00490, Train Grad.: 91.9\n",
      "Epoch 5401/150000, Train Loss: 13445, Val Loss: 13677,  Lear. Rate: 0.00490, Train Grad.: 90.5\n",
      "Epoch 5501/150000, Train Loss: 13259, Val Loss: 13488,  Lear. Rate: 0.00490, Train Grad.: 89.2\n",
      "Epoch 5601/150000, Train Loss: 13075, Val Loss: 13302,  Lear. Rate: 0.00490, Train Grad.: 87.9\n",
      "Epoch 5701/150000, Train Loss: 12893, Val Loss: 13119,  Lear. Rate: 0.00491, Train Grad.: 86.6\n",
      "Epoch 5801/150000, Train Loss: 12715, Val Loss: 12938,  Lear. Rate: 0.00491, Train Grad.: 85.3\n",
      "Epoch 5901/150000, Train Loss: 12539, Val Loss: 12760,  Lear. Rate: 0.00491, Train Grad.: 84.1\n",
      "Epoch 6001/150000, Train Loss: 12365, Val Loss: 12584,  Lear. Rate: 0.00491, Train Grad.: 82.8\n",
      "Epoch 6101/150000, Train Loss: 12194, Val Loss: 12412,  Lear. Rate: 0.00492, Train Grad.: 81.7\n",
      "Epoch 6201/150000, Train Loss: 12026, Val Loss: 12241,  Lear. Rate: 0.00492, Train Grad.: 80.5\n",
      "Epoch 6301/150000, Train Loss: 11859, Val Loss: 12073,  Lear. Rate: 0.00492, Train Grad.: 79.4\n",
      "Epoch 6401/150000, Train Loss: 11695, Val Loss: 11907,  Lear. Rate: 0.00492, Train Grad.: 78.0\n",
      "Epoch 6501/150000, Train Loss: 11533, Val Loss: 11744,  Lear. Rate: 0.00492, Train Grad.: 77.1\n",
      "Epoch 6601/150000, Train Loss: 11374, Val Loss: 11582,  Lear. Rate: 0.00493, Train Grad.: 75.9\n",
      "Epoch 6701/150000, Train Loss: 11216, Val Loss: 11423,  Lear. Rate: 0.00493, Train Grad.: 75.0\n",
      "Epoch 6801/150000, Train Loss: 11061, Val Loss: 11267,  Lear. Rate: 0.00493, Train Grad.: 73.9\n",
      "Epoch 6901/150000, Train Loss: 10908, Val Loss: 11112,  Lear. Rate: 0.00493, Train Grad.: 72.9\n",
      "Epoch 7001/150000, Train Loss: 10756, Val Loss: 10959,  Lear. Rate: 0.00493, Train Grad.: 71.9\n",
      "Epoch 7101/150000, Train Loss: 10607, Val Loss: 10809,  Lear. Rate: 0.00494, Train Grad.: 70.9\n",
      "Epoch 7201/150000, Train Loss: 10459, Val Loss: 10661,  Lear. Rate: 0.00494, Train Grad.: 69.9\n",
      "Epoch 7301/150000, Train Loss: 10314, Val Loss: 10514,  Lear. Rate: 0.00494, Train Grad.: 69.0\n",
      "Epoch 7401/150000, Train Loss: 10170, Val Loss: 10369,  Lear. Rate: 0.00494, Train Grad.: 68.1\n",
      "Epoch 7501/150000, Train Loss: 10029, Val Loss: 10226,  Lear. Rate: 0.00494, Train Grad.: 66.8\n",
      "Epoch 7601/150000, Train Loss: 9888, Val Loss: 10085,  Lear. Rate: 0.00494, Train Grad.: 66.2\n",
      "Epoch 7701/150000, Train Loss: 9750, Val Loss: 9945,  Lear. Rate: 0.00495, Train Grad.: 65.5\n",
      "Epoch 7801/150000, Train Loss: 9613, Val Loss: 9807,  Lear. Rate: 0.00495, Train Grad.: 64.5\n",
      "Epoch 7901/150000, Train Loss: 9478, Val Loss: 9670,  Lear. Rate: 0.00495, Train Grad.: 63.5\n",
      "Epoch 8001/150000, Train Loss: 9345, Val Loss: 9535,  Lear. Rate: 0.00495, Train Grad.: 62.9\n",
      "Epoch 8101/150000, Train Loss: 9213, Val Loss: 9401,  Lear. Rate: 0.00495, Train Grad.: 62.1\n",
      "Epoch 8201/150000, Train Loss: 9082, Val Loss: 9269,  Lear. Rate: 0.00495, Train Grad.: 61.3\n",
      "Epoch 8301/150000, Train Loss: 8953, Val Loss: 9139,  Lear. Rate: 0.00495, Train Grad.: 60.8\n",
      "Epoch 8401/150000, Train Loss: 8825, Val Loss: 9010,  Lear. Rate: 0.00496, Train Grad.: 59.9\n",
      "Epoch 8501/150000, Train Loss: 8699, Val Loss: 8883,  Lear. Rate: 0.00496, Train Grad.: 59.1\n",
      "Epoch 8601/150000, Train Loss: 8574, Val Loss: 8757,  Lear. Rate: 0.00496, Train Grad.: 58.4\n",
      "Epoch 8701/150000, Train Loss: 8450, Val Loss: 8632,  Lear. Rate: 0.00496, Train Grad.: 57.7\n",
      "Epoch 8801/150000, Train Loss: 8328, Val Loss: 8508,  Lear. Rate: 0.00496, Train Grad.: 57.2\n",
      "Epoch 8901/150000, Train Loss: 8206, Val Loss: 8385,  Lear. Rate: 0.00496, Train Grad.: 56.4\n",
      "Epoch 9001/150000, Train Loss: 8086, Val Loss: 8264,  Lear. Rate: 0.00496, Train Grad.: 55.1\n",
      "Epoch 9101/150000, Train Loss: 7967, Val Loss: 8143,  Lear. Rate: 0.00496, Train Grad.: 55.1\n",
      "Epoch 9201/150000, Train Loss: 7850, Val Loss: 8025,  Lear. Rate: 0.00496, Train Grad.: 54.5\n",
      "Epoch 9301/150000, Train Loss: 7733, Val Loss: 7907,  Lear. Rate: 0.00497, Train Grad.: 53.9\n",
      "Epoch 9401/150000, Train Loss: 7618, Val Loss: 7791,  Lear. Rate: 0.00497, Train Grad.: 53.3\n",
      "Epoch 9501/150000, Train Loss: 7504, Val Loss: 7676,  Lear. Rate: 0.00497, Train Grad.: 52.4\n",
      "Epoch 9601/150000, Train Loss: 7391, Val Loss: 7562,  Lear. Rate: 0.00497, Train Grad.: 52.1\n",
      "Epoch 9701/150000, Train Loss: 7279, Val Loss: 7449,  Lear. Rate: 0.00497, Train Grad.: 51.5\n",
      "Epoch 9801/150000, Train Loss: 7168, Val Loss: 7338,  Lear. Rate: 0.00497, Train Grad.: 50.8\n",
      "Epoch 9901/150000, Train Loss: 7058, Val Loss: 7227,  Lear. Rate: 0.00497, Train Grad.: 50.3\n",
      "Epoch 10001/150000, Train Loss: 6950, Val Loss: 7118,  Lear. Rate: 0.00497, Train Grad.: 49.8\n",
      "Epoch 10101/150000, Train Loss: 6842, Val Loss: 7009,  Lear. Rate: 0.00497, Train Grad.: 49.2\n",
      "Epoch 10201/150000, Train Loss: 6736, Val Loss: 6902,  Lear. Rate: 0.00497, Train Grad.: 48.7\n",
      "Epoch 10301/150000, Train Loss: 6631, Val Loss: 6796,  Lear. Rate: 0.00497, Train Grad.: 48.2\n",
      "Epoch 10401/150000, Train Loss: 6527, Val Loss: 6691,  Lear. Rate: 0.00498, Train Grad.: 47.6\n",
      "Epoch 10501/150000, Train Loss: 6424, Val Loss: 6587,  Lear. Rate: 0.00498, Train Grad.: 47.0\n",
      "Epoch 10601/150000, Train Loss: 6322, Val Loss: 6485,  Lear. Rate: 0.00498, Train Grad.: 46.5\n",
      "Epoch 10701/150000, Train Loss: 6221, Val Loss: 6383,  Lear. Rate: 0.00498, Train Grad.: 46.0\n",
      "Epoch 10801/150000, Train Loss: 6122, Val Loss: 6283,  Lear. Rate: 0.00498, Train Grad.: 45.3\n",
      "Epoch 10901/150000, Train Loss: 6023, Val Loss: 6184,  Lear. Rate: 0.00498, Train Grad.: 44.9\n",
      "Epoch 11001/150000, Train Loss: 5925, Val Loss: 6086,  Lear. Rate: 0.00498, Train Grad.: 44.4\n",
      "Epoch 11101/150000, Train Loss: 5829, Val Loss: 5989,  Lear. Rate: 0.00498, Train Grad.: 43.9\n",
      "Epoch 11201/150000, Train Loss: 5734, Val Loss: 5894,  Lear. Rate: 0.00498, Train Grad.: 43.4\n",
      "Epoch 11301/150000, Train Loss: 5640, Val Loss: 5799,  Lear. Rate: 0.00498, Train Grad.: 42.9\n",
      "Epoch 11401/150000, Train Loss: 5547, Val Loss: 5705,  Lear. Rate: 0.00498, Train Grad.: 42.3\n",
      "Epoch 11501/150000, Train Loss: 5455, Val Loss: 5613,  Lear. Rate: 0.00498, Train Grad.: 41.8\n",
      "Epoch 11601/150000, Train Loss: 5365, Val Loss: 5522,  Lear. Rate: 0.00498, Train Grad.: 41.3\n",
      "Epoch 11701/150000, Train Loss: 5275, Val Loss: 5431,  Lear. Rate: 0.00498, Train Grad.: 40.8\n",
      "Epoch 11801/150000, Train Loss: 5187, Val Loss: 5342,  Lear. Rate: 0.00498, Train Grad.: 40.1\n",
      "Epoch 11901/150000, Train Loss: 5100, Val Loss: 5254,  Lear. Rate: 0.00498, Train Grad.: 39.8\n",
      "Epoch 12001/150000, Train Loss: 5013, Val Loss: 5168,  Lear. Rate: 0.00499, Train Grad.: 39.3\n",
      "Epoch 12101/150000, Train Loss: 4928, Val Loss: 5082,  Lear. Rate: 0.00499, Train Grad.: 38.9\n",
      "Epoch 12201/150000, Train Loss: 4844, Val Loss: 4998,  Lear. Rate: 0.00499, Train Grad.: 38.4\n",
      "Epoch 12301/150000, Train Loss: 4761, Val Loss: 4915,  Lear. Rate: 0.00499, Train Grad.: 37.9\n",
      "Epoch 12401/150000, Train Loss: 4679, Val Loss: 4833,  Lear. Rate: 0.00499, Train Grad.: 37.5\n",
      "Epoch 12501/150000, Train Loss: 4598, Val Loss: 4752,  Lear. Rate: 0.00499, Train Grad.: 37.0\n",
      "Epoch 12601/150000, Train Loss: 4518, Val Loss: 4672,  Lear. Rate: 0.00499, Train Grad.: 36.6\n",
      "Epoch 12701/150000, Train Loss: 4438, Val Loss: 4593,  Lear. Rate: 0.00499, Train Grad.: 36.1\n",
      "Epoch 12801/150000, Train Loss: 4360, Val Loss: 4515,  Lear. Rate: 0.00499, Train Grad.: 35.6\n",
      "Epoch 12901/150000, Train Loss: 4283, Val Loss: 4438,  Lear. Rate: 0.00499, Train Grad.: 35.2\n",
      "Epoch 13001/150000, Train Loss: 4207, Val Loss: 4362,  Lear. Rate: 0.00499, Train Grad.: 35.3\n",
      "Epoch 13101/150000, Train Loss: 4132, Val Loss: 4287,  Lear. Rate: 0.00499, Train Grad.: 34.3\n",
      "Epoch 13201/150000, Train Loss: 4058, Val Loss: 4213,  Lear. Rate: 0.00499, Train Grad.: 33.8\n",
      "Epoch 13301/150000, Train Loss: 3985, Val Loss: 4140,  Lear. Rate: 0.00499, Train Grad.: 33.5\n",
      "Epoch 13401/150000, Train Loss: 3912, Val Loss: 4068,  Lear. Rate: 0.00499, Train Grad.: 33.1\n",
      "Epoch 13501/150000, Train Loss: 3841, Val Loss: 3997,  Lear. Rate: 0.00499, Train Grad.: 32.7\n",
      "Epoch 13601/150000, Train Loss: 3771, Val Loss: 3927,  Lear. Rate: 0.00499, Train Grad.: 32.1\n",
      "Epoch 13701/150000, Train Loss: 3702, Val Loss: 3858,  Lear. Rate: 0.00499, Train Grad.: 31.8\n",
      "Epoch 13801/150000, Train Loss: 3634, Val Loss: 3789,  Lear. Rate: 0.00499, Train Grad.: 31.3\n",
      "Epoch 13901/150000, Train Loss: 3566, Val Loss: 3722,  Lear. Rate: 0.00499, Train Grad.: 30.9\n",
      "Epoch 14001/150000, Train Loss: 3500, Val Loss: 3656,  Lear. Rate: 0.00499, Train Grad.: 30.5\n",
      "Epoch 14101/150000, Train Loss: 3435, Val Loss: 3591,  Lear. Rate: 0.00499, Train Grad.: 30.1\n",
      "Epoch 14201/150000, Train Loss: 3370, Val Loss: 3527,  Lear. Rate: 0.00499, Train Grad.: 29.8\n",
      "Epoch 14301/150000, Train Loss: 3307, Val Loss: 3464,  Lear. Rate: 0.00499, Train Grad.: 29.2\n",
      "Epoch 14401/150000, Train Loss: 3244, Val Loss: 3401,  Lear. Rate: 0.00499, Train Grad.: 28.8\n",
      "Epoch 14501/150000, Train Loss: 3183, Val Loss: 3340,  Lear. Rate: 0.00499, Train Grad.: 28.3\n",
      "Epoch 14601/150000, Train Loss: 3122, Val Loss: 3279,  Lear. Rate: 0.00499, Train Grad.: 27.8\n",
      "Epoch 14701/150000, Train Loss: 3062, Val Loss: 3220,  Lear. Rate: 0.00499, Train Grad.: 27.6\n",
      "Epoch 14801/150000, Train Loss: 3004, Val Loss: 3161,  Lear. Rate: 0.00499, Train Grad.: 27.3\n",
      "Epoch 14901/150000, Train Loss: 2946, Val Loss: 3103,  Lear. Rate: 0.00499, Train Grad.: 27.0\n",
      "Epoch 15001/150000, Train Loss: 2888, Val Loss: 3046,  Lear. Rate: 0.00499, Train Grad.: 26.5\n",
      "Epoch 15101/150000, Train Loss: 2832, Val Loss: 2990,  Lear. Rate: 0.00500, Train Grad.: 26.1\n",
      "Epoch 15201/150000, Train Loss: 2777, Val Loss: 2935,  Lear. Rate: 0.00500, Train Grad.: 25.8\n",
      "Epoch 15301/150000, Train Loss: 2722, Val Loss: 2880,  Lear. Rate: 0.00500, Train Grad.: 25.4\n",
      "Epoch 15401/150000, Train Loss: 2668, Val Loss: 2827,  Lear. Rate: 0.00500, Train Grad.: 25.0\n",
      "Epoch 15501/150000, Train Loss: 2615, Val Loss: 2774,  Lear. Rate: 0.00500, Train Grad.: 24.7\n",
      "Epoch 15601/150000, Train Loss: 2563, Val Loss: 2723,  Lear. Rate: 0.00500, Train Grad.: 24.3\n",
      "Epoch 15701/150000, Train Loss: 2512, Val Loss: 2672,  Lear. Rate: 0.00500, Train Grad.: 24.1\n",
      "Epoch 15801/150000, Train Loss: 2461, Val Loss: 2622,  Lear. Rate: 0.00500, Train Grad.: 23.5\n",
      "Epoch 15901/150000, Train Loss: 2412, Val Loss: 2572,  Lear. Rate: 0.00500, Train Grad.: 23.0\n",
      "Epoch 16001/150000, Train Loss: 2363, Val Loss: 2524,  Lear. Rate: 0.00500, Train Grad.: 22.9\n",
      "Epoch 16101/150000, Train Loss: 2315, Val Loss: 2476,  Lear. Rate: 0.00500, Train Grad.: 22.4\n",
      "Epoch 16201/150000, Train Loss: 2267, Val Loss: 2429,  Lear. Rate: 0.00500, Train Grad.: 22.4\n",
      "Epoch 16301/150000, Train Loss: 2220, Val Loss: 2383,  Lear. Rate: 0.00500, Train Grad.: 21.9\n",
      "Epoch 16401/150000, Train Loss: 2174, Val Loss: 2338,  Lear. Rate: 0.00500, Train Grad.: 21.9\n",
      "Epoch 16501/150000, Train Loss: 2129, Val Loss: 2293,  Lear. Rate: 0.00500, Train Grad.: 21.3\n",
      "Epoch 16601/150000, Train Loss: 2084, Val Loss: 2250,  Lear. Rate: 0.00500, Train Grad.: 20.7\n",
      "Epoch 16701/150000, Train Loss: 2040, Val Loss: 2205,  Lear. Rate: 0.00500, Train Grad.: 20.7\n",
      "Epoch 16801/150000, Train Loss: 1997, Val Loss: 2163,  Lear. Rate: 0.00500, Train Grad.: 20.2\n",
      "Epoch 16901/150000, Train Loss: 1954, Val Loss: 2120,  Lear. Rate: 0.00500, Train Grad.: 19.8\n",
      "Epoch 17001/150000, Train Loss: 1912, Val Loss: 2080,  Lear. Rate: 0.00500, Train Grad.: 19.7\n",
      "Epoch 17101/150000, Train Loss: 1871, Val Loss: 2037,  Lear. Rate: 0.00500, Train Grad.: 19.4\n",
      "Epoch 17201/150000, Train Loss: 1830, Val Loss: 1997,  Lear. Rate: 0.00500, Train Grad.: 19.1\n",
      "Epoch 17301/150000, Train Loss: 1790, Val Loss: 1958,  Lear. Rate: 0.00500, Train Grad.: 18.8\n",
      "Epoch 17401/150000, Train Loss: 1751, Val Loss: 1919,  Lear. Rate: 0.00500, Train Grad.: 18.6\n",
      "Epoch 17501/150000, Train Loss: 1713, Val Loss: 1881,  Lear. Rate: 0.00500, Train Grad.: 18.3\n",
      "Epoch 17601/150000, Train Loss: 1675, Val Loss: 1844,  Lear. Rate: 0.00500, Train Grad.: 18.0\n",
      "Epoch 17701/150000, Train Loss: 1638, Val Loss: 1807,  Lear. Rate: 0.00500, Train Grad.: 17.7\n",
      "Epoch 17801/150000, Train Loss: 1601, Val Loss: 1772,  Lear. Rate: 0.00500, Train Grad.: 17.2\n",
      "Epoch 17901/150000, Train Loss: 1565, Val Loss: 1736,  Lear. Rate: 0.00500, Train Grad.: 17.1\n",
      "Epoch 18001/150000, Train Loss: 1530, Val Loss: 1702,  Lear. Rate: 0.00500, Train Grad.: 16.7\n",
      "Epoch 18101/150000, Train Loss: 1496, Val Loss: 1668,  Lear. Rate: 0.00500, Train Grad.: 16.4\n",
      "Epoch 18201/150000, Train Loss: 1462, Val Loss: 1635,  Lear. Rate: 0.00500, Train Grad.: 16.3\n",
      "Epoch 18301/150000, Train Loss: 1428, Val Loss: 1603,  Lear. Rate: 0.00500, Train Grad.: 15.5\n",
      "Epoch 18401/150000, Train Loss: 1396, Val Loss: 1571,  Lear. Rate: 0.00500, Train Grad.: 15.7\n",
      "Epoch 18501/150000, Train Loss: 1363, Val Loss: 1540,  Lear. Rate: 0.00500, Train Grad.: 15.4\n",
      "Epoch 18601/150000, Train Loss: 1332, Val Loss: 1510,  Lear. Rate: 0.00500, Train Grad.: 15.2\n",
      "Epoch 18701/150000, Train Loss: 1300, Val Loss: 1480,  Lear. Rate: 0.00500, Train Grad.: 14.9\n",
      "Epoch 18801/150000, Train Loss: 1270, Val Loss: 1450,  Lear. Rate: 0.00500, Train Grad.: 14.7\n",
      "Epoch 18901/150000, Train Loss: 1240, Val Loss: 1421,  Lear. Rate: 0.00500, Train Grad.: 14.4\n",
      "Epoch 19001/150000, Train Loss: 1210, Val Loss: 1393,  Lear. Rate: 0.00500, Train Grad.: 14.4\n",
      "Epoch 19101/150000, Train Loss: 1181, Val Loss: 1365,  Lear. Rate: 0.00500, Train Grad.: 14.1\n",
      "Epoch 19201/150000, Train Loss: 1152, Val Loss: 1338,  Lear. Rate: 0.00500, Train Grad.: 13.7\n",
      "Epoch 19301/150000, Train Loss: 1125, Val Loss: 1311,  Lear. Rate: 0.00500, Train Grad.: 13.7\n",
      "Epoch 19401/150000, Train Loss: 1097, Val Loss: 1285,  Lear. Rate: 0.00500, Train Grad.: 13.1\n",
      "Epoch 19501/150000, Train Loss: 1070, Val Loss: 1259,  Lear. Rate: 0.00500, Train Grad.: 12.9\n",
      "Epoch 19601/150000, Train Loss: 1044, Val Loss: 1234,  Lear. Rate: 0.00500, Train Grad.: 12.7\n",
      "Epoch 19701/150000, Train Loss: 1018, Val Loss: 1209,  Lear. Rate: 0.00500, Train Grad.: 12.5\n",
      "Epoch 19801/150000, Train Loss: 993, Val Loss: 1185,  Lear. Rate: 0.00500, Train Grad.: 12.0\n",
      "Epoch 19901/150000, Train Loss: 968, Val Loss: 1161,  Lear. Rate: 0.00500, Train Grad.: 12.2\n",
      "Epoch 20001/150000, Train Loss: 944, Val Loss: 1139,  Lear. Rate: 0.00500, Train Grad.: 11.2\n",
      "Epoch 20101/150000, Train Loss: 920, Val Loss: 1116,  Lear. Rate: 0.00500, Train Grad.: 11.4\n",
      "Epoch 20201/150000, Train Loss: 897, Val Loss: 1095,  Lear. Rate: 0.00500, Train Grad.: 11.4\n",
      "Epoch 20301/150000, Train Loss: 874, Val Loss: 1074,  Lear. Rate: 0.00500, Train Grad.: 11.2\n",
      "Epoch 20401/150000, Train Loss: 851, Val Loss: 1053,  Lear. Rate: 0.00500, Train Grad.: 11.0\n",
      "Epoch 20501/150000, Train Loss: 830, Val Loss: 1033,  Lear. Rate: 0.00500, Train Grad.: 10.8\n",
      "Epoch 20601/150000, Train Loss: 808, Val Loss: 1013,  Lear. Rate: 0.00500, Train Grad.: 10.5\n",
      "Epoch 20701/150000, Train Loss: 787, Val Loss: 994,  Lear. Rate: 0.00500, Train Grad.: 10.3\n",
      "Epoch 20801/150000, Train Loss: 766, Val Loss: 975,  Lear. Rate: 0.00500, Train Grad.: 10.2\n",
      "Epoch 20901/150000, Train Loss: 746, Val Loss: 957,  Lear. Rate: 0.00500, Train Grad.: 10.0\n",
      "Epoch 21001/150000, Train Loss: 726, Val Loss: 940,  Lear. Rate: 0.00500, Train Grad.: 9.7\n",
      "Epoch 21101/150000, Train Loss: 706, Val Loss: 924,  Lear. Rate: 0.00500, Train Grad.: 9.6\n",
      "Epoch 21201/150000, Train Loss: 686, Val Loss: 909,  Lear. Rate: 0.00500, Train Grad.: 8.6\n",
      "Early stopping at epoch 21248 with validation loss 899.1615600585938.\n",
      "Test Loss: 882.0489501953125\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 30088, Val Loss: 30585,  Lear. Rate: 0.00500, Train Grad.: 225.1\n",
      "Epoch 101/150000, Train Loss: 29401, Val Loss: 29887,  Lear. Rate: 0.00453, Train Grad.: 219.0\n",
      "Epoch 201/150000, Train Loss: 28796, Val Loss: 29276,  Lear. Rate: 0.00454, Train Grad.: 213.4\n",
      "Epoch 301/150000, Train Loss: 28273, Val Loss: 28747,  Lear. Rate: 0.00456, Train Grad.: 208.4\n",
      "Epoch 401/150000, Train Loss: 27784, Val Loss: 28253,  Lear. Rate: 0.00457, Train Grad.: 203.7\n",
      "Epoch 501/150000, Train Loss: 27319, Val Loss: 27783,  Lear. Rate: 0.00459, Train Grad.: 199.1\n",
      "Epoch 601/150000, Train Loss: 26874, Val Loss: 27331,  Lear. Rate: 0.00460, Train Grad.: 194.5\n",
      "Epoch 701/150000, Train Loss: 26445, Val Loss: 26897,  Lear. Rate: 0.00461, Train Grad.: 190.1\n",
      "Epoch 801/150000, Train Loss: 26031, Val Loss: 26478,  Lear. Rate: 0.00463, Train Grad.: 185.7\n",
      "Epoch 901/150000, Train Loss: 25631, Val Loss: 26073,  Lear. Rate: 0.00464, Train Grad.: 181.3\n",
      "Epoch 1001/150000, Train Loss: 25245, Val Loss: 25681,  Lear. Rate: 0.00465, Train Grad.: 177.0\n",
      "Epoch 1101/150000, Train Loss: 24872, Val Loss: 25303,  Lear. Rate: 0.00466, Train Grad.: 172.7\n",
      "Epoch 1201/150000, Train Loss: 24511, Val Loss: 24937,  Lear. Rate: 0.00467, Train Grad.: 168.5\n",
      "Epoch 1301/150000, Train Loss: 24163, Val Loss: 24584,  Lear. Rate: 0.00468, Train Grad.: 164.3\n",
      "Epoch 1401/150000, Train Loss: 23826, Val Loss: 24242,  Lear. Rate: 0.00468, Train Grad.: 160.2\n",
      "Epoch 1501/150000, Train Loss: 23501, Val Loss: 23912,  Lear. Rate: 0.00469, Train Grad.: 156.1\n",
      "Epoch 1601/150000, Train Loss: 23187, Val Loss: 23593,  Lear. Rate: 0.00470, Train Grad.: 152.0\n",
      "Epoch 1701/150000, Train Loss: 22884, Val Loss: 23285,  Lear. Rate: 0.00471, Train Grad.: 147.9\n",
      "Epoch 1801/150000, Train Loss: 22592, Val Loss: 22988,  Lear. Rate: 0.00472, Train Grad.: 143.9\n",
      "Epoch 1901/150000, Train Loss: 22310, Val Loss: 22701,  Lear. Rate: 0.00472, Train Grad.: 140.0\n",
      "Epoch 2001/150000, Train Loss: 22039, Val Loss: 22425,  Lear. Rate: 0.00473, Train Grad.: 136.0\n",
      "Epoch 2101/150000, Train Loss: 21777, Val Loss: 22158,  Lear. Rate: 0.00474, Train Grad.: 132.1\n",
      "Epoch 2201/150000, Train Loss: 21305, Val Loss: 21714,  Lear. Rate: 0.00475, Train Grad.: 145.3\n",
      "Epoch 2301/150000, Train Loss: 20989, Val Loss: 21389,  Lear. Rate: 0.00475, Train Grad.: 143.4\n",
      "Epoch 2401/150000, Train Loss: 20680, Val Loss: 21073,  Lear. Rate: 0.00476, Train Grad.: 141.4\n",
      "Epoch 2501/150000, Train Loss: 20378, Val Loss: 20766,  Lear. Rate: 0.00477, Train Grad.: 139.3\n",
      "Epoch 2601/150000, Train Loss: 20081, Val Loss: 20465,  Lear. Rate: 0.00477, Train Grad.: 137.2\n",
      "Epoch 2701/150000, Train Loss: 19789, Val Loss: 20169,  Lear. Rate: 0.00478, Train Grad.: 135.2\n",
      "Epoch 2801/150000, Train Loss: 19503, Val Loss: 19877,  Lear. Rate: 0.00479, Train Grad.: 133.3\n",
      "Epoch 2901/150000, Train Loss: 19221, Val Loss: 19590,  Lear. Rate: 0.00479, Train Grad.: 131.3\n",
      "Epoch 3001/150000, Train Loss: 18944, Val Loss: 19307,  Lear. Rate: 0.00480, Train Grad.: 129.5\n",
      "Epoch 3101/150000, Train Loss: 18671, Val Loss: 19029,  Lear. Rate: 0.00480, Train Grad.: 127.6\n",
      "Epoch 3201/150000, Train Loss: 18402, Val Loss: 18755,  Lear. Rate: 0.00481, Train Grad.: 125.8\n",
      "Epoch 3301/150000, Train Loss: 18138, Val Loss: 18486,  Lear. Rate: 0.00481, Train Grad.: 124.0\n",
      "Epoch 3401/150000, Train Loss: 17878, Val Loss: 18221,  Lear. Rate: 0.00482, Train Grad.: 122.3\n",
      "Epoch 3501/150000, Train Loss: 17622, Val Loss: 17961,  Lear. Rate: 0.00483, Train Grad.: 120.5\n",
      "Epoch 3601/150000, Train Loss: 17369, Val Loss: 17704,  Lear. Rate: 0.00483, Train Grad.: 118.8\n",
      "Epoch 3701/150000, Train Loss: 17121, Val Loss: 17452,  Lear. Rate: 0.00483, Train Grad.: 117.0\n",
      "Epoch 3801/150000, Train Loss: 16877, Val Loss: 17204,  Lear. Rate: 0.00484, Train Grad.: 115.3\n",
      "Epoch 3901/150000, Train Loss: 16636, Val Loss: 16959,  Lear. Rate: 0.00484, Train Grad.: 113.6\n",
      "Epoch 4001/150000, Train Loss: 16399, Val Loss: 16719,  Lear. Rate: 0.00485, Train Grad.: 111.9\n",
      "Epoch 4101/150000, Train Loss: 16167, Val Loss: 16482,  Lear. Rate: 0.00485, Train Grad.: 110.3\n",
      "Epoch 4201/150000, Train Loss: 15938, Val Loss: 16250,  Lear. Rate: 0.00486, Train Grad.: 108.6\n",
      "Epoch 4301/150000, Train Loss: 15712, Val Loss: 16021,  Lear. Rate: 0.00486, Train Grad.: 107.0\n",
      "Epoch 4401/150000, Train Loss: 15490, Val Loss: 15795,  Lear. Rate: 0.00486, Train Grad.: 105.3\n",
      "Epoch 4501/150000, Train Loss: 15272, Val Loss: 15574,  Lear. Rate: 0.00487, Train Grad.: 103.7\n",
      "Epoch 4601/150000, Train Loss: 15058, Val Loss: 15356,  Lear. Rate: 0.00487, Train Grad.: 102.1\n",
      "Epoch 4701/150000, Train Loss: 14847, Val Loss: 15141,  Lear. Rate: 0.00488, Train Grad.: 100.7\n",
      "Epoch 4801/150000, Train Loss: 14639, Val Loss: 14930,  Lear. Rate: 0.00488, Train Grad.: 99.2\n",
      "Epoch 4901/150000, Train Loss: 14434, Val Loss: 14723,  Lear. Rate: 0.00488, Train Grad.: 97.7\n",
      "Epoch 5001/150000, Train Loss: 14232, Val Loss: 14518,  Lear. Rate: 0.00489, Train Grad.: 96.2\n",
      "Epoch 5101/150000, Train Loss: 14034, Val Loss: 14316,  Lear. Rate: 0.00489, Train Grad.: 94.8\n",
      "Epoch 5201/150000, Train Loss: 13838, Val Loss: 14118,  Lear. Rate: 0.00489, Train Grad.: 93.3\n",
      "Epoch 5301/150000, Train Loss: 13646, Val Loss: 13923,  Lear. Rate: 0.00489, Train Grad.: 91.9\n",
      "Epoch 5401/150000, Train Loss: 13456, Val Loss: 13731,  Lear. Rate: 0.00490, Train Grad.: 90.6\n",
      "Epoch 5501/150000, Train Loss: 13270, Val Loss: 13541,  Lear. Rate: 0.00490, Train Grad.: 89.2\n",
      "Epoch 5601/150000, Train Loss: 13086, Val Loss: 13355,  Lear. Rate: 0.00490, Train Grad.: 87.9\n",
      "Epoch 5701/150000, Train Loss: 12904, Val Loss: 13171,  Lear. Rate: 0.00491, Train Grad.: 86.6\n",
      "Epoch 5801/150000, Train Loss: 12726, Val Loss: 12990,  Lear. Rate: 0.00491, Train Grad.: 85.4\n",
      "Epoch 5901/150000, Train Loss: 12550, Val Loss: 12811,  Lear. Rate: 0.00491, Train Grad.: 84.1\n",
      "Epoch 6001/150000, Train Loss: 12376, Val Loss: 12635,  Lear. Rate: 0.00491, Train Grad.: 82.9\n",
      "Epoch 6101/150000, Train Loss: 12205, Val Loss: 12461,  Lear. Rate: 0.00492, Train Grad.: 81.7\n",
      "Epoch 6201/150000, Train Loss: 12036, Val Loss: 12290,  Lear. Rate: 0.00492, Train Grad.: 80.5\n",
      "Epoch 6301/150000, Train Loss: 11870, Val Loss: 12122,  Lear. Rate: 0.00492, Train Grad.: 79.4\n",
      "Epoch 6401/150000, Train Loss: 11706, Val Loss: 11955,  Lear. Rate: 0.00492, Train Grad.: 78.2\n",
      "Epoch 6501/150000, Train Loss: 11544, Val Loss: 11791,  Lear. Rate: 0.00492, Train Grad.: 77.2\n",
      "Epoch 6601/150000, Train Loss: 11384, Val Loss: 11629,  Lear. Rate: 0.00493, Train Grad.: 76.1\n",
      "Epoch 6701/150000, Train Loss: 11227, Val Loss: 11469,  Lear. Rate: 0.00493, Train Grad.: 75.0\n",
      "Epoch 6801/150000, Train Loss: 11071, Val Loss: 11312,  Lear. Rate: 0.00493, Train Grad.: 74.0\n",
      "Epoch 6901/150000, Train Loss: 10918, Val Loss: 11157,  Lear. Rate: 0.00493, Train Grad.: 72.9\n",
      "Epoch 7001/150000, Train Loss: 10766, Val Loss: 11004,  Lear. Rate: 0.00493, Train Grad.: 72.1\n",
      "Epoch 7101/150000, Train Loss: 10617, Val Loss: 10853,  Lear. Rate: 0.00494, Train Grad.: 71.0\n",
      "Epoch 7201/150000, Train Loss: 10469, Val Loss: 10704,  Lear. Rate: 0.00494, Train Grad.: 70.0\n",
      "Epoch 7301/150000, Train Loss: 10324, Val Loss: 10558,  Lear. Rate: 0.00494, Train Grad.: 69.1\n",
      "Epoch 7401/150000, Train Loss: 10180, Val Loss: 10413,  Lear. Rate: 0.00494, Train Grad.: 68.1\n",
      "Epoch 7501/150000, Train Loss: 10038, Val Loss: 10270,  Lear. Rate: 0.00494, Train Grad.: 67.2\n",
      "Epoch 7601/150000, Train Loss: 9898, Val Loss: 10130,  Lear. Rate: 0.00494, Train Grad.: 66.3\n",
      "Epoch 7701/150000, Train Loss: 9759, Val Loss: 9992,  Lear. Rate: 0.00495, Train Grad.: 65.5\n",
      "Epoch 7801/150000, Train Loss: 9623, Val Loss: 9858,  Lear. Rate: 0.00495, Train Grad.: 64.6\n",
      "Epoch 7901/150000, Train Loss: 9488, Val Loss: 9727,  Lear. Rate: 0.00495, Train Grad.: 63.7\n",
      "Epoch 8001/150000, Train Loss: 9354, Val Loss: 9618,  Lear. Rate: 0.00495, Train Grad.: 62.9\n",
      "Epoch 8101/150000, Train Loss: 9222, Val Loss: 9504,  Lear. Rate: 0.00495, Train Grad.: 62.1\n",
      "Epoch 8201/150000, Train Loss: 9092, Val Loss: 9309,  Lear. Rate: 0.00495, Train Grad.: 61.5\n",
      "Epoch 8301/150000, Train Loss: 8962, Val Loss: 9218,  Lear. Rate: 0.00495, Train Grad.: 60.6\n",
      "Epoch 8401/150000, Train Loss: 8834, Val Loss: 9119,  Lear. Rate: 0.00495, Train Grad.: 59.9\n",
      "Epoch 8501/150000, Train Loss: 8708, Val Loss: 8993,  Lear. Rate: 0.00496, Train Grad.: 59.2\n",
      "Epoch 8601/150000, Train Loss: 8583, Val Loss: 8867,  Lear. Rate: 0.00496, Train Grad.: 58.5\n",
      "Epoch 8701/150000, Train Loss: 8459, Val Loss: 8741,  Lear. Rate: 0.00496, Train Grad.: 57.8\n",
      "Epoch 8801/150000, Train Loss: 8336, Val Loss: 8618,  Lear. Rate: 0.00496, Train Grad.: 56.8\n",
      "Epoch 8901/150000, Train Loss: 8215, Val Loss: 8494,  Lear. Rate: 0.00496, Train Grad.: 56.5\n",
      "Epoch 9001/150000, Train Loss: 8095, Val Loss: 8371,  Lear. Rate: 0.00496, Train Grad.: 55.8\n",
      "Epoch 9101/150000, Train Loss: 7976, Val Loss: 8248,  Lear. Rate: 0.00496, Train Grad.: 55.1\n",
      "Epoch 9201/150000, Train Loss: 7858, Val Loss: 8127,  Lear. Rate: 0.00496, Train Grad.: 54.5\n",
      "Epoch 9301/150000, Train Loss: 7741, Val Loss: 8007,  Lear. Rate: 0.00496, Train Grad.: 53.9\n",
      "Epoch 9401/150000, Train Loss: 7626, Val Loss: 7887,  Lear. Rate: 0.00497, Train Grad.: 53.4\n",
      "Epoch 9501/150000, Train Loss: 7512, Val Loss: 7770,  Lear. Rate: 0.00497, Train Grad.: 52.7\n",
      "Epoch 9601/150000, Train Loss: 7399, Val Loss: 7653,  Lear. Rate: 0.00497, Train Grad.: 52.2\n",
      "Epoch 9701/150000, Train Loss: 7287, Val Loss: 7538,  Lear. Rate: 0.00497, Train Grad.: 51.5\n",
      "Epoch 9801/150000, Train Loss: 7176, Val Loss: 7424,  Lear. Rate: 0.00497, Train Grad.: 50.9\n",
      "Epoch 9901/150000, Train Loss: 7066, Val Loss: 7310,  Lear. Rate: 0.00497, Train Grad.: 50.4\n",
      "Epoch 10001/150000, Train Loss: 6958, Val Loss: 7198,  Lear. Rate: 0.00497, Train Grad.: 49.8\n",
      "Epoch 10101/150000, Train Loss: 6850, Val Loss: 7087,  Lear. Rate: 0.00497, Train Grad.: 49.2\n",
      "Epoch 10201/150000, Train Loss: 6744, Val Loss: 6975,  Lear. Rate: 0.00497, Train Grad.: 48.9\n",
      "Epoch 10301/150000, Train Loss: 6639, Val Loss: 6867,  Lear. Rate: 0.00497, Train Grad.: 48.2\n",
      "Epoch 10401/150000, Train Loss: 6535, Val Loss: 6760,  Lear. Rate: 0.00497, Train Grad.: 47.6\n",
      "Epoch 10501/150000, Train Loss: 6432, Val Loss: 6653,  Lear. Rate: 0.00498, Train Grad.: 47.1\n",
      "Epoch 10601/150000, Train Loss: 6330, Val Loss: 6547,  Lear. Rate: 0.00498, Train Grad.: 46.5\n",
      "Epoch 10701/150000, Train Loss: 6229, Val Loss: 6442,  Lear. Rate: 0.00498, Train Grad.: 46.1\n",
      "Epoch 10801/150000, Train Loss: 6129, Val Loss: 6339,  Lear. Rate: 0.00498, Train Grad.: 45.5\n",
      "Epoch 10901/150000, Train Loss: 6030, Val Loss: 6237,  Lear. Rate: 0.00498, Train Grad.: 44.9\n",
      "Epoch 11001/150000, Train Loss: 5933, Val Loss: 6137,  Lear. Rate: 0.00498, Train Grad.: 44.5\n",
      "Epoch 11101/150000, Train Loss: 5837, Val Loss: 6039,  Lear. Rate: 0.00498, Train Grad.: 43.9\n",
      "Epoch 11201/150000, Train Loss: 5741, Val Loss: 5942,  Lear. Rate: 0.00498, Train Grad.: 43.3\n",
      "Epoch 11301/150000, Train Loss: 5647, Val Loss: 5846,  Lear. Rate: 0.00498, Train Grad.: 42.8\n",
      "Epoch 11401/150000, Train Loss: 5554, Val Loss: 5750,  Lear. Rate: 0.00498, Train Grad.: 42.4\n",
      "Epoch 11501/150000, Train Loss: 5462, Val Loss: 5657,  Lear. Rate: 0.00498, Train Grad.: 41.9\n",
      "Epoch 11601/150000, Train Loss: 5372, Val Loss: 5564,  Lear. Rate: 0.00498, Train Grad.: 41.4\n",
      "Epoch 11701/150000, Train Loss: 5282, Val Loss: 5473,  Lear. Rate: 0.00498, Train Grad.: 40.8\n",
      "Epoch 11801/150000, Train Loss: 5194, Val Loss: 5383,  Lear. Rate: 0.00498, Train Grad.: 40.2\n",
      "Epoch 11901/150000, Train Loss: 5107, Val Loss: 5294,  Lear. Rate: 0.00498, Train Grad.: 39.8\n",
      "Epoch 12001/150000, Train Loss: 5021, Val Loss: 5207,  Lear. Rate: 0.00499, Train Grad.: 39.4\n",
      "Epoch 12101/150000, Train Loss: 4935, Val Loss: 5120,  Lear. Rate: 0.00499, Train Grad.: 39.0\n",
      "Epoch 12201/150000, Train Loss: 4851, Val Loss: 5036,  Lear. Rate: 0.00499, Train Grad.: 38.3\n",
      "Epoch 12301/150000, Train Loss: 4768, Val Loss: 4952,  Lear. Rate: 0.00499, Train Grad.: 37.9\n",
      "Epoch 12401/150000, Train Loss: 4686, Val Loss: 4869,  Lear. Rate: 0.00499, Train Grad.: 37.5\n",
      "Epoch 12501/150000, Train Loss: 4605, Val Loss: 4787,  Lear. Rate: 0.00499, Train Grad.: 37.0\n",
      "Epoch 12601/150000, Train Loss: 4525, Val Loss: 4705,  Lear. Rate: 0.00499, Train Grad.: 36.9\n",
      "Epoch 12701/150000, Train Loss: 4446, Val Loss: 4626,  Lear. Rate: 0.00499, Train Grad.: 36.1\n",
      "Epoch 12801/150000, Train Loss: 4368, Val Loss: 4548,  Lear. Rate: 0.00499, Train Grad.: 35.7\n",
      "Epoch 12901/150000, Train Loss: 4290, Val Loss: 4466,  Lear. Rate: 0.00499, Train Grad.: 35.3\n",
      "Epoch 13001/150000, Train Loss: 4214, Val Loss: 4390,  Lear. Rate: 0.00499, Train Grad.: 34.8\n",
      "Epoch 13101/150000, Train Loss: 4139, Val Loss: 4316,  Lear. Rate: 0.00499, Train Grad.: 34.3\n",
      "Epoch 13201/150000, Train Loss: 4065, Val Loss: 4242,  Lear. Rate: 0.00499, Train Grad.: 33.9\n",
      "Epoch 13301/150000, Train Loss: 3992, Val Loss: 4168,  Lear. Rate: 0.00499, Train Grad.: 33.5\n",
      "Epoch 13401/150000, Train Loss: 3919, Val Loss: 4096,  Lear. Rate: 0.00499, Train Grad.: 33.0\n",
      "Epoch 13501/150000, Train Loss: 3848, Val Loss: 4025,  Lear. Rate: 0.00499, Train Grad.: 32.6\n",
      "Epoch 13601/150000, Train Loss: 3778, Val Loss: 3955,  Lear. Rate: 0.00499, Train Grad.: 32.2\n",
      "Epoch 13701/150000, Train Loss: 3709, Val Loss: 3886,  Lear. Rate: 0.00499, Train Grad.: 31.8\n",
      "Epoch 13801/150000, Train Loss: 3640, Val Loss: 3818,  Lear. Rate: 0.00499, Train Grad.: 31.3\n",
      "Epoch 13901/150000, Train Loss: 3573, Val Loss: 3751,  Lear. Rate: 0.00499, Train Grad.: 30.9\n",
      "Epoch 14001/150000, Train Loss: 3507, Val Loss: 3685,  Lear. Rate: 0.00499, Train Grad.: 30.5\n",
      "Epoch 14101/150000, Train Loss: 3440, Val Loss: 3613,  Lear. Rate: 0.00499, Train Grad.: 30.1\n",
      "Epoch 14201/150000, Train Loss: 3376, Val Loss: 3548,  Lear. Rate: 0.00499, Train Grad.: 29.7\n",
      "Epoch 14301/150000, Train Loss: 3312, Val Loss: 3484,  Lear. Rate: 0.00499, Train Grad.: 29.4\n",
      "Epoch 14401/150000, Train Loss: 3249, Val Loss: 3421,  Lear. Rate: 0.00499, Train Grad.: 29.0\n",
      "Epoch 14501/150000, Train Loss: 3188, Val Loss: 3358,  Lear. Rate: 0.00499, Train Grad.: 28.5\n",
      "Epoch 14601/150000, Train Loss: 3127, Val Loss: 3301,  Lear. Rate: 0.00499, Train Grad.: 28.0\n",
      "Epoch 14701/150000, Train Loss: 3067, Val Loss: 3242,  Lear. Rate: 0.00499, Train Grad.: 27.7\n",
      "Epoch 14801/150000, Train Loss: 3008, Val Loss: 3183,  Lear. Rate: 0.00499, Train Grad.: 27.3\n",
      "Epoch 14901/150000, Train Loss: 2950, Val Loss: 3124,  Lear. Rate: 0.00499, Train Grad.: 26.9\n",
      "Epoch 15001/150000, Train Loss: 2893, Val Loss: 3066,  Lear. Rate: 0.00499, Train Grad.: 26.5\n",
      "Epoch 15101/150000, Train Loss: 2837, Val Loss: 3010,  Lear. Rate: 0.00500, Train Grad.: 26.1\n",
      "Epoch 15201/150000, Train Loss: 2781, Val Loss: 2953,  Lear. Rate: 0.00500, Train Grad.: 25.7\n",
      "Epoch 15301/150000, Train Loss: 2727, Val Loss: 2898,  Lear. Rate: 0.00500, Train Grad.: 25.4\n",
      "Epoch 15401/150000, Train Loss: 2672, Val Loss: 2842,  Lear. Rate: 0.00500, Train Grad.: 25.0\n",
      "Epoch 15501/150000, Train Loss: 2619, Val Loss: 2788,  Lear. Rate: 0.00500, Train Grad.: 24.7\n",
      "Epoch 15601/150000, Train Loss: 2567, Val Loss: 2736,  Lear. Rate: 0.00500, Train Grad.: 24.4\n",
      "Epoch 15701/150000, Train Loss: 2515, Val Loss: 2686,  Lear. Rate: 0.00500, Train Grad.: 23.9\n",
      "Epoch 15801/150000, Train Loss: 2465, Val Loss: 2637,  Lear. Rate: 0.00500, Train Grad.: 23.7\n",
      "Epoch 15901/150000, Train Loss: 2415, Val Loss: 2588,  Lear. Rate: 0.00500, Train Grad.: 23.2\n",
      "Epoch 16001/150000, Train Loss: 2366, Val Loss: 2540,  Lear. Rate: 0.00500, Train Grad.: 22.8\n",
      "Epoch 16101/150000, Train Loss: 2318, Val Loss: 2493,  Lear. Rate: 0.00500, Train Grad.: 22.5\n",
      "Early stopping at epoch 16138 with validation loss 2501.677001953125.\n",
      "Test Loss: 2480.735595703125\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=25, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 30106, Val Loss: 30699,  Lear. Rate: 0.00500, Train Grad.: 225.2\n",
      "Epoch 101/150000, Train Loss: 29420, Val Loss: 30000,  Lear. Rate: 0.00452, Train Grad.: 219.2\n",
      "Epoch 201/150000, Train Loss: 28814, Val Loss: 29387,  Lear. Rate: 0.00454, Train Grad.: 213.5\n",
      "Epoch 301/150000, Train Loss: 28291, Val Loss: 28857,  Lear. Rate: 0.00456, Train Grad.: 208.6\n",
      "Epoch 401/150000, Train Loss: 27803, Val Loss: 28362,  Lear. Rate: 0.00457, Train Grad.: 203.8\n",
      "Epoch 501/150000, Train Loss: 27338, Val Loss: 27890,  Lear. Rate: 0.00459, Train Grad.: 199.2\n",
      "Epoch 601/150000, Train Loss: 26892, Val Loss: 27437,  Lear. Rate: 0.00460, Train Grad.: 194.7\n",
      "Epoch 701/150000, Train Loss: 26463, Val Loss: 27001,  Lear. Rate: 0.00461, Train Grad.: 190.2\n",
      "Epoch 801/150000, Train Loss: 26049, Val Loss: 26580,  Lear. Rate: 0.00462, Train Grad.: 185.8\n",
      "Epoch 901/150000, Train Loss: 25649, Val Loss: 26174,  Lear. Rate: 0.00463, Train Grad.: 181.4\n",
      "Epoch 1001/150000, Train Loss: 25263, Val Loss: 25781,  Lear. Rate: 0.00464, Train Grad.: 177.1\n",
      "Epoch 1101/150000, Train Loss: 24890, Val Loss: 25402,  Lear. Rate: 0.00465, Train Grad.: 172.9\n",
      "Epoch 1201/150000, Train Loss: 24529, Val Loss: 25034,  Lear. Rate: 0.00466, Train Grad.: 168.6\n",
      "Epoch 1301/150000, Train Loss: 24181, Val Loss: 24679,  Lear. Rate: 0.00467, Train Grad.: 164.4\n",
      "Epoch 1401/150000, Train Loss: 23844, Val Loss: 24336,  Lear. Rate: 0.00468, Train Grad.: 160.3\n",
      "Epoch 1501/150000, Train Loss: 23519, Val Loss: 24005,  Lear. Rate: 0.00469, Train Grad.: 156.2\n",
      "Epoch 1601/150000, Train Loss: 23204, Val Loss: 23684,  Lear. Rate: 0.00470, Train Grad.: 152.1\n",
      "Epoch 1701/150000, Train Loss: 22901, Val Loss: 23375,  Lear. Rate: 0.00471, Train Grad.: 148.1\n",
      "Epoch 1801/150000, Train Loss: 22609, Val Loss: 23076,  Lear. Rate: 0.00471, Train Grad.: 144.1\n",
      "Epoch 1901/150000, Train Loss: 22327, Val Loss: 22788,  Lear. Rate: 0.00472, Train Grad.: 140.1\n",
      "Epoch 2001/150000, Train Loss: 22055, Val Loss: 22511,  Lear. Rate: 0.00473, Train Grad.: 136.2\n",
      "Epoch 2101/150000, Train Loss: 21621, Val Loss: 22103,  Lear. Rate: 0.00474, Train Grad.: 148.5\n",
      "Epoch 2201/150000, Train Loss: 21295, Val Loss: 21768,  Lear. Rate: 0.00474, Train Grad.: 145.7\n",
      "Epoch 2301/150000, Train Loss: 20984, Val Loss: 21452,  Lear. Rate: 0.00475, Train Grad.: 143.4\n",
      "Epoch 2401/150000, Train Loss: 20679, Val Loss: 21141,  Lear. Rate: 0.00476, Train Grad.: 141.3\n",
      "Epoch 2501/150000, Train Loss: 20379, Val Loss: 20836,  Lear. Rate: 0.00477, Train Grad.: 139.0\n",
      "Epoch 2601/150000, Train Loss: 20084, Val Loss: 20536,  Lear. Rate: 0.00477, Train Grad.: 137.3\n",
      "Epoch 2701/150000, Train Loss: 19793, Val Loss: 20241,  Lear. Rate: 0.00478, Train Grad.: 135.3\n",
      "Epoch 2801/150000, Train Loss: 19508, Val Loss: 19950,  Lear. Rate: 0.00478, Train Grad.: 133.3\n",
      "Epoch 2901/150000, Train Loss: 19227, Val Loss: 19664,  Lear. Rate: 0.00479, Train Grad.: 131.4\n",
      "Epoch 3001/150000, Train Loss: 18951, Val Loss: 19383,  Lear. Rate: 0.00480, Train Grad.: 129.5\n",
      "Epoch 3101/150000, Train Loss: 18679, Val Loss: 19105,  Lear. Rate: 0.00480, Train Grad.: 127.7\n",
      "Epoch 3201/150000, Train Loss: 18411, Val Loss: 18831,  Lear. Rate: 0.00481, Train Grad.: 125.9\n",
      "Epoch 3301/150000, Train Loss: 18148, Val Loss: 18561,  Lear. Rate: 0.00481, Train Grad.: 124.1\n",
      "Epoch 3401/150000, Train Loss: 17888, Val Loss: 18295,  Lear. Rate: 0.00482, Train Grad.: 122.3\n",
      "Epoch 3501/150000, Train Loss: 17632, Val Loss: 18033,  Lear. Rate: 0.00482, Train Grad.: 120.6\n",
      "Epoch 3601/150000, Train Loss: 17380, Val Loss: 17775,  Lear. Rate: 0.00483, Train Grad.: 118.9\n",
      "Epoch 3701/150000, Train Loss: 17132, Val Loss: 17521,  Lear. Rate: 0.00483, Train Grad.: 117.1\n",
      "Epoch 3801/150000, Train Loss: 16888, Val Loss: 17271,  Lear. Rate: 0.00484, Train Grad.: 115.5\n",
      "Epoch 3901/150000, Train Loss: 16647, Val Loss: 17026,  Lear. Rate: 0.00484, Train Grad.: 113.7\n",
      "Epoch 4001/150000, Train Loss: 16411, Val Loss: 16784,  Lear. Rate: 0.00485, Train Grad.: 112.0\n",
      "Epoch 4101/150000, Train Loss: 16178, Val Loss: 16547,  Lear. Rate: 0.00485, Train Grad.: 110.4\n",
      "Epoch 4201/150000, Train Loss: 15949, Val Loss: 16314,  Lear. Rate: 0.00486, Train Grad.: 108.7\n",
      "Epoch 4301/150000, Train Loss: 15724, Val Loss: 16085,  Lear. Rate: 0.00486, Train Grad.: 106.8\n",
      "Epoch 4401/150000, Train Loss: 15502, Val Loss: 15859,  Lear. Rate: 0.00486, Train Grad.: 105.4\n",
      "Epoch 4501/150000, Train Loss: 15284, Val Loss: 15637,  Lear. Rate: 0.00487, Train Grad.: 103.8\n",
      "Epoch 4601/150000, Train Loss: 15070, Val Loss: 15418,  Lear. Rate: 0.00487, Train Grad.: 102.2\n",
      "Epoch 4701/150000, Train Loss: 14859, Val Loss: 15202,  Lear. Rate: 0.00487, Train Grad.: 100.7\n",
      "Epoch 4801/150000, Train Loss: 14651, Val Loss: 14991,  Lear. Rate: 0.00488, Train Grad.: 99.2\n",
      "Epoch 4901/150000, Train Loss: 14446, Val Loss: 14782,  Lear. Rate: 0.00488, Train Grad.: 97.7\n",
      "Epoch 5001/150000, Train Loss: 14244, Val Loss: 14577,  Lear. Rate: 0.00488, Train Grad.: 96.3\n",
      "Epoch 5101/150000, Train Loss: 14046, Val Loss: 14375,  Lear. Rate: 0.00489, Train Grad.: 94.8\n",
      "Epoch 5201/150000, Train Loss: 13850, Val Loss: 14175,  Lear. Rate: 0.00489, Train Grad.: 93.4\n",
      "Epoch 5301/150000, Train Loss: 13658, Val Loss: 13980,  Lear. Rate: 0.00489, Train Grad.: 92.0\n",
      "Epoch 5401/150000, Train Loss: 13468, Val Loss: 13787,  Lear. Rate: 0.00490, Train Grad.: 90.6\n",
      "Epoch 5501/150000, Train Loss: 13282, Val Loss: 13597,  Lear. Rate: 0.00490, Train Grad.: 89.3\n",
      "Epoch 5601/150000, Train Loss: 13098, Val Loss: 13410,  Lear. Rate: 0.00490, Train Grad.: 88.0\n",
      "Epoch 5701/150000, Train Loss: 12916, Val Loss: 13225,  Lear. Rate: 0.00490, Train Grad.: 86.7\n",
      "Epoch 5801/150000, Train Loss: 12737, Val Loss: 13043,  Lear. Rate: 0.00491, Train Grad.: 85.5\n",
      "Epoch 5901/150000, Train Loss: 12561, Val Loss: 12864,  Lear. Rate: 0.00491, Train Grad.: 84.2\n",
      "Epoch 6001/150000, Train Loss: 12388, Val Loss: 12687,  Lear. Rate: 0.00491, Train Grad.: 83.0\n",
      "Epoch 6101/150000, Train Loss: 12216, Val Loss: 12513,  Lear. Rate: 0.00491, Train Grad.: 81.8\n",
      "Epoch 6201/150000, Train Loss: 12048, Val Loss: 12342,  Lear. Rate: 0.00492, Train Grad.: 80.6\n",
      "Epoch 6301/150000, Train Loss: 11881, Val Loss: 12172,  Lear. Rate: 0.00492, Train Grad.: 79.5\n",
      "Epoch 6401/150000, Train Loss: 11717, Val Loss: 12005,  Lear. Rate: 0.00492, Train Grad.: 78.3\n",
      "Epoch 6501/150000, Train Loss: 11555, Val Loss: 11841,  Lear. Rate: 0.00492, Train Grad.: 77.3\n",
      "Epoch 6601/150000, Train Loss: 11395, Val Loss: 11678,  Lear. Rate: 0.00493, Train Grad.: 76.1\n",
      "Epoch 6701/150000, Train Loss: 11237, Val Loss: 11518,  Lear. Rate: 0.00493, Train Grad.: 75.1\n",
      "Epoch 6801/150000, Train Loss: 11082, Val Loss: 11360,  Lear. Rate: 0.00493, Train Grad.: 74.1\n",
      "Epoch 6901/150000, Train Loss: 10928, Val Loss: 11204,  Lear. Rate: 0.00493, Train Grad.: 73.0\n",
      "Epoch 7001/150000, Train Loss: 10777, Val Loss: 11050,  Lear. Rate: 0.00493, Train Grad.: 71.9\n",
      "Epoch 7101/150000, Train Loss: 10628, Val Loss: 10899,  Lear. Rate: 0.00494, Train Grad.: 71.0\n",
      "Epoch 7201/150000, Train Loss: 10480, Val Loss: 10750,  Lear. Rate: 0.00494, Train Grad.: 70.4\n",
      "Epoch 7301/150000, Train Loss: 10334, Val Loss: 10602,  Lear. Rate: 0.00494, Train Grad.: 69.1\n",
      "Epoch 7401/150000, Train Loss: 10191, Val Loss: 10456,  Lear. Rate: 0.00494, Train Grad.: 68.2\n",
      "Epoch 7501/150000, Train Loss: 10049, Val Loss: 10312,  Lear. Rate: 0.00494, Train Grad.: 67.1\n",
      "Epoch 7601/150000, Train Loss: 9908, Val Loss: 10168,  Lear. Rate: 0.00494, Train Grad.: 66.4\n",
      "Epoch 7701/150000, Train Loss: 9770, Val Loss: 10027,  Lear. Rate: 0.00495, Train Grad.: 65.7\n",
      "Epoch 7801/150000, Train Loss: 9633, Val Loss: 9888,  Lear. Rate: 0.00495, Train Grad.: 64.6\n",
      "Epoch 7901/150000, Train Loss: 9497, Val Loss: 9750,  Lear. Rate: 0.00495, Train Grad.: 63.8\n",
      "Epoch 8001/150000, Train Loss: 9363, Val Loss: 9614,  Lear. Rate: 0.00495, Train Grad.: 62.9\n",
      "Epoch 8101/150000, Train Loss: 9231, Val Loss: 9480,  Lear. Rate: 0.00495, Train Grad.: 62.2\n",
      "Epoch 8201/150000, Train Loss: 9101, Val Loss: 9346,  Lear. Rate: 0.00495, Train Grad.: 61.5\n",
      "Epoch 8301/150000, Train Loss: 8971, Val Loss: 9215,  Lear. Rate: 0.00495, Train Grad.: 60.7\n",
      "Epoch 8401/150000, Train Loss: 8843, Val Loss: 9085,  Lear. Rate: 0.00495, Train Grad.: 60.0\n",
      "Epoch 8501/150000, Train Loss: 8717, Val Loss: 8956,  Lear. Rate: 0.00496, Train Grad.: 59.2\n",
      "Epoch 8601/150000, Train Loss: 8591, Val Loss: 8829,  Lear. Rate: 0.00496, Train Grad.: 58.5\n",
      "Epoch 8701/150000, Train Loss: 8467, Val Loss: 8703,  Lear. Rate: 0.00496, Train Grad.: 57.8\n",
      "Epoch 8801/150000, Train Loss: 8345, Val Loss: 8578,  Lear. Rate: 0.00496, Train Grad.: 57.2\n",
      "Epoch 8901/150000, Train Loss: 8223, Val Loss: 8455,  Lear. Rate: 0.00496, Train Grad.: 56.5\n",
      "Epoch 9001/150000, Train Loss: 8103, Val Loss: 8332,  Lear. Rate: 0.00496, Train Grad.: 56.1\n",
      "Epoch 9101/150000, Train Loss: 7984, Val Loss: 8211,  Lear. Rate: 0.00496, Train Grad.: 55.2\n",
      "Epoch 9201/150000, Train Loss: 7866, Val Loss: 8092,  Lear. Rate: 0.00496, Train Grad.: 54.5\n",
      "Epoch 9301/150000, Train Loss: 7749, Val Loss: 7973,  Lear. Rate: 0.00497, Train Grad.: 54.0\n",
      "Epoch 9401/150000, Train Loss: 7634, Val Loss: 7856,  Lear. Rate: 0.00497, Train Grad.: 53.4\n",
      "Epoch 9501/150000, Train Loss: 7519, Val Loss: 7740,  Lear. Rate: 0.00497, Train Grad.: 52.4\n",
      "Epoch 9601/150000, Train Loss: 7406, Val Loss: 7625,  Lear. Rate: 0.00497, Train Grad.: 52.2\n",
      "Epoch 9701/150000, Train Loss: 7294, Val Loss: 7511,  Lear. Rate: 0.00497, Train Grad.: 51.6\n",
      "Epoch 9801/150000, Train Loss: 7183, Val Loss: 7398,  Lear. Rate: 0.00497, Train Grad.: 51.2\n",
      "Epoch 9901/150000, Train Loss: 7073, Val Loss: 7287,  Lear. Rate: 0.00497, Train Grad.: 50.4\n",
      "Epoch 10001/150000, Train Loss: 6964, Val Loss: 7177,  Lear. Rate: 0.00497, Train Grad.: 50.1\n",
      "Epoch 10101/150000, Train Loss: 6857, Val Loss: 7068,  Lear. Rate: 0.00497, Train Grad.: 49.3\n",
      "Epoch 10201/150000, Train Loss: 6750, Val Loss: 6960,  Lear. Rate: 0.00497, Train Grad.: 48.7\n",
      "Epoch 10301/150000, Train Loss: 6645, Val Loss: 6853,  Lear. Rate: 0.00497, Train Grad.: 48.2\n",
      "Epoch 10401/150000, Train Loss: 6541, Val Loss: 6747,  Lear. Rate: 0.00498, Train Grad.: 47.7\n",
      "Epoch 10501/150000, Train Loss: 6437, Val Loss: 6642,  Lear. Rate: 0.00498, Train Grad.: 47.1\n",
      "Epoch 10601/150000, Train Loss: 6335, Val Loss: 6539,  Lear. Rate: 0.00498, Train Grad.: 46.6\n",
      "Epoch 10701/150000, Train Loss: 6234, Val Loss: 6436,  Lear. Rate: 0.00498, Train Grad.: 46.2\n",
      "Epoch 10801/150000, Train Loss: 6135, Val Loss: 6335,  Lear. Rate: 0.00498, Train Grad.: 45.5\n",
      "Epoch 10901/150000, Train Loss: 6036, Val Loss: 6235,  Lear. Rate: 0.00498, Train Grad.: 45.0\n",
      "Epoch 11001/150000, Train Loss: 5938, Val Loss: 6137,  Lear. Rate: 0.00498, Train Grad.: 44.5\n",
      "Epoch 11101/150000, Train Loss: 5842, Val Loss: 6039,  Lear. Rate: 0.00498, Train Grad.: 44.0\n",
      "Epoch 11201/150000, Train Loss: 5746, Val Loss: 5942,  Lear. Rate: 0.00498, Train Grad.: 43.5\n",
      "Epoch 11301/150000, Train Loss: 5652, Val Loss: 5847,  Lear. Rate: 0.00498, Train Grad.: 42.9\n",
      "Epoch 11401/150000, Train Loss: 5559, Val Loss: 5753,  Lear. Rate: 0.00498, Train Grad.: 42.5\n",
      "Epoch 11501/150000, Train Loss: 5467, Val Loss: 5660,  Lear. Rate: 0.00498, Train Grad.: 41.9\n",
      "Epoch 11601/150000, Train Loss: 5376, Val Loss: 5568,  Lear. Rate: 0.00498, Train Grad.: 41.5\n",
      "Epoch 11701/150000, Train Loss: 5287, Val Loss: 5477,  Lear. Rate: 0.00498, Train Grad.: 40.9\n",
      "Epoch 11801/150000, Train Loss: 5198, Val Loss: 5388,  Lear. Rate: 0.00498, Train Grad.: 40.4\n",
      "Epoch 11901/150000, Train Loss: 5111, Val Loss: 5300,  Lear. Rate: 0.00498, Train Grad.: 39.9\n",
      "Epoch 12001/150000, Train Loss: 5025, Val Loss: 5212,  Lear. Rate: 0.00499, Train Grad.: 39.4\n",
      "Epoch 12101/150000, Train Loss: 4939, Val Loss: 5127,  Lear. Rate: 0.00499, Train Grad.: 38.9\n",
      "Epoch 12201/150000, Train Loss: 4855, Val Loss: 5042,  Lear. Rate: 0.00499, Train Grad.: 38.5\n",
      "Epoch 12301/150000, Train Loss: 4772, Val Loss: 4958,  Lear. Rate: 0.00499, Train Grad.: 38.0\n",
      "Epoch 12401/150000, Train Loss: 4689, Val Loss: 4876,  Lear. Rate: 0.00499, Train Grad.: 37.2\n",
      "Epoch 12501/150000, Train Loss: 4608, Val Loss: 4794,  Lear. Rate: 0.00499, Train Grad.: 37.1\n",
      "Epoch 12601/150000, Train Loss: 4528, Val Loss: 4713,  Lear. Rate: 0.00499, Train Grad.: 36.7\n",
      "Epoch 12701/150000, Train Loss: 4448, Val Loss: 4634,  Lear. Rate: 0.00499, Train Grad.: 36.2\n",
      "Epoch 12801/150000, Train Loss: 4370, Val Loss: 4555,  Lear. Rate: 0.00499, Train Grad.: 35.7\n",
      "Epoch 12901/150000, Train Loss: 4293, Val Loss: 4478,  Lear. Rate: 0.00499, Train Grad.: 35.3\n",
      "Epoch 13001/150000, Train Loss: 4217, Val Loss: 4402,  Lear. Rate: 0.00499, Train Grad.: 34.6\n",
      "Epoch 13101/150000, Train Loss: 4141, Val Loss: 4326,  Lear. Rate: 0.00499, Train Grad.: 34.4\n",
      "Epoch 13201/150000, Train Loss: 4067, Val Loss: 4251,  Lear. Rate: 0.00499, Train Grad.: 34.1\n",
      "Epoch 13301/150000, Train Loss: 3994, Val Loss: 4177,  Lear. Rate: 0.00499, Train Grad.: 33.6\n",
      "Epoch 13401/150000, Train Loss: 3922, Val Loss: 4105,  Lear. Rate: 0.00499, Train Grad.: 33.1\n",
      "Epoch 13501/150000, Train Loss: 3850, Val Loss: 4033,  Lear. Rate: 0.00499, Train Grad.: 32.7\n",
      "Epoch 13601/150000, Train Loss: 3780, Val Loss: 3962,  Lear. Rate: 0.00499, Train Grad.: 32.2\n",
      "Epoch 13701/150000, Train Loss: 3710, Val Loss: 3893,  Lear. Rate: 0.00499, Train Grad.: 31.8\n",
      "Epoch 13801/150000, Train Loss: 3642, Val Loss: 3824,  Lear. Rate: 0.00499, Train Grad.: 31.4\n",
      "Epoch 13901/150000, Train Loss: 3575, Val Loss: 3756,  Lear. Rate: 0.00499, Train Grad.: 31.1\n",
      "Epoch 14001/150000, Train Loss: 3508, Val Loss: 3690,  Lear. Rate: 0.00499, Train Grad.: 30.6\n",
      "Epoch 14101/150000, Train Loss: 3443, Val Loss: 3624,  Lear. Rate: 0.00499, Train Grad.: 30.1\n",
      "Epoch 14201/150000, Train Loss: 3378, Val Loss: 3559,  Lear. Rate: 0.00499, Train Grad.: 29.6\n",
      "Epoch 14301/150000, Train Loss: 3314, Val Loss: 3495,  Lear. Rate: 0.00499, Train Grad.: 29.3\n",
      "Epoch 14401/150000, Train Loss: 3252, Val Loss: 3432,  Lear. Rate: 0.00499, Train Grad.: 29.0\n",
      "Epoch 14501/150000, Train Loss: 3190, Val Loss: 3370,  Lear. Rate: 0.00499, Train Grad.: 28.4\n",
      "Epoch 14601/150000, Train Loss: 3129, Val Loss: 3308,  Lear. Rate: 0.00499, Train Grad.: 28.0\n",
      "Epoch 14701/150000, Train Loss: 3070, Val Loss: 3248,  Lear. Rate: 0.00499, Train Grad.: 27.7\n",
      "Epoch 14801/150000, Train Loss: 3010, Val Loss: 3188,  Lear. Rate: 0.00499, Train Grad.: 27.3\n",
      "Epoch 14901/150000, Train Loss: 2952, Val Loss: 3129,  Lear. Rate: 0.00499, Train Grad.: 27.0\n",
      "Epoch 15001/150000, Train Loss: 2895, Val Loss: 3070,  Lear. Rate: 0.00499, Train Grad.: 26.5\n",
      "Epoch 15101/150000, Train Loss: 2839, Val Loss: 3013,  Lear. Rate: 0.00500, Train Grad.: 26.1\n",
      "Epoch 15201/150000, Train Loss: 2783, Val Loss: 2957,  Lear. Rate: 0.00500, Train Grad.: 25.9\n",
      "Epoch 15301/150000, Train Loss: 2728, Val Loss: 2902,  Lear. Rate: 0.00500, Train Grad.: 25.4\n",
      "Epoch 15401/150000, Train Loss: 2674, Val Loss: 2848,  Lear. Rate: 0.00500, Train Grad.: 25.1\n",
      "Epoch 15501/150000, Train Loss: 2621, Val Loss: 2795,  Lear. Rate: 0.00500, Train Grad.: 24.7\n",
      "Epoch 15601/150000, Train Loss: 2569, Val Loss: 2743,  Lear. Rate: 0.00500, Train Grad.: 24.4\n",
      "Epoch 15701/150000, Train Loss: 2518, Val Loss: 2691,  Lear. Rate: 0.00500, Train Grad.: 24.0\n",
      "Epoch 15801/150000, Train Loss: 2467, Val Loss: 2641,  Lear. Rate: 0.00500, Train Grad.: 23.7\n",
      "Epoch 15901/150000, Train Loss: 2418, Val Loss: 2591,  Lear. Rate: 0.00500, Train Grad.: 23.6\n",
      "Epoch 16001/150000, Train Loss: 2369, Val Loss: 2543,  Lear. Rate: 0.00500, Train Grad.: 22.9\n",
      "Epoch 16101/150000, Train Loss: 2321, Val Loss: 2495,  Lear. Rate: 0.00500, Train Grad.: 22.5\n",
      "Epoch 16201/150000, Train Loss: 2273, Val Loss: 2448,  Lear. Rate: 0.00500, Train Grad.: 22.2\n",
      "Epoch 16301/150000, Train Loss: 2226, Val Loss: 2401,  Lear. Rate: 0.00500, Train Grad.: 22.2\n",
      "Epoch 16401/150000, Train Loss: 2180, Val Loss: 2356,  Lear. Rate: 0.00500, Train Grad.: 21.7\n",
      "Epoch 16501/150000, Train Loss: 2134, Val Loss: 2311,  Lear. Rate: 0.00500, Train Grad.: 21.3\n",
      "Epoch 16601/150000, Train Loss: 2090, Val Loss: 2267,  Lear. Rate: 0.00500, Train Grad.: 21.1\n",
      "Epoch 16701/150000, Train Loss: 2045, Val Loss: 2223,  Lear. Rate: 0.00500, Train Grad.: 20.8\n",
      "Epoch 16801/150000, Train Loss: 2002, Val Loss: 2180,  Lear. Rate: 0.00500, Train Grad.: 20.4\n",
      "Epoch 16901/150000, Train Loss: 1959, Val Loss: 2138,  Lear. Rate: 0.00500, Train Grad.: 20.1\n",
      "Epoch 17001/150000, Train Loss: 1917, Val Loss: 2096,  Lear. Rate: 0.00500, Train Grad.: 19.8\n",
      "Epoch 17101/150000, Train Loss: 1876, Val Loss: 2055,  Lear. Rate: 0.00500, Train Grad.: 19.4\n",
      "Epoch 17201/150000, Train Loss: 1835, Val Loss: 2015,  Lear. Rate: 0.00500, Train Grad.: 19.2\n",
      "Epoch 17301/150000, Train Loss: 1796, Val Loss: 1977,  Lear. Rate: 0.00500, Train Grad.: 19.0\n",
      "Epoch 17401/150000, Train Loss: 1756, Val Loss: 1937,  Lear. Rate: 0.00500, Train Grad.: 18.5\n",
      "Epoch 17501/150000, Train Loss: 1717, Val Loss: 1899,  Lear. Rate: 0.00500, Train Grad.: 18.2\n",
      "Epoch 17601/150000, Train Loss: 1679, Val Loss: 1861,  Lear. Rate: 0.00500, Train Grad.: 18.1\n",
      "Epoch 17701/150000, Train Loss: 1642, Val Loss: 1824,  Lear. Rate: 0.00500, Train Grad.: 17.7\n",
      "Epoch 17801/150000, Train Loss: 1605, Val Loss: 1788,  Lear. Rate: 0.00500, Train Grad.: 17.4\n",
      "Epoch 17901/150000, Train Loss: 1570, Val Loss: 1753,  Lear. Rate: 0.00500, Train Grad.: 17.1\n",
      "Epoch 18001/150000, Train Loss: 1534, Val Loss: 1718,  Lear. Rate: 0.00500, Train Grad.: 16.8\n",
      "Epoch 18101/150000, Train Loss: 1500, Val Loss: 1684,  Lear. Rate: 0.00500, Train Grad.: 16.7\n",
      "Epoch 18201/150000, Train Loss: 1466, Val Loss: 1651,  Lear. Rate: 0.00500, Train Grad.: 16.2\n",
      "Epoch 18301/150000, Train Loss: 1432, Val Loss: 1618,  Lear. Rate: 0.00500, Train Grad.: 15.9\n",
      "Epoch 18401/150000, Train Loss: 1399, Val Loss: 1586,  Lear. Rate: 0.00500, Train Grad.: 15.8\n",
      "Epoch 18501/150000, Train Loss: 1367, Val Loss: 1555,  Lear. Rate: 0.00500, Train Grad.: 15.5\n",
      "Epoch 18601/150000, Train Loss: 1335, Val Loss: 1524,  Lear. Rate: 0.00500, Train Grad.: 15.2\n",
      "Epoch 18701/150000, Train Loss: 1304, Val Loss: 1494,  Lear. Rate: 0.00500, Train Grad.: 14.9\n",
      "Epoch 18801/150000, Train Loss: 1273, Val Loss: 1464,  Lear. Rate: 0.00500, Train Grad.: 14.6\n",
      "Epoch 18901/150000, Train Loss: 1243, Val Loss: 1435,  Lear. Rate: 0.00500, Train Grad.: 14.9\n",
      "Epoch 19001/150000, Train Loss: 1214, Val Loss: 1407,  Lear. Rate: 0.00500, Train Grad.: 14.2\n",
      "Epoch 19101/150000, Train Loss: 1184, Val Loss: 1376,  Lear. Rate: 0.00500, Train Grad.: 14.0\n",
      "Epoch 19201/150000, Train Loss: 1156, Val Loss: 1349,  Lear. Rate: 0.00500, Train Grad.: 13.7\n",
      "Epoch 19301/150000, Train Loss: 1128, Val Loss: 1322,  Lear. Rate: 0.00500, Train Grad.: 13.5\n",
      "Epoch 19401/150000, Train Loss: 1100, Val Loss: 1296,  Lear. Rate: 0.00500, Train Grad.: 13.2\n",
      "Epoch 19501/150000, Train Loss: 1073, Val Loss: 1271,  Lear. Rate: 0.00500, Train Grad.: 13.0\n",
      "Epoch 19601/150000, Train Loss: 1047, Val Loss: 1246,  Lear. Rate: 0.00500, Train Grad.: 12.4\n",
      "Epoch 19701/150000, Train Loss: 1021, Val Loss: 1221,  Lear. Rate: 0.00500, Train Grad.: 12.5\n",
      "Epoch 19801/150000, Train Loss: 996, Val Loss: 1197,  Lear. Rate: 0.00500, Train Grad.: 12.8\n",
      "Epoch 19901/150000, Train Loss: 971, Val Loss: 1174,  Lear. Rate: 0.00500, Train Grad.: 12.1\n",
      "Epoch 20001/150000, Train Loss: 947, Val Loss: 1152,  Lear. Rate: 0.00500, Train Grad.: 11.9\n",
      "Epoch 20101/150000, Train Loss: 923, Val Loss: 1130,  Lear. Rate: 0.00500, Train Grad.: 11.6\n",
      "Epoch 20201/150000, Train Loss: 899, Val Loss: 1111,  Lear. Rate: 0.00500, Train Grad.: 11.3\n",
      "Epoch 20301/150000, Train Loss: 876, Val Loss: 1087,  Lear. Rate: 0.00500, Train Grad.: 11.2\n",
      "Epoch 20401/150000, Train Loss: 854, Val Loss: 1063,  Lear. Rate: 0.00500, Train Grad.: 10.9\n",
      "Epoch 20501/150000, Train Loss: 832, Val Loss: 1043,  Lear. Rate: 0.00500, Train Grad.: 10.8\n",
      "Epoch 20601/150000, Train Loss: 811, Val Loss: 1023,  Lear. Rate: 0.00500, Train Grad.: 10.6\n",
      "Epoch 20701/150000, Train Loss: 790, Val Loss: 1004,  Lear. Rate: 0.00500, Train Grad.: 10.4\n",
      "Epoch 20801/150000, Train Loss: 769, Val Loss: 986,  Lear. Rate: 0.00500, Train Grad.: 10.6\n",
      "Epoch 20901/150000, Train Loss: 749, Val Loss: 967,  Lear. Rate: 0.00500, Train Grad.: 10.0\n",
      "Epoch 21001/150000, Train Loss: 729, Val Loss: 949,  Lear. Rate: 0.00500, Train Grad.: 9.8\n",
      "Epoch 21101/150000, Train Loss: 710, Val Loss: 932,  Lear. Rate: 0.00500, Train Grad.: 9.7\n",
      "Epoch 21201/150000, Train Loss: 691, Val Loss: 914,  Lear. Rate: 0.00500, Train Grad.: 9.4\n",
      "Epoch 21301/150000, Train Loss: 672, Val Loss: 898,  Lear. Rate: 0.00500, Train Grad.: 9.1\n",
      "Epoch 21401/150000, Train Loss: 654, Val Loss: 882,  Lear. Rate: 0.00500, Train Grad.: 9.0\n",
      "Epoch 21501/150000, Train Loss: 635, Val Loss: 866,  Lear. Rate: 0.00500, Train Grad.: 8.9\n",
      "Epoch 21601/150000, Train Loss: 616, Val Loss: 850,  Lear. Rate: 0.00500, Train Grad.: 8.7\n",
      "Epoch 21701/150000, Train Loss: 598, Val Loss: 835,  Lear. Rate: 0.00500, Train Grad.: 8.5\n",
      "Epoch 21801/150000, Train Loss: 580, Val Loss: 820,  Lear. Rate: 0.00500, Train Grad.: 8.3\n",
      "Epoch 21901/150000, Train Loss: 563, Val Loss: 805,  Lear. Rate: 0.00500, Train Grad.: 8.1\n",
      "Epoch 22001/150000, Train Loss: 547, Val Loss: 791,  Lear. Rate: 0.00500, Train Grad.: 8.0\n",
      "Epoch 22101/150000, Train Loss: 532, Val Loss: 776,  Lear. Rate: 0.00500, Train Grad.: 8.0\n",
      "Epoch 22201/150000, Train Loss: 517, Val Loss: 763,  Lear. Rate: 0.00500, Train Grad.: 7.8\n",
      "Epoch 22301/150000, Train Loss: 509, Val Loss: 749,  Lear. Rate: 0.00500, Train Grad.: 7.5\n",
      "Epoch 22401/150000, Train Loss: 494, Val Loss: 733,  Lear. Rate: 0.00500, Train Grad.: 7.8\n",
      "Epoch 22501/150000, Train Loss: 474, Val Loss: 724,  Lear. Rate: 0.00500, Train Grad.: 6.9\n",
      "Epoch 22601/150000, Train Loss: 460, Val Loss: 712,  Lear. Rate: 0.00500, Train Grad.: 6.8\n",
      "Epoch 22701/150000, Train Loss: 447, Val Loss: 700,  Lear. Rate: 0.00500, Train Grad.: 6.8\n",
      "Epoch 22801/150000, Train Loss: 434, Val Loss: 689,  Lear. Rate: 0.00500, Train Grad.: 6.6\n",
      "Epoch 22901/150000, Train Loss: 421, Val Loss: 678,  Lear. Rate: 0.00500, Train Grad.: 6.5\n",
      "Epoch 23001/150000, Train Loss: 409, Val Loss: 667,  Lear. Rate: 0.00500, Train Grad.: 6.4\n",
      "Epoch 23101/150000, Train Loss: 397, Val Loss: 656,  Lear. Rate: 0.00500, Train Grad.: 6.2\n",
      "Epoch 23201/150000, Train Loss: 385, Val Loss: 646,  Lear. Rate: 0.00500, Train Grad.: 6.0\n",
      "Epoch 23301/150000, Train Loss: 374, Val Loss: 636,  Lear. Rate: 0.00500, Train Grad.: 5.9\n",
      "Epoch 23401/150000, Train Loss: 362, Val Loss: 626,  Lear. Rate: 0.00500, Train Grad.: 5.7\n",
      "Epoch 23501/150000, Train Loss: 352, Val Loss: 617,  Lear. Rate: 0.00500, Train Grad.: 5.8\n",
      "Epoch 23601/150000, Train Loss: 341, Val Loss: 608,  Lear. Rate: 0.00500, Train Grad.: 5.4\n",
      "Epoch 23701/150000, Train Loss: 331, Val Loss: 600,  Lear. Rate: 0.00500, Train Grad.: 4.9\n",
      "Epoch 23801/150000, Train Loss: 321, Val Loss: 592,  Lear. Rate: 0.00500, Train Grad.: 5.1\n",
      "Epoch 23901/150000, Train Loss: 312, Val Loss: 584,  Lear. Rate: 0.00500, Train Grad.: 5.0\n",
      "Epoch 24001/150000, Train Loss: 303, Val Loss: 576,  Lear. Rate: 0.00500, Train Grad.: 4.7\n",
      "Epoch 24101/150000, Train Loss: 294, Val Loss: 569,  Lear. Rate: 0.00500, Train Grad.: 4.7\n",
      "Epoch 24201/150000, Train Loss: 285, Val Loss: 562,  Lear. Rate: 0.00500, Train Grad.: 4.6\n",
      "Epoch 24301/150000, Train Loss: 277, Val Loss: 554,  Lear. Rate: 0.00500, Train Grad.: 4.4\n",
      "Epoch 24401/150000, Train Loss: 277, Val Loss: 545,  Lear. Rate: 0.00500, Train Grad.: 4.6\n",
      "Epoch 24501/150000, Train Loss: 268, Val Loss: 542,  Lear. Rate: 0.00500, Train Grad.: 4.3\n",
      "Epoch 24601/150000, Train Loss: 261, Val Loss: 537,  Lear. Rate: 0.00500, Train Grad.: 4.1\n",
      "Epoch 24701/150000, Train Loss: 253, Val Loss: 531,  Lear. Rate: 0.00500, Train Grad.: 4.0\n",
      "Epoch 24801/150000, Train Loss: 246, Val Loss: 528,  Lear. Rate: 0.00500, Train Grad.: 3.9\n",
      "Epoch 24901/150000, Train Loss: 239, Val Loss: 523,  Lear. Rate: 0.00500, Train Grad.: 3.8\n",
      "Epoch 25001/150000, Train Loss: 232, Val Loss: 518,  Lear. Rate: 0.00500, Train Grad.: 3.6\n",
      "Epoch 25101/150000, Train Loss: 225, Val Loss: 512,  Lear. Rate: 0.00500, Train Grad.: 3.5\n",
      "Epoch 25201/150000, Train Loss: 219, Val Loss: 504,  Lear. Rate: 0.00500, Train Grad.: 3.4\n",
      "Epoch 25301/150000, Train Loss: 212, Val Loss: 498,  Lear. Rate: 0.00500, Train Grad.: 3.3\n",
      "Epoch 25401/150000, Train Loss: 207, Val Loss: 483,  Lear. Rate: 0.00500, Train Grad.: 3.7\n",
      "Early stopping at epoch 25434 with validation loss 500.00689697265625.\n",
      "Test Loss: 474.0687561035156\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np  # Added this line\n",
    "\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "\n",
    "learning_rates = [0.005]\n",
    "\n",
    "window_sizes = [15, 20, 25]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "factors = [0.5]\n",
    "patience_lr = [10]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "def set_random_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    set_random_seeds(42)\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Lear. Rate: {current_lr:.5f}, Train Grad.: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
