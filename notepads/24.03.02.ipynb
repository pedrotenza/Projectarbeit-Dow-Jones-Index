{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "#print(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "y_data = data[\"close\"].values.reshape(-1, 1)  # Reshape y_data if it's 1D\n",
    "\n",
    "# Initialize the scalers\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both training and test data\n",
    "x_data_scaled = scaler_x.fit_transform(x_data)\n",
    "y_data_scaled = scaler_y.fit_transform(y_data)\n",
    "\n",
    "# Convert scaled data to Tensors\n",
    "x_feature_tensors = torch.tensor(x_data_scaled, dtype=torch.float32)\n",
    "y_feature_tensors = torch.tensor(y_data_scaled, dtype=torch.float32)\n",
    "\n",
    "# Split the training data into training and temporary sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_feature_tensors, y_feature_tensors, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert scaled labels back to numpy arrays\n",
    "y_train = y_train.numpy()\n",
    "y_val = y_val.numpy()\n",
    "y_test = y_test.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Now you can use x_train_scaled, x_val, x_test, y_train_scaled, y_val_scaled, y_test_scaled for your model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 4\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_9356\\3789981942.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_9356\\3789981942.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000, Train Loss: 1.0508795976638794, Validation Loss: 2.2516603469848633\n",
      "Iteration 2000, Train Loss: 2.080430507659912, Validation Loss: 2.2376272678375244\n",
      "Iteration 3000, Train Loss: 0.23619112372398376, Validation Loss: 2.238978147506714\n",
      "Iteration 4000, Train Loss: 0.3774866461753845, Validation Loss: 2.219364881515503\n",
      "Iteration 5000, Train Loss: 0.3024561405181885, Validation Loss: 2.222336769104004\n",
      "Iteration 6000, Train Loss: 0.16863176226615906, Validation Loss: 2.225689172744751\n",
      "Iteration 7000, Train Loss: 0.228369802236557, Validation Loss: 2.2307190895080566\n",
      "Iteration 8000, Train Loss: 0.3323546051979065, Validation Loss: 2.2233834266662598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_9356\\3789981942.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.41863125562667847\n",
      "Iteration 1000, Train Loss: 1.049631953239441, Validation Loss: 2.2504501342773438\n",
      "Iteration 2000, Train Loss: 2.0760912895202637, Validation Loss: 2.2360541820526123\n",
      "Iteration 3000, Train Loss: 0.2388816773891449, Validation Loss: 2.237497329711914\n",
      "Iteration 4000, Train Loss: 0.3783760666847229, Validation Loss: 2.2188658714294434\n",
      "Iteration 5000, Train Loss: 0.3043081760406494, Validation Loss: 2.221346616744995\n",
      "Iteration 6000, Train Loss: 0.16978013515472412, Validation Loss: 2.224794864654541\n",
      "Iteration 7000, Train Loss: 0.22999408841133118, Validation Loss: 2.2298755645751953\n",
      "Iteration 8000, Train Loss: 0.3340439796447754, Validation Loss: 2.2227566242218018\n",
      "Test Loss: 0.4180432856082916\n",
      "Iteration 1000, Train Loss: 1.0496667623519897, Validation Loss: 2.250458240509033\n",
      "Iteration 2000, Train Loss: 2.0759952068328857, Validation Loss: 2.236020088195801\n",
      "Iteration 3000, Train Loss: 0.2388318032026291, Validation Loss: 2.237531900405884\n",
      "Iteration 4000, Train Loss: 0.3782051205635071, Validation Loss: 2.2189598083496094\n",
      "Iteration 5000, Train Loss: 0.3042157292366028, Validation Loss: 2.221393585205078\n",
      "Iteration 6000, Train Loss: 0.16932162642478943, Validation Loss: 2.225146770477295\n",
      "Iteration 7000, Train Loss: 0.23002171516418457, Validation Loss: 2.2298598289489746\n",
      "Iteration 8000, Train Loss: 0.33350133895874023, Validation Loss: 2.222952127456665\n",
      "Test Loss: 0.41783007979393005\n",
      "Iteration 1000, Train Loss: 1.0503581762313843, Validation Loss: 2.2510976791381836\n",
      "Iteration 2000, Train Loss: 2.0783987045288086, Validation Loss: 2.2368903160095215\n",
      "Iteration 3000, Train Loss: 0.23705020546913147, Validation Loss: 2.2385125160217285\n",
      "Iteration 4000, Train Loss: 0.3780849575996399, Validation Loss: 2.2190334796905518\n",
      "Iteration 5000, Train Loss: 0.3031507134437561, Validation Loss: 2.221956729888916\n",
      "Iteration 6000, Train Loss: 0.16871199011802673, Validation Loss: 2.22562575340271\n",
      "Iteration 7000, Train Loss: 0.22832337021827698, Validation Loss: 2.2307441234588623\n",
      "Iteration 8000, Train Loss: 0.3323828876018524, Validation Loss: 2.223383665084839\n",
      "Test Loss: 0.41908735036849976\n",
      "Iteration 1000, Train Loss: 1.0514436960220337, Validation Loss: 2.2521328926086426\n",
      "Iteration 2000, Train Loss: 2.080782413482666, Validation Loss: 2.2377572059631348\n",
      "Iteration 3000, Train Loss: 0.23573875427246094, Validation Loss: 2.2392373085021973\n",
      "Iteration 4000, Train Loss: 0.37792983651161194, Validation Loss: 2.2191219329833984\n",
      "Iteration 5000, Train Loss: 0.30263012647628784, Validation Loss: 2.2222373485565186\n",
      "Iteration 6000, Train Loss: 0.1684584617614746, Validation Loss: 2.2258248329162598\n",
      "Iteration 7000, Train Loss: 0.22815188765525818, Validation Loss: 2.2308335304260254\n",
      "Iteration 8000, Train Loss: 0.33191707730293274, Validation Loss: 2.223555088043213\n",
      "Test Loss: 0.41905274987220764\n",
      "Iteration 1000, Train Loss: 1.0505702495574951, Validation Loss: 2.2514028549194336\n",
      "Iteration 2000, Train Loss: 2.080781936645508, Validation Loss: 2.237753391265869\n",
      "Iteration 3000, Train Loss: 0.236008882522583, Validation Loss: 2.2390716075897217\n",
      "Iteration 4000, Train Loss: 0.3766481876373291, Validation Loss: 2.219831705093384\n",
      "Iteration 5000, Train Loss: 0.3019786477088928, Validation Loss: 2.222602367401123\n",
      "Iteration 6000, Train Loss: 0.1681338995695114, Validation Loss: 2.2260799407958984\n",
      "Iteration 7000, Train Loss: 0.22804808616638184, Validation Loss: 2.230886220932007\n",
      "Iteration 8000, Train Loss: 0.33082127571105957, Validation Loss: 2.22395396232605\n",
      "Test Loss: 0.41878801584243774\n",
      "Iteration 1000, Train Loss: 1.0487892627716064, Validation Loss: 2.2495782375335693\n",
      "Iteration 2000, Train Loss: 2.073060989379883, Validation Loss: 2.2349674701690674\n",
      "Iteration 3000, Train Loss: 0.24031415581703186, Validation Loss: 2.236733913421631\n",
      "Iteration 4000, Train Loss: 0.3793935179710388, Validation Loss: 2.218306541442871\n",
      "Iteration 5000, Train Loss: 0.30572816729545593, Validation Loss: 2.220587968826294\n",
      "Iteration 6000, Train Loss: 0.17057709395885468, Validation Loss: 2.224182605743408\n",
      "Iteration 7000, Train Loss: 0.23112674057483673, Validation Loss: 2.2292938232421875\n",
      "Iteration 8000, Train Loss: 0.335351824760437, Validation Loss: 2.222273349761963\n",
      "Test Loss: 0.41758137941360474\n",
      "Iteration 1000, Train Loss: 1.0492088794708252, Validation Loss: 2.2498068809509277\n",
      "Iteration 2000, Train Loss: 2.0724732875823975, Validation Loss: 2.2347638607025146\n",
      "Iteration 3000, Train Loss: 0.2396775782108307, Validation Loss: 2.237105369567871\n",
      "Iteration 4000, Train Loss: 0.3813464641571045, Validation Loss: 2.2172656059265137\n",
      "Iteration 5000, Train Loss: 0.3064664602279663, Validation Loss: 2.2201809883117676\n",
      "Iteration 6000, Train Loss: 0.17089924216270447, Validation Loss: 2.2239418029785156\n",
      "Iteration 7000, Train Loss: 0.23052725195884705, Validation Loss: 2.2296061515808105\n",
      "Iteration 8000, Train Loss: 0.3371061086654663, Validation Loss: 2.2216579914093018\n",
      "Test Loss: 0.41844281554222107\n",
      "Iteration 1000, Train Loss: 1.0476775169372559, Validation Loss: 2.24845814704895\n",
      "Iteration 2000, Train Loss: 2.0687320232391357, Validation Loss: 2.2334258556365967\n",
      "Iteration 3000, Train Loss: 0.24258849024772644, Validation Loss: 2.23553204536438\n",
      "Iteration 4000, Train Loss: 0.380461722612381, Validation Loss: 2.217724323272705\n",
      "Iteration 5000, Train Loss: 0.3074724078178406, Validation Loss: 2.2196784019470215\n",
      "Iteration 6000, Train Loss: 0.1711597740650177, Validation Loss: 2.223738193511963\n",
      "Iteration 7000, Train Loss: 0.23248347640037537, Validation Loss: 2.228605270385742\n",
      "Iteration 8000, Train Loss: 0.3370974361896515, Validation Loss: 2.2216391563415527\n",
      "Test Loss: 0.41719889640808105\n",
      "Iteration 1000, Train Loss: 1.0500855445861816, Validation Loss: 2.250894069671631\n",
      "Iteration 2000, Train Loss: 2.0781140327453613, Validation Loss: 2.2367844581604004\n",
      "Iteration 3000, Train Loss: 0.2377208024263382, Validation Loss: 2.2381298542022705\n",
      "Iteration 4000, Train Loss: 0.3775426745414734, Validation Loss: 2.2193281650543213\n",
      "Iteration 5000, Train Loss: 0.3033522963523865, Validation Loss: 2.2218575477600098\n",
      "Iteration 6000, Train Loss: 0.16908152401447296, Validation Loss: 2.225334644317627\n",
      "Iteration 7000, Train Loss: 0.22957921028137207, Validation Loss: 2.2300877571105957\n",
      "Iteration 8000, Train Loss: 0.33269649744033813, Validation Loss: 2.2232489585876465\n",
      "Test Loss: 0.4180065989494324\n",
      "Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 0.41719889640808105}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled, x_test_scaled, y_test_scaled,\n",
    "# input_size, output_size, test_window_size, scaler are available\n",
    "\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'learning_rate': [0.00005],\n",
    "    'window_size': [ 5],\n",
    "    'hidden_dim': [256],\n",
    "    'n_layers': [11],\n",
    "    'batch_evaluation_frequency': [4]\n",
    "}\n",
    "\n",
    "#Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 0.41702309250831604}\n",
    "#              'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 6463.20556640625}\n",
    "\n",
    "\n",
    "# Number of random search iterations\n",
    "num_iterations = 10\n",
    "\n",
    "best_params = None\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    params = {\n",
    "        'learning_rate': random.choice(search_space['learning_rate']),\n",
    "        'window_size': random.choice(search_space['window_size']),\n",
    "        'hidden_dim': random.choice(search_space['hidden_dim']),\n",
    "        'n_layers': random.choice(search_space['n_layers']),\n",
    "        'batch_evaluation_frequency': random.choice(search_space['batch_evaluation_frequency'])\n",
    "    }\n",
    "\n",
    "    model = LSTMModel(input_size, params['hidden_dim'], params['n_layers'], output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Training using Walk-Forward Validation\n",
    "    for i in range(params['window_size'], len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_idx = i - params['window_size']\n",
    "        end_idx = i\n",
    "        x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        y_window = torch.tensor(y_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        x_window = x_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        outputs, hidden = model(x_window, hidden)\n",
    "        loss = criterion(outputs, y_window)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % params['batch_evaluation_frequency'] == 0:\n",
    "            with torch.no_grad():\n",
    "                x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n",
    "                y_val_window = torch.tensor(y_val[:params['window_size']], dtype=torch.float32)\n",
    "                x_val_window = x_val_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "                hidden = model.init_hidden(1)\n",
    "                val_outputs, _ = model(x_val_window, hidden)\n",
    "                val_loss = criterion(val_outputs, y_val_window)\n",
    "\n",
    "                if i % 1000 == 0:  # Print every 1000 iterations\n",
    "                    print(f\"Iteration {i}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # Test set evaluation\n",
    "    with torch.no_grad():\n",
    "        x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n",
    "        y_test_window = torch.tensor(y_test[:params['window_size']], dtype=torch.float32)\n",
    "        x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        test_outputs, _ = model(x_test_window, hidden)\n",
    "        test_loss = criterion(test_outputs, y_test_window)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "    # Update the best_params if the current model performs better on the test set\n",
    "    if best_params is None:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "    elif test_loss < best_params['test_loss']:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of last_window: torch.Size([1, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of last_window:\", last_window.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size: 1\n",
    "Time steps (window size): 5\n",
    "Features (input size): 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Training Window:\n",
      "Input Features:\n",
      "tensor([[[-0.4585, -0.4632, -0.4611,  0.2122, -0.4199, -0.3131,  0.9702],\n",
      "         [-0.6824, -0.6819, -0.6794, -0.5100, -0.4514,  0.6253, -0.4263],\n",
      "         [-0.6759, -0.6739, -0.6726, -0.4536, -0.4510,  0.2627,  0.1182],\n",
      "         [-0.6574, -0.6620, -0.6573, -0.4358, -0.4505, -0.6401, -0.1456],\n",
      "         [-0.6137, -0.6046, -0.6089, -0.6418, -0.4574,  1.9724, -0.3608]]])\n",
      "True Output:\n",
      "tensor([[-0.4617],\n",
      "        [-0.6794],\n",
      "        [-0.6724],\n",
      "        [-0.6614],\n",
      "        [-0.6015]])\n"
     ]
    }
   ],
   "source": [
    "# Print the last window\n",
    "print(\"Last Training Window:\")\n",
    "print(\"Input Features:\")\n",
    "print(x_window)\n",
    "print(\"True Output:\")\n",
    "print(y_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Training Window in Original Scale:\n",
      "Input Features:\n",
      "[[ 5.1718800e+01  5.1928802e+01  5.0551201e+01  2.4210100e+07\n",
      "   1.5550994e+00 -7.6999998e-01  5.8811258e+08]\n",
      " [ 2.2198395e+01  2.2847994e+01  2.2052801e+01  4.6991990e+06\n",
      "   3.4270084e-01  1.8700000e+00  2.0789803e+08]\n",
      " [ 2.3049606e+01  2.3911997e+01  2.2937605e+01  6.2216000e+06\n",
      "   3.5679996e-01  8.5000002e-01  3.5614374e+08]\n",
      " [ 2.5491196e+01  2.5491205e+01  2.4931196e+01  6.7037000e+06\n",
      "   3.7870014e-01 -1.6900002e+00  2.8430387e+08]\n",
      " [ 3.1248001e+01  3.3129597e+01  3.1247999e+01  1.1384001e+06\n",
      "   1.1379994e-01  5.6599998e+00  2.2571920e+08]]\n",
      "True Output:\n",
      "[[51.2988  ]\n",
      " [22.6128  ]\n",
      " [23.542404]\n",
      " [24.981598]\n",
      " [32.883194]]\n"
     ]
    }
   ],
   "source": [
    "# Before the loop where x_window and y_window are defined\n",
    "\n",
    "# Create inverse scalers\n",
    "inverse_scaler_x = StandardScaler()\n",
    "inverse_scaler_y = StandardScaler()\n",
    "\n",
    "# Fit inverse scalers to the original data\n",
    "inverse_scaler_x.fit(x_data)\n",
    "inverse_scaler_y.fit(y_data)\n",
    "\n",
    "# Inside the loop after defining x_window and y_window\n",
    "\n",
    "# Convert x_window and y_window to numpy arrays\n",
    "x_window_np = x_window.view(-1, input_size).numpy()\n",
    "y_window_np = y_window.numpy()\n",
    "\n",
    "# Inverse transform to original scale\n",
    "x_window_original_scale = inverse_scaler_x.inverse_transform(x_window_np)\n",
    "y_window_original_scale = inverse_scaler_y.inverse_transform(y_window_np)\n",
    "\n",
    "# Print the last training window in the original scale\n",
    "print(\"Last Training Window in Original Scale:\")\n",
    "print(\"Input Features:\")\n",
    "print(x_window_original_scale)\n",
    "print(\"True Output:\")\n",
    "print(y_window_original_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m x_window_original_scale \u001b[38;5;241m=\u001b[39m scaler_x\u001b[38;5;241m.\u001b[39minverse_transform(x_window\u001b[38;5;241m.\u001b[39mview(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      3\u001b[0m y_window_original_scale \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(y_window\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m----> 4\u001b[0m outputs_original_scale \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Print the last window in original scale\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast Training Window (Original Scale):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "\n",
    "# Inverse transform to get original scales\n",
    "x_window_original_scale = scaler_x.inverse_transform(x_window.view(params['window_size'], -1).numpy())\n",
    "y_window_original_scale = scaler_y.inverse_transform(y_window.numpy())\n",
    "outputs_original_scale = scaler_y.inverse_transform(outputs.numpy())\n",
    "\n",
    "# Print the last window in original scale\n",
    "print(\"Last Training Window (Original Scale):\")\n",
    "print(\"Input Features:\")\n",
    "print(x_window_original_scale)\n",
    "print(\"True Output:\")\n",
    "print(y_window_original_scale)\n",
    "print(\"Predicted Output:\")\n",
    "print(outputs_original_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last Window in the Test Set - Original Values:\n",
      "Last 5 Entries - Features:\n",
      "[[ 1.9333000e+02  1.9444000e+02  1.9292000e+02  3.7283200e+07\n",
      "   1.9335890e+02  4.5000000e-01  5.2369165e+07]\n",
      " [ 1.9367000e+02  1.9564000e+02  1.9332000e+02  4.7471900e+07\n",
      "   1.9423770e+02  4.5000000e-01  5.2206220e+07]\n",
      " [ 1.9602000e+02  1.9720000e+02  1.9255000e+02  4.7460200e+07\n",
      "   1.9295940e+02 -6.6000000e-01  5.2018390e+07]\n",
      " [ 1.9467000e+02  1.9663000e+02  1.9414000e+02  4.8291400e+07\n",
      "   1.9556590e+02  1.3500000e+00  5.2115595e+07]\n",
      " [ 1.9606000e+02  1.9649000e+02  1.9526000e+02  3.8824100e+07\n",
      "   1.9618510e+02  3.2000000e-01  4.9803320e+07]]\n",
      "\n",
      "Original Values:\n",
      "[614.4796    31.628807  39.748802  35.369606 563.9004  ]\n"
     ]
    }
   ],
   "source": [
    "# Print the original values of the last window in the test set with the last 5 entries and 7 features\n",
    "last_window_features = data.iloc[-params['window_size']:][['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "original_last_window_values = scaler_y.inverse_transform(y_test[-params['window_size']:].reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"\\nLast Window in the Test Set - Original Values:\")\n",
    "print(\"Last 5 Entries - Features:\")\n",
    "print(last_window_features)\n",
    "print(\"\\nOriginal Values:\")\n",
    "print(original_last_window_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the scaling on the initial last window values\n",
    "last_window_original_scale = torch.tensor(scaler_x.inverse_transform(last_window.squeeze().numpy()), dtype=torch.float32)\n",
    "\n",
    "# Print the initial last window values in the original scale\n",
    "print(\"Initial Last Window from the Test Set (Original Scale):\")\n",
    "print(last_window_original_scale.numpy())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m original_last_window_features \u001b[38;5;241m=\u001b[39m scaler_x\u001b[38;5;241m.\u001b[39minverse_transform(last_window_features_scaled\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Reverse the scaled target variable\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m original_last_window_target \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(\u001b[43mlast_window_target_scaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLast Window in the Test Set - Original Values:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast 5 Entries - Scaled Features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "# Print the original values of the last window in the test set with the last 5 entries and 7 features\n",
    "last_window_features_scaled = x_test[-params['window_size']:]\n",
    "last_window_target_scaled = y_test[-params['window_size']:]\n",
    "\n",
    "# Reverse the scaled features\n",
    "original_last_window_features = scaler_x.inverse_transform(last_window_features_scaled.numpy())\n",
    "\n",
    "# Reverse the scaled target variable\n",
    "original_last_window_target = scaler_y.inverse_transform(last_window_target_scaled.numpy().reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"\\nLast Window in the Test Set - Original Values:\")\n",
    "print(\"Last 5 Entries - Scaled Features:\")\n",
    "print(last_window_features_scaled.numpy())\n",
    "print(\"\\nOriginal Values - Features:\")\n",
    "print(original_last_window_features)\n",
    "print(\"\\nOriginal Values - Target Variable:\")\n",
    "print(original_last_window_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Last Window from the Test Set:\n",
      "[[ 7.09588468e-01  6.94068849e-01  7.08556354e-01 -1.27636999e-01\n",
      "  -2.99862981e-01 -2.56235391e-01  7.99318731e-01]\n",
      " [-6.33621991e-01 -6.37571454e-01 -6.29014373e-01 -6.79282546e-01\n",
      "  -4.57734585e-01  1.22595763e+00 -1.09407175e+00]\n",
      " [-5.18926620e-01 -5.19055605e-01 -5.15072644e-01 -6.30042851e-01\n",
      "  -4.52214748e-01  2.69818723e-01 -6.40534580e-01]\n",
      " [ 1.76104784e+00  1.73794806e+00  1.74404860e+00 -7.56878555e-02\n",
      "  -1.91887781e-01 -3.87748927e-01  5.77984333e-01]\n",
      " [-6.61148906e-01 -6.64778769e-01 -6.60588562e-01 -6.33377790e-01\n",
      "  -4.55435306e-01 -3.94857764e-01 -6.09052598e-01]\n",
      " [-7.27204919e-01 -7.26226747e-01 -7.29571342e-01 -4.12115514e-01\n",
      "  -4.54135150e-01 -2.83674407e+00 -1.59285605e-01]\n",
      " [-5.54949462e-01 -5.56623518e-01 -5.55312634e-01 -6.36760890e-01\n",
      "  -4.53156799e-01  7.67437458e-01 -6.42346621e-01]\n",
      " [-6.97171748e-01 -6.87311113e-01 -6.95122838e-01  1.40832931e-01\n",
      "  -4.51625675e-01  3.63229990e+00  3.22337365e+00]\n",
      " [-3.63365680e-01 -3.70468467e-01 -3.81053895e-01 -6.19745553e-01\n",
      "  -4.48825538e-01 -1.63535023e+00 -3.66791904e-01]\n",
      " [ 4.45088148e-01  4.54152077e-01  4.55919832e-01 -2.58211285e-01\n",
      "  -3.23636889e-01  2.84036398e-01  3.09900522e-01]\n",
      " [ 2.21012282e+00  2.20184040e+00  2.24269366e+00 -4.46386755e-01\n",
      "  -1.41690344e-01 -5.00790514e-02 -1.02783203e-01]\n",
      " [ 1.82726178e-02  2.85435058e-02  5.38687268e-03 -5.29623866e-01\n",
      "  -4.37490046e-01  2.66264290e-01  4.85626042e-01]\n",
      " [-7.32684791e-01 -7.34271049e-01 -7.33732581e-01 -5.12578905e-01\n",
      "  -4.54231173e-01 -4.30401951e-01 -1.42607719e-01]\n",
      " [-8.21283236e-02 -8.27494189e-02 -1.04779638e-01  4.38510627e-01\n",
      "  -3.83342832e-01 -1.66378558e+00  1.98065710e+00]\n",
      " [ 1.30878687e+00  1.30438244e+00  1.27261424e+00  1.30792880e+00\n",
      "   1.33377099e+00 -5.22816896e-01 -4.20761824e-01]\n",
      " [ 4.61846411e-01  4.50996339e-01  4.63035077e-01  2.33412355e-01\n",
      "   5.92243671e-01 -5.76133192e-01 -7.48582661e-01]\n",
      " [-2.39401404e-02 -3.39453891e-02 -2.13488527e-02 -1.81592315e-01\n",
      "   1.78609893e-01 -2.27800041e-01 -5.65173447e-01]\n",
      " [-5.74829996e-01 -5.79198003e-01 -5.78306854e-01 -6.02419257e-01\n",
      "  -4.53470796e-01 -8.96030962e-01 -3.77146006e-01]\n",
      " [-7.42667556e-01 -7.40883350e-01 -7.42012262e-01 -4.41415787e-01\n",
      "  -4.54773545e-01 -8.21388125e-01  4.72832501e-01]\n",
      " [-6.46026075e-01 -6.43130839e-01 -6.41541064e-01 -6.56045139e-01\n",
      "  -4.57822829e-01  7.99427271e-01 -5.03606677e-01]\n",
      " [-5.62510848e-01 -5.66057622e-01 -5.63892543e-01 -6.16399467e-01\n",
      "  -4.52905059e-01 -5.08599222e-01 -3.22578073e-01]\n",
      " [-5.96664608e-01 -6.00845873e-01 -6.00271523e-01 -5.94801784e-01\n",
      "  -4.54013169e-01 -9.10248637e-01 -2.56600827e-01]\n",
      " [-6.59279764e-01 -6.44984007e-01 -6.56813383e-01  7.00991690e-01\n",
      "  -4.55129087e-01  1.17932472e+01  8.29636514e-01]\n",
      " [-5.69137692e-01 -5.55696964e-01 -5.64836323e-01 -5.10909557e-01\n",
      "  -4.52767521e-01  1.74845731e+00 -3.37434620e-01]\n",
      " [-7.14630902e-01 -7.15487063e-01 -7.10824132e-01 -4.73203421e-01\n",
      "  -4.53229457e-01 -9.27320868e-02  1.01851732e-01]\n",
      " [ 5.67211449e-01  5.82008839e-01  5.82388282e-01  5.33099890e-01\n",
      "   7.27773845e-01 -2.42017716e-01 -7.16755390e-01]\n",
      " [-6.50698841e-01 -6.43130839e-01 -6.47203803e-01 -4.60503906e-01\n",
      "  -4.54796910e-01  2.50910306e+00  8.46759737e-01]\n",
      " [-5.28357089e-01 -5.32701313e-01 -5.26054978e-01 -5.86680949e-01\n",
      "  -4.52007145e-01 -1.49602801e-01 -6.62280247e-02]\n",
      " [-6.35576069e-01 -6.39340401e-01 -6.40511453e-01 -5.56677401e-01\n",
      "  -4.55014884e-01 -1.13773143e+00 -5.44873059e-01]\n",
      " [-6.88845694e-01 -6.86005473e-01 -6.86371267e-01 -5.87121367e-01\n",
      "  -4.51851428e-01  1.80958226e-01 -3.15930367e-01]\n",
      " [-6.53502524e-01 -6.56271219e-01 -6.62476122e-01 -6.21544421e-01\n",
      "  -4.58038211e-01 -1.10574174e+00 -6.23882413e-01]\n",
      " [-6.65906608e-01 -6.67558432e-01 -6.63505733e-01 -6.46639884e-01\n",
      "  -4.55510557e-01 -3.94157916e-02 -6.90386176e-01]\n",
      " [-6.59279764e-01 -6.62841380e-01 -6.54925764e-01 -6.80666864e-01\n",
      "  -4.58038211e-01  2.21053195e+00 -1.11377215e+00]\n",
      " [-6.54522061e-01 -6.59135163e-01 -6.50121033e-01 -6.62278295e-01\n",
      "  -4.57981110e-01  8.56297970e-01 -2.88341969e-01]\n",
      " [ 4.95520839e-03  7.12725101e-03 -2.80697597e-03  4.89142060e-01\n",
      "  -3.73224467e-01 -7.32527614e-01  1.49631107e+00]\n",
      " [ 3.74083900e+00  3.76331067e+00  3.77957654e+00 -1.82339996e-01\n",
      "   2.26431061e-02  4.29767609e-01  1.29724473e-01]\n",
      " [ 1.59658742e+00  1.58791697e+00  1.58938277e+00  2.46700376e-01\n",
      "   1.57167089e+00 -4.05521035e-01 -7.10638106e-01]\n",
      " [-6.32772386e-01 -6.36560678e-01 -6.28156364e-01 -6.06383502e-01\n",
      "  -4.57724214e-01 -3.94157916e-02 -1.18985057e+00]\n",
      " [ 1.60363615e-02  2.83269063e-02  1.96357127e-02  1.04639518e+00\n",
      "   2.02565446e-01 -9.98409316e-02 -4.89707947e-01]\n",
      " [-7.03076422e-01 -7.05800235e-01 -7.04903960e-01 -3.43898714e-01\n",
      "  -4.52915430e-01 -5.54806650e-01 -7.14318305e-02]\n",
      " [ 5.16159832e-01  5.16126513e-01  5.20030439e-01  1.93888798e-01\n",
      "   6.64722800e-01 -1.74483746e-01 -6.73066139e-01]\n",
      " [-6.98233724e-01 -7.02304602e-01 -6.98168695e-01 -5.92899263e-01\n",
      "  -4.52578068e-01 -5.58361053e-01 -3.27369481e-01]\n",
      " [-1.07085191e-01 -7.45577514e-02 -1.03621341e-01  9.07048643e-01\n",
      "  -3.81585956e-01  8.14344808e-02  3.27334571e+00]\n",
      " [-6.93306088e-01 -6.97587550e-01 -7.00485289e-01 -1.70266032e-01\n",
      "  -4.52523589e-01 -2.83674407e+00  2.19013635e-02]\n",
      " [ 3.74750829e+00  3.78121018e+00  3.79450560e+00  1.18694864e-01\n",
      "   2.31076311e-02  4.01332259e-01  1.48066366e+00]\n",
      " [-6.08983696e-01 -6.13986254e-01 -6.06020093e-01 -6.79971039e-01\n",
      "  -4.57464695e-01 -3.30878198e-01 -8.98198962e-01]\n",
      " [-5.53080320e-01 -5.52917242e-01 -5.49564064e-01 -6.56141400e-01\n",
      "  -4.53102291e-01  2.98254073e-01 -7.27923512e-01]\n",
      " [-5.87234080e-01 -5.87705493e-01 -5.83025813e-01 -6.49423361e-01\n",
      "  -4.57127333e-01  7.31893301e-01 -5.15050948e-01]\n",
      " [ 3.45864582e+00  3.42938995e+00  3.46754527e+00 -4.14203078e-01\n",
      "  -1.50709215e-03 -2.74007499e-01  1.02354117e-01]\n",
      " [ 3.30672711e-02  3.30672711e-02  3.30672711e-02  3.30672711e-02\n",
      "   3.30672711e-02  3.30672711e-02  3.30672711e-02]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Last Window from the Test Set:\")\n",
    "print(last_window.squeeze().numpy())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Values in Original Scale:\n",
      "[[116.5111242]]\n"
     ]
    }
   ],
   "source": [
    "# Convert predicted values back to the original scale using inverse transformation\n",
    "predicted_values = np.array(predicted_values).reshape(-1, 1)\n",
    "\n",
    "# Invert the scaling on predicted values\n",
    "predicted_values_original_scale = scaler_y.inverse_transform(predicted_values)\n",
    "\n",
    "# Print or use the predicted values as needed\n",
    "print(\"Predicted Values in Original Scale:\")\n",
    "print(predicted_values_original_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Values in Original Scale:\n",
      "[[116.5111242]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_9356\\2961749471.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  last_window = torch.tensor(x_test[-window_size:], dtype=torch.float32).view(1, window_size, input_size)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose the number of steps to predict\n",
    "n_steps = 1  # You can adjust this based on your needs\n",
    "\n",
    "# Extract the last window from the test set\n",
    "last_window = torch.tensor(x_test[-window_size:], dtype=torch.float32).view(1, window_size, input_size)\n",
    "\n",
    "# Initialize the hidden state\n",
    "hidden = model.init_hidden(1)\n",
    "\n",
    "# Generate predictions for the next 'n_steps' in a loop\n",
    "predicted_values = []\n",
    "\n",
    "for _ in range(n_steps):\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get the next prediction\n",
    "        prediction, hidden = model(last_window, hidden)\n",
    "\n",
    "        # Append the prediction to the list of predicted values\n",
    "        predicted_values.append(prediction.item())\n",
    "\n",
    "        # Update the last window for the next iteration\n",
    "        last_window = torch.cat([last_window[:, 1:, :], prediction.view(1, 1, -1).expand(-1, -1, last_window.size(2))], dim=1)\n",
    "\n",
    "# Convert predicted values back to the original scale using inverse transformation\n",
    "predicted_values = np.array(predicted_values).reshape(-1, 1)\n",
    "predicted_values_original_scale = scaler_y.inverse_transform(predicted_values)\n",
    "\n",
    "# Print or use the predicted values as needed\n",
    "print(\"Predicted Values in Original Scale:\")\n",
    "print(predicted_values_original_scale)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
