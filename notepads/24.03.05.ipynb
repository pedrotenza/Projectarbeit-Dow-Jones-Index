{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "#print(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "y_data = data[\"close\"].values.reshape(-1, 1)  # Reshape y_data if it's 1D\n",
    "\n",
    "# Initialize the scalers\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both training and test data\n",
    "x_data_scaled = scaler_x.fit_transform(x_data)\n",
    "y_data_scaled = scaler_y.fit_transform(y_data)\n",
    "\n",
    "# Convert scaled data to Tensors\n",
    "x_feature_tensors = torch.tensor(x_data_scaled, dtype=torch.float32)\n",
    "y_feature_tensors = torch.tensor(y_data_scaled, dtype=torch.float32)\n",
    "\n",
    "# Split the training data into training and temporary sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_feature_tensors, y_feature_tensors, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert scaled labels back to numpy arrays\n",
    "y_train = y_train.numpy()\n",
    "y_val = y_val.numpy()\n",
    "y_test = y_test.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Now you can use x_train_scaled, x_val, x_test, y_train_scaled, y_val_scaled, y_test_scaled for your model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 4\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_6296\\267311635.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_6296\\267311635.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000, Train Loss: 1.0491517782211304, Validation Loss: 2.2497355937957764\n",
      "Iteration 2000, Train Loss: 2.0725128650665283, Validation Loss: 2.234780788421631\n",
      "Iteration 3000, Train Loss: 0.23910768330097198, Validation Loss: 2.2374267578125\n",
      "Iteration 4000, Train Loss: 0.3821171820163727, Validation Loss: 2.2168617248535156\n",
      "Iteration 5000, Train Loss: 0.30708590149879456, Validation Loss: 2.219841957092285\n",
      "Iteration 6000, Train Loss: 0.17089402675628662, Validation Loss: 2.223947763442993\n",
      "Iteration 7000, Train Loss: 0.23000116646289825, Validation Loss: 2.229879856109619\n",
      "Iteration 8000, Train Loss: 0.33763909339904785, Validation Loss: 2.2214770317077637\n",
      "Test Loss: 0.3516319990158081\n",
      "Iteration 1000, Train Loss: 1.0487529039382935, Validation Loss: 2.2495007514953613\n",
      "Iteration 2000, Train Loss: 2.072859525680542, Validation Loss: 2.234898090362549\n",
      "Iteration 3000, Train Loss: 0.23985806107521057, Validation Loss: 2.2369916439056396\n",
      "Iteration 4000, Train Loss: 0.380383163690567, Validation Loss: 2.2177765369415283\n",
      "Iteration 5000, Train Loss: 0.3059905767440796, Validation Loss: 2.220439910888672\n",
      "Iteration 6000, Train Loss: 0.17067933082580566, Validation Loss: 2.224107265472412\n",
      "Iteration 7000, Train Loss: 0.2306607961654663, Validation Loss: 2.2295351028442383\n",
      "Iteration 8000, Train Loss: 0.33587509393692017, Validation Loss: 2.2220942974090576\n",
      "Test Loss: 0.3538498282432556\n",
      "Iteration 1000, Train Loss: 1.0483180284500122, Validation Loss: 2.248770236968994\n",
      "Iteration 2000, Train Loss: 2.0707080364227295, Validation Loss: 2.234142780303955\n",
      "Iteration 3000, Train Loss: 0.23959708213806152, Validation Loss: 2.237189769744873\n",
      "Iteration 4000, Train Loss: 0.3839988112449646, Validation Loss: 2.215885639190674\n",
      "Iteration 5000, Train Loss: 0.3081856071949005, Validation Loss: 2.2192623615264893\n",
      "Iteration 6000, Train Loss: 0.17161592841148376, Validation Loss: 2.223407030105591\n",
      "Iteration 7000, Train Loss: 0.23001527786254883, Validation Loss: 2.2298476696014404\n",
      "Iteration 8000, Train Loss: 0.049725163727998734, Validation Loss: 2.199866771697998\n",
      "Test Loss: 0.010023962706327438\n",
      "Iteration 1000, Train Loss: 1.049080729484558, Validation Loss: 2.249504804611206\n",
      "Iteration 2000, Train Loss: 2.0717742443084717, Validation Loss: 2.2345223426818848\n",
      "Iteration 3000, Train Loss: 0.23888131976127625, Validation Loss: 2.237572193145752\n",
      "Iteration 4000, Train Loss: 0.38327085971832275, Validation Loss: 2.216261863708496\n",
      "Iteration 5000, Train Loss: 0.30767208337783813, Validation Loss: 2.2195332050323486\n",
      "Iteration 6000, Train Loss: 0.17153096199035645, Validation Loss: 2.2234694957733154\n",
      "Iteration 7000, Train Loss: 0.23037071526050568, Validation Loss: 2.229691982269287\n",
      "Iteration 8000, Train Loss: 0.3390655815601349, Validation Loss: 2.2209696769714355\n",
      "Test Loss: 0.351618230342865\n",
      "Iteration 1000, Train Loss: 1.0496653318405151, Validation Loss: 2.2505135536193848\n",
      "Iteration 2000, Train Loss: 2.077150821685791, Validation Loss: 2.236436367034912\n",
      "Iteration 3000, Train Loss: 0.23811808228492737, Validation Loss: 2.2379186153411865\n",
      "Iteration 4000, Train Loss: 0.37795788049697876, Validation Loss: 2.21909761428833\n",
      "Iteration 5000, Train Loss: 0.30374544858932495, Validation Loss: 2.221644401550293\n",
      "Iteration 6000, Train Loss: 0.16894741356372833, Validation Loss: 2.225437879562378\n",
      "Iteration 7000, Train Loss: 0.22941748797893524, Validation Loss: 2.2301723957061768\n",
      "Iteration 8000, Train Loss: 0.33263280987739563, Validation Loss: 2.223283529281616\n",
      "Test Loss: 0.3528047800064087\n",
      "Iteration 1000, Train Loss: 1.048370122909546, Validation Loss: 2.2489373683929443\n",
      "Iteration 2000, Train Loss: 2.0707736015319824, Validation Loss: 2.2341599464416504\n",
      "Iteration 3000, Train Loss: 0.24014417827129364, Validation Loss: 2.236865520477295\n",
      "Iteration 4000, Train Loss: 0.38221269845962524, Validation Loss: 2.216808319091797\n",
      "Iteration 5000, Train Loss: 0.30730366706848145, Validation Loss: 2.2197396755218506\n",
      "Iteration 6000, Train Loss: 0.17088982462882996, Validation Loss: 2.223947763442993\n",
      "Iteration 7000, Train Loss: 0.23078927397727966, Validation Loss: 2.2294716835021973\n",
      "Iteration 8000, Train Loss: 0.27980348467826843, Validation Loss: 2.2113804817199707\n",
      "Test Loss: 0.07341979444026947\n",
      "Iteration 1000, Train Loss: 1.0483620166778564, Validation Loss: 2.2490501403808594\n",
      "Iteration 2000, Train Loss: 2.07053804397583, Validation Loss: 2.2340712547302246\n",
      "Iteration 3000, Train Loss: 0.24105755984783173, Validation Loss: 2.236355781555176\n",
      "Iteration 4000, Train Loss: 0.3808917999267578, Validation Loss: 2.217501163482666\n",
      "Iteration 5000, Train Loss: 0.3067944645881653, Validation Loss: 2.2200217247009277\n",
      "Iteration 6000, Train Loss: 0.170893594622612, Validation Loss: 2.2239432334899902\n",
      "Iteration 7000, Train Loss: 0.23141054809093475, Validation Loss: 2.229151725769043\n",
      "Iteration 8000, Train Loss: 0.07188820838928223, Validation Loss: 2.184450387954712\n",
      "Test Loss: 0.002290204167366028\n",
      "Iteration 1000, Train Loss: 1.0483176708221436, Validation Loss: 2.2491848468780518\n",
      "Iteration 2000, Train Loss: 2.0727639198303223, Validation Loss: 2.2348592281341553\n",
      "Iteration 3000, Train Loss: 0.24067647755146027, Validation Loss: 2.2365341186523438\n",
      "Iteration 4000, Train Loss: 0.3788855969905853, Validation Loss: 2.2185816764831543\n",
      "Iteration 5000, Train Loss: 0.30547481775283813, Validation Loss: 2.220729351043701\n",
      "Iteration 6000, Train Loss: 0.1703082025051117, Validation Loss: 2.224386692047119\n",
      "Iteration 7000, Train Loss: 0.2312026023864746, Validation Loss: 2.2292544841766357\n",
      "Iteration 8000, Train Loss: 0.33500051498413086, Validation Loss: 2.222398281097412\n",
      "Test Loss: 0.35576027631759644\n",
      "Iteration 1000, Train Loss: 1.0504186153411865, Validation Loss: 2.2512385845184326\n",
      "Iteration 2000, Train Loss: 2.0804104804992676, Validation Loss: 2.2376184463500977\n",
      "Iteration 3000, Train Loss: 0.23609881103038788, Validation Loss: 2.2390239238739014\n",
      "Iteration 4000, Train Loss: 0.3770178258419037, Validation Loss: 2.2196266651153564\n",
      "Iteration 5000, Train Loss: 0.3020762801170349, Validation Loss: 2.2225499153137207\n",
      "Iteration 6000, Train Loss: 0.16837036609649658, Validation Loss: 2.225895404815674\n",
      "Iteration 7000, Train Loss: 0.22792312502861023, Validation Loss: 2.2309532165527344\n",
      "Iteration 8000, Train Loss: 0.331755667924881, Validation Loss: 2.223607063293457\n",
      "Test Loss: 0.35188525915145874\n",
      "Iteration 1000, Train Loss: 1.048803687095642, Validation Loss: 2.249565601348877\n",
      "Iteration 2000, Train Loss: 2.072826862335205, Validation Loss: 2.2348854541778564\n",
      "Iteration 3000, Train Loss: 0.24012234807014465, Validation Loss: 2.2368454933166504\n",
      "Iteration 4000, Train Loss: 0.38003960251808167, Validation Loss: 2.2179596424102783\n",
      "Iteration 5000, Train Loss: 0.3059768080711365, Validation Loss: 2.2204489707946777\n",
      "Iteration 6000, Train Loss: 0.1707763969898224, Validation Loss: 2.2240328788757324\n",
      "Iteration 7000, Train Loss: 0.23092107474803925, Validation Loss: 2.229401111602783\n",
      "Iteration 8000, Train Loss: 0.33599501848220825, Validation Loss: 2.2220492362976074\n",
      "Test Loss: 0.35406485199928284\n",
      "Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 0.002290204167366028}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled, x_test_scaled, y_test_scaled,\n",
    "# input_size, output_size, test_window_size, scaler are available\n",
    "\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'learning_rate': [0.00005],\n",
    "    'window_size': [ 5],\n",
    "    'hidden_dim': [256],\n",
    "    'n_layers': [11],\n",
    "    'batch_evaluation_frequency': [4]\n",
    "}\n",
    "\n",
    "#Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 0.41702309250831604}\n",
    "#              'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 6463.20556640625}\n",
    "\n",
    "\n",
    "# Number of random search iterations\n",
    "num_iterations = 10\n",
    "\n",
    "best_params = None\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    params = {\n",
    "        'learning_rate': random.choice(search_space['learning_rate']),\n",
    "        'window_size': random.choice(search_space['window_size']),\n",
    "        'hidden_dim': random.choice(search_space['hidden_dim']),\n",
    "        'n_layers': random.choice(search_space['n_layers']),\n",
    "        'batch_evaluation_frequency': random.choice(search_space['batch_evaluation_frequency'])\n",
    "    }\n",
    "\n",
    "    model = LSTMModel(input_size, params['hidden_dim'], params['n_layers'], output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Training using Walk-Forward Validation\n",
    "    for i in range(params['window_size'], len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_idx = i - params['window_size']\n",
    "        end_idx = i\n",
    "        x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        y_window = torch.tensor(y_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        x_window = x_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        outputs, hidden = model(x_window, hidden)\n",
    "        loss = criterion(outputs, y_window)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % params['batch_evaluation_frequency'] == 0:\n",
    "            with torch.no_grad():\n",
    "                x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n",
    "                y_val_window = torch.tensor(y_val[:params['window_size']], dtype=torch.float32)\n",
    "                x_val_window = x_val_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "                hidden = model.init_hidden(1)\n",
    "                val_outputs, _ = model(x_val_window, hidden)\n",
    "                val_loss = criterion(val_outputs, y_val_window)\n",
    "\n",
    "                if i % 1000 == 0:  # Print every 1000 iterations\n",
    "                    print(f\"Iteration {i}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Test set evaluation\n",
    "    with torch.no_grad():\n",
    "    # Use the last 'window_size' entries from the original data as the input for the model\n",
    "        x_test_window = torch.tensor(x_data_scaled[-params['window_size']:], dtype=torch.float32)\n",
    "        y_test_window = torch.tensor(y_data_scaled[-params['window_size']:], dtype=torch.float32)\n",
    "        x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        test_outputs, _ = model(x_test_window, hidden)\n",
    "        test_loss = criterion(test_outputs, y_test_window)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "    # Update the best_params if the current model performs better on the test set\n",
    "    if best_params is None:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "    elif test_loss < best_params['test_loss']:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5 Rows of Original Data:\n",
      "             date    open    high     low   close      volume  adjusted_close  \\\n",
      "10742  2023-07-25  193.33  194.44  192.92  193.62  37283200.0        193.3589   \n",
      "10743  2023-07-26  193.67  195.64  193.32  194.50  47471900.0        194.2377   \n",
      "10744  2023-07-27  196.02  197.20  192.55  193.22  47460200.0        192.9594   \n",
      "10745  2023-07-28  194.67  196.63  194.14  195.83  48291400.0        195.5659   \n",
      "10746  2023-07-31  196.06  196.49  195.26  196.45  38824100.0        196.1851   \n",
      "\n",
      "       change_percent  avg_vol_20d  \n",
      "10742            0.45   52369165.0  \n",
      "10743            0.45   52206220.0  \n",
      "10744           -0.66   52018390.0  \n",
      "10745            1.35   52115595.0  \n",
      "10746            0.32   49803320.0  \n"
     ]
    }
   ],
   "source": [
    "# Print the last 5 rows of the original data\n",
    "print(\"Last 5 Rows of Original Data:\")\n",
    "print(data.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5 Rows of Original Data (Scaled):\n",
      "[[ 0.61575973  0.60863248  0.62957796  0.69611326  4.55759397  0.1205331\n",
      "  -0.99750238]\n",
      " [ 0.61833886  0.61765745  0.63264222  1.07323779  4.5803999   0.1205331\n",
      "  -0.99810086]\n",
      " [ 0.6361652   0.62938991  0.62674351  1.07280473  4.54722646 -0.2740075\n",
      "  -0.99879075]\n",
      " [ 0.62592454  0.62510305  0.63892397  1.10357077  4.6148683   0.44043087\n",
      "  -0.99843372]\n",
      " [ 0.63646862  0.62405014  0.64750392  0.75314813  4.63093729  0.07432564\n",
      "  -1.00692654]]\n"
     ]
    }
   ],
   "source": [
    "# Apply the same scaling operation to the last 5 rows of the original data\n",
    "last_5_rows_scaled = scaler_x.transform(x_data[-5:])\n",
    "print(\"Last 5 Rows of Original Data (Scaled):\")\n",
    "print(last_5_rows_scaled)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Training Window in Original Scale:\n",
      "[[ 2.21983979e+01  2.28479971e+01  2.20528002e+01  4.69919920e+06\n",
      "   3.42700308e-01  1.87000004e+00  2.07898037e+08]\n",
      " [ 2.30496039e+01  2.39120000e+01  2.29376033e+01  6.22159998e+06\n",
      "   3.56800340e-01  8.50000004e-01  3.56143759e+08]\n",
      " [ 2.54911971e+01  2.54912027e+01  2.49311975e+01  6.70369970e+06\n",
      "   3.78700292e-01 -1.69000008e+00  2.84303878e+08]\n",
      " [ 3.12479993e+01  3.31295967e+01  3.12480012e+01  1.13840055e+06\n",
      "   1.13800448e-01  5.65999996e+00  2.25719198e+08]\n",
      " [ 1.87739997e+02  1.90010800e+02  1.87300398e+02  2.16101999e+07\n",
      "   5.76009959e+00  2.14999995e+00  5.13932433e+08]]\n",
      "Last 5 Rows of Original Data in Same Scale:\n",
      "[[ 0.61575973  0.60863248  0.62957796  0.69611326  4.55759397  0.1205331\n",
      "  -0.99750238]\n",
      " [ 0.61833886  0.61765745  0.63264222  1.07323779  4.5803999   0.1205331\n",
      "  -0.99810086]\n",
      " [ 0.6361652   0.62938991  0.62674351  1.07280473  4.54722646 -0.2740075\n",
      "  -0.99879075]\n",
      " [ 0.62592454  0.62510305  0.63892397  1.10357077  4.6148683   0.44043087\n",
      "  -0.99843372]\n",
      " [ 0.63646862  0.62405014  0.64750392  0.75314813  4.63093729  0.07432564\n",
      "  -1.00692654]]\n"
     ]
    }
   ],
   "source": [
    "# Print the last training window in original scale\n",
    "print(\"Last Training Window in Original Scale:\")\n",
    "print(last_training_window_original_scale)\n",
    "\n",
    "# Print the last 5 rows of original data in the same scale as the last training window\n",
    "print(\"Last 5 Rows of Original Data in Same Scale:\")\n",
    "print(last_5_rows_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
