{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "#print(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "y_data = data[\"close\"].values.reshape(-1, 1)  # Reshape y_data if it's 1D\n",
    "\n",
    "# Initialize the scalers\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both training and test data\n",
    "x_data_scaled = scaler_x.fit_transform(x_data)\n",
    "y_data_scaled = scaler_y.fit_transform(y_data)\n",
    "\n",
    "# Convert scaled data to Tensors\n",
    "x_feature_tensors = torch.tensor(x_data_scaled, dtype=torch.float32)\n",
    "y_feature_tensors = torch.tensor(y_data_scaled, dtype=torch.float32)\n",
    "\n",
    "# Split the training data into training and temporary sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_feature_tensors, y_feature_tensors, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert scaled labels back to numpy arrays\n",
    "y_train = y_train.numpy()\n",
    "y_val = y_val.numpy()\n",
    "y_test = y_test.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Now you can use x_train_scaled, x_val, x_test, y_train_scaled, y_val_scaled, y_test_scaled for your model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 4\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_7532\\3789981942.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_7532\\3789981942.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000, Train Loss: 1.0490267276763916, Validation Loss: 2.2497267723083496\n",
      "Iteration 2000, Train Loss: 2.07322359085083, Validation Loss: 2.2350287437438965\n",
      "Iteration 3000, Train Loss: 0.23972375690937042, Validation Loss: 2.2370657920837402\n",
      "Iteration 4000, Train Loss: 0.3800167441368103, Validation Loss: 2.2179744243621826\n",
      "Iteration 5000, Train Loss: 0.30551591515541077, Validation Loss: 2.2206883430480957\n",
      "Iteration 6000, Train Loss: 0.17002463340759277, Validation Loss: 2.2246057987213135\n",
      "Iteration 7000, Train Loss: 0.23032350838184357, Validation Loss: 2.2297072410583496\n",
      "Iteration 8000, Train Loss: 0.33546438813209534, Validation Loss: 2.2222461700439453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_7532\\3789981942.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4182278513908386\n",
      "Iteration 1000, Train Loss: 1.0481255054473877, Validation Loss: 2.2489237785339355\n",
      "Iteration 2000, Train Loss: 2.071110486984253, Validation Loss: 2.2342686653137207\n",
      "Iteration 3000, Train Loss: 0.24180194735527039, Validation Loss: 2.235933780670166\n",
      "Iteration 4000, Train Loss: 0.37913402915000916, Validation Loss: 2.2184417247772217\n",
      "Iteration 5000, Train Loss: 0.3061540126800537, Validation Loss: 2.2203803062438965\n",
      "Iteration 6000, Train Loss: 0.17040665447711945, Validation Loss: 2.2243080139160156\n",
      "Iteration 7000, Train Loss: 0.23221734166145325, Validation Loss: 2.2287352085113525\n",
      "Iteration 8000, Train Loss: 0.3347059488296509, Validation Loss: 2.222489595413208\n",
      "Test Loss: 0.4165410101413727\n",
      "Iteration 1000, Train Loss: 1.0487163066864014, Validation Loss: 2.249377489089966\n",
      "Iteration 2000, Train Loss: 2.0708460807800293, Validation Loss: 2.2341809272766113\n",
      "Iteration 3000, Train Loss: 0.24090614914894104, Validation Loss: 2.2364439964294434\n",
      "Iteration 4000, Train Loss: 0.3810795247554779, Validation Loss: 2.2174017429351807\n",
      "Iteration 5000, Train Loss: 0.3070460557937622, Validation Loss: 2.219883918762207\n",
      "Iteration 6000, Train Loss: 0.170868381857872, Validation Loss: 2.2239623069763184\n",
      "Iteration 7000, Train Loss: 0.23112356662750244, Validation Loss: 2.229299306869507\n",
      "Iteration 8000, Train Loss: 0.3372843563556671, Validation Loss: 2.221588611602783\n",
      "Test Loss: 0.4181090295314789\n",
      "Iteration 1000, Train Loss: 1.0485045909881592, Validation Loss: 2.2490620613098145\n",
      "Iteration 2000, Train Loss: 2.070399284362793, Validation Loss: 2.2340266704559326\n",
      "Iteration 3000, Train Loss: 0.24046111106872559, Validation Loss: 2.2366950511932373\n",
      "Iteration 4000, Train Loss: 0.3823164105415344, Validation Loss: 2.216752290725708\n",
      "Iteration 5000, Train Loss: 0.30770209431648254, Validation Loss: 2.219533920288086\n",
      "Iteration 6000, Train Loss: 0.17152926325798035, Validation Loss: 2.2234673500061035\n",
      "Iteration 7000, Train Loss: 0.23115916550159454, Validation Loss: 2.2292847633361816\n",
      "Iteration 8000, Train Loss: 0.1384631097316742, Validation Loss: 2.184174060821533\n",
      "Test Loss: 0.37126243114471436\n",
      "Iteration 1000, Train Loss: 1.0498924255371094, Validation Loss: 2.250710964202881\n",
      "Iteration 2000, Train Loss: 2.077970504760742, Validation Loss: 2.2367336750030518\n",
      "Iteration 3000, Train Loss: 0.2376246452331543, Validation Loss: 2.2381858825683594\n",
      "Iteration 4000, Train Loss: 0.3779626488685608, Validation Loss: 2.21909761428833\n",
      "Iteration 5000, Train Loss: 0.3034021556377411, Validation Loss: 2.221831798553467\n",
      "Iteration 6000, Train Loss: 0.1695324182510376, Validation Loss: 2.224987268447876\n",
      "Iteration 7000, Train Loss: 0.22949378192424774, Validation Loss: 2.230133533477783\n",
      "Iteration 8000, Train Loss: 0.33322539925575256, Validation Loss: 2.223057270050049\n",
      "Test Loss: 0.4181889593601227\n",
      "Iteration 1000, Train Loss: 1.0488404035568237, Validation Loss: 2.2497143745422363\n",
      "Iteration 2000, Train Loss: 2.074195146560669, Validation Loss: 2.2353694438934326\n",
      "Iteration 3000, Train Loss: 0.24025985598564148, Validation Loss: 2.236746311187744\n",
      "Iteration 4000, Train Loss: 0.3777051568031311, Validation Loss: 2.2192294597625732\n",
      "Iteration 5000, Train Loss: 0.3046063780784607, Validation Loss: 2.2211971282958984\n",
      "Iteration 6000, Train Loss: 0.16961553692817688, Validation Loss: 2.2249157428741455\n",
      "Iteration 7000, Train Loss: 0.23104019463062286, Validation Loss: 2.2293336391448975\n",
      "Iteration 8000, Train Loss: 0.3331521451473236, Validation Loss: 2.2230629920959473\n",
      "Test Loss: 0.416903018951416\n",
      "Iteration 1000, Train Loss: 1.0512690544128418, Validation Loss: 2.2519733905792236\n",
      "Iteration 2000, Train Loss: 2.0818238258361816, Validation Loss: 2.2381372451782227\n",
      "Iteration 3000, Train Loss: 0.23482127487659454, Validation Loss: 2.2397499084472656\n",
      "Iteration 4000, Train Loss: 0.3777095377445221, Validation Loss: 2.219247817993164\n",
      "Iteration 5000, Train Loss: 0.302292138338089, Validation Loss: 2.2224087715148926\n",
      "Iteration 6000, Train Loss: 0.16829535365104675, Validation Loss: 2.225956916809082\n",
      "Iteration 7000, Train Loss: 0.22702226042747498, Validation Loss: 2.231431484222412\n",
      "Iteration 8000, Train Loss: 0.331854909658432, Validation Loss: 2.2235937118530273\n",
      "Test Loss: 0.41995564103126526\n",
      "Iteration 1000, Train Loss: 1.0501148700714111, Validation Loss: 2.2509772777557373\n",
      "Iteration 2000, Train Loss: 2.0796327590942383, Validation Loss: 2.2373359203338623\n",
      "Iteration 3000, Train Loss: 0.236606627702713, Validation Loss: 2.238746166229248\n",
      "Iteration 4000, Train Loss: 0.3774726092815399, Validation Loss: 2.2193715572357178\n",
      "Iteration 5000, Train Loss: 0.30265524983406067, Validation Loss: 2.2222299575805664\n",
      "Iteration 6000, Train Loss: 0.16851869225502014, Validation Loss: 2.2257771492004395\n",
      "Iteration 7000, Train Loss: 0.22813864052295685, Validation Loss: 2.2308406829833984\n",
      "Iteration 8000, Train Loss: 0.33184510469436646, Validation Loss: 2.223581075668335\n",
      "Test Loss: 0.4190567135810852\n",
      "Iteration 1000, Train Loss: 1.0483852624893188, Validation Loss: 2.2491660118103027\n",
      "Iteration 2000, Train Loss: 2.0715267658233643, Validation Loss: 2.234419584274292\n",
      "Iteration 3000, Train Loss: 0.24109351634979248, Validation Loss: 2.2363219261169434\n",
      "Iteration 4000, Train Loss: 0.37980741262435913, Validation Loss: 2.218080997467041\n",
      "Iteration 5000, Train Loss: 0.30628514289855957, Validation Loss: 2.220297336578369\n",
      "Iteration 6000, Train Loss: 0.17070630192756653, Validation Loss: 2.224083423614502\n",
      "Iteration 7000, Train Loss: 0.2315453737974167, Validation Loss: 2.2290806770324707\n",
      "Iteration 8000, Train Loss: 0.3360208570957184, Validation Loss: 2.222029447555542\n",
      "Test Loss: 0.41743534803390503\n",
      "Iteration 1000, Train Loss: 1.0490028858184814, Validation Loss: 2.2497506141662598\n",
      "Iteration 2000, Train Loss: 2.0734074115753174, Validation Loss: 2.235093593597412\n",
      "Iteration 3000, Train Loss: 0.23978133499622345, Validation Loss: 2.2370312213897705\n",
      "Iteration 4000, Train Loss: 0.38012516498565674, Validation Loss: 2.217914581298828\n",
      "Iteration 5000, Train Loss: 0.3059667944908142, Validation Loss: 2.220452070236206\n",
      "Iteration 6000, Train Loss: 0.17059043049812317, Validation Loss: 2.224174737930298\n",
      "Iteration 7000, Train Loss: 0.23048916459083557, Validation Loss: 2.229623794555664\n",
      "Iteration 8000, Train Loss: 0.33628958463668823, Validation Loss: 2.2219479084014893\n",
      "Test Loss: 0.4182772636413574\n",
      "Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 0.37126243114471436}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled, x_test_scaled, y_test_scaled,\n",
    "# input_size, output_size, test_window_size, scaler are available\n",
    "\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'learning_rate': [0.00005],\n",
    "    'window_size': [ 5],\n",
    "    'hidden_dim': [256],\n",
    "    'n_layers': [11],\n",
    "    'batch_evaluation_frequency': [4]\n",
    "}\n",
    "\n",
    "#Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 0.41702309250831604}\n",
    "#              'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 6463.20556640625}\n",
    "\n",
    "\n",
    "# Number of random search iterations\n",
    "num_iterations = 10\n",
    "\n",
    "best_params = None\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    params = {\n",
    "        'learning_rate': random.choice(search_space['learning_rate']),\n",
    "        'window_size': random.choice(search_space['window_size']),\n",
    "        'hidden_dim': random.choice(search_space['hidden_dim']),\n",
    "        'n_layers': random.choice(search_space['n_layers']),\n",
    "        'batch_evaluation_frequency': random.choice(search_space['batch_evaluation_frequency'])\n",
    "    }\n",
    "\n",
    "    model = LSTMModel(input_size, params['hidden_dim'], params['n_layers'], output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Training using Walk-Forward Validation\n",
    "    for i in range(params['window_size'], len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_idx = i - params['window_size']\n",
    "        end_idx = i\n",
    "        x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        y_window = torch.tensor(y_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        x_window = x_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        outputs, hidden = model(x_window, hidden)\n",
    "        loss = criterion(outputs, y_window)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % params['batch_evaluation_frequency'] == 0:\n",
    "            with torch.no_grad():\n",
    "                x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n",
    "                y_val_window = torch.tensor(y_val[:params['window_size']], dtype=torch.float32)\n",
    "                x_val_window = x_val_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "                hidden = model.init_hidden(1)\n",
    "                val_outputs, _ = model(x_val_window, hidden)\n",
    "                val_loss = criterion(val_outputs, y_val_window)\n",
    "\n",
    "                if i % 1000 == 0:  # Print every 1000 iterations\n",
    "                    print(f\"Iteration {i}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # Test set evaluation\n",
    "    with torch.no_grad():\n",
    "        x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n",
    "        y_test_window = torch.tensor(y_test[:params['window_size']], dtype=torch.float32)\n",
    "        x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        test_outputs, _ = model(x_test_window, hidden)\n",
    "        test_loss = criterion(test_outputs, y_test_window)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "    # Update the best_params if the current model performs better on the test set\n",
    "    if best_params is None:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "    elif test_loss < best_params['test_loss']:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'x_test' is your test dataset, and you want to predict the next day after the last day in the test set\n",
    "last_day_index = -1  # Index of the last day in the test set\n",
    "next_day_features = x_test[last_day_index].reshape(1, 1, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_7532\\231502711.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  initial_window = torch.tensor(x_train[-params['window_size']:], dtype=torch.float32).view(1, params['window_size'], input_size)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_7532\\231502711.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_day_features = torch.tensor(next_day_features, dtype=torch.float32).view(1, 1, input_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[352.3540612]\n",
      " [130.973271 ]\n",
      " [130.973271 ]\n",
      " [130.973271 ]]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a trained model named 'model'\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store the predicted values\n",
    "predicted_values = []\n",
    "\n",
    "# Use the last window of your training data to initialize the prediction\n",
    "initial_window = torch.tensor(x_train[-params['window_size']:], dtype=torch.float32).view(1, params['window_size'], input_size)\n",
    "hidden = model.init_hidden(1)\n",
    "outputs, hidden = model(initial_window, hidden)\n",
    "\n",
    "# Iteratively predict the next day and update the window\n",
    "for _ in range(num_future_days):\n",
    "    # Assuming 'next_day_features' contains the features for the next day\n",
    "    next_day_features = torch.tensor(next_day_features, dtype=torch.float32).view(1, 1, input_size)\n",
    "\n",
    "    # Make a prediction for the next day\n",
    "    with torch.no_grad():\n",
    "        prediction, _ = model(next_day_features, hidden)\n",
    "\n",
    "    # Update the window by removing the first day and adding the predicted value\n",
    "    initial_window = torch.cat((initial_window[:, 1:, :], next_day_features), dim=1)\n",
    "\n",
    "    # Update the hidden state for the next prediction\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    # Append the predicted value to the list\n",
    "    predicted_values.append(prediction.item())\n",
    "\n",
    "# Convert the predicted values back to the original scale if you used scaling\n",
    "predicted_values = scaler_y.inverse_transform(np.array(predicted_values).reshape(-1, 1))\n",
    "\n",
    "# Print or use the predicted values as needed\n",
    "print(predicted_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date    open    high     low   close      volume  adjusted_close  \\\n",
      "10742  2023-07-25  193.33  194.44  192.92  193.62  37283200.0        193.3589   \n",
      "10743  2023-07-26  193.67  195.64  193.32  194.50  47471900.0        194.2377   \n",
      "10744  2023-07-27  196.02  197.20  192.55  193.22  47460200.0        192.9594   \n",
      "10745  2023-07-28  194.67  196.63  194.14  195.83  48291400.0        195.5659   \n",
      "10746  2023-07-31  196.06  196.49  195.26  196.45  38824100.0        196.1851   \n",
      "\n",
      "       change_percent  avg_vol_20d  \n",
      "10742            0.45   52369165.0  \n",
      "10743            0.45   52206220.0  \n",
      "10744           -0.66   52018390.0  \n",
      "10745            1.35   52115595.0  \n",
      "10746            0.32   49803320.0  \n"
     ]
    }
   ],
   "source": [
    "# Assuming 'data' is your original DataFrame containing the time series data\n",
    "print(data.tail(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orignial rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Rolling Window (Original Values):\n",
      " [[ 1.9333000e+02  1.9444000e+02  1.9292000e+02  3.7283200e+07\n",
      "   1.9335890e+02  4.5000000e-01  5.2369165e+07]\n",
      " [ 1.9367000e+02  1.9564000e+02  1.9332000e+02  4.7471900e+07\n",
      "   1.9423770e+02  4.5000000e-01  5.2206220e+07]\n",
      " [ 1.9602000e+02  1.9720000e+02  1.9255000e+02  4.7460200e+07\n",
      "   1.9295940e+02 -6.6000000e-01  5.2018390e+07]\n",
      " [ 1.9467000e+02  1.9663000e+02  1.9414000e+02  4.8291400e+07\n",
      "   1.9556590e+02  1.3500000e+00  5.2115595e+07]\n",
      " [ 1.9606000e+02  1.9649000e+02  1.9526000e+02  3.8824100e+07\n",
      "   1.9618510e+02  3.2000000e-01  4.9803320e+07]]\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'window_size', model, scaler_x, scaler_y, input_size are defined\n",
    "params['window_size'] = 5  # Adjust window_size as needed\n",
    "\n",
    "# Initialize the rolling window with the last values from the training set, excluding 'close'\n",
    "initial_rolling_window = data.iloc[-params['window_size']:][['open', 'high', 'low', 'volume', 'adjusted_close',\n",
    "                                                             'change_percent', 'avg_vol_20d']].values\n",
    "rolling_window_original = initial_rolling_window  # Save for printing later\n",
    "\n",
    "# Set the correct input size based on the number of features in your data\n",
    "input_size = initial_rolling_window.shape[1]\n",
    "\n",
    "# Print the initial rolling window values and dimensions\n",
    "print(\"Initial Rolling Window (Original Values):\\n\", rolling_window_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Rolling Window (Original Values):\n",
      " [[ 1.9333000e+02  1.9444000e+02  1.9292000e+02  3.7283200e+07\n",
      "   1.9335890e+02  4.5000000e-01  5.2369165e+07]\n",
      " [ 1.9367000e+02  1.9564000e+02  1.9332000e+02  4.7471900e+07\n",
      "   1.9423770e+02  4.5000000e-01  5.2206220e+07]\n",
      " [ 1.9602000e+02  1.9720000e+02  1.9255000e+02  4.7460200e+07\n",
      "   1.9295940e+02 -6.6000000e-01  5.2018390e+07]\n",
      " [ 1.9467000e+02  1.9663000e+02  1.9414000e+02  4.8291400e+07\n",
      "   1.9556590e+02  1.3500000e+00  5.2115595e+07]\n",
      " [ 1.9606000e+02  1.9649000e+02  1.9526000e+02  3.8824100e+07\n",
      "   1.9618510e+02  3.2000000e-01  4.9803320e+07]]\n",
      "Predicted Close Value for the Next Day (Original Scale): [[116.38846]]\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model', 'scaler_x', 'scaler_y', 'input_size' are defined\n",
    "\n",
    "# Set the window size for prediction\n",
    "prediction_window_size = 1  # Predicting for the next day\n",
    "\n",
    "# Initialize the rolling window with the last values from the training set\n",
    "initial_rolling_window = data.iloc[-params['window_size']:][['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "\n",
    "# Print the initial rolling window values and dimensions\n",
    "print(\"Initial Rolling Window (Original Values):\\n\", initial_rolling_window)\n",
    "\n",
    "\n",
    "# Check the number of features in the rolling window\n",
    "if initial_rolling_window.shape[1] != input_size:\n",
    "    # Print or handle the inconsistency\n",
    "    print(\"Number of features in the rolling window does not match the expected input_size.\")\n",
    "\n",
    "# Scale the input features using the trained scaler\n",
    "scaled_input = scaler_x.transform(initial_rolling_window)\n",
    "\n",
    "# Convert the scaled input features to a PyTorch tensor\n",
    "input_tensor = torch.tensor(scaled_input, dtype=torch.float32).view(1, params['window_size'], input_size)\n",
    "\n",
    "# Initialize the hidden state\n",
    "hidden = model.init_hidden(1)\n",
    "\n",
    "# Perform the prediction\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predicted_close_scaled, _ = model(input_tensor, hidden)\n",
    "\n",
    "# Inverse transform the predicted close value to the original scale\n",
    "predicted_close_original = scaler_y.inverse_transform(predicted_close_scaled.numpy())\n",
    "\n",
    "# Print the predicted close value\n",
    "print(\"Predicted Close Value for the Next Day (Original Scale):\", predicted_close_original)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
