{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate Scheduler:\n",
    "\n",
    "Implement a learning rate scheduler.\n",
    "Gradient Clipping:\n",
    "\n",
    "Clip the gradients during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Gradienten zeigen die Richtung und die Steigung der Verlustfunktion im Modellraum an.\n",
    "can help diagnose issues such as vanishing or exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploding Gradients: Conversely, exploding gradients occur when the gradients become extremely large. This can happen during training when the gradients are multiplied across many layers, leading to exponentially growing values. Exploding gradients can cause instability during training, with large parameter updates that may overshoot the optimal solution and result in numerical overflow. This can prevent the network from converging to a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vanishing Gradients: This occurs when the gradients become very small as they are backpropagated through the network \n",
    "layers during training. When gradients vanish, it means that the updates to the parameters become negligible,\n",
    "leading to slow or no learning. This often happens in deep networks with many layers, especially when using activation\n",
    " functions like sigmoid or tanh, which have derivatives that tend to zero in certain regions. As a result, the network\n",
    " fails to learn long-range dependencies and may plateau prematurely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Clipping: Gradient clipping involves scaling gradients when their norm exceeds a certain threshold.\n",
    "This prevents gradients from becoming too small or too large, helping to mitigate the vanishing gradient problem and \n",
    "stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    # Initialize lists to store training and temporary sets\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    # Iterate through the data with the specified window size\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i+split_window_size+1]\n",
    "        y_out1_out2 = y_in.iloc[i:i+split_window_size+1]\n",
    "\n",
    "        # Separate the last row for the temporary set\n",
    "        # [ :-1]: all elements except the last one\n",
    "        # [-1:]:  selects only the last element\n",
    "        # (:) is used to indicate slicing of a sequence\n",
    "        # sequence[start : end : step]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    # Concatenate the lists into pandas DataFrames\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "# Split Data to train and temp\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 3\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split temp into val and test\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 1\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.01, window_size=20\n",
      "Epoch 1/150000, Loss: 29889.6171875, Validation Loss: 30381.451171875, Gradient: 223.41114807128906\n",
      "Epoch 101/150000, Loss: 28942.427734375, Validation Loss: 29417.9140625, Gradient: 214.7916717529297\n",
      "Epoch 201/150000, Loss: 27956.173828125, Validation Loss: 28422.931640625, Gradient: 205.37060546875\n",
      "Epoch 301/150000, Loss: 27119.40625, Validation Loss: 27576.611328125, Gradient: 197.04705810546875\n",
      "Epoch 401/150000, Loss: 26354.35546875, Validation Loss: 26802.232421875, Gradient: 189.11952209472656\n",
      "Epoch 501/150000, Loss: 25644.25390625, Validation Loss: 26083.021484375, Gradient: 181.4528350830078\n",
      "Epoch 601/150000, Loss: 24981.658203125, Validation Loss: 25411.521484375, Gradient: 173.99517822265625\n",
      "Epoch 701/150000, Loss: 24362.10546875, Validation Loss: 24783.2578125, Gradient: 166.72084045410156\n",
      "Epoch 801/150000, Loss: 23782.435546875, Validation Loss: 24195.0625, Gradient: 159.6150665283203\n",
      "Epoch 901/150000, Loss: 23240.158203125, Validation Loss: 23644.43359375, Gradient: 152.66867065429688\n",
      "Epoch 1001/150000, Loss: 22733.173828125, Validation Loss: 23129.26953125, Gradient: 145.87559509277344\n",
      "Epoch 1101/150000, Loss: 22259.62890625, Validation Loss: 22647.71875, Gradient: 139.2317352294922\n",
      "Epoch 1201/150000, Loss: 21646.974609375, Validation Loss: 22047.0546875, Gradient: 148.51083374023438\n",
      "Epoch 1301/150000, Loss: 21122.53515625, Validation Loss: 21520.171875, Gradient: 144.31822204589844\n",
      "Epoch 1401/150000, Loss: 20620.75390625, Validation Loss: 21013.181640625, Gradient: 140.8124237060547\n",
      "Epoch 1501/150000, Loss: 20134.99609375, Validation Loss: 20518.9375, Gradient: 137.5883026123047\n",
      "Epoch 1601/150000, Loss: 19664.248046875, Validation Loss: 20040.05078125, Gradient: 134.3055419921875\n",
      "Epoch 1701/150000, Loss: 19207.837890625, Validation Loss: 19575.6484375, Gradient: 131.20126342773438\n",
      "Epoch 1801/150000, Loss: 18764.994140625, Validation Loss: 19125.298828125, Gradient: 128.20651245117188\n",
      "Epoch 1901/150000, Loss: 18334.900390625, Validation Loss: 18688.142578125, Gradient: 125.31965637207031\n",
      "Epoch 2001/150000, Loss: 17916.8984375, Validation Loss: 18263.41796875, Gradient: 122.5112533569336\n",
      "Epoch 2101/150000, Loss: 17510.544921875, Validation Loss: 17850.595703125, Gradient: 119.74919128417969\n",
      "Epoch 2201/150000, Loss: 17115.671875, Validation Loss: 17449.552734375, Gradient: 117.00967407226562\n",
      "Epoch 2301/150000, Loss: 16732.103515625, Validation Loss: 17060.029296875, Gradient: 114.30693054199219\n",
      "Epoch 2401/150000, Loss: 16359.5634765625, Validation Loss: 16681.740234375, Gradient: 111.64897155761719\n",
      "Epoch 2501/150000, Loss: 15997.765625, Validation Loss: 16314.4951171875, Gradient: 109.03756713867188\n",
      "Epoch 2601/150000, Loss: 15646.4091796875, Validation Loss: 15958.08203125, Gradient: 106.47567749023438\n",
      "Epoch 2701/150000, Loss: 15305.177734375, Validation Loss: 15612.3017578125, Gradient: 103.962646484375\n",
      "Epoch 2801/150000, Loss: 14973.7861328125, Validation Loss: 15276.7509765625, Gradient: 101.48684692382812\n",
      "Epoch 2901/150000, Loss: 14652.029296875, Validation Loss: 14950.5908203125, Gradient: 99.05160522460938\n",
      "Epoch 3001/150000, Loss: 14338.302734375, Validation Loss: 14632.02734375, Gradient: 96.9299087524414\n",
      "Epoch 3101/150000, Loss: 14032.9521484375, Validation Loss: 14321.72265625, Gradient: 94.70370483398438\n",
      "Epoch 3201/150000, Loss: 13735.7509765625, Validation Loss: 14019.568359375, Gradient: 92.53089904785156\n",
      "Epoch 3301/150000, Loss: 13446.3837890625, Validation Loss: 13726.513671875, Gradient: 90.42633056640625\n",
      "Epoch 3401/150000, Loss: 13164.4931640625, Validation Loss: 13441.625, Gradient: 88.38311767578125\n",
      "Epoch 3501/150000, Loss: 12889.724609375, Validation Loss: 13164.357421875, Gradient: 86.40028381347656\n",
      "Epoch 3601/150000, Loss: 12621.8408203125, Validation Loss: 12893.5439453125, Gradient: 84.46797943115234\n",
      "Epoch 3701/150000, Loss: 12360.2998046875, Validation Loss: 12628.193359375, Gradient: 82.6427230834961\n",
      "Epoch 3801/150000, Loss: 12104.1904296875, Validation Loss: 12367.0888671875, Gradient: 80.92716979980469\n",
      "Epoch 3901/150000, Loss: 11853.400390625, Validation Loss: 12109.490234375, Gradient: 79.22219848632812\n",
      "Epoch 4001/150000, Loss: 11608.6123046875, Validation Loss: 11861.7802734375, Gradient: 77.57987213134766\n",
      "Epoch 4101/150000, Loss: 11369.2939453125, Validation Loss: 11619.64453125, Gradient: 75.94937133789062\n",
      "Epoch 4201/150000, Loss: 11135.3486328125, Validation Loss: 11383.0107421875, Gradient: 74.35618591308594\n",
      "Epoch 4301/150000, Loss: 10906.4462890625, Validation Loss: 11150.892578125, Gradient: 72.82176971435547\n",
      "Epoch 4401/150000, Loss: 10682.3046875, Validation Loss: 10923.5595703125, Gradient: 71.34335327148438\n",
      "Epoch 4501/150000, Loss: 10462.6376953125, Validation Loss: 10701.302734375, Gradient: 69.91503143310547\n",
      "Epoch 4601/150000, Loss: 10247.447265625, Validation Loss: 10483.4736328125, Gradient: 68.51993560791016\n",
      "Epoch 4701/150000, Loss: 10036.5087890625, Validation Loss: 10269.72265625, Gradient: 67.37547302246094\n",
      "Epoch 4801/150000, Loss: 9829.5537109375, Validation Loss: 10059.880859375, Gradient: 65.84464263916016\n",
      "Epoch 4901/150000, Loss: 9626.8349609375, Validation Loss: 9853.7392578125, Gradient: 64.56588745117188\n",
      "Epoch 5001/150000, Loss: 9427.5244140625, Validation Loss: 9650.970703125, Gradient: 63.35307693481445\n",
      "Epoch 5101/150000, Loss: 9231.8515625, Validation Loss: 9451.4189453125, Gradient: 62.187347412109375\n",
      "Epoch 5201/150000, Loss: 9039.658203125, Validation Loss: 9256.1650390625, Gradient: 61.07274627685547\n",
      "Epoch 5301/150000, Loss: 8850.4541015625, Validation Loss: 9065.3759765625, Gradient: 59.98353576660156\n",
      "Epoch 5401/150000, Loss: 8674.2841796875, Validation Loss: 8877.25390625, Gradient: 59.04275894165039\n",
      "Epoch 5501/150000, Loss: 8481.150390625, Validation Loss: 8689.8349609375, Gradient: 57.90808868408203\n",
      "Epoch 5601/150000, Loss: 8300.625, Validation Loss: 8506.904296875, Gradient: 56.914913177490234\n",
      "Epoch 5701/150000, Loss: 8122.869140625, Validation Loss: 8327.091796875, Gradient: 55.94779968261719\n",
      "Epoch 5801/150000, Loss: 7947.60888671875, Validation Loss: 8149.919921875, Gradient: 54.98604202270508\n",
      "Epoch 5901/150000, Loss: 7775.0048828125, Validation Loss: 7975.20263671875, Gradient: 54.089874267578125\n",
      "Epoch 6001/150000, Loss: 7605.0029296875, Validation Loss: 7802.9794921875, Gradient: 53.178489685058594\n",
      "Epoch 6101/150000, Loss: 7437.482421875, Validation Loss: 7633.17919921875, Gradient: 52.308265686035156\n",
      "Epoch 6201/150000, Loss: 7272.3662109375, Validation Loss: 7465.8291015625, Gradient: 51.45093536376953\n",
      "Epoch 6301/150000, Loss: 7109.6279296875, Validation Loss: 7301.2119140625, Gradient: 50.60582733154297\n",
      "Epoch 6401/150000, Loss: 6949.3193359375, Validation Loss: 7139.20068359375, Gradient: 49.75330352783203\n",
      "Epoch 6501/150000, Loss: 6791.4150390625, Validation Loss: 6979.63134765625, Gradient: 48.908042907714844\n",
      "Epoch 6601/150000, Loss: 6635.90234375, Validation Loss: 6822.29541015625, Gradient: 48.14508819580078\n",
      "Epoch 6701/150000, Loss: 6482.77734375, Validation Loss: 6667.34521484375, Gradient: 47.6191291809082\n",
      "Epoch 6801/150000, Loss: 6331.96044921875, Validation Loss: 6514.64501953125, Gradient: 46.56084442138672\n",
      "Epoch 6901/150000, Loss: 6183.51708984375, Validation Loss: 6364.44677734375, Gradient: 45.80886459350586\n",
      "Epoch 7001/150000, Loss: 6037.515625, Validation Loss: 6217.029296875, Gradient: 44.52574157714844\n",
      "Epoch 7101/150000, Loss: 5893.76416015625, Validation Loss: 6071.6572265625, Gradient: 44.23698425292969\n",
      "Epoch 7201/150000, Loss: 5752.49951171875, Validation Loss: 5928.96044921875, Gradient: 43.50375747680664\n",
      "Epoch 7301/150000, Loss: 5613.6630859375, Validation Loss: 5788.7802734375, Gradient: 42.705604553222656\n",
      "Epoch 7401/150000, Loss: 5477.2939453125, Validation Loss: 5651.01171875, Gradient: 41.91214370727539\n",
      "Epoch 7501/150000, Loss: 5343.39697265625, Validation Loss: 5515.8583984375, Gradient: 41.19533157348633\n",
      "Epoch 7601/150000, Loss: 5212.025390625, Validation Loss: 5383.20947265625, Gradient: 40.101139068603516\n",
      "Epoch 7701/150000, Loss: 5083.04345703125, Validation Loss: 5252.9638671875, Gradient: 39.711910247802734\n",
      "Epoch 7801/150000, Loss: 4956.39892578125, Validation Loss: 5125.21630859375, Gradient: 39.01673889160156\n",
      "Epoch 7901/150000, Loss: 4831.9228515625, Validation Loss: 5000.1923828125, Gradient: 38.34012985229492\n",
      "Epoch 8001/150000, Loss: 4709.70849609375, Validation Loss: 4877.34912109375, Gradient: 37.646114349365234\n",
      "Epoch 8101/150000, Loss: 4589.8095703125, Validation Loss: 4756.775390625, Gradient: 37.46456527709961\n",
      "Epoch 8201/150000, Loss: 4471.8583984375, Validation Loss: 4639.095703125, Gradient: 36.3315544128418\n",
      "Epoch 8301/150000, Loss: 4356.072265625, Validation Loss: 4523.03955078125, Gradient: 35.632259368896484\n",
      "Epoch 8401/150000, Loss: 4242.5126953125, Validation Loss: 4409.0625, Gradient: 35.00518798828125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3,5]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.01]\n",
    "window_sizes = [20]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "                # Monitor gradients, print(\"Gradients:\"), print(f\"{name}: {param.grad.norm().item()}\")\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Gradient: {param.grad.norm().item()}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.01, window_size=20\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 27.775970458984375\n",
      "lstm.weight_hh_l0: 8.216531753540039\n",
      "lstm.bias_ih_l0: 29.54005241394043\n",
      "lstm.bias_hh_l0: 29.54005241394043\n",
      "fc.weight: 62.21171951293945\n",
      "fc.bias: 225.13833618164062\n",
      "Epoch 1/150000, Loss: 30088.00390625, Validation Loss: 30581.4140625, Gradient: 225.13833618164062\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 3.076817750930786\n",
      "lstm.weight_hh_l0: 7.473586559295654\n",
      "lstm.bias_ih_l0: 3.7648816108703613\n",
      "lstm.bias_hh_l0: 3.7648816108703613\n",
      "fc.weight: 422.8688049316406\n",
      "fc.bias: 212.78114318847656\n",
      "Epoch 101/150000, Loss: 28729.052734375, Validation Loss: 29201.6015625, Gradient: 212.78114318847656\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 1.2316036224365234\n",
      "lstm.weight_hh_l0: 3.015854835510254\n",
      "lstm.bias_ih_l0: 1.510603904724121\n",
      "lstm.bias_hh_l0: 1.510603904724121\n",
      "fc.weight: 403.360107421875\n",
      "fc.bias: 201.97323608398438\n",
      "Epoch 201/150000, Loss: 27610.033203125, Validation Loss: 28070.958984375, Gradient: 201.97323608398438\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 0.6855803728103638\n",
      "lstm.weight_hh_l0: 1.6663349866867065\n",
      "lstm.bias_ih_l0: 0.8338017463684082\n",
      "lstm.bias_hh_l0: 0.8338017463684082\n",
      "fc.weight: 383.6659240722656\n",
      "fc.bias: 191.94725036621094\n",
      "Epoch 301/150000, Loss: 26623.275390625, Validation Loss: 27072.642578125, Gradient: 191.94725036621094\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 0.4423336386680603\n",
      "lstm.weight_hh_l0: 1.057904601097107\n",
      "lstm.bias_ih_l0: 0.5291673541069031\n",
      "lstm.bias_hh_l0: 0.5291673541069031\n",
      "fc.weight: 364.57098388671875\n",
      "fc.bias: 182.34210205078125\n",
      "Epoch 401/150000, Loss: 25724.80859375, Validation Loss: 26162.921875, Gradient: 182.34210205078125\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 0.31051480770111084\n",
      "lstm.weight_hh_l0: 0.7248368859291077\n",
      "lstm.bias_ih_l0: 0.36250656843185425\n",
      "lstm.bias_hh_l0: 0.36250656843185425\n",
      "fc.weight: 346.0562744140625\n",
      "fc.bias: 173.06007385253906\n",
      "Epoch 501/150000, Loss: 24900.275390625, Validation Loss: 25327.443359375, Gradient: 173.06007385253906\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 0.23021480441093445\n",
      "lstm.weight_hh_l0: 0.5195866823196411\n",
      "lstm.bias_ih_l0: 0.2598338723182678\n",
      "lstm.bias_hh_l0: 0.2598338723182678\n",
      "fc.weight: 328.0816650390625\n",
      "fc.bias: 164.06031799316406\n",
      "Epoch 601/150000, Loss: 24141.890625, Validation Loss: 24558.400390625, Gradient: 164.06031799316406\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 0.17737270891666412\n",
      "lstm.weight_hh_l0: 0.38234058022499084\n",
      "lstm.bias_ih_l0: 0.19118890166282654\n",
      "lstm.bias_hh_l0: 0.19118890166282654\n",
      "fc.weight: 310.61865234375\n",
      "fc.bias: 155.3219451904297\n",
      "Epoch 701/150000, Loss: 23444.244140625, Validation Loss: 23850.380859375, Gradient: 155.3219451904297\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 0.14071916043758392\n",
      "lstm.weight_hh_l0: 0.28478163480758667\n",
      "lstm.bias_ih_l0: 0.14240024983882904\n",
      "lstm.bias_hh_l0: 0.14240024983882904\n",
      "fc.weight: 293.64947509765625\n",
      "fc.bias: 146.83309936523438\n",
      "Epoch 801/150000, Loss: 22803.05859375, Validation Loss: 23199.09375, Gradient: 146.83309936523438\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 0.1143733486533165\n",
      "lstm.weight_hh_l0: 0.21191050112247467\n",
      "lstm.bias_ih_l0: 0.10596022754907608\n",
      "lstm.bias_hh_l0: 0.10596022754907608\n",
      "fc.weight: 277.1621398925781\n",
      "fc.bias: 138.586669921875\n",
      "Epoch 901/150000, Loss: 22214.666015625, Validation Loss: 22600.87109375, Gradient: 138.586669921875\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 0.09566216170787811\n",
      "lstm.weight_hh_l0: 0.15887489914894104\n",
      "lstm.bias_ih_l0: 0.0794515535235405\n",
      "lstm.bias_hh_l0: 0.0794515535235405\n",
      "fc.weight: 261.1500244140625\n",
      "fc.bias: 130.5784912109375\n",
      "Epoch 1001/150000, Loss: 21675.794921875, Validation Loss: 22052.4375, Gradient: 130.5784912109375\n",
      "Gradients:\n",
      "lstm.weight_ih_l0: 25.456371307373047\n",
      "lstm.weight_hh_l0: 47.807029724121094\n",
      "lstm.bias_ih_l0: 36.51267623901367\n",
      "lstm.bias_hh_l0: 36.51267623901367\n",
      "fc.weight: 286.1449890136719\n",
      "fc.bias: 143.22027587890625\n",
      "Epoch 1101/150000, Loss: 20909.634765625, Validation Loss: 21312.365234375, Gradient: 143.22027587890625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Split the training data into sliding windows\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m x_train_seq, y_train_seq \u001b[38;5;241m=\u001b[39m \u001b[43msplit_data_with_sliding_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     98\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(x_train_seq)\n",
      "Cell \u001b[1;32mIn[4], line 79\u001b[0m, in \u001b[0;36msplit_data_with_sliding_window\u001b[1;34m(x_train_tensor, y_train_tensor, window_size)\u001b[0m\n\u001b[0;32m     76\u001b[0m     y_window \u001b[38;5;241m=\u001b[39m y_train_tensor[i\u001b[38;5;241m+\u001b[39mwindow_size]  \u001b[38;5;66;03m# Next entry as target output\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     x_seq_list\u001b[38;5;241m.\u001b[39mappend(x_window)\n\u001b[1;32m---> 79\u001b[0m     y_seq_list\u001b[38;5;241m.\u001b[39mappend(y_window)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Concatenate the lists into tensors\u001b[39;00m\n\u001b[0;32m     82\u001b[0m x_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(x_seq_list)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.01]\n",
    "window_sizes = [20]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "         # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    #print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            zval_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, \")\n",
    "\n",
    "          \n",
    "                    print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
