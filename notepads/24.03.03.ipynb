{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "#print(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "y_data = data[\"close\"].values.reshape(-1, 1)  # Reshape y_data if it's 1D\n",
    "\n",
    "# Initialize the scalers\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both training and test data\n",
    "x_data_scaled = scaler_x.fit_transform(x_data)\n",
    "y_data_scaled = scaler_y.fit_transform(y_data)\n",
    "\n",
    "# Convert scaled data to Tensors\n",
    "x_feature_tensors = torch.tensor(x_data_scaled, dtype=torch.float32)\n",
    "y_feature_tensors = torch.tensor(y_data_scaled, dtype=torch.float32)\n",
    "\n",
    "# Split the training data into training and temporary sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_feature_tensors, y_feature_tensors, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert scaled labels back to numpy arrays\n",
    "y_train = y_train.numpy()\n",
    "y_val = y_val.numpy()\n",
    "y_test = y_test.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Now you can use x_train_scaled, x_val, x_test, y_train_scaled, y_val_scaled, y_test_scaled for your model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Last 5 values of x\\nprint(\"Last 5 values of x:\")\\nprint(x_data[-5:])\\nprint(\"\\n\")\\n\\n# Last 5 values of y\\nprint(\"Last 5 values of y:\")\\nprint(y_data[-5:])\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Last 5 values of x\n",
    "print(\"Last 5 values of x:\")\n",
    "print(x_data[-5:])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Last 5 values of y\n",
    "print(\"Last 5 values of y:\")\n",
    "print(y_data[-5:])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Inverse transform the scaled data to get original scales\\nx_original = scaler_x.inverse_transform(x_feature_tensors.numpy())\\ny_original = scaler_y.inverse_transform(y_feature_tensors.numpy())\\n\\n# Print the last 5 values of original x and y\\nprint(\"Last 5 values of original x:\")\\nprint(x_original[-5:])\\nprint(\"\\n\")\\n\\nprint(\"Last 5 values of original y:\")\\nprint(y_original[-5:])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Inverse transform the scaled data to get original scales\n",
    "x_original = scaler_x.inverse_transform(x_feature_tensors.numpy())\n",
    "y_original = scaler_y.inverse_transform(y_feature_tensors.numpy())\n",
    "\n",
    "# Print the last 5 values of original x and y\n",
    "print(\"Last 5 values of original x:\")\n",
    "print(x_original[-5:])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Last 5 values of original y:\")\n",
    "print(y_original[-5:])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 4\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_11712\\3789981942.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_11712\\3789981942.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000, Train Loss: 1.0496785640716553, Validation Loss: 2.250458240509033\n",
      "Iteration 2000, Train Loss: 2.076263427734375, Validation Loss: 2.236117124557495\n",
      "Iteration 3000, Train Loss: 0.23874036967754364, Validation Loss: 2.2375779151916504\n",
      "Iteration 4000, Train Loss: 0.3783578872680664, Validation Loss: 2.2188756465911865\n",
      "Iteration 5000, Train Loss: 0.30434587597846985, Validation Loss: 2.2213265895843506\n",
      "Iteration 6000, Train Loss: 0.16998176276683807, Validation Loss: 2.2246384620666504\n",
      "Iteration 7000, Train Loss: 0.2303958386182785, Validation Loss: 2.2296674251556396\n",
      "Iteration 8000, Train Loss: 0.33391833305358887, Validation Loss: 2.2227959632873535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_11712\\3789981942.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4176923334598541\n",
      "Iteration 1000, Train Loss: 1.0503355264663696, Validation Loss: 2.251126289367676\n",
      "Iteration 2000, Train Loss: 2.078392744064331, Validation Loss: 2.236886501312256\n",
      "Iteration 3000, Train Loss: 0.23733869194984436, Validation Loss: 2.2383456230163574\n",
      "Iteration 4000, Train Loss: 0.377674400806427, Validation Loss: 2.21925687789917\n",
      "Iteration 5000, Train Loss: 0.30314236879348755, Validation Loss: 2.2219700813293457\n",
      "Iteration 6000, Train Loss: 0.1688717156648636, Validation Loss: 2.225498676300049\n",
      "Iteration 7000, Train Loss: 0.22905020415782928, Validation Loss: 2.230363368988037\n",
      "Iteration 8000, Train Loss: 0.33254170417785645, Validation Loss: 2.2233097553253174\n",
      "Test Loss: 0.41825518012046814\n",
      "Iteration 1000, Train Loss: 1.0484178066253662, Validation Loss: 2.2493083477020264\n",
      "Iteration 2000, Train Loss: 2.0740513801574707, Validation Loss: 2.235318899154663\n",
      "Iteration 3000, Train Loss: 0.2400786429643631, Validation Loss: 2.2368478775024414\n",
      "Iteration 4000, Train Loss: 0.37785103917121887, Validation Loss: 2.2191507816314697\n",
      "Iteration 5000, Train Loss: 0.30437853932380676, Validation Loss: 2.2213187217712402\n",
      "Iteration 6000, Train Loss: 0.1694381982088089, Validation Loss: 2.2250542640686035\n",
      "Iteration 7000, Train Loss: 0.23070672154426575, Validation Loss: 2.2295053005218506\n",
      "Iteration 8000, Train Loss: 0.33312374353408813, Validation Loss: 2.223076343536377\n",
      "Test Loss: 0.4170778691768646\n",
      "Iteration 1000, Train Loss: 1.0487817525863647, Validation Loss: 2.2494912147521973\n",
      "Iteration 2000, Train Loss: 2.072580099105835, Validation Loss: 2.2347986698150635\n",
      "Iteration 3000, Train Loss: 0.2400791347026825, Validation Loss: 2.23687481880188\n",
      "Iteration 4000, Train Loss: 0.38014546036720276, Validation Loss: 2.2179040908813477\n",
      "Iteration 5000, Train Loss: 0.30577903985977173, Validation Loss: 2.220552444458008\n",
      "Iteration 6000, Train Loss: 0.17016158998012543, Validation Loss: 2.2245001792907715\n",
      "Iteration 7000, Train Loss: 0.23066337406635284, Validation Loss: 2.229531764984131\n",
      "Iteration 8000, Train Loss: 0.3350953459739685, Validation Loss: 2.222371816635132\n",
      "Test Loss: 0.4177941381931305\n",
      "Iteration 1000, Train Loss: 1.0486176013946533, Validation Loss: 2.2491605281829834\n",
      "Iteration 2000, Train Loss: 2.071002721786499, Validation Loss: 2.2342426776885986\n",
      "Iteration 3000, Train Loss: 0.23992690443992615, Validation Loss: 2.236985445022583\n",
      "Iteration 4000, Train Loss: 0.382459819316864, Validation Loss: 2.216679811477661\n",
      "Iteration 5000, Train Loss: 0.30731359124183655, Validation Loss: 2.2197306156158447\n",
      "Iteration 6000, Train Loss: 0.1712830513715744, Validation Loss: 2.2236533164978027\n",
      "Iteration 7000, Train Loss: 0.23073513805866241, Validation Loss: 2.229501247406006\n",
      "Iteration 8000, Train Loss: 0.2883072793483734, Validation Loss: 2.211120367050171\n",
      "Test Loss: 0.37347155809402466\n",
      "Iteration 1000, Train Loss: 1.0488241910934448, Validation Loss: 2.2496023178100586\n",
      "Iteration 2000, Train Loss: 2.0737979412078857, Validation Loss: 2.2352333068847656\n",
      "Iteration 3000, Train Loss: 0.23947644233703613, Validation Loss: 2.237196445465088\n",
      "Iteration 4000, Train Loss: 0.3795665204524994, Validation Loss: 2.218217372894287\n",
      "Iteration 5000, Train Loss: 0.3052158057689667, Validation Loss: 2.220855236053467\n",
      "Iteration 6000, Train Loss: 0.17052045464515686, Validation Loss: 2.2242283821105957\n",
      "Iteration 7000, Train Loss: 0.23060207068920135, Validation Loss: 2.2295644283294678\n",
      "Iteration 8000, Train Loss: 0.33557599782943726, Validation Loss: 2.222203016281128\n",
      "Test Loss: 0.41810083389282227\n",
      "Iteration 1000, Train Loss: 1.0489017963409424, Validation Loss: 2.2498090267181396\n",
      "Iteration 2000, Train Loss: 2.075518846511841, Validation Loss: 2.2358450889587402\n",
      "Iteration 3000, Train Loss: 0.23957869410514832, Validation Loss: 2.2371084690093994\n",
      "Iteration 4000, Train Loss: 0.37704235315322876, Validation Loss: 2.2195992469787598\n",
      "Iteration 5000, Train Loss: 0.3038685619831085, Validation Loss: 2.221592664718628\n",
      "Iteration 6000, Train Loss: 0.16929367184638977, Validation Loss: 2.2251667976379395\n",
      "Iteration 7000, Train Loss: 0.23056364059448242, Validation Loss: 2.2295782566070557\n",
      "Iteration 8000, Train Loss: 0.3327566087245941, Validation Loss: 2.223212480545044\n",
      "Test Loss: 0.41715455055236816\n",
      "Iteration 1000, Train Loss: 1.0486063957214355, Validation Loss: 2.249448776245117\n",
      "Iteration 2000, Train Loss: 2.0734927654266357, Validation Loss: 2.235119342803955\n",
      "Iteration 3000, Train Loss: 0.24036724865436554, Validation Loss: 2.2366926670074463\n",
      "Iteration 4000, Train Loss: 0.378288209438324, Validation Loss: 2.2189080715179443\n",
      "Iteration 5000, Train Loss: 0.3049757778644562, Validation Loss: 2.220998764038086\n",
      "Iteration 6000, Train Loss: 0.17008662223815918, Validation Loss: 2.224555492401123\n",
      "Iteration 7000, Train Loss: 0.23125679790973663, Validation Loss: 2.229224681854248\n",
      "Iteration 8000, Train Loss: 0.33420950174331665, Validation Loss: 2.2226784229278564\n",
      "Test Loss: 0.4170411229133606\n",
      "Iteration 1000, Train Loss: 1.049216389656067, Validation Loss: 2.250068187713623\n",
      "Iteration 2000, Train Loss: 2.076308488845825, Validation Loss: 2.236131191253662\n",
      "Iteration 3000, Train Loss: 0.23873896896839142, Validation Loss: 2.2375693321228027\n",
      "Iteration 4000, Train Loss: 0.3775738775730133, Validation Loss: 2.219308376312256\n",
      "Iteration 5000, Train Loss: 0.3035350739955902, Validation Loss: 2.221766948699951\n",
      "Iteration 6000, Train Loss: 0.16928531229496002, Validation Loss: 2.2251758575439453\n",
      "Iteration 7000, Train Loss: 0.22995933890342712, Validation Loss: 2.229891061782837\n",
      "Iteration 8000, Train Loss: 0.33294254541397095, Validation Loss: 2.2231500148773193\n",
      "Test Loss: 0.4175959527492523\n",
      "Iteration 1000, Train Loss: 1.0503222942352295, Validation Loss: 2.2511281967163086\n",
      "Iteration 2000, Train Loss: 2.079409599304199, Validation Loss: 2.2372565269470215\n",
      "Iteration 3000, Train Loss: 0.2365502417087555, Validation Loss: 2.238781452178955\n",
      "Iteration 4000, Train Loss: 0.37807223200798035, Validation Loss: 2.219041585922241\n",
      "Iteration 5000, Train Loss: 0.3030431568622589, Validation Loss: 2.2220118045806885\n",
      "Iteration 6000, Train Loss: 0.16886065900325775, Validation Loss: 2.225511074066162\n",
      "Iteration 7000, Train Loss: 0.22826290130615234, Validation Loss: 2.2307770252227783\n",
      "Iteration 8000, Train Loss: 0.3327123522758484, Validation Loss: 2.2232651710510254\n",
      "Test Loss: 0.41924038529396057\n",
      "Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 0.37347155809402466}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled, x_test_scaled, y_test_scaled,\n",
    "# input_size, output_size, test_window_size, scaler are available\n",
    "\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'learning_rate': [0.00005],\n",
    "    'window_size': [ 5],\n",
    "    'hidden_dim': [256],\n",
    "    'n_layers': [11],\n",
    "    'batch_evaluation_frequency': [4]\n",
    "}\n",
    "\n",
    "#Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 0.41702309250831604}\n",
    "#              'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 6463.20556640625}\n",
    "\n",
    "\n",
    "# Number of random search iterations\n",
    "num_iterations = 10\n",
    "\n",
    "best_params = None\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    params = {\n",
    "        'learning_rate': random.choice(search_space['learning_rate']),\n",
    "        'window_size': random.choice(search_space['window_size']),\n",
    "        'hidden_dim': random.choice(search_space['hidden_dim']),\n",
    "        'n_layers': random.choice(search_space['n_layers']),\n",
    "        'batch_evaluation_frequency': random.choice(search_space['batch_evaluation_frequency'])\n",
    "    }\n",
    "\n",
    "    model = LSTMModel(input_size, params['hidden_dim'], params['n_layers'], output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Training using Walk-Forward Validation\n",
    "    for i in range(params['window_size'], len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_idx = i - params['window_size']\n",
    "        end_idx = i\n",
    "        x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        y_window = torch.tensor(y_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        x_window = x_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        outputs, hidden = model(x_window, hidden)\n",
    "        loss = criterion(outputs, y_window)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % params['batch_evaluation_frequency'] == 0:\n",
    "            with torch.no_grad():\n",
    "                x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n",
    "                y_val_window = torch.tensor(y_val[:params['window_size']], dtype=torch.float32)\n",
    "                x_val_window = x_val_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "                hidden = model.init_hidden(1)\n",
    "                val_outputs, _ = model(x_val_window, hidden)\n",
    "                val_loss = criterion(val_outputs, y_val_window)\n",
    "\n",
    "                if i % 1000 == 0:  # Print every 1000 iterations\n",
    "                    print(f\"Iteration {i}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # Test set evaluation\n",
    "    with torch.no_grad():\n",
    "        x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n",
    "        y_test_window = torch.tensor(y_test[:params['window_size']], dtype=torch.float32)\n",
    "        x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        test_outputs, _ = model(x_test_window, hidden)\n",
    "        test_loss = criterion(test_outputs, y_test_window)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "    # Update the best_params if the current model performs better on the test set\n",
    "    if best_params is None:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "    elif test_loss < best_params['test_loss']:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last Window of Features Used by the Model (Original Scale):\n",
      "         open        high         low        volume  adjusted_close  \\\n",
      "0  606.180410  616.279988  606.060000  2.168320e+07         18.6277   \n",
      "1   31.875203   31.875196   31.628800  1.057993e+05          0.1094   \n",
      "2   39.244802   39.995203   38.998398  7.496000e+05          0.2775   \n",
      "3   34.742399   35.369603   34.630403  9.310999e+05          0.1224   \n",
      "4  568.100409  569.500412  563.379596  7.286000e+06         17.6792   \n",
      "\n",
      "   change_percent   avg_vol_20d  \n",
      "0            1.24  7.270806e+08  \n",
      "1           -0.82  7.940575e+07  \n",
      "2            0.95  1.257654e+08  \n",
      "3            2.17  1.837226e+08  \n",
      "4           -0.66  3.518187e+08  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_11712\\2384865010.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_window = torch.tensor(x_test[-params['window_size']:], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# ... (previous code)\n",
    "\n",
    "# Test set evaluation\n",
    "with torch.no_grad():\n",
    "    x_test_window = torch.tensor(x_test[-params['window_size']:], dtype=torch.float32)\n",
    "    y_test_window = torch.tensor(y_test[-params['window_size']:], dtype=torch.float32)\n",
    "    x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "    hidden = model.init_hidden(1)\n",
    "    _, _ = model(x_test_window, hidden)  # We only need to run the model to get the hidden states\n",
    "\n",
    "# Convert the scaled features back to original scale\n",
    "original_last_window_features = scaler_x.inverse_transform(x_test[-params['window_size']:])\n",
    "\n",
    "# Print the last window of features that the model uses in original scale\n",
    "last_window_features = pd.DataFrame(original_last_window_features, columns=['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d'])\n",
    "\n",
    "\n",
    "#print(\"\\nLast Window of Features Used by the Model (Original Scale):\")\n",
    "#print(last_window_features)\n",
    "# ... (rest of the code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Test Window:\n",
      "Predicted (scaled): [[0.03434191]]\n",
      "Actual (scaled): [[-0.6992109 ]\n",
      " [ 0.6115489 ]\n",
      " [ 0.5079761 ]\n",
      " [-0.66462284]\n",
      " [-0.681237  ]]\n",
      "\n",
      "Last 5 Entries of Original Data:\n",
      "         open    high     low      volume  adjusted_close  change_percent  \\\n",
      "10742  193.33  194.44  192.92  37283200.0        193.3589            0.45   \n",
      "10743  193.67  195.64  193.32  47471900.0        194.2377            0.45   \n",
      "10744  196.02  197.20  192.55  47460200.0        192.9594           -0.66   \n",
      "10745  194.67  196.63  194.14  48291400.0        195.5659            1.35   \n",
      "10746  196.06  196.49  195.26  38824100.0        196.1851            0.32   \n",
      "\n",
      "       avg_vol_20d  \n",
      "10742   52369165.0  \n",
      "10743   52206220.0  \n",
      "10744   52018390.0  \n",
      "10745   52115595.0  \n",
      "10746   49803320.0  \n",
      "Actual (original): [[ 20.003197]\n",
      " [192.75    ]\n",
      " [179.09999 ]\n",
      " [ 24.561607]\n",
      " [ 22.372002]]\n",
      "Predicted (original): [[116.67911]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_11712\\4261605350.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n",
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# ... (previous code)\n",
    "\n",
    "# Test set evaluation\n",
    "with torch.no_grad():\n",
    "    x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n",
    "    y_test_window = torch.tensor(y_test[:params['window_size']], dtype=torch.float32)\n",
    "    x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "    hidden = model.init_hidden(1)\n",
    "    test_outputs, _ = model(x_test_window, hidden)\n",
    "    test_loss = criterion(test_outputs, y_test_window)\n",
    "\n",
    "# Print the last test window and compare with the last 5 entries of the original data\n",
    "print(\"Last Test Window:\")\n",
    "print(\"Predicted (scaled):\", test_outputs.numpy())\n",
    "print(\"Actual (scaled):\", y_test_window.numpy())\n",
    "\n",
    "# Convert the scaled predictions back to original scale\n",
    "predicted_values = scaler_y.inverse_transform(test_outputs.numpy())\n",
    "actual_values = scaler_y.inverse_transform(y_test_window.numpy())\n",
    "\n",
    "print(\"\\nLast 5 Entries of Original Data:\")\n",
    "print(data.tail(5)[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']])\n",
    "print(\"Actual (original):\", actual_values[-5:])\n",
    "print(\"Predicted (original):\", predicted_values[-5:])\n",
    "\n",
    "# ... (rest of the code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
