{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# x_train_tensor inverse\\n\\nx_test_original = scaler.inverse_transform(x_train_tensor.numpy())\\nprint(\"\\nFirst row of x_test_original:\")\\nprint(x_test_original[0])\\n\\nprint(\"\\nFirst row of x_train:\")\\nprint(x_train.head(1))\\n\\n\\n\\nprint(\"\\nLast row of x_test_original:\")\\nprint(x_test_original[-1])\\n\\nprint(\"\\nLast row of x_train:\")\\nprint(x_train.tail(1))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# size of the window for data preparation\n",
    "split_window_size = 20\n",
    "\n",
    "# Initialize lists to store training and temporary sets\n",
    "x_train_list, y_train_list, x_temp_list, y_temp_list = [], [], [], []\n",
    "\n",
    "# Iterate through the data with the specified window size\n",
    "for i in range(0, len(x_data) - split_window_size, split_window_size + 1):\n",
    "    x_train_temp = x_data.iloc[i:i+split_window_size+1]\n",
    "    y_train_temp = y_data.iloc[i:i+split_window_size+1]\n",
    "\n",
    "    # Separate the last row for the temporary set\n",
    "    x_train = x_train_temp.iloc[:-1]\n",
    "    y_train = y_train_temp.iloc[:-1]\n",
    "\n",
    "    x_temp = x_train_temp.iloc[-1:]\n",
    "    y_temp = y_train_temp.iloc[-1:]\n",
    "\n",
    "    x_train_list.append(x_train)\n",
    "    y_train_list.append(y_train)\n",
    "    x_temp_list.append(x_temp)\n",
    "    y_temp_list.append(y_temp)\n",
    "\n",
    "# Concatenate the lists into pandas DataFrames\n",
    "x_train = pd.concat(x_train_list)\n",
    "y_train = pd.concat(y_train_list)\n",
    "x_temp = pd.concat(x_temp_list)\n",
    "y_temp = pd.concat(y_temp_list)\n",
    "\n",
    "# print(y_train.head(50))\n",
    "x_temp_train, x_temp_val, y_temp_train, y_temp_val = train_test_split(x_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Split x_temp and y_temp into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameters: (7, 1, 1, 0.05, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], Training Loss: 5783.797695039127, Validation Loss: 24890.080078125\n",
      "Epoch [2/250], Training Loss: 6629.436997168329, Validation Loss: 21181.5859375\n",
      "Epoch [3/250], Training Loss: 3588.818219767616, Validation Loss: 13155.716796875\n",
      "Epoch [4/250], Training Loss: 2080.035836915165, Validation Loss: 10396.0849609375\n",
      "Epoch [5/250], Training Loss: 1595.4826714875335, Validation Loss: 10820.595703125\n",
      "Epoch [6/250], Training Loss: 1581.5505834016008, Validation Loss: 13306.0634765625\n",
      "Epoch [7/250], Training Loss: 1561.5162595959625, Validation Loss: 11396.19140625\n",
      "Epoch [8/250], Training Loss: 1526.6856800041246, Validation Loss: 13680.48828125\n",
      "Epoch [9/250], Training Loss: 2037.7854366671545, Validation Loss: 17058.76171875\n",
      "Epoch [10/250], Training Loss: 2274.553359637241, Validation Loss: 11469.751953125\n",
      "Epoch [11/250], Training Loss: 3374.038573829839, Validation Loss: 11277.0029296875\n",
      "Epoch [12/250], Training Loss: 2196.8512393495726, Validation Loss: 11775.0859375\n",
      "Epoch [13/250], Training Loss: 3208.831298615835, Validation Loss: 11046.0302734375\n",
      "Epoch [14/250], Training Loss: 2101.743578162987, Validation Loss: 11206.6484375\n",
      "Epoch [15/250], Training Loss: 3311.3568585942357, Validation Loss: 11338.7060546875\n",
      "Epoch [16/250], Training Loss: 2401.4751102275686, Validation Loss: 10372.154296875\n",
      "Epoch [17/250], Training Loss: 3013.459786806007, Validation Loss: 11020.1494140625\n",
      "Epoch [18/250], Training Loss: 5203.02506145657, Validation Loss: 14130.55859375\n",
      "Epoch [19/250], Training Loss: 3507.5382412510107, Validation Loss: 11301.4033203125\n",
      "Epoch [20/250], Training Loss: 2589.056261497966, Validation Loss: 11610.806640625\n",
      "Epoch [21/250], Training Loss: 3402.611760775093, Validation Loss: 12512.5693359375\n",
      "Epoch [22/250], Training Loss: 3229.3864093677807, Validation Loss: 12816.8193359375\n",
      "Epoch [23/250], Training Loss: 3225.5643774022933, Validation Loss: 12331.0888671875\n",
      "Epoch [24/250], Training Loss: 3729.7640917963913, Validation Loss: 13046.509765625\n",
      "Epoch [25/250], Training Loss: 3397.013977462312, Validation Loss: 14115.8466796875\n",
      "Epoch [26/250], Training Loss: 2659.102506549513, Validation Loss: 11742.640625\n",
      "Epoch [27/250], Training Loss: 3164.440185435686, Validation Loss: 12213.6025390625\n",
      "Epoch [28/250], Training Loss: 2656.0633282757763, Validation Loss: 11442.33984375\n",
      "Epoch [29/250], Training Loss: 3397.448796962473, Validation Loss: 12344.69921875\n",
      "Epoch [30/250], Training Loss: 3972.537485804091, Validation Loss: 12398.6005859375\n",
      "Epoch [31/250], Training Loss: 3384.493512311626, Validation Loss: 13214.2021484375\n",
      "Epoch [32/250], Training Loss: 2673.255164568736, Validation Loss: 12018.42578125\n",
      "Epoch [33/250], Training Loss: 3222.9162474094123, Validation Loss: 13048.3076171875\n",
      "Epoch [34/250], Training Loss: 2671.0997090486253, Validation Loss: 12242.5849609375\n",
      "Epoch [35/250], Training Loss: 3187.2964176722044, Validation Loss: 12559.4599609375\n",
      "Epoch [36/250], Training Loss: 2665.036452731828, Validation Loss: 12995.455078125\n",
      "Epoch [37/250], Training Loss: 2564.5004409941694, Validation Loss: 11800.208984375\n",
      "Epoch [38/250], Training Loss: 3379.057712026003, Validation Loss: 13582.0341796875\n",
      "Epoch [39/250], Training Loss: 2840.959669281871, Validation Loss: 11975.02734375\n",
      "Epoch [40/250], Training Loss: 2618.3579673275385, Validation Loss: 13220.84375\n",
      "Epoch [41/250], Training Loss: 3280.934453393938, Validation Loss: 12455.9892578125\n",
      "Epoch [42/250], Training Loss: 3610.9753219957033, Validation Loss: 12745.5263671875\n",
      "Epoch [43/250], Training Loss: 2910.2659670764615, Validation Loss: 12486.3759765625\n",
      "Epoch [44/250], Training Loss: 2625.389663644928, Validation Loss: 12393.1435546875\n",
      "Epoch [45/250], Training Loss: 2616.048114795843, Validation Loss: 12175.3603515625\n",
      "Epoch [46/250], Training Loss: 2578.6729319262954, Validation Loss: 11956.15625\n",
      "Epoch [47/250], Training Loss: 2930.989079251027, Validation Loss: 12263.5341796875\n",
      "Epoch [48/250], Training Loss: 2688.575282459614, Validation Loss: 11876.89453125\n",
      "Epoch [49/250], Training Loss: 2831.8020101805473, Validation Loss: 12976.8212890625\n",
      "Epoch [50/250], Training Loss: 2730.3786678492534, Validation Loss: 11596.5283203125\n",
      "Epoch [51/250], Training Loss: 2917.001821284931, Validation Loss: 11783.19140625\n",
      "Epoch [52/250], Training Loss: 2932.6812642498567, Validation Loss: 12142.3955078125\n",
      "Epoch [53/250], Training Loss: 2538.3426776646206, Validation Loss: 11787.0673828125\n",
      "Epoch [54/250], Training Loss: 2503.8611256588033, Validation Loss: 11609.5869140625\n",
      "Epoch [55/250], Training Loss: 2891.3470510941506, Validation Loss: 11690.4736328125\n",
      "Epoch [56/250], Training Loss: 2843.106062026337, Validation Loss: 11943.5458984375\n",
      "Epoch [57/250], Training Loss: 2644.563938789434, Validation Loss: 11996.0390625\n",
      "Epoch [58/250], Training Loss: 2641.232665767591, Validation Loss: 11451.787109375\n",
      "Epoch [59/250], Training Loss: 2513.9065520569006, Validation Loss: 11341.2734375\n",
      "Epoch [60/250], Training Loss: 2767.5377361144956, Validation Loss: 11935.986328125\n",
      "Epoch [61/250], Training Loss: 2826.783066562604, Validation Loss: 11683.78515625\n",
      "Epoch [62/250], Training Loss: 2616.111584329227, Validation Loss: 12101.53125\n",
      "Epoch [63/250], Training Loss: 2610.954984931102, Validation Loss: 11741.1923828125\n",
      "Epoch [64/250], Training Loss: 2675.761921114908, Validation Loss: 12343.8017578125\n",
      "Epoch [65/250], Training Loss: 2659.0043575454647, Validation Loss: 11297.4677734375\n",
      "Epoch [66/250], Training Loss: 2509.085488477143, Validation Loss: 11332.6787109375\n",
      "Epoch [67/250], Training Loss: 2603.6862235271287, Validation Loss: 11863.234375\n",
      "Epoch [68/250], Training Loss: 2806.252416960546, Validation Loss: 12254.203125\n",
      "Epoch [69/250], Training Loss: 2865.2080860790797, Validation Loss: 12502.4921875\n",
      "Epoch [70/250], Training Loss: 2709.358478526474, Validation Loss: 12030.46484375\n",
      "Epoch [71/250], Training Loss: 2771.753977233452, Validation Loss: 11246.4345703125\n",
      "Epoch [72/250], Training Loss: 2681.9144689968007, Validation Loss: 11793.6123046875\n",
      "Epoch [73/250], Training Loss: 2742.1365710644473, Validation Loss: 11336.953125\n",
      "Epoch [74/250], Training Loss: 2675.886488169313, Validation Loss: 11478.0185546875\n",
      "Epoch [75/250], Training Loss: 2497.0850415727054, Validation Loss: 11700.51953125\n",
      "Epoch [76/250], Training Loss: 2751.102274308525, Validation Loss: 12366.0126953125\n",
      "Epoch [77/250], Training Loss: 2719.8637518296955, Validation Loss: 11344.0673828125\n",
      "Epoch [78/250], Training Loss: 2617.023063744094, Validation Loss: 11539.771484375\n",
      "Epoch [79/250], Training Loss: 2581.2952031698887, Validation Loss: 12206.4873046875\n",
      "Epoch [80/250], Training Loss: 2666.0144284422827, Validation Loss: 12251.556640625\n",
      "Epoch [81/250], Training Loss: 2582.818703214364, Validation Loss: 11657.78515625\n",
      "Epoch [82/250], Training Loss: 2609.1926692758907, Validation Loss: 11723.1142578125\n",
      "Epoch [83/250], Training Loss: 2890.013162515899, Validation Loss: 12231.8759765625\n",
      "Epoch [84/250], Training Loss: 2814.49071063453, Validation Loss: 11900.0068359375\n",
      "Epoch [85/250], Training Loss: 2803.2407853254963, Validation Loss: 11223.7900390625\n",
      "Epoch [86/250], Training Loss: 2795.657326619673, Validation Loss: 13180.3349609375\n",
      "Epoch [87/250], Training Loss: 2887.8394803979613, Validation Loss: 12616.328125\n",
      "Epoch [88/250], Training Loss: 2740.5975083569238, Validation Loss: 11596.568359375\n",
      "Epoch [89/250], Training Loss: 2850.7259731908734, Validation Loss: 11954.283203125\n",
      "Epoch [90/250], Training Loss: 2757.7656779846084, Validation Loss: 12938.7255859375\n",
      "Epoch [91/250], Training Loss: 2715.746308182041, Validation Loss: 11997.3896484375\n",
      "Epoch [92/250], Training Loss: 2587.8142408005783, Validation Loss: 11812.1376953125\n",
      "Epoch [93/250], Training Loss: 2747.2362311432494, Validation Loss: 12350.0439453125\n",
      "Epoch [94/250], Training Loss: 2675.645410533118, Validation Loss: 11666.8837890625\n",
      "Epoch [95/250], Training Loss: 2624.549140091892, Validation Loss: 11897.4736328125\n",
      "Epoch [96/250], Training Loss: 2819.5349380847542, Validation Loss: 11892.66015625\n",
      "Epoch [97/250], Training Loss: 2889.0527503497915, Validation Loss: 11985.349609375\n",
      "Epoch [98/250], Training Loss: 2650.404792439369, Validation Loss: 12202.294921875\n",
      "Epoch [99/250], Training Loss: 2793.672077582139, Validation Loss: 12477.9111328125\n",
      "Epoch [100/250], Training Loss: 2698.095747282589, Validation Loss: 11769.576171875\n",
      "Epoch [101/250], Training Loss: 2709.481403214321, Validation Loss: 11912.2841796875\n",
      "Epoch [102/250], Training Loss: 2816.0333072556914, Validation Loss: 11973.3515625\n",
      "Epoch [103/250], Training Loss: 2610.0547728979855, Validation Loss: 11796.1787109375\n",
      "Epoch [104/250], Training Loss: 2776.147359975087, Validation Loss: 12200.75390625\n",
      "Epoch [105/250], Training Loss: 2676.9514938790085, Validation Loss: 11952.8388671875\n",
      "Epoch [106/250], Training Loss: 2687.7742881663444, Validation Loss: 11993.681640625\n",
      "Epoch [107/250], Training Loss: 2674.989611973844, Validation Loss: 11863.53515625\n",
      "Epoch [108/250], Training Loss: 2714.606605436643, Validation Loss: 12432.0732421875\n",
      "Epoch [109/250], Training Loss: 2651.6864619820435, Validation Loss: 11485.49609375\n",
      "Epoch [110/250], Training Loss: 2486.614457778765, Validation Loss: 11474.416015625\n",
      "Epoch [111/250], Training Loss: 2768.3893757091582, Validation Loss: 12141.203125\n",
      "Epoch [112/250], Training Loss: 2648.3862707265553, Validation Loss: 11568.57421875\n",
      "Epoch [113/250], Training Loss: 2833.3354592803116, Validation Loss: 13156.6728515625\n",
      "Epoch [114/250], Training Loss: 2886.5444388827436, Validation Loss: 12391.779296875\n",
      "Epoch [115/250], Training Loss: 2689.0606131359277, Validation Loss: 11712.181640625\n",
      "Epoch [116/250], Training Loss: 2835.823953814806, Validation Loss: 12469.0146484375\n",
      "Epoch [117/250], Training Loss: 2808.3393886320555, Validation Loss: 12286.6982421875\n",
      "Epoch [118/250], Training Loss: 2925.8793751545218, Validation Loss: 12330.3134765625\n",
      "Epoch [119/250], Training Loss: 2699.240353862709, Validation Loss: 11783.54296875\n",
      "Epoch [120/250], Training Loss: 2776.1833638635417, Validation Loss: 12260.0263671875\n",
      "Epoch [121/250], Training Loss: 2655.5014977743417, Validation Loss: 11776.1025390625\n",
      "Epoch [122/250], Training Loss: 2828.5508247032817, Validation Loss: 12002.7978515625\n",
      "Epoch [123/250], Training Loss: 2691.5240082789237, Validation Loss: 11990.49609375\n",
      "Epoch [124/250], Training Loss: 2643.6438352693244, Validation Loss: 11843.6328125\n",
      "Epoch [125/250], Training Loss: 2807.253181267754, Validation Loss: 12088.388671875\n",
      "Epoch [126/250], Training Loss: 2755.9722168842663, Validation Loss: 11611.2060546875\n",
      "Epoch [127/250], Training Loss: 2687.198603509684, Validation Loss: 12177.9765625\n",
      "Epoch [128/250], Training Loss: 2667.1691185049285, Validation Loss: 11638.5576171875\n",
      "Epoch [129/250], Training Loss: 2606.4812085186545, Validation Loss: 11946.466796875\n",
      "Epoch [130/250], Training Loss: 2804.3611124329454, Validation Loss: 12149.185546875\n",
      "Epoch [131/250], Training Loss: 2691.0897887155706, Validation Loss: 11554.9560546875\n",
      "Epoch [132/250], Training Loss: 2893.324076854423, Validation Loss: 12503.4873046875\n",
      "Epoch [133/250], Training Loss: 2878.069057732392, Validation Loss: 12290.8359375\n",
      "Epoch [134/250], Training Loss: 2856.357830655236, Validation Loss: 12073.455078125\n",
      "Epoch [135/250], Training Loss: 2712.0885857189546, Validation Loss: 11673.552734375\n",
      "Epoch [136/250], Training Loss: 2560.3772516079953, Validation Loss: 11688.6875\n",
      "Epoch [137/250], Training Loss: 2664.2961172335063, Validation Loss: 12445.4208984375\n",
      "Epoch [138/250], Training Loss: 2772.55462139907, Validation Loss: 12159.759765625\n",
      "Epoch [139/250], Training Loss: 2700.0599184782036, Validation Loss: 12054.8203125\n",
      "Epoch [140/250], Training Loss: 2661.9807390527667, Validation Loss: 11690.92578125\n",
      "Epoch [141/250], Training Loss: 2618.877966647015, Validation Loss: 11421.501953125\n",
      "Epoch [142/250], Training Loss: 2648.6793077112825, Validation Loss: 12414.0947265625\n",
      "Epoch [143/250], Training Loss: 2756.9777325916507, Validation Loss: 11483.388671875\n",
      "Epoch [144/250], Training Loss: 2964.2628576899315, Validation Loss: 12373.9072265625\n",
      "Epoch [145/250], Training Loss: 2767.1986758333755, Validation Loss: 12081.7236328125\n",
      "Epoch [146/250], Training Loss: 2717.4208984000147, Validation Loss: 11245.5029296875\n",
      "Epoch [147/250], Training Loss: 2611.7459068129738, Validation Loss: 11535.15625\n",
      "Epoch [148/250], Training Loss: 2804.0594767454077, Validation Loss: 12052.0830078125\n",
      "Epoch [149/250], Training Loss: 2669.0507804415, Validation Loss: 11961.607421875\n",
      "Epoch [150/250], Training Loss: 2643.6393679361013, Validation Loss: 11624.0537109375\n",
      "Epoch [151/250], Training Loss: 2682.5289217202258, Validation Loss: 11797.70703125\n",
      "Epoch [152/250], Training Loss: 2644.8917450609692, Validation Loss: 11797.9931640625\n",
      "Epoch [153/250], Training Loss: 2636.3604008636235, Validation Loss: 11010.4169921875\n",
      "Epoch [154/250], Training Loss: 2555.503330857191, Validation Loss: 11608.23828125\n",
      "Epoch [155/250], Training Loss: 2605.6486347304826, Validation Loss: 11814.623046875\n",
      "Epoch [156/250], Training Loss: 2844.0790313904135, Validation Loss: 11494.25390625\n",
      "Epoch [157/250], Training Loss: 2616.1519929393044, Validation Loss: 11642.1123046875\n",
      "Epoch [158/250], Training Loss: 2732.5573812015787, Validation Loss: 11854.88671875\n",
      "Epoch [159/250], Training Loss: 2762.3613805807463, Validation Loss: 12103.4140625\n",
      "Epoch [160/250], Training Loss: 2676.4464135509106, Validation Loss: 11419.4375\n",
      "Epoch [161/250], Training Loss: 2579.9743651595563, Validation Loss: 11379.4072265625\n",
      "Epoch [162/250], Training Loss: 2660.617074548648, Validation Loss: 11575.0283203125\n",
      "Epoch [163/250], Training Loss: 2658.9887482087643, Validation Loss: 11724.0654296875\n",
      "Epoch [164/250], Training Loss: 2677.6626589107223, Validation Loss: 11682.873046875\n",
      "Epoch [165/250], Training Loss: 2690.443904173357, Validation Loss: 11847.9775390625\n",
      "Epoch [166/250], Training Loss: 2711.247500594567, Validation Loss: 12183.7451171875\n",
      "Epoch [167/250], Training Loss: 2759.123121731416, Validation Loss: 11381.7529296875\n",
      "Epoch [168/250], Training Loss: 2599.846596190587, Validation Loss: 11367.064453125\n",
      "Epoch [169/250], Training Loss: 2777.8994008258846, Validation Loss: 12944.2373046875\n",
      "Epoch [170/250], Training Loss: 2770.602117457792, Validation Loss: 11983.787109375\n",
      "Epoch [171/250], Training Loss: 2726.231263702635, Validation Loss: 12197.595703125\n",
      "Epoch [172/250], Training Loss: 2729.3751677988453, Validation Loss: 12101.751953125\n",
      "Epoch [173/250], Training Loss: 2695.846238223564, Validation Loss: 11559.318359375\n",
      "Epoch [174/250], Training Loss: 2769.098863761977, Validation Loss: 11903.3818359375\n",
      "Epoch [175/250], Training Loss: 2670.6049480582865, Validation Loss: 12011.7197265625\n",
      "Epoch [176/250], Training Loss: 2659.1033132308166, Validation Loss: 11945.5322265625\n",
      "Epoch [177/250], Training Loss: 2668.7426036949337, Validation Loss: 11279.96484375\n",
      "Epoch [178/250], Training Loss: 2772.910282651782, Validation Loss: 12067.7353515625\n",
      "Epoch [179/250], Training Loss: 2794.9989420770967, Validation Loss: 11374.8720703125\n",
      "Epoch [180/250], Training Loss: 2647.8586197195245, Validation Loss: 12329.33203125\n",
      "Epoch [181/250], Training Loss: 2745.5004201929414, Validation Loss: 12012.15625\n",
      "Epoch [182/250], Training Loss: 2673.223440790121, Validation Loss: 11796.8623046875\n",
      "Epoch [183/250], Training Loss: 2795.925676156899, Validation Loss: 13060.5517578125\n",
      "Epoch [184/250], Training Loss: 2867.173427171564, Validation Loss: 12436.84375\n",
      "Epoch [185/250], Training Loss: 2720.990552554174, Validation Loss: 11564.8515625\n",
      "Epoch [186/250], Training Loss: 2781.0545914790023, Validation Loss: 11988.4951171875\n",
      "Epoch [187/250], Training Loss: 2652.5833385597084, Validation Loss: 11768.43359375\n",
      "Epoch [188/250], Training Loss: 2655.9847965863105, Validation Loss: 11555.208984375\n",
      "Epoch [189/250], Training Loss: 2674.4796535354008, Validation Loss: 11706.4091796875\n",
      "Epoch [190/250], Training Loss: 2741.764943614428, Validation Loss: 11590.576171875\n",
      "Epoch [191/250], Training Loss: 2776.432487025913, Validation Loss: 12354.9111328125\n",
      "Epoch [192/250], Training Loss: 2774.8184227382644, Validation Loss: 12115.9423828125\n",
      "Epoch [193/250], Training Loss: 2828.070571686625, Validation Loss: 12181.28515625\n",
      "Epoch [194/250], Training Loss: 2744.225350707564, Validation Loss: 11659.2646484375\n",
      "Epoch [195/250], Training Loss: 2598.160464286243, Validation Loss: 11505.681640625\n",
      "Epoch [196/250], Training Loss: 2682.923045341908, Validation Loss: 11990.4697265625\n",
      "Epoch [197/250], Training Loss: 2726.3200935824793, Validation Loss: 11613.44140625\n",
      "Epoch [198/250], Training Loss: 2688.5955851665826, Validation Loss: 12403.484375\n",
      "Epoch [199/250], Training Loss: 2850.1964322791455, Validation Loss: 12254.5634765625\n",
      "Epoch [200/250], Training Loss: 2876.0880681277954, Validation Loss: 13045.5380859375\n",
      "Epoch [201/250], Training Loss: 2881.6489432819594, Validation Loss: 12288.4345703125\n",
      "Epoch [202/250], Training Loss: 2862.6691581866744, Validation Loss: 12307.53125\n",
      "Epoch [203/250], Training Loss: 2747.6046102614496, Validation Loss: 12788.271484375\n",
      "Epoch [204/250], Training Loss: 2830.4634203107507, Validation Loss: 12290.7802734375\n",
      "Epoch [205/250], Training Loss: 2662.868087839854, Validation Loss: 11738.06640625\n",
      "Epoch [206/250], Training Loss: 2609.6440477511946, Validation Loss: 11761.4638671875\n",
      "Epoch [207/250], Training Loss: 2809.9033233391083, Validation Loss: 12120.8095703125\n",
      "Epoch [208/250], Training Loss: 2743.388890491264, Validation Loss: 11869.8798828125\n",
      "Epoch [209/250], Training Loss: 2731.6098299632245, Validation Loss: 12042.05078125\n",
      "Epoch [210/250], Training Loss: 2705.6222003705007, Validation Loss: 11063.3173828125\n",
      "Epoch [211/250], Training Loss: 2595.034544921618, Validation Loss: 11505.8291015625\n",
      "Epoch [212/250], Training Loss: 2656.70365130208, Validation Loss: 11611.62890625\n",
      "Epoch [213/250], Training Loss: 2667.826471642873, Validation Loss: 12227.9140625\n",
      "Epoch [214/250], Training Loss: 2644.430045611272, Validation Loss: 11945.4287109375\n",
      "Epoch [215/250], Training Loss: 2706.02897505838, Validation Loss: 11482.712890625\n",
      "Epoch [216/250], Training Loss: 2648.131403083668, Validation Loss: 11998.3916015625\n",
      "Epoch [217/250], Training Loss: 2658.012769618045, Validation Loss: 11868.271484375\n",
      "Epoch [218/250], Training Loss: 2665.852644534238, Validation Loss: 11976.9296875\n",
      "Epoch [219/250], Training Loss: 2774.5976666296883, Validation Loss: 12177.109375\n",
      "Epoch [220/250], Training Loss: 2728.4655253802193, Validation Loss: 12030.5654296875\n",
      "Epoch [221/250], Training Loss: 2674.016964007992, Validation Loss: 11509.4677734375\n",
      "Epoch [222/250], Training Loss: 2600.253990555159, Validation Loss: 11571.6279296875\n",
      "Epoch [223/250], Training Loss: 2803.5717362764153, Validation Loss: 11981.1298828125\n",
      "Epoch [224/250], Training Loss: 2724.6184245251584, Validation Loss: 12157.06640625\n",
      "Epoch [225/250], Training Loss: 2750.2100830821573, Validation Loss: 11176.1875\n",
      "Epoch [226/250], Training Loss: 2645.018864074419, Validation Loss: 11374.208984375\n",
      "Epoch [227/250], Training Loss: 2664.508727319881, Validation Loss: 11150.6240234375\n",
      "Epoch [228/250], Training Loss: 2677.997923158537, Validation Loss: 12112.8955078125\n",
      "Epoch [229/250], Training Loss: 2710.297406095733, Validation Loss: 11551.7529296875\n",
      "Epoch [230/250], Training Loss: 2578.7072976801496, Validation Loss: 11429.6416015625\n",
      "Epoch [231/250], Training Loss: 2690.521018578613, Validation Loss: 11510.775390625\n",
      "Epoch [232/250], Training Loss: 2610.572477540911, Validation Loss: 11261.5576171875\n",
      "Epoch [233/250], Training Loss: 2788.103646521035, Validation Loss: 11155.658203125\n",
      "Epoch [234/250], Training Loss: 2646.909706961247, Validation Loss: 11392.72265625\n",
      "Epoch [235/250], Training Loss: 2702.741953320967, Validation Loss: 12173.9951171875\n",
      "Epoch [236/250], Training Loss: 2724.4383985414506, Validation Loss: 12091.126953125\n",
      "Epoch [237/250], Training Loss: 2687.6224558745084, Validation Loss: 11526.873046875\n",
      "Epoch [238/250], Training Loss: 2768.258236881598, Validation Loss: 12270.2138671875\n",
      "Epoch [239/250], Training Loss: 2707.71615738576, Validation Loss: 12037.28515625\n",
      "Epoch [240/250], Training Loss: 2679.0665390302615, Validation Loss: 11573.7822265625\n",
      "Epoch [241/250], Training Loss: 2672.841476022863, Validation Loss: 11989.0654296875\n",
      "Epoch [242/250], Training Loss: 2687.7014945830892, Validation Loss: 11884.84375\n",
      "Epoch [243/250], Training Loss: 2669.1596581366084, Validation Loss: 11866.9130859375\n",
      "Epoch [244/250], Training Loss: 2632.3222005125144, Validation Loss: 11444.578125\n",
      "Epoch [245/250], Training Loss: 2734.949423435433, Validation Loss: 12187.10546875\n",
      "Epoch [246/250], Training Loss: 2701.093310787211, Validation Loss: 11591.1630859375\n",
      "Epoch [247/250], Training Loss: 2725.9790115252586, Validation Loss: 11498.224609375\n",
      "Epoch [248/250], Training Loss: 2686.6068349323173, Validation Loss: 11568.9365234375\n",
      "Epoch [249/250], Training Loss: 2701.548823623006, Validation Loss: 12432.7392578125\n",
      "Epoch [250/250], Training Loss: 2750.3784798448332, Validation Loss: 12055.24609375\n",
      "Test Loss: 12288.0810546875\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.05, 2)\n",
      "Epoch [1/250], Training Loss: 4370.912388516467, Validation Loss: 17964.314453125\n",
      "Epoch [2/250], Training Loss: 5132.160665314738, Validation Loss: 11698.052734375\n",
      "Epoch [3/250], Training Loss: 2407.248397234896, Validation Loss: 7912.37451171875\n",
      "Epoch [4/250], Training Loss: 2744.307125612376, Validation Loss: 20831.7734375\n",
      "Epoch [5/250], Training Loss: 3274.4102574858175, Validation Loss: 40161.91796875\n",
      "Epoch [6/250], Training Loss: 6904.568782928308, Validation Loss: 23067.353515625\n",
      "Epoch [7/250], Training Loss: 427.50314173556546, Validation Loss: 8159.2119140625\n",
      "Epoch [8/250], Training Loss: 966.9239337465965, Validation Loss: 12354.451171875\n",
      "Epoch [9/250], Training Loss: 1577.791785457811, Validation Loss: 11046.4150390625\n",
      "Epoch [10/250], Training Loss: 1230.2990529022227, Validation Loss: 11352.470703125\n",
      "Epoch [11/250], Training Loss: 1732.2539008164345, Validation Loss: 11325.1611328125\n",
      "Epoch [12/250], Training Loss: 1978.3917419719755, Validation Loss: 15532.212890625\n",
      "Epoch [13/250], Training Loss: 1988.7687709246127, Validation Loss: 9986.7958984375\n",
      "Epoch [14/250], Training Loss: 5901.916832652326, Validation Loss: 11876.876953125\n",
      "Epoch [15/250], Training Loss: 1214.061251884248, Validation Loss: 13264.1953125\n",
      "Epoch [16/250], Training Loss: 1366.3258900656997, Validation Loss: 16879.501953125\n",
      "Epoch [17/250], Training Loss: 868.6564601557644, Validation Loss: 21751.78515625\n",
      "Epoch [18/250], Training Loss: 840.1083495213268, Validation Loss: 13262.95703125\n",
      "Epoch [19/250], Training Loss: 2497.575435432642, Validation Loss: 14049.34765625\n",
      "Epoch [20/250], Training Loss: 2228.0290847553915, Validation Loss: 15084.0302734375\n",
      "Epoch [21/250], Training Loss: 2522.4165863374888, Validation Loss: 14065.509765625\n",
      "Epoch [22/250], Training Loss: 2583.8804311405743, Validation Loss: 13262.580078125\n",
      "Epoch [23/250], Training Loss: 2512.7855055483465, Validation Loss: 14423.1083984375\n",
      "Epoch [24/250], Training Loss: 1237.3762749849955, Validation Loss: 14176.1904296875\n",
      "Epoch [25/250], Training Loss: 1005.1981064354552, Validation Loss: 18677.51171875\n",
      "Epoch [26/250], Training Loss: 797.9573033841141, Validation Loss: 14100.8935546875\n",
      "Epoch [27/250], Training Loss: 620.7780919467516, Validation Loss: 13802.158203125\n",
      "Epoch [28/250], Training Loss: 611.0680632177529, Validation Loss: 13662.953125\n",
      "Epoch [29/250], Training Loss: 489.28892663431293, Validation Loss: 13853.994140625\n",
      "Epoch [30/250], Training Loss: 548.8031414348436, Validation Loss: 13536.68359375\n",
      "Epoch [31/250], Training Loss: 474.6421336353405, Validation Loss: 13568.98828125\n",
      "Epoch [32/250], Training Loss: 490.07479186647805, Validation Loss: 13578.19140625\n",
      "Epoch [33/250], Training Loss: 503.49147380457384, Validation Loss: 13492.03125\n",
      "Epoch [34/250], Training Loss: 529.2900477742749, Validation Loss: 12376.515625\n",
      "Epoch [35/250], Training Loss: 395.5229194265235, Validation Loss: 12068.087890625\n",
      "Epoch [36/250], Training Loss: 348.27667770683956, Validation Loss: 11935.98828125\n",
      "Epoch [37/250], Training Loss: 278.93022500375884, Validation Loss: 11861.6494140625\n",
      "Epoch [38/250], Training Loss: 223.65804663257893, Validation Loss: 11870.818359375\n",
      "Epoch [39/250], Training Loss: 187.3949061740506, Validation Loss: 11908.244140625\n",
      "Epoch [40/250], Training Loss: 174.0617577906304, Validation Loss: 12124.7216796875\n",
      "Epoch [41/250], Training Loss: 170.21376282408076, Validation Loss: 12270.1318359375\n",
      "Epoch [42/250], Training Loss: 166.20054462220074, Validation Loss: 12454.53125\n",
      "Epoch [43/250], Training Loss: 164.77260188791192, Validation Loss: 12329.9111328125\n",
      "Epoch [44/250], Training Loss: 168.69864884842383, Validation Loss: 12870.783203125\n",
      "Epoch [45/250], Training Loss: 240.81062031892947, Validation Loss: 17009.19140625\n",
      "Epoch [46/250], Training Loss: 120.85016956133565, Validation Loss: 14170.658203125\n",
      "Epoch [47/250], Training Loss: 216.70454070415548, Validation Loss: 15917.70703125\n",
      "Epoch [48/250], Training Loss: 166.13659989869217, Validation Loss: 16826.505859375\n",
      "Epoch [49/250], Training Loss: 107.87291562107727, Validation Loss: 14986.326171875\n",
      "Epoch [50/250], Training Loss: 169.51322044175015, Validation Loss: 16900.33203125\n",
      "Epoch [51/250], Training Loss: 125.72210653877255, Validation Loss: 15495.55859375\n",
      "Epoch [52/250], Training Loss: 115.26519326707562, Validation Loss: 15008.0302734375\n",
      "Epoch [53/250], Training Loss: 113.01902448237733, Validation Loss: 16057.330078125\n",
      "Epoch [54/250], Training Loss: 507.90224969645556, Validation Loss: 29473.404296875\n",
      "Epoch [55/250], Training Loss: 519.7938052462217, Validation Loss: 21197.341796875\n",
      "Epoch [56/250], Training Loss: 252.7923683805054, Validation Loss: 22046.029296875\n",
      "Epoch [57/250], Training Loss: 97.56133189201431, Validation Loss: 18935.2109375\n",
      "Epoch [58/250], Training Loss: 150.25624132061037, Validation Loss: 18939.439453125\n",
      "Epoch [59/250], Training Loss: 127.82533790965974, Validation Loss: 17992.62109375\n",
      "Epoch [60/250], Training Loss: 160.4082573793152, Validation Loss: 13092.6181640625\n",
      "Epoch [61/250], Training Loss: 1265.11132653568, Validation Loss: 12170.5810546875\n",
      "Epoch [62/250], Training Loss: 2420.74791177052, Validation Loss: 12988.1416015625\n",
      "Epoch [63/250], Training Loss: 3411.002271455091, Validation Loss: 13863.5439453125\n",
      "Epoch [64/250], Training Loss: 3706.892918527425, Validation Loss: 13208.9404296875\n",
      "Epoch [65/250], Training Loss: 3601.1791592291484, Validation Loss: 12628.6962890625\n",
      "Epoch [66/250], Training Loss: 3196.7982729154273, Validation Loss: 13348.341796875\n",
      "Epoch [67/250], Training Loss: 3047.055986749092, Validation Loss: 12916.36328125\n",
      "Epoch [68/250], Training Loss: 2939.010908823378, Validation Loss: 13049.751953125\n",
      "Epoch [69/250], Training Loss: 3448.309765445826, Validation Loss: 14505.4716796875\n",
      "Epoch [70/250], Training Loss: 3343.576323052048, Validation Loss: 13846.6142578125\n",
      "Epoch [71/250], Training Loss: 2345.42576567008, Validation Loss: 11809.9482421875\n",
      "Epoch [72/250], Training Loss: 1227.5684745105782, Validation Loss: 13650.7822265625\n",
      "Epoch [73/250], Training Loss: 793.7668476106907, Validation Loss: 14328.65234375\n",
      "Epoch [74/250], Training Loss: 579.5003618629846, Validation Loss: 13625.408203125\n",
      "Epoch [75/250], Training Loss: 403.75417738810427, Validation Loss: 12377.1044921875\n",
      "Epoch [76/250], Training Loss: 262.9577370637603, Validation Loss: 11802.2412109375\n",
      "Epoch [77/250], Training Loss: 175.36246149576613, Validation Loss: 11715.271484375\n",
      "Epoch [78/250], Training Loss: 155.6390972706872, Validation Loss: 12314.7900390625\n",
      "Epoch [79/250], Training Loss: 143.37005828625038, Validation Loss: 12847.5927734375\n",
      "Epoch [80/250], Training Loss: 137.85266987879868, Validation Loss: 13661.962890625\n",
      "Epoch [81/250], Training Loss: 135.1674459697391, Validation Loss: 13989.947265625\n",
      "Epoch [82/250], Training Loss: 130.66381851846344, Validation Loss: 14860.06640625\n",
      "Epoch [83/250], Training Loss: 136.94017928434306, Validation Loss: 14642.296875\n",
      "Epoch [84/250], Training Loss: 133.8363392364519, Validation Loss: 15188.92578125\n",
      "Epoch [85/250], Training Loss: 144.10092046850014, Validation Loss: 14229.8203125\n",
      "Epoch [86/250], Training Loss: 145.65724340994024, Validation Loss: 14224.8134765625\n",
      "Epoch [87/250], Training Loss: 146.5598964490101, Validation Loss: 15315.1884765625\n",
      "Epoch [88/250], Training Loss: 144.01583960948733, Validation Loss: 14367.91796875\n",
      "Epoch [89/250], Training Loss: 145.4631810749449, Validation Loss: 14544.8896484375\n",
      "Epoch [90/250], Training Loss: 144.26969639767097, Validation Loss: 15475.0703125\n",
      "Epoch [91/250], Training Loss: 142.32579915440223, Validation Loss: 14421.65234375\n",
      "Epoch [92/250], Training Loss: 144.24012194530695, Validation Loss: 14568.953125\n",
      "Epoch [93/250], Training Loss: 144.36099811419365, Validation Loss: 15345.3896484375\n",
      "Epoch [94/250], Training Loss: 142.79977697882188, Validation Loss: 14363.1162109375\n",
      "Epoch [95/250], Training Loss: 143.7983514382582, Validation Loss: 14580.462890625\n",
      "Epoch [96/250], Training Loss: 143.93743613320453, Validation Loss: 15327.2978515625\n",
      "Epoch [97/250], Training Loss: 141.8244680952649, Validation Loss: 14553.06640625\n",
      "Epoch [98/250], Training Loss: 141.2118738131374, Validation Loss: 14617.7001953125\n",
      "Epoch [99/250], Training Loss: 146.89145604002314, Validation Loss: 14963.0283203125\n",
      "Epoch [100/250], Training Loss: 145.2054829657484, Validation Loss: 14770.822265625\n",
      "Epoch [101/250], Training Loss: 142.43449989416695, Validation Loss: 14794.1123046875\n",
      "Epoch [102/250], Training Loss: 141.19169911010212, Validation Loss: 15475.39453125\n",
      "Epoch [103/250], Training Loss: 139.73316132563548, Validation Loss: 14734.205078125\n",
      "Epoch [104/250], Training Loss: 139.40712511720122, Validation Loss: 14685.7861328125\n",
      "Epoch [105/250], Training Loss: 146.6649151706785, Validation Loss: 14835.2734375\n",
      "Epoch [106/250], Training Loss: 145.6125774746937, Validation Loss: 14777.62890625\n",
      "Epoch [107/250], Training Loss: 143.51786032963003, Validation Loss: 14980.84375\n",
      "Epoch [108/250], Training Loss: 143.42592821404688, Validation Loss: 14709.8720703125\n",
      "Epoch [109/250], Training Loss: 144.1704725559359, Validation Loss: 14421.0517578125\n",
      "Epoch [110/250], Training Loss: 148.52882628924323, Validation Loss: 14296.8447265625\n",
      "Epoch [111/250], Training Loss: 151.2341134079327, Validation Loss: 14152.8916015625\n",
      "Epoch [112/250], Training Loss: 154.03661388611604, Validation Loss: 14110.36328125\n",
      "Epoch [113/250], Training Loss: 159.33811392495582, Validation Loss: 13901.078125\n",
      "Epoch [114/250], Training Loss: 153.28144079997693, Validation Loss: 14021.4892578125\n",
      "Epoch [115/250], Training Loss: 154.67854409096932, Validation Loss: 14691.4072265625\n",
      "Epoch [116/250], Training Loss: 148.5494598004702, Validation Loss: 14270.34375\n",
      "Epoch [117/250], Training Loss: 149.37135956424416, Validation Loss: 14165.564453125\n",
      "Epoch [118/250], Training Loss: 155.29278721458078, Validation Loss: 14110.7392578125\n",
      "Epoch [119/250], Training Loss: 157.22770062046294, Validation Loss: 13881.5537109375\n",
      "Epoch [120/250], Training Loss: 165.03630958511704, Validation Loss: 13858.291015625\n",
      "Epoch [121/250], Training Loss: 156.00528403584286, Validation Loss: 13994.5\n",
      "Epoch [122/250], Training Loss: 157.40593348412958, Validation Loss: 14626.984375\n",
      "Epoch [123/250], Training Loss: 150.53002691871635, Validation Loss: 14126.662109375\n",
      "Epoch [124/250], Training Loss: 151.08331400782555, Validation Loss: 14039.53125\n",
      "Epoch [125/250], Training Loss: 158.89442349895887, Validation Loss: 14059.361328125\n",
      "Epoch [126/250], Training Loss: 157.66730833464607, Validation Loss: 13916.8349609375\n",
      "Epoch [127/250], Training Loss: 167.35332561917062, Validation Loss: 13735.0390625\n",
      "Epoch [128/250], Training Loss: 169.54393621956956, Validation Loss: 13710.173828125\n",
      "Epoch [129/250], Training Loss: 172.1223625942918, Validation Loss: 13574.7060546875\n",
      "Epoch [130/250], Training Loss: 170.4971148197647, Validation Loss: 13901.3759765625\n",
      "Epoch [131/250], Training Loss: 171.72869105972245, Validation Loss: 13421.9111328125\n",
      "Epoch [132/250], Training Loss: 178.65660024200534, Validation Loss: 13880.9609375\n",
      "Epoch [133/250], Training Loss: 173.23082582496232, Validation Loss: 13428.1689453125\n",
      "Epoch [134/250], Training Loss: 179.9418822362866, Validation Loss: 14322.8935546875\n",
      "Epoch [135/250], Training Loss: 166.33350808691026, Validation Loss: 13643.3125\n",
      "Epoch [136/250], Training Loss: 168.83507641424907, Validation Loss: 13696.177734375\n",
      "Epoch [137/250], Training Loss: 172.43997175230658, Validation Loss: 13459.9853515625\n",
      "Epoch [138/250], Training Loss: 175.3297367484957, Validation Loss: 14002.9033203125\n",
      "Epoch [139/250], Training Loss: 168.9724148686833, Validation Loss: 13518.921875\n",
      "Epoch [140/250], Training Loss: 173.78950658516706, Validation Loss: 13413.099609375\n",
      "Epoch [141/250], Training Loss: 176.24756790931497, Validation Loss: 14060.7548828125\n",
      "Epoch [142/250], Training Loss: 166.99342080063218, Validation Loss: 13652.65625\n",
      "Epoch [143/250], Training Loss: 170.02338108187476, Validation Loss: 13605.443359375\n",
      "Epoch [144/250], Training Loss: 172.51858267679503, Validation Loss: 13445.8095703125\n",
      "Epoch [145/250], Training Loss: 177.07182309085383, Validation Loss: 13070.02734375\n",
      "Epoch [146/250], Training Loss: 187.4176589681015, Validation Loss: 13595.7431640625\n",
      "Epoch [147/250], Training Loss: 178.20038569263522, Validation Loss: 13206.6943359375\n",
      "Epoch [148/250], Training Loss: 182.61093577678864, Validation Loss: 13838.2294921875\n",
      "Epoch [149/250], Training Loss: 172.20635896468366, Validation Loss: 13282.8984375\n",
      "Epoch [150/250], Training Loss: 181.32671160711024, Validation Loss: 12951.0654296875\n",
      "Epoch [151/250], Training Loss: 189.85016979436895, Validation Loss: 13810.62890625\n",
      "Epoch [152/250], Training Loss: 173.0650862718536, Validation Loss: 13135.3046875\n",
      "Epoch [153/250], Training Loss: 182.98373919740638, Validation Loss: 13625.9921875\n",
      "Epoch [154/250], Training Loss: 175.43329807683145, Validation Loss: 13170.5029296875\n",
      "Epoch [155/250], Training Loss: 182.29214625600324, Validation Loss: 12998.912109375\n",
      "Epoch [156/250], Training Loss: 186.66771258167114, Validation Loss: 13614.6669921875\n",
      "Epoch [157/250], Training Loss: 175.40571524514448, Validation Loss: 13054.4970703125\n",
      "Epoch [158/250], Training Loss: 183.31788448509312, Validation Loss: 12949.666015625\n",
      "Epoch [159/250], Training Loss: 187.9497275897576, Validation Loss: 12741.8671875\n",
      "Epoch [160/250], Training Loss: 192.44347157172518, Validation Loss: 13439.568359375\n",
      "Epoch [161/250], Training Loss: 178.25530744323567, Validation Loss: 12913.130859375\n",
      "Epoch [162/250], Training Loss: 186.56315435545355, Validation Loss: 13035.28515625\n",
      "Epoch [163/250], Training Loss: 186.76760253018148, Validation Loss: 12799.357421875\n",
      "Epoch [164/250], Training Loss: 189.8604419033617, Validation Loss: 12826.80078125\n",
      "Epoch [165/250], Training Loss: 193.93177564021565, Validation Loss: 12463.2890625\n",
      "Epoch [166/250], Training Loss: 195.64592918998997, Validation Loss: 13068.7314453125\n",
      "Epoch [167/250], Training Loss: 182.71573241753384, Validation Loss: 12841.1572265625\n",
      "Epoch [168/250], Training Loss: 185.22356900465425, Validation Loss: 13057.2998046875\n",
      "Epoch [169/250], Training Loss: 185.85099924611094, Validation Loss: 12869.0078125\n",
      "Epoch [170/250], Training Loss: 186.43728779817707, Validation Loss: 13301.01171875\n",
      "Epoch [171/250], Training Loss: 180.05647218428308, Validation Loss: 13208.7412109375\n",
      "Epoch [172/250], Training Loss: 179.65546944367952, Validation Loss: 13190.0029296875\n",
      "Epoch [173/250], Training Loss: 182.16117396739244, Validation Loss: 13114.74609375\n",
      "Epoch [174/250], Training Loss: 186.5472857979054, Validation Loss: 13568.3349609375\n",
      "Epoch [175/250], Training Loss: 178.39111681701002, Validation Loss: 13556.8271484375\n",
      "Epoch [176/250], Training Loss: 174.33523171759583, Validation Loss: 13646.4794921875\n",
      "Epoch [177/250], Training Loss: 170.95053213645966, Validation Loss: 13822.8037109375\n",
      "Epoch [178/250], Training Loss: 167.42379704488297, Validation Loss: 13875.1767578125\n",
      "Epoch [179/250], Training Loss: 164.28540278445124, Validation Loss: 14127.71875\n",
      "Epoch [180/250], Training Loss: 161.3073322905132, Validation Loss: 14255.970703125\n",
      "Epoch [181/250], Training Loss: 159.04378922861943, Validation Loss: 14355.71875\n",
      "Epoch [182/250], Training Loss: 157.09597302102085, Validation Loss: 14449.6171875\n",
      "Epoch [183/250], Training Loss: 155.16325576335996, Validation Loss: 14565.015625\n",
      "Epoch [184/250], Training Loss: 152.92993713387963, Validation Loss: 14685.2021484375\n",
      "Epoch [185/250], Training Loss: 150.9030240543415, Validation Loss: 14771.279296875\n",
      "Epoch [186/250], Training Loss: 149.34164006172085, Validation Loss: 14847.8271484375\n",
      "Epoch [187/250], Training Loss: 147.67470787349498, Validation Loss: 14916.1953125\n",
      "Epoch [188/250], Training Loss: 146.29441705676095, Validation Loss: 14962.03125\n",
      "Epoch [189/250], Training Loss: 145.25835560685678, Validation Loss: 15000.810546875\n",
      "Epoch [190/250], Training Loss: 144.22724758028207, Validation Loss: 15029.16796875\n",
      "Epoch [191/250], Training Loss: 143.41885030916282, Validation Loss: 15048.5107421875\n",
      "Epoch [192/250], Training Loss: 142.723689345194, Validation Loss: 15061.67578125\n",
      "Epoch [193/250], Training Loss: 142.11935080691282, Validation Loss: 15069.1044921875\n",
      "Epoch [194/250], Training Loss: 141.615564232254, Validation Loss: 15072.8291015625\n",
      "Epoch [195/250], Training Loss: 141.17283499697507, Validation Loss: 15072.763671875\n",
      "Epoch [196/250], Training Loss: 140.79969143927084, Validation Loss: 15070.3134765625\n",
      "Epoch [197/250], Training Loss: 140.4832051591874, Validation Loss: 15066.07421875\n",
      "Epoch [198/250], Training Loss: 140.21981842357107, Validation Loss: 15061.2607421875\n",
      "Epoch [199/250], Training Loss: 140.00302995231445, Validation Loss: 15055.9638671875\n",
      "Epoch [200/250], Training Loss: 139.84771779938254, Validation Loss: 15052.2705078125\n",
      "Epoch [201/250], Training Loss: 139.73671440609746, Validation Loss: 15048.47265625\n",
      "Epoch [202/250], Training Loss: 139.6872498381172, Validation Loss: 15045.2529296875\n",
      "Epoch [203/250], Training Loss: 139.696613236912, Validation Loss: 15041.4404296875\n",
      "Epoch [204/250], Training Loss: 139.77479860554809, Validation Loss: 15035.3154296875\n",
      "Epoch [205/250], Training Loss: 139.94598293599375, Validation Loss: 15025.615234375\n",
      "Epoch [206/250], Training Loss: 140.2414182904816, Validation Loss: 15009.0625\n",
      "Epoch [207/250], Training Loss: 140.69508319508824, Validation Loss: 14983.9501953125\n",
      "Epoch [208/250], Training Loss: 141.3356778080841, Validation Loss: 14948.671875\n",
      "Epoch [209/250], Training Loss: 142.16754234834667, Validation Loss: 14901.939453125\n",
      "Epoch [210/250], Training Loss: 143.22354930410054, Validation Loss: 14846.2001953125\n",
      "Epoch [211/250], Training Loss: 144.47685558431968, Validation Loss: 14792.3017578125\n",
      "Epoch [212/250], Training Loss: 146.06765249963843, Validation Loss: 14740.6240234375\n",
      "Epoch [213/250], Training Loss: 148.17724135275404, Validation Loss: 14659.208984375\n",
      "Epoch [214/250], Training Loss: 151.03501878134665, Validation Loss: 14500.474609375\n",
      "Epoch [215/250], Training Loss: 156.44919655305821, Validation Loss: 14216.591796875\n",
      "Epoch [216/250], Training Loss: 157.74824883128468, Validation Loss: 13928.2529296875\n",
      "Epoch [217/250], Training Loss: 159.08018502146095, Validation Loss: 14596.9912109375\n",
      "Epoch [218/250], Training Loss: 151.6417843767008, Validation Loss: 14362.6142578125\n",
      "Epoch [219/250], Training Loss: 155.28373119427553, Validation Loss: 14290.69140625\n",
      "Epoch [220/250], Training Loss: 158.51628655932055, Validation Loss: 14154.05859375\n",
      "Epoch [221/250], Training Loss: 165.17389370376813, Validation Loss: 13785.208984375\n",
      "Epoch [222/250], Training Loss: 162.57295922739655, Validation Loss: 13695.091796875\n",
      "Epoch [223/250], Training Loss: 164.47167162800716, Validation Loss: 14416.208984375\n",
      "Epoch [224/250], Training Loss: 155.38696086161428, Validation Loss: 14074.283203125\n",
      "Epoch [225/250], Training Loss: 159.52417783184174, Validation Loss: 13965.861328125\n",
      "Epoch [226/250], Training Loss: 162.30917936910961, Validation Loss: 13873.8798828125\n",
      "Epoch [227/250], Training Loss: 167.78547656130067, Validation Loss: 13640.314453125\n",
      "Epoch [228/250], Training Loss: 171.60484596847508, Validation Loss: 13315.0029296875\n",
      "Epoch [229/250], Training Loss: 175.00975933760668, Validation Loss: 13947.8955078125\n",
      "Epoch [230/250], Training Loss: 166.3790155122909, Validation Loss: 13639.0146484375\n",
      "Epoch [231/250], Training Loss: 175.2459614455712, Validation Loss: 13352.2412109375\n",
      "Epoch [232/250], Training Loss: 169.03108513044413, Validation Loss: 13340.7939453125\n",
      "Epoch [233/250], Training Loss: 172.33226537006274, Validation Loss: 13959.0087890625\n",
      "Epoch [234/250], Training Loss: 162.14978447595823, Validation Loss: 13702.7880859375\n",
      "Epoch [235/250], Training Loss: 166.16539362874505, Validation Loss: 13630.97265625\n",
      "Epoch [236/250], Training Loss: 168.34705552635563, Validation Loss: 13557.2294921875\n",
      "Epoch [237/250], Training Loss: 172.1743735420841, Validation Loss: 13448.064453125\n",
      "Epoch [238/250], Training Loss: 176.70551454097514, Validation Loss: 13240.7236328125\n",
      "Epoch [239/250], Training Loss: 183.41482244455338, Validation Loss: 12949.7666015625\n",
      "Epoch [240/250], Training Loss: 176.6179947070276, Validation Loss: 13017.1787109375\n",
      "Epoch [241/250], Training Loss: 179.32358751404954, Validation Loss: 13612.2802734375\n",
      "Epoch [242/250], Training Loss: 167.65497950118615, Validation Loss: 13409.4326171875\n",
      "Epoch [243/250], Training Loss: 171.08635060164136, Validation Loss: 13379.150390625\n",
      "Epoch [244/250], Training Loss: 172.96868711020937, Validation Loss: 13346.32421875\n",
      "Epoch [245/250], Training Loss: 176.2259439441666, Validation Loss: 13229.9765625\n",
      "Epoch [246/250], Training Loss: 180.8482525298052, Validation Loss: 13048.912109375\n",
      "Epoch [247/250], Training Loss: 186.5693279918165, Validation Loss: 12829.5458984375\n",
      "Epoch [248/250], Training Loss: 188.66814597778367, Validation Loss: 12583.8564453125\n",
      "Epoch [249/250], Training Loss: 190.798695330117, Validation Loss: 13047.9853515625\n",
      "Epoch [250/250], Training Loss: 183.45649234692618, Validation Loss: 12835.7314453125\n",
      "Test Loss: 12545.8896484375\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.05, 5)\n",
      "Epoch [1/250], Training Loss: 9842.602663754395, Validation Loss: 24550.6953125\n",
      "Epoch [2/250], Training Loss: 4026.9540413020763, Validation Loss: 16406.69921875\n",
      "Epoch [3/250], Training Loss: 3932.7335592261497, Validation Loss: 12034.44140625\n",
      "Epoch [4/250], Training Loss: 2522.423889906312, Validation Loss: 36718.80078125\n",
      "Epoch [5/250], Training Loss: 11683.687609698605, Validation Loss: 23934.994140625\n",
      "Epoch [6/250], Training Loss: 11230.99794661137, Validation Loss: 29865.927734375\n",
      "Epoch [7/250], Training Loss: 3894.5649639039343, Validation Loss: 20592.439453125\n",
      "Epoch [8/250], Training Loss: 1592.0877122672102, Validation Loss: 10534.6826171875\n",
      "Epoch [9/250], Training Loss: 855.8641364541879, Validation Loss: 11838.3076171875\n",
      "Epoch [10/250], Training Loss: 965.784728963411, Validation Loss: 12236.6767578125\n",
      "Epoch [11/250], Training Loss: 799.8473858064804, Validation Loss: 12440.48046875\n",
      "Epoch [12/250], Training Loss: 745.1094773676762, Validation Loss: 12451.8779296875\n",
      "Epoch [13/250], Training Loss: 673.5783208374686, Validation Loss: 16810.318359375\n",
      "Epoch [14/250], Training Loss: 512.3062418087767, Validation Loss: 11592.9375\n",
      "Epoch [15/250], Training Loss: 788.2212108564282, Validation Loss: 11462.6513671875\n",
      "Epoch [16/250], Training Loss: 612.3380470976003, Validation Loss: 9084.15625\n",
      "Epoch [17/250], Training Loss: 439.01834649162237, Validation Loss: 8863.0361328125\n",
      "Epoch [18/250], Training Loss: 360.268483442639, Validation Loss: 7798.17578125\n",
      "Epoch [19/250], Training Loss: 314.0855748431381, Validation Loss: 8717.25390625\n",
      "Epoch [20/250], Training Loss: 406.19942070641025, Validation Loss: 9452.306640625\n",
      "Epoch [21/250], Training Loss: 397.8703895111261, Validation Loss: 8741.45703125\n",
      "Epoch [22/250], Training Loss: 365.4678350401059, Validation Loss: 8505.2548828125\n",
      "Epoch [23/250], Training Loss: 359.25841950099084, Validation Loss: 10448.591796875\n",
      "Epoch [24/250], Training Loss: 444.12767816146277, Validation Loss: 9604.4404296875\n",
      "Epoch [25/250], Training Loss: 312.50909137597876, Validation Loss: 7645.9931640625\n",
      "Epoch [26/250], Training Loss: 251.70887592740652, Validation Loss: 8152.08154296875\n",
      "Epoch [27/250], Training Loss: 296.6326115232618, Validation Loss: 7435.74853515625\n",
      "Epoch [28/250], Training Loss: 263.1054088948066, Validation Loss: 8715.4775390625\n",
      "Epoch [29/250], Training Loss: 334.857727668458, Validation Loss: 7175.3447265625\n",
      "Epoch [30/250], Training Loss: 212.6205442664826, Validation Loss: 6213.59912109375\n",
      "Epoch [31/250], Training Loss: 212.52656053092173, Validation Loss: 8894.10546875\n",
      "Epoch [32/250], Training Loss: 356.5147431080431, Validation Loss: 7992.23291015625\n",
      "Epoch [33/250], Training Loss: 300.0223823881813, Validation Loss: 9126.8046875\n",
      "Epoch [34/250], Training Loss: 278.7203653274267, Validation Loss: 8051.46142578125\n",
      "Epoch [35/250], Training Loss: 288.05983980051536, Validation Loss: 7092.58642578125\n",
      "Epoch [36/250], Training Loss: 188.09083833239927, Validation Loss: 6749.8408203125\n",
      "Epoch [37/250], Training Loss: 187.09344892021755, Validation Loss: 5777.0107421875\n",
      "Epoch [38/250], Training Loss: 133.111454772818, Validation Loss: 5546.38720703125\n",
      "Epoch [39/250], Training Loss: 130.64126145111513, Validation Loss: 5850.5341796875\n",
      "Epoch [40/250], Training Loss: 131.9157051049543, Validation Loss: 5549.765625\n",
      "Epoch [41/250], Training Loss: 123.10974457524578, Validation Loss: 5617.5703125\n",
      "Epoch [42/250], Training Loss: 135.83608003898337, Validation Loss: 5636.43896484375\n",
      "Epoch [43/250], Training Loss: 121.37131155058641, Validation Loss: 5443.00537109375\n",
      "Epoch [44/250], Training Loss: 121.209853476366, Validation Loss: 5417.94775390625\n",
      "Epoch [45/250], Training Loss: 156.6639258095124, Validation Loss: 5229.7060546875\n",
      "Epoch [46/250], Training Loss: 122.13625917206758, Validation Loss: 5207.64990234375\n",
      "Epoch [47/250], Training Loss: 123.66995929522103, Validation Loss: 5459.9013671875\n",
      "Epoch [48/250], Training Loss: 138.63373336844407, Validation Loss: 5713.8740234375\n",
      "Epoch [49/250], Training Loss: 145.5932905712758, Validation Loss: 6193.50146484375\n",
      "Epoch [50/250], Training Loss: 158.82531458715835, Validation Loss: 3953.489990234375\n",
      "Epoch [51/250], Training Loss: 98.20116171532699, Validation Loss: 3347.802734375\n",
      "Epoch [52/250], Training Loss: 168.382682708406, Validation Loss: 3935.55126953125\n",
      "Epoch [53/250], Training Loss: 101.33309164213902, Validation Loss: 4128.1533203125\n",
      "Epoch [54/250], Training Loss: 103.12098925377505, Validation Loss: 4500.80712890625\n",
      "Epoch [55/250], Training Loss: 149.22809996631432, Validation Loss: 4874.7275390625\n",
      "Epoch [56/250], Training Loss: 127.43469088199592, Validation Loss: 4694.76708984375\n",
      "Epoch [57/250], Training Loss: 129.9614785597913, Validation Loss: 4330.4814453125\n",
      "Epoch [58/250], Training Loss: 141.27013745629785, Validation Loss: 10310.3564453125\n",
      "Epoch [59/250], Training Loss: 347.56915853723586, Validation Loss: 9199.7275390625\n",
      "Epoch [60/250], Training Loss: 399.12336046257616, Validation Loss: 6722.94775390625\n",
      "Epoch [61/250], Training Loss: 253.6595177486619, Validation Loss: 5376.7080078125\n",
      "Epoch [62/250], Training Loss: 238.55052229801058, Validation Loss: 5265.26416015625\n",
      "Epoch [63/250], Training Loss: 184.07839644599653, Validation Loss: 4997.06591796875\n",
      "Epoch [64/250], Training Loss: 163.1892116113, Validation Loss: 4896.2529296875\n",
      "Epoch [65/250], Training Loss: 150.70199021957643, Validation Loss: 3983.79931640625\n",
      "Epoch [66/250], Training Loss: 130.62345429804378, Validation Loss: 4378.876953125\n",
      "Epoch [67/250], Training Loss: 150.34564259692058, Validation Loss: 5079.44580078125\n",
      "Epoch [68/250], Training Loss: 252.06128863849386, Validation Loss: 3383.92431640625\n",
      "Epoch [69/250], Training Loss: 114.6605026718514, Validation Loss: 5538.03662109375\n",
      "Epoch [70/250], Training Loss: 153.7152940465465, Validation Loss: 3989.744384765625\n",
      "Epoch [71/250], Training Loss: 133.60444455249817, Validation Loss: 10678.0986328125\n",
      "Epoch [72/250], Training Loss: 283.89479954573164, Validation Loss: 16557.0625\n",
      "Epoch [73/250], Training Loss: 566.2609729455452, Validation Loss: 20413.552734375\n",
      "Epoch [74/250], Training Loss: 486.2028289796151, Validation Loss: 20236.90625\n",
      "Epoch [75/250], Training Loss: 549.5952884810134, Validation Loss: 9344.9296875\n",
      "Epoch [76/250], Training Loss: 355.70598161257806, Validation Loss: 7718.86669921875\n",
      "Epoch [77/250], Training Loss: 153.69561835068248, Validation Loss: 3953.47607421875\n",
      "Epoch [78/250], Training Loss: 105.98997995693357, Validation Loss: 5029.39111328125\n",
      "Epoch [79/250], Training Loss: 170.50885655434925, Validation Loss: 4917.07470703125\n",
      "Epoch [80/250], Training Loss: 135.94454141293653, Validation Loss: 4380.80859375\n",
      "Epoch [81/250], Training Loss: 104.81847382214093, Validation Loss: 4607.81396484375\n",
      "Epoch [82/250], Training Loss: 152.5757710345254, Validation Loss: 4843.48583984375\n",
      "Epoch [83/250], Training Loss: 119.08466216846477, Validation Loss: 4170.6259765625\n",
      "Epoch [84/250], Training Loss: 99.10644947888066, Validation Loss: 3840.80224609375\n",
      "Epoch [85/250], Training Loss: 100.97738065678328, Validation Loss: 5054.35986328125\n",
      "Epoch [86/250], Training Loss: 159.90467484196216, Validation Loss: 3977.17431640625\n",
      "Epoch [87/250], Training Loss: 96.43933489557709, Validation Loss: 3305.665771484375\n",
      "Epoch [88/250], Training Loss: 84.93537388842877, Validation Loss: 4019.120849609375\n",
      "Epoch [89/250], Training Loss: 106.41627585432192, Validation Loss: 5659.69580078125\n",
      "Epoch [90/250], Training Loss: 149.6379376753454, Validation Loss: 4908.15380859375\n",
      "Epoch [91/250], Training Loss: 201.39310489685974, Validation Loss: 3001.212158203125\n",
      "Epoch [92/250], Training Loss: 101.19001915095296, Validation Loss: 4314.14013671875\n",
      "Epoch [93/250], Training Loss: 173.83008492054873, Validation Loss: 5393.66845703125\n",
      "Epoch [94/250], Training Loss: 186.28287406928249, Validation Loss: 5761.75048828125\n",
      "Epoch [95/250], Training Loss: 183.35285969792307, Validation Loss: 4243.5966796875\n",
      "Epoch [96/250], Training Loss: 135.71620802607842, Validation Loss: 5364.22265625\n",
      "Epoch [97/250], Training Loss: 160.9470828159896, Validation Loss: 3581.338134765625\n",
      "Epoch [98/250], Training Loss: 118.38319591028872, Validation Loss: 3873.91796875\n",
      "Epoch [99/250], Training Loss: 123.26461561162255, Validation Loss: 5141.98291015625\n",
      "Epoch [100/250], Training Loss: 155.41404076040536, Validation Loss: 5645.759765625\n",
      "Epoch [101/250], Training Loss: 168.7336991877268, Validation Loss: 5409.34033203125\n",
      "Epoch [102/250], Training Loss: 165.50965777736118, Validation Loss: 5194.80908203125\n",
      "Epoch [103/250], Training Loss: 161.916876070913, Validation Loss: 5400.16748046875\n",
      "Epoch [104/250], Training Loss: 160.89198665121074, Validation Loss: 5242.06396484375\n",
      "Epoch [105/250], Training Loss: 230.69159302631624, Validation Loss: 4035.170654296875\n",
      "Epoch [106/250], Training Loss: 119.328402675271, Validation Loss: 5540.59716796875\n",
      "Epoch [107/250], Training Loss: 148.92435160006013, Validation Loss: 5271.3349609375\n",
      "Epoch [108/250], Training Loss: 175.5796875264957, Validation Loss: 5238.884765625\n",
      "Epoch [109/250], Training Loss: 128.1071468174969, Validation Loss: 5465.2265625\n",
      "Epoch [110/250], Training Loss: 140.11363681216648, Validation Loss: 2441.157958984375\n",
      "Epoch [111/250], Training Loss: 81.33114059200231, Validation Loss: 2319.898681640625\n",
      "Epoch [112/250], Training Loss: 96.95545777951729, Validation Loss: 5058.0478515625\n",
      "Epoch [113/250], Training Loss: 130.51516651523391, Validation Loss: 2269.38330078125\n",
      "Epoch [114/250], Training Loss: 113.42428181178828, Validation Loss: 5009.32373046875\n",
      "Epoch [115/250], Training Loss: 177.97889784392788, Validation Loss: 2665.2626953125\n",
      "Epoch [116/250], Training Loss: 121.81903214837614, Validation Loss: 2092.704833984375\n",
      "Epoch [117/250], Training Loss: 73.2024361523063, Validation Loss: 3928.392333984375\n",
      "Epoch [118/250], Training Loss: 184.0227505571278, Validation Loss: 6154.66796875\n",
      "Epoch [119/250], Training Loss: 188.31503279725374, Validation Loss: 3415.587646484375\n",
      "Epoch [120/250], Training Loss: 95.35091668904501, Validation Loss: 3496.6728515625\n",
      "Epoch [121/250], Training Loss: 104.02694546854191, Validation Loss: 5989.3134765625\n",
      "Epoch [122/250], Training Loss: 174.56804094742623, Validation Loss: 3835.979248046875\n",
      "Epoch [123/250], Training Loss: 174.32301678661614, Validation Loss: 3077.212890625\n",
      "Epoch [124/250], Training Loss: 170.70890850671634, Validation Loss: 4258.29052734375\n",
      "Epoch [125/250], Training Loss: 150.8612800422148, Validation Loss: 5893.412109375\n",
      "Epoch [126/250], Training Loss: 234.61714917871993, Validation Loss: 5480.3017578125\n",
      "Epoch [127/250], Training Loss: 200.03746748540965, Validation Loss: 5013.6494140625\n",
      "Epoch [128/250], Training Loss: 161.02490815894853, Validation Loss: 4845.7392578125\n",
      "Epoch [129/250], Training Loss: 191.06131168106097, Validation Loss: 3526.696044921875\n",
      "Epoch [130/250], Training Loss: 109.77121105975452, Validation Loss: 5267.18212890625\n",
      "Epoch [131/250], Training Loss: 154.76818685585863, Validation Loss: 5122.19775390625\n",
      "Epoch [132/250], Training Loss: 180.341102858959, Validation Loss: 5250.69091796875\n",
      "Epoch [133/250], Training Loss: 115.47332644587054, Validation Loss: 5327.1494140625\n",
      "Epoch [134/250], Training Loss: 147.02297715574898, Validation Loss: 4284.68212890625\n",
      "Epoch [135/250], Training Loss: 130.75591710606795, Validation Loss: 3198.431884765625\n",
      "Epoch [136/250], Training Loss: 83.10911787726234, Validation Loss: 5338.9951171875\n",
      "Epoch [137/250], Training Loss: 134.8013511918561, Validation Loss: 4699.0380859375\n",
      "Epoch [138/250], Training Loss: 110.23829021369203, Validation Loss: 4836.89306640625\n",
      "Epoch [139/250], Training Loss: 119.52158317776338, Validation Loss: 5352.58056640625\n",
      "Epoch [140/250], Training Loss: 136.5928932900116, Validation Loss: 2364.718017578125\n",
      "Epoch [141/250], Training Loss: 72.4228843102098, Validation Loss: 4406.30712890625\n",
      "Epoch [142/250], Training Loss: 102.5411501780471, Validation Loss: 4544.01416015625\n",
      "Epoch [143/250], Training Loss: 106.2252242547621, Validation Loss: 4995.7197265625\n",
      "Epoch [144/250], Training Loss: 128.84927069312712, Validation Loss: 2278.910888671875\n",
      "Epoch [145/250], Training Loss: 107.49087970728472, Validation Loss: 5030.513671875\n",
      "Epoch [146/250], Training Loss: 145.40701203037665, Validation Loss: 5470.421875\n",
      "Epoch [147/250], Training Loss: 173.80449989175955, Validation Loss: 3394.228515625\n",
      "Epoch [148/250], Training Loss: 97.11933262862016, Validation Loss: 4970.7548828125\n",
      "Epoch [149/250], Training Loss: 126.64315621187573, Validation Loss: 4800.822265625\n",
      "Epoch [150/250], Training Loss: 159.3067847125048, Validation Loss: 5368.728515625\n",
      "Epoch [151/250], Training Loss: 145.61814102823192, Validation Loss: 2382.466796875\n",
      "Epoch [152/250], Training Loss: 74.96300713818836, Validation Loss: 4865.421875\n",
      "Epoch [153/250], Training Loss: 738.5389398020059, Validation Loss: 5207.6884765625\n",
      "Epoch [154/250], Training Loss: 134.83769100781018, Validation Loss: 2180.610595703125\n",
      "Epoch [155/250], Training Loss: 125.31691478049437, Validation Loss: 4072.360107421875\n",
      "Epoch [156/250], Training Loss: 127.98819733034765, Validation Loss: 4011.670654296875\n",
      "Epoch [157/250], Training Loss: 92.40381063644656, Validation Loss: 2408.15234375\n",
      "Epoch [158/250], Training Loss: 105.30465520530882, Validation Loss: 5083.66748046875\n",
      "Epoch [159/250], Training Loss: 155.62685100540506, Validation Loss: 2374.87890625\n",
      "Epoch [160/250], Training Loss: 77.87032830747947, Validation Loss: 5005.23095703125\n",
      "Epoch [161/250], Training Loss: 132.96452793248682, Validation Loss: 5119.349609375\n",
      "Epoch [162/250], Training Loss: 171.67772374782237, Validation Loss: 5260.22021484375\n",
      "Epoch [163/250], Training Loss: 153.5678172163261, Validation Loss: 5537.06298828125\n",
      "Epoch [164/250], Training Loss: 176.84763258695696, Validation Loss: 4441.2470703125\n",
      "Epoch [165/250], Training Loss: 175.75202334939178, Validation Loss: 5380.73876953125\n",
      "Epoch [166/250], Training Loss: 145.45634151321266, Validation Loss: 4635.85986328125\n",
      "Epoch [167/250], Training Loss: 107.34998977708976, Validation Loss: 4980.322265625\n",
      "Epoch [168/250], Training Loss: 141.05799477697133, Validation Loss: 4182.23193359375\n",
      "Epoch [169/250], Training Loss: 102.00118489513756, Validation Loss: 5235.3720703125\n",
      "Epoch [170/250], Training Loss: 153.58923814838704, Validation Loss: 5820.87158203125\n",
      "Epoch [171/250], Training Loss: 170.97334009656666, Validation Loss: 5034.58837890625\n",
      "Epoch [172/250], Training Loss: 129.5616695109952, Validation Loss: 2237.121826171875\n",
      "Epoch [173/250], Training Loss: 127.87653683698115, Validation Loss: 4369.9599609375\n",
      "Epoch [174/250], Training Loss: 135.26925161457862, Validation Loss: 4807.58544921875\n",
      "Epoch [175/250], Training Loss: 127.93177282115093, Validation Loss: 4922.6884765625\n",
      "Epoch [176/250], Training Loss: 181.6006802437639, Validation Loss: 2292.27587890625\n",
      "Epoch [177/250], Training Loss: 88.63418905212649, Validation Loss: 5393.7939453125\n",
      "Epoch [178/250], Training Loss: 157.5108826399758, Validation Loss: 4947.0615234375\n",
      "Epoch [179/250], Training Loss: 168.25632373808168, Validation Loss: 5283.08740234375\n",
      "Epoch [180/250], Training Loss: 144.63812780137906, Validation Loss: 5315.337890625\n",
      "Epoch [181/250], Training Loss: 149.97718201457752, Validation Loss: 4980.515625\n",
      "Epoch [182/250], Training Loss: 180.4940419167682, Validation Loss: 5945.1142578125\n",
      "Epoch [183/250], Training Loss: 192.62449480283735, Validation Loss: 3510.760009765625\n",
      "Epoch [184/250], Training Loss: 89.28238935061319, Validation Loss: 2700.74853515625\n",
      "Epoch [185/250], Training Loss: 86.43117265847403, Validation Loss: 4093.893798828125\n",
      "Epoch [186/250], Training Loss: 88.12914694699474, Validation Loss: 2980.063720703125\n",
      "Epoch [187/250], Training Loss: 126.43719678250808, Validation Loss: 4445.3447265625\n",
      "Epoch [188/250], Training Loss: 91.82559490913621, Validation Loss: 4512.1865234375\n",
      "Epoch [189/250], Training Loss: 100.04136678024112, Validation Loss: 6356.08154296875\n",
      "Epoch [190/250], Training Loss: 178.2072600938427, Validation Loss: 4627.265625\n",
      "Epoch [191/250], Training Loss: 148.50550607362646, Validation Loss: 4940.521484375\n",
      "Epoch [192/250], Training Loss: 125.57775095889617, Validation Loss: 5273.8779296875\n",
      "Epoch [193/250], Training Loss: 142.6553211757614, Validation Loss: 2127.675048828125\n",
      "Epoch [194/250], Training Loss: 76.91671230209862, Validation Loss: 3913.98388671875\n",
      "Epoch [195/250], Training Loss: 137.81234905219145, Validation Loss: 3225.6142578125\n",
      "Epoch [196/250], Training Loss: 90.07595660244408, Validation Loss: 5840.11572265625\n",
      "Epoch [197/250], Training Loss: 294.4839221575989, Validation Loss: 3335.71435546875\n",
      "Epoch [198/250], Training Loss: 90.90168502101125, Validation Loss: 5415.93212890625\n",
      "Epoch [199/250], Training Loss: 151.75222612915303, Validation Loss: 3449.368408203125\n",
      "Epoch [200/250], Training Loss: 128.60414985385677, Validation Loss: 5171.1181640625\n",
      "Epoch [201/250], Training Loss: 924.122874478437, Validation Loss: 4226.0166015625\n",
      "Epoch [202/250], Training Loss: 288.55992801985695, Validation Loss: 11317.8701171875\n",
      "Epoch [203/250], Training Loss: 252.25235909895986, Validation Loss: 7215.68212890625\n",
      "Epoch [204/250], Training Loss: 227.9768215948561, Validation Loss: 4925.1259765625\n",
      "Epoch [205/250], Training Loss: 133.07859385439065, Validation Loss: 4944.5283203125\n",
      "Epoch [206/250], Training Loss: 213.52039301651047, Validation Loss: 4917.63525390625\n",
      "Epoch [207/250], Training Loss: 269.45694636429664, Validation Loss: 4958.81640625\n",
      "Epoch [208/250], Training Loss: 146.86807106081466, Validation Loss: 4941.48681640625\n",
      "Epoch [209/250], Training Loss: 136.21365055739795, Validation Loss: 5197.4052734375\n",
      "Epoch [210/250], Training Loss: 184.8247646878, Validation Loss: 4726.931640625\n",
      "Epoch [211/250], Training Loss: 129.9489254411472, Validation Loss: 5290.7177734375\n",
      "Epoch [212/250], Training Loss: 143.89936750630753, Validation Loss: 4952.16552734375\n",
      "Epoch [213/250], Training Loss: 134.74843040931722, Validation Loss: 5482.75244140625\n",
      "Epoch [214/250], Training Loss: 151.1607258802009, Validation Loss: 3791.091064453125\n",
      "Epoch [215/250], Training Loss: 525.4953336971098, Validation Loss: 9704.1953125\n",
      "Epoch [216/250], Training Loss: 336.1186309006959, Validation Loss: 7503.892578125\n",
      "Epoch [217/250], Training Loss: 235.94919263910185, Validation Loss: 5241.54931640625\n",
      "Epoch [218/250], Training Loss: 666.5568795234166, Validation Loss: 10014.486328125\n",
      "Epoch [219/250], Training Loss: 261.1639218047883, Validation Loss: 6729.5146484375\n",
      "Epoch [220/250], Training Loss: 178.89676376407766, Validation Loss: 5323.12548828125\n",
      "Epoch [221/250], Training Loss: 138.87190914561356, Validation Loss: 4822.11962890625\n",
      "Epoch [222/250], Training Loss: 136.27526936676455, Validation Loss: 5303.90673828125\n",
      "Epoch [223/250], Training Loss: 202.88374168375447, Validation Loss: 3511.341796875\n",
      "Epoch [224/250], Training Loss: 215.97165294953308, Validation Loss: 2541.138427734375\n",
      "Epoch [225/250], Training Loss: 95.06856433758412, Validation Loss: 3202.68994140625\n",
      "Epoch [226/250], Training Loss: 116.5331537069389, Validation Loss: 2491.921630859375\n",
      "Epoch [227/250], Training Loss: 140.91782738206683, Validation Loss: 5434.07958984375\n",
      "Epoch [228/250], Training Loss: 396.7080379709564, Validation Loss: 5372.7861328125\n",
      "Epoch [229/250], Training Loss: 162.82003224584594, Validation Loss: 5488.349609375\n",
      "Epoch [230/250], Training Loss: 215.78264497144727, Validation Loss: 5649.85107421875\n",
      "Epoch [231/250], Training Loss: 175.19007947140656, Validation Loss: 5614.2158203125\n",
      "Epoch [232/250], Training Loss: 153.99944144558847, Validation Loss: 5339.83203125\n",
      "Epoch [233/250], Training Loss: 170.6262794923011, Validation Loss: 4656.07421875\n",
      "Epoch [234/250], Training Loss: 132.7651780395176, Validation Loss: 2737.42333984375\n",
      "Epoch [235/250], Training Loss: 138.94869078513966, Validation Loss: 2967.365234375\n",
      "Epoch [236/250], Training Loss: 128.50972032404826, Validation Loss: 2630.198486328125\n",
      "Epoch [237/250], Training Loss: 77.37392248133364, Validation Loss: 4691.05126953125\n",
      "Epoch [238/250], Training Loss: 117.50196716641115, Validation Loss: 2980.000732421875\n",
      "Epoch [239/250], Training Loss: 102.28067327171993, Validation Loss: 4885.8076171875\n",
      "Epoch [240/250], Training Loss: 112.38350169060395, Validation Loss: 3779.879150390625\n",
      "Epoch [241/250], Training Loss: 171.06674518228974, Validation Loss: 2540.908203125\n",
      "Epoch [242/250], Training Loss: 138.3325193542625, Validation Loss: 4358.2216796875\n",
      "Epoch [243/250], Training Loss: 124.78869437812705, Validation Loss: 3635.3037109375\n",
      "Epoch [244/250], Training Loss: 90.7575539940364, Validation Loss: 6477.87841796875\n",
      "Epoch [245/250], Training Loss: 145.93902311032906, Validation Loss: 6079.47998046875\n",
      "Epoch [246/250], Training Loss: 168.81085304494042, Validation Loss: 5466.8056640625\n",
      "Epoch [247/250], Training Loss: 128.19875197468602, Validation Loss: 6018.0009765625\n",
      "Epoch [248/250], Training Loss: 156.94058293730924, Validation Loss: 6101.7529296875\n",
      "Epoch [249/250], Training Loss: 166.3564301899978, Validation Loss: 4704.97705078125\n",
      "Epoch [250/250], Training Loss: 112.5491690843509, Validation Loss: 4596.94677734375\n",
      "Test Loss: 4932.84765625\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.05, 10)\n",
      "Epoch [1/250], Training Loss: 4667.081850868444, Validation Loss: 21849.125\n",
      "Epoch [2/250], Training Loss: 11547.625150221334, Validation Loss: 19525.517578125\n",
      "Epoch [3/250], Training Loss: 4803.667386026134, Validation Loss: 22756.98828125\n",
      "Epoch [4/250], Training Loss: 5077.097429958896, Validation Loss: 20002.73046875\n",
      "Epoch [5/250], Training Loss: 3273.9672630526265, Validation Loss: 14540.751953125\n",
      "Epoch [6/250], Training Loss: 1954.136114411676, Validation Loss: 17949.1796875\n",
      "Epoch [7/250], Training Loss: 1283.3089060539733, Validation Loss: 11924.7568359375\n",
      "Epoch [8/250], Training Loss: 830.9167643093186, Validation Loss: 8370.9228515625\n",
      "Epoch [9/250], Training Loss: 553.9826853540571, Validation Loss: 8689.818359375\n",
      "Epoch [10/250], Training Loss: 445.77568373134216, Validation Loss: 8458.9345703125\n",
      "Epoch [11/250], Training Loss: 454.97131968913607, Validation Loss: 12768.3828125\n",
      "Epoch [12/250], Training Loss: 405.166669819863, Validation Loss: 7274.5791015625\n",
      "Epoch [13/250], Training Loss: 258.36750722300303, Validation Loss: 15551.5927734375\n",
      "Epoch [14/250], Training Loss: 610.8187733691093, Validation Loss: 10535.4931640625\n",
      "Epoch [15/250], Training Loss: 539.2493573546424, Validation Loss: 8770.337890625\n",
      "Epoch [16/250], Training Loss: 840.8714715718157, Validation Loss: 17062.78125\n",
      "Epoch [17/250], Training Loss: 305.29878094700166, Validation Loss: 12745.3896484375\n",
      "Epoch [18/250], Training Loss: 371.152947238169, Validation Loss: 11489.3564453125\n",
      "Epoch [19/250], Training Loss: 282.2097363308777, Validation Loss: 9061.962890625\n",
      "Epoch [20/250], Training Loss: 418.4499098453823, Validation Loss: 14112.556640625\n",
      "Epoch [21/250], Training Loss: 871.7414959283772, Validation Loss: 8871.068359375\n",
      "Epoch [22/250], Training Loss: 530.5888611511242, Validation Loss: 10153.22265625\n",
      "Epoch [23/250], Training Loss: 402.2683309506947, Validation Loss: 8043.1484375\n",
      "Epoch [24/250], Training Loss: 346.2212698908559, Validation Loss: 5307.28759765625\n",
      "Epoch [25/250], Training Loss: 267.69530076574546, Validation Loss: 7768.21875\n",
      "Epoch [26/250], Training Loss: 290.08759064433474, Validation Loss: 7573.9033203125\n",
      "Epoch [27/250], Training Loss: 291.1645697242328, Validation Loss: 12902.9482421875\n",
      "Epoch [28/250], Training Loss: 447.8707236035657, Validation Loss: 8152.75537109375\n",
      "Epoch [29/250], Training Loss: 397.4880224694655, Validation Loss: 6559.91357421875\n",
      "Epoch [30/250], Training Loss: 414.1668442920818, Validation Loss: 9059.0791015625\n",
      "Epoch [31/250], Training Loss: 386.6802026469194, Validation Loss: 11069.6005859375\n",
      "Epoch [32/250], Training Loss: 473.99508022290854, Validation Loss: 6742.171875\n",
      "Epoch [33/250], Training Loss: 401.14335394166284, Validation Loss: 13773.05078125\n",
      "Epoch [34/250], Training Loss: 1133.2894886493254, Validation Loss: 13760.8837890625\n",
      "Epoch [35/250], Training Loss: 749.226034953406, Validation Loss: 14004.3408203125\n",
      "Epoch [36/250], Training Loss: 657.7580895767929, Validation Loss: 13959.3046875\n",
      "Epoch [37/250], Training Loss: 742.8719637876786, Validation Loss: 13840.7412109375\n",
      "Epoch [38/250], Training Loss: 311.46031802460203, Validation Loss: 12453.9580078125\n",
      "Epoch [39/250], Training Loss: 267.3445760484573, Validation Loss: 12263.6494140625\n",
      "Epoch [40/250], Training Loss: 417.3657371940463, Validation Loss: 17068.232421875\n",
      "Epoch [41/250], Training Loss: 379.5476682637594, Validation Loss: 13341.3720703125\n",
      "Epoch [42/250], Training Loss: 568.0069321101269, Validation Loss: 17455.458984375\n",
      "Epoch [43/250], Training Loss: 572.7337953082363, Validation Loss: 14294.271484375\n",
      "Epoch [44/250], Training Loss: 854.3250330592077, Validation Loss: 11385.666015625\n",
      "Epoch [45/250], Training Loss: 608.8390375546345, Validation Loss: 32000.16796875\n",
      "Epoch [46/250], Training Loss: 4175.903344438266, Validation Loss: 13040.185546875\n",
      "Epoch [47/250], Training Loss: 369.470914523275, Validation Loss: 16519.337890625\n",
      "Epoch [48/250], Training Loss: 663.3288028834737, Validation Loss: 9819.3759765625\n",
      "Epoch [49/250], Training Loss: 302.10289050981226, Validation Loss: 10771.0322265625\n",
      "Epoch [50/250], Training Loss: 623.6760436813739, Validation Loss: 16444.150390625\n",
      "Epoch [51/250], Training Loss: 1223.1978986530246, Validation Loss: 12967.2705078125\n",
      "Epoch [52/250], Training Loss: 323.04487653951327, Validation Loss: 5012.41748046875\n",
      "Epoch [53/250], Training Loss: 143.83108306709923, Validation Loss: 13776.7197265625\n",
      "Epoch [54/250], Training Loss: 793.66448404726, Validation Loss: 14718.419921875\n",
      "Epoch [55/250], Training Loss: 636.2077236500689, Validation Loss: 5187.892578125\n",
      "Epoch [56/250], Training Loss: 387.1384960136217, Validation Loss: 12508.5400390625\n",
      "Epoch [57/250], Training Loss: 791.3293501948287, Validation Loss: 11254.6953125\n",
      "Epoch [58/250], Training Loss: 312.4147761823571, Validation Loss: 9808.6328125\n",
      "Epoch [59/250], Training Loss: 486.55620296889964, Validation Loss: 15310.1435546875\n",
      "Epoch [60/250], Training Loss: 543.6557744150318, Validation Loss: 12461.8876953125\n",
      "Epoch [61/250], Training Loss: 483.3060694920909, Validation Loss: 29767.421875\n",
      "Epoch [62/250], Training Loss: 815.5206123600713, Validation Loss: 12515.451171875\n",
      "Epoch [63/250], Training Loss: 655.6934656957001, Validation Loss: 29240.853515625\n",
      "Epoch [64/250], Training Loss: 1080.3587508047292, Validation Loss: 14165.212890625\n",
      "Epoch [65/250], Training Loss: 649.2737017949322, Validation Loss: 11831.419921875\n",
      "Epoch [66/250], Training Loss: 394.2157274031752, Validation Loss: 13338.3251953125\n",
      "Epoch [67/250], Training Loss: 766.3083083412145, Validation Loss: 14054.5703125\n",
      "Epoch [68/250], Training Loss: 510.30181259924353, Validation Loss: 11280.4296875\n",
      "Epoch [69/250], Training Loss: 351.9659222804932, Validation Loss: 13200.9970703125\n",
      "Epoch [70/250], Training Loss: 572.7779644517399, Validation Loss: 12634.4501953125\n",
      "Epoch [71/250], Training Loss: 792.4308485386638, Validation Loss: 14844.5244140625\n",
      "Epoch [72/250], Training Loss: 1017.7448352485578, Validation Loss: 18532.6328125\n",
      "Epoch [73/250], Training Loss: 601.5911860929638, Validation Loss: 15160.1533203125\n",
      "Epoch [74/250], Training Loss: 624.3829323489595, Validation Loss: 14611.6943359375\n",
      "Epoch [75/250], Training Loss: 481.99632171594106, Validation Loss: 9680.140625\n",
      "Epoch [76/250], Training Loss: 514.359606242575, Validation Loss: 9101.0712890625\n",
      "Epoch [77/250], Training Loss: 280.27036546691284, Validation Loss: 8114.58154296875\n",
      "Epoch [78/250], Training Loss: 257.73140078435904, Validation Loss: 15001.689453125\n",
      "Epoch [79/250], Training Loss: 807.3069417061464, Validation Loss: 14860.5712890625\n",
      "Epoch [80/250], Training Loss: 389.88255194804015, Validation Loss: 12677.439453125\n",
      "Epoch [81/250], Training Loss: 804.9144274368462, Validation Loss: 12634.5107421875\n",
      "Epoch [82/250], Training Loss: 359.7273832231539, Validation Loss: 11280.8720703125\n",
      "Epoch [83/250], Training Loss: 359.44688373008285, Validation Loss: 10847.0009765625\n",
      "Epoch [84/250], Training Loss: 329.7728402646856, Validation Loss: 12876.015625\n",
      "Epoch [85/250], Training Loss: 395.105162542734, Validation Loss: 12090.7177734375\n",
      "Epoch [86/250], Training Loss: 418.437647221952, Validation Loss: 11516.232421875\n",
      "Epoch [87/250], Training Loss: 369.8059220781595, Validation Loss: 18247.47265625\n",
      "Epoch [88/250], Training Loss: 1305.253333341801, Validation Loss: 13815.3046875\n",
      "Epoch [89/250], Training Loss: 653.2638755742315, Validation Loss: 6074.80029296875\n",
      "Epoch [90/250], Training Loss: 136.26800617903808, Validation Loss: 6534.62109375\n",
      "Epoch [91/250], Training Loss: 105.75267307086196, Validation Loss: 5683.57958984375\n",
      "Epoch [92/250], Training Loss: 167.90424916129962, Validation Loss: 9201.029296875\n",
      "Epoch [93/250], Training Loss: 372.4036237703398, Validation Loss: 4557.67919921875\n",
      "Epoch [94/250], Training Loss: 204.54433999819318, Validation Loss: 10986.2666015625\n",
      "Epoch [95/250], Training Loss: 551.162058863861, Validation Loss: 7912.83544921875\n",
      "Epoch [96/250], Training Loss: 394.6134317401299, Validation Loss: 12630.55859375\n",
      "Epoch [97/250], Training Loss: 1034.232329822743, Validation Loss: 15473.5654296875\n",
      "Epoch [98/250], Training Loss: 611.32237020932, Validation Loss: 22154.41796875\n",
      "Epoch [99/250], Training Loss: 3485.3327820647705, Validation Loss: 13806.130859375\n",
      "Epoch [100/250], Training Loss: 663.8436052833486, Validation Loss: 6412.3115234375\n",
      "Epoch [101/250], Training Loss: 299.38634355511596, Validation Loss: 92084.0078125\n",
      "Epoch [102/250], Training Loss: 9930.497291292677, Validation Loss: 6310.58203125\n",
      "Epoch [103/250], Training Loss: 204.65700642980056, Validation Loss: 10905.927734375\n",
      "Epoch [104/250], Training Loss: 530.3312653852593, Validation Loss: 29995.306640625\n",
      "Epoch [105/250], Training Loss: 616.2076037060892, Validation Loss: 12342.3056640625\n",
      "Epoch [106/250], Training Loss: 508.1480297675665, Validation Loss: 11969.7490234375\n",
      "Epoch [107/250], Training Loss: 392.7173699096342, Validation Loss: 13783.119140625\n",
      "Epoch [108/250], Training Loss: 335.3036725949834, Validation Loss: 15852.3310546875\n",
      "Epoch [109/250], Training Loss: 1313.460245309713, Validation Loss: 17101.095703125\n",
      "Epoch [110/250], Training Loss: 666.7206566827962, Validation Loss: 12107.3486328125\n",
      "Epoch [111/250], Training Loss: 707.6547604916432, Validation Loss: 22099.095703125\n",
      "Epoch [112/250], Training Loss: 707.6354950752532, Validation Loss: 11461.7978515625\n",
      "Epoch [113/250], Training Loss: 676.815093192409, Validation Loss: 20022.048828125\n",
      "Epoch [114/250], Training Loss: 794.8392418322217, Validation Loss: 11685.384765625\n",
      "Epoch [115/250], Training Loss: 604.5370990102252, Validation Loss: 10922.6591796875\n",
      "Epoch [116/250], Training Loss: 286.6537339927527, Validation Loss: 9259.990234375\n",
      "Epoch [117/250], Training Loss: 255.03725902242476, Validation Loss: 10614.2802734375\n",
      "Epoch [118/250], Training Loss: 438.9022639007942, Validation Loss: 10500.1875\n",
      "Epoch [119/250], Training Loss: 423.29142922821643, Validation Loss: 11583.4306640625\n",
      "Epoch [120/250], Training Loss: 340.448081534405, Validation Loss: 13641.41796875\n",
      "Epoch [121/250], Training Loss: 432.8296641856621, Validation Loss: 13320.58984375\n",
      "Epoch [122/250], Training Loss: 508.4302840120757, Validation Loss: 16210.9541015625\n",
      "Epoch [123/250], Training Loss: 568.5402702814035, Validation Loss: 15824.556640625\n",
      "Epoch [124/250], Training Loss: 389.9927945122726, Validation Loss: 16591.138671875\n",
      "Epoch [125/250], Training Loss: 475.4985882284766, Validation Loss: 15204.0205078125\n",
      "Epoch [126/250], Training Loss: 884.7279971771113, Validation Loss: 18394.841796875\n",
      "Epoch [127/250], Training Loss: 898.4045216795187, Validation Loss: 14437.10546875\n",
      "Epoch [128/250], Training Loss: 135.67777119466962, Validation Loss: 14880.16796875\n",
      "Epoch [129/250], Training Loss: 420.8954052780579, Validation Loss: 16665.859375\n",
      "Epoch [130/250], Training Loss: 464.3628966947966, Validation Loss: 7377.75732421875\n",
      "Epoch [131/250], Training Loss: 120.86182014948086, Validation Loss: 17357.27734375\n",
      "Epoch [132/250], Training Loss: 1092.5424833126465, Validation Loss: 10801.466796875\n",
      "Epoch [133/250], Training Loss: 202.35290440483712, Validation Loss: 9886.0126953125\n",
      "Epoch [134/250], Training Loss: 420.37767170105894, Validation Loss: 9435.7783203125\n",
      "Epoch [135/250], Training Loss: 414.27195452695173, Validation Loss: 9496.1328125\n",
      "Epoch [136/250], Training Loss: 499.93515104471226, Validation Loss: 13200.888671875\n",
      "Epoch [137/250], Training Loss: 624.8253165810337, Validation Loss: 9006.291015625\n",
      "Epoch [138/250], Training Loss: 332.14529360930516, Validation Loss: 10132.662109375\n",
      "Epoch [139/250], Training Loss: 530.827720263268, Validation Loss: 18341.259765625\n",
      "Epoch [140/250], Training Loss: 676.6313152038593, Validation Loss: 8539.2685546875\n",
      "Epoch [141/250], Training Loss: 281.23662051747476, Validation Loss: 11604.4189453125\n",
      "Epoch [142/250], Training Loss: 493.43081682894075, Validation Loss: 7485.197265625\n",
      "Epoch [143/250], Training Loss: 345.40541575531176, Validation Loss: 11092.5029296875\n",
      "Epoch [144/250], Training Loss: 627.199419132545, Validation Loss: 11743.017578125\n",
      "Epoch [145/250], Training Loss: 850.525830963839, Validation Loss: 12738.939453125\n",
      "Epoch [146/250], Training Loss: 680.930709501751, Validation Loss: 8278.41015625\n",
      "Epoch [147/250], Training Loss: 287.4984658635303, Validation Loss: 9374.701171875\n",
      "Epoch [148/250], Training Loss: 475.2402909723462, Validation Loss: 9036.4697265625\n",
      "Epoch [149/250], Training Loss: 509.44637599701935, Validation Loss: 9315.5078125\n",
      "Epoch [150/250], Training Loss: 533.1292553969386, Validation Loss: 9627.708984375\n",
      "Epoch [151/250], Training Loss: 414.6119325166868, Validation Loss: 8131.10400390625\n",
      "Epoch [152/250], Training Loss: 366.5062407265811, Validation Loss: 10570.90625\n",
      "Epoch [153/250], Training Loss: 520.7299352886366, Validation Loss: 8599.8466796875\n",
      "Epoch [154/250], Training Loss: 346.71153008905685, Validation Loss: 8169.1181640625\n",
      "Epoch [155/250], Training Loss: 346.344957599912, Validation Loss: 10712.4375\n",
      "Epoch [156/250], Training Loss: 539.0970310984095, Validation Loss: 12327.6396484375\n",
      "Epoch [157/250], Training Loss: 576.2089561056044, Validation Loss: 7910.6572265625\n",
      "Epoch [158/250], Training Loss: 263.9606963597014, Validation Loss: 9360.4736328125\n",
      "Epoch [159/250], Training Loss: 463.09331937923304, Validation Loss: 8927.8154296875\n",
      "Epoch [160/250], Training Loss: 390.14834690511486, Validation Loss: 8512.8505859375\n",
      "Epoch [161/250], Training Loss: 515.0431129128419, Validation Loss: 9536.02734375\n",
      "Epoch [162/250], Training Loss: 453.29916078107306, Validation Loss: 10994.734375\n",
      "Epoch [163/250], Training Loss: 12403.460059150686, Validation Loss: 89442.2890625\n",
      "Epoch [164/250], Training Loss: 52924.63061835285, Validation Loss: 28923.34765625\n",
      "Epoch [165/250], Training Loss: 3313.7527724071497, Validation Loss: 8825.2138671875\n",
      "Epoch [166/250], Training Loss: 903.734615280379, Validation Loss: 9832.5546875\n",
      "Epoch [167/250], Training Loss: 945.7052991614395, Validation Loss: 10104.82421875\n",
      "Epoch [168/250], Training Loss: 886.6462050280709, Validation Loss: 10553.837890625\n",
      "Epoch [169/250], Training Loss: 938.5709286410568, Validation Loss: 12579.068359375\n",
      "Epoch [170/250], Training Loss: 930.0135634329877, Validation Loss: 7088.7255859375\n",
      "Epoch [171/250], Training Loss: 340.12522775753257, Validation Loss: 8265.8740234375\n",
      "Epoch [172/250], Training Loss: 397.90104326525903, Validation Loss: 8007.59521484375\n",
      "Epoch [173/250], Training Loss: 395.46611148621633, Validation Loss: 8214.4814453125\n",
      "Epoch [174/250], Training Loss: 375.6251665883606, Validation Loss: 8312.0234375\n",
      "Epoch [175/250], Training Loss: 401.1992550740241, Validation Loss: 13200.6962890625\n",
      "Epoch [176/250], Training Loss: 922.8001984800312, Validation Loss: 7747.25\n",
      "Epoch [177/250], Training Loss: 371.66826341151807, Validation Loss: 8265.7158203125\n",
      "Epoch [178/250], Training Loss: 333.3788878333621, Validation Loss: 8162.001953125\n",
      "Epoch [179/250], Training Loss: 376.4270019835078, Validation Loss: 8477.357421875\n",
      "Epoch [180/250], Training Loss: 421.7899501435336, Validation Loss: 7896.00634765625\n",
      "Epoch [181/250], Training Loss: 438.1427371708584, Validation Loss: 11612.025390625\n",
      "Epoch [182/250], Training Loss: 662.5517831827967, Validation Loss: 13557.2900390625\n",
      "Epoch [183/250], Training Loss: 774.2578430764848, Validation Loss: 15422.0810546875\n",
      "Epoch [184/250], Training Loss: 1013.5907082083417, Validation Loss: 18257.857421875\n",
      "Epoch [185/250], Training Loss: 897.8956831850862, Validation Loss: 12051.423828125\n",
      "Epoch [186/250], Training Loss: 314.050312352774, Validation Loss: 10018.0654296875\n",
      "Epoch [187/250], Training Loss: 395.6593662990597, Validation Loss: 11048.6767578125\n",
      "Epoch [188/250], Training Loss: 12130.706287085946, Validation Loss: 75350.6484375\n",
      "Epoch [189/250], Training Loss: 56058.30650130284, Validation Loss: 27300.51953125\n",
      "Epoch [190/250], Training Loss: 3020.182462971696, Validation Loss: 6152.83203125\n",
      "Epoch [191/250], Training Loss: 1098.9849820281752, Validation Loss: 7651.86279296875\n",
      "Epoch [192/250], Training Loss: 400.31949521817654, Validation Loss: 8701.5\n",
      "Epoch [193/250], Training Loss: 536.4229945632268, Validation Loss: 10232.0283203125\n",
      "Epoch [194/250], Training Loss: 713.3753776112535, Validation Loss: 15766.73828125\n",
      "Epoch [195/250], Training Loss: 617.6384610527666, Validation Loss: 9171.7314453125\n",
      "Epoch [196/250], Training Loss: 350.5329093452726, Validation Loss: 8284.6015625\n",
      "Epoch [197/250], Training Loss: 380.6921621499372, Validation Loss: 10103.06640625\n",
      "Epoch [198/250], Training Loss: 417.67147681919965, Validation Loss: 9318.1123046875\n",
      "Epoch [199/250], Training Loss: 639.9482009499121, Validation Loss: 28667.900390625\n",
      "Epoch [200/250], Training Loss: 161.25709675536882, Validation Loss: 9117.35546875\n",
      "Epoch [201/250], Training Loss: 765.6914518639085, Validation Loss: 9565.384765625\n",
      "Epoch [202/250], Training Loss: 383.61555226393165, Validation Loss: 8882.9658203125\n",
      "Epoch [203/250], Training Loss: 428.26742546775745, Validation Loss: 9925.1630859375\n",
      "Epoch [204/250], Training Loss: 481.51817888155955, Validation Loss: 10579.4267578125\n",
      "Epoch [205/250], Training Loss: 543.51650003575, Validation Loss: 13416.95703125\n",
      "Epoch [206/250], Training Loss: 782.6446299117288, Validation Loss: 10871.1376953125\n",
      "Epoch [207/250], Training Loss: 327.165715901636, Validation Loss: 10321.861328125\n",
      "Epoch [208/250], Training Loss: 496.9583778251334, Validation Loss: 11887.10546875\n",
      "Epoch [209/250], Training Loss: 271.4160260838748, Validation Loss: 10564.79296875\n",
      "Epoch [210/250], Training Loss: 472.2293818540509, Validation Loss: 10306.03125\n",
      "Epoch [211/250], Training Loss: 689.3071539777889, Validation Loss: 18650.814453125\n",
      "Epoch [212/250], Training Loss: 818.074215198607, Validation Loss: 12007.939453125\n",
      "Epoch [213/250], Training Loss: 244.41526824130833, Validation Loss: 15104.740234375\n",
      "Epoch [214/250], Training Loss: 1865.089477610006, Validation Loss: 10966.21875\n",
      "Epoch [215/250], Training Loss: 583.9090322507125, Validation Loss: 10577.4951171875\n",
      "Epoch [216/250], Training Loss: 466.11369152796016, Validation Loss: 8307.7705078125\n",
      "Epoch [217/250], Training Loss: 351.4318407259168, Validation Loss: 10682.48046875\n",
      "Epoch [218/250], Training Loss: 501.2870949807627, Validation Loss: 10373.939453125\n",
      "Epoch [219/250], Training Loss: 474.93579930325564, Validation Loss: 9647.7626953125\n",
      "Epoch [220/250], Training Loss: 483.2359011795373, Validation Loss: 9748.7890625\n",
      "Epoch [221/250], Training Loss: 414.5560887265093, Validation Loss: 9101.8994140625\n",
      "Epoch [222/250], Training Loss: 403.4150759205458, Validation Loss: 10540.80859375\n",
      "Epoch [223/250], Training Loss: 650.7421727835854, Validation Loss: 11349.546875\n",
      "Epoch [224/250], Training Loss: 1547.521505583633, Validation Loss: 37746.91015625\n",
      "Epoch [225/250], Training Loss: 1368.137017775049, Validation Loss: 10964.537109375\n",
      "Epoch [226/250], Training Loss: 586.8859351337936, Validation Loss: 9598.69921875\n",
      "Epoch [227/250], Training Loss: 492.88093157586354, Validation Loss: 9358.7841796875\n",
      "Epoch [228/250], Training Loss: 465.55247295270254, Validation Loss: 9540.490234375\n",
      "Epoch [229/250], Training Loss: 455.55391200627884, Validation Loss: 11415.666015625\n",
      "Epoch [230/250], Training Loss: 467.9979388781498, Validation Loss: 11771.3271484375\n",
      "Epoch [231/250], Training Loss: 486.2886710420606, Validation Loss: 10457.2685546875\n",
      "Epoch [232/250], Training Loss: 664.1806126898373, Validation Loss: 17273.7578125\n",
      "Epoch [233/250], Training Loss: 4080.0737592791756, Validation Loss: 10913.49609375\n",
      "Epoch [234/250], Training Loss: 626.2871533528934, Validation Loss: 11521.1806640625\n",
      "Epoch [235/250], Training Loss: 650.6523410814586, Validation Loss: 10949.8076171875\n",
      "Epoch [236/250], Training Loss: 1952.028789069116, Validation Loss: 44049.23046875\n",
      "Epoch [237/250], Training Loss: 861.1266009683801, Validation Loss: 11069.169921875\n",
      "Epoch [238/250], Training Loss: 626.445396871545, Validation Loss: 11884.2666015625\n",
      "Epoch [239/250], Training Loss: 2486.622369768797, Validation Loss: 53719.4375\n",
      "Epoch [240/250], Training Loss: 11064.66770890656, Validation Loss: 34222.63671875\n",
      "Epoch [241/250], Training Loss: 800.8209161184328, Validation Loss: 16985.005859375\n",
      "Epoch [242/250], Training Loss: 1145.725869454369, Validation Loss: 19657.859375\n",
      "Epoch [243/250], Training Loss: 2920.6941889615873, Validation Loss: 11285.5810546875\n",
      "Epoch [244/250], Training Loss: 509.3209509021095, Validation Loss: 12047.79296875\n",
      "Epoch [245/250], Training Loss: 475.40971816919586, Validation Loss: 10233.080078125\n",
      "Epoch [246/250], Training Loss: 462.3230962686227, Validation Loss: 17094.07421875\n",
      "Epoch [247/250], Training Loss: 373.5042443862891, Validation Loss: 9843.396484375\n",
      "Epoch [248/250], Training Loss: 374.4998572040271, Validation Loss: 9421.931640625\n",
      "Epoch [249/250], Training Loss: 426.9009862989261, Validation Loss: 11085.1416015625\n",
      "Epoch [250/250], Training Loss: 479.60936062011973, Validation Loss: 15025.365234375\n",
      "Test Loss: 16975.375\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.05, 15)\n",
      "Epoch [1/250], Training Loss: 4404.202485135248, Validation Loss: 13948.57421875\n",
      "Epoch [2/250], Training Loss: 3300.553221966984, Validation Loss: 9133.3876953125\n",
      "Epoch [3/250], Training Loss: 1694.2255782428003, Validation Loss: 6517.36328125\n",
      "Epoch [4/250], Training Loss: 989.0242480833864, Validation Loss: 6811.75927734375\n",
      "Epoch [5/250], Training Loss: 602.4853433494038, Validation Loss: 6394.1494140625\n",
      "Epoch [6/250], Training Loss: 447.7019366459572, Validation Loss: 4974.771484375\n",
      "Epoch [7/250], Training Loss: 360.08781169745487, Validation Loss: 6022.234375\n",
      "Epoch [8/250], Training Loss: 285.58085874817334, Validation Loss: 4477.3525390625\n",
      "Epoch [9/250], Training Loss: 272.7082371050107, Validation Loss: 4294.7978515625\n",
      "Epoch [10/250], Training Loss: 10231.606545972521, Validation Loss: 24544.080078125\n",
      "Epoch [11/250], Training Loss: 5587.35108906417, Validation Loss: 22484.521484375\n",
      "Epoch [12/250], Training Loss: 2322.9551470007905, Validation Loss: 24325.501953125\n",
      "Epoch [13/250], Training Loss: 1046.674379735425, Validation Loss: 20577.0703125\n",
      "Epoch [14/250], Training Loss: 901.3265021864423, Validation Loss: 20251.73046875\n",
      "Epoch [15/250], Training Loss: 1080.3974043429193, Validation Loss: 19912.515625\n",
      "Epoch [16/250], Training Loss: 865.7694099815625, Validation Loss: 19918.640625\n",
      "Epoch [17/250], Training Loss: 809.7731817376717, Validation Loss: 19943.482421875\n",
      "Epoch [18/250], Training Loss: 762.5777131847996, Validation Loss: 19950.89453125\n",
      "Epoch [19/250], Training Loss: 769.1317020389862, Validation Loss: 19909.814453125\n",
      "Epoch [20/250], Training Loss: 724.0137496685313, Validation Loss: 19815.302734375\n",
      "Epoch [21/250], Training Loss: 704.4619969082509, Validation Loss: 19982.236328125\n",
      "Epoch [22/250], Training Loss: 844.6931026109208, Validation Loss: 19906.201171875\n",
      "Epoch [23/250], Training Loss: 701.5245598874749, Validation Loss: 20052.466796875\n",
      "Epoch [24/250], Training Loss: 742.0674091702144, Validation Loss: 19931.55859375\n",
      "Epoch [25/250], Training Loss: 778.9560900739705, Validation Loss: 19981.06640625\n",
      "Epoch [26/250], Training Loss: 749.4435177480059, Validation Loss: 20027.505859375\n",
      "Epoch [27/250], Training Loss: 706.7477676806151, Validation Loss: 19995.33984375\n",
      "Epoch [28/250], Training Loss: 677.5022519010582, Validation Loss: 20145.81640625\n",
      "Epoch [29/250], Training Loss: 628.2006963303212, Validation Loss: 20028.046875\n",
      "Epoch [30/250], Training Loss: 684.9900152324915, Validation Loss: 19991.201171875\n",
      "Epoch [31/250], Training Loss: 644.3081843926221, Validation Loss: 20276.94140625\n",
      "Epoch [32/250], Training Loss: 691.5066259783314, Validation Loss: 20090.7890625\n",
      "Epoch [33/250], Training Loss: 958.6417459347003, Validation Loss: 19901.455078125\n",
      "Epoch [34/250], Training Loss: 710.4731036423103, Validation Loss: 19996.1875\n",
      "Epoch [35/250], Training Loss: 752.1340771262089, Validation Loss: 19934.986328125\n",
      "Epoch [36/250], Training Loss: 982.252025205927, Validation Loss: 18872.53515625\n",
      "Epoch [37/250], Training Loss: 1077.6564076368268, Validation Loss: 19470.3828125\n",
      "Epoch [38/250], Training Loss: 831.3951600831651, Validation Loss: 19184.244140625\n",
      "Epoch [39/250], Training Loss: 621.924950785577, Validation Loss: 18224.703125\n",
      "Epoch [40/250], Training Loss: 487.5602134564652, Validation Loss: 19925.541015625\n",
      "Epoch [41/250], Training Loss: 664.5860936791835, Validation Loss: 19931.619140625\n",
      "Epoch [42/250], Training Loss: 654.6701280127002, Validation Loss: 20017.96875\n",
      "Epoch [43/250], Training Loss: 656.2520941285654, Validation Loss: 19919.283203125\n",
      "Epoch [44/250], Training Loss: 624.9736391977445, Validation Loss: 20030.326171875\n",
      "Epoch [45/250], Training Loss: 739.4198953757171, Validation Loss: 18976.599609375\n",
      "Epoch [46/250], Training Loss: 569.4094519949248, Validation Loss: 20015.888671875\n",
      "Epoch [47/250], Training Loss: 730.4418958045735, Validation Loss: 18547.3203125\n",
      "Epoch [48/250], Training Loss: 542.7179495010935, Validation Loss: 19714.546875\n",
      "Epoch [49/250], Training Loss: 626.0556616728378, Validation Loss: 20032.443359375\n",
      "Epoch [50/250], Training Loss: 903.4681883120159, Validation Loss: 19774.630859375\n",
      "Epoch [51/250], Training Loss: 645.28617051155, Validation Loss: 19337.01171875\n",
      "Epoch [52/250], Training Loss: 1121.5204121451159, Validation Loss: 19166.171875\n",
      "Epoch [53/250], Training Loss: 2004.9367030147303, Validation Loss: 19464.18359375\n",
      "Epoch [54/250], Training Loss: 1332.6102225061518, Validation Loss: 18568.515625\n",
      "Epoch [55/250], Training Loss: 627.0302924820946, Validation Loss: 18230.001953125\n",
      "Epoch [56/250], Training Loss: 488.60480095096074, Validation Loss: 19780.169921875\n",
      "Epoch [57/250], Training Loss: 643.3960331472532, Validation Loss: 19942.359375\n",
      "Epoch [58/250], Training Loss: 632.7186492666283, Validation Loss: 20025.8671875\n",
      "Epoch [59/250], Training Loss: 671.9695813716281, Validation Loss: 19969.537109375\n",
      "Epoch [60/250], Training Loss: 1050.4601924093188, Validation Loss: 19666.646484375\n",
      "Epoch [61/250], Training Loss: 663.5649374644656, Validation Loss: 19166.79296875\n",
      "Epoch [62/250], Training Loss: 1013.7673895427637, Validation Loss: 18700.0\n",
      "Epoch [63/250], Training Loss: 607.1039267691395, Validation Loss: 17955.517578125\n",
      "Epoch [64/250], Training Loss: 936.8934057837644, Validation Loss: 19048.037109375\n",
      "Epoch [65/250], Training Loss: 665.7837718511976, Validation Loss: 19035.78125\n",
      "Epoch [66/250], Training Loss: 563.0550853985255, Validation Loss: 20025.8671875\n",
      "Epoch [67/250], Training Loss: 653.9167979347903, Validation Loss: 20024.158203125\n",
      "Epoch [68/250], Training Loss: 660.5096583196599, Validation Loss: 20015.32421875\n",
      "Epoch [69/250], Training Loss: 930.8073296319077, Validation Loss: 19332.974609375\n",
      "Epoch [70/250], Training Loss: 809.0746297899941, Validation Loss: 18642.599609375\n",
      "Epoch [71/250], Training Loss: 672.2065381054276, Validation Loss: 18818.3125\n",
      "Epoch [72/250], Training Loss: 636.8701055454414, Validation Loss: 19012.986328125\n",
      "Epoch [73/250], Training Loss: 570.1731418719854, Validation Loss: 19180.927734375\n",
      "Epoch [74/250], Training Loss: 766.8027792403291, Validation Loss: 18682.279296875\n",
      "Epoch [75/250], Training Loss: 690.8690876206363, Validation Loss: 19025.57421875\n",
      "Epoch [76/250], Training Loss: 850.0597032489818, Validation Loss: 19478.244140625\n",
      "Epoch [77/250], Training Loss: 684.9877381286938, Validation Loss: 18825.271484375\n",
      "Epoch [78/250], Training Loss: 701.090908830812, Validation Loss: 18815.60546875\n",
      "Epoch [79/250], Training Loss: 847.6443201167017, Validation Loss: 19304.091796875\n",
      "Epoch [80/250], Training Loss: 633.099592271694, Validation Loss: 18792.484375\n",
      "Epoch [81/250], Training Loss: 888.3744079340216, Validation Loss: 19325.46875\n",
      "Epoch [82/250], Training Loss: 807.5773412706319, Validation Loss: 18861.1640625\n",
      "Epoch [83/250], Training Loss: 745.803631559029, Validation Loss: 18870.55078125\n",
      "Epoch [84/250], Training Loss: 730.0909971341973, Validation Loss: 18896.978515625\n",
      "Epoch [85/250], Training Loss: 907.7992818148128, Validation Loss: 18949.123046875\n",
      "Epoch [86/250], Training Loss: 842.0860841638885, Validation Loss: 18646.427734375\n",
      "Epoch [87/250], Training Loss: 737.4297793165401, Validation Loss: 18888.734375\n",
      "Epoch [88/250], Training Loss: 745.9358479003165, Validation Loss: 18679.50390625\n",
      "Epoch [89/250], Training Loss: 726.3474939626946, Validation Loss: 18757.015625\n",
      "Epoch [90/250], Training Loss: 723.0560182323993, Validation Loss: 18875.220703125\n",
      "Epoch [91/250], Training Loss: 811.6362054359678, Validation Loss: 18997.107421875\n",
      "Epoch [92/250], Training Loss: 760.113132596617, Validation Loss: 18965.109375\n",
      "Epoch [93/250], Training Loss: 1394.7669389874745, Validation Loss: 19054.958984375\n",
      "Epoch [94/250], Training Loss: 813.4341978334307, Validation Loss: 18608.48046875\n",
      "Epoch [95/250], Training Loss: 670.5839563077369, Validation Loss: 18642.185546875\n",
      "Epoch [96/250], Training Loss: 1318.4246587679552, Validation Loss: 18835.306640625\n",
      "Epoch [97/250], Training Loss: 769.4598980520067, Validation Loss: 18360.498046875\n",
      "Epoch [98/250], Training Loss: 858.8859963708704, Validation Loss: 18732.12890625\n",
      "Epoch [99/250], Training Loss: 1006.8172899102983, Validation Loss: 18827.53125\n",
      "Epoch [100/250], Training Loss: 720.9123247303379, Validation Loss: 18540.533203125\n",
      "Epoch [101/250], Training Loss: 841.1981666342595, Validation Loss: 18852.818359375\n",
      "Epoch [102/250], Training Loss: 714.1693565197116, Validation Loss: 18075.34375\n",
      "Epoch [103/250], Training Loss: 602.7637501109734, Validation Loss: 18605.697265625\n",
      "Epoch [104/250], Training Loss: 763.4855911216855, Validation Loss: 18849.736328125\n",
      "Epoch [105/250], Training Loss: 806.1737425831011, Validation Loss: 18705.2265625\n",
      "Epoch [106/250], Training Loss: 670.5366372090898, Validation Loss: 18833.515625\n",
      "Epoch [107/250], Training Loss: 1319.5763564339986, Validation Loss: 19340.1953125\n",
      "Epoch [108/250], Training Loss: 836.422573793214, Validation Loss: 18530.869140625\n",
      "Epoch [109/250], Training Loss: 1496.2977335615822, Validation Loss: 19044.443359375\n",
      "Epoch [110/250], Training Loss: 827.1833129157434, Validation Loss: 18725.62890625\n",
      "Epoch [111/250], Training Loss: 670.6703842866887, Validation Loss: 18753.486328125\n",
      "Epoch [112/250], Training Loss: 874.2682113705564, Validation Loss: 18559.572265625\n",
      "Epoch [113/250], Training Loss: 978.1070174864266, Validation Loss: 18809.51171875\n",
      "Epoch [114/250], Training Loss: 668.6069191065881, Validation Loss: 17682.9921875\n",
      "Epoch [115/250], Training Loss: 688.9430945135208, Validation Loss: 18844.1796875\n",
      "Epoch [116/250], Training Loss: 859.9730697392433, Validation Loss: 18570.6640625\n",
      "Epoch [117/250], Training Loss: 748.2204376941879, Validation Loss: 18971.326171875\n",
      "Epoch [118/250], Training Loss: 784.059811500367, Validation Loss: 18932.142578125\n",
      "Epoch [119/250], Training Loss: 806.1727663378239, Validation Loss: 18598.7421875\n",
      "Epoch [120/250], Training Loss: 564.1409719112174, Validation Loss: 18166.326171875\n",
      "Epoch [121/250], Training Loss: 653.6833859091653, Validation Loss: 18708.06640625\n",
      "Epoch [122/250], Training Loss: 595.4276900644446, Validation Loss: 18346.09765625\n",
      "Epoch [123/250], Training Loss: 729.0815416589613, Validation Loss: 19045.486328125\n",
      "Epoch [124/250], Training Loss: 641.8343579966785, Validation Loss: 18635.931640625\n",
      "Epoch [125/250], Training Loss: 1170.227254728784, Validation Loss: 19227.193359375\n",
      "Epoch [126/250], Training Loss: 747.853121032653, Validation Loss: 17659.91796875\n",
      "Epoch [127/250], Training Loss: 803.6381222835756, Validation Loss: 18755.384765625\n",
      "Epoch [128/250], Training Loss: 739.647761036158, Validation Loss: 18387.068359375\n",
      "Epoch [129/250], Training Loss: 849.6417528411713, Validation Loss: 19088.609375\n",
      "Epoch [130/250], Training Loss: 1011.6118976480625, Validation Loss: 18773.2109375\n",
      "Epoch [131/250], Training Loss: 1055.827125405028, Validation Loss: 19032.04296875\n",
      "Epoch [132/250], Training Loss: 690.9395803868799, Validation Loss: 18395.126953125\n",
      "Epoch [133/250], Training Loss: 866.9581090683655, Validation Loss: 19776.509765625\n",
      "Epoch [134/250], Training Loss: 1213.8201617251586, Validation Loss: 19193.1640625\n",
      "Epoch [135/250], Training Loss: 760.2950021395686, Validation Loss: 18528.859375\n",
      "Epoch [136/250], Training Loss: 805.0171578350648, Validation Loss: 18645.708984375\n",
      "Epoch [137/250], Training Loss: 740.5383718676887, Validation Loss: 18484.271484375\n",
      "Epoch [138/250], Training Loss: 1029.5611083030485, Validation Loss: 19006.666015625\n",
      "Epoch [139/250], Training Loss: 747.9493135080672, Validation Loss: 18347.708984375\n",
      "Epoch [140/250], Training Loss: 597.6079198349165, Validation Loss: 18911.482421875\n",
      "Epoch [141/250], Training Loss: 864.0687385766633, Validation Loss: 19743.435546875\n",
      "Epoch [142/250], Training Loss: 1224.3793192117703, Validation Loss: 19143.79296875\n",
      "Epoch [143/250], Training Loss: 791.3105999910279, Validation Loss: 18196.5\n",
      "Epoch [144/250], Training Loss: 813.0352345527252, Validation Loss: 18854.685546875\n",
      "Epoch [145/250], Training Loss: 1143.0681431349096, Validation Loss: 19144.681640625\n",
      "Epoch [146/250], Training Loss: 773.4329923392344, Validation Loss: 18277.724609375\n",
      "Epoch [147/250], Training Loss: 641.1851354381976, Validation Loss: 18928.107421875\n",
      "Epoch [148/250], Training Loss: 697.105492162503, Validation Loss: 19019.998046875\n",
      "Epoch [149/250], Training Loss: 833.0782249839444, Validation Loss: 19776.3125\n",
      "Epoch [150/250], Training Loss: 1105.9755345242215, Validation Loss: 19709.01953125\n",
      "Epoch [151/250], Training Loss: 1114.2335454280953, Validation Loss: 19084.28125\n",
      "Epoch [152/250], Training Loss: 680.6310501514731, Validation Loss: 18431.2734375\n",
      "Epoch [153/250], Training Loss: 727.3334316248032, Validation Loss: 19089.6953125\n",
      "Epoch [154/250], Training Loss: 729.7824534772249, Validation Loss: 19037.9453125\n",
      "Epoch [155/250], Training Loss: 897.4966245587153, Validation Loss: 19546.63671875\n",
      "Epoch [156/250], Training Loss: 812.1231274637563, Validation Loss: 19036.212890625\n",
      "Epoch [157/250], Training Loss: 919.759326448506, Validation Loss: 19762.0859375\n",
      "Epoch [158/250], Training Loss: 886.4068636562283, Validation Loss: 19472.90234375\n",
      "Epoch [159/250], Training Loss: 912.5481188173171, Validation Loss: 18945.626953125\n",
      "Epoch [160/250], Training Loss: 720.2397790978943, Validation Loss: 18558.091796875\n",
      "Epoch [161/250], Training Loss: 963.2963457911882, Validation Loss: 19770.787109375\n",
      "Epoch [162/250], Training Loss: 866.8504883774474, Validation Loss: 18616.326171875\n",
      "Epoch [163/250], Training Loss: 1201.002508364021, Validation Loss: 19408.09375\n",
      "Epoch [164/250], Training Loss: 872.8737051431946, Validation Loss: 18349.0390625\n",
      "Epoch [165/250], Training Loss: 882.5551219601731, Validation Loss: 18938.6875\n",
      "Epoch [166/250], Training Loss: 751.950060032993, Validation Loss: 18558.33984375\n",
      "Epoch [167/250], Training Loss: 650.4956150188659, Validation Loss: 18892.216796875\n",
      "Epoch [168/250], Training Loss: 1152.6165002335533, Validation Loss: 19261.76953125\n",
      "Epoch [169/250], Training Loss: 725.8261341713376, Validation Loss: 18245.267578125\n",
      "Epoch [170/250], Training Loss: 717.9599759416117, Validation Loss: 19540.060546875\n",
      "Epoch [171/250], Training Loss: 1235.369988440726, Validation Loss: 19348.919921875\n",
      "Epoch [172/250], Training Loss: 726.887255197408, Validation Loss: 18297.2890625\n",
      "Epoch [173/250], Training Loss: 935.0190391924651, Validation Loss: 19377.103515625\n",
      "Epoch [174/250], Training Loss: 752.9719105914004, Validation Loss: 18309.595703125\n",
      "Epoch [175/250], Training Loss: 886.4401259992793, Validation Loss: 19111.974609375\n",
      "Epoch [176/250], Training Loss: 644.3112291241772, Validation Loss: 18664.5703125\n",
      "Epoch [177/250], Training Loss: 440.04877241951897, Validation Loss: 18716.677734375\n",
      "Epoch [178/250], Training Loss: 1718.6290295326462, Validation Loss: 19780.224609375\n",
      "Epoch [179/250], Training Loss: 1006.4036908905191, Validation Loss: 18348.24609375\n",
      "Epoch [180/250], Training Loss: 1030.011956484241, Validation Loss: 19408.162109375\n",
      "Epoch [181/250], Training Loss: 993.7012930209377, Validation Loss: 18954.80859375\n",
      "Epoch [182/250], Training Loss: 843.8988586037755, Validation Loss: 19003.259765625\n",
      "Epoch [183/250], Training Loss: 912.0877365484845, Validation Loss: 19165.03515625\n",
      "Epoch [184/250], Training Loss: 936.4765678518258, Validation Loss: 19022.001953125\n",
      "Epoch [185/250], Training Loss: 774.5829779935469, Validation Loss: 19142.19921875\n",
      "Epoch [186/250], Training Loss: 833.456791720565, Validation Loss: 19751.623046875\n",
      "Epoch [187/250], Training Loss: 740.3477407477252, Validation Loss: 19648.44140625\n",
      "Epoch [188/250], Training Loss: 747.9241032932003, Validation Loss: 19391.982421875\n",
      "Epoch [189/250], Training Loss: 886.3284416119214, Validation Loss: 19818.01953125\n",
      "Epoch [190/250], Training Loss: 862.297000265304, Validation Loss: 19212.802734375\n",
      "Epoch [191/250], Training Loss: 1116.0465630495232, Validation Loss: 19653.390625\n",
      "Epoch [192/250], Training Loss: 809.1752345110765, Validation Loss: 19537.92578125\n",
      "Epoch [193/250], Training Loss: 592.736796069925, Validation Loss: 18109.107421875\n",
      "Epoch [194/250], Training Loss: 494.6257218011544, Validation Loss: 19885.513671875\n",
      "Epoch [195/250], Training Loss: 949.813153617262, Validation Loss: 19515.69921875\n",
      "Epoch [196/250], Training Loss: 725.4955836786314, Validation Loss: 19229.416015625\n",
      "Epoch [197/250], Training Loss: 527.770239474952, Validation Loss: 18691.701171875\n",
      "Epoch [198/250], Training Loss: 420.4248105472568, Validation Loss: 20231.947265625\n",
      "Epoch [199/250], Training Loss: 1624.4191898688678, Validation Loss: 20069.873046875\n",
      "Epoch [200/250], Training Loss: 945.2958923313723, Validation Loss: 18784.908203125\n",
      "Epoch [201/250], Training Loss: 962.8039461270212, Validation Loss: 19100.453125\n",
      "Epoch [202/250], Training Loss: 920.7421898178205, Validation Loss: 18777.95703125\n",
      "Epoch [203/250], Training Loss: 1172.1846803662177, Validation Loss: 19208.7265625\n",
      "Epoch [204/250], Training Loss: 949.2551001184662, Validation Loss: 18554.765625\n",
      "Epoch [205/250], Training Loss: 1026.1395761587262, Validation Loss: 19209.89453125\n",
      "Epoch [206/250], Training Loss: 1367.856434898142, Validation Loss: 19141.28125\n",
      "Epoch [207/250], Training Loss: 802.5858581586563, Validation Loss: 18340.8984375\n",
      "Epoch [208/250], Training Loss: 974.4244493120015, Validation Loss: 19170.060546875\n",
      "Epoch [209/250], Training Loss: 1137.58618094275, Validation Loss: 19104.083984375\n",
      "Epoch [210/250], Training Loss: 667.1223522130302, Validation Loss: 18390.45703125\n",
      "Epoch [211/250], Training Loss: 687.6573630395759, Validation Loss: 18707.345703125\n",
      "Epoch [212/250], Training Loss: 1056.7859620488744, Validation Loss: 19057.15234375\n",
      "Epoch [213/250], Training Loss: 1783.41636952171, Validation Loss: 19430.46484375\n",
      "Epoch [214/250], Training Loss: 844.4335301955123, Validation Loss: 18249.919921875\n",
      "Epoch [215/250], Training Loss: 1049.1502883113003, Validation Loss: 19090.603515625\n",
      "Epoch [216/250], Training Loss: 927.599811135757, Validation Loss: 18428.39453125\n",
      "Epoch [217/250], Training Loss: 997.7421056305328, Validation Loss: 19160.443359375\n",
      "Epoch [218/250], Training Loss: 1834.1446249384494, Validation Loss: 19528.720703125\n",
      "Epoch [219/250], Training Loss: 1022.4267760835502, Validation Loss: 18466.9765625\n",
      "Epoch [220/250], Training Loss: 874.7573771096021, Validation Loss: 18720.501953125\n",
      "Epoch [221/250], Training Loss: 1407.4015230468206, Validation Loss: 19323.611328125\n",
      "Epoch [222/250], Training Loss: 934.4162409836359, Validation Loss: 18440.431640625\n",
      "Epoch [223/250], Training Loss: 863.4303742875635, Validation Loss: 18974.3203125\n",
      "Epoch [224/250], Training Loss: 919.8561666084936, Validation Loss: 18732.865234375\n",
      "Epoch [225/250], Training Loss: 586.7469457744561, Validation Loss: 18053.05078125\n",
      "Epoch [226/250], Training Loss: 741.4030242114267, Validation Loss: 18970.39453125\n",
      "Epoch [227/250], Training Loss: 614.307736806304, Validation Loss: 18076.470703125\n",
      "Epoch [228/250], Training Loss: 931.0810160408901, Validation Loss: 19426.974609375\n",
      "Epoch [229/250], Training Loss: 696.5067985650364, Validation Loss: 18559.599609375\n",
      "Epoch [230/250], Training Loss: 929.2323131229064, Validation Loss: 19546.61328125\n",
      "Epoch [231/250], Training Loss: 1487.9435308004097, Validation Loss: 19295.521484375\n",
      "Epoch [232/250], Training Loss: 753.9555873201875, Validation Loss: 18293.0703125\n",
      "Epoch [233/250], Training Loss: 759.2465012522538, Validation Loss: 19040.951171875\n",
      "Epoch [234/250], Training Loss: 1099.5210249758177, Validation Loss: 19159.51171875\n",
      "Epoch [235/250], Training Loss: 658.3722510533362, Validation Loss: 18204.59765625\n",
      "Epoch [236/250], Training Loss: 809.6856860059429, Validation Loss: 19770.337890625\n",
      "Epoch [237/250], Training Loss: 1111.0566946466604, Validation Loss: 19760.0\n",
      "Epoch [238/250], Training Loss: 1012.2480194881309, Validation Loss: 19423.224609375\n",
      "Epoch [239/250], Training Loss: 1447.9400400983425, Validation Loss: 19265.560546875\n",
      "Epoch [240/250], Training Loss: 839.171156268022, Validation Loss: 18322.630859375\n",
      "Epoch [241/250], Training Loss: 846.1243970943812, Validation Loss: 18839.728515625\n",
      "Epoch [242/250], Training Loss: 1014.4185854754172, Validation Loss: 19216.455078125\n",
      "Epoch [243/250], Training Loss: 970.7575230141878, Validation Loss: 18908.771484375\n",
      "Epoch [244/250], Training Loss: 1126.1083402993938, Validation Loss: 19145.533203125\n",
      "Epoch [245/250], Training Loss: 833.8281180187677, Validation Loss: 18385.16796875\n",
      "Epoch [246/250], Training Loss: 956.2227103127287, Validation Loss: 19315.26171875\n",
      "Epoch [247/250], Training Loss: 694.3318593717163, Validation Loss: 17834.7890625\n",
      "Epoch [248/250], Training Loss: 870.6165082184087, Validation Loss: 19771.76171875\n",
      "Epoch [249/250], Training Loss: 1458.7134145134492, Validation Loss: 19201.26171875\n",
      "Epoch [250/250], Training Loss: 889.2604256118742, Validation Loss: 18409.77734375\n",
      "Test Loss: 17138.24609375\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.05, 20)\n",
      "Epoch [1/250], Training Loss: 4295.05273843884, Validation Loss: 14280.9541015625\n",
      "Epoch [2/250], Training Loss: 3323.9032178590237, Validation Loss: 8595.7353515625\n",
      "Epoch [3/250], Training Loss: 1701.166709386619, Validation Loss: 5877.15234375\n",
      "Epoch [4/250], Training Loss: 1463.554274462659, Validation Loss: 4735.82958984375\n",
      "Epoch [5/250], Training Loss: 751.3000897990452, Validation Loss: 4887.236328125\n",
      "Epoch [6/250], Training Loss: 419.3392357068325, Validation Loss: 4942.73095703125\n",
      "Epoch [7/250], Training Loss: 357.2225981901777, Validation Loss: 6820.5478515625\n",
      "Epoch [8/250], Training Loss: 364.11273054097387, Validation Loss: 5967.568359375\n",
      "Epoch [9/250], Training Loss: 208.66340788266376, Validation Loss: 8526.455078125\n",
      "Epoch [10/250], Training Loss: 160.409688766707, Validation Loss: 3223.1103515625\n",
      "Epoch [11/250], Training Loss: 149.07207099747575, Validation Loss: 4524.1533203125\n",
      "Epoch [12/250], Training Loss: 814.9004278376469, Validation Loss: 7486.28662109375\n",
      "Epoch [13/250], Training Loss: 299.2110917992918, Validation Loss: 7380.60107421875\n",
      "Epoch [14/250], Training Loss: 137.1369073679308, Validation Loss: 10178.8193359375\n",
      "Epoch [15/250], Training Loss: 129.57327634814726, Validation Loss: 7200.314453125\n",
      "Epoch [16/250], Training Loss: 240.0498665333987, Validation Loss: 5812.33544921875\n",
      "Epoch [17/250], Training Loss: 164.9897072527344, Validation Loss: 6824.20068359375\n",
      "Epoch [18/250], Training Loss: 269.9336428936986, Validation Loss: 4587.3251953125\n",
      "Epoch [19/250], Training Loss: 225.61548744850563, Validation Loss: 5324.67333984375\n",
      "Epoch [20/250], Training Loss: 112.52094473558134, Validation Loss: 4075.621337890625\n",
      "Epoch [21/250], Training Loss: 122.85343117921617, Validation Loss: 4861.4443359375\n",
      "Epoch [22/250], Training Loss: 131.71616082895903, Validation Loss: 4126.53662109375\n",
      "Epoch [23/250], Training Loss: 133.9477448160752, Validation Loss: 5256.083984375\n",
      "Epoch [24/250], Training Loss: 144.72754600370138, Validation Loss: 2611.310546875\n",
      "Epoch [25/250], Training Loss: 244.48553532200407, Validation Loss: 4525.7890625\n",
      "Epoch [26/250], Training Loss: 214.48221405756692, Validation Loss: 7684.9423828125\n",
      "Epoch [27/250], Training Loss: 106.71939685022826, Validation Loss: 5327.17822265625\n",
      "Epoch [28/250], Training Loss: 104.794110438951, Validation Loss: 6591.56201171875\n",
      "Epoch [29/250], Training Loss: 222.21679259523012, Validation Loss: 6035.01806640625\n",
      "Epoch [30/250], Training Loss: 141.4334140061849, Validation Loss: 7248.66748046875\n",
      "Epoch [31/250], Training Loss: 103.54052303595714, Validation Loss: 3887.238037109375\n",
      "Epoch [32/250], Training Loss: 114.09068823680109, Validation Loss: 7466.61669921875\n",
      "Epoch [33/250], Training Loss: 236.77218186760916, Validation Loss: 6097.48583984375\n",
      "Epoch [34/250], Training Loss: 160.61436208015635, Validation Loss: 7750.2529296875\n",
      "Epoch [35/250], Training Loss: 110.15644425192612, Validation Loss: 4168.45361328125\n",
      "Epoch [36/250], Training Loss: 122.65593709135072, Validation Loss: 6070.7373046875\n",
      "Epoch [37/250], Training Loss: 124.47913052060615, Validation Loss: 4153.4482421875\n",
      "Epoch [38/250], Training Loss: 131.98794967513462, Validation Loss: 5173.8193359375\n",
      "Epoch [39/250], Training Loss: 222.41379419293213, Validation Loss: 6031.03076171875\n",
      "Epoch [40/250], Training Loss: 128.71156068661136, Validation Loss: 6340.79296875\n",
      "Epoch [41/250], Training Loss: 106.06927043791323, Validation Loss: 4396.92138671875\n",
      "Epoch [42/250], Training Loss: 113.08702864251612, Validation Loss: 6811.2177734375\n",
      "Epoch [43/250], Training Loss: 112.60106920408012, Validation Loss: 4551.10302734375\n",
      "Epoch [44/250], Training Loss: 119.01158296221622, Validation Loss: 5135.88037109375\n",
      "Epoch [45/250], Training Loss: 125.61616653105956, Validation Loss: 3276.737548828125\n",
      "Epoch [46/250], Training Loss: 118.54704917922105, Validation Loss: 5691.73583984375\n",
      "Epoch [47/250], Training Loss: 119.12298125069123, Validation Loss: 4735.75048828125\n",
      "Epoch [48/250], Training Loss: 127.0206305916199, Validation Loss: 6573.78515625\n",
      "Epoch [49/250], Training Loss: 131.4939745392762, Validation Loss: 4899.69140625\n",
      "Epoch [50/250], Training Loss: 127.87047375817852, Validation Loss: 5564.90478515625\n",
      "Epoch [51/250], Training Loss: 133.43973424103712, Validation Loss: 3943.9560546875\n",
      "Epoch [52/250], Training Loss: 124.20764805723903, Validation Loss: 4605.77392578125\n",
      "Epoch [53/250], Training Loss: 133.47459842286483, Validation Loss: 3647.619873046875\n",
      "Epoch [54/250], Training Loss: 126.94318190078735, Validation Loss: 4999.4208984375\n",
      "Epoch [55/250], Training Loss: 132.59702775601212, Validation Loss: 3522.216796875\n",
      "Epoch [56/250], Training Loss: 115.75074381470155, Validation Loss: 6432.595703125\n",
      "Epoch [57/250], Training Loss: 285.06044729820786, Validation Loss: 5280.55908203125\n",
      "Epoch [58/250], Training Loss: 341.2251258314355, Validation Loss: 7767.015625\n",
      "Epoch [59/250], Training Loss: 145.15320692239237, Validation Loss: 6322.2802734375\n",
      "Epoch [60/250], Training Loss: 109.65815828821012, Validation Loss: 3915.548828125\n",
      "Epoch [61/250], Training Loss: 118.27421944988578, Validation Loss: 5506.83251953125\n",
      "Epoch [62/250], Training Loss: 109.44636397470411, Validation Loss: 4033.701416015625\n",
      "Epoch [63/250], Training Loss: 121.99716376484751, Validation Loss: 6125.98974609375\n",
      "Epoch [64/250], Training Loss: 117.07623518083345, Validation Loss: 5796.06103515625\n",
      "Epoch [65/250], Training Loss: 127.3427390789088, Validation Loss: 5944.21630859375\n",
      "Epoch [66/250], Training Loss: 131.98690979596478, Validation Loss: 7341.36181640625\n",
      "Epoch [67/250], Training Loss: 122.57871240274821, Validation Loss: 7378.974609375\n",
      "Epoch [68/250], Training Loss: 127.00948822848568, Validation Loss: 3996.318115234375\n",
      "Epoch [69/250], Training Loss: 135.09514489694115, Validation Loss: 5043.28173828125\n",
      "Epoch [70/250], Training Loss: 139.97389692263778, Validation Loss: 4609.16943359375\n",
      "Epoch [71/250], Training Loss: 142.3037558856403, Validation Loss: 7439.517578125\n",
      "Epoch [72/250], Training Loss: 126.92634997776297, Validation Loss: 4870.95263671875\n",
      "Epoch [73/250], Training Loss: 129.70726553067936, Validation Loss: 3814.961181640625\n",
      "Epoch [74/250], Training Loss: 127.68217884486295, Validation Loss: 5072.4072265625\n",
      "Epoch [75/250], Training Loss: 133.4247994036791, Validation Loss: 4724.3662109375\n",
      "Epoch [76/250], Training Loss: 258.11720040701675, Validation Loss: 6800.8876953125\n",
      "Epoch [77/250], Training Loss: 272.8889445753923, Validation Loss: 6670.412109375\n",
      "Epoch [78/250], Training Loss: 122.79158016948173, Validation Loss: 7439.673828125\n",
      "Epoch [79/250], Training Loss: 100.91083746392776, Validation Loss: 5081.52978515625\n",
      "Epoch [80/250], Training Loss: 115.0197288964757, Validation Loss: 7436.490234375\n",
      "Epoch [81/250], Training Loss: 132.73370358190698, Validation Loss: 5686.48291015625\n",
      "Epoch [82/250], Training Loss: 123.50676272509016, Validation Loss: 7501.50732421875\n",
      "Epoch [83/250], Training Loss: 128.38035226292615, Validation Loss: 3945.74560546875\n",
      "Epoch [84/250], Training Loss: 186.92424930848406, Validation Loss: 7024.8427734375\n",
      "Epoch [85/250], Training Loss: 135.09065734436228, Validation Loss: 8604.5126953125\n",
      "Epoch [86/250], Training Loss: 255.82978929241384, Validation Loss: 6636.16455078125\n",
      "Epoch [87/250], Training Loss: 162.70383168184185, Validation Loss: 6460.705078125\n",
      "Epoch [88/250], Training Loss: 106.57140240025335, Validation Loss: 6894.72607421875\n",
      "Epoch [89/250], Training Loss: 115.34589209948034, Validation Loss: 6308.6728515625\n",
      "Epoch [90/250], Training Loss: 205.67381478656483, Validation Loss: 7659.6650390625\n",
      "Epoch [91/250], Training Loss: 145.8347925106674, Validation Loss: 9394.2919921875\n",
      "Epoch [92/250], Training Loss: 110.26602140435052, Validation Loss: 4785.51904296875\n",
      "Epoch [93/250], Training Loss: 126.8991517616622, Validation Loss: 5288.77685546875\n",
      "Epoch [94/250], Training Loss: 111.75805135499654, Validation Loss: 4758.64501953125\n",
      "Epoch [95/250], Training Loss: 122.99325744820537, Validation Loss: 6172.849609375\n",
      "Epoch [96/250], Training Loss: 150.45458835530968, Validation Loss: 6265.8916015625\n",
      "Epoch [97/250], Training Loss: 134.8642062500286, Validation Loss: 6120.68896484375\n",
      "Epoch [98/250], Training Loss: 144.74797584259414, Validation Loss: 4626.87451171875\n",
      "Epoch [99/250], Training Loss: 119.04089634457891, Validation Loss: 9601.830078125\n",
      "Epoch [100/250], Training Loss: 293.87257520480296, Validation Loss: 6688.00244140625\n",
      "Epoch [101/250], Training Loss: 192.58518817987965, Validation Loss: 6162.716796875\n",
      "Epoch [102/250], Training Loss: 134.72202011942503, Validation Loss: 6557.40234375\n",
      "Epoch [103/250], Training Loss: 127.1237435269309, Validation Loss: 5797.01318359375\n",
      "Epoch [104/250], Training Loss: 323.5404567068859, Validation Loss: 7101.70166015625\n",
      "Epoch [105/250], Training Loss: 362.48468873545295, Validation Loss: 6666.677734375\n",
      "Epoch [106/250], Training Loss: 138.20937873010962, Validation Loss: 8571.6259765625\n",
      "Epoch [107/250], Training Loss: 106.77803936911154, Validation Loss: 5329.662109375\n",
      "Epoch [108/250], Training Loss: 231.1058592096249, Validation Loss: 9875.197265625\n",
      "Epoch [109/250], Training Loss: 163.5744694208479, Validation Loss: 8182.0029296875\n",
      "Epoch [110/250], Training Loss: 119.20124346497916, Validation Loss: 6171.6318359375\n",
      "Epoch [111/250], Training Loss: 184.70512337858815, Validation Loss: 9432.5302734375\n",
      "Epoch [112/250], Training Loss: 147.30247849409182, Validation Loss: 8012.42333984375\n",
      "Epoch [113/250], Training Loss: 327.63365651465733, Validation Loss: 7311.978515625\n",
      "Epoch [114/250], Training Loss: 206.91316632935585, Validation Loss: 6166.14306640625\n",
      "Epoch [115/250], Training Loss: 131.16140915792167, Validation Loss: 9411.5498046875\n",
      "Epoch [116/250], Training Loss: 162.02202100310038, Validation Loss: 11302.78125\n",
      "Epoch [117/250], Training Loss: 116.07131929455294, Validation Loss: 6223.26171875\n",
      "Epoch [118/250], Training Loss: 102.66343290444786, Validation Loss: 4051.256103515625\n",
      "Epoch [119/250], Training Loss: 213.22593028272263, Validation Loss: 9133.5244140625\n",
      "Epoch [120/250], Training Loss: 146.29633629449728, Validation Loss: 8053.3671875\n",
      "Epoch [121/250], Training Loss: 104.49675766137725, Validation Loss: 10153.298828125\n",
      "Epoch [122/250], Training Loss: 123.85570114894391, Validation Loss: 6556.43701171875\n",
      "Epoch [123/250], Training Loss: 126.04807085989329, Validation Loss: 6207.0908203125\n",
      "Epoch [124/250], Training Loss: 250.84011618731344, Validation Loss: 9339.9013671875\n",
      "Epoch [125/250], Training Loss: 183.85641114908464, Validation Loss: 8207.30859375\n",
      "Epoch [126/250], Training Loss: 230.67708706959257, Validation Loss: 8396.4462890625\n",
      "Epoch [127/250], Training Loss: 276.3161254630327, Validation Loss: 9955.4853515625\n",
      "Epoch [128/250], Training Loss: 110.58065467035352, Validation Loss: 5917.7412109375\n",
      "Epoch [129/250], Training Loss: 208.87143196879262, Validation Loss: 5463.82373046875\n",
      "Epoch [130/250], Training Loss: 155.5271777147459, Validation Loss: 6905.4384765625\n",
      "Epoch [131/250], Training Loss: 114.87410705102438, Validation Loss: 5986.27880859375\n",
      "Epoch [132/250], Training Loss: 199.83117731429684, Validation Loss: 7084.33154296875\n",
      "Epoch [133/250], Training Loss: 160.01571373724795, Validation Loss: 5377.9921875\n",
      "Epoch [134/250], Training Loss: 266.15814058177534, Validation Loss: 7302.4609375\n",
      "Epoch [135/250], Training Loss: 139.35562892051598, Validation Loss: 3698.022216796875\n",
      "Epoch [136/250], Training Loss: 97.80138821027204, Validation Loss: 4227.32421875\n",
      "Epoch [137/250], Training Loss: 103.97707069247934, Validation Loss: 3224.97412109375\n",
      "Epoch [138/250], Training Loss: 107.15753900798225, Validation Loss: 5594.982421875\n",
      "Epoch [139/250], Training Loss: 101.59830360956165, Validation Loss: 3894.36572265625\n",
      "Epoch [140/250], Training Loss: 110.58271736876715, Validation Loss: 7091.3916015625\n",
      "Epoch [141/250], Training Loss: 122.45155433760851, Validation Loss: 5191.96533203125\n",
      "Epoch [142/250], Training Loss: 128.15339204929722, Validation Loss: 4953.52099609375\n",
      "Epoch [143/250], Training Loss: 123.01988899832602, Validation Loss: 4065.66845703125\n",
      "Epoch [144/250], Training Loss: 112.84431071583985, Validation Loss: 6852.08740234375\n",
      "Epoch [145/250], Training Loss: 122.61893216867874, Validation Loss: 5359.72509765625\n",
      "Epoch [146/250], Training Loss: 141.5032809342252, Validation Loss: 4880.33935546875\n",
      "Epoch [147/250], Training Loss: 250.7505997997949, Validation Loss: 7845.8525390625\n",
      "Epoch [148/250], Training Loss: 440.25334567492126, Validation Loss: 7540.76806640625\n",
      "Epoch [149/250], Training Loss: 229.5776094241478, Validation Loss: 5535.04443359375\n",
      "Epoch [150/250], Training Loss: 100.54338809522712, Validation Loss: 4210.86083984375\n",
      "Epoch [151/250], Training Loss: 143.16359030996546, Validation Loss: 7521.3779296875\n",
      "Epoch [152/250], Training Loss: 140.30827758348363, Validation Loss: 6579.38720703125\n",
      "Epoch [153/250], Training Loss: 139.8532264923506, Validation Loss: 7639.68505859375\n",
      "Epoch [154/250], Training Loss: 129.53077884086005, Validation Loss: 8971.0703125\n",
      "Epoch [155/250], Training Loss: 256.569879273265, Validation Loss: 6764.5107421875\n",
      "Epoch [156/250], Training Loss: 175.9473714645939, Validation Loss: 5330.2685546875\n",
      "Epoch [157/250], Training Loss: 221.00592664055785, Validation Loss: 9009.12109375\n",
      "Epoch [158/250], Training Loss: 138.73760201909084, Validation Loss: 6321.083984375\n",
      "Epoch [159/250], Training Loss: 103.04940794200724, Validation Loss: 7003.1845703125\n",
      "Epoch [160/250], Training Loss: 212.64338169936875, Validation Loss: 7910.33642578125\n",
      "Epoch [161/250], Training Loss: 131.68267175837173, Validation Loss: 9165.59375\n",
      "Epoch [162/250], Training Loss: 105.22199237531362, Validation Loss: 5992.24169921875\n",
      "Epoch [163/250], Training Loss: 116.45109217538281, Validation Loss: 8003.63525390625\n",
      "Epoch [164/250], Training Loss: 384.1851215008411, Validation Loss: 8423.9755859375\n",
      "Epoch [165/250], Training Loss: 268.31469849022636, Validation Loss: 4938.189453125\n",
      "Epoch [166/250], Training Loss: 107.45805178185209, Validation Loss: 3278.77197265625\n",
      "Epoch [167/250], Training Loss: 111.49899228732974, Validation Loss: 3918.4970703125\n",
      "Epoch [168/250], Training Loss: 205.6946770282526, Validation Loss: 8066.33935546875\n",
      "Epoch [169/250], Training Loss: 136.5732592025735, Validation Loss: 6323.8876953125\n",
      "Epoch [170/250], Training Loss: 99.95732408563143, Validation Loss: 6943.837890625\n",
      "Epoch [171/250], Training Loss: 109.73593881377695, Validation Loss: 6945.3701171875\n",
      "Epoch [172/250], Training Loss: 346.5099964921925, Validation Loss: 7636.1240234375\n",
      "Epoch [173/250], Training Loss: 271.3534192894106, Validation Loss: 5727.560546875\n",
      "Epoch [174/250], Training Loss: 132.92702102400924, Validation Loss: 5587.546875\n",
      "Epoch [175/250], Training Loss: 131.31205654150355, Validation Loss: 6693.748046875\n",
      "Epoch [176/250], Training Loss: 118.03599108429452, Validation Loss: 4725.986328125\n",
      "Epoch [177/250], Training Loss: 242.55978477575812, Validation Loss: 6925.4931640625\n",
      "Epoch [178/250], Training Loss: 132.09759570595148, Validation Loss: 7129.03076171875\n",
      "Epoch [179/250], Training Loss: 106.52411800804853, Validation Loss: 8475.8759765625\n",
      "Epoch [180/250], Training Loss: 164.93489747694105, Validation Loss: 7495.279296875\n",
      "Epoch [181/250], Training Loss: 138.58730541433204, Validation Loss: 6270.681640625\n",
      "Epoch [182/250], Training Loss: 280.01029517022425, Validation Loss: 8859.2607421875\n",
      "Epoch [183/250], Training Loss: 212.23078060375303, Validation Loss: 7854.88037109375\n",
      "Epoch [184/250], Training Loss: 117.00893298389886, Validation Loss: 6433.9345703125\n",
      "Epoch [185/250], Training Loss: 140.9167823805655, Validation Loss: 7456.87451171875\n",
      "Epoch [186/250], Training Loss: 133.4794574482674, Validation Loss: 8523.5380859375\n",
      "Epoch [187/250], Training Loss: 206.53393688658946, Validation Loss: 7424.630859375\n",
      "Epoch [188/250], Training Loss: 315.73177962573993, Validation Loss: 7756.26025390625\n",
      "Epoch [189/250], Training Loss: 132.09837388792826, Validation Loss: 7011.7236328125\n",
      "Epoch [190/250], Training Loss: 269.222060571152, Validation Loss: 8633.326171875\n",
      "Epoch [191/250], Training Loss: 281.30842065289016, Validation Loss: 8995.2939453125\n",
      "Epoch [192/250], Training Loss: 130.95382373455544, Validation Loss: 8324.7802734375\n",
      "Epoch [193/250], Training Loss: 117.14036210466129, Validation Loss: 7983.7412109375\n",
      "Epoch [194/250], Training Loss: 130.33738634682712, Validation Loss: 8874.607421875\n",
      "Epoch [195/250], Training Loss: 299.4986351505465, Validation Loss: 10963.240234375\n",
      "Epoch [196/250], Training Loss: 323.19569867108254, Validation Loss: 7215.72265625\n",
      "Epoch [197/250], Training Loss: 144.71395191389024, Validation Loss: 6678.33642578125\n",
      "Epoch [198/250], Training Loss: 107.37174904195138, Validation Loss: 8642.90234375\n",
      "Epoch [199/250], Training Loss: 215.95347280567594, Validation Loss: 11025.5498046875\n",
      "Epoch [200/250], Training Loss: 164.28626756212893, Validation Loss: 10147.1474609375\n",
      "Epoch [201/250], Training Loss: 109.95215012583273, Validation Loss: 11868.4306640625\n",
      "Epoch [202/250], Training Loss: 135.51446131135253, Validation Loss: 9455.5791015625\n",
      "Epoch [203/250], Training Loss: 134.29190117153766, Validation Loss: 8639.91796875\n",
      "Epoch [204/250], Training Loss: 299.5267862353566, Validation Loss: 9686.3291015625\n",
      "Epoch [205/250], Training Loss: 11166.740079060686, Validation Loss: 9248.4541015625\n",
      "Epoch [206/250], Training Loss: 1187.535734489214, Validation Loss: 5784.984375\n",
      "Epoch [207/250], Training Loss: 11532.874761608105, Validation Loss: 12285.8662109375\n",
      "Epoch [208/250], Training Loss: 1701.6413928202846, Validation Loss: 4555.92529296875\n",
      "Epoch [209/250], Training Loss: 194.48927062153365, Validation Loss: 5223.8515625\n",
      "Epoch [210/250], Training Loss: 187.57350626408626, Validation Loss: 4503.4990234375\n",
      "Epoch [211/250], Training Loss: 105.20596698087803, Validation Loss: 4632.8759765625\n",
      "Epoch [212/250], Training Loss: 172.0663762230055, Validation Loss: 5141.76953125\n",
      "Epoch [213/250], Training Loss: 226.35654451375865, Validation Loss: 6752.9677734375\n",
      "Epoch [214/250], Training Loss: 124.75426526105436, Validation Loss: 4797.45458984375\n",
      "Epoch [215/250], Training Loss: 11338.589696384166, Validation Loss: 6956.05810546875\n",
      "Epoch [216/250], Training Loss: 12420.002363762174, Validation Loss: 21566.220703125\n",
      "Epoch [217/250], Training Loss: 2341.6137967121635, Validation Loss: 7189.962890625\n",
      "Epoch [218/250], Training Loss: 255.14027432846362, Validation Loss: 8212.763671875\n",
      "Epoch [219/250], Training Loss: 294.52916050710587, Validation Loss: 9914.9033203125\n",
      "Epoch [220/250], Training Loss: 127.6063811580764, Validation Loss: 10342.3291015625\n",
      "Epoch [221/250], Training Loss: 170.68838757428, Validation Loss: 7850.4521484375\n",
      "Epoch [222/250], Training Loss: 160.92882486586853, Validation Loss: 8025.44384765625\n",
      "Epoch [223/250], Training Loss: 121.81163364971663, Validation Loss: 9485.6171875\n",
      "Epoch [224/250], Training Loss: 143.26934883663768, Validation Loss: 9430.5439453125\n",
      "Epoch [225/250], Training Loss: 165.40169257047242, Validation Loss: 5733.388671875\n",
      "Epoch [226/250], Training Loss: 120.44950134330547, Validation Loss: 5466.84912109375\n",
      "Epoch [227/250], Training Loss: 110.89454462098888, Validation Loss: 7351.60205078125\n",
      "Epoch [228/250], Training Loss: 114.47504187894185, Validation Loss: 6795.23095703125\n",
      "Epoch [229/250], Training Loss: 122.98748519021443, Validation Loss: 7219.78564453125\n",
      "Epoch [230/250], Training Loss: 120.19615319008508, Validation Loss: 9112.470703125\n",
      "Epoch [231/250], Training Loss: 154.04699951407935, Validation Loss: 7037.7255859375\n",
      "Epoch [232/250], Training Loss: 181.0627194987718, Validation Loss: 6185.75732421875\n",
      "Epoch [233/250], Training Loss: 195.5751002643922, Validation Loss: 8314.197265625\n",
      "Epoch [234/250], Training Loss: 186.5818453552665, Validation Loss: 11043.916015625\n",
      "Epoch [235/250], Training Loss: 127.67504590044004, Validation Loss: 9096.8759765625\n",
      "Epoch [236/250], Training Loss: 109.44816243895824, Validation Loss: 8568.9990234375\n",
      "Epoch [237/250], Training Loss: 120.93991005122425, Validation Loss: 9273.6611328125\n",
      "Epoch [238/250], Training Loss: 120.4819206902834, Validation Loss: 9984.224609375\n",
      "Epoch [239/250], Training Loss: 132.67834338101108, Validation Loss: 9173.296875\n",
      "Epoch [240/250], Training Loss: 134.47671620585987, Validation Loss: 8719.3583984375\n",
      "Epoch [241/250], Training Loss: 136.42073180470666, Validation Loss: 8846.6875\n",
      "Epoch [242/250], Training Loss: 136.22632149471738, Validation Loss: 9299.71484375\n",
      "Epoch [243/250], Training Loss: 143.53845371892987, Validation Loss: 6211.322265625\n",
      "Epoch [244/250], Training Loss: 227.0322995408667, Validation Loss: 7088.1640625\n",
      "Epoch [245/250], Training Loss: 131.30879576377782, Validation Loss: 6139.4697265625\n",
      "Epoch [246/250], Training Loss: 11726.466867070743, Validation Loss: 10287.58203125\n",
      "Epoch [247/250], Training Loss: 1722.2619889923294, Validation Loss: 4077.047119140625\n",
      "Epoch [248/250], Training Loss: 230.21208805495823, Validation Loss: 4543.2890625\n",
      "Epoch [249/250], Training Loss: 197.46901301173074, Validation Loss: 4927.2294921875\n",
      "Epoch [250/250], Training Loss: 119.37006244061236, Validation Loss: 6161.087890625\n",
      "Test Loss: 6100.11962890625\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.05, 25)\n",
      "Epoch [1/250], Training Loss: 3849.714663294931, Validation Loss: 12877.861328125\n",
      "Epoch [2/250], Training Loss: 3917.3834233506896, Validation Loss: 8457.994140625\n",
      "Epoch [3/250], Training Loss: 1844.2014404438257, Validation Loss: 10408.00390625\n",
      "Epoch [4/250], Training Loss: 943.9494564935425, Validation Loss: 9054.3671875\n",
      "Epoch [5/250], Training Loss: 556.4130499061279, Validation Loss: 8649.5556640625\n",
      "Epoch [6/250], Training Loss: 396.23541545514047, Validation Loss: 7850.884765625\n",
      "Epoch [7/250], Training Loss: 301.2211402339429, Validation Loss: 8449.857421875\n",
      "Epoch [8/250], Training Loss: 259.51283115300674, Validation Loss: 8895.328125\n",
      "Epoch [9/250], Training Loss: 208.65200058193824, Validation Loss: 7961.837890625\n",
      "Epoch [10/250], Training Loss: 194.6701159300864, Validation Loss: 7424.720703125\n",
      "Epoch [11/250], Training Loss: 1040.2664337051679, Validation Loss: 6887.89501953125\n",
      "Epoch [12/250], Training Loss: 7336.924464938161, Validation Loss: 30838.701171875\n",
      "Epoch [13/250], Training Loss: 3292.455598142877, Validation Loss: 43046.3125\n",
      "Epoch [14/250], Training Loss: 1195.7252144722138, Validation Loss: 26249.001953125\n",
      "Epoch [15/250], Training Loss: 1345.3228318642305, Validation Loss: 21632.673828125\n",
      "Epoch [16/250], Training Loss: 1274.190928527892, Validation Loss: 19499.841796875\n",
      "Epoch [17/250], Training Loss: 759.308436287459, Validation Loss: 20068.38671875\n",
      "Epoch [18/250], Training Loss: 822.874037758055, Validation Loss: 19301.625\n",
      "Epoch [19/250], Training Loss: 719.0047730183355, Validation Loss: 19066.671875\n",
      "Epoch [20/250], Training Loss: 708.1841538616195, Validation Loss: 18875.86328125\n",
      "Epoch [21/250], Training Loss: 691.1514104043007, Validation Loss: 18635.267578125\n",
      "Epoch [22/250], Training Loss: 607.5043576015121, Validation Loss: 18411.90234375\n",
      "Epoch [23/250], Training Loss: 599.7260274117041, Validation Loss: 18676.052734375\n",
      "Epoch [24/250], Training Loss: 673.3478185554646, Validation Loss: 18450.669921875\n",
      "Epoch [25/250], Training Loss: 653.1029460289775, Validation Loss: 18323.54296875\n",
      "Epoch [26/250], Training Loss: 584.3826195091577, Validation Loss: 18085.93359375\n",
      "Epoch [27/250], Training Loss: 569.9222460188494, Validation Loss: 18281.046875\n",
      "Epoch [28/250], Training Loss: 607.3379973607277, Validation Loss: 18190.28125\n",
      "Epoch [29/250], Training Loss: 578.851846471825, Validation Loss: 18229.8671875\n",
      "Epoch [30/250], Training Loss: 510.6483826007628, Validation Loss: 18330.349609375\n",
      "Epoch [31/250], Training Loss: 526.9391735985394, Validation Loss: 19341.068359375\n",
      "Epoch [32/250], Training Loss: 871.0960311613339, Validation Loss: 18563.388671875\n",
      "Epoch [33/250], Training Loss: 686.7420594363316, Validation Loss: 18269.19921875\n",
      "Epoch [34/250], Training Loss: 1858.0273342727728, Validation Loss: 19143.513671875\n",
      "Epoch [35/250], Training Loss: 1182.528876900975, Validation Loss: 18512.076171875\n",
      "Epoch [36/250], Training Loss: 659.0583573901388, Validation Loss: 18217.802734375\n",
      "Epoch [37/250], Training Loss: 568.895639827649, Validation Loss: 18721.759765625\n",
      "Epoch [38/250], Training Loss: 583.2928436526738, Validation Loss: 19441.140625\n",
      "Epoch [39/250], Training Loss: 687.3699889857395, Validation Loss: 18497.427734375\n",
      "Epoch [40/250], Training Loss: 561.9645480175745, Validation Loss: 18571.5625\n",
      "Epoch [41/250], Training Loss: 556.9818907965272, Validation Loss: 19993.45703125\n",
      "Epoch [42/250], Training Loss: 671.2284759633889, Validation Loss: 19038.208984375\n",
      "Epoch [43/250], Training Loss: 698.1879764276829, Validation Loss: 19194.931640625\n",
      "Epoch [44/250], Training Loss: 873.8481442997928, Validation Loss: 19105.474609375\n",
      "Epoch [45/250], Training Loss: 754.4789922952584, Validation Loss: 18418.208984375\n",
      "Epoch [46/250], Training Loss: 559.4910696930208, Validation Loss: 18291.951171875\n",
      "Epoch [47/250], Training Loss: 692.1164862302348, Validation Loss: 18354.1171875\n",
      "Epoch [48/250], Training Loss: 585.8243810312603, Validation Loss: 18592.591796875\n",
      "Epoch [49/250], Training Loss: 567.6483888425072, Validation Loss: 18190.873046875\n",
      "Epoch [50/250], Training Loss: 678.7419303905715, Validation Loss: 18417.91015625\n",
      "Epoch [51/250], Training Loss: 596.5958329298562, Validation Loss: 18634.4765625\n",
      "Epoch [52/250], Training Loss: 828.2394629823095, Validation Loss: 18355.96875\n",
      "Epoch [53/250], Training Loss: 628.4528555336647, Validation Loss: 18377.197265625\n",
      "Epoch [54/250], Training Loss: 624.5136315954414, Validation Loss: 18147.736328125\n",
      "Epoch [55/250], Training Loss: 505.68362863719074, Validation Loss: 18069.404296875\n",
      "Epoch [56/250], Training Loss: 541.7122787367466, Validation Loss: 18159.33203125\n",
      "Epoch [57/250], Training Loss: 526.9938361676067, Validation Loss: 18132.90234375\n",
      "Epoch [58/250], Training Loss: 478.4875253458987, Validation Loss: 17973.62890625\n",
      "Epoch [59/250], Training Loss: 568.5025892594072, Validation Loss: 18389.341796875\n",
      "Epoch [60/250], Training Loss: 702.3246225194082, Validation Loss: 18574.1015625\n",
      "Epoch [61/250], Training Loss: 952.5822052003896, Validation Loss: 18541.08984375\n",
      "Epoch [62/250], Training Loss: 639.5949114446971, Validation Loss: 18186.01953125\n",
      "Epoch [63/250], Training Loss: 551.1698214783925, Validation Loss: 18216.283203125\n",
      "Epoch [64/250], Training Loss: 630.0803882621789, Validation Loss: 18221.595703125\n",
      "Epoch [65/250], Training Loss: 580.1540130302797, Validation Loss: 18251.802734375\n",
      "Epoch [66/250], Training Loss: 792.0597336834729, Validation Loss: 18419.095703125\n",
      "Epoch [67/250], Training Loss: 622.4094451769414, Validation Loss: 18306.5078125\n",
      "Epoch [68/250], Training Loss: 552.2706491474124, Validation Loss: 17741.14453125\n",
      "Epoch [69/250], Training Loss: 546.6856796435754, Validation Loss: 18415.4609375\n",
      "Epoch [70/250], Training Loss: 523.3421811117885, Validation Loss: 18320.65234375\n",
      "Epoch [71/250], Training Loss: 647.5219216597931, Validation Loss: 18505.236328125\n",
      "Epoch [72/250], Training Loss: 742.6329100715747, Validation Loss: 18411.80078125\n",
      "Epoch [73/250], Training Loss: 672.525716050301, Validation Loss: 18274.57421875\n",
      "Epoch [74/250], Training Loss: 704.4544799032867, Validation Loss: 18287.080078125\n",
      "Epoch [75/250], Training Loss: 669.0865271824125, Validation Loss: 18275.19921875\n",
      "Epoch [76/250], Training Loss: 744.4368085659013, Validation Loss: 18285.9453125\n",
      "Epoch [77/250], Training Loss: 581.018628889994, Validation Loss: 18323.68359375\n",
      "Epoch [78/250], Training Loss: 660.1729375265697, Validation Loss: 18400.662109375\n",
      "Epoch [79/250], Training Loss: 1160.4550316551781, Validation Loss: 18721.767578125\n",
      "Epoch [80/250], Training Loss: 651.2624172120697, Validation Loss: 18133.35546875\n",
      "Epoch [81/250], Training Loss: 506.8432349617644, Validation Loss: 18130.3203125\n",
      "Epoch [82/250], Training Loss: 674.1449518777627, Validation Loss: 18390.98046875\n",
      "Epoch [83/250], Training Loss: 1044.6979813989533, Validation Loss: 18707.0546875\n",
      "Epoch [84/250], Training Loss: 657.4784024328665, Validation Loss: 18114.517578125\n",
      "Epoch [85/250], Training Loss: 633.4754159014998, Validation Loss: 18295.91015625\n",
      "Epoch [86/250], Training Loss: 686.305272152075, Validation Loss: 18296.919921875\n",
      "Epoch [87/250], Training Loss: 601.4257610929608, Validation Loss: 18205.7578125\n",
      "Epoch [88/250], Training Loss: 560.3838223259404, Validation Loss: 17901.087890625\n",
      "Epoch [89/250], Training Loss: 501.422961959055, Validation Loss: 18248.248046875\n",
      "Epoch [90/250], Training Loss: 646.3546010847757, Validation Loss: 18230.845703125\n",
      "Epoch [91/250], Training Loss: 632.821321341353, Validation Loss: 18205.560546875\n",
      "Epoch [92/250], Training Loss: 525.5064924913956, Validation Loss: 17622.083984375\n",
      "Epoch [93/250], Training Loss: 505.4650335905099, Validation Loss: 18429.708984375\n",
      "Epoch [94/250], Training Loss: 569.0849326726786, Validation Loss: 18509.62109375\n",
      "Epoch [95/250], Training Loss: 500.5645455148571, Validation Loss: 18099.8203125\n",
      "Epoch [96/250], Training Loss: 584.156720282113, Validation Loss: 18492.9296875\n",
      "Epoch [97/250], Training Loss: 773.8289602396752, Validation Loss: 18602.73828125\n",
      "Epoch [98/250], Training Loss: 647.9282823914912, Validation Loss: 18132.576171875\n",
      "Epoch [99/250], Training Loss: 715.1225251636745, Validation Loss: 18369.16796875\n",
      "Epoch [100/250], Training Loss: 565.4736784768543, Validation Loss: 18000.658203125\n",
      "Epoch [101/250], Training Loss: 455.44898687928736, Validation Loss: 18140.24609375\n",
      "Epoch [102/250], Training Loss: 517.6114143099084, Validation Loss: 18253.142578125\n",
      "Epoch [103/250], Training Loss: 600.3945800701821, Validation Loss: 18378.685546875\n",
      "Epoch [104/250], Training Loss: 567.1064066852753, Validation Loss: 18379.337890625\n",
      "Epoch [105/250], Training Loss: 624.1173565019984, Validation Loss: 18422.466796875\n",
      "Epoch [106/250], Training Loss: 696.4959062172783, Validation Loss: 18336.138671875\n",
      "Epoch [107/250], Training Loss: 635.9232034242697, Validation Loss: 18330.2578125\n",
      "Epoch [108/250], Training Loss: 548.9507253618697, Validation Loss: 17800.0078125\n",
      "Epoch [109/250], Training Loss: 656.6982437800995, Validation Loss: 18538.783203125\n",
      "Epoch [110/250], Training Loss: 672.5326695091799, Validation Loss: 18242.505859375\n",
      "Epoch [111/250], Training Loss: 998.0301908094398, Validation Loss: 18643.70703125\n",
      "Epoch [112/250], Training Loss: 645.8656922026473, Validation Loss: 18202.5703125\n",
      "Epoch [113/250], Training Loss: 519.9454674750829, Validation Loss: 18164.3671875\n",
      "Epoch [114/250], Training Loss: 500.942961470709, Validation Loss: 18111.525390625\n",
      "Epoch [115/250], Training Loss: 472.8082532417866, Validation Loss: 18240.92578125\n",
      "Epoch [116/250], Training Loss: 510.15168408990695, Validation Loss: 18521.1015625\n",
      "Epoch [117/250], Training Loss: 558.4255560904134, Validation Loss: 18474.20703125\n",
      "Epoch [118/250], Training Loss: 554.3595842061968, Validation Loss: 18686.109375\n",
      "Epoch [119/250], Training Loss: 553.6396447440783, Validation Loss: 18101.080078125\n",
      "Epoch [120/250], Training Loss: 840.4260266021055, Validation Loss: 19043.533203125\n",
      "Epoch [121/250], Training Loss: 658.0589640768471, Validation Loss: 18316.01953125\n",
      "Epoch [122/250], Training Loss: 480.9879129713117, Validation Loss: 18023.072265625\n",
      "Epoch [123/250], Training Loss: 844.1252876722842, Validation Loss: 18739.5\n",
      "Epoch [124/250], Training Loss: 709.4344667332218, Validation Loss: 18198.49609375\n",
      "Epoch [125/250], Training Loss: 519.1270655414535, Validation Loss: 17617.126953125\n",
      "Epoch [126/250], Training Loss: 517.5984176578283, Validation Loss: 18720.384765625\n",
      "Epoch [127/250], Training Loss: 815.1327748808933, Validation Loss: 18607.92578125\n",
      "Epoch [128/250], Training Loss: 546.5876410737496, Validation Loss: 18174.05859375\n",
      "Epoch [129/250], Training Loss: 956.0030564549195, Validation Loss: 18942.97265625\n",
      "Epoch [130/250], Training Loss: 750.7909653637265, Validation Loss: 18235.53515625\n",
      "Epoch [131/250], Training Loss: 556.7889423855407, Validation Loss: 18270.255859375\n",
      "Epoch [132/250], Training Loss: 1241.9861691561118, Validation Loss: 18865.904296875\n",
      "Epoch [133/250], Training Loss: 698.4164889003938, Validation Loss: 18024.109375\n",
      "Epoch [134/250], Training Loss: 482.18085483461135, Validation Loss: 17929.142578125\n",
      "Epoch [135/250], Training Loss: 733.0392383639985, Validation Loss: 18471.19140625\n",
      "Epoch [136/250], Training Loss: 552.6765366354637, Validation Loss: 18048.92578125\n",
      "Epoch [137/250], Training Loss: 510.4326499726567, Validation Loss: 18254.29296875\n",
      "Epoch [138/250], Training Loss: 1302.5967643686608, Validation Loss: 18939.00390625\n",
      "Epoch [139/250], Training Loss: 708.8183119844581, Validation Loss: 18018.828125\n",
      "Epoch [140/250], Training Loss: 463.95578768045357, Validation Loss: 18025.806640625\n",
      "Epoch [141/250], Training Loss: 437.48489258483454, Validation Loss: 18310.751953125\n",
      "Epoch [142/250], Training Loss: 466.545178556921, Validation Loss: 18319.728515625\n",
      "Epoch [143/250], Training Loss: 447.09191356325573, Validation Loss: 18131.26953125\n",
      "Epoch [144/250], Training Loss: 659.3811854290685, Validation Loss: 18528.4921875\n",
      "Epoch [145/250], Training Loss: 609.1181400824511, Validation Loss: 18577.779296875\n",
      "Epoch [146/250], Training Loss: 767.4896649780911, Validation Loss: 18511.755859375\n",
      "Epoch [147/250], Training Loss: 630.7119269534741, Validation Loss: 18209.72265625\n",
      "Epoch [148/250], Training Loss: 1001.9832882881424, Validation Loss: 18777.0234375\n",
      "Epoch [149/250], Training Loss: 663.2181030703977, Validation Loss: 18160.572265625\n",
      "Epoch [150/250], Training Loss: 509.4860755163788, Validation Loss: 18054.708984375\n",
      "Epoch [151/250], Training Loss: 438.6287055744752, Validation Loss: 18130.70703125\n",
      "Epoch [152/250], Training Loss: 497.9020569595931, Validation Loss: 18112.93359375\n",
      "Epoch [153/250], Training Loss: 451.78254828187016, Validation Loss: 18228.18359375\n",
      "Epoch [154/250], Training Loss: 484.0786008106077, Validation Loss: 18221.267578125\n",
      "Epoch [155/250], Training Loss: 505.6584354229879, Validation Loss: 18224.62109375\n",
      "Epoch [156/250], Training Loss: 520.1663877100543, Validation Loss: 18253.751953125\n",
      "Epoch [157/250], Training Loss: 538.3353716977749, Validation Loss: 18279.828125\n",
      "Epoch [158/250], Training Loss: 484.06821339997464, Validation Loss: 18172.009765625\n",
      "Epoch [159/250], Training Loss: 680.7587988407191, Validation Loss: 18458.240234375\n",
      "Epoch [160/250], Training Loss: 566.7201981473103, Validation Loss: 18387.966796875\n",
      "Epoch [161/250], Training Loss: 610.5816944540279, Validation Loss: 18493.2421875\n",
      "Epoch [162/250], Training Loss: 654.0032019072377, Validation Loss: 18368.41015625\n",
      "Epoch [163/250], Training Loss: 581.5020216041002, Validation Loss: 18248.0\n",
      "Epoch [164/250], Training Loss: 514.5064755321259, Validation Loss: 18068.994140625\n",
      "Epoch [165/250], Training Loss: 680.0793405779967, Validation Loss: 18495.791015625\n",
      "Epoch [166/250], Training Loss: 759.9002283496345, Validation Loss: 18496.439453125\n",
      "Epoch [167/250], Training Loss: 636.3446122662651, Validation Loss: 18123.095703125\n",
      "Epoch [168/250], Training Loss: 520.0692916825564, Validation Loss: 18070.619140625\n",
      "Epoch [169/250], Training Loss: 556.1807026345034, Validation Loss: 18232.96484375\n",
      "Epoch [170/250], Training Loss: 628.3336621602087, Validation Loss: 18233.435546875\n",
      "Epoch [171/250], Training Loss: 521.6690460876425, Validation Loss: 17729.7265625\n",
      "Epoch [172/250], Training Loss: 609.6556237417145, Validation Loss: 18475.576171875\n",
      "Epoch [173/250], Training Loss: 560.8315812000985, Validation Loss: 17762.421875\n",
      "Epoch [174/250], Training Loss: 674.7806588391551, Validation Loss: 18641.189453125\n",
      "Epoch [175/250], Training Loss: 597.1445455079668, Validation Loss: 18294.724609375\n",
      "Epoch [176/250], Training Loss: 723.4514890449777, Validation Loss: 18538.806640625\n",
      "Epoch [177/250], Training Loss: 596.5652708241387, Validation Loss: 18210.765625\n",
      "Epoch [178/250], Training Loss: 668.263455643941, Validation Loss: 18553.00390625\n",
      "Epoch [179/250], Training Loss: 638.5584937057541, Validation Loss: 18255.81640625\n",
      "Epoch [180/250], Training Loss: 1007.3850162711009, Validation Loss: 18927.06640625\n",
      "Epoch [181/250], Training Loss: 707.2057428446484, Validation Loss: 18286.1171875\n",
      "Epoch [182/250], Training Loss: 698.4357057500874, Validation Loss: 18509.62109375\n",
      "Epoch [183/250], Training Loss: 633.698657659729, Validation Loss: 18245.5625\n",
      "Epoch [184/250], Training Loss: 521.469643361409, Validation Loss: 17709.896484375\n",
      "Epoch [185/250], Training Loss: 625.479495909265, Validation Loss: 18601.328125\n",
      "Epoch [186/250], Training Loss: 649.2194754831946, Validation Loss: 18234.615234375\n",
      "Epoch [187/250], Training Loss: 610.0991931009178, Validation Loss: 18301.1015625\n",
      "Epoch [188/250], Training Loss: 520.1573700300946, Validation Loss: 17875.62109375\n",
      "Epoch [189/250], Training Loss: 758.9788428293081, Validation Loss: 18701.80078125\n",
      "Epoch [190/250], Training Loss: 584.3969028750894, Validation Loss: 17872.921875\n",
      "Epoch [191/250], Training Loss: 746.165638203269, Validation Loss: 18677.3125\n",
      "Epoch [192/250], Training Loss: 797.4299258145705, Validation Loss: 18460.09765625\n",
      "Epoch [193/250], Training Loss: 543.0959286347576, Validation Loss: 17855.416015625\n",
      "Epoch [194/250], Training Loss: 650.0215679898772, Validation Loss: 18646.294921875\n",
      "Epoch [195/250], Training Loss: 602.6010656459723, Validation Loss: 18239.578125\n",
      "Epoch [196/250], Training Loss: 622.8246596674858, Validation Loss: 18346.515625\n",
      "Epoch [197/250], Training Loss: 671.9207863135697, Validation Loss: 18433.796875\n",
      "Epoch [198/250], Training Loss: 861.9140957910449, Validation Loss: 18976.65234375\n",
      "Epoch [199/250], Training Loss: 656.4107628172391, Validation Loss: 18244.11328125\n",
      "Epoch [200/250], Training Loss: 583.6010889412621, Validation Loss: 18275.001953125\n",
      "Epoch [201/250], Training Loss: 628.1122129931953, Validation Loss: 18615.9296875\n",
      "Epoch [202/250], Training Loss: 666.0094970239588, Validation Loss: 18705.849609375\n",
      "Epoch [203/250], Training Loss: 640.7935875083508, Validation Loss: 17973.861328125\n",
      "Epoch [204/250], Training Loss: 488.99051533562255, Validation Loss: 18581.29296875\n",
      "Epoch [205/250], Training Loss: 547.1921571998208, Validation Loss: 18247.294921875\n",
      "Epoch [206/250], Training Loss: 507.5629696159586, Validation Loss: 18173.623046875\n",
      "Epoch [207/250], Training Loss: 858.6383662546162, Validation Loss: 18670.169921875\n",
      "Epoch [208/250], Training Loss: 627.2648730629716, Validation Loss: 18163.330078125\n",
      "Epoch [209/250], Training Loss: 993.7587172116511, Validation Loss: 18776.091796875\n",
      "Epoch [210/250], Training Loss: 674.9627810574755, Validation Loss: 18201.751953125\n",
      "Epoch [211/250], Training Loss: 510.0507517821318, Validation Loss: 18085.876953125\n",
      "Epoch [212/250], Training Loss: 487.75311019658045, Validation Loss: 18210.095703125\n",
      "Epoch [213/250], Training Loss: 500.2392602752834, Validation Loss: 18426.796875\n",
      "Epoch [214/250], Training Loss: 481.2487212632668, Validation Loss: 17959.96875\n",
      "Epoch [215/250], Training Loss: 600.0702262189529, Validation Loss: 18527.728515625\n",
      "Epoch [216/250], Training Loss: 633.819607721444, Validation Loss: 18448.111328125\n",
      "Epoch [217/250], Training Loss: 547.7529980208001, Validation Loss: 18818.875\n",
      "Epoch [218/250], Training Loss: 506.4595590868944, Validation Loss: 18091.4765625\n",
      "Epoch [219/250], Training Loss: 551.364862186126, Validation Loss: 18636.72265625\n",
      "Epoch [220/250], Training Loss: 732.071465000914, Validation Loss: 18535.3359375\n",
      "Epoch [221/250], Training Loss: 586.6256339440139, Validation Loss: 18287.578125\n",
      "Epoch [222/250], Training Loss: 660.7202948049012, Validation Loss: 18425.96484375\n",
      "Epoch [223/250], Training Loss: 598.2574210447971, Validation Loss: 18204.623046875\n",
      "Epoch [224/250], Training Loss: 916.5243453549332, Validation Loss: 18647.18359375\n",
      "Epoch [225/250], Training Loss: 621.2533772195668, Validation Loss: 18097.52734375\n",
      "Epoch [226/250], Training Loss: 470.13217007131954, Validation Loss: 17749.275390625\n",
      "Epoch [227/250], Training Loss: 472.75079127756374, Validation Loss: 18295.9296875\n",
      "Epoch [228/250], Training Loss: 495.7946427862231, Validation Loss: 18272.3125\n",
      "Epoch [229/250], Training Loss: 683.2931803565488, Validation Loss: 18534.44140625\n",
      "Epoch [230/250], Training Loss: 655.1711207888653, Validation Loss: 18487.5625\n",
      "Epoch [231/250], Training Loss: 761.0472509899149, Validation Loss: 18551.5703125\n",
      "Epoch [232/250], Training Loss: 683.8427829467435, Validation Loss: 18401.91015625\n",
      "Epoch [233/250], Training Loss: 871.5909636196777, Validation Loss: 18730.037109375\n",
      "Epoch [234/250], Training Loss: 670.8207544625867, Validation Loss: 18287.416015625\n",
      "Epoch [235/250], Training Loss: 510.51513827062126, Validation Loss: 17797.8984375\n",
      "Epoch [236/250], Training Loss: 721.8749984465069, Validation Loss: 18688.3125\n",
      "Epoch [237/250], Training Loss: 818.6030036326508, Validation Loss: 18534.13671875\n",
      "Epoch [238/250], Training Loss: 644.4592055140544, Validation Loss: 18352.3671875\n",
      "Epoch [239/250], Training Loss: 536.7759472204208, Validation Loss: 18066.935546875\n",
      "Epoch [240/250], Training Loss: 593.1076108498268, Validation Loss: 18444.83984375\n",
      "Epoch [241/250], Training Loss: 665.1423855833673, Validation Loss: 18234.654296875\n",
      "Epoch [242/250], Training Loss: 1020.1028432630176, Validation Loss: 18881.423828125\n",
      "Epoch [243/250], Training Loss: 707.5259151298626, Validation Loss: 18228.53125\n",
      "Epoch [244/250], Training Loss: 599.2226667223721, Validation Loss: 18202.79296875\n",
      "Epoch [245/250], Training Loss: 506.4748559162009, Validation Loss: 18115.349609375\n",
      "Epoch [246/250], Training Loss: 1197.4516019013763, Validation Loss: 18662.556640625\n",
      "Epoch [247/250], Training Loss: 625.9760634228153, Validation Loss: 18009.650390625\n",
      "Epoch [248/250], Training Loss: 829.0036412028313, Validation Loss: 18642.7265625\n",
      "Epoch [249/250], Training Loss: 582.7638467670696, Validation Loss: 17976.67578125\n",
      "Epoch [250/250], Training Loss: 443.6347813544123, Validation Loss: 18220.701171875\n",
      "Test Loss: 16704.36328125\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.05, 30)\n",
      "Epoch [1/250], Training Loss: 4772.169991802198, Validation Loss: 14686.4130859375\n",
      "Epoch [2/250], Training Loss: 3576.5124316705023, Validation Loss: 9259.5927734375\n",
      "Epoch [3/250], Training Loss: 1783.8148682182878, Validation Loss: 6072.54345703125\n",
      "Epoch [4/250], Training Loss: 954.2004657990075, Validation Loss: 5733.78564453125\n",
      "Epoch [5/250], Training Loss: 465.21909157184945, Validation Loss: 8179.078125\n",
      "Epoch [6/250], Training Loss: 310.3090205534954, Validation Loss: 9201.9296875\n",
      "Epoch [7/250], Training Loss: 352.91978463274893, Validation Loss: 10262.537109375\n",
      "Epoch [8/250], Training Loss: 189.4549669262596, Validation Loss: 10735.244140625\n",
      "Epoch [9/250], Training Loss: 173.61977109120718, Validation Loss: 9936.423828125\n",
      "Epoch [10/250], Training Loss: 164.6570843466256, Validation Loss: 8833.662109375\n",
      "Epoch [11/250], Training Loss: 172.88565199412443, Validation Loss: 6933.74853515625\n",
      "Epoch [12/250], Training Loss: 415.7315863442358, Validation Loss: 8387.17578125\n",
      "Epoch [13/250], Training Loss: 230.34786898884923, Validation Loss: 6537.08154296875\n",
      "Epoch [14/250], Training Loss: 360.2314382082779, Validation Loss: 6569.90283203125\n",
      "Epoch [15/250], Training Loss: 331.5621027756137, Validation Loss: 7414.388671875\n",
      "Epoch [16/250], Training Loss: 201.06026119423456, Validation Loss: 8759.2763671875\n",
      "Epoch [17/250], Training Loss: 112.44249930582806, Validation Loss: 11437.52734375\n",
      "Epoch [18/250], Training Loss: 366.23690799113393, Validation Loss: 8174.97705078125\n",
      "Epoch [19/250], Training Loss: 500.7878997622312, Validation Loss: 6866.2841796875\n",
      "Epoch [20/250], Training Loss: 400.7807170273831, Validation Loss: 6808.36083984375\n",
      "Epoch [21/250], Training Loss: 440.2753441304739, Validation Loss: 8803.912109375\n",
      "Epoch [22/250], Training Loss: 115.02431912224759, Validation Loss: 6382.0927734375\n",
      "Epoch [23/250], Training Loss: 99.93564691599666, Validation Loss: 6981.41748046875\n",
      "Epoch [24/250], Training Loss: 107.45365835263112, Validation Loss: 7541.71044921875\n",
      "Epoch [25/250], Training Loss: 214.43704513530722, Validation Loss: 10197.009765625\n",
      "Epoch [26/250], Training Loss: 139.6693852370082, Validation Loss: 10829.6298828125\n",
      "Epoch [27/250], Training Loss: 129.53213585878387, Validation Loss: 9687.5009765625\n",
      "Epoch [28/250], Training Loss: 284.21093900709224, Validation Loss: 9363.1533203125\n",
      "Epoch [29/250], Training Loss: 226.1908627854675, Validation Loss: 8866.7294921875\n",
      "Epoch [30/250], Training Loss: 128.58562426796317, Validation Loss: 9813.9833984375\n",
      "Epoch [31/250], Training Loss: 240.15797586730423, Validation Loss: 7744.78271484375\n",
      "Epoch [32/250], Training Loss: 252.40382172026472, Validation Loss: 7698.771484375\n",
      "Epoch [33/250], Training Loss: 431.2115206040053, Validation Loss: 9063.267578125\n",
      "Epoch [34/250], Training Loss: 251.39438644527283, Validation Loss: 6971.90087890625\n",
      "Epoch [35/250], Training Loss: 121.2631007827779, Validation Loss: 5717.75634765625\n",
      "Epoch [36/250], Training Loss: 244.88631149078273, Validation Loss: 9383.486328125\n",
      "Epoch [37/250], Training Loss: 240.2968310297518, Validation Loss: 10301.109375\n",
      "Epoch [38/250], Training Loss: 155.05555263131828, Validation Loss: 10271.44140625\n",
      "Epoch [39/250], Training Loss: 764.8751386680251, Validation Loss: 9337.84375\n",
      "Epoch [40/250], Training Loss: 535.3104352125022, Validation Loss: 4620.86572265625\n",
      "Epoch [41/250], Training Loss: 104.89832409116109, Validation Loss: 8281.6083984375\n",
      "Epoch [42/250], Training Loss: 123.07128861326328, Validation Loss: 8093.09130859375\n",
      "Epoch [43/250], Training Loss: 128.9596229708618, Validation Loss: 7581.11669921875\n",
      "Epoch [44/250], Training Loss: 133.4641628969817, Validation Loss: 6867.89892578125\n",
      "Epoch [45/250], Training Loss: 125.57521847581474, Validation Loss: 7520.5888671875\n",
      "Epoch [46/250], Training Loss: 123.70939475924592, Validation Loss: 6846.67822265625\n",
      "Epoch [47/250], Training Loss: 129.901263812958, Validation Loss: 5548.5107421875\n",
      "Epoch [48/250], Training Loss: 148.0748140789377, Validation Loss: 6170.7890625\n",
      "Epoch [49/250], Training Loss: 144.5707315826775, Validation Loss: 7520.966796875\n",
      "Epoch [50/250], Training Loss: 121.24083127867696, Validation Loss: 5703.63232421875\n",
      "Epoch [51/250], Training Loss: 295.780629833265, Validation Loss: 6324.62255859375\n",
      "Epoch [52/250], Training Loss: 709.6591224667058, Validation Loss: 8982.212890625\n",
      "Epoch [53/250], Training Loss: 266.7893584639737, Validation Loss: 6367.63671875\n",
      "Epoch [54/250], Training Loss: 114.2276086630215, Validation Loss: 7468.03759765625\n",
      "Epoch [55/250], Training Loss: 132.0731854844109, Validation Loss: 6402.10693359375\n",
      "Epoch [56/250], Training Loss: 217.5304997413515, Validation Loss: 9441.8466796875\n",
      "Epoch [57/250], Training Loss: 143.75384785622623, Validation Loss: 10865.634765625\n",
      "Epoch [58/250], Training Loss: 143.683091604135, Validation Loss: 9536.865234375\n",
      "Epoch [59/250], Training Loss: 959.2642530264619, Validation Loss: 10859.650390625\n",
      "Epoch [60/250], Training Loss: 455.86575234637473, Validation Loss: 5880.57763671875\n",
      "Epoch [61/250], Training Loss: 101.35643026166834, Validation Loss: 6081.09228515625\n",
      "Epoch [62/250], Training Loss: 227.75969612418615, Validation Loss: 8066.568359375\n",
      "Epoch [63/250], Training Loss: 133.31773010000833, Validation Loss: 8175.8095703125\n",
      "Epoch [64/250], Training Loss: 110.01659764969449, Validation Loss: 7754.27734375\n",
      "Epoch [65/250], Training Loss: 114.75901652041011, Validation Loss: 10463.486328125\n",
      "Epoch [66/250], Training Loss: 118.63692266646508, Validation Loss: 7990.9970703125\n",
      "Epoch [67/250], Training Loss: 274.1790444628158, Validation Loss: 9356.9931640625\n",
      "Epoch [68/250], Training Loss: 665.0852282843802, Validation Loss: 10280.478515625\n",
      "Epoch [69/250], Training Loss: 322.989998245205, Validation Loss: 7180.90185546875\n",
      "Epoch [70/250], Training Loss: 117.57441571969643, Validation Loss: 9429.345703125\n",
      "Epoch [71/250], Training Loss: 126.4816709343408, Validation Loss: 8339.49609375\n",
      "Epoch [72/250], Training Loss: 138.12998752327587, Validation Loss: 6086.91064453125\n",
      "Epoch [73/250], Training Loss: 152.77398206467365, Validation Loss: 7363.89501953125\n",
      "Epoch [74/250], Training Loss: 133.34699505548622, Validation Loss: 9204.9892578125\n",
      "Epoch [75/250], Training Loss: 117.70296355725242, Validation Loss: 7817.8525390625\n",
      "Epoch [76/250], Training Loss: 141.56134540047947, Validation Loss: 8228.4267578125\n",
      "Epoch [77/250], Training Loss: 132.3324142275389, Validation Loss: 6917.05517578125\n",
      "Epoch [78/250], Training Loss: 312.6869856759915, Validation Loss: 7010.6435546875\n",
      "Epoch [79/250], Training Loss: 213.058554736069, Validation Loss: 5692.82958984375\n",
      "Epoch [80/250], Training Loss: 107.96795054197342, Validation Loss: 7129.658203125\n",
      "Epoch [81/250], Training Loss: 122.345672159774, Validation Loss: 8651.404296875\n",
      "Epoch [82/250], Training Loss: 131.66013795042107, Validation Loss: 8330.8203125\n",
      "Epoch [83/250], Training Loss: 164.25353368800072, Validation Loss: 7441.4921875\n",
      "Epoch [84/250], Training Loss: 159.32648463690006, Validation Loss: 7083.03515625\n",
      "Epoch [85/250], Training Loss: 124.92378709735732, Validation Loss: 7828.482421875\n",
      "Epoch [86/250], Training Loss: 585.6836495781279, Validation Loss: 9813.443359375\n",
      "Epoch [87/250], Training Loss: 366.2446540400451, Validation Loss: 7256.623046875\n",
      "Epoch [88/250], Training Loss: 133.23291113096818, Validation Loss: 8589.4619140625\n",
      "Epoch [89/250], Training Loss: 117.84278012956787, Validation Loss: 6030.37841796875\n",
      "Epoch [90/250], Training Loss: 131.57801003710205, Validation Loss: 7339.3759765625\n",
      "Epoch [91/250], Training Loss: 123.44943451451718, Validation Loss: 7184.85791015625\n",
      "Epoch [92/250], Training Loss: 312.8857275301832, Validation Loss: 9041.2001953125\n",
      "Epoch [93/250], Training Loss: 454.24227803753325, Validation Loss: 8957.30859375\n",
      "Epoch [94/250], Training Loss: 221.98484517628785, Validation Loss: 7398.78955078125\n",
      "Epoch [95/250], Training Loss: 116.75645802778098, Validation Loss: 5732.6572265625\n",
      "Epoch [96/250], Training Loss: 164.09969758720374, Validation Loss: 7238.6298828125\n",
      "Epoch [97/250], Training Loss: 140.1333424225396, Validation Loss: 8594.4736328125\n",
      "Epoch [98/250], Training Loss: 131.35070558385158, Validation Loss: 9864.2275390625\n",
      "Epoch [99/250], Training Loss: 341.63814099737283, Validation Loss: 9201.947265625\n",
      "Epoch [100/250], Training Loss: 168.73444956086198, Validation Loss: 7531.140625\n",
      "Epoch [101/250], Training Loss: 105.41930130955004, Validation Loss: 9154.796875\n",
      "Epoch [102/250], Training Loss: 121.6390210595853, Validation Loss: 7670.208984375\n",
      "Epoch [103/250], Training Loss: 135.52561339383607, Validation Loss: 9012.84765625\n",
      "Epoch [104/250], Training Loss: 127.07693067910613, Validation Loss: 6871.4853515625\n",
      "Epoch [105/250], Training Loss: 146.59636715865406, Validation Loss: 8181.51318359375\n",
      "Epoch [106/250], Training Loss: 122.31841687227747, Validation Loss: 5961.07373046875\n",
      "Epoch [107/250], Training Loss: 135.67050354479483, Validation Loss: 6811.82763671875\n",
      "Epoch [108/250], Training Loss: 118.11518112685938, Validation Loss: 7996.5654296875\n",
      "Epoch [109/250], Training Loss: 140.10537973454245, Validation Loss: 7765.45263671875\n",
      "Epoch [110/250], Training Loss: 391.094801202661, Validation Loss: 9928.501953125\n",
      "Epoch [111/250], Training Loss: 824.5244275740514, Validation Loss: 10040.7431640625\n",
      "Epoch [112/250], Training Loss: 375.7462745065833, Validation Loss: 6569.00732421875\n",
      "Epoch [113/250], Training Loss: 114.14483319802406, Validation Loss: 8171.18017578125\n",
      "Epoch [114/250], Training Loss: 140.7229115470726, Validation Loss: 6646.8798828125\n",
      "Epoch [115/250], Training Loss: 150.1109531652487, Validation Loss: 7780.1064453125\n",
      "Epoch [116/250], Training Loss: 116.13706448477048, Validation Loss: 8205.0693359375\n",
      "Epoch [117/250], Training Loss: 140.23342915251703, Validation Loss: 6962.46630859375\n",
      "Epoch [118/250], Training Loss: 133.04214396092993, Validation Loss: 7749.40576171875\n",
      "Epoch [119/250], Training Loss: 157.59827066240624, Validation Loss: 10087.7578125\n",
      "Epoch [120/250], Training Loss: 141.28210329822713, Validation Loss: 6423.4345703125\n",
      "Epoch [121/250], Training Loss: 120.54391690839283, Validation Loss: 7829.802734375\n",
      "Epoch [122/250], Training Loss: 125.0392369383048, Validation Loss: 9883.669921875\n",
      "Epoch [123/250], Training Loss: 129.87940569188444, Validation Loss: 8048.28515625\n",
      "Epoch [124/250], Training Loss: 134.22656075232865, Validation Loss: 8521.666015625\n",
      "Epoch [125/250], Training Loss: 128.00596423973417, Validation Loss: 8284.779296875\n",
      "Epoch [126/250], Training Loss: 155.22134067834966, Validation Loss: 6720.0029296875\n",
      "Epoch [127/250], Training Loss: 392.0257264955112, Validation Loss: 6851.16748046875\n",
      "Epoch [128/250], Training Loss: 278.76742402898157, Validation Loss: 7506.77587890625\n",
      "Epoch [129/250], Training Loss: 280.31323900415896, Validation Loss: 8027.880859375\n",
      "Epoch [130/250], Training Loss: 123.03457682453384, Validation Loss: 6808.58935546875\n",
      "Epoch [131/250], Training Loss: 101.65774169122409, Validation Loss: 9351.5380859375\n",
      "Epoch [132/250], Training Loss: 111.8386339967188, Validation Loss: 10893.4306640625\n",
      "Epoch [133/250], Training Loss: 190.84552729819876, Validation Loss: 10317.36328125\n",
      "Epoch [134/250], Training Loss: 381.6236339827419, Validation Loss: 8559.9853515625\n",
      "Epoch [135/250], Training Loss: 315.02956941322213, Validation Loss: 6969.7158203125\n",
      "Epoch [136/250], Training Loss: 151.1992132534418, Validation Loss: 7043.7705078125\n",
      "Epoch [137/250], Training Loss: 106.30811798618771, Validation Loss: 9464.146484375\n",
      "Epoch [138/250], Training Loss: 126.42264611857168, Validation Loss: 10726.087890625\n",
      "Epoch [139/250], Training Loss: 125.58782902056373, Validation Loss: 8006.0712890625\n",
      "Epoch [140/250], Training Loss: 134.90017404081394, Validation Loss: 7242.62060546875\n",
      "Epoch [141/250], Training Loss: 488.3443660530894, Validation Loss: 8737.361328125\n",
      "Epoch [142/250], Training Loss: 212.6927161691158, Validation Loss: 7468.7021484375\n",
      "Epoch [143/250], Training Loss: 419.99328761523, Validation Loss: 8791.546875\n",
      "Epoch [144/250], Training Loss: 291.57628782210844, Validation Loss: 7598.7021484375\n",
      "Epoch [145/250], Training Loss: 116.61909018169224, Validation Loss: 8876.4365234375\n",
      "Epoch [146/250], Training Loss: 144.73268488182111, Validation Loss: 7575.5703125\n",
      "Epoch [147/250], Training Loss: 153.4314255316069, Validation Loss: 8595.1767578125\n",
      "Epoch [148/250], Training Loss: 118.99445707195918, Validation Loss: 6592.24169921875\n",
      "Epoch [149/250], Training Loss: 138.49960110832282, Validation Loss: 6613.53857421875\n",
      "Epoch [150/250], Training Loss: 113.9451914857102, Validation Loss: 6640.19287109375\n",
      "Epoch [151/250], Training Loss: 125.67810819673565, Validation Loss: 7872.52587890625\n",
      "Epoch [152/250], Training Loss: 127.11082498100083, Validation Loss: 8994.9580078125\n",
      "Epoch [153/250], Training Loss: 119.20233254349951, Validation Loss: 7606.234375\n",
      "Epoch [154/250], Training Loss: 137.60779041582927, Validation Loss: 8336.166015625\n",
      "Epoch [155/250], Training Loss: 241.55901679846608, Validation Loss: 9068.626953125\n",
      "Epoch [156/250], Training Loss: 202.81660563440727, Validation Loss: 6550.01416015625\n",
      "Epoch [157/250], Training Loss: 119.75637929956547, Validation Loss: 7974.1142578125\n",
      "Epoch [158/250], Training Loss: 123.96592746597223, Validation Loss: 8304.7548828125\n",
      "Epoch [159/250], Training Loss: 134.50074367338036, Validation Loss: 8889.9111328125\n",
      "Epoch [160/250], Training Loss: 136.41827192948102, Validation Loss: 7034.1884765625\n",
      "Epoch [161/250], Training Loss: 455.30037617591114, Validation Loss: 8629.798828125\n",
      "Epoch [162/250], Training Loss: 306.94844656601674, Validation Loss: 6995.98486328125\n",
      "Epoch [163/250], Training Loss: 339.46042822032746, Validation Loss: 10032.876953125\n",
      "Epoch [164/250], Training Loss: 162.63555608436167, Validation Loss: 8600.1025390625\n",
      "Epoch [165/250], Training Loss: 110.00968042593415, Validation Loss: 6766.72216796875\n",
      "Epoch [166/250], Training Loss: 130.7996708011848, Validation Loss: 7211.43212890625\n",
      "Epoch [167/250], Training Loss: 116.5432677403516, Validation Loss: 8349.537109375\n",
      "Epoch [168/250], Training Loss: 129.999039590284, Validation Loss: 5368.76904296875\n",
      "Epoch [169/250], Training Loss: 133.78940527812588, Validation Loss: 5949.77001953125\n",
      "Epoch [170/250], Training Loss: 119.10559700887475, Validation Loss: 8156.017578125\n",
      "Epoch [171/250], Training Loss: 120.30039639620439, Validation Loss: 7059.8876953125\n",
      "Epoch [172/250], Training Loss: 347.58752317261025, Validation Loss: 8794.53515625\n",
      "Epoch [173/250], Training Loss: 259.48885427481594, Validation Loss: 7208.0234375\n",
      "Epoch [174/250], Training Loss: 118.51681577584463, Validation Loss: 6757.12841796875\n",
      "Epoch [175/250], Training Loss: 146.49170599082302, Validation Loss: 6902.5712890625\n",
      "Epoch [176/250], Training Loss: 123.36591950354273, Validation Loss: 7603.04443359375\n",
      "Epoch [177/250], Training Loss: 124.63555248369549, Validation Loss: 8164.75341796875\n",
      "Epoch [178/250], Training Loss: 236.47198403360616, Validation Loss: 7313.2451171875\n",
      "Epoch [179/250], Training Loss: 159.9569893246379, Validation Loss: 4824.22216796875\n",
      "Epoch [180/250], Training Loss: 234.51878196976253, Validation Loss: 6771.43798828125\n",
      "Epoch [181/250], Training Loss: 161.3802970054449, Validation Loss: 6965.07470703125\n",
      "Epoch [182/250], Training Loss: 106.94310318607843, Validation Loss: 7317.2275390625\n",
      "Epoch [183/250], Training Loss: 120.72044353753883, Validation Loss: 10430.9580078125\n",
      "Epoch [184/250], Training Loss: 125.55019909126416, Validation Loss: 9446.376953125\n",
      "Epoch [185/250], Training Loss: 137.83701082910505, Validation Loss: 7152.44287109375\n",
      "Epoch [186/250], Training Loss: 138.23610162359154, Validation Loss: 7631.029296875\n",
      "Epoch [187/250], Training Loss: 129.72700198695955, Validation Loss: 6584.62890625\n",
      "Epoch [188/250], Training Loss: 137.0797097690352, Validation Loss: 5861.912109375\n",
      "Epoch [189/250], Training Loss: 128.11125791325134, Validation Loss: 6307.82470703125\n",
      "Epoch [190/250], Training Loss: 122.87062055343705, Validation Loss: 7080.41845703125\n",
      "Epoch [191/250], Training Loss: 134.86634789779052, Validation Loss: 7910.8837890625\n",
      "Epoch [192/250], Training Loss: 130.89053011941778, Validation Loss: 7761.45458984375\n",
      "Epoch [193/250], Training Loss: 134.73958141339756, Validation Loss: 8047.927734375\n",
      "Epoch [194/250], Training Loss: 281.7830739070966, Validation Loss: 8576.14453125\n",
      "Epoch [195/250], Training Loss: 159.02340614869334, Validation Loss: 9447.466796875\n",
      "Epoch [196/250], Training Loss: 123.61723554774177, Validation Loss: 7173.39697265625\n",
      "Epoch [197/250], Training Loss: 144.35441913835987, Validation Loss: 7318.1025390625\n",
      "Epoch [198/250], Training Loss: 133.13965999222268, Validation Loss: 7918.10107421875\n",
      "Epoch [199/250], Training Loss: 133.33605288360155, Validation Loss: 7233.03466796875\n",
      "Epoch [200/250], Training Loss: 284.410146451677, Validation Loss: 8836.5517578125\n",
      "Epoch [201/250], Training Loss: 352.25740548212104, Validation Loss: 9476.1943359375\n",
      "Epoch [202/250], Training Loss: 171.73438312362643, Validation Loss: 8388.234375\n",
      "Epoch [203/250], Training Loss: 110.84430523976836, Validation Loss: 9287.6220703125\n",
      "Epoch [204/250], Training Loss: 132.30639556632508, Validation Loss: 9220.7021484375\n",
      "Epoch [205/250], Training Loss: 138.04525840595366, Validation Loss: 8475.455078125\n",
      "Epoch [206/250], Training Loss: 993.521789199848, Validation Loss: 10249.9794921875\n",
      "Epoch [207/250], Training Loss: 542.4735400170422, Validation Loss: 5401.35400390625\n",
      "Epoch [208/250], Training Loss: 280.3085927941199, Validation Loss: 8853.751953125\n",
      "Epoch [209/250], Training Loss: 145.01952093672077, Validation Loss: 7242.27294921875\n",
      "Epoch [210/250], Training Loss: 99.62658374250556, Validation Loss: 9725.0595703125\n",
      "Epoch [211/250], Training Loss: 590.3475115629025, Validation Loss: 9234.0673828125\n",
      "Epoch [212/250], Training Loss: 349.63992690567846, Validation Loss: 5906.10595703125\n",
      "Epoch [213/250], Training Loss: 112.28957289103006, Validation Loss: 7091.05126953125\n",
      "Epoch [214/250], Training Loss: 334.9281057359111, Validation Loss: 9570.9453125\n",
      "Epoch [215/250], Training Loss: 141.88878451036265, Validation Loss: 9093.17578125\n",
      "Epoch [216/250], Training Loss: 399.91643603702585, Validation Loss: 7625.70654296875\n",
      "Epoch [217/250], Training Loss: 322.86846396996503, Validation Loss: 5843.98681640625\n",
      "Epoch [218/250], Training Loss: 108.93668403088986, Validation Loss: 8354.9287109375\n",
      "Epoch [219/250], Training Loss: 137.82829100397043, Validation Loss: 10676.45703125\n",
      "Epoch [220/250], Training Loss: 131.00837335910256, Validation Loss: 6019.8505859375\n",
      "Epoch [221/250], Training Loss: 138.809012547965, Validation Loss: 5877.16015625\n",
      "Epoch [222/250], Training Loss: 113.97071261324832, Validation Loss: 7698.451171875\n",
      "Epoch [223/250], Training Loss: 120.55825311232086, Validation Loss: 7440.56494140625\n",
      "Epoch [224/250], Training Loss: 131.60854959101718, Validation Loss: 8025.43408203125\n",
      "Epoch [225/250], Training Loss: 124.36520338229396, Validation Loss: 6703.58349609375\n",
      "Epoch [226/250], Training Loss: 139.45579737117964, Validation Loss: 7487.494140625\n",
      "Epoch [227/250], Training Loss: 129.0506719118505, Validation Loss: 7101.9423828125\n",
      "Epoch [228/250], Training Loss: 328.71228739903864, Validation Loss: 8317.9736328125\n",
      "Epoch [229/250], Training Loss: 168.68324191094345, Validation Loss: 7493.787109375\n",
      "Epoch [230/250], Training Loss: 113.87580549966012, Validation Loss: 7961.41064453125\n",
      "Epoch [231/250], Training Loss: 315.55103229183635, Validation Loss: 9038.02734375\n",
      "Epoch [232/250], Training Loss: 2681.6427605388044, Validation Loss: 8493.498046875\n",
      "Epoch [233/250], Training Loss: 626.6908107275625, Validation Loss: 3153.923828125\n",
      "Epoch [234/250], Training Loss: 84.09760800417635, Validation Loss: 5905.94921875\n",
      "Epoch [235/250], Training Loss: 99.19410280625075, Validation Loss: 8357.7353515625\n",
      "Epoch [236/250], Training Loss: 501.57076165868295, Validation Loss: 8822.3984375\n",
      "Epoch [237/250], Training Loss: 214.48998064857994, Validation Loss: 6984.3740234375\n",
      "Epoch [238/250], Training Loss: 106.17373802177187, Validation Loss: 7990.52783203125\n",
      "Epoch [239/250], Training Loss: 134.0266245056304, Validation Loss: 8079.630859375\n",
      "Epoch [240/250], Training Loss: 287.2405800118543, Validation Loss: 7344.962890625\n",
      "Epoch [241/250], Training Loss: 142.65233744960506, Validation Loss: 7886.43896484375\n",
      "Epoch [242/250], Training Loss: 234.52693637287027, Validation Loss: 9539.083984375\n",
      "Epoch [243/250], Training Loss: 760.2568837920261, Validation Loss: 9703.53515625\n",
      "Epoch [244/250], Training Loss: 357.1102050636935, Validation Loss: 6396.35498046875\n",
      "Epoch [245/250], Training Loss: 116.80521653427478, Validation Loss: 8492.1162109375\n",
      "Epoch [246/250], Training Loss: 122.10440989807519, Validation Loss: 6005.55126953125\n",
      "Epoch [247/250], Training Loss: 137.86871016531478, Validation Loss: 7020.5478515625\n",
      "Epoch [248/250], Training Loss: 194.87956549492773, Validation Loss: 8572.7236328125\n",
      "Epoch [249/250], Training Loss: 752.0532830319787, Validation Loss: 10172.732421875\n",
      "Epoch [250/250], Training Loss: 464.98064626620163, Validation Loss: 7133.5810546875\n",
      "Test Loss: 7466.56884765625\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.01, 1)\n",
      "Epoch [1/250], Training Loss: 15325.913058225833, Validation Loss: 18545.5625\n",
      "Epoch [2/250], Training Loss: 9863.045589570691, Validation Loss: 10940.4619140625\n",
      "Epoch [3/250], Training Loss: 7052.08036438531, Validation Loss: 11414.044921875\n",
      "Epoch [4/250], Training Loss: 5258.4081835430125, Validation Loss: 15528.4462890625\n",
      "Epoch [5/250], Training Loss: 3948.6943579782214, Validation Loss: 17775.6953125\n",
      "Epoch [6/250], Training Loss: 8647.831266683552, Validation Loss: 9827.3349609375\n",
      "Epoch [7/250], Training Loss: 7551.812675374443, Validation Loss: 8833.58203125\n",
      "Epoch [8/250], Training Loss: 6667.006226574737, Validation Loss: 7730.19384765625\n",
      "Epoch [9/250], Training Loss: 5891.469561591004, Validation Loss: 6865.39501953125\n",
      "Epoch [10/250], Training Loss: 5205.36276180191, Validation Loss: 6114.12548828125\n",
      "Epoch [11/250], Training Loss: 4612.793500393678, Validation Loss: 5412.85986328125\n",
      "Epoch [12/250], Training Loss: 1258.8668632081456, Validation Loss: 4764.21337890625\n",
      "Epoch [13/250], Training Loss: 1621.7708627245433, Validation Loss: 5368.28515625\n",
      "Epoch [14/250], Training Loss: 1792.271523391669, Validation Loss: 6991.3427734375\n",
      "Epoch [15/250], Training Loss: 2557.853978544851, Validation Loss: 9543.814453125\n",
      "Epoch [16/250], Training Loss: 3258.0228421553743, Validation Loss: 4893.5302734375\n",
      "Epoch [17/250], Training Loss: 3804.547292965906, Validation Loss: 19366.068359375\n",
      "Epoch [18/250], Training Loss: 3354.0542237794716, Validation Loss: 6708.5966796875\n",
      "Epoch [19/250], Training Loss: 2960.1532029049836, Validation Loss: 4325.22998046875\n",
      "Epoch [20/250], Training Loss: 2660.6683403003844, Validation Loss: 3556.11669921875\n",
      "Epoch [21/250], Training Loss: 2406.216388986208, Validation Loss: 3177.837890625\n",
      "Epoch [22/250], Training Loss: 2185.0038769316343, Validation Loss: 2946.196533203125\n",
      "Epoch [23/250], Training Loss: 1989.7269190375541, Validation Loss: 2771.2548828125\n",
      "Epoch [24/250], Training Loss: 1814.0077137361275, Validation Loss: 2618.07568359375\n",
      "Epoch [25/250], Training Loss: 1656.9156983548385, Validation Loss: 2477.5517578125\n",
      "Epoch [26/250], Training Loss: 1518.0291166964164, Validation Loss: 2352.0439453125\n",
      "Epoch [27/250], Training Loss: 1395.2062158712552, Validation Loss: 2242.5869140625\n",
      "Epoch [28/250], Training Loss: 1285.671798527176, Validation Loss: 2147.48974609375\n",
      "Epoch [29/250], Training Loss: 1186.9344713913467, Validation Loss: 2063.36279296875\n",
      "Epoch [30/250], Training Loss: 1097.2437764673023, Validation Loss: 1986.7572021484375\n",
      "Epoch [31/250], Training Loss: 1015.5049386767841, Validation Loss: 1915.5672607421875\n",
      "Epoch [32/250], Training Loss: 940.9329869985502, Validation Loss: 1849.07421875\n",
      "Epoch [33/250], Training Loss: 873.0330597337937, Validation Loss: 1787.9010009765625\n",
      "Epoch [34/250], Training Loss: 811.3764054224065, Validation Loss: 1733.05908203125\n",
      "Epoch [35/250], Training Loss: 755.5841790341697, Validation Loss: 1685.4326171875\n",
      "Epoch [36/250], Training Loss: 705.1299313305342, Validation Loss: 1644.7769775390625\n",
      "Epoch [37/250], Training Loss: 659.3283525626834, Validation Loss: 1608.5181884765625\n",
      "Epoch [38/250], Training Loss: 617.2222051170659, Validation Loss: 1571.7198486328125\n",
      "Epoch [39/250], Training Loss: 578.2161745302552, Validation Loss: 1532.774169921875\n",
      "Epoch [40/250], Training Loss: 542.452227519524, Validation Loss: 1494.900390625\n",
      "Epoch [41/250], Training Loss: 509.98350103376004, Validation Loss: 1459.44482421875\n",
      "Epoch [42/250], Training Loss: 480.5971218408286, Validation Loss: 1427.099609375\n",
      "Epoch [43/250], Training Loss: 453.8213057624746, Validation Loss: 1396.8817138671875\n",
      "Epoch [44/250], Training Loss: 429.33202848458086, Validation Loss: 1366.820068359375\n",
      "Epoch [45/250], Training Loss: 406.83251367050565, Validation Loss: 1337.1881103515625\n",
      "Epoch [46/250], Training Loss: 386.0773710838376, Validation Loss: 1309.6412353515625\n",
      "Epoch [47/250], Training Loss: 366.9055726317491, Validation Loss: 1283.4022216796875\n",
      "Epoch [48/250], Training Loss: 349.2244282195538, Validation Loss: 1255.3480224609375\n",
      "Epoch [49/250], Training Loss: 332.9127724471221, Validation Loss: 1224.0277099609375\n",
      "Epoch [50/250], Training Loss: 317.85461127324396, Validation Loss: 1191.8172607421875\n",
      "Epoch [51/250], Training Loss: 303.93524481207066, Validation Loss: 1162.5765380859375\n",
      "Epoch [52/250], Training Loss: 291.05673825688746, Validation Loss: 1138.342041015625\n",
      "Epoch [53/250], Training Loss: 279.18619261113764, Validation Loss: 1118.703369140625\n",
      "Epoch [54/250], Training Loss: 268.29540060024505, Validation Loss: 1101.975830078125\n",
      "Epoch [55/250], Training Loss: 258.38449982014686, Validation Loss: 1086.9302978515625\n",
      "Epoch [56/250], Training Loss: 249.42064416491692, Validation Loss: 1073.1719970703125\n",
      "Epoch [57/250], Training Loss: 241.2045589998391, Validation Loss: 1060.66552734375\n",
      "Epoch [58/250], Training Loss: 233.61532352111965, Validation Loss: 1049.3284912109375\n",
      "Epoch [59/250], Training Loss: 226.50543205112524, Validation Loss: 1038.7813720703125\n",
      "Epoch [60/250], Training Loss: 219.75681899700035, Validation Loss: 1028.662109375\n",
      "Epoch [61/250], Training Loss: 213.29243325730275, Validation Loss: 1018.7877197265625\n",
      "Epoch [62/250], Training Loss: 207.0561459962715, Validation Loss: 1009.081298828125\n",
      "Epoch [63/250], Training Loss: 201.031061943056, Validation Loss: 999.4912109375\n",
      "Epoch [64/250], Training Loss: 195.20469900326668, Validation Loss: 989.9542846679688\n",
      "Epoch [65/250], Training Loss: 189.50757258369885, Validation Loss: 980.6510009765625\n",
      "Epoch [66/250], Training Loss: 183.92362431320802, Validation Loss: 972.123291015625\n",
      "Epoch [67/250], Training Loss: 178.39286350700877, Validation Loss: 965.1227416992188\n",
      "Epoch [68/250], Training Loss: 172.88567794054333, Validation Loss: 960.2276611328125\n",
      "Epoch [69/250], Training Loss: 167.35525830611667, Validation Loss: 957.7035522460938\n",
      "Epoch [70/250], Training Loss: 161.85938486170107, Validation Loss: 957.5556030273438\n",
      "Epoch [71/250], Training Loss: 156.42887607812622, Validation Loss: 958.9114990234375\n",
      "Epoch [72/250], Training Loss: 151.16684108282976, Validation Loss: 961.0590209960938\n",
      "Epoch [73/250], Training Loss: 146.11736500614387, Validation Loss: 963.640625\n",
      "Epoch [74/250], Training Loss: 141.33102415676228, Validation Loss: 967.1314086914062\n",
      "Epoch [75/250], Training Loss: 136.8516618694213, Validation Loss: 971.91015625\n",
      "Epoch [76/250], Training Loss: 132.69827929893444, Validation Loss: 977.6635131835938\n",
      "Epoch [77/250], Training Loss: 128.91055692347987, Validation Loss: 984.2023315429688\n",
      "Epoch [78/250], Training Loss: 125.43515021175804, Validation Loss: 991.02880859375\n",
      "Epoch [79/250], Training Loss: 122.23177024999521, Validation Loss: 998.0790405273438\n",
      "Epoch [80/250], Training Loss: 119.24846323186847, Validation Loss: 1005.0773315429688\n",
      "Epoch [81/250], Training Loss: 116.45139485843964, Validation Loss: 1011.8201293945312\n",
      "Epoch [82/250], Training Loss: 113.81979654963281, Validation Loss: 1017.7027587890625\n",
      "Epoch [83/250], Training Loss: 111.30717555689135, Validation Loss: 1022.3453979492188\n",
      "Epoch [84/250], Training Loss: 108.90147431815265, Validation Loss: 1025.51318359375\n",
      "Epoch [85/250], Training Loss: 106.57535284325034, Validation Loss: 1027.337646484375\n",
      "Epoch [86/250], Training Loss: 104.33491347845967, Validation Loss: 1028.1533203125\n",
      "Epoch [87/250], Training Loss: 102.1963169764731, Validation Loss: 1028.423095703125\n",
      "Epoch [88/250], Training Loss: 100.14306315313603, Validation Loss: 1028.6715087890625\n",
      "Epoch [89/250], Training Loss: 98.18155774999025, Validation Loss: 1029.5322265625\n",
      "Epoch [90/250], Training Loss: 96.31866158496302, Validation Loss: 1031.2576904296875\n",
      "Epoch [91/250], Training Loss: 94.54032906491695, Validation Loss: 1034.3077392578125\n",
      "Epoch [92/250], Training Loss: 92.87362572283499, Validation Loss: 1038.3765869140625\n",
      "Epoch [93/250], Training Loss: 91.28212764509081, Validation Loss: 1043.212158203125\n",
      "Epoch [94/250], Training Loss: 89.78396804531204, Validation Loss: 1048.5667724609375\n",
      "Epoch [95/250], Training Loss: 88.37475958793988, Validation Loss: 1054.02294921875\n",
      "Epoch [96/250], Training Loss: 87.05020816151193, Validation Loss: 1059.187255859375\n",
      "Epoch [97/250], Training Loss: 85.80385825843534, Validation Loss: 1063.788818359375\n",
      "Epoch [98/250], Training Loss: 84.63450492054174, Validation Loss: 1067.6187744140625\n",
      "Epoch [99/250], Training Loss: 83.52653723912071, Validation Loss: 1070.75830078125\n",
      "Epoch [100/250], Training Loss: 82.49120223154439, Validation Loss: 1073.166748046875\n",
      "Epoch [101/250], Training Loss: 81.51579763280155, Validation Loss: 1074.806884765625\n",
      "Epoch [102/250], Training Loss: 80.593549348006, Validation Loss: 1075.78173828125\n",
      "Epoch [103/250], Training Loss: 79.720912880808, Validation Loss: 1075.984619140625\n",
      "Epoch [104/250], Training Loss: 78.89791792836061, Validation Loss: 1075.572509765625\n",
      "Epoch [105/250], Training Loss: 78.12281986240394, Validation Loss: 1074.1448974609375\n",
      "Epoch [106/250], Training Loss: 77.37234886583933, Validation Loss: 1071.6324462890625\n",
      "Epoch [107/250], Training Loss: 76.65972261305036, Validation Loss: 1068.0953369140625\n",
      "Epoch [108/250], Training Loss: 75.98331232267111, Validation Loss: 1063.474609375\n",
      "Epoch [109/250], Training Loss: 75.33318552993492, Validation Loss: 1057.578369140625\n",
      "Epoch [110/250], Training Loss: 74.71085827672198, Validation Loss: 1050.5382080078125\n",
      "Epoch [111/250], Training Loss: 74.10707544460253, Validation Loss: 1042.3492431640625\n",
      "Epoch [112/250], Training Loss: 73.53074057253065, Validation Loss: 1033.354736328125\n",
      "Epoch [113/250], Training Loss: 72.97760701223352, Validation Loss: 1023.6467895507812\n",
      "Epoch [114/250], Training Loss: 72.44662757008774, Validation Loss: 1013.4837036132812\n",
      "Epoch [115/250], Training Loss: 71.93541310644213, Validation Loss: 1003.0128784179688\n",
      "Epoch [116/250], Training Loss: 71.44369251722362, Validation Loss: 992.5780639648438\n",
      "Epoch [117/250], Training Loss: 70.97255769906208, Validation Loss: 982.3572387695312\n",
      "Epoch [118/250], Training Loss: 70.51771470855631, Validation Loss: 972.6280517578125\n",
      "Epoch [119/250], Training Loss: 70.08529032802375, Validation Loss: 963.3912353515625\n",
      "Epoch [120/250], Training Loss: 69.65965471025736, Validation Loss: 954.71826171875\n",
      "Epoch [121/250], Training Loss: 69.25187333456968, Validation Loss: 946.7732543945312\n",
      "Epoch [122/250], Training Loss: 68.85969662809099, Validation Loss: 939.5615844726562\n",
      "Epoch [123/250], Training Loss: 68.47780038950745, Validation Loss: 933.01904296875\n",
      "Epoch [124/250], Training Loss: 68.11087323124208, Validation Loss: 927.1346435546875\n",
      "Epoch [125/250], Training Loss: 67.74864647534652, Validation Loss: 921.87646484375\n",
      "Epoch [126/250], Training Loss: 67.40009020916284, Validation Loss: 917.1064453125\n",
      "Epoch [127/250], Training Loss: 67.06077229677088, Validation Loss: 913.0130615234375\n",
      "Epoch [128/250], Training Loss: 66.73742186081006, Validation Loss: 909.321044921875\n",
      "Epoch [129/250], Training Loss: 66.41628805145757, Validation Loss: 906.0725708007812\n",
      "Epoch [130/250], Training Loss: 66.10603502040289, Validation Loss: 903.2021484375\n",
      "Epoch [131/250], Training Loss: 65.80619154216646, Validation Loss: 900.5840454101562\n",
      "Epoch [132/250], Training Loss: 65.51121073858376, Validation Loss: 898.2412109375\n",
      "Epoch [133/250], Training Loss: 65.2226611739502, Validation Loss: 896.0557250976562\n",
      "Epoch [134/250], Training Loss: 64.94169588718374, Validation Loss: 894.1159057617188\n",
      "Epoch [135/250], Training Loss: 64.6674854818211, Validation Loss: 892.3206176757812\n",
      "Epoch [136/250], Training Loss: 64.39998540915826, Validation Loss: 890.6354370117188\n",
      "Epoch [137/250], Training Loss: 64.13845750612307, Validation Loss: 889.0458374023438\n",
      "Epoch [138/250], Training Loss: 63.87807487772854, Validation Loss: 887.48388671875\n",
      "Epoch [139/250], Training Loss: 63.62304201733609, Validation Loss: 885.9317626953125\n",
      "Epoch [140/250], Training Loss: 63.368343185897466, Validation Loss: 884.4032592773438\n",
      "Epoch [141/250], Training Loss: 63.121223180942735, Validation Loss: 882.9269409179688\n",
      "Epoch [142/250], Training Loss: 62.87819977373884, Validation Loss: 881.5618896484375\n",
      "Epoch [143/250], Training Loss: 62.6417288383674, Validation Loss: 880.2544555664062\n",
      "Epoch [144/250], Training Loss: 62.4094139976262, Validation Loss: 879.0950927734375\n",
      "Epoch [145/250], Training Loss: 62.186278732111376, Validation Loss: 878.0083618164062\n",
      "Epoch [146/250], Training Loss: 61.96383082229337, Validation Loss: 876.910400390625\n",
      "Epoch [147/250], Training Loss: 61.74786682965509, Validation Loss: 875.85986328125\n",
      "Epoch [148/250], Training Loss: 61.53119397714334, Validation Loss: 874.8411865234375\n",
      "Epoch [149/250], Training Loss: 61.32346079319738, Validation Loss: 873.8604736328125\n",
      "Epoch [150/250], Training Loss: 61.11685376281735, Validation Loss: 872.8809204101562\n",
      "Epoch [151/250], Training Loss: 60.91074685315422, Validation Loss: 871.891357421875\n",
      "Epoch [152/250], Training Loss: 60.710948659773685, Validation Loss: 870.9419555664062\n",
      "Epoch [153/250], Training Loss: 60.51572568480617, Validation Loss: 870.06689453125\n",
      "Epoch [154/250], Training Loss: 60.32507515749491, Validation Loss: 869.27978515625\n",
      "Epoch [155/250], Training Loss: 60.136204966340564, Validation Loss: 868.5380859375\n",
      "Epoch [156/250], Training Loss: 59.956808994502154, Validation Loss: 867.8922729492188\n",
      "Epoch [157/250], Training Loss: 59.780190076464, Validation Loss: 867.2650756835938\n",
      "Epoch [158/250], Training Loss: 59.60644511674845, Validation Loss: 866.5795288085938\n",
      "Epoch [159/250], Training Loss: 59.43101397180639, Validation Loss: 865.8865356445312\n",
      "Epoch [160/250], Training Loss: 59.26337331556549, Validation Loss: 865.2942504882812\n",
      "Epoch [161/250], Training Loss: 59.09619786830753, Validation Loss: 864.6915893554688\n",
      "Epoch [162/250], Training Loss: 58.93674857592317, Validation Loss: 864.1631469726562\n",
      "Epoch [163/250], Training Loss: 58.778451437925796, Validation Loss: 863.66650390625\n",
      "Epoch [164/250], Training Loss: 58.62184438575345, Validation Loss: 863.1543579101562\n",
      "Epoch [165/250], Training Loss: 58.468572819743265, Validation Loss: 862.722412109375\n",
      "Epoch [166/250], Training Loss: 58.32355688346273, Validation Loss: 862.2797241210938\n",
      "Epoch [167/250], Training Loss: 58.176968177208025, Validation Loss: 861.80322265625\n",
      "Epoch [168/250], Training Loss: 58.033326945286376, Validation Loss: 861.297607421875\n",
      "Epoch [169/250], Training Loss: 57.88806733383337, Validation Loss: 860.86474609375\n",
      "Epoch [170/250], Training Loss: 57.75060486101382, Validation Loss: 860.496826171875\n",
      "Epoch [171/250], Training Loss: 57.617362288909966, Validation Loss: 860.27490234375\n",
      "Epoch [172/250], Training Loss: 57.48999700998209, Validation Loss: 860.085205078125\n",
      "Epoch [173/250], Training Loss: 57.365002495784495, Validation Loss: 859.8440551757812\n",
      "Epoch [174/250], Training Loss: 57.24183716537645, Validation Loss: 859.5882568359375\n",
      "Epoch [175/250], Training Loss: 57.12050949918191, Validation Loss: 859.3584594726562\n",
      "Epoch [176/250], Training Loss: 57.00513418464705, Validation Loss: 859.1034545898438\n",
      "Epoch [177/250], Training Loss: 56.88706811939142, Validation Loss: 858.8353271484375\n",
      "Epoch [178/250], Training Loss: 56.77456424590608, Validation Loss: 858.688720703125\n",
      "Epoch [179/250], Training Loss: 56.66935380762085, Validation Loss: 858.5386962890625\n",
      "Epoch [180/250], Training Loss: 56.56026016254854, Validation Loss: 858.3273315429688\n",
      "Epoch [181/250], Training Loss: 56.45492945185719, Validation Loss: 858.1420288085938\n",
      "Epoch [182/250], Training Loss: 56.35216563703437, Validation Loss: 857.9947509765625\n",
      "Epoch [183/250], Training Loss: 56.25325590438252, Validation Loss: 857.8876953125\n",
      "Epoch [184/250], Training Loss: 56.151640612577495, Validation Loss: 857.7360229492188\n",
      "Epoch [185/250], Training Loss: 56.053783237530716, Validation Loss: 857.6785888671875\n",
      "Epoch [186/250], Training Loss: 55.964912336969945, Validation Loss: 857.674072265625\n",
      "Epoch [187/250], Training Loss: 55.8737133329269, Validation Loss: 857.5777587890625\n",
      "Epoch [188/250], Training Loss: 55.78329243150618, Validation Loss: 857.33740234375\n",
      "Epoch [189/250], Training Loss: 55.690629727710125, Validation Loss: 857.170166015625\n",
      "Epoch [190/250], Training Loss: 55.60398888471561, Validation Loss: 857.0672607421875\n",
      "Epoch [191/250], Training Loss: 55.52033121169186, Validation Loss: 856.8675537109375\n",
      "Epoch [192/250], Training Loss: 55.43239617071781, Validation Loss: 856.6549072265625\n",
      "Epoch [193/250], Training Loss: 55.35092108609552, Validation Loss: 856.4888305664062\n",
      "Epoch [194/250], Training Loss: 55.27263235742055, Validation Loss: 856.3639526367188\n",
      "Epoch [195/250], Training Loss: 55.195202676246716, Validation Loss: 856.2153930664062\n",
      "Epoch [196/250], Training Loss: 55.12094613986881, Validation Loss: 856.1055297851562\n",
      "Epoch [197/250], Training Loss: 55.04844966841729, Validation Loss: 855.9686279296875\n",
      "Epoch [198/250], Training Loss: 54.97401088104866, Validation Loss: 855.8191528320312\n",
      "Epoch [199/250], Training Loss: 54.904046215910334, Validation Loss: 855.6969604492188\n",
      "Epoch [200/250], Training Loss: 54.83439562111948, Validation Loss: 855.5452270507812\n",
      "Epoch [201/250], Training Loss: 54.76637159476985, Validation Loss: 855.4073486328125\n",
      "Epoch [202/250], Training Loss: 54.70012782555433, Validation Loss: 855.2068481445312\n",
      "Epoch [203/250], Training Loss: 54.63079459961805, Validation Loss: 854.9448852539062\n",
      "Epoch [204/250], Training Loss: 54.56334828800071, Validation Loss: 854.7190551757812\n",
      "Epoch [205/250], Training Loss: 54.50072010065789, Validation Loss: 854.4937744140625\n",
      "Epoch [206/250], Training Loss: 54.439451551453374, Validation Loss: 854.2664794921875\n",
      "Epoch [207/250], Training Loss: 54.37712843520856, Validation Loss: 854.0257568359375\n",
      "Epoch [208/250], Training Loss: 54.31609770123935, Validation Loss: 853.7587280273438\n",
      "Epoch [209/250], Training Loss: 54.256421721970106, Validation Loss: 853.4547119140625\n",
      "Epoch [210/250], Training Loss: 54.19587776974013, Validation Loss: 853.0745239257812\n",
      "Epoch [211/250], Training Loss: 54.135246916942265, Validation Loss: 852.7439575195312\n",
      "Epoch [212/250], Training Loss: 54.0808778184415, Validation Loss: 852.4884643554688\n",
      "Epoch [213/250], Training Loss: 54.02894012685804, Validation Loss: 852.2303466796875\n",
      "Epoch [214/250], Training Loss: 53.97325487258742, Validation Loss: 851.932861328125\n",
      "Epoch [215/250], Training Loss: 53.91942062952381, Validation Loss: 851.6614379882812\n",
      "Epoch [216/250], Training Loss: 53.86872594420281, Validation Loss: 851.462890625\n",
      "Epoch [217/250], Training Loss: 53.820352453196584, Validation Loss: 851.2218017578125\n",
      "Epoch [218/250], Training Loss: 53.770621749454286, Validation Loss: 850.9866943359375\n",
      "Epoch [219/250], Training Loss: 53.72347443787064, Validation Loss: 850.7418823242188\n",
      "Epoch [220/250], Training Loss: 53.67658666769768, Validation Loss: 850.518798828125\n",
      "Epoch [221/250], Training Loss: 53.631091142582925, Validation Loss: 850.2684326171875\n",
      "Epoch [222/250], Training Loss: 53.584571071654885, Validation Loss: 849.9251098632812\n",
      "Epoch [223/250], Training Loss: 53.535655333538614, Validation Loss: 849.5447998046875\n",
      "Epoch [224/250], Training Loss: 53.48975285928646, Validation Loss: 849.1986694335938\n",
      "Epoch [225/250], Training Loss: 53.444164839539894, Validation Loss: 848.9295654296875\n",
      "Epoch [226/250], Training Loss: 53.40278325760049, Validation Loss: 848.67919921875\n",
      "Epoch [227/250], Training Loss: 53.361429492069554, Validation Loss: 848.463623046875\n",
      "Epoch [228/250], Training Loss: 53.32193109889703, Validation Loss: 848.2608642578125\n",
      "Epoch [229/250], Training Loss: 53.28162084131381, Validation Loss: 847.9937133789062\n",
      "Epoch [230/250], Training Loss: 53.24074616345087, Validation Loss: 847.735107421875\n",
      "Epoch [231/250], Training Loss: 53.202036492001604, Validation Loss: 847.5083618164062\n",
      "Epoch [232/250], Training Loss: 53.164973554042625, Validation Loss: 847.3064575195312\n",
      "Epoch [233/250], Training Loss: 53.12991754741347, Validation Loss: 847.0991821289062\n",
      "Epoch [234/250], Training Loss: 53.09330038341627, Validation Loss: 846.8763427734375\n",
      "Epoch [235/250], Training Loss: 53.0575826298837, Validation Loss: 846.6919555664062\n",
      "Epoch [236/250], Training Loss: 53.02403315421318, Validation Loss: 846.4871215820312\n",
      "Epoch [237/250], Training Loss: 52.98959853285141, Validation Loss: 846.218505859375\n",
      "Epoch [238/250], Training Loss: 52.95326118818716, Validation Loss: 845.9352416992188\n",
      "Epoch [239/250], Training Loss: 52.9189629928246, Validation Loss: 845.6620483398438\n",
      "Epoch [240/250], Training Loss: 52.88412032293613, Validation Loss: 845.400634765625\n",
      "Epoch [241/250], Training Loss: 52.85058849878744, Validation Loss: 845.1300048828125\n",
      "Epoch [242/250], Training Loss: 52.81683688636373, Validation Loss: 844.8475341796875\n",
      "Epoch [243/250], Training Loss: 52.785227535693615, Validation Loss: 844.6047973632812\n",
      "Epoch [244/250], Training Loss: 52.75595153316701, Validation Loss: 844.3847045898438\n",
      "Epoch [245/250], Training Loss: 52.72574288537916, Validation Loss: 844.1651000976562\n",
      "Epoch [246/250], Training Loss: 52.69519060133031, Validation Loss: 843.8984985351562\n",
      "Epoch [247/250], Training Loss: 52.66414850934341, Validation Loss: 843.6099853515625\n",
      "Epoch [248/250], Training Loss: 52.63352829782008, Validation Loss: 843.3143920898438\n",
      "Epoch [249/250], Training Loss: 52.60459958987904, Validation Loss: 843.0596313476562\n",
      "Epoch [250/250], Training Loss: 52.575483452929824, Validation Loss: 842.7362670898438\n",
      "Test Loss: 837.5494995117188\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.01, 2)\n",
      "Epoch [1/250], Training Loss: 15477.49048378131, Validation Loss: 18581.296875\n",
      "Epoch [2/250], Training Loss: 11110.061954157884, Validation Loss: 18717.0390625\n",
      "Epoch [3/250], Training Loss: 8435.257628392244, Validation Loss: 14174.673828125\n",
      "Epoch [4/250], Training Loss: 11159.39252954195, Validation Loss: 17997.802734375\n",
      "Epoch [5/250], Training Loss: 8971.937752533697, Validation Loss: 9862.185546875\n",
      "Epoch [6/250], Training Loss: 7604.706083239329, Validation Loss: 8422.9462890625\n",
      "Epoch [7/250], Training Loss: 6552.917106778909, Validation Loss: 7316.09814453125\n",
      "Epoch [8/250], Training Loss: 5637.288822202462, Validation Loss: 6450.1982421875\n",
      "Epoch [9/250], Training Loss: 4872.703470648271, Validation Loss: 5732.71044921875\n",
      "Epoch [10/250], Training Loss: 4239.590324135998, Validation Loss: 5133.3212890625\n",
      "Epoch [11/250], Training Loss: 3694.2156733143074, Validation Loss: 4604.3076171875\n",
      "Epoch [12/250], Training Loss: 3234.560923410693, Validation Loss: 4131.67431640625\n",
      "Epoch [13/250], Training Loss: 2823.481013303429, Validation Loss: 3719.633056640625\n",
      "Epoch [14/250], Training Loss: 2496.895169620139, Validation Loss: 3372.895263671875\n",
      "Epoch [15/250], Training Loss: 2206.132754246335, Validation Loss: 3069.892333984375\n",
      "Epoch [16/250], Training Loss: 1971.7526403843945, Validation Loss: 3518.0849609375\n",
      "Epoch [17/250], Training Loss: 1757.6713709819508, Validation Loss: 2619.567626953125\n",
      "Epoch [18/250], Training Loss: 1569.8919715965428, Validation Loss: 4571.14794921875\n",
      "Epoch [19/250], Training Loss: 1413.4826032589492, Validation Loss: 2537.131103515625\n",
      "Epoch [20/250], Training Loss: 1270.8283390094623, Validation Loss: 3686.466064453125\n",
      "Epoch [21/250], Training Loss: 1147.4951857263604, Validation Loss: 2279.56787109375\n",
      "Epoch [22/250], Training Loss: 1999.9398420370655, Validation Loss: 3013.1328125\n",
      "Epoch [23/250], Training Loss: 1154.7576603916905, Validation Loss: 2418.90478515625\n",
      "Epoch [24/250], Training Loss: 977.2732116326869, Validation Loss: 2577.929931640625\n",
      "Epoch [25/250], Training Loss: 962.2026736496631, Validation Loss: 2108.00390625\n",
      "Epoch [26/250], Training Loss: 705.6013457447582, Validation Loss: 2187.495361328125\n",
      "Epoch [27/250], Training Loss: 256.5155843389257, Validation Loss: 2369.374755859375\n",
      "Epoch [28/250], Training Loss: 261.04686777765505, Validation Loss: 2442.516357421875\n",
      "Epoch [29/250], Training Loss: 186.35524140526135, Validation Loss: 2471.3515625\n",
      "Epoch [30/250], Training Loss: 138.6835394893134, Validation Loss: 2768.427001953125\n",
      "Epoch [31/250], Training Loss: 153.7449865643123, Validation Loss: 2949.131103515625\n",
      "Epoch [32/250], Training Loss: 138.89930080548555, Validation Loss: 2695.869873046875\n",
      "Epoch [33/250], Training Loss: 783.8866359865293, Validation Loss: 2074.114990234375\n",
      "Epoch [34/250], Training Loss: 597.6062813528262, Validation Loss: 2103.884521484375\n",
      "Epoch [35/250], Training Loss: 390.8939306063565, Validation Loss: 1849.349365234375\n",
      "Epoch [36/250], Training Loss: 157.39034712919099, Validation Loss: 2511.55908203125\n",
      "Epoch [37/250], Training Loss: 204.41973926139607, Validation Loss: 2380.528076171875\n",
      "Epoch [38/250], Training Loss: 283.150203976813, Validation Loss: 2630.1884765625\n",
      "Epoch [39/250], Training Loss: 394.88176157492427, Validation Loss: 2206.615234375\n",
      "Epoch [40/250], Training Loss: 524.6573607632689, Validation Loss: 1906.2803955078125\n",
      "Epoch [41/250], Training Loss: 467.4647434693838, Validation Loss: 1804.22412109375\n",
      "Epoch [42/250], Training Loss: 493.201327907053, Validation Loss: 7072.1123046875\n",
      "Epoch [43/250], Training Loss: 221.16547670128716, Validation Loss: 8876.5341796875\n",
      "Epoch [44/250], Training Loss: 190.9859626084977, Validation Loss: 8634.7265625\n",
      "Epoch [45/250], Training Loss: 237.91931738725813, Validation Loss: 8986.7744140625\n",
      "Epoch [46/250], Training Loss: 241.05372753625275, Validation Loss: 9322.6826171875\n",
      "Epoch [47/250], Training Loss: 224.02362978846918, Validation Loss: 11135.5712890625\n",
      "Epoch [48/250], Training Loss: 417.11086926600956, Validation Loss: 14417.7783203125\n",
      "Epoch [49/250], Training Loss: 351.35606844808376, Validation Loss: 15367.2529296875\n",
      "Epoch [50/250], Training Loss: 302.78406533895327, Validation Loss: 11207.662109375\n",
      "Epoch [51/250], Training Loss: 457.4425690610835, Validation Loss: 15708.0859375\n",
      "Epoch [52/250], Training Loss: 308.48299429947235, Validation Loss: 13092.1875\n",
      "Epoch [53/250], Training Loss: 226.57466889547672, Validation Loss: 12260.87890625\n",
      "Epoch [54/250], Training Loss: 226.84399928924628, Validation Loss: 8391.486328125\n",
      "Epoch [55/250], Training Loss: 240.50958683047656, Validation Loss: 7308.48046875\n",
      "Epoch [56/250], Training Loss: 221.5834319381602, Validation Loss: 8095.94482421875\n",
      "Epoch [57/250], Training Loss: 183.9479045675525, Validation Loss: 5867.81982421875\n",
      "Epoch [58/250], Training Loss: 197.86297671667828, Validation Loss: 7108.65478515625\n",
      "Epoch [59/250], Training Loss: 165.91626720756693, Validation Loss: 6123.23193359375\n",
      "Epoch [60/250], Training Loss: 138.58861266459272, Validation Loss: 5905.57568359375\n",
      "Epoch [61/250], Training Loss: 190.18667080837398, Validation Loss: 5775.5224609375\n",
      "Epoch [62/250], Training Loss: 162.7080170661826, Validation Loss: 5801.69287109375\n",
      "Epoch [63/250], Training Loss: 197.86422806906634, Validation Loss: 5503.75146484375\n",
      "Epoch [64/250], Training Loss: 215.74409339180937, Validation Loss: 4973.52734375\n",
      "Epoch [65/250], Training Loss: 320.5380558237235, Validation Loss: 6989.111328125\n",
      "Epoch [66/250], Training Loss: 294.3196731890202, Validation Loss: 6821.68701171875\n",
      "Epoch [67/250], Training Loss: 183.866928633149, Validation Loss: 8952.927734375\n",
      "Epoch [68/250], Training Loss: 264.40154333776474, Validation Loss: 10160.0927734375\n",
      "Epoch [69/250], Training Loss: 248.60182518224687, Validation Loss: 7494.26318359375\n",
      "Epoch [70/250], Training Loss: 251.52589092371136, Validation Loss: 8012.6787109375\n",
      "Epoch [71/250], Training Loss: 221.50448252826558, Validation Loss: 10126.271484375\n",
      "Epoch [72/250], Training Loss: 164.4036206973653, Validation Loss: 13012.658203125\n",
      "Epoch [73/250], Training Loss: 251.74895915347872, Validation Loss: 12746.4599609375\n",
      "Epoch [74/250], Training Loss: 214.36862067757875, Validation Loss: 11996.193359375\n",
      "Epoch [75/250], Training Loss: 226.01364771509142, Validation Loss: 14067.6875\n",
      "Epoch [76/250], Training Loss: 203.35118161108235, Validation Loss: 9314.07421875\n",
      "Epoch [77/250], Training Loss: 191.70851278717933, Validation Loss: 5325.0\n",
      "Epoch [78/250], Training Loss: 235.65638295501213, Validation Loss: 7272.01513671875\n",
      "Epoch [79/250], Training Loss: 210.52241265384268, Validation Loss: 7888.20263671875\n",
      "Epoch [80/250], Training Loss: 174.44381519618088, Validation Loss: 7799.44775390625\n",
      "Epoch [81/250], Training Loss: 231.52318932447042, Validation Loss: 9439.0849609375\n",
      "Epoch [82/250], Training Loss: 213.05390643360295, Validation Loss: 11938.88671875\n",
      "Epoch [83/250], Training Loss: 211.4180215395822, Validation Loss: 13821.6005859375\n",
      "Epoch [84/250], Training Loss: 207.05148121201955, Validation Loss: 12365.51171875\n",
      "Epoch [85/250], Training Loss: 188.62299493140995, Validation Loss: 10700.8740234375\n",
      "Epoch [86/250], Training Loss: 90.81367838619454, Validation Loss: 33260.9453125\n",
      "Epoch [87/250], Training Loss: 104.75964179241862, Validation Loss: 11019.685546875\n",
      "Epoch [88/250], Training Loss: 198.1361970783978, Validation Loss: 8708.0146484375\n",
      "Epoch [89/250], Training Loss: 172.53515325936, Validation Loss: 9747.0\n",
      "Epoch [90/250], Training Loss: 202.01679917945913, Validation Loss: 7240.7216796875\n",
      "Epoch [91/250], Training Loss: 204.0364198708686, Validation Loss: 4412.60986328125\n",
      "Epoch [92/250], Training Loss: 132.11891353844308, Validation Loss: 10138.177734375\n",
      "Epoch [93/250], Training Loss: 200.60133185276433, Validation Loss: 5914.62744140625\n",
      "Epoch [94/250], Training Loss: 169.39658328603295, Validation Loss: 4293.103515625\n",
      "Epoch [95/250], Training Loss: 212.8278374405987, Validation Loss: 5564.68408203125\n",
      "Epoch [96/250], Training Loss: 197.2207189419632, Validation Loss: 3149.12744140625\n",
      "Epoch [97/250], Training Loss: 224.44178682770226, Validation Loss: 5695.68310546875\n",
      "Epoch [98/250], Training Loss: 197.4674951040671, Validation Loss: 2925.829345703125\n",
      "Epoch [99/250], Training Loss: 225.85709917167532, Validation Loss: 5104.23388671875\n",
      "Epoch [100/250], Training Loss: 181.9640104129808, Validation Loss: 3198.9716796875\n",
      "Epoch [101/250], Training Loss: 213.00333623316607, Validation Loss: 5417.16650390625\n",
      "Epoch [102/250], Training Loss: 211.8272055749408, Validation Loss: 3100.760986328125\n",
      "Epoch [103/250], Training Loss: 158.71195600283227, Validation Loss: 5558.7919921875\n",
      "Epoch [104/250], Training Loss: 95.7843204831806, Validation Loss: 9080.86328125\n",
      "Epoch [105/250], Training Loss: 94.37521866406719, Validation Loss: 4066.158447265625\n",
      "Epoch [106/250], Training Loss: 106.23680073309181, Validation Loss: 2834.994873046875\n",
      "Epoch [107/250], Training Loss: 165.1523475364262, Validation Loss: 4722.974609375\n",
      "Epoch [108/250], Training Loss: 212.4741480339436, Validation Loss: 3742.010986328125\n",
      "Epoch [109/250], Training Loss: 231.03224899458877, Validation Loss: 2408.47998046875\n",
      "Epoch [110/250], Training Loss: 218.7630474797551, Validation Loss: 3932.990966796875\n",
      "Epoch [111/250], Training Loss: 160.09750178167977, Validation Loss: 3244.534423828125\n",
      "Epoch [112/250], Training Loss: 154.66498331156802, Validation Loss: 2354.06640625\n",
      "Epoch [113/250], Training Loss: 154.3705193394529, Validation Loss: 6195.92138671875\n",
      "Epoch [114/250], Training Loss: 131.07077390922558, Validation Loss: 2295.901611328125\n",
      "Epoch [115/250], Training Loss: 122.39648227217266, Validation Loss: 2388.402099609375\n",
      "Epoch [116/250], Training Loss: 128.7298863752207, Validation Loss: 2349.115234375\n",
      "Epoch [117/250], Training Loss: 119.74149176274875, Validation Loss: 2171.415283203125\n",
      "Epoch [118/250], Training Loss: 142.05984143753955, Validation Loss: 2344.8828125\n",
      "Epoch [119/250], Training Loss: 136.40128418079033, Validation Loss: 2546.53271484375\n",
      "Epoch [120/250], Training Loss: 141.34463638544744, Validation Loss: 2505.50048828125\n",
      "Epoch [121/250], Training Loss: 232.16511660581244, Validation Loss: 2568.21435546875\n",
      "Epoch [122/250], Training Loss: 193.36197953775914, Validation Loss: 4369.31005859375\n",
      "Epoch [123/250], Training Loss: 245.86438096160006, Validation Loss: 2855.359130859375\n",
      "Epoch [124/250], Training Loss: 200.5978122215477, Validation Loss: 3039.56298828125\n",
      "Epoch [125/250], Training Loss: 153.60150823128714, Validation Loss: 16415.203125\n",
      "Epoch [126/250], Training Loss: 121.12254761330881, Validation Loss: 8661.251953125\n",
      "Epoch [127/250], Training Loss: 75.67818781165015, Validation Loss: 2181.01416015625\n",
      "Epoch [128/250], Training Loss: 91.91094386016883, Validation Loss: 2111.472900390625\n",
      "Epoch [129/250], Training Loss: 82.95746570677282, Validation Loss: 2210.0556640625\n",
      "Epoch [130/250], Training Loss: 81.29744420500543, Validation Loss: 2043.0927734375\n",
      "Epoch [131/250], Training Loss: 74.25281134765798, Validation Loss: 1923.2489013671875\n",
      "Epoch [132/250], Training Loss: 79.92962223967415, Validation Loss: 1874.390869140625\n",
      "Epoch [133/250], Training Loss: 71.9040949589268, Validation Loss: 1853.490966796875\n",
      "Epoch [134/250], Training Loss: 91.86545650033705, Validation Loss: 2503.32421875\n",
      "Epoch [135/250], Training Loss: 75.61919184280154, Validation Loss: 1869.8065185546875\n",
      "Epoch [136/250], Training Loss: 69.95820684609816, Validation Loss: 1841.51171875\n",
      "Epoch [137/250], Training Loss: 76.44987324735432, Validation Loss: 2052.50927734375\n",
      "Epoch [138/250], Training Loss: 70.9211424529758, Validation Loss: 1878.5135498046875\n",
      "Epoch [139/250], Training Loss: 71.4219747631656, Validation Loss: 2258.095458984375\n",
      "Epoch [140/250], Training Loss: 62.76468110971152, Validation Loss: 1887.630615234375\n",
      "Epoch [141/250], Training Loss: 46.3317703174933, Validation Loss: 1630.451171875\n",
      "Epoch [142/250], Training Loss: 61.661280020040714, Validation Loss: 1909.6063232421875\n",
      "Epoch [143/250], Training Loss: 46.24463680330242, Validation Loss: 1701.4012451171875\n",
      "Epoch [144/250], Training Loss: 58.34333435616725, Validation Loss: 1944.551513671875\n",
      "Epoch [145/250], Training Loss: 57.300926040408406, Validation Loss: 1792.0985107421875\n",
      "Epoch [146/250], Training Loss: 52.04713312529571, Validation Loss: 1745.0999755859375\n",
      "Epoch [147/250], Training Loss: 49.911922536967936, Validation Loss: 1801.4107666015625\n",
      "Epoch [148/250], Training Loss: 55.02103027011871, Validation Loss: 1997.142333984375\n",
      "Epoch [149/250], Training Loss: 49.992651420392164, Validation Loss: 1831.3377685546875\n",
      "Epoch [150/250], Training Loss: 70.55430180461072, Validation Loss: 1577.3143310546875\n",
      "Epoch [151/250], Training Loss: 53.12197492725224, Validation Loss: 2494.965087890625\n",
      "Epoch [152/250], Training Loss: 46.416126696863756, Validation Loss: 1622.1654052734375\n",
      "Epoch [153/250], Training Loss: 48.32234049911358, Validation Loss: 2461.15283203125\n",
      "Epoch [154/250], Training Loss: 42.18720841237487, Validation Loss: 1675.8223876953125\n",
      "Epoch [155/250], Training Loss: 48.399267020120796, Validation Loss: 1635.3365478515625\n",
      "Epoch [156/250], Training Loss: 52.51430313410799, Validation Loss: 1619.356201171875\n",
      "Epoch [157/250], Training Loss: 40.95931673833443, Validation Loss: 1897.642578125\n",
      "Epoch [158/250], Training Loss: 48.65086943389708, Validation Loss: 2406.9130859375\n",
      "Epoch [159/250], Training Loss: 45.22340312166907, Validation Loss: 1527.661376953125\n",
      "Epoch [160/250], Training Loss: 43.918864887113756, Validation Loss: 1514.7431640625\n",
      "Epoch [161/250], Training Loss: 41.170194521066016, Validation Loss: 1565.72119140625\n",
      "Epoch [162/250], Training Loss: 30.734539971320665, Validation Loss: 1524.865966796875\n",
      "Epoch [163/250], Training Loss: 35.36559886745432, Validation Loss: 1586.7384033203125\n",
      "Epoch [164/250], Training Loss: 32.98767911454417, Validation Loss: 1543.7708740234375\n",
      "Epoch [165/250], Training Loss: 36.12164460725291, Validation Loss: 1528.005615234375\n",
      "Epoch [166/250], Training Loss: 39.41428568831472, Validation Loss: 1533.3873291015625\n",
      "Epoch [167/250], Training Loss: 32.99765393651796, Validation Loss: 1662.9764404296875\n",
      "Epoch [168/250], Training Loss: 32.25992328651721, Validation Loss: 1685.4913330078125\n",
      "Epoch [169/250], Training Loss: 31.419791008649643, Validation Loss: 1622.5352783203125\n",
      "Epoch [170/250], Training Loss: 32.916676228389804, Validation Loss: 1744.2904052734375\n",
      "Epoch [171/250], Training Loss: 43.43883355972817, Validation Loss: 1545.413330078125\n",
      "Epoch [172/250], Training Loss: 84.71550921891112, Validation Loss: 1749.0989990234375\n",
      "Epoch [173/250], Training Loss: 47.14359400880465, Validation Loss: 1772.067626953125\n",
      "Epoch [174/250], Training Loss: 42.45492773785827, Validation Loss: 1806.396484375\n",
      "Epoch [175/250], Training Loss: 42.49966962775835, Validation Loss: 1850.7645263671875\n",
      "Epoch [176/250], Training Loss: 53.390586263413404, Validation Loss: 1767.12109375\n",
      "Epoch [177/250], Training Loss: 44.28325448802283, Validation Loss: 1678.8267822265625\n",
      "Epoch [178/250], Training Loss: 44.28020453341941, Validation Loss: 2099.8408203125\n",
      "Epoch [179/250], Training Loss: 59.68021198246378, Validation Loss: 1905.82861328125\n",
      "Epoch [180/250], Training Loss: 59.87811403214891, Validation Loss: 1759.48291015625\n",
      "Epoch [181/250], Training Loss: 49.45498695723191, Validation Loss: 1695.6888427734375\n",
      "Epoch [182/250], Training Loss: 55.12901054870808, Validation Loss: 1503.3468017578125\n",
      "Epoch [183/250], Training Loss: 357.1101035010902, Validation Loss: 1493.7274169921875\n",
      "Epoch [184/250], Training Loss: 284.6861305572662, Validation Loss: 2036.41796875\n",
      "Epoch [185/250], Training Loss: 253.98683877565458, Validation Loss: 2294.8671875\n",
      "Epoch [186/250], Training Loss: 220.82583330623748, Validation Loss: 2306.626953125\n",
      "Epoch [187/250], Training Loss: 218.86692320210184, Validation Loss: 2720.869140625\n",
      "Epoch [188/250], Training Loss: 202.8316495161834, Validation Loss: 2529.79296875\n",
      "Epoch [189/250], Training Loss: 191.84568719884876, Validation Loss: 2414.7939453125\n",
      "Epoch [190/250], Training Loss: 183.15914030815944, Validation Loss: 2294.170654296875\n",
      "Epoch [191/250], Training Loss: 171.5034620436459, Validation Loss: 2227.0498046875\n",
      "Epoch [192/250], Training Loss: 161.47257820970356, Validation Loss: 2190.304931640625\n",
      "Epoch [193/250], Training Loss: 152.67793594198056, Validation Loss: 2176.526611328125\n",
      "Epoch [194/250], Training Loss: 144.56190050815437, Validation Loss: 2170.718017578125\n",
      "Epoch [195/250], Training Loss: 137.44386398161333, Validation Loss: 2166.1640625\n",
      "Epoch [196/250], Training Loss: 130.89773516438268, Validation Loss: 2171.353515625\n",
      "Epoch [197/250], Training Loss: 124.75486703338787, Validation Loss: 2185.243896484375\n",
      "Epoch [198/250], Training Loss: 119.26701208922577, Validation Loss: 2190.706787109375\n",
      "Epoch [199/250], Training Loss: 114.64953045996693, Validation Loss: 2198.267578125\n",
      "Epoch [200/250], Training Loss: 110.08399363905062, Validation Loss: 2200.007080078125\n",
      "Epoch [201/250], Training Loss: 106.11863173435927, Validation Loss: 2202.07373046875\n",
      "Epoch [202/250], Training Loss: 101.77215414699242, Validation Loss: 2206.6796875\n",
      "Epoch [203/250], Training Loss: 97.56669777059153, Validation Loss: 2226.521240234375\n",
      "Epoch [204/250], Training Loss: 96.38730167294861, Validation Loss: 2286.984130859375\n",
      "Epoch [205/250], Training Loss: 86.36010281187369, Validation Loss: 2375.265869140625\n",
      "Epoch [206/250], Training Loss: 82.66567289376266, Validation Loss: 2492.383544921875\n",
      "Epoch [207/250], Training Loss: 81.4646587883865, Validation Loss: 2530.808349609375\n",
      "Epoch [208/250], Training Loss: 79.13946522197355, Validation Loss: 2552.432373046875\n",
      "Epoch [209/250], Training Loss: 77.40636773889068, Validation Loss: 2621.794189453125\n",
      "Epoch [210/250], Training Loss: 75.65709577724596, Validation Loss: 2680.12548828125\n",
      "Epoch [211/250], Training Loss: 73.87724832151736, Validation Loss: 2773.523193359375\n",
      "Epoch [212/250], Training Loss: 71.90509891329648, Validation Loss: 2887.65283203125\n",
      "Epoch [213/250], Training Loss: 70.81864908917291, Validation Loss: 3057.199462890625\n",
      "Epoch [214/250], Training Loss: 69.71685787831748, Validation Loss: 3104.056884765625\n",
      "Epoch [215/250], Training Loss: 69.1948991427614, Validation Loss: 3263.7568359375\n",
      "Epoch [216/250], Training Loss: 68.05119437859827, Validation Loss: 3306.9814453125\n",
      "Epoch [217/250], Training Loss: 66.7377488759705, Validation Loss: 3309.028076171875\n",
      "Epoch [218/250], Training Loss: 65.74393243536521, Validation Loss: 3434.814453125\n",
      "Epoch [219/250], Training Loss: 64.62460614559792, Validation Loss: 3564.84716796875\n",
      "Epoch [220/250], Training Loss: 63.57591228167934, Validation Loss: 3712.018310546875\n",
      "Epoch [221/250], Training Loss: 62.81297330466389, Validation Loss: 3910.13818359375\n",
      "Epoch [222/250], Training Loss: 61.447249046893205, Validation Loss: 4061.698486328125\n",
      "Epoch [223/250], Training Loss: 59.17227749249261, Validation Loss: 4376.41845703125\n",
      "Epoch [224/250], Training Loss: 60.4241893285089, Validation Loss: 3640.019287109375\n",
      "Epoch [225/250], Training Loss: 59.43096060068572, Validation Loss: 3592.37060546875\n",
      "Epoch [226/250], Training Loss: 65.86061702400737, Validation Loss: 3908.592041015625\n",
      "Epoch [227/250], Training Loss: 75.8098101972238, Validation Loss: 4465.32666015625\n",
      "Epoch [228/250], Training Loss: 93.52884580741232, Validation Loss: 4676.69677734375\n",
      "Epoch [229/250], Training Loss: 57.614378376194836, Validation Loss: 4593.0546875\n",
      "Epoch [230/250], Training Loss: 63.455041939267865, Validation Loss: 4785.96240234375\n",
      "Epoch [231/250], Training Loss: 55.339343350106205, Validation Loss: 5118.57373046875\n",
      "Epoch [232/250], Training Loss: 55.112515996556574, Validation Loss: 5283.72021484375\n",
      "Epoch [233/250], Training Loss: 60.51909979607816, Validation Loss: 5096.57373046875\n",
      "Epoch [234/250], Training Loss: 46.59287677355302, Validation Loss: 4979.75634765625\n",
      "Epoch [235/250], Training Loss: 67.21093269451929, Validation Loss: 5286.33154296875\n",
      "Epoch [236/250], Training Loss: 41.79880475008, Validation Loss: 4642.28369140625\n",
      "Epoch [237/250], Training Loss: 67.11552630649558, Validation Loss: 5898.72998046875\n",
      "Epoch [238/250], Training Loss: 41.94150686692917, Validation Loss: 5197.38330078125\n",
      "Epoch [239/250], Training Loss: 42.99417519405636, Validation Loss: 5354.07421875\n",
      "Epoch [240/250], Training Loss: 44.48481754146371, Validation Loss: 5402.9111328125\n",
      "Epoch [241/250], Training Loss: 166.2164472668495, Validation Loss: 4666.05615234375\n",
      "Epoch [242/250], Training Loss: 102.5035320167542, Validation Loss: 4541.8984375\n",
      "Epoch [243/250], Training Loss: 39.23383669049742, Validation Loss: 4318.3056640625\n",
      "Epoch [244/250], Training Loss: 41.4490319399865, Validation Loss: 4519.62939453125\n",
      "Epoch [245/250], Training Loss: 43.18034838685793, Validation Loss: 4332.0703125\n",
      "Epoch [246/250], Training Loss: 163.8523339229661, Validation Loss: 3808.730224609375\n",
      "Epoch [247/250], Training Loss: 55.36802883136866, Validation Loss: 3444.414794921875\n",
      "Epoch [248/250], Training Loss: 48.988506746853155, Validation Loss: 3256.09228515625\n",
      "Epoch [249/250], Training Loss: 34.71096436182754, Validation Loss: 3265.168212890625\n",
      "Epoch [250/250], Training Loss: 34.813855734555766, Validation Loss: 3195.435302734375\n",
      "Test Loss: 2833.73876953125\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.01, 5)\n",
      "Epoch [1/250], Training Loss: 14443.755563255232, Validation Loss: 18777.943359375\n",
      "Epoch [2/250], Training Loss: 15916.05505463977, Validation Loss: 18683.451171875\n",
      "Epoch [3/250], Training Loss: 12252.888942962487, Validation Loss: 14936.4765625\n",
      "Epoch [4/250], Training Loss: 10375.17125722409, Validation Loss: 11271.8466796875\n",
      "Epoch [5/250], Training Loss: 8789.35903847909, Validation Loss: 12039.8427734375\n",
      "Epoch [6/250], Training Loss: 7577.635638326498, Validation Loss: 10064.703125\n",
      "Epoch [7/250], Training Loss: 6699.047348618932, Validation Loss: 8629.23046875\n",
      "Epoch [8/250], Training Loss: 5813.271376681775, Validation Loss: 7329.310546875\n",
      "Epoch [9/250], Training Loss: 5361.504529339735, Validation Loss: 6368.83056640625\n",
      "Epoch [10/250], Training Loss: 4446.3812614316275, Validation Loss: 5565.24658203125\n",
      "Epoch [11/250], Training Loss: 3842.8730082839566, Validation Loss: 4981.173828125\n",
      "Epoch [12/250], Training Loss: 3315.2717864022957, Validation Loss: 4555.583984375\n",
      "Epoch [13/250], Training Loss: 2867.5316253359856, Validation Loss: 4262.17822265625\n",
      "Epoch [14/250], Training Loss: 2492.935734124544, Validation Loss: 4499.056640625\n",
      "Epoch [15/250], Training Loss: 2184.3291310853474, Validation Loss: 4142.8486328125\n",
      "Epoch [16/250], Training Loss: 1923.2140484600009, Validation Loss: 3936.288330078125\n",
      "Epoch [17/250], Training Loss: 1692.8336561131857, Validation Loss: 3843.8359375\n",
      "Epoch [18/250], Training Loss: 1499.1144117408205, Validation Loss: 3803.36083984375\n",
      "Epoch [19/250], Training Loss: 1335.9593158460298, Validation Loss: 3740.8115234375\n",
      "Epoch [20/250], Training Loss: 1194.3250945997972, Validation Loss: 3645.515625\n",
      "Epoch [21/250], Training Loss: 1068.8308237928961, Validation Loss: 3576.72509765625\n",
      "Epoch [22/250], Training Loss: 957.1715994948255, Validation Loss: 3503.69970703125\n",
      "Epoch [23/250], Training Loss: 858.6810048884751, Validation Loss: 3446.79052734375\n",
      "Epoch [24/250], Training Loss: 771.4581349985617, Validation Loss: 3400.5244140625\n",
      "Epoch [25/250], Training Loss: 684.3033714817924, Validation Loss: 80465.015625\n",
      "Epoch [26/250], Training Loss: 2374.987331387079, Validation Loss: 4607.79931640625\n",
      "Epoch [27/250], Training Loss: 596.0202443992092, Validation Loss: 4212.30029296875\n",
      "Epoch [28/250], Training Loss: 441.5263528192643, Validation Loss: 1918.0191650390625\n",
      "Epoch [29/250], Training Loss: 656.5554411304327, Validation Loss: 1813.377685546875\n",
      "Epoch [30/250], Training Loss: 348.43183138679825, Validation Loss: 3193.719970703125\n",
      "Epoch [31/250], Training Loss: 542.6232587142607, Validation Loss: 2351.30615234375\n",
      "Epoch [32/250], Training Loss: 550.3581123333904, Validation Loss: 10773.0087890625\n",
      "Epoch [33/250], Training Loss: 509.87839111421573, Validation Loss: 5370.82421875\n",
      "Epoch [34/250], Training Loss: 238.53778841145575, Validation Loss: 4300.11279296875\n",
      "Epoch [35/250], Training Loss: 251.25942609488777, Validation Loss: 2687.004150390625\n",
      "Epoch [36/250], Training Loss: 332.2237568703854, Validation Loss: 2393.69287109375\n",
      "Epoch [37/250], Training Loss: 270.6391240971628, Validation Loss: 1788.988037109375\n",
      "Epoch [38/250], Training Loss: 217.0387427576123, Validation Loss: 3342.54736328125\n",
      "Epoch [39/250], Training Loss: 184.75902980246798, Validation Loss: 2679.70947265625\n",
      "Epoch [40/250], Training Loss: 268.69365874603744, Validation Loss: 3405.968994140625\n",
      "Epoch [41/250], Training Loss: 392.77641565901513, Validation Loss: 2281.97119140625\n",
      "Epoch [42/250], Training Loss: 171.52617898545918, Validation Loss: 2515.661865234375\n",
      "Epoch [43/250], Training Loss: 273.95423647229956, Validation Loss: 1713.84716796875\n",
      "Epoch [44/250], Training Loss: 145.79312495301025, Validation Loss: 3509.689208984375\n",
      "Epoch [45/250], Training Loss: 379.7912492407889, Validation Loss: 2726.86962890625\n",
      "Epoch [46/250], Training Loss: 331.22874865372756, Validation Loss: 2562.1435546875\n",
      "Epoch [47/250], Training Loss: 187.9191572211065, Validation Loss: 4445.5380859375\n",
      "Epoch [48/250], Training Loss: 97.02201155099041, Validation Loss: 4168.95458984375\n",
      "Epoch [49/250], Training Loss: 212.9220309460478, Validation Loss: 9419.923828125\n",
      "Epoch [50/250], Training Loss: 132.2222473005531, Validation Loss: 3994.9326171875\n",
      "Epoch [51/250], Training Loss: 247.47772627902535, Validation Loss: 3849.5458984375\n",
      "Epoch [52/250], Training Loss: 270.1578028519707, Validation Loss: 3858.418701171875\n",
      "Epoch [53/250], Training Loss: 388.4276543228085, Validation Loss: 1409.1943359375\n",
      "Epoch [54/250], Training Loss: 273.3718096323837, Validation Loss: 3456.336181640625\n",
      "Epoch [55/250], Training Loss: 363.4221548411773, Validation Loss: 3339.5537109375\n",
      "Epoch [56/250], Training Loss: 299.4567423607339, Validation Loss: 2533.216796875\n",
      "Epoch [57/250], Training Loss: 287.82089766336657, Validation Loss: 1349.326171875\n",
      "Epoch [58/250], Training Loss: 235.7982772288732, Validation Loss: 4700.2275390625\n",
      "Epoch [59/250], Training Loss: 267.75068842830547, Validation Loss: 4265.87353515625\n",
      "Epoch [60/250], Training Loss: 236.88828891584842, Validation Loss: 2068.627685546875\n",
      "Epoch [61/250], Training Loss: 143.96987366380006, Validation Loss: 3266.547119140625\n",
      "Epoch [62/250], Training Loss: 140.0293371524667, Validation Loss: 7122.43017578125\n",
      "Epoch [63/250], Training Loss: 256.1482597183685, Validation Loss: 1860.562255859375\n",
      "Epoch [64/250], Training Loss: 181.98649279746778, Validation Loss: 1125.0572509765625\n",
      "Epoch [65/250], Training Loss: 235.49069040446832, Validation Loss: 3839.6572265625\n",
      "Epoch [66/250], Training Loss: 527.256243853575, Validation Loss: 7503.84814453125\n",
      "Epoch [67/250], Training Loss: 368.47004074197974, Validation Loss: 8580.1201171875\n",
      "Epoch [68/250], Training Loss: 480.52887606004026, Validation Loss: 8825.28125\n",
      "Epoch [69/250], Training Loss: 957.013151159755, Validation Loss: 9565.9541015625\n",
      "Epoch [70/250], Training Loss: 1836.5226032451742, Validation Loss: 15720.037109375\n",
      "Epoch [71/250], Training Loss: 1998.8907419587224, Validation Loss: 10170.955078125\n",
      "Epoch [72/250], Training Loss: 2244.81323075724, Validation Loss: 10844.7021484375\n",
      "Epoch [73/250], Training Loss: 2925.9421129994053, Validation Loss: 11932.8095703125\n",
      "Epoch [74/250], Training Loss: 2125.533133483767, Validation Loss: 15026.2958984375\n",
      "Epoch [75/250], Training Loss: 2635.8631713914588, Validation Loss: 31820.966796875\n",
      "Epoch [76/250], Training Loss: 1562.0902199851541, Validation Loss: 37669.55859375\n",
      "Epoch [77/250], Training Loss: 1208.239668081011, Validation Loss: 19302.287109375\n",
      "Epoch [78/250], Training Loss: 2517.1099480178373, Validation Loss: 14839.486328125\n",
      "Epoch [79/250], Training Loss: 1316.8758185997274, Validation Loss: 10813.9501953125\n",
      "Epoch [80/250], Training Loss: 918.9804618920923, Validation Loss: 11466.8515625\n",
      "Epoch [81/250], Training Loss: 717.8483414229194, Validation Loss: 11614.3212890625\n",
      "Epoch [82/250], Training Loss: 1126.584097125035, Validation Loss: 28335.27734375\n",
      "Epoch [83/250], Training Loss: 1999.9230149321786, Validation Loss: 16287.9208984375\n",
      "Epoch [84/250], Training Loss: 691.4860510876181, Validation Loss: 19171.39453125\n",
      "Epoch [85/250], Training Loss: 1615.3499959814872, Validation Loss: 17542.3984375\n",
      "Epoch [86/250], Training Loss: 1843.0081484608986, Validation Loss: 19348.20703125\n",
      "Epoch [87/250], Training Loss: 3787.2717274887673, Validation Loss: 9956.873046875\n",
      "Epoch [88/250], Training Loss: 1494.0639989533506, Validation Loss: 16798.42578125\n",
      "Epoch [89/250], Training Loss: 1658.024783262265, Validation Loss: 13837.5576171875\n",
      "Epoch [90/250], Training Loss: 1122.2097326192563, Validation Loss: 10596.474609375\n",
      "Epoch [91/250], Training Loss: 1193.9171455982741, Validation Loss: 16454.75390625\n",
      "Epoch [92/250], Training Loss: 3029.040463150047, Validation Loss: 15347.1875\n",
      "Epoch [93/250], Training Loss: 1419.090248265403, Validation Loss: 17772.4375\n",
      "Epoch [94/250], Training Loss: 2677.7309931808622, Validation Loss: 14460.8076171875\n",
      "Epoch [95/250], Training Loss: 1724.138462403757, Validation Loss: 11158.0498046875\n",
      "Epoch [96/250], Training Loss: 1905.5338907041214, Validation Loss: 16282.28125\n",
      "Epoch [97/250], Training Loss: 1721.9968282785455, Validation Loss: 14325.0244140625\n",
      "Epoch [98/250], Training Loss: 3375.102244217188, Validation Loss: 14880.109375\n",
      "Epoch [99/250], Training Loss: 1097.2197140431892, Validation Loss: 31454.033203125\n",
      "Epoch [100/250], Training Loss: 1467.9151385008588, Validation Loss: 12232.9892578125\n",
      "Epoch [101/250], Training Loss: 1450.9546137022187, Validation Loss: 9516.2412109375\n",
      "Epoch [102/250], Training Loss: 1505.9456114803454, Validation Loss: 7628.23779296875\n",
      "Epoch [103/250], Training Loss: 939.4578319197387, Validation Loss: 39936.5703125\n",
      "Epoch [104/250], Training Loss: 794.992170058643, Validation Loss: 28486.69140625\n",
      "Epoch [105/250], Training Loss: 599.6280765801914, Validation Loss: 27555.8203125\n",
      "Epoch [106/250], Training Loss: 316.70720351496584, Validation Loss: 19982.4921875\n",
      "Epoch [107/250], Training Loss: 474.68814672688313, Validation Loss: 16448.646484375\n",
      "Epoch [108/250], Training Loss: 262.38076660201847, Validation Loss: 18610.923828125\n",
      "Epoch [109/250], Training Loss: 463.75942130312626, Validation Loss: 19573.03125\n",
      "Epoch [110/250], Training Loss: 453.8479347415063, Validation Loss: 13545.197265625\n",
      "Epoch [111/250], Training Loss: 343.68631629554386, Validation Loss: 15227.7197265625\n",
      "Epoch [112/250], Training Loss: 290.95389346303256, Validation Loss: 24255.505859375\n",
      "Epoch [113/250], Training Loss: 259.810793851225, Validation Loss: 15278.57421875\n",
      "Epoch [114/250], Training Loss: 247.02191592684332, Validation Loss: 6992.673828125\n",
      "Epoch [115/250], Training Loss: 144.75656637131442, Validation Loss: 21739.935546875\n",
      "Epoch [116/250], Training Loss: 373.98671805441603, Validation Loss: 19408.671875\n",
      "Epoch [117/250], Training Loss: 200.3383489369454, Validation Loss: 4940.5888671875\n",
      "Epoch [118/250], Training Loss: 215.66197435381082, Validation Loss: 24640.828125\n",
      "Epoch [119/250], Training Loss: 290.6230443218848, Validation Loss: 20451.697265625\n",
      "Epoch [120/250], Training Loss: 263.51846322575375, Validation Loss: 24478.7265625\n",
      "Epoch [121/250], Training Loss: 408.8641133451805, Validation Loss: 2519.81298828125\n",
      "Epoch [122/250], Training Loss: 343.33946454944754, Validation Loss: 15286.0078125\n",
      "Epoch [123/250], Training Loss: 277.28421162182764, Validation Loss: 4189.0966796875\n",
      "Epoch [124/250], Training Loss: 356.885171073113, Validation Loss: 3884.93603515625\n",
      "Epoch [125/250], Training Loss: 287.58932834497665, Validation Loss: 2959.347412109375\n",
      "Epoch [126/250], Training Loss: 260.1457595290306, Validation Loss: 1615.6029052734375\n",
      "Epoch [127/250], Training Loss: 159.95998694444566, Validation Loss: 2206.33837890625\n",
      "Epoch [128/250], Training Loss: 265.3061242539405, Validation Loss: 9323.26171875\n",
      "Epoch [129/250], Training Loss: 132.1697559943075, Validation Loss: 1990.2447509765625\n",
      "Epoch [130/250], Training Loss: 179.66649533915103, Validation Loss: 938.50439453125\n",
      "Epoch [131/250], Training Loss: 98.18374925265763, Validation Loss: 1715.79052734375\n",
      "Epoch [132/250], Training Loss: 209.1413903540991, Validation Loss: 1727.65283203125\n",
      "Epoch [133/250], Training Loss: 104.26210780911002, Validation Loss: 1743.355712890625\n",
      "Epoch [134/250], Training Loss: 174.01981518327284, Validation Loss: 2063.61328125\n",
      "Epoch [135/250], Training Loss: 239.76906837759267, Validation Loss: 2356.092041015625\n",
      "Epoch [136/250], Training Loss: 141.51213490147367, Validation Loss: 2299.950927734375\n",
      "Epoch [137/250], Training Loss: 120.18415481765142, Validation Loss: 1093.1063232421875\n",
      "Epoch [138/250], Training Loss: 76.26554270265595, Validation Loss: 2206.447021484375\n",
      "Epoch [139/250], Training Loss: 103.33454878279235, Validation Loss: 4113.22705078125\n",
      "Epoch [140/250], Training Loss: 170.76637367003133, Validation Loss: 3684.18310546875\n",
      "Epoch [141/250], Training Loss: 158.2873235441124, Validation Loss: 1588.795654296875\n",
      "Epoch [142/250], Training Loss: 83.94127236371622, Validation Loss: 1939.278564453125\n",
      "Epoch [143/250], Training Loss: 171.30823692073622, Validation Loss: 36909.80859375\n",
      "Epoch [144/250], Training Loss: 367.4293940518605, Validation Loss: 14864.66015625\n",
      "Epoch [145/250], Training Loss: 190.06662643905764, Validation Loss: 2360.9931640625\n",
      "Epoch [146/250], Training Loss: 151.5387071313185, Validation Loss: 2304.304931640625\n",
      "Epoch [147/250], Training Loss: 161.44093617115473, Validation Loss: 1994.027587890625\n",
      "Epoch [148/250], Training Loss: 130.5004413041402, Validation Loss: 4548.7998046875\n",
      "Epoch [149/250], Training Loss: 312.2275830999663, Validation Loss: 10924.5478515625\n",
      "Epoch [150/250], Training Loss: 134.34085231196497, Validation Loss: 1411.118896484375\n",
      "Epoch [151/250], Training Loss: 79.72173008175618, Validation Loss: 2053.726318359375\n",
      "Epoch [152/250], Training Loss: 210.18856913829114, Validation Loss: 3251.667724609375\n",
      "Epoch [153/250], Training Loss: 97.30087940603228, Validation Loss: 2026.453369140625\n",
      "Epoch [154/250], Training Loss: 225.4085056108376, Validation Loss: 3850.17138671875\n",
      "Epoch [155/250], Training Loss: 138.1426152528162, Validation Loss: 1974.97412109375\n",
      "Epoch [156/250], Training Loss: 105.16162907843247, Validation Loss: 1499.5078125\n",
      "Epoch [157/250], Training Loss: 64.18221039243059, Validation Loss: 2455.357177734375\n",
      "Epoch [158/250], Training Loss: 305.84921256138546, Validation Loss: 1669.69189453125\n",
      "Epoch [159/250], Training Loss: 107.6261823386199, Validation Loss: 1615.89990234375\n",
      "Epoch [160/250], Training Loss: 419.33515118113206, Validation Loss: 2306.461181640625\n",
      "Epoch [161/250], Training Loss: 202.30266289844803, Validation Loss: 3834.5576171875\n",
      "Epoch [162/250], Training Loss: 672.7489897723663, Validation Loss: 3185.287109375\n",
      "Epoch [163/250], Training Loss: 502.5627529001843, Validation Loss: 23075.37109375\n",
      "Epoch [164/250], Training Loss: 403.26340521794066, Validation Loss: 3188.00830078125\n",
      "Epoch [165/250], Training Loss: 129.63992153053775, Validation Loss: 3218.197509765625\n",
      "Epoch [166/250], Training Loss: 200.31786514754748, Validation Loss: 3308.211181640625\n",
      "Epoch [167/250], Training Loss: 130.42226410726627, Validation Loss: 19101.333984375\n",
      "Epoch [168/250], Training Loss: 164.9384076028322, Validation Loss: 1222.8450927734375\n",
      "Epoch [169/250], Training Loss: 75.81419359538444, Validation Loss: 1376.1258544921875\n",
      "Epoch [170/250], Training Loss: 275.6013151427382, Validation Loss: 15244.013671875\n",
      "Epoch [171/250], Training Loss: 90.35903832257227, Validation Loss: 3500.045654296875\n",
      "Epoch [172/250], Training Loss: 125.78431422528234, Validation Loss: 2673.903076171875\n",
      "Epoch [173/250], Training Loss: 66.73753206978641, Validation Loss: 2789.89404296875\n",
      "Epoch [174/250], Training Loss: 190.2084786535805, Validation Loss: 3113.6015625\n",
      "Epoch [175/250], Training Loss: 207.87650714298408, Validation Loss: 2395.651123046875\n",
      "Epoch [176/250], Training Loss: 80.40419885108048, Validation Loss: 2110.215087890625\n",
      "Epoch [177/250], Training Loss: 531.7147549799575, Validation Loss: 29059.94921875\n",
      "Epoch [178/250], Training Loss: 669.7374810932982, Validation Loss: 2869.567626953125\n",
      "Epoch [179/250], Training Loss: 146.91085370824678, Validation Loss: 2697.79150390625\n",
      "Epoch [180/250], Training Loss: 191.01015373797006, Validation Loss: 19072.248046875\n",
      "Epoch [181/250], Training Loss: 377.37861978579286, Validation Loss: 1956.208251953125\n",
      "Epoch [182/250], Training Loss: 166.86001278457715, Validation Loss: 1782.317138671875\n",
      "Epoch [183/250], Training Loss: 69.50535811125066, Validation Loss: 2184.556640625\n",
      "Epoch [184/250], Training Loss: 205.11056845249507, Validation Loss: 19872.8203125\n",
      "Epoch [185/250], Training Loss: 99.5540925030284, Validation Loss: 1604.6761474609375\n",
      "Epoch [186/250], Training Loss: 41.964358542080845, Validation Loss: 2890.33203125\n",
      "Epoch [187/250], Training Loss: 22.273255889888205, Validation Loss: 2872.027099609375\n",
      "Epoch [188/250], Training Loss: 23.257016835322307, Validation Loss: 2806.18310546875\n",
      "Epoch [189/250], Training Loss: 26.679378256778193, Validation Loss: 4551.673828125\n",
      "Epoch [190/250], Training Loss: 28.67847310868609, Validation Loss: 5430.056640625\n",
      "Epoch [191/250], Training Loss: 36.16560559238497, Validation Loss: 6068.56396484375\n",
      "Epoch [192/250], Training Loss: 39.13423768446991, Validation Loss: 7019.57666015625\n",
      "Epoch [193/250], Training Loss: 45.39598414397056, Validation Loss: 6901.48193359375\n",
      "Epoch [194/250], Training Loss: 45.04886948741593, Validation Loss: 6939.767578125\n",
      "Epoch [195/250], Training Loss: 44.591633669836895, Validation Loss: 6806.15771484375\n",
      "Epoch [196/250], Training Loss: 46.95425007197816, Validation Loss: 6540.0546875\n",
      "Epoch [197/250], Training Loss: 47.47656271523888, Validation Loss: 5993.3291015625\n",
      "Epoch [198/250], Training Loss: 44.17796843415108, Validation Loss: 5888.0830078125\n",
      "Epoch [199/250], Training Loss: 42.910874144713944, Validation Loss: 6090.662109375\n",
      "Epoch [200/250], Training Loss: 47.95475326850345, Validation Loss: 6148.58251953125\n",
      "Epoch [201/250], Training Loss: 51.0316272478989, Validation Loss: 6359.74755859375\n",
      "Epoch [202/250], Training Loss: 58.448709672962295, Validation Loss: 6428.18408203125\n",
      "Epoch [203/250], Training Loss: 1165.5554840129803, Validation Loss: 2654.942626953125\n",
      "Epoch [204/250], Training Loss: 556.8902152951141, Validation Loss: 4049.0634765625\n",
      "Epoch [205/250], Training Loss: 435.61831433079925, Validation Loss: 2505.534423828125\n",
      "Epoch [206/250], Training Loss: 158.844600511967, Validation Loss: 2462.481201171875\n",
      "Epoch [207/250], Training Loss: 700.5588866464016, Validation Loss: 6809.34130859375\n",
      "Epoch [208/250], Training Loss: 316.3170287398291, Validation Loss: 3222.7646484375\n",
      "Epoch [209/250], Training Loss: 259.98456040315045, Validation Loss: 23540.76171875\n",
      "Epoch [210/250], Training Loss: 386.77670535003074, Validation Loss: 3173.322509765625\n",
      "Epoch [211/250], Training Loss: 246.4814307042319, Validation Loss: 7644.88623046875\n",
      "Epoch [212/250], Training Loss: 518.1421070262744, Validation Loss: 2563.909423828125\n",
      "Epoch [213/250], Training Loss: 174.83438239499287, Validation Loss: 3063.7431640625\n",
      "Epoch [214/250], Training Loss: 498.03383297372477, Validation Loss: 2225.9736328125\n",
      "Epoch [215/250], Training Loss: 134.34589068915938, Validation Loss: 2534.314697265625\n",
      "Epoch [216/250], Training Loss: 220.70333614592494, Validation Loss: 19757.615234375\n",
      "Epoch [217/250], Training Loss: 632.1444934132223, Validation Loss: 2682.24462890625\n",
      "Epoch [218/250], Training Loss: 179.9370225422556, Validation Loss: 2652.1025390625\n",
      "Epoch [219/250], Training Loss: 132.5906364490863, Validation Loss: 2320.21728515625\n",
      "Epoch [220/250], Training Loss: 228.7844734943124, Validation Loss: 3852.142333984375\n",
      "Epoch [221/250], Training Loss: 143.76594859953744, Validation Loss: 4757.34228515625\n",
      "Epoch [222/250], Training Loss: 78.23258703955089, Validation Loss: 6576.94140625\n",
      "Epoch [223/250], Training Loss: 535.014535051864, Validation Loss: 3104.3681640625\n",
      "Epoch [224/250], Training Loss: 139.24504247808636, Validation Loss: 2750.95068359375\n",
      "Epoch [225/250], Training Loss: 47.830450522376786, Validation Loss: 3005.47021484375\n",
      "Epoch [226/250], Training Loss: 1925.3383681050698, Validation Loss: 2340.00830078125\n",
      "Epoch [227/250], Training Loss: 186.10820008898528, Validation Loss: 2932.053466796875\n",
      "Epoch [228/250], Training Loss: 50.19289323797866, Validation Loss: 3293.222900390625\n",
      "Epoch [229/250], Training Loss: 2871.1946960138266, Validation Loss: 2325.440185546875\n",
      "Epoch [230/250], Training Loss: 192.9416318958631, Validation Loss: 2926.4384765625\n",
      "Epoch [231/250], Training Loss: 49.57701290322213, Validation Loss: 3321.436279296875\n",
      "Epoch [232/250], Training Loss: 1158.122960258748, Validation Loss: 14381.578125\n",
      "Epoch [233/250], Training Loss: 1216.7032281892002, Validation Loss: 6942.232421875\n",
      "Epoch [234/250], Training Loss: 294.1222489040989, Validation Loss: 4252.79052734375\n",
      "Epoch [235/250], Training Loss: 504.3873713148714, Validation Loss: 8625.1689453125\n",
      "Epoch [236/250], Training Loss: 2258.024456233343, Validation Loss: 3800.39794921875\n",
      "Epoch [237/250], Training Loss: 429.4233960612278, Validation Loss: 2525.05419921875\n",
      "Epoch [238/250], Training Loss: 1054.876695855536, Validation Loss: 4028.662353515625\n",
      "Epoch [239/250], Training Loss: 301.9650616932092, Validation Loss: 6121.40380859375\n",
      "Epoch [240/250], Training Loss: 214.1923161397627, Validation Loss: 2859.505126953125\n",
      "Epoch [241/250], Training Loss: 348.37313507024015, Validation Loss: 3493.427978515625\n",
      "Epoch [242/250], Training Loss: 390.9077864152346, Validation Loss: 2720.533203125\n",
      "Epoch [243/250], Training Loss: 307.7237402121162, Validation Loss: 21086.23828125\n",
      "Epoch [244/250], Training Loss: 320.0465016466982, Validation Loss: 2918.377685546875\n",
      "Epoch [245/250], Training Loss: 368.2326367567908, Validation Loss: 3772.140625\n",
      "Epoch [246/250], Training Loss: 131.01935879498097, Validation Loss: 2777.24169921875\n",
      "Epoch [247/250], Training Loss: 157.5030161392376, Validation Loss: 3016.849365234375\n",
      "Epoch [248/250], Training Loss: 660.3155809573586, Validation Loss: 3765.66064453125\n",
      "Epoch [249/250], Training Loss: 127.86740283165031, Validation Loss: 3632.3017578125\n",
      "Epoch [250/250], Training Loss: 369.72376377000006, Validation Loss: 3811.072265625\n",
      "Test Loss: 3792.685791015625\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.01, 10)\n",
      "Epoch [1/250], Training Loss: 14059.713561870674, Validation Loss: 19071.29296875\n",
      "Epoch [2/250], Training Loss: 8643.478865567557, Validation Loss: 19126.6796875\n",
      "Epoch [3/250], Training Loss: 5904.081107287947, Validation Loss: 18335.498046875\n",
      "Epoch [4/250], Training Loss: 4052.6638600424762, Validation Loss: 18333.177734375\n",
      "Epoch [5/250], Training Loss: 2666.9500981945653, Validation Loss: 18301.5546875\n",
      "Epoch [6/250], Training Loss: 1878.5461366188842, Validation Loss: 19363.8515625\n",
      "Epoch [7/250], Training Loss: 1345.4773892777125, Validation Loss: 20063.205078125\n",
      "Epoch [8/250], Training Loss: 999.0529016220472, Validation Loss: 19382.79296875\n",
      "Epoch [9/250], Training Loss: 739.3921870828599, Validation Loss: 19428.71875\n",
      "Epoch [10/250], Training Loss: 529.4605675511763, Validation Loss: 21163.390625\n",
      "Epoch [11/250], Training Loss: 370.72027716749346, Validation Loss: 19222.53125\n",
      "Epoch [12/250], Training Loss: 263.41198859786425, Validation Loss: 18965.732421875\n",
      "Epoch [13/250], Training Loss: 193.3780660969889, Validation Loss: 20054.1328125\n",
      "Epoch [14/250], Training Loss: 151.52176168446124, Validation Loss: 19446.4609375\n",
      "Epoch [15/250], Training Loss: 105.4823782791375, Validation Loss: 19756.451171875\n",
      "Epoch [16/250], Training Loss: 214.96541565204222, Validation Loss: 18842.833984375\n",
      "Epoch [17/250], Training Loss: 88.99281730867838, Validation Loss: 21316.919921875\n",
      "Epoch [18/250], Training Loss: 216.1424941369438, Validation Loss: 18931.291015625\n",
      "Epoch [19/250], Training Loss: 82.55313531030276, Validation Loss: 21609.322265625\n",
      "Epoch [20/250], Training Loss: 49.6177921302036, Validation Loss: 20976.65234375\n",
      "Epoch [21/250], Training Loss: 1243.2351992861468, Validation Loss: 18600.19921875\n",
      "Epoch [22/250], Training Loss: 7279.0230226465455, Validation Loss: 18898.41015625\n",
      "Epoch [23/250], Training Loss: 3809.2601254898655, Validation Loss: 43263.07421875\n",
      "Epoch [24/250], Training Loss: 4721.446155359926, Validation Loss: 20302.91015625\n",
      "Epoch [25/250], Training Loss: 178.2802551979453, Validation Loss: 19238.87890625\n",
      "Epoch [26/250], Training Loss: 21.91555090009452, Validation Loss: 19187.697265625\n",
      "Epoch [27/250], Training Loss: 23.025445957979127, Validation Loss: 20447.33203125\n",
      "Epoch [28/250], Training Loss: 21.895321346150297, Validation Loss: 20951.34765625\n",
      "Epoch [29/250], Training Loss: 27.410819841391923, Validation Loss: 20585.279296875\n",
      "Epoch [30/250], Training Loss: 21.320447663868503, Validation Loss: 20542.427734375\n",
      "Epoch [31/250], Training Loss: 121.89528417389162, Validation Loss: 20946.125\n",
      "Epoch [32/250], Training Loss: 16.852108906265233, Validation Loss: 20730.322265625\n",
      "Epoch [33/250], Training Loss: 11.758446889000885, Validation Loss: 21289.373046875\n",
      "Epoch [34/250], Training Loss: 11.536107634401334, Validation Loss: 21997.892578125\n",
      "Epoch [35/250], Training Loss: 1058.663416732423, Validation Loss: 22813.060546875\n",
      "Epoch [36/250], Training Loss: 150.04794072590943, Validation Loss: 18467.017578125\n",
      "Epoch [37/250], Training Loss: 32.56523669080887, Validation Loss: 20545.505859375\n",
      "Epoch [38/250], Training Loss: 11.176182465294467, Validation Loss: 21483.9375\n",
      "Epoch [39/250], Training Loss: 11.5458655587468, Validation Loss: 22008.1875\n",
      "Epoch [40/250], Training Loss: 32.322204160869695, Validation Loss: 21815.619140625\n",
      "Epoch [41/250], Training Loss: 12.2340217704855, Validation Loss: 22207.28515625\n",
      "Epoch [42/250], Training Loss: 15.258493987493383, Validation Loss: 22179.224609375\n",
      "Epoch [43/250], Training Loss: 30.107200191585964, Validation Loss: 20850.796875\n",
      "Epoch [44/250], Training Loss: 19.952986453130677, Validation Loss: 21469.677734375\n",
      "Epoch [45/250], Training Loss: 13.497350554292026, Validation Loss: 21390.853515625\n",
      "Epoch [46/250], Training Loss: 17.30448909852429, Validation Loss: 22286.984375\n",
      "Epoch [47/250], Training Loss: 21.42108090735383, Validation Loss: 22801.708984375\n",
      "Epoch [48/250], Training Loss: 20.58380963331156, Validation Loss: 22357.005859375\n",
      "Epoch [49/250], Training Loss: 23.92050264565107, Validation Loss: 21107.56640625\n",
      "Epoch [50/250], Training Loss: 23.205237248091805, Validation Loss: 21416.529296875\n",
      "Epoch [51/250], Training Loss: 21.518565207533534, Validation Loss: 21498.28125\n",
      "Epoch [52/250], Training Loss: 28.8982749104135, Validation Loss: 21383.521484375\n",
      "Epoch [53/250], Training Loss: 26.722569157994933, Validation Loss: 21474.015625\n",
      "Epoch [54/250], Training Loss: 29.719451015614585, Validation Loss: 21650.654296875\n",
      "Epoch [55/250], Training Loss: 195.6800708181319, Validation Loss: 20786.609375\n",
      "Epoch [56/250], Training Loss: 50.66577253753678, Validation Loss: 21402.4375\n",
      "Epoch [57/250], Training Loss: 37.10874699639528, Validation Loss: 21142.10546875\n",
      "Epoch [58/250], Training Loss: 837.721966689321, Validation Loss: 22448.37109375\n",
      "Epoch [59/250], Training Loss: 2904.329552837155, Validation Loss: 19080.740234375\n",
      "Epoch [60/250], Training Loss: 578.3205623924362, Validation Loss: 18444.1484375\n",
      "Epoch [61/250], Training Loss: 50.804880804444075, Validation Loss: 21005.44140625\n",
      "Epoch [62/250], Training Loss: 277.8738641294007, Validation Loss: 21660.888671875\n",
      "Epoch [63/250], Training Loss: 1956.5168882981911, Validation Loss: 20186.6328125\n",
      "Epoch [64/250], Training Loss: 747.6520594981793, Validation Loss: 18517.953125\n",
      "Epoch [65/250], Training Loss: 1145.6560835678608, Validation Loss: 19507.734375\n",
      "Epoch [66/250], Training Loss: 52.479952819983694, Validation Loss: 20941.80859375\n",
      "Epoch [67/250], Training Loss: 19.452214488873476, Validation Loss: 21219.486328125\n",
      "Epoch [68/250], Training Loss: 21.689318165177543, Validation Loss: 21855.5078125\n",
      "Epoch [69/250], Training Loss: 1256.4345767060554, Validation Loss: 19397.599609375\n",
      "Epoch [70/250], Training Loss: 276.0529935072656, Validation Loss: 19458.814453125\n",
      "Epoch [71/250], Training Loss: 60.940646335543995, Validation Loss: 22159.162109375\n",
      "Epoch [72/250], Training Loss: 17.88593066715633, Validation Loss: 22002.27734375\n",
      "Epoch [73/250], Training Loss: 18.590832010860254, Validation Loss: 22056.388671875\n",
      "Epoch [74/250], Training Loss: 996.8550146182187, Validation Loss: 19916.375\n",
      "Epoch [75/250], Training Loss: 944.2122711048073, Validation Loss: 18709.771484375\n",
      "Epoch [76/250], Training Loss: 336.76768485319496, Validation Loss: 19399.22265625\n",
      "Epoch [77/250], Training Loss: 63.64427836209332, Validation Loss: 19336.294921875\n",
      "Epoch [78/250], Training Loss: 19.12401621940629, Validation Loss: 19930.15234375\n",
      "Epoch [79/250], Training Loss: 17.149597179082317, Validation Loss: 20439.927734375\n",
      "Epoch [80/250], Training Loss: 282.60668933447926, Validation Loss: 21127.23828125\n",
      "Epoch [81/250], Training Loss: 42.22985695864256, Validation Loss: 21386.3359375\n",
      "Epoch [82/250], Training Loss: 461.30587990232186, Validation Loss: 20361.98046875\n",
      "Epoch [83/250], Training Loss: 93.76357875736348, Validation Loss: 20491.595703125\n",
      "Epoch [84/250], Training Loss: 23.737093744220864, Validation Loss: 22268.552734375\n",
      "Epoch [85/250], Training Loss: 21.341480435067535, Validation Loss: 22105.505859375\n",
      "Epoch [86/250], Training Loss: 41.47522460554724, Validation Loss: 22414.32421875\n",
      "Epoch [87/250], Training Loss: 29.44489509339485, Validation Loss: 21437.998046875\n",
      "Epoch [88/250], Training Loss: 33.36862669503551, Validation Loss: 21160.0546875\n",
      "Epoch [89/250], Training Loss: 571.076486389171, Validation Loss: 19165.337890625\n",
      "Epoch [90/250], Training Loss: 48.780334185588174, Validation Loss: 21762.0546875\n",
      "Epoch [91/250], Training Loss: 63.90963889320703, Validation Loss: 20381.185546875\n",
      "Epoch [92/250], Training Loss: 24.73594588732489, Validation Loss: 21595.86328125\n",
      "Epoch [93/250], Training Loss: 43.76373933029577, Validation Loss: 21451.271484375\n",
      "Epoch [94/250], Training Loss: 21.05769018149467, Validation Loss: 22190.943359375\n",
      "Epoch [95/250], Training Loss: 23.628118784544913, Validation Loss: 21525.900390625\n",
      "Epoch [96/250], Training Loss: 25.112728817812098, Validation Loss: 20953.673828125\n",
      "Epoch [97/250], Training Loss: 72.22693482140119, Validation Loss: 19744.46484375\n",
      "Epoch [98/250], Training Loss: 19.42741982734087, Validation Loss: 20745.798828125\n",
      "Epoch [99/250], Training Loss: 23.875719146919813, Validation Loss: 21927.2578125\n",
      "Epoch [100/250], Training Loss: 34.210647011000454, Validation Loss: 22536.556640625\n",
      "Epoch [101/250], Training Loss: 53.83918331464748, Validation Loss: 20246.615234375\n",
      "Epoch [102/250], Training Loss: 31.106841034952023, Validation Loss: 21361.654296875\n",
      "Epoch [103/250], Training Loss: 49.50061369990892, Validation Loss: 20272.59765625\n",
      "Epoch [104/250], Training Loss: 51.28069269049092, Validation Loss: 19990.298828125\n",
      "Epoch [105/250], Training Loss: 27.44426014317977, Validation Loss: 21802.421875\n",
      "Epoch [106/250], Training Loss: 21.68748643202387, Validation Loss: 21103.720703125\n",
      "Epoch [107/250], Training Loss: 575.2787516803673, Validation Loss: 19428.5078125\n",
      "Epoch [108/250], Training Loss: 66.89263540807471, Validation Loss: 21805.45703125\n",
      "Epoch [109/250], Training Loss: 28.329538425524646, Validation Loss: 21555.2578125\n",
      "Epoch [110/250], Training Loss: 37.34605969697148, Validation Loss: 22577.595703125\n",
      "Epoch [111/250], Training Loss: 31.852405729121546, Validation Loss: 22628.408203125\n",
      "Epoch [112/250], Training Loss: 31.995660942804005, Validation Loss: 22323.994140625\n",
      "Epoch [113/250], Training Loss: 39.11052583031652, Validation Loss: 21957.4296875\n",
      "Epoch [114/250], Training Loss: 52.02329440456701, Validation Loss: 19793.15234375\n",
      "Epoch [115/250], Training Loss: 37.346269967662735, Validation Loss: 20737.751953125\n",
      "Epoch [116/250], Training Loss: 28.824670760442007, Validation Loss: 21169.861328125\n",
      "Epoch [117/250], Training Loss: 38.04351958256165, Validation Loss: 22254.81640625\n",
      "Epoch [118/250], Training Loss: 324.7599668199107, Validation Loss: 18428.65234375\n",
      "Epoch [119/250], Training Loss: 65.25726086174576, Validation Loss: 20948.958984375\n",
      "Epoch [120/250], Training Loss: 36.26307576195678, Validation Loss: 21571.75390625\n",
      "Epoch [121/250], Training Loss: 3199.078986013838, Validation Loss: 18931.271484375\n",
      "Epoch [122/250], Training Loss: 2982.3451434418157, Validation Loss: 19430.76953125\n",
      "Epoch [123/250], Training Loss: 3582.610992421022, Validation Loss: 19545.181640625\n",
      "Epoch [124/250], Training Loss: 2343.0972847938615, Validation Loss: 22224.32421875\n",
      "Epoch [125/250], Training Loss: 619.4352187794018, Validation Loss: 19439.62890625\n",
      "Epoch [126/250], Training Loss: 176.81105522275047, Validation Loss: 20361.537109375\n",
      "Epoch [127/250], Training Loss: 103.46705328137605, Validation Loss: 20465.16015625\n",
      "Epoch [128/250], Training Loss: 51.10888068636636, Validation Loss: 21347.47265625\n",
      "Epoch [129/250], Training Loss: 25.63252508615962, Validation Loss: 21335.767578125\n",
      "Epoch [130/250], Training Loss: 58.742768615432226, Validation Loss: 21959.12890625\n",
      "Epoch [131/250], Training Loss: 1304.138145893019, Validation Loss: 18595.625\n",
      "Epoch [132/250], Training Loss: 2882.774503279708, Validation Loss: 18973.365234375\n",
      "Epoch [133/250], Training Loss: 671.6263678083736, Validation Loss: 19967.724609375\n",
      "Epoch [134/250], Training Loss: 49.45321835819188, Validation Loss: 19016.76953125\n",
      "Epoch [135/250], Training Loss: 56.90662095713306, Validation Loss: 20293.09765625\n",
      "Epoch [136/250], Training Loss: 45.983214211325155, Validation Loss: 22115.65625\n",
      "Epoch [137/250], Training Loss: 98.70110578775812, Validation Loss: 20009.16015625\n",
      "Epoch [138/250], Training Loss: 28.084803319017333, Validation Loss: 23873.19140625\n",
      "Epoch [139/250], Training Loss: 57.75651335594369, Validation Loss: 20427.974609375\n",
      "Epoch [140/250], Training Loss: 30.645470185820006, Validation Loss: 20660.98828125\n",
      "Epoch [141/250], Training Loss: 40.69453809498756, Validation Loss: 19481.498046875\n",
      "Epoch [142/250], Training Loss: 27.148917253796675, Validation Loss: 21035.236328125\n",
      "Epoch [143/250], Training Loss: 2578.320216006929, Validation Loss: 19020.50390625\n",
      "Epoch [144/250], Training Loss: 1777.3131423059742, Validation Loss: 20869.95703125\n",
      "Epoch [145/250], Training Loss: 256.8265995046982, Validation Loss: 21722.349609375\n",
      "Epoch [146/250], Training Loss: 30.727281616332395, Validation Loss: 21744.669921875\n",
      "Epoch [147/250], Training Loss: 89.37801472226856, Validation Loss: 20052.658203125\n",
      "Epoch [148/250], Training Loss: 1450.2305617549189, Validation Loss: 21487.65625\n",
      "Epoch [149/250], Training Loss: 92.19569138054601, Validation Loss: 23250.814453125\n",
      "Epoch [150/250], Training Loss: 289.0630235742763, Validation Loss: 21135.115234375\n",
      "Epoch [151/250], Training Loss: 38.64866043990672, Validation Loss: 20773.3671875\n",
      "Epoch [152/250], Training Loss: 19.728010379267083, Validation Loss: 20726.412109375\n",
      "Epoch [153/250], Training Loss: 40.13513815469817, Validation Loss: 22375.5\n",
      "Epoch [154/250], Training Loss: 291.4298358808307, Validation Loss: 19276.9375\n",
      "Epoch [155/250], Training Loss: 4654.398376012893, Validation Loss: 20281.767578125\n",
      "Epoch [156/250], Training Loss: 2796.1727139905593, Validation Loss: 18933.91015625\n",
      "Epoch [157/250], Training Loss: 174.41250291386748, Validation Loss: 19815.162109375\n",
      "Epoch [158/250], Training Loss: 28.377260983905337, Validation Loss: 21584.5\n",
      "Epoch [159/250], Training Loss: 21.25389501106575, Validation Loss: 22385.982421875\n",
      "Epoch [160/250], Training Loss: 20.21834201348989, Validation Loss: 21797.056640625\n",
      "Epoch [161/250], Training Loss: 20.898595443077745, Validation Loss: 22040.845703125\n",
      "Epoch [162/250], Training Loss: 19.42220961344376, Validation Loss: 22035.669921875\n",
      "Epoch [163/250], Training Loss: 20.00885672907491, Validation Loss: 21642.03125\n",
      "Epoch [164/250], Training Loss: 1353.7019432389566, Validation Loss: 23425.88671875\n",
      "Epoch [165/250], Training Loss: 321.983330916398, Validation Loss: 19030.7265625\n",
      "Epoch [166/250], Training Loss: 356.71883217691703, Validation Loss: 20588.24609375\n",
      "Epoch [167/250], Training Loss: 34.815344849433515, Validation Loss: 21456.609375\n",
      "Epoch [168/250], Training Loss: 20.178616878753193, Validation Loss: 22065.1875\n",
      "Epoch [169/250], Training Loss: 19.573546440139474, Validation Loss: 22393.1484375\n",
      "Epoch [170/250], Training Loss: 15.998044356674518, Validation Loss: 22641.609375\n",
      "Epoch [171/250], Training Loss: 515.4528472309377, Validation Loss: 22169.564453125\n",
      "Epoch [172/250], Training Loss: 28.362893806573705, Validation Loss: 22825.232421875\n",
      "Epoch [173/250], Training Loss: 18.03024306872025, Validation Loss: 22989.77734375\n",
      "Epoch [174/250], Training Loss: 1420.3430831458973, Validation Loss: 23432.12109375\n",
      "Epoch [175/250], Training Loss: 953.1255383201035, Validation Loss: 16006.9716796875\n",
      "Epoch [176/250], Training Loss: 83.49514741067439, Validation Loss: 19277.90234375\n",
      "Epoch [177/250], Training Loss: 35.955081855501504, Validation Loss: 20084.615234375\n",
      "Epoch [178/250], Training Loss: 19.01457582007911, Validation Loss: 21129.494140625\n",
      "Epoch [179/250], Training Loss: 15.403190180943726, Validation Loss: 20942.7421875\n",
      "Epoch [180/250], Training Loss: 17.54147548322276, Validation Loss: 21751.416015625\n",
      "Epoch [181/250], Training Loss: 35.22385110976198, Validation Loss: 22437.990234375\n",
      "Epoch [182/250], Training Loss: 28.645597204786792, Validation Loss: 22079.8046875\n",
      "Epoch [183/250], Training Loss: 37.980256042740095, Validation Loss: 21534.546875\n",
      "Epoch [184/250], Training Loss: 27.518245286883438, Validation Loss: 21399.154296875\n",
      "Epoch [185/250], Training Loss: 2589.440663255561, Validation Loss: 21424.72265625\n",
      "Epoch [186/250], Training Loss: 1910.9347272540879, Validation Loss: 18521.505859375\n",
      "Epoch [187/250], Training Loss: 361.30414230687535, Validation Loss: 19845.736328125\n",
      "Epoch [188/250], Training Loss: 42.469770242733816, Validation Loss: 21237.39453125\n",
      "Epoch [189/250], Training Loss: 22.067497880781264, Validation Loss: 22381.0625\n",
      "Epoch [190/250], Training Loss: 25.989131973066645, Validation Loss: 22549.404296875\n",
      "Epoch [191/250], Training Loss: 44.928811158228804, Validation Loss: 20859.666015625\n",
      "Epoch [192/250], Training Loss: 21.027077739079584, Validation Loss: 20771.60546875\n",
      "Epoch [193/250], Training Loss: 20.740142836897544, Validation Loss: 21942.4453125\n",
      "Epoch [194/250], Training Loss: 25.61517018210097, Validation Loss: 22408.92578125\n",
      "Epoch [195/250], Training Loss: 28.799884932457715, Validation Loss: 22021.05078125\n",
      "Epoch [196/250], Training Loss: 1526.29117376743, Validation Loss: 19700.435546875\n",
      "Epoch [197/250], Training Loss: 1103.6011377506327, Validation Loss: 18360.2890625\n",
      "Epoch [198/250], Training Loss: 408.36920386506694, Validation Loss: 20370.77734375\n",
      "Epoch [199/250], Training Loss: 34.30093628977187, Validation Loss: 21725.658203125\n",
      "Epoch [200/250], Training Loss: 32.24018338047114, Validation Loss: 20564.447265625\n",
      "Epoch [201/250], Training Loss: 22.679078251512117, Validation Loss: 20715.146484375\n",
      "Epoch [202/250], Training Loss: 84.65576367328062, Validation Loss: 20301.1484375\n",
      "Epoch [203/250], Training Loss: 20.01017963527381, Validation Loss: 21513.919921875\n",
      "Epoch [204/250], Training Loss: 18.539273435730045, Validation Loss: 20864.68359375\n",
      "Epoch [205/250], Training Loss: 19.503578088301687, Validation Loss: 21703.734375\n",
      "Epoch [206/250], Training Loss: 27.45365824490935, Validation Loss: 22504.599609375\n",
      "Epoch [207/250], Training Loss: 48.26028603373475, Validation Loss: 21115.697265625\n",
      "Epoch [208/250], Training Loss: 3182.907811092535, Validation Loss: 21789.025390625\n",
      "Epoch [209/250], Training Loss: 4198.493583178575, Validation Loss: 26317.658203125\n",
      "Epoch [210/250], Training Loss: 2100.0543459727346, Validation Loss: 19148.4609375\n",
      "Epoch [211/250], Training Loss: 4326.216337200278, Validation Loss: 19182.66015625\n",
      "Epoch [212/250], Training Loss: 3597.290686091638, Validation Loss: 18868.9609375\n",
      "Epoch [213/250], Training Loss: 1000.7357043478053, Validation Loss: 20108.353515625\n",
      "Epoch [214/250], Training Loss: 1898.4430189289258, Validation Loss: 25546.95703125\n",
      "Epoch [215/250], Training Loss: 18308.66071373573, Validation Loss: 18930.189453125\n",
      "Epoch [216/250], Training Loss: 3014.8246104790524, Validation Loss: 24944.556640625\n",
      "Epoch [217/250], Training Loss: 10047.946202071693, Validation Loss: 43377.046875\n",
      "Epoch [218/250], Training Loss: 27323.315013058043, Validation Loss: 41358.48046875\n",
      "Epoch [219/250], Training Loss: 8864.888244345108, Validation Loss: 30939.62890625\n",
      "Epoch [220/250], Training Loss: 9513.918446660075, Validation Loss: 21730.97265625\n",
      "Epoch [221/250], Training Loss: 4138.351094347631, Validation Loss: 17569.85546875\n",
      "Epoch [222/250], Training Loss: 6370.212628298297, Validation Loss: 20439.3984375\n",
      "Epoch [223/250], Training Loss: 203.00470223208677, Validation Loss: 19105.2734375\n",
      "Epoch [224/250], Training Loss: 600.8153263119003, Validation Loss: 20607.037109375\n",
      "Epoch [225/250], Training Loss: 930.0321210935721, Validation Loss: 18597.00390625\n",
      "Epoch [226/250], Training Loss: 150.97839453748728, Validation Loss: 20516.501953125\n",
      "Epoch [227/250], Training Loss: 415.67085696438403, Validation Loss: 21949.189453125\n",
      "Epoch [228/250], Training Loss: 794.3944424635276, Validation Loss: 19087.04296875\n",
      "Epoch [229/250], Training Loss: 2364.7497442931344, Validation Loss: 19273.578125\n",
      "Epoch [230/250], Training Loss: 3800.722371418781, Validation Loss: 17800.064453125\n",
      "Epoch [231/250], Training Loss: 3398.9527549621635, Validation Loss: 19604.267578125\n",
      "Epoch [232/250], Training Loss: 1981.1010082021523, Validation Loss: 17315.998046875\n",
      "Epoch [233/250], Training Loss: 1916.0053235823743, Validation Loss: 19096.873046875\n",
      "Epoch [234/250], Training Loss: 1318.0632032137235, Validation Loss: 19988.84375\n",
      "Epoch [235/250], Training Loss: 2479.72738387422, Validation Loss: 19357.015625\n",
      "Epoch [236/250], Training Loss: 2201.771661704265, Validation Loss: 20637.0859375\n",
      "Epoch [237/250], Training Loss: 2735.7438697029006, Validation Loss: 25248.95703125\n",
      "Epoch [238/250], Training Loss: 2002.5389204157045, Validation Loss: 20974.845703125\n",
      "Epoch [239/250], Training Loss: 1101.3162643757662, Validation Loss: 19151.68359375\n",
      "Epoch [240/250], Training Loss: 2682.7021999321532, Validation Loss: 19225.634765625\n",
      "Epoch [241/250], Training Loss: 996.8344868815725, Validation Loss: 21850.798828125\n",
      "Epoch [242/250], Training Loss: 731.9642469387004, Validation Loss: 19398.53515625\n",
      "Epoch [243/250], Training Loss: 2001.6623135584787, Validation Loss: 23321.154296875\n",
      "Epoch [244/250], Training Loss: 185.34332650471262, Validation Loss: 19272.05859375\n",
      "Epoch [245/250], Training Loss: 83.02631994857671, Validation Loss: 19211.57421875\n",
      "Epoch [246/250], Training Loss: 39.18521439290071, Validation Loss: 20048.02734375\n",
      "Epoch [247/250], Training Loss: 24.398392390697516, Validation Loss: 19968.685546875\n",
      "Epoch [248/250], Training Loss: 2353.940498499513, Validation Loss: 22872.806640625\n",
      "Epoch [249/250], Training Loss: 3188.31711677732, Validation Loss: 20345.546875\n",
      "Epoch [250/250], Training Loss: 2465.8200209764223, Validation Loss: 21787.716796875\n",
      "Test Loss: 19310.484375\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.01, 15)\n",
      "Epoch [1/250], Training Loss: 14065.970936808419, Validation Loss: 18102.48046875\n",
      "Epoch [2/250], Training Loss: 8675.33042372525, Validation Loss: 11195.9697265625\n",
      "Epoch [3/250], Training Loss: 5723.846488130005, Validation Loss: 10272.9931640625\n",
      "Epoch [4/250], Training Loss: 3892.2093866032646, Validation Loss: 10643.544921875\n",
      "Epoch [5/250], Training Loss: 2609.0983790243454, Validation Loss: 8898.0947265625\n",
      "Epoch [6/250], Training Loss: 1838.078527625644, Validation Loss: 7145.80029296875\n",
      "Epoch [7/250], Training Loss: 1352.1142790345882, Validation Loss: 6678.19775390625\n",
      "Epoch [8/250], Training Loss: 1021.3639192681827, Validation Loss: 5896.57763671875\n",
      "Epoch [9/250], Training Loss: 768.8097257161986, Validation Loss: 6010.6396484375\n",
      "Epoch [10/250], Training Loss: 586.0601419729795, Validation Loss: 7104.84814453125\n",
      "Epoch [11/250], Training Loss: 441.537026480407, Validation Loss: 7657.01123046875\n",
      "Epoch [12/250], Training Loss: 370.02455203301434, Validation Loss: 9133.9580078125\n",
      "Epoch [13/250], Training Loss: 286.73112425139107, Validation Loss: 6866.07861328125\n",
      "Epoch [14/250], Training Loss: 290.7220006409282, Validation Loss: 14799.70703125\n",
      "Epoch [15/250], Training Loss: 256.7537192054169, Validation Loss: 13814.505859375\n",
      "Epoch [16/250], Training Loss: 185.73848061934345, Validation Loss: 7999.76611328125\n",
      "Epoch [17/250], Training Loss: 205.44002765311046, Validation Loss: 16143.4052734375\n",
      "Epoch [18/250], Training Loss: 172.37024432211604, Validation Loss: 16642.984375\n",
      "Epoch [19/250], Training Loss: 158.37401628021846, Validation Loss: 15696.46484375\n",
      "Epoch [20/250], Training Loss: 150.47901733740014, Validation Loss: 15172.9345703125\n",
      "Epoch [21/250], Training Loss: 145.38096046033965, Validation Loss: 16699.65625\n",
      "Epoch [22/250], Training Loss: 159.8190732213149, Validation Loss: 7220.6728515625\n",
      "Epoch [23/250], Training Loss: 141.1617761624663, Validation Loss: 17861.130859375\n",
      "Epoch [24/250], Training Loss: 132.30794175082872, Validation Loss: 6801.53955078125\n",
      "Epoch [25/250], Training Loss: 126.71053587570664, Validation Loss: 12413.126953125\n",
      "Epoch [26/250], Training Loss: 113.34668295725167, Validation Loss: 10255.818359375\n",
      "Epoch [27/250], Training Loss: 101.15575969135129, Validation Loss: 16667.83203125\n",
      "Epoch [28/250], Training Loss: 78.16152640910178, Validation Loss: 8644.8310546875\n",
      "Epoch [29/250], Training Loss: 100.9852481672601, Validation Loss: 9494.1943359375\n",
      "Epoch [30/250], Training Loss: 159.71240108493683, Validation Loss: 8516.0546875\n",
      "Epoch [31/250], Training Loss: 3517.9776905323893, Validation Loss: 10561.3525390625\n",
      "Epoch [32/250], Training Loss: 825.775041568533, Validation Loss: 8758.0263671875\n",
      "Epoch [33/250], Training Loss: 110.56241658917267, Validation Loss: 8518.5009765625\n",
      "Epoch [34/250], Training Loss: 73.59765288558695, Validation Loss: 7881.87255859375\n",
      "Epoch [35/250], Training Loss: 733.7405329915615, Validation Loss: 7963.296875\n",
      "Epoch [36/250], Training Loss: 445.6983253286378, Validation Loss: 34616.59375\n",
      "Epoch [37/250], Training Loss: 842.9567014858441, Validation Loss: 12229.6123046875\n",
      "Epoch [38/250], Training Loss: 877.6980438437361, Validation Loss: 9116.294921875\n",
      "Epoch [39/250], Training Loss: 2839.8749348020847, Validation Loss: 11299.8662109375\n",
      "Epoch [40/250], Training Loss: 462.96635662277396, Validation Loss: 19919.78515625\n",
      "Epoch [41/250], Training Loss: 48.56571853901375, Validation Loss: 18410.267578125\n",
      "Epoch [42/250], Training Loss: 37.12444872750736, Validation Loss: 12887.841796875\n",
      "Epoch [43/250], Training Loss: 42.51215898253933, Validation Loss: 12317.8701171875\n",
      "Epoch [44/250], Training Loss: 39.90229533909798, Validation Loss: 10679.4072265625\n",
      "Epoch [45/250], Training Loss: 46.33692501995993, Validation Loss: 10964.529296875\n",
      "Epoch [46/250], Training Loss: 58.96369503331487, Validation Loss: 9984.837890625\n",
      "Epoch [47/250], Training Loss: 172.319677646144, Validation Loss: 6210.89111328125\n",
      "Epoch [48/250], Training Loss: 83.39197396339549, Validation Loss: 8486.7119140625\n",
      "Epoch [49/250], Training Loss: 46.567681379956234, Validation Loss: 7071.01513671875\n",
      "Epoch [50/250], Training Loss: 137.57555140817453, Validation Loss: 13648.7041015625\n",
      "Epoch [51/250], Training Loss: 340.94213274202417, Validation Loss: 17004.669921875\n",
      "Epoch [52/250], Training Loss: 971.0361420618104, Validation Loss: 17106.384765625\n",
      "Epoch [53/250], Training Loss: 65.91146450784174, Validation Loss: 11873.544921875\n",
      "Epoch [54/250], Training Loss: 37.802327631435745, Validation Loss: 12399.0234375\n",
      "Epoch [55/250], Training Loss: 45.931601276612575, Validation Loss: 8981.384765625\n",
      "Epoch [56/250], Training Loss: 701.9422045695594, Validation Loss: 18416.103515625\n",
      "Epoch [57/250], Training Loss: 201.01689244586632, Validation Loss: 22149.556640625\n",
      "Epoch [58/250], Training Loss: 8966.700792124142, Validation Loss: 19075.384765625\n",
      "Epoch [59/250], Training Loss: 5952.7441256626835, Validation Loss: 23372.544921875\n",
      "Epoch [60/250], Training Loss: 38791.2769315553, Validation Loss: 27676.904296875\n",
      "Epoch [61/250], Training Loss: 14158.835402505161, Validation Loss: 13416.951171875\n",
      "Epoch [62/250], Training Loss: 8520.905573762704, Validation Loss: 16893.90234375\n",
      "Epoch [63/250], Training Loss: 7644.961592406624, Validation Loss: 19268.1015625\n",
      "Epoch [64/250], Training Loss: 2947.366856236281, Validation Loss: 21792.7578125\n",
      "Epoch [65/250], Training Loss: 1465.507986879742, Validation Loss: 7501.5556640625\n",
      "Epoch [66/250], Training Loss: 526.871293146147, Validation Loss: 26083.265625\n",
      "Epoch [67/250], Training Loss: 979.0474489094794, Validation Loss: 17717.951171875\n",
      "Epoch [68/250], Training Loss: 268.2341685079828, Validation Loss: 8386.494140625\n",
      "Epoch [69/250], Training Loss: 175.44088138260767, Validation Loss: 7046.1787109375\n",
      "Epoch [70/250], Training Loss: 94.23063976449433, Validation Loss: 5901.833984375\n",
      "Epoch [71/250], Training Loss: 100.59835025543434, Validation Loss: 6636.85107421875\n",
      "Epoch [72/250], Training Loss: 61.473764964163855, Validation Loss: 6530.2607421875\n",
      "Epoch [73/250], Training Loss: 146.57620881606567, Validation Loss: 8718.8720703125\n",
      "Epoch [74/250], Training Loss: 127.79206357333949, Validation Loss: 8272.193359375\n",
      "Epoch [75/250], Training Loss: 140.8117540135736, Validation Loss: 11245.6181640625\n",
      "Epoch [76/250], Training Loss: 84.27126652457316, Validation Loss: 7607.09765625\n",
      "Epoch [77/250], Training Loss: 242.08139278735243, Validation Loss: 8437.8603515625\n",
      "Epoch [78/250], Training Loss: 175.64646812643417, Validation Loss: 6410.869140625\n",
      "Epoch [79/250], Training Loss: 128.56663783151478, Validation Loss: 7345.96484375\n",
      "Epoch [80/250], Training Loss: 305.21337077800246, Validation Loss: 8614.7880859375\n",
      "Epoch [81/250], Training Loss: 159.54629782150133, Validation Loss: 15128.8798828125\n",
      "Epoch [82/250], Training Loss: 91.0149911522778, Validation Loss: 8647.18359375\n",
      "Epoch [83/250], Training Loss: 98.34846528356499, Validation Loss: 6981.22119140625\n",
      "Epoch [84/250], Training Loss: 316.8088685422182, Validation Loss: 8487.12109375\n",
      "Epoch [85/250], Training Loss: 94.54953600877751, Validation Loss: 9044.89453125\n",
      "Epoch [86/250], Training Loss: 735.8840382571001, Validation Loss: 10182.263671875\n",
      "Epoch [87/250], Training Loss: 872.7618177355808, Validation Loss: 11958.642578125\n",
      "Epoch [88/250], Training Loss: 758.8121700449477, Validation Loss: 14498.7978515625\n",
      "Epoch [89/250], Training Loss: 331.92988139207716, Validation Loss: 8708.8974609375\n",
      "Epoch [90/250], Training Loss: 67.95401090882449, Validation Loss: 6872.30029296875\n",
      "Epoch [91/250], Training Loss: 64.68210231778528, Validation Loss: 6450.529296875\n",
      "Epoch [92/250], Training Loss: 71.08530387808348, Validation Loss: 5907.435546875\n",
      "Epoch [93/250], Training Loss: 391.9016368917873, Validation Loss: 8882.4345703125\n",
      "Epoch [94/250], Training Loss: 108.4730172408477, Validation Loss: 5994.7802734375\n",
      "Epoch [95/250], Training Loss: 165.93663273400182, Validation Loss: 4923.421875\n",
      "Epoch [96/250], Training Loss: 82.65144606177361, Validation Loss: 5831.9345703125\n",
      "Epoch [97/250], Training Loss: 45.97552607417234, Validation Loss: 5367.3935546875\n",
      "Epoch [98/250], Training Loss: 493.67493780620856, Validation Loss: 5534.4453125\n",
      "Epoch [99/250], Training Loss: 168.60010234779799, Validation Loss: 5415.39794921875\n",
      "Epoch [100/250], Training Loss: 216.91841341036329, Validation Loss: 5542.89697265625\n",
      "Epoch [101/250], Training Loss: 60.00238224796435, Validation Loss: 4570.064453125\n",
      "Epoch [102/250], Training Loss: 43.8353964538163, Validation Loss: 4601.123046875\n",
      "Epoch [103/250], Training Loss: 47.80886442182479, Validation Loss: 5291.05517578125\n",
      "Epoch [104/250], Training Loss: 38.96043519436154, Validation Loss: 6087.44580078125\n",
      "Epoch [105/250], Training Loss: 58.69699550209186, Validation Loss: 6418.9140625\n",
      "Epoch [106/250], Training Loss: 44.73435149410775, Validation Loss: 6305.947265625\n",
      "Epoch [107/250], Training Loss: 158.95237621876473, Validation Loss: 5553.6904296875\n",
      "Epoch [108/250], Training Loss: 247.61286287471205, Validation Loss: 5552.9833984375\n",
      "Epoch [109/250], Training Loss: 436.4524892961211, Validation Loss: 5178.72802734375\n",
      "Epoch [110/250], Training Loss: 235.58710022960815, Validation Loss: 5143.49169921875\n",
      "Epoch [111/250], Training Loss: 52.401587888458195, Validation Loss: 5181.1982421875\n",
      "Epoch [112/250], Training Loss: 46.83725903916243, Validation Loss: 5608.9736328125\n",
      "Epoch [113/250], Training Loss: 67.30311621069555, Validation Loss: 5727.0224609375\n",
      "Epoch [114/250], Training Loss: 67.73946232830362, Validation Loss: 5892.2099609375\n",
      "Epoch [115/250], Training Loss: 44.69653617425226, Validation Loss: 5624.3173828125\n",
      "Epoch [116/250], Training Loss: 672.249733235824, Validation Loss: 5144.833984375\n",
      "Epoch [117/250], Training Loss: 191.39056921848953, Validation Loss: 5992.15625\n",
      "Epoch [118/250], Training Loss: 51.18289406351202, Validation Loss: 6263.41845703125\n",
      "Epoch [119/250], Training Loss: 33.72985732626058, Validation Loss: 6063.13330078125\n",
      "Epoch [120/250], Training Loss: 386.27853524723145, Validation Loss: 10877.736328125\n",
      "Epoch [121/250], Training Loss: 144.34553571167578, Validation Loss: 5003.6826171875\n",
      "Epoch [122/250], Training Loss: 72.22715178657309, Validation Loss: 4868.22607421875\n",
      "Epoch [123/250], Training Loss: 72.56894159210661, Validation Loss: 5954.4521484375\n",
      "Epoch [124/250], Training Loss: 58.96294654137749, Validation Loss: 5334.6259765625\n",
      "Epoch [125/250], Training Loss: 104.38258578928667, Validation Loss: 5822.09033203125\n",
      "Epoch [126/250], Training Loss: 80.2335603085581, Validation Loss: 4382.080078125\n",
      "Epoch [127/250], Training Loss: 105.33783172724533, Validation Loss: 4202.876953125\n",
      "Epoch [128/250], Training Loss: 75.02986939948994, Validation Loss: 5622.26318359375\n",
      "Epoch [129/250], Training Loss: 62.09709461093971, Validation Loss: 4964.8076171875\n",
      "Epoch [130/250], Training Loss: 67.91938509435968, Validation Loss: 5092.32763671875\n",
      "Epoch [131/250], Training Loss: 67.86114287571999, Validation Loss: 5167.80126953125\n",
      "Epoch [132/250], Training Loss: 63.691875664040644, Validation Loss: 5419.97998046875\n",
      "Epoch [133/250], Training Loss: 65.02034003705292, Validation Loss: 5311.23876953125\n",
      "Epoch [134/250], Training Loss: 108.42997335100594, Validation Loss: 4556.8134765625\n",
      "Epoch [135/250], Training Loss: 85.72519016487573, Validation Loss: 4426.28369140625\n",
      "Epoch [136/250], Training Loss: 133.37379287867213, Validation Loss: 4823.4345703125\n",
      "Epoch [137/250], Training Loss: 80.00920066539564, Validation Loss: 4515.90185546875\n",
      "Epoch [138/250], Training Loss: 146.91673167865247, Validation Loss: 5313.0791015625\n",
      "Epoch [139/250], Training Loss: 219.62065990928966, Validation Loss: 5480.6240234375\n",
      "Epoch [140/250], Training Loss: 993.6327887317228, Validation Loss: 12916.833984375\n",
      "Epoch [141/250], Training Loss: 115.15810987027939, Validation Loss: 10972.01171875\n",
      "Epoch [142/250], Training Loss: 114.14382049583747, Validation Loss: 4806.84326171875\n",
      "Epoch [143/250], Training Loss: 99.65415299040428, Validation Loss: 5536.12158203125\n",
      "Epoch [144/250], Training Loss: 82.89780294689197, Validation Loss: 5245.8251953125\n",
      "Epoch [145/250], Training Loss: 69.66304988006188, Validation Loss: 4722.52392578125\n",
      "Epoch [146/250], Training Loss: 54.91942657876071, Validation Loss: 5412.84521484375\n",
      "Epoch [147/250], Training Loss: 102.20337406052619, Validation Loss: 5340.375\n",
      "Epoch [148/250], Training Loss: 273.30273030820445, Validation Loss: 6326.32958984375\n",
      "Epoch [149/250], Training Loss: 162.29829276184643, Validation Loss: 9887.755859375\n",
      "Epoch [150/250], Training Loss: 80.3512358667346, Validation Loss: 7285.7587890625\n",
      "Epoch [151/250], Training Loss: 106.28653585812539, Validation Loss: 6457.84814453125\n",
      "Epoch [152/250], Training Loss: 65.4471688952034, Validation Loss: 6335.65576171875\n",
      "Epoch [153/250], Training Loss: 70.5711307325845, Validation Loss: 10096.056640625\n",
      "Epoch [154/250], Training Loss: 413.8740949100174, Validation Loss: 18481.533203125\n",
      "Epoch [155/250], Training Loss: 102.5668549933517, Validation Loss: 10229.0048828125\n",
      "Epoch [156/250], Training Loss: 98.24627151768192, Validation Loss: 14199.400390625\n",
      "Epoch [157/250], Training Loss: 117.27799789449412, Validation Loss: 13979.0390625\n",
      "Epoch [158/250], Training Loss: 309.71630307886625, Validation Loss: 8024.0869140625\n",
      "Epoch [159/250], Training Loss: 65.84429213402322, Validation Loss: 9875.4912109375\n",
      "Epoch [160/250], Training Loss: 59.308021976776445, Validation Loss: 5754.2841796875\n",
      "Epoch [161/250], Training Loss: 68.73410210365832, Validation Loss: 10281.921875\n",
      "Epoch [162/250], Training Loss: 69.26996718175388, Validation Loss: 10207.8056640625\n",
      "Epoch [163/250], Training Loss: 62.62587399989788, Validation Loss: 11151.2646484375\n",
      "Epoch [164/250], Training Loss: 64.11469334969598, Validation Loss: 10858.12890625\n",
      "Epoch [165/250], Training Loss: 64.45018801326486, Validation Loss: 12215.0810546875\n",
      "Epoch [166/250], Training Loss: 70.83847328718723, Validation Loss: 12879.708984375\n",
      "Epoch [167/250], Training Loss: 59.989336853021406, Validation Loss: 12990.6845703125\n",
      "Epoch [168/250], Training Loss: 81.8518294907625, Validation Loss: 11270.1982421875\n",
      "Epoch [169/250], Training Loss: 94.1334716342856, Validation Loss: 12535.919921875\n",
      "Epoch [170/250], Training Loss: 52.33934431553248, Validation Loss: 11392.7685546875\n",
      "Epoch [171/250], Training Loss: 45.793920448236904, Validation Loss: 13037.626953125\n",
      "Epoch [172/250], Training Loss: 48.50347151892903, Validation Loss: 13222.2744140625\n",
      "Epoch [173/250], Training Loss: 41.52951609497714, Validation Loss: 13760.392578125\n",
      "Epoch [174/250], Training Loss: 52.70690581877253, Validation Loss: 13230.423828125\n",
      "Epoch [175/250], Training Loss: 58.85293953210362, Validation Loss: 15178.353515625\n",
      "Epoch [176/250], Training Loss: 56.0347265994464, Validation Loss: 14641.8994140625\n",
      "Epoch [177/250], Training Loss: 255.1118532001544, Validation Loss: 15522.38671875\n",
      "Epoch [178/250], Training Loss: 67.02655110069256, Validation Loss: 16244.5888671875\n",
      "Epoch [179/250], Training Loss: 132.1759952062748, Validation Loss: 16715.15625\n",
      "Epoch [180/250], Training Loss: 194.67427309801363, Validation Loss: 15245.9228515625\n",
      "Epoch [181/250], Training Loss: 112.17610365116165, Validation Loss: 16683.84765625\n",
      "Epoch [182/250], Training Loss: 350.76834718904655, Validation Loss: 16969.060546875\n",
      "Epoch [183/250], Training Loss: 130.2271078664264, Validation Loss: 17896.5\n",
      "Epoch [184/250], Training Loss: 401.3673197847195, Validation Loss: 15308.4091796875\n",
      "Epoch [185/250], Training Loss: 384.0822573413842, Validation Loss: 13580.9365234375\n",
      "Epoch [186/250], Training Loss: 36.93707395422013, Validation Loss: 15158.7412109375\n",
      "Epoch [187/250], Training Loss: 14636.655074831528, Validation Loss: 20037.06640625\n",
      "Epoch [188/250], Training Loss: 9811.065621111391, Validation Loss: 13836.9501953125\n",
      "Epoch [189/250], Training Loss: 490.50932078863025, Validation Loss: 14083.6396484375\n",
      "Epoch [190/250], Training Loss: 146.309623331521, Validation Loss: 15411.7802734375\n",
      "Epoch [191/250], Training Loss: 66.05170968169548, Validation Loss: 16007.431640625\n",
      "Epoch [192/250], Training Loss: 266.8835822423474, Validation Loss: 16626.51953125\n",
      "Epoch [193/250], Training Loss: 409.4432121859602, Validation Loss: 16730.505859375\n",
      "Epoch [194/250], Training Loss: 249.69065042833128, Validation Loss: 16887.01171875\n",
      "Epoch [195/250], Training Loss: 51.24628925639913, Validation Loss: 16730.064453125\n",
      "Epoch [196/250], Training Loss: 160.93745957394822, Validation Loss: 17901.892578125\n",
      "Epoch [197/250], Training Loss: 57.87885657584687, Validation Loss: 19099.435546875\n",
      "Epoch [198/250], Training Loss: 244.84193830511984, Validation Loss: 18579.416015625\n",
      "Epoch [199/250], Training Loss: 88.03752316800684, Validation Loss: 19544.3203125\n",
      "Epoch [200/250], Training Loss: 48.40935332927205, Validation Loss: 18803.775390625\n",
      "Epoch [201/250], Training Loss: 131.73267677271613, Validation Loss: 19117.794921875\n",
      "Epoch [202/250], Training Loss: 77.27306713969945, Validation Loss: 16152.1162109375\n",
      "Epoch [203/250], Training Loss: 51.01110609618047, Validation Loss: 13436.5810546875\n",
      "Epoch [204/250], Training Loss: 1020.5352677162365, Validation Loss: 9861.7158203125\n",
      "Epoch [205/250], Training Loss: 160.8327179130602, Validation Loss: 11247.29296875\n",
      "Epoch [206/250], Training Loss: 82.80081227622696, Validation Loss: 11919.0986328125\n",
      "Epoch [207/250], Training Loss: 60.61104307119293, Validation Loss: 10628.994140625\n",
      "Epoch [208/250], Training Loss: 92.36073350738394, Validation Loss: 7946.23291015625\n",
      "Epoch [209/250], Training Loss: 49.18242723830491, Validation Loss: 9298.7109375\n",
      "Epoch [210/250], Training Loss: 53.95400761724894, Validation Loss: 7380.01708984375\n",
      "Epoch [211/250], Training Loss: 71.1436731898126, Validation Loss: 6329.79248046875\n",
      "Epoch [212/250], Training Loss: 209.7430360870114, Validation Loss: 9027.767578125\n",
      "Epoch [213/250], Training Loss: 73.75961932654599, Validation Loss: 7557.3515625\n",
      "Epoch [214/250], Training Loss: 53.073009451632885, Validation Loss: 7161.0966796875\n",
      "Epoch [215/250], Training Loss: 44.7005774901087, Validation Loss: 5472.19970703125\n",
      "Epoch [216/250], Training Loss: 212.34754293085334, Validation Loss: 5607.68408203125\n",
      "Epoch [217/250], Training Loss: 50.73610652051964, Validation Loss: 5451.466796875\n",
      "Epoch [218/250], Training Loss: 491.2965756202649, Validation Loss: 4417.72314453125\n",
      "Epoch [219/250], Training Loss: 51.20703147226215, Validation Loss: 6581.03125\n",
      "Epoch [220/250], Training Loss: 1816.2943179915599, Validation Loss: 27866.955078125\n",
      "Epoch [221/250], Training Loss: 570.5757768836698, Validation Loss: 20470.58984375\n",
      "Epoch [222/250], Training Loss: 295.3568268575235, Validation Loss: 7925.89453125\n",
      "Epoch [223/250], Training Loss: 584.8003731387637, Validation Loss: 19606.833984375\n",
      "Epoch [224/250], Training Loss: 96.33548101684057, Validation Loss: 6189.74169921875\n",
      "Epoch [225/250], Training Loss: 194.24788861721294, Validation Loss: 20388.81640625\n",
      "Epoch [226/250], Training Loss: 2353.3035155851285, Validation Loss: 8220.4580078125\n",
      "Epoch [227/250], Training Loss: 118.46391308082224, Validation Loss: 4812.5859375\n",
      "Epoch [228/250], Training Loss: 86.40679617455766, Validation Loss: 4899.62255859375\n",
      "Epoch [229/250], Training Loss: 932.1995007448751, Validation Loss: 7473.97705078125\n",
      "Epoch [230/250], Training Loss: 547.8713938081373, Validation Loss: 7509.85302734375\n",
      "Epoch [231/250], Training Loss: 583.9070881599775, Validation Loss: 7273.17626953125\n",
      "Epoch [232/250], Training Loss: 81.93945294458771, Validation Loss: 7536.498046875\n",
      "Epoch [233/250], Training Loss: 44.9022819198392, Validation Loss: 5637.63671875\n",
      "Epoch [234/250], Training Loss: 160.08340761996237, Validation Loss: 4403.50634765625\n",
      "Epoch [235/250], Training Loss: 94.73055077729667, Validation Loss: 4912.44970703125\n",
      "Epoch [236/250], Training Loss: 66.03416084583517, Validation Loss: 4612.47412109375\n",
      "Epoch [237/250], Training Loss: 86.39508081554287, Validation Loss: 4278.2939453125\n",
      "Epoch [238/250], Training Loss: 101.01276262756075, Validation Loss: 4906.32666015625\n",
      "Epoch [239/250], Training Loss: 178.72754138467488, Validation Loss: 4229.19482421875\n",
      "Epoch [240/250], Training Loss: 64.72418638430798, Validation Loss: 5652.86572265625\n",
      "Epoch [241/250], Training Loss: 147.38778923322874, Validation Loss: 4781.43896484375\n",
      "Epoch [242/250], Training Loss: 138.9659800933897, Validation Loss: 5687.625\n",
      "Epoch [243/250], Training Loss: 183.06890577342014, Validation Loss: 7568.400390625\n",
      "Epoch [244/250], Training Loss: 2917.279826709963, Validation Loss: 14475.50390625\n",
      "Epoch [245/250], Training Loss: 1951.9713359926436, Validation Loss: 13707.7880859375\n",
      "Epoch [246/250], Training Loss: 650.1358343877953, Validation Loss: 8698.2001953125\n",
      "Epoch [247/250], Training Loss: 1160.6386320236788, Validation Loss: 4920.42041015625\n",
      "Epoch [248/250], Training Loss: 1210.5063673617024, Validation Loss: 6403.03955078125\n",
      "Epoch [249/250], Training Loss: 4084.1649943875123, Validation Loss: 7719.62109375\n",
      "Epoch [250/250], Training Loss: 3164.534766301106, Validation Loss: 5063.6328125\n",
      "Test Loss: 4915.24560546875\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.01, 20)\n",
      "Epoch [1/250], Training Loss: 20214.334859576014, Validation Loss: 18782.302734375\n",
      "Epoch [2/250], Training Loss: 14868.253814783311, Validation Loss: 16407.255859375\n",
      "Epoch [3/250], Training Loss: 12313.894954525507, Validation Loss: 12847.5595703125\n",
      "Epoch [4/250], Training Loss: 10280.013031255201, Validation Loss: 10808.93359375\n",
      "Epoch [5/250], Training Loss: 8662.568075860849, Validation Loss: 10813.0869140625\n",
      "Epoch [6/250], Training Loss: 3521.7679866025273, Validation Loss: 14517.939453125\n",
      "Epoch [7/250], Training Loss: 2141.6514986266575, Validation Loss: 7464.46435546875\n",
      "Epoch [8/250], Training Loss: 1473.6325709028138, Validation Loss: 5084.1796875\n",
      "Epoch [9/250], Training Loss: 1112.931435967474, Validation Loss: 5707.40771484375\n",
      "Epoch [10/250], Training Loss: 851.2744600157158, Validation Loss: 5377.74267578125\n",
      "Epoch [11/250], Training Loss: 706.9319644052703, Validation Loss: 4995.478515625\n",
      "Epoch [12/250], Training Loss: 511.65206558145456, Validation Loss: 4927.791015625\n",
      "Epoch [13/250], Training Loss: 410.0105821875093, Validation Loss: 4644.978515625\n",
      "Epoch [14/250], Training Loss: 329.5563868687584, Validation Loss: 4418.5595703125\n",
      "Epoch [15/250], Training Loss: 241.73749688529546, Validation Loss: 4234.5556640625\n",
      "Epoch [16/250], Training Loss: 194.19987187336721, Validation Loss: 4207.916015625\n",
      "Epoch [17/250], Training Loss: 167.37148769225274, Validation Loss: 4095.131103515625\n",
      "Epoch [18/250], Training Loss: 136.3462610219749, Validation Loss: 4225.50146484375\n",
      "Epoch [19/250], Training Loss: 128.39377234277342, Validation Loss: 3976.20263671875\n",
      "Epoch [20/250], Training Loss: 116.29635432553339, Validation Loss: 3991.783935546875\n",
      "Epoch [21/250], Training Loss: 90.29695723825125, Validation Loss: 4008.021484375\n",
      "Epoch [22/250], Training Loss: 82.55766197898326, Validation Loss: 4322.478515625\n",
      "Epoch [23/250], Training Loss: 69.7876559867476, Validation Loss: 4310.8330078125\n",
      "Epoch [24/250], Training Loss: 65.00310815354983, Validation Loss: 4365.88623046875\n",
      "Epoch [25/250], Training Loss: 81.07186570646296, Validation Loss: 4384.5009765625\n",
      "Epoch [26/250], Training Loss: 57.266541998204, Validation Loss: 4782.26171875\n",
      "Epoch [27/250], Training Loss: 66.04034448591587, Validation Loss: 4837.6640625\n",
      "Epoch [28/250], Training Loss: 52.37827705513812, Validation Loss: 5177.9931640625\n",
      "Epoch [29/250], Training Loss: 58.29611393351845, Validation Loss: 5117.73095703125\n",
      "Epoch [30/250], Training Loss: 50.003247917819465, Validation Loss: 5193.67724609375\n",
      "Epoch [31/250], Training Loss: 57.93301215846574, Validation Loss: 5148.951171875\n",
      "Epoch [32/250], Training Loss: 48.52549365601278, Validation Loss: 5205.6611328125\n",
      "Epoch [33/250], Training Loss: 92.44540345988682, Validation Loss: 5137.92333984375\n",
      "Epoch [34/250], Training Loss: 57.50318129636849, Validation Loss: 5677.00732421875\n",
      "Epoch [35/250], Training Loss: 45.259485531217415, Validation Loss: 5468.34814453125\n",
      "Epoch [36/250], Training Loss: 51.16058903441226, Validation Loss: 5557.22021484375\n",
      "Epoch [37/250], Training Loss: 42.92650445075724, Validation Loss: 5670.54638671875\n",
      "Epoch [38/250], Training Loss: 41.7939007378282, Validation Loss: 5469.67578125\n",
      "Epoch [39/250], Training Loss: 69.04028502679526, Validation Loss: 5327.36962890625\n",
      "Epoch [40/250], Training Loss: 51.485342832112984, Validation Loss: 6071.76806640625\n",
      "Epoch [41/250], Training Loss: 41.378442632989554, Validation Loss: 5837.103515625\n",
      "Epoch [42/250], Training Loss: 68.78316348608472, Validation Loss: 5966.45751953125\n",
      "Epoch [43/250], Training Loss: 69.57543376841737, Validation Loss: 6163.462890625\n",
      "Epoch [44/250], Training Loss: 40.843874172213205, Validation Loss: 5945.791015625\n",
      "Epoch [45/250], Training Loss: 76.11785297328183, Validation Loss: 6180.94384765625\n",
      "Epoch [46/250], Training Loss: 40.58759237951563, Validation Loss: 5958.03369140625\n",
      "Epoch [47/250], Training Loss: 85.19152944516716, Validation Loss: 6228.21337890625\n",
      "Epoch [48/250], Training Loss: 39.66979720916064, Validation Loss: 6162.45458984375\n",
      "Epoch [49/250], Training Loss: 69.0247596785451, Validation Loss: 6164.05810546875\n",
      "Epoch [50/250], Training Loss: 49.69207734138176, Validation Loss: 6241.3056640625\n",
      "Epoch [51/250], Training Loss: 48.99272514154402, Validation Loss: 6379.14501953125\n",
      "Epoch [52/250], Training Loss: 79.02352177836393, Validation Loss: 6587.67919921875\n",
      "Epoch [53/250], Training Loss: 39.60142877363193, Validation Loss: 6447.8427734375\n",
      "Epoch [54/250], Training Loss: 88.53838058297399, Validation Loss: 6275.884765625\n",
      "Epoch [55/250], Training Loss: 40.172230507730845, Validation Loss: 6359.8974609375\n",
      "Epoch [56/250], Training Loss: 1574.907115610219, Validation Loss: 6658.48876953125\n",
      "Epoch [57/250], Training Loss: 58.97550975963115, Validation Loss: 8266.8857421875\n",
      "Epoch [58/250], Training Loss: 48.75866337001554, Validation Loss: 7825.15234375\n",
      "Epoch [59/250], Training Loss: 1400.8260888383202, Validation Loss: 6472.59521484375\n",
      "Epoch [60/250], Training Loss: 1247.725497626474, Validation Loss: 6549.2255859375\n",
      "Epoch [61/250], Training Loss: 1111.6365417276354, Validation Loss: 8431.185546875\n",
      "Epoch [62/250], Training Loss: 991.9675943080957, Validation Loss: 6399.150390625\n",
      "Epoch [63/250], Training Loss: 893.9553713736739, Validation Loss: 7477.376953125\n",
      "Epoch [64/250], Training Loss: 812.453297339538, Validation Loss: 7016.109375\n",
      "Epoch [65/250], Training Loss: 734.3590475474749, Validation Loss: 6776.751953125\n",
      "Epoch [66/250], Training Loss: 673.4898467267418, Validation Loss: 6931.3623046875\n",
      "Epoch [67/250], Training Loss: 607.0463640281214, Validation Loss: 7415.91943359375\n",
      "Epoch [68/250], Training Loss: 569.2384402651261, Validation Loss: 7691.7177734375\n",
      "Epoch [69/250], Training Loss: 522.0689369213754, Validation Loss: 7909.3876953125\n",
      "Epoch [70/250], Training Loss: 473.34427686647985, Validation Loss: 8159.4580078125\n",
      "Epoch [71/250], Training Loss: 44.219356106407034, Validation Loss: 8151.65771484375\n",
      "Epoch [72/250], Training Loss: 441.20021214966323, Validation Loss: 8526.91015625\n",
      "Epoch [73/250], Training Loss: 403.9075303942973, Validation Loss: 8511.11328125\n",
      "Epoch [74/250], Training Loss: 364.8893871683483, Validation Loss: 8806.4072265625\n",
      "Epoch [75/250], Training Loss: 44.469885170818024, Validation Loss: 8006.9228515625\n",
      "Epoch [76/250], Training Loss: 344.502025977087, Validation Loss: 8506.2509765625\n",
      "Epoch [77/250], Training Loss: 315.35478270372334, Validation Loss: 8957.4814453125\n",
      "Epoch [78/250], Training Loss: 291.99498328844027, Validation Loss: 9313.9970703125\n",
      "Epoch [79/250], Training Loss: 269.54997637794077, Validation Loss: 9132.2490234375\n",
      "Epoch [80/250], Training Loss: 249.96498176611965, Validation Loss: 9245.8935546875\n",
      "Epoch [81/250], Training Loss: 231.51637563550747, Validation Loss: 9325.0546875\n",
      "Epoch [82/250], Training Loss: 215.91877299242537, Validation Loss: 9335.76171875\n",
      "Epoch [83/250], Training Loss: 200.54456996547603, Validation Loss: 8741.96875\n",
      "Epoch [84/250], Training Loss: 189.19166609313655, Validation Loss: 9063.212890625\n",
      "Epoch [85/250], Training Loss: 176.20916478761777, Validation Loss: 8907.4111328125\n",
      "Epoch [86/250], Training Loss: 167.18146941274804, Validation Loss: 9023.123046875\n",
      "Epoch [87/250], Training Loss: 157.85862940014312, Validation Loss: 9238.880859375\n",
      "Epoch [88/250], Training Loss: 149.3383697103256, Validation Loss: 9444.76953125\n",
      "Epoch [89/250], Training Loss: 141.31150429420552, Validation Loss: 9608.5068359375\n",
      "Epoch [90/250], Training Loss: 133.53669601539772, Validation Loss: 9646.7578125\n",
      "Epoch [91/250], Training Loss: 127.85392551876886, Validation Loss: 9843.32421875\n",
      "Epoch [92/250], Training Loss: 122.0730621173824, Validation Loss: 10026.2099609375\n",
      "Epoch [93/250], Training Loss: 116.50532313219183, Validation Loss: 10095.404296875\n",
      "Epoch [94/250], Training Loss: 111.07065033854936, Validation Loss: 10126.2861328125\n",
      "Epoch [95/250], Training Loss: 106.64229059370886, Validation Loss: 9999.00390625\n",
      "Epoch [96/250], Training Loss: 102.62616953755904, Validation Loss: 9785.99609375\n",
      "Epoch [97/250], Training Loss: 98.75852416964233, Validation Loss: 9523.7802734375\n",
      "Epoch [98/250], Training Loss: 95.58807048456443, Validation Loss: 9487.0498046875\n",
      "Epoch [99/250], Training Loss: 92.92675764396422, Validation Loss: 9343.1806640625\n",
      "Epoch [100/250], Training Loss: 89.38066186462822, Validation Loss: 9198.9482421875\n",
      "Epoch [101/250], Training Loss: 28.256971294517076, Validation Loss: 8558.3154296875\n",
      "Epoch [102/250], Training Loss: 90.15847303048977, Validation Loss: 10072.828125\n",
      "Epoch [103/250], Training Loss: 85.86598160132557, Validation Loss: 9659.91796875\n",
      "Epoch [104/250], Training Loss: 83.56036777728538, Validation Loss: 9542.2802734375\n",
      "Epoch [105/250], Training Loss: 82.42598023889113, Validation Loss: 9521.4990234375\n",
      "Epoch [106/250], Training Loss: 78.27415279640252, Validation Loss: 9730.1875\n",
      "Epoch [107/250], Training Loss: 74.83447304303553, Validation Loss: 7805.16796875\n",
      "Epoch [108/250], Training Loss: 77.62171360723916, Validation Loss: 9348.22265625\n",
      "Epoch [109/250], Training Loss: 78.75430764930744, Validation Loss: 8886.072265625\n",
      "Epoch [110/250], Training Loss: 79.98038955162454, Validation Loss: 8945.6728515625\n",
      "Epoch [111/250], Training Loss: 78.67397889660782, Validation Loss: 8126.296875\n",
      "Epoch [112/250], Training Loss: 75.22463895598574, Validation Loss: 8477.8642578125\n",
      "Epoch [113/250], Training Loss: 27.321527983070172, Validation Loss: 9072.173828125\n",
      "Epoch [114/250], Training Loss: 75.11457422001949, Validation Loss: 9592.33203125\n",
      "Epoch [115/250], Training Loss: 23.593907312768025, Validation Loss: 8658.4033203125\n",
      "Epoch [116/250], Training Loss: 75.77788740746837, Validation Loss: 8104.76318359375\n",
      "Epoch [117/250], Training Loss: 24.966580975084376, Validation Loss: 8767.5390625\n",
      "Epoch [118/250], Training Loss: 76.14378423047958, Validation Loss: 6530.20458984375\n",
      "Epoch [119/250], Training Loss: 24.604115808775116, Validation Loss: 8564.453125\n",
      "Epoch [120/250], Training Loss: 78.42507440060602, Validation Loss: 4750.10595703125\n",
      "Epoch [121/250], Training Loss: 24.34999896084704, Validation Loss: 9011.6875\n",
      "Epoch [122/250], Training Loss: 81.16137564372471, Validation Loss: 3993.994140625\n",
      "Epoch [123/250], Training Loss: 24.908141149384623, Validation Loss: 9546.724609375\n",
      "Epoch [124/250], Training Loss: 82.83676071113518, Validation Loss: 3421.24951171875\n",
      "Epoch [125/250], Training Loss: 24.634798546919274, Validation Loss: 9030.0693359375\n",
      "Epoch [126/250], Training Loss: 83.4718263892376, Validation Loss: 3208.5048828125\n",
      "Epoch [127/250], Training Loss: 24.462080107711774, Validation Loss: 9469.41015625\n",
      "Epoch [128/250], Training Loss: 87.08810907056824, Validation Loss: 2864.21337890625\n",
      "Epoch [129/250], Training Loss: 80.93037053055117, Validation Loss: 3571.315185546875\n",
      "Epoch [130/250], Training Loss: 24.476935967718095, Validation Loss: 9113.6904296875\n",
      "Epoch [131/250], Training Loss: 81.980179431654, Validation Loss: 3113.751953125\n",
      "Epoch [132/250], Training Loss: 24.40806673543276, Validation Loss: 9592.380859375\n",
      "Epoch [133/250], Training Loss: 86.30347551329987, Validation Loss: 2775.8857421875\n",
      "Epoch [134/250], Training Loss: 80.12807747259647, Validation Loss: 3316.095947265625\n",
      "Epoch [135/250], Training Loss: 27.216123974870253, Validation Loss: 9367.7119140625\n",
      "Epoch [136/250], Training Loss: 79.79071025948802, Validation Loss: 3148.874755859375\n",
      "Epoch [137/250], Training Loss: 25.142777956959065, Validation Loss: 8376.634765625\n",
      "Epoch [138/250], Training Loss: 19.99167410448289, Validation Loss: 10031.318359375\n",
      "Epoch [139/250], Training Loss: 89.66020291116361, Validation Loss: 2709.823974609375\n",
      "Epoch [140/250], Training Loss: 80.51901233668148, Validation Loss: 3060.40869140625\n",
      "Epoch [141/250], Training Loss: 75.1580990158573, Validation Loss: 3610.2919921875\n",
      "Epoch [142/250], Training Loss: 33.824957241808114, Validation Loss: 8628.2978515625\n",
      "Epoch [143/250], Training Loss: 76.10352866048918, Validation Loss: 3437.6591796875\n",
      "Epoch [144/250], Training Loss: 65.70443294251581, Validation Loss: 4173.2197265625\n",
      "Epoch [145/250], Training Loss: 30.077728180255928, Validation Loss: 8247.11328125\n",
      "Epoch [146/250], Training Loss: 74.6534023584728, Validation Loss: 3489.2763671875\n",
      "Epoch [147/250], Training Loss: 68.54834323037305, Validation Loss: 4125.6484375\n",
      "Epoch [148/250], Training Loss: 30.739973155091164, Validation Loss: 9545.6318359375\n",
      "Epoch [149/250], Training Loss: 77.16555787410411, Validation Loss: 3101.721923828125\n",
      "Epoch [150/250], Training Loss: 68.64701465181197, Validation Loss: 3726.71142578125\n",
      "Epoch [151/250], Training Loss: 59.86746326534368, Validation Loss: 20303.369140625\n",
      "Epoch [152/250], Training Loss: 169.91202499109136, Validation Loss: 7490.77490234375\n",
      "Epoch [153/250], Training Loss: 69.44424817972615, Validation Loss: 10076.5947265625\n",
      "Epoch [154/250], Training Loss: 28.382208226745888, Validation Loss: 15365.91015625\n",
      "Epoch [155/250], Training Loss: 19.223772153997952, Validation Loss: 12865.701171875\n",
      "Epoch [156/250], Training Loss: 87.08145558341037, Validation Loss: 6109.66357421875\n",
      "Epoch [157/250], Training Loss: 131.6701137570908, Validation Loss: 6666.91162109375\n",
      "Epoch [158/250], Training Loss: 68.86677127905688, Validation Loss: 7716.66064453125\n",
      "Epoch [159/250], Training Loss: 62.53040288861403, Validation Loss: 12566.7626953125\n",
      "Epoch [160/250], Training Loss: 75.26678599859534, Validation Loss: 19474.783203125\n",
      "Epoch [161/250], Training Loss: 81.05582463246478, Validation Loss: 7724.0771484375\n",
      "Epoch [162/250], Training Loss: 64.16945434186003, Validation Loss: 8288.5439453125\n",
      "Epoch [163/250], Training Loss: 63.451379575707165, Validation Loss: 10936.953125\n",
      "Epoch [164/250], Training Loss: 92.55287359240572, Validation Loss: 10322.962890625\n",
      "Epoch [165/250], Training Loss: 60.02054975690391, Validation Loss: 12159.0380859375\n",
      "Epoch [166/250], Training Loss: 72.79648966950158, Validation Loss: 11901.453125\n",
      "Epoch [167/250], Training Loss: 58.0252029288079, Validation Loss: 12381.603515625\n",
      "Epoch [168/250], Training Loss: 55.55807464153767, Validation Loss: 14063.16796875\n",
      "Epoch [169/250], Training Loss: 85.16875171328083, Validation Loss: 12393.82421875\n",
      "Epoch [170/250], Training Loss: 53.39361946364199, Validation Loss: 14099.0234375\n",
      "Epoch [171/250], Training Loss: 86.6460754622341, Validation Loss: 12925.47265625\n",
      "Epoch [172/250], Training Loss: 51.74309212191324, Validation Loss: 21821.8515625\n",
      "Epoch [173/250], Training Loss: 215.73630971542292, Validation Loss: 14022.0107421875\n",
      "Epoch [174/250], Training Loss: 49.945868201191416, Validation Loss: 19720.4609375\n",
      "Epoch [175/250], Training Loss: 49.607826706914494, Validation Loss: 25989.8359375\n",
      "Epoch [176/250], Training Loss: 247.13592910530946, Validation Loss: 12409.974609375\n",
      "Epoch [177/250], Training Loss: 48.45620315824959, Validation Loss: 13399.607421875\n",
      "Epoch [178/250], Training Loss: 48.230629924350296, Validation Loss: 20113.947265625\n",
      "Epoch [179/250], Training Loss: 50.79232760972159, Validation Loss: 13195.8828125\n",
      "Epoch [180/250], Training Loss: 47.67805393531089, Validation Loss: 13723.47265625\n",
      "Epoch [181/250], Training Loss: 47.686329102576, Validation Loss: 15649.1025390625\n",
      "Epoch [182/250], Training Loss: 80.77806174172355, Validation Loss: 14248.0185546875\n",
      "Epoch [183/250], Training Loss: 48.40540887885415, Validation Loss: 17621.841796875\n",
      "Epoch [184/250], Training Loss: 47.586839734353035, Validation Loss: 23445.43359375\n",
      "Epoch [185/250], Training Loss: 46.30511056831154, Validation Loss: 22911.576171875\n",
      "Epoch [186/250], Training Loss: 46.773111029146385, Validation Loss: 22409.23828125\n",
      "Epoch [187/250], Training Loss: 45.99065479248978, Validation Loss: 22703.640625\n",
      "Epoch [188/250], Training Loss: 45.373167953921815, Validation Loss: 22991.07421875\n",
      "Epoch [189/250], Training Loss: 45.835074309932956, Validation Loss: 23097.244140625\n",
      "Epoch [190/250], Training Loss: 45.7629732881033, Validation Loss: 21956.5234375\n",
      "Epoch [191/250], Training Loss: 73.45748321489343, Validation Loss: 15918.5830078125\n",
      "Epoch [192/250], Training Loss: 43.34325532909217, Validation Loss: 20535.703125\n",
      "Epoch [193/250], Training Loss: 42.009762295444126, Validation Loss: 19751.896484375\n",
      "Epoch [194/250], Training Loss: 42.6397279863073, Validation Loss: 23613.794921875\n",
      "Epoch [195/250], Training Loss: 43.288483767479946, Validation Loss: 20422.65234375\n",
      "Epoch [196/250], Training Loss: 41.85670522531291, Validation Loss: 19412.255859375\n",
      "Epoch [197/250], Training Loss: 49.37475236260938, Validation Loss: 27329.861328125\n",
      "Epoch [198/250], Training Loss: 258.69564455734866, Validation Loss: 30284.76171875\n",
      "Epoch [199/250], Training Loss: 318.61744063468393, Validation Loss: 22168.841796875\n",
      "Epoch [200/250], Training Loss: 40.62997956031148, Validation Loss: 17117.90234375\n",
      "Epoch [201/250], Training Loss: 27.38856962782885, Validation Loss: 9243.26953125\n",
      "Epoch [202/250], Training Loss: 27.94466914225748, Validation Loss: 9816.310546875\n",
      "Epoch [203/250], Training Loss: 22.454868927928736, Validation Loss: 7934.73486328125\n",
      "Epoch [204/250], Training Loss: 56.97144948634877, Validation Loss: 4894.77734375\n",
      "Epoch [205/250], Training Loss: 55.33276516569192, Validation Loss: 8962.4921875\n",
      "Epoch [206/250], Training Loss: 22.30957121181787, Validation Loss: 10173.1787109375\n",
      "Epoch [207/250], Training Loss: 21.925908753541766, Validation Loss: 8340.2119140625\n",
      "Epoch [208/250], Training Loss: 18.098631866761906, Validation Loss: 7378.89599609375\n",
      "Epoch [209/250], Training Loss: 15.966490838973892, Validation Loss: 7220.6669921875\n",
      "Epoch [210/250], Training Loss: 19.22752358668695, Validation Loss: 6315.111328125\n",
      "Epoch [211/250], Training Loss: 125.5980516628996, Validation Loss: 3266.591064453125\n",
      "Epoch [212/250], Training Loss: 426.4292659746152, Validation Loss: 4616.76123046875\n",
      "Epoch [213/250], Training Loss: 38.53651114519633, Validation Loss: 2158.108642578125\n",
      "Epoch [214/250], Training Loss: 42.897458318754005, Validation Loss: 2031.4794921875\n",
      "Epoch [215/250], Training Loss: 41.66268891889991, Validation Loss: 1764.694580078125\n",
      "Epoch [216/250], Training Loss: 36.307867843688825, Validation Loss: 2044.1541748046875\n",
      "Epoch [217/250], Training Loss: 32.879880807213425, Validation Loss: 2031.59228515625\n",
      "Epoch [218/250], Training Loss: 31.101995854682446, Validation Loss: 2047.21240234375\n",
      "Epoch [219/250], Training Loss: 29.659637490749258, Validation Loss: 2114.21484375\n",
      "Epoch [220/250], Training Loss: 49.42672506359843, Validation Loss: 1972.5721435546875\n",
      "Epoch [221/250], Training Loss: 319.88512845035876, Validation Loss: 1764.6429443359375\n",
      "Epoch [222/250], Training Loss: 27.651696770538802, Validation Loss: 2023.96435546875\n",
      "Epoch [223/250], Training Loss: 27.31454177871667, Validation Loss: 1957.9139404296875\n",
      "Epoch [224/250], Training Loss: 160.55155185996583, Validation Loss: 1725.433349609375\n",
      "Epoch [225/250], Training Loss: 26.42897419514298, Validation Loss: 1868.543212890625\n",
      "Epoch [226/250], Training Loss: 26.13753486957996, Validation Loss: 1861.4649658203125\n",
      "Epoch [227/250], Training Loss: 25.53014398217997, Validation Loss: 1844.093994140625\n",
      "Epoch [228/250], Training Loss: 24.994933672935108, Validation Loss: 1804.126953125\n",
      "Epoch [229/250], Training Loss: 25.35659704798658, Validation Loss: 1776.441650390625\n",
      "Epoch [230/250], Training Loss: 24.02383491754937, Validation Loss: 1808.889404296875\n",
      "Epoch [231/250], Training Loss: 23.624745808399982, Validation Loss: 1771.617431640625\n",
      "Epoch [232/250], Training Loss: 23.261276975840637, Validation Loss: 1750.739013671875\n",
      "Epoch [233/250], Training Loss: 22.942653699738766, Validation Loss: 1728.2960205078125\n",
      "Epoch [234/250], Training Loss: 85.2654173557793, Validation Loss: 4930.04931640625\n",
      "Epoch [235/250], Training Loss: 22.473401228572964, Validation Loss: 4537.3486328125\n",
      "Epoch [236/250], Training Loss: 22.29158006431212, Validation Loss: 2982.455078125\n",
      "Epoch [237/250], Training Loss: 22.112435579632002, Validation Loss: 2180.69970703125\n",
      "Epoch [238/250], Training Loss: 21.954817281413106, Validation Loss: 1839.928466796875\n",
      "Epoch [239/250], Training Loss: 21.7965399612272, Validation Loss: 1678.9742431640625\n",
      "Epoch [240/250], Training Loss: 21.625464192888806, Validation Loss: 1601.1080322265625\n",
      "Epoch [241/250], Training Loss: 21.470296098427877, Validation Loss: 1577.85693359375\n",
      "Epoch [242/250], Training Loss: 21.25421338301367, Validation Loss: 1528.5897216796875\n",
      "Epoch [243/250], Training Loss: 21.099199797426984, Validation Loss: 1495.1287841796875\n",
      "Epoch [244/250], Training Loss: 20.93025902909621, Validation Loss: 1497.1177978515625\n",
      "Epoch [245/250], Training Loss: 20.712820783883323, Validation Loss: 1458.0604248046875\n",
      "Epoch [246/250], Training Loss: 20.49866590532183, Validation Loss: 1464.2523193359375\n",
      "Epoch [247/250], Training Loss: 20.272746776713223, Validation Loss: 1472.82666015625\n",
      "Epoch [248/250], Training Loss: 20.086523612514732, Validation Loss: 1461.988037109375\n",
      "Epoch [249/250], Training Loss: 19.870498165716548, Validation Loss: 1446.0400390625\n",
      "Epoch [250/250], Training Loss: 19.639320259404503, Validation Loss: 1431.614501953125\n",
      "Test Loss: 3228.990234375\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.01, 25)\n",
      "Epoch [1/250], Training Loss: 19748.403629880842, Validation Loss: 18544.666015625\n",
      "Epoch [2/250], Training Loss: 14922.851131513653, Validation Loss: 16282.1376953125\n",
      "Epoch [3/250], Training Loss: 12384.689455463671, Validation Loss: 12051.1767578125\n",
      "Epoch [4/250], Training Loss: 10449.84796737867, Validation Loss: 10275.6494140625\n",
      "Epoch [5/250], Training Loss: 8894.824757029017, Validation Loss: 9125.896484375\n",
      "Epoch [6/250], Training Loss: 7853.663983225857, Validation Loss: 8395.9169921875\n",
      "Epoch [7/250], Training Loss: 7124.970413534225, Validation Loss: 7892.90771484375\n",
      "Epoch [8/250], Training Loss: 6578.370657633471, Validation Loss: 7596.91259765625\n",
      "Epoch [9/250], Training Loss: 6143.213931475571, Validation Loss: 7162.88671875\n",
      "Epoch [10/250], Training Loss: 4888.9910697522755, Validation Loss: 14438.8544921875\n",
      "Epoch [11/250], Training Loss: 4567.737934940843, Validation Loss: 7533.2578125\n",
      "Epoch [12/250], Training Loss: 4169.213692566598, Validation Loss: 5675.19873046875\n",
      "Epoch [13/250], Training Loss: 3497.9749547725387, Validation Loss: 4807.56396484375\n",
      "Epoch [14/250], Training Loss: 3079.4754042128766, Validation Loss: 4169.69580078125\n",
      "Epoch [15/250], Training Loss: 2697.2137243137113, Validation Loss: 3661.7041015625\n",
      "Epoch [16/250], Training Loss: 2365.2609117605475, Validation Loss: 3318.052490234375\n",
      "Epoch [17/250], Training Loss: 2083.3633127665557, Validation Loss: 3110.632080078125\n",
      "Epoch [18/250], Training Loss: 1837.8753531673594, Validation Loss: 2969.014404296875\n",
      "Epoch [19/250], Training Loss: 1625.8202956428456, Validation Loss: 2898.37158203125\n",
      "Epoch [20/250], Training Loss: 1446.3190005866932, Validation Loss: 2843.451171875\n",
      "Epoch [21/250], Training Loss: 1293.3094637633037, Validation Loss: 2811.735107421875\n",
      "Epoch [22/250], Training Loss: 1159.343585002391, Validation Loss: 2760.160888671875\n",
      "Epoch [23/250], Training Loss: 1040.4033153507573, Validation Loss: 2715.097900390625\n",
      "Epoch [24/250], Training Loss: 934.511856881726, Validation Loss: 2668.077392578125\n",
      "Epoch [25/250], Training Loss: 840.6274134632688, Validation Loss: 2623.468505859375\n",
      "Epoch [26/250], Training Loss: 757.5895584165119, Validation Loss: 2571.287353515625\n",
      "Epoch [27/250], Training Loss: 683.6300924133736, Validation Loss: 2542.732666015625\n",
      "Epoch [28/250], Training Loss: 617.2653767507242, Validation Loss: 2534.990234375\n",
      "Epoch [29/250], Training Loss: 558.6422739164688, Validation Loss: 2548.430908203125\n",
      "Epoch [30/250], Training Loss: 506.5559054911072, Validation Loss: 2580.692626953125\n",
      "Epoch [31/250], Training Loss: 459.88914430906755, Validation Loss: 2643.967529296875\n",
      "Epoch [32/250], Training Loss: 418.1839059656161, Validation Loss: 2699.690185546875\n",
      "Epoch [33/250], Training Loss: 381.4187600695621, Validation Loss: 2786.04052734375\n",
      "Epoch [34/250], Training Loss: 349.0046237584641, Validation Loss: 2855.361572265625\n",
      "Epoch [35/250], Training Loss: 320.35153412376593, Validation Loss: 2936.248046875\n",
      "Epoch [36/250], Training Loss: 294.89080113644627, Validation Loss: 2989.12646484375\n",
      "Epoch [37/250], Training Loss: 272.1872602597539, Validation Loss: 3074.54638671875\n",
      "Epoch [38/250], Training Loss: 251.76648913823823, Validation Loss: 3123.366943359375\n",
      "Epoch [39/250], Training Loss: 233.34473833573404, Validation Loss: 3185.765380859375\n",
      "Epoch [40/250], Training Loss: 216.5913051781611, Validation Loss: 3202.41796875\n",
      "Epoch [41/250], Training Loss: 201.42796691237007, Validation Loss: 3239.48193359375\n",
      "Epoch [42/250], Training Loss: 187.69155502719738, Validation Loss: 3247.434326171875\n",
      "Epoch [43/250], Training Loss: 175.3324746502113, Validation Loss: 3281.960693359375\n",
      "Epoch [44/250], Training Loss: 164.2250259101464, Validation Loss: 3292.91650390625\n",
      "Epoch [45/250], Training Loss: 154.26790950895446, Validation Loss: 3329.6328125\n",
      "Epoch [46/250], Training Loss: 145.28295685831006, Validation Loss: 3348.410400390625\n",
      "Epoch [47/250], Training Loss: 137.11159519631025, Validation Loss: 3390.279296875\n",
      "Epoch [48/250], Training Loss: 129.6253368798551, Validation Loss: 3416.708251953125\n",
      "Epoch [49/250], Training Loss: 122.77012850021258, Validation Loss: 3461.146240234375\n",
      "Epoch [50/250], Training Loss: 116.43894128550802, Validation Loss: 3485.33935546875\n",
      "Epoch [51/250], Training Loss: 110.57141164794218, Validation Loss: 3514.609375\n",
      "Epoch [52/250], Training Loss: 105.09255153742784, Validation Loss: 3518.187255859375\n",
      "Epoch [53/250], Training Loss: 99.95102344461384, Validation Loss: 3521.562255859375\n",
      "Epoch [54/250], Training Loss: 95.13422363314164, Validation Loss: 3507.85205078125\n",
      "Epoch [55/250], Training Loss: 90.66613763213385, Validation Loss: 3503.9228515625\n",
      "Epoch [56/250], Training Loss: 86.62748905606139, Validation Loss: 3504.762451171875\n",
      "Epoch [57/250], Training Loss: 83.02004210033161, Validation Loss: 3520.615234375\n",
      "Epoch [58/250], Training Loss: 79.84557021430989, Validation Loss: 3539.392578125\n",
      "Epoch [59/250], Training Loss: 77.06121963891253, Validation Loss: 3561.14501953125\n",
      "Epoch [60/250], Training Loss: 74.62353925951925, Validation Loss: 3579.95068359375\n",
      "Epoch [61/250], Training Loss: 72.44482192022485, Validation Loss: 3593.712158203125\n",
      "Epoch [62/250], Training Loss: 70.49363359505688, Validation Loss: 3599.3955078125\n",
      "Epoch [63/250], Training Loss: 68.73651076143243, Validation Loss: 3599.560546875\n",
      "Epoch [64/250], Training Loss: 67.16521906110712, Validation Loss: 3598.2001953125\n",
      "Epoch [65/250], Training Loss: 65.74771100173031, Validation Loss: 3600.083740234375\n",
      "Epoch [66/250], Training Loss: 64.47727724819441, Validation Loss: 3607.7119140625\n",
      "Epoch [67/250], Training Loss: 63.32058873617194, Validation Loss: 3621.5732421875\n",
      "Epoch [68/250], Training Loss: 62.258045570294456, Validation Loss: 3640.543701171875\n",
      "Epoch [69/250], Training Loss: 61.260206609002, Validation Loss: 3662.5009765625\n",
      "Epoch [70/250], Training Loss: 60.303938814369666, Validation Loss: 3685.614013671875\n",
      "Epoch [71/250], Training Loss: 59.387213561772036, Validation Loss: 3708.6572265625\n",
      "Epoch [72/250], Training Loss: 58.49504915088765, Validation Loss: 3730.57275390625\n",
      "Epoch [73/250], Training Loss: 57.62615053222962, Validation Loss: 3750.710693359375\n",
      "Epoch [74/250], Training Loss: 56.76908735638888, Validation Loss: 3768.981689453125\n",
      "Epoch [75/250], Training Loss: 55.92586235441286, Validation Loss: 3785.259765625\n",
      "Epoch [76/250], Training Loss: 55.10959897961699, Validation Loss: 3799.926513671875\n",
      "Epoch [77/250], Training Loss: 54.31297331444177, Validation Loss: 3813.351318359375\n",
      "Epoch [78/250], Training Loss: 53.53802718408106, Validation Loss: 3825.914794921875\n",
      "Epoch [79/250], Training Loss: 52.787129144860685, Validation Loss: 3837.9521484375\n",
      "Epoch [80/250], Training Loss: 52.065751403723084, Validation Loss: 3849.748291015625\n",
      "Epoch [81/250], Training Loss: 51.377628449257394, Validation Loss: 3861.451904296875\n",
      "Epoch [82/250], Training Loss: 50.710140453990014, Validation Loss: 3873.081298828125\n",
      "Epoch [83/250], Training Loss: 50.076289204565235, Validation Loss: 3884.985595703125\n",
      "Epoch [84/250], Training Loss: 49.474386178606906, Validation Loss: 3897.124267578125\n",
      "Epoch [85/250], Training Loss: 48.913022215910644, Validation Loss: 3909.517578125\n",
      "Epoch [86/250], Training Loss: 48.372409026070116, Validation Loss: 3921.98291015625\n",
      "Epoch [87/250], Training Loss: 47.8587476463905, Validation Loss: 3934.472900390625\n",
      "Epoch [88/250], Training Loss: 47.374795517999246, Validation Loss: 3947.142578125\n",
      "Epoch [89/250], Training Loss: 46.92141105372265, Validation Loss: 3959.72412109375\n",
      "Epoch [90/250], Training Loss: 46.497411735114, Validation Loss: 3971.983642578125\n",
      "Epoch [91/250], Training Loss: 46.09068629215589, Validation Loss: 3983.797119140625\n",
      "Epoch [92/250], Training Loss: 45.708303075780265, Validation Loss: 3995.180419921875\n",
      "Epoch [93/250], Training Loss: 45.34558398070461, Validation Loss: 4006.06396484375\n",
      "Epoch [94/250], Training Loss: 45.00020664502458, Validation Loss: 4016.28662109375\n",
      "Epoch [95/250], Training Loss: 44.671354395080044, Validation Loss: 4025.861328125\n",
      "Epoch [96/250], Training Loss: 44.357643538333114, Validation Loss: 4034.709228515625\n",
      "Epoch [97/250], Training Loss: 44.06183743242181, Validation Loss: 4043.031982421875\n",
      "Epoch [98/250], Training Loss: 43.78168852087845, Validation Loss: 4050.838134765625\n",
      "Epoch [99/250], Training Loss: 43.51800259819202, Validation Loss: 4057.97265625\n",
      "Epoch [100/250], Training Loss: 43.258942422160416, Validation Loss: 4064.313720703125\n",
      "Epoch [101/250], Training Loss: 43.01214999016119, Validation Loss: 4070.001220703125\n",
      "Epoch [102/250], Training Loss: 42.77534822113958, Validation Loss: 4075.077880859375\n",
      "Epoch [103/250], Training Loss: 42.5535077700144, Validation Loss: 4079.6650390625\n",
      "Epoch [104/250], Training Loss: 42.33688319126663, Validation Loss: 4083.6474609375\n",
      "Epoch [105/250], Training Loss: 42.129730164464796, Validation Loss: 4087.0859375\n",
      "Epoch [106/250], Training Loss: 41.92913439785958, Validation Loss: 4089.88623046875\n",
      "Epoch [107/250], Training Loss: 41.74115140763971, Validation Loss: 4092.23828125\n",
      "Epoch [108/250], Training Loss: 41.55303022416676, Validation Loss: 4094.068603515625\n",
      "Epoch [109/250], Training Loss: 41.37547767121673, Validation Loss: 4095.413818359375\n",
      "Epoch [110/250], Training Loss: 41.20299067598723, Validation Loss: 4096.30224609375\n",
      "Epoch [111/250], Training Loss: 41.03391379452599, Validation Loss: 4096.693359375\n",
      "Epoch [112/250], Training Loss: 40.872356810996315, Validation Loss: 4096.68310546875\n",
      "Epoch [113/250], Training Loss: 40.71694684515693, Validation Loss: 4096.26904296875\n",
      "Epoch [114/250], Training Loss: 40.56494266998514, Validation Loss: 4095.408203125\n",
      "Epoch [115/250], Training Loss: 40.41289933992266, Validation Loss: 4094.22021484375\n",
      "Epoch [116/250], Training Loss: 40.272406429125425, Validation Loss: 4092.89501953125\n",
      "Epoch [117/250], Training Loss: 40.1369945788156, Validation Loss: 4091.483642578125\n",
      "Epoch [118/250], Training Loss: 40.00194571282873, Validation Loss: 4089.800537109375\n",
      "Epoch [119/250], Training Loss: 39.87210545047566, Validation Loss: 4087.86767578125\n",
      "Epoch [120/250], Training Loss: 39.749013984059395, Validation Loss: 4085.747802734375\n",
      "Epoch [121/250], Training Loss: 39.626646534973624, Validation Loss: 4083.47705078125\n",
      "Epoch [122/250], Training Loss: 39.51134109708633, Validation Loss: 4081.113525390625\n",
      "Epoch [123/250], Training Loss: 39.396863466938946, Validation Loss: 4078.615234375\n",
      "Epoch [124/250], Training Loss: 39.28212955028058, Validation Loss: 4075.952880859375\n",
      "Epoch [125/250], Training Loss: 39.17282252154032, Validation Loss: 4073.121337890625\n",
      "Epoch [126/250], Training Loss: 39.06681713079919, Validation Loss: 4070.292724609375\n",
      "Epoch [127/250], Training Loss: 38.962039796417734, Validation Loss: 4067.506103515625\n",
      "Epoch [128/250], Training Loss: 38.86350780659717, Validation Loss: 4064.75341796875\n",
      "Epoch [129/250], Training Loss: 38.7661640050354, Validation Loss: 4062.11474609375\n",
      "Epoch [130/250], Training Loss: 38.67259438881411, Validation Loss: 4059.530029296875\n",
      "Epoch [131/250], Training Loss: 38.58099805517254, Validation Loss: 4056.928466796875\n",
      "Epoch [132/250], Training Loss: 38.491375107371816, Validation Loss: 4054.385986328125\n",
      "Epoch [133/250], Training Loss: 38.40746161756103, Validation Loss: 4051.89794921875\n",
      "Epoch [134/250], Training Loss: 38.32306395285005, Validation Loss: 4049.444091796875\n",
      "Epoch [135/250], Training Loss: 38.24048307913086, Validation Loss: 4046.94189453125\n",
      "Epoch [136/250], Training Loss: 38.16075209439508, Validation Loss: 4044.38427734375\n",
      "Epoch [137/250], Training Loss: 38.077717267597, Validation Loss: 4041.704833984375\n",
      "Epoch [138/250], Training Loss: 37.999580473551156, Validation Loss: 4039.17041015625\n",
      "Epoch [139/250], Training Loss: 37.9201017703394, Validation Loss: 4036.612548828125\n",
      "Epoch [140/250], Training Loss: 37.84668708356658, Validation Loss: 4034.224609375\n",
      "Epoch [141/250], Training Loss: 37.77402539247856, Validation Loss: 4031.844482421875\n",
      "Epoch [142/250], Training Loss: 37.70303979822089, Validation Loss: 4029.614013671875\n",
      "Epoch [143/250], Training Loss: 37.6342947584131, Validation Loss: 4027.413818359375\n",
      "Epoch [144/250], Training Loss: 37.568823939140216, Validation Loss: 4025.25927734375\n",
      "Epoch [145/250], Training Loss: 37.503749817418985, Validation Loss: 4023.064453125\n",
      "Epoch [146/250], Training Loss: 37.43694881141554, Validation Loss: 4020.817138671875\n",
      "Epoch [147/250], Training Loss: 37.36982163074055, Validation Loss: 4018.490234375\n",
      "Epoch [148/250], Training Loss: 37.30640176961292, Validation Loss: 4016.3515625\n",
      "Epoch [149/250], Training Loss: 37.24457856651735, Validation Loss: 4014.23193359375\n",
      "Epoch [150/250], Training Loss: 37.182944786466564, Validation Loss: 4012.125244140625\n",
      "Epoch [151/250], Training Loss: 37.12353743002949, Validation Loss: 4010.064208984375\n",
      "Epoch [152/250], Training Loss: 37.06380402178595, Validation Loss: 4007.951904296875\n",
      "Epoch [153/250], Training Loss: 37.00562435377039, Validation Loss: 4005.938232421875\n",
      "Epoch [154/250], Training Loss: 36.947680239683876, Validation Loss: 4003.950927734375\n",
      "Epoch [155/250], Training Loss: 36.888477569734235, Validation Loss: 4001.997314453125\n",
      "Epoch [156/250], Training Loss: 36.83439853672127, Validation Loss: 4000.166015625\n",
      "Epoch [157/250], Training Loss: 36.77989000826472, Validation Loss: 3998.389892578125\n",
      "Epoch [158/250], Training Loss: 36.72566661925477, Validation Loss: 3996.603759765625\n",
      "Epoch [159/250], Training Loss: 36.674055061000026, Validation Loss: 3994.83935546875\n",
      "Epoch [160/250], Training Loss: 36.6196201356886, Validation Loss: 3992.98291015625\n",
      "Epoch [161/250], Training Loss: 36.56906362151295, Validation Loss: 3991.16845703125\n",
      "Epoch [162/250], Training Loss: 36.51777831416066, Validation Loss: 3989.438232421875\n",
      "Epoch [163/250], Training Loss: 36.46803944425259, Validation Loss: 3987.703369140625\n",
      "Epoch [164/250], Training Loss: 36.41594959664018, Validation Loss: 3985.843505859375\n",
      "Epoch [165/250], Training Loss: 36.36710235867856, Validation Loss: 3984.02490234375\n",
      "Epoch [166/250], Training Loss: 36.31927820743807, Validation Loss: 3982.362060546875\n",
      "Epoch [167/250], Training Loss: 36.27127357440192, Validation Loss: 3980.66943359375\n",
      "Epoch [168/250], Training Loss: 36.224163081304134, Validation Loss: 3978.973876953125\n",
      "Epoch [169/250], Training Loss: 36.177546636765605, Validation Loss: 3977.22509765625\n",
      "Epoch [170/250], Training Loss: 36.131243072695035, Validation Loss: 3975.433349609375\n",
      "Epoch [171/250], Training Loss: 36.082906879213276, Validation Loss: 3973.575439453125\n",
      "Epoch [172/250], Training Loss: 36.035637272170504, Validation Loss: 3971.771728515625\n",
      "Epoch [173/250], Training Loss: 35.99174124679939, Validation Loss: 3970.087890625\n",
      "Epoch [174/250], Training Loss: 35.94717419359581, Validation Loss: 3968.423095703125\n",
      "Epoch [175/250], Training Loss: 35.902860213093966, Validation Loss: 3966.8388671875\n",
      "Epoch [176/250], Training Loss: 35.86096739833468, Validation Loss: 3965.24609375\n",
      "Epoch [177/250], Training Loss: 35.818692702691905, Validation Loss: 3963.653076171875\n",
      "Epoch [178/250], Training Loss: 35.775580765543005, Validation Loss: 3962.069091796875\n",
      "Epoch [179/250], Training Loss: 35.734098947728754, Validation Loss: 3960.455322265625\n",
      "Epoch [180/250], Training Loss: 35.69268750212679, Validation Loss: 3958.904541015625\n",
      "Epoch [181/250], Training Loss: 35.65323731351614, Validation Loss: 3957.292236328125\n",
      "Epoch [182/250], Training Loss: 35.61450515714871, Validation Loss: 3955.7119140625\n",
      "Epoch [183/250], Training Loss: 35.57511647272177, Validation Loss: 3954.123046875\n",
      "Epoch [184/250], Training Loss: 35.53505827565488, Validation Loss: 3952.47900390625\n",
      "Epoch [185/250], Training Loss: 35.496065690543794, Validation Loss: 3950.836669921875\n",
      "Epoch [186/250], Training Loss: 35.459784943082646, Validation Loss: 3949.24951171875\n",
      "Epoch [187/250], Training Loss: 35.42193645282912, Validation Loss: 3947.603515625\n",
      "Epoch [188/250], Training Loss: 35.38426364016374, Validation Loss: 3945.927001953125\n",
      "Epoch [189/250], Training Loss: 35.34595940787123, Validation Loss: 3944.143310546875\n",
      "Epoch [190/250], Training Loss: 35.30762214655912, Validation Loss: 3942.441162109375\n",
      "Epoch [191/250], Training Loss: 35.27047426340733, Validation Loss: 3940.77685546875\n",
      "Epoch [192/250], Training Loss: 35.23447695845175, Validation Loss: 3939.163330078125\n",
      "Epoch [193/250], Training Loss: 35.197736833819505, Validation Loss: 3937.603759765625\n",
      "Epoch [194/250], Training Loss: 35.16171166830653, Validation Loss: 3936.134521484375\n",
      "Epoch [195/250], Training Loss: 35.12705569370047, Validation Loss: 3934.666748046875\n",
      "Epoch [196/250], Training Loss: 35.09412466597522, Validation Loss: 3933.374755859375\n",
      "Epoch [197/250], Training Loss: 35.06038634416825, Validation Loss: 3932.10107421875\n",
      "Epoch [198/250], Training Loss: 35.02793918509758, Validation Loss: 3930.801513671875\n",
      "Epoch [199/250], Training Loss: 34.995771928125635, Validation Loss: 3929.489990234375\n",
      "Epoch [200/250], Training Loss: 34.96342797046757, Validation Loss: 3928.139892578125\n",
      "Epoch [201/250], Training Loss: 34.93007207150065, Validation Loss: 3926.728759765625\n",
      "Epoch [202/250], Training Loss: 34.8957191697815, Validation Loss: 3925.283203125\n",
      "Epoch [203/250], Training Loss: 34.86535996429418, Validation Loss: 3923.9033203125\n",
      "Epoch [204/250], Training Loss: 34.83479599915898, Validation Loss: 3922.578857421875\n",
      "Epoch [205/250], Training Loss: 34.802322509519826, Validation Loss: 3921.18896484375\n",
      "Epoch [206/250], Training Loss: 34.771336204524786, Validation Loss: 3919.78173828125\n",
      "Epoch [207/250], Training Loss: 34.74091234408746, Validation Loss: 3918.423583984375\n",
      "Epoch [208/250], Training Loss: 34.70923384062441, Validation Loss: 3917.092529296875\n",
      "Epoch [209/250], Training Loss: 34.67892139429174, Validation Loss: 3915.791259765625\n",
      "Epoch [210/250], Training Loss: 34.64769080290444, Validation Loss: 3914.519775390625\n",
      "Epoch [211/250], Training Loss: 34.61754445545855, Validation Loss: 3913.271728515625\n",
      "Epoch [212/250], Training Loss: 34.590647935642735, Validation Loss: 3912.099609375\n",
      "Epoch [213/250], Training Loss: 34.55945370497215, Validation Loss: 3910.906494140625\n",
      "Epoch [214/250], Training Loss: 34.53172447252509, Validation Loss: 3909.77392578125\n",
      "Epoch [215/250], Training Loss: 34.505401312951925, Validation Loss: 3908.607421875\n",
      "Epoch [216/250], Training Loss: 34.47593369357769, Validation Loss: 3907.427978515625\n",
      "Epoch [217/250], Training Loss: 34.447849565004475, Validation Loss: 3906.2021484375\n",
      "Epoch [218/250], Training Loss: 34.42120375470574, Validation Loss: 3904.976318359375\n",
      "Epoch [219/250], Training Loss: 34.393794640764035, Validation Loss: 3903.8056640625\n",
      "Epoch [220/250], Training Loss: 34.36615415100727, Validation Loss: 3902.6240234375\n",
      "Epoch [221/250], Training Loss: 34.33899899040161, Validation Loss: 3901.4736328125\n",
      "Epoch [222/250], Training Loss: 34.312932949431925, Validation Loss: 3900.347900390625\n",
      "Epoch [223/250], Training Loss: 34.28904083358498, Validation Loss: 3899.277099609375\n",
      "Epoch [224/250], Training Loss: 34.26302367835522, Validation Loss: 3898.16552734375\n",
      "Epoch [225/250], Training Loss: 34.238031835835734, Validation Loss: 3897.0556640625\n",
      "Epoch [226/250], Training Loss: 34.212531716939615, Validation Loss: 3895.947021484375\n",
      "Epoch [227/250], Training Loss: 34.1889210093778, Validation Loss: 3894.9208984375\n",
      "Epoch [228/250], Training Loss: 34.16451877575692, Validation Loss: 3893.864013671875\n",
      "Epoch [229/250], Training Loss: 34.14218915010235, Validation Loss: 3892.893310546875\n",
      "Epoch [230/250], Training Loss: 34.118872073893954, Validation Loss: 3891.95263671875\n",
      "Epoch [231/250], Training Loss: 34.09618481936542, Validation Loss: 3890.9794921875\n",
      "Epoch [232/250], Training Loss: 34.07396397114791, Validation Loss: 3890.048095703125\n",
      "Epoch [233/250], Training Loss: 34.0521844415041, Validation Loss: 3889.1318359375\n",
      "Epoch [234/250], Training Loss: 34.03022240076666, Validation Loss: 3888.1572265625\n",
      "Epoch [235/250], Training Loss: 34.00749824950267, Validation Loss: 3887.18115234375\n",
      "Epoch [236/250], Training Loss: 33.98572548616294, Validation Loss: 3886.230712890625\n",
      "Epoch [237/250], Training Loss: 33.964213613986445, Validation Loss: 3885.2783203125\n",
      "Epoch [238/250], Training Loss: 33.94230084707571, Validation Loss: 3884.312255859375\n",
      "Epoch [239/250], Training Loss: 33.91928503515679, Validation Loss: 3883.346435546875\n",
      "Epoch [240/250], Training Loss: 33.89895603499836, Validation Loss: 3882.43408203125\n",
      "Epoch [241/250], Training Loss: 33.87839229033356, Validation Loss: 3881.605224609375\n",
      "Epoch [242/250], Training Loss: 33.86001493941142, Validation Loss: 3880.7587890625\n",
      "Epoch [243/250], Training Loss: 33.839250089856506, Validation Loss: 3879.905517578125\n",
      "Epoch [244/250], Training Loss: 33.81969936171535, Validation Loss: 3879.0654296875\n",
      "Epoch [245/250], Training Loss: 33.799411101451, Validation Loss: 3878.2314453125\n",
      "Epoch [246/250], Training Loss: 33.78018050542286, Validation Loss: 3877.39306640625\n",
      "Epoch [247/250], Training Loss: 33.76142051975366, Validation Loss: 3876.538330078125\n",
      "Epoch [248/250], Training Loss: 33.74043673961567, Validation Loss: 3875.60693359375\n",
      "Epoch [249/250], Training Loss: 33.71946212458582, Validation Loss: 3874.668212890625\n",
      "Epoch [250/250], Training Loss: 33.701156828247306, Validation Loss: 3873.79248046875\n",
      "Test Loss: 3581.885986328125\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.01, 30)\n",
      "Epoch [1/250], Training Loss: 14075.17679656264, Validation Loss: 18036.541015625\n",
      "Epoch [2/250], Training Loss: 10596.197732393794, Validation Loss: 13394.8544921875\n",
      "Epoch [3/250], Training Loss: 6886.063065660436, Validation Loss: 11018.7158203125\n",
      "Epoch [4/250], Training Loss: 4489.625967110118, Validation Loss: 11667.357421875\n",
      "Epoch [5/250], Training Loss: 2983.3175635410526, Validation Loss: 15813.4609375\n",
      "Epoch [6/250], Training Loss: 2024.264833322565, Validation Loss: 9401.19140625\n",
      "Epoch [7/250], Training Loss: 1423.9221651362254, Validation Loss: 7272.009765625\n",
      "Epoch [8/250], Training Loss: 1054.0414578670443, Validation Loss: 6512.310546875\n",
      "Epoch [9/250], Training Loss: 779.1323210661084, Validation Loss: 6121.873046875\n",
      "Epoch [10/250], Training Loss: 580.7987740759097, Validation Loss: 6389.9287109375\n",
      "Epoch [11/250], Training Loss: 434.28701082356235, Validation Loss: 8184.4482421875\n",
      "Epoch [12/250], Training Loss: 327.0027432701956, Validation Loss: 10013.1884765625\n",
      "Epoch [13/250], Training Loss: 248.63161482671256, Validation Loss: 11328.4853515625\n",
      "Epoch [14/250], Training Loss: 192.52509419660467, Validation Loss: 10817.5888671875\n",
      "Epoch [15/250], Training Loss: 152.91711569808194, Validation Loss: 13050.009765625\n",
      "Epoch [16/250], Training Loss: 135.96454084330435, Validation Loss: 11822.0341796875\n",
      "Epoch [17/250], Training Loss: 115.14255067122227, Validation Loss: 12340.4033203125\n",
      "Epoch [18/250], Training Loss: 108.05556360437868, Validation Loss: 11656.8203125\n",
      "Epoch [19/250], Training Loss: 98.82415370729683, Validation Loss: 11854.1337890625\n",
      "Epoch [20/250], Training Loss: 96.05889728951642, Validation Loss: 11896.6943359375\n",
      "Epoch [21/250], Training Loss: 86.34616896082427, Validation Loss: 14041.39453125\n",
      "Epoch [22/250], Training Loss: 81.31128102712005, Validation Loss: 14018.7001953125\n",
      "Epoch [23/250], Training Loss: 74.10073230949995, Validation Loss: 14395.0126953125\n",
      "Epoch [24/250], Training Loss: 74.0611917037233, Validation Loss: 15329.4541015625\n",
      "Epoch [25/250], Training Loss: 76.28658077190842, Validation Loss: 15685.310546875\n",
      "Epoch [26/250], Training Loss: 68.94103234310303, Validation Loss: 14433.5322265625\n",
      "Epoch [27/250], Training Loss: 65.24505151386741, Validation Loss: 14252.1142578125\n",
      "Epoch [28/250], Training Loss: 58.321280007293616, Validation Loss: 14136.203125\n",
      "Epoch [29/250], Training Loss: 52.753316858799174, Validation Loss: 14302.81640625\n",
      "Epoch [30/250], Training Loss: 51.7674926217612, Validation Loss: 15355.0732421875\n",
      "Epoch [31/250], Training Loss: 57.53928571543668, Validation Loss: 14815.419921875\n",
      "Epoch [32/250], Training Loss: 145.90267068090236, Validation Loss: 24606.953125\n",
      "Epoch [33/250], Training Loss: 87.41886807943618, Validation Loss: 15408.4580078125\n",
      "Epoch [34/250], Training Loss: 54.3925139791493, Validation Loss: 13438.5625\n",
      "Epoch [35/250], Training Loss: 41.773440400814955, Validation Loss: 14668.1220703125\n",
      "Epoch [36/250], Training Loss: 36.5277794804853, Validation Loss: 15671.39453125\n",
      "Epoch [37/250], Training Loss: 41.22886590257103, Validation Loss: 16671.841796875\n",
      "Epoch [38/250], Training Loss: 73.6926418994771, Validation Loss: 18443.376953125\n",
      "Epoch [39/250], Training Loss: 38.5973845136469, Validation Loss: 17580.94140625\n",
      "Epoch [40/250], Training Loss: 78.27585263259046, Validation Loss: 18908.193359375\n",
      "Epoch [41/250], Training Loss: 58.121900498811996, Validation Loss: 20440.798828125\n",
      "Epoch [42/250], Training Loss: 58.62266676426925, Validation Loss: 16951.66015625\n",
      "Epoch [43/250], Training Loss: 35.964611448411475, Validation Loss: 17523.748046875\n",
      "Epoch [44/250], Training Loss: 51.69741474067095, Validation Loss: 17405.728515625\n",
      "Epoch [45/250], Training Loss: 7479.929461405848, Validation Loss: 12709.3330078125\n",
      "Epoch [46/250], Training Loss: 10715.513953239031, Validation Loss: 19730.484375\n",
      "Epoch [47/250], Training Loss: 849.4690372094683, Validation Loss: 3456.2041015625\n",
      "Epoch [48/250], Training Loss: 79.44585357975338, Validation Loss: 6068.8857421875\n",
      "Epoch [49/250], Training Loss: 24.830331453224034, Validation Loss: 11510.1123046875\n",
      "Epoch [50/250], Training Loss: 122.41230678146374, Validation Loss: 9769.3486328125\n",
      "Epoch [51/250], Training Loss: 100.39366244884518, Validation Loss: 7789.00537109375\n",
      "Epoch [52/250], Training Loss: 54.42115743200335, Validation Loss: 9193.4755859375\n",
      "Epoch [53/250], Training Loss: 132.13471804134883, Validation Loss: 8793.2255859375\n",
      "Epoch [54/250], Training Loss: 34.953493447830915, Validation Loss: 4972.662109375\n",
      "Epoch [55/250], Training Loss: 37.9911492579688, Validation Loss: 4588.58740234375\n",
      "Epoch [56/250], Training Loss: 32.5252036221852, Validation Loss: 4467.96240234375\n",
      "Epoch [57/250], Training Loss: 39.74551796690071, Validation Loss: 4923.25439453125\n",
      "Epoch [58/250], Training Loss: 36.43875553389729, Validation Loss: 5783.83203125\n",
      "Epoch [59/250], Training Loss: 31.002608684379616, Validation Loss: 5486.9296875\n",
      "Epoch [60/250], Training Loss: 55.789698333839965, Validation Loss: 7100.89306640625\n",
      "Epoch [61/250], Training Loss: 31.460702162417594, Validation Loss: 6528.00439453125\n",
      "Epoch [62/250], Training Loss: 31.295016358698522, Validation Loss: 8913.876953125\n",
      "Epoch [63/250], Training Loss: 150.94008514066275, Validation Loss: 8267.1875\n",
      "Epoch [64/250], Training Loss: 77.17625393400628, Validation Loss: 7091.84130859375\n",
      "Epoch [65/250], Training Loss: 91.25594662270174, Validation Loss: 12133.099609375\n",
      "Epoch [66/250], Training Loss: 2802.982489547483, Validation Loss: 7270.125\n",
      "Epoch [67/250], Training Loss: 2539.464011841784, Validation Loss: 6342.43603515625\n",
      "Epoch [68/250], Training Loss: 2365.6914573482422, Validation Loss: 5571.8564453125\n",
      "Epoch [69/250], Training Loss: 2211.3482597159937, Validation Loss: 5220.09765625\n",
      "Epoch [70/250], Training Loss: 2051.8114849789445, Validation Loss: 5078.48779296875\n",
      "Epoch [71/250], Training Loss: 1928.3080313932223, Validation Loss: 5572.767578125\n",
      "Epoch [72/250], Training Loss: 1818.7584885219408, Validation Loss: 4948.73193359375\n",
      "Epoch [73/250], Training Loss: 1657.4720498123474, Validation Loss: 6360.97216796875\n",
      "Epoch [74/250], Training Loss: 1520.37886513321, Validation Loss: 5170.00439453125\n",
      "Epoch [75/250], Training Loss: 1548.1676052757782, Validation Loss: 5097.60888671875\n",
      "Epoch [76/250], Training Loss: 1302.9912347494844, Validation Loss: 4862.75830078125\n",
      "Epoch [77/250], Training Loss: 1213.751933636688, Validation Loss: 4942.29443359375\n",
      "Epoch [78/250], Training Loss: 1187.388148603387, Validation Loss: 5147.00732421875\n",
      "Epoch [79/250], Training Loss: 1080.7393528485825, Validation Loss: 4816.52001953125\n",
      "Epoch [80/250], Training Loss: 942.3462423164652, Validation Loss: 3918.989501953125\n",
      "Epoch [81/250], Training Loss: 836.7495434093829, Validation Loss: 3715.18310546875\n",
      "Epoch [82/250], Training Loss: 754.4058381294986, Validation Loss: 3972.662109375\n",
      "Epoch [83/250], Training Loss: 655.0609864019457, Validation Loss: 4280.16015625\n",
      "Epoch [84/250], Training Loss: 597.2650202457068, Validation Loss: 4779.203125\n",
      "Epoch [85/250], Training Loss: 538.8200564112974, Validation Loss: 5133.7705078125\n",
      "Epoch [86/250], Training Loss: 488.44469241366164, Validation Loss: 5182.90869140625\n",
      "Epoch [87/250], Training Loss: 447.6019019724902, Validation Loss: 5224.5546875\n",
      "Epoch [88/250], Training Loss: 407.6230283779655, Validation Loss: 5441.42236328125\n",
      "Epoch [89/250], Training Loss: 376.34580494903906, Validation Loss: 5038.1220703125\n",
      "Epoch [90/250], Training Loss: 349.9035393703423, Validation Loss: 5310.0947265625\n",
      "Epoch [91/250], Training Loss: 319.8506846345686, Validation Loss: 4738.37939453125\n",
      "Epoch [92/250], Training Loss: 297.4591560988431, Validation Loss: 4716.9013671875\n",
      "Epoch [93/250], Training Loss: 276.2041198296346, Validation Loss: 4998.083984375\n",
      "Epoch [94/250], Training Loss: 265.0227583568907, Validation Loss: 4172.50244140625\n",
      "Epoch [95/250], Training Loss: 251.47641916408088, Validation Loss: 4330.06005859375\n",
      "Epoch [96/250], Training Loss: 230.18464472010558, Validation Loss: 5516.486328125\n",
      "Epoch [97/250], Training Loss: 212.67006363286134, Validation Loss: 4670.76416015625\n",
      "Epoch [98/250], Training Loss: 197.60865010186572, Validation Loss: 4531.49462890625\n",
      "Epoch [99/250], Training Loss: 187.6489211155699, Validation Loss: 4706.98486328125\n",
      "Epoch [100/250], Training Loss: 176.25875237485852, Validation Loss: 4571.40478515625\n",
      "Epoch [101/250], Training Loss: 169.554948299222, Validation Loss: 4932.16552734375\n",
      "Epoch [102/250], Training Loss: 159.85301105083823, Validation Loss: 4571.7392578125\n",
      "Epoch [103/250], Training Loss: 151.2735203478548, Validation Loss: 4511.060546875\n",
      "Epoch [104/250], Training Loss: 143.855066295122, Validation Loss: 4467.9736328125\n",
      "Epoch [105/250], Training Loss: 136.0673714607876, Validation Loss: 4406.8779296875\n",
      "Epoch [106/250], Training Loss: 129.60554617433576, Validation Loss: 4447.0751953125\n",
      "Epoch [107/250], Training Loss: 123.78999378941782, Validation Loss: 4486.16748046875\n",
      "Epoch [108/250], Training Loss: 118.37159727971788, Validation Loss: 4520.638671875\n",
      "Epoch [109/250], Training Loss: 113.42279860752794, Validation Loss: 4533.06640625\n",
      "Epoch [110/250], Training Loss: 108.68517370412587, Validation Loss: 4516.126953125\n",
      "Epoch [111/250], Training Loss: 104.14898561595052, Validation Loss: 4478.111328125\n",
      "Epoch [112/250], Training Loss: 99.97938672991829, Validation Loss: 4429.744140625\n",
      "Epoch [113/250], Training Loss: 96.21743132144965, Validation Loss: 4395.412109375\n",
      "Epoch [114/250], Training Loss: 92.73315470704206, Validation Loss: 4363.4501953125\n",
      "Epoch [115/250], Training Loss: 89.37999126260694, Validation Loss: 4331.3271484375\n",
      "Epoch [116/250], Training Loss: 86.17096077203743, Validation Loss: 4314.39794921875\n",
      "Epoch [117/250], Training Loss: 83.36939830682104, Validation Loss: 4326.7724609375\n",
      "Epoch [118/250], Training Loss: 80.78838365633682, Validation Loss: 4334.11962890625\n",
      "Epoch [119/250], Training Loss: 78.44575027761414, Validation Loss: 4332.9677734375\n",
      "Epoch [120/250], Training Loss: 76.20375508798351, Validation Loss: 4308.35693359375\n",
      "Epoch [121/250], Training Loss: 74.04710617083897, Validation Loss: 4263.7431640625\n",
      "Epoch [122/250], Training Loss: 71.97193404834755, Validation Loss: 4199.01708984375\n",
      "Epoch [123/250], Training Loss: 70.06034268865542, Validation Loss: 4125.646484375\n",
      "Epoch [124/250], Training Loss: 68.26770440177013, Validation Loss: 4040.09130859375\n",
      "Epoch [125/250], Training Loss: 66.58334912173126, Validation Loss: 3932.22998046875\n",
      "Epoch [126/250], Training Loss: 65.06357034138149, Validation Loss: 3821.550048828125\n",
      "Epoch [127/250], Training Loss: 63.67332282085678, Validation Loss: 3729.19384765625\n",
      "Epoch [128/250], Training Loss: 62.310053887118336, Validation Loss: 3651.392822265625\n",
      "Epoch [129/250], Training Loss: 61.07578430657422, Validation Loss: 3560.775146484375\n",
      "Epoch [130/250], Training Loss: 59.960043855756204, Validation Loss: 3463.092041015625\n",
      "Epoch [131/250], Training Loss: 58.936240774727246, Validation Loss: 3370.33935546875\n",
      "Epoch [132/250], Training Loss: 57.986136356174356, Validation Loss: 3288.2841796875\n",
      "Epoch [133/250], Training Loss: 57.10895250871983, Validation Loss: 3217.753662109375\n",
      "Epoch [134/250], Training Loss: 56.29489517693944, Validation Loss: 3157.903564453125\n",
      "Epoch [135/250], Training Loss: 55.54631656290975, Validation Loss: 3108.173095703125\n",
      "Epoch [136/250], Training Loss: 54.86092026823201, Validation Loss: 3068.799072265625\n",
      "Epoch [137/250], Training Loss: 54.23650965636241, Validation Loss: 3039.99560546875\n",
      "Epoch [138/250], Training Loss: 53.65016034152104, Validation Loss: 3021.215576171875\n",
      "Epoch [139/250], Training Loss: 53.10200215712998, Validation Loss: 3010.8330078125\n",
      "Epoch [140/250], Training Loss: 52.58377829172121, Validation Loss: 3006.602783203125\n",
      "Epoch [141/250], Training Loss: 52.10017211091808, Validation Loss: 3006.749755859375\n",
      "Epoch [142/250], Training Loss: 51.64230840069569, Validation Loss: 3009.855712890625\n",
      "Epoch [143/250], Training Loss: 51.22306571158342, Validation Loss: 3015.190673828125\n",
      "Epoch [144/250], Training Loss: 50.82900859701758, Validation Loss: 3021.81494140625\n",
      "Epoch [145/250], Training Loss: 50.46042195009887, Validation Loss: 3029.4111328125\n",
      "Epoch [146/250], Training Loss: 50.11200990813759, Validation Loss: 3037.42333984375\n",
      "Epoch [147/250], Training Loss: 49.789024623146645, Validation Loss: 3045.7470703125\n",
      "Epoch [148/250], Training Loss: 49.49065028323043, Validation Loss: 3054.16796875\n",
      "Epoch [149/250], Training Loss: 49.206257579774864, Validation Loss: 3062.47705078125\n",
      "Epoch [150/250], Training Loss: 48.94058177622252, Validation Loss: 3070.743408203125\n",
      "Epoch [151/250], Training Loss: 48.69914227840702, Validation Loss: 3079.014404296875\n",
      "Epoch [152/250], Training Loss: 48.463981926642525, Validation Loss: 3087.301513671875\n",
      "Epoch [153/250], Training Loss: 48.24199864982709, Validation Loss: 3095.72412109375\n",
      "Epoch [154/250], Training Loss: 48.03755922386186, Validation Loss: 3104.8876953125\n",
      "Epoch [155/250], Training Loss: 47.84406494669316, Validation Loss: 3115.019287109375\n",
      "Epoch [156/250], Training Loss: 47.65970972457262, Validation Loss: 3126.759521484375\n",
      "Epoch [157/250], Training Loss: 47.48258046654422, Validation Loss: 3140.332763671875\n",
      "Epoch [158/250], Training Loss: 47.316850948583905, Validation Loss: 3156.122314453125\n",
      "Epoch [159/250], Training Loss: 47.15052778436103, Validation Loss: 3173.740478515625\n",
      "Epoch [160/250], Training Loss: 46.98536242920898, Validation Loss: 3193.29541015625\n",
      "Epoch [161/250], Training Loss: 46.83195346990312, Validation Loss: 3214.84912109375\n",
      "Epoch [162/250], Training Loss: 46.67110055567832, Validation Loss: 3237.185791015625\n",
      "Epoch [163/250], Training Loss: 46.507349467521244, Validation Loss: 3260.1357421875\n",
      "Epoch [164/250], Training Loss: 46.34713515133924, Validation Loss: 3283.752685546875\n",
      "Epoch [165/250], Training Loss: 46.18425944616806, Validation Loss: 3307.7841796875\n",
      "Epoch [166/250], Training Loss: 46.02027238799812, Validation Loss: 3331.936767578125\n",
      "Epoch [167/250], Training Loss: 45.860880460388515, Validation Loss: 3356.39794921875\n",
      "Epoch [168/250], Training Loss: 45.69908256397332, Validation Loss: 3380.64208984375\n",
      "Epoch [169/250], Training Loss: 45.53980989328266, Validation Loss: 3405.03173828125\n",
      "Epoch [170/250], Training Loss: 45.38209717530038, Validation Loss: 3429.8232421875\n",
      "Epoch [171/250], Training Loss: 45.22887034144068, Validation Loss: 3454.77197265625\n",
      "Epoch [172/250], Training Loss: 45.08124845875996, Validation Loss: 3480.204833984375\n",
      "Epoch [173/250], Training Loss: 44.93333738294899, Validation Loss: 3505.914794921875\n",
      "Epoch [174/250], Training Loss: 44.79009245867231, Validation Loss: 3532.000732421875\n",
      "Epoch [175/250], Training Loss: 44.6477082146229, Validation Loss: 3558.533447265625\n",
      "Epoch [176/250], Training Loss: 44.5131617817786, Validation Loss: 3585.5791015625\n",
      "Epoch [177/250], Training Loss: 44.37743406624804, Validation Loss: 3612.958251953125\n",
      "Epoch [178/250], Training Loss: 44.24654942676744, Validation Loss: 3640.616455078125\n",
      "Epoch [179/250], Training Loss: 44.112918777714974, Validation Loss: 3668.58447265625\n",
      "Epoch [180/250], Training Loss: 43.985938161037325, Validation Loss: 3696.7216796875\n",
      "Epoch [181/250], Training Loss: 43.851553872829015, Validation Loss: 3724.48974609375\n",
      "Epoch [182/250], Training Loss: 43.72275282851863, Validation Loss: 3751.904052734375\n",
      "Epoch [183/250], Training Loss: 43.58566825691335, Validation Loss: 3778.591796875\n",
      "Epoch [184/250], Training Loss: 43.44451214490051, Validation Loss: 3804.270263671875\n",
      "Epoch [185/250], Training Loss: 43.29816265501006, Validation Loss: 3828.746337890625\n",
      "Epoch [186/250], Training Loss: 43.145524201659036, Validation Loss: 3851.94384765625\n",
      "Epoch [187/250], Training Loss: 42.98618967187731, Validation Loss: 3873.90087890625\n",
      "Epoch [188/250], Training Loss: 42.819071159719435, Validation Loss: 3894.2607421875\n",
      "Epoch [189/250], Training Loss: 42.642166288620764, Validation Loss: 3913.412353515625\n",
      "Epoch [190/250], Training Loss: 42.45642916016741, Validation Loss: 3931.3603515625\n",
      "Epoch [191/250], Training Loss: 42.269031958989885, Validation Loss: 3948.761474609375\n",
      "Epoch [192/250], Training Loss: 42.07499914404048, Validation Loss: 3965.796630859375\n",
      "Epoch [193/250], Training Loss: 41.87895130912117, Validation Loss: 3982.9521484375\n",
      "Epoch [194/250], Training Loss: 41.684571182856835, Validation Loss: 4000.232177734375\n",
      "Epoch [195/250], Training Loss: 41.48259081949408, Validation Loss: 4017.543212890625\n",
      "Epoch [196/250], Training Loss: 41.28464670786318, Validation Loss: 4034.98681640625\n",
      "Epoch [197/250], Training Loss: 41.08737435734765, Validation Loss: 4052.83251953125\n",
      "Epoch [198/250], Training Loss: 40.89651548054269, Validation Loss: 4070.482666015625\n",
      "Epoch [199/250], Training Loss: 40.702640999218666, Validation Loss: 4088.19921875\n",
      "Epoch [200/250], Training Loss: 40.510389828796285, Validation Loss: 4105.77734375\n",
      "Epoch [201/250], Training Loss: 40.32496743515434, Validation Loss: 4123.93798828125\n",
      "Epoch [202/250], Training Loss: 40.14370483779938, Validation Loss: 4142.57763671875\n",
      "Epoch [203/250], Training Loss: 39.96378699186133, Validation Loss: 4162.1689453125\n",
      "Epoch [204/250], Training Loss: 39.78839100332954, Validation Loss: 4182.62646484375\n",
      "Epoch [205/250], Training Loss: 39.61240796106613, Validation Loss: 4204.25439453125\n",
      "Epoch [206/250], Training Loss: 39.437803824127165, Validation Loss: 4226.73193359375\n",
      "Epoch [207/250], Training Loss: 39.26460368711571, Validation Loss: 4249.5\n",
      "Epoch [208/250], Training Loss: 39.08642377963126, Validation Loss: 4272.65234375\n",
      "Epoch [209/250], Training Loss: 38.90305435927775, Validation Loss: 4295.89404296875\n",
      "Epoch [210/250], Training Loss: 38.72184778717551, Validation Loss: 4319.18115234375\n",
      "Epoch [211/250], Training Loss: 38.541401561533455, Validation Loss: 4342.4287109375\n",
      "Epoch [212/250], Training Loss: 38.36002408665703, Validation Loss: 4365.51171875\n",
      "Epoch [213/250], Training Loss: 38.17431944376288, Validation Loss: 4388.1572265625\n",
      "Epoch [214/250], Training Loss: 37.99013648082739, Validation Loss: 4410.7861328125\n",
      "Epoch [215/250], Training Loss: 37.807603193134256, Validation Loss: 4433.52099609375\n",
      "Epoch [216/250], Training Loss: 37.622056332945704, Validation Loss: 4455.986328125\n",
      "Epoch [217/250], Training Loss: 37.43996035170609, Validation Loss: 4478.8564453125\n",
      "Epoch [218/250], Training Loss: 37.25223985926109, Validation Loss: 4501.34521484375\n",
      "Epoch [219/250], Training Loss: 37.06738169375718, Validation Loss: 4524.15234375\n",
      "Epoch [220/250], Training Loss: 36.88314933397717, Validation Loss: 4547.1259765625\n",
      "Epoch [221/250], Training Loss: 36.698308763221775, Validation Loss: 4569.9267578125\n",
      "Epoch [222/250], Training Loss: 36.51414411000714, Validation Loss: 4592.37548828125\n",
      "Epoch [223/250], Training Loss: 36.32816095575508, Validation Loss: 4615.2529296875\n",
      "Epoch [224/250], Training Loss: 36.14742033588139, Validation Loss: 4638.35498046875\n",
      "Epoch [225/250], Training Loss: 35.967382452407996, Validation Loss: 4661.13037109375\n",
      "Epoch [226/250], Training Loss: 35.78232760473827, Validation Loss: 4684.20556640625\n",
      "Epoch [227/250], Training Loss: 35.600758142102976, Validation Loss: 4707.24609375\n",
      "Epoch [228/250], Training Loss: 35.41536394263493, Validation Loss: 4730.2080078125\n",
      "Epoch [229/250], Training Loss: 35.22952128865442, Validation Loss: 4753.18212890625\n",
      "Epoch [230/250], Training Loss: 35.04215336196567, Validation Loss: 4775.18212890625\n",
      "Epoch [231/250], Training Loss: 34.85089207510724, Validation Loss: 4795.322265625\n",
      "Epoch [232/250], Training Loss: 34.657388745440166, Validation Loss: 4813.28173828125\n",
      "Epoch [233/250], Training Loss: 34.46473156170859, Validation Loss: 4828.57080078125\n",
      "Epoch [234/250], Training Loss: 34.276911963336275, Validation Loss: 4841.8564453125\n",
      "Epoch [235/250], Training Loss: 34.10471489730735, Validation Loss: 4855.01318359375\n",
      "Epoch [236/250], Training Loss: 33.94293492161992, Validation Loss: 4871.931640625\n",
      "Epoch [237/250], Training Loss: 33.78700377152921, Validation Loss: 4896.623046875\n",
      "Epoch [238/250], Training Loss: 33.61241008891668, Validation Loss: 4928.427734375\n",
      "Epoch [239/250], Training Loss: 33.39686934329986, Validation Loss: 4955.87353515625\n",
      "Epoch [240/250], Training Loss: 33.13405558034809, Validation Loss: 4971.4228515625\n",
      "Epoch [241/250], Training Loss: 32.83661266879583, Validation Loss: 4979.7060546875\n",
      "Epoch [242/250], Training Loss: 32.52411084373866, Validation Loss: 4993.404296875\n",
      "Epoch [243/250], Training Loss: 32.50771642077569, Validation Loss: 5012.71875\n",
      "Epoch [244/250], Training Loss: 32.404534881149196, Validation Loss: 5031.0771484375\n",
      "Epoch [245/250], Training Loss: 32.22632692294832, Validation Loss: 5035.908203125\n",
      "Epoch [246/250], Training Loss: 32.0344476641136, Validation Loss: 5029.921875\n",
      "Epoch [247/250], Training Loss: 31.826764432137335, Validation Loss: 5013.4072265625\n",
      "Epoch [248/250], Training Loss: 31.65219713019988, Validation Loss: 5043.0078125\n",
      "Epoch [249/250], Training Loss: 31.493381227241162, Validation Loss: 5042.20458984375\n",
      "Epoch [250/250], Training Loss: 31.336330391296425, Validation Loss: 5057.6318359375\n",
      "Test Loss: 3741.117431640625\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.005, 1)\n",
      "Epoch [1/250], Training Loss: 19706.677615989312, Validation Loss: 20241.173828125\n",
      "Epoch [2/250], Training Loss: 14640.90949594959, Validation Loss: 16171.009765625\n",
      "Epoch [3/250], Training Loss: 11587.92121752712, Validation Loss: 12271.8291015625\n",
      "Epoch [4/250], Training Loss: 9528.940704527371, Validation Loss: 10877.291015625\n",
      "Epoch [5/250], Training Loss: 8026.183466301792, Validation Loss: 14484.40234375\n",
      "Epoch [6/250], Training Loss: 12209.324050941508, Validation Loss: 13178.3359375\n",
      "Epoch [7/250], Training Loss: 11083.730594645642, Validation Loss: 11822.029296875\n",
      "Epoch [8/250], Training Loss: 10450.45717314266, Validation Loss: 11419.9443359375\n",
      "Epoch [9/250], Training Loss: 9910.907212752352, Validation Loss: 11082.1552734375\n",
      "Epoch [10/250], Training Loss: 7768.33572677908, Validation Loss: 6285.9921875\n",
      "Epoch [11/250], Training Loss: 3735.7985663693776, Validation Loss: 5712.70263671875\n",
      "Epoch [12/250], Training Loss: 3324.105742192469, Validation Loss: 5236.35400390625\n",
      "Epoch [13/250], Training Loss: 2996.877411086792, Validation Loss: 4871.27197265625\n",
      "Epoch [14/250], Training Loss: 2738.207502213531, Validation Loss: 4638.0947265625\n",
      "Epoch [15/250], Training Loss: 2501.591596192252, Validation Loss: 4489.84423828125\n",
      "Epoch [16/250], Training Loss: 2404.257578947213, Validation Loss: 4466.40576171875\n",
      "Epoch [17/250], Training Loss: 2333.6332156910553, Validation Loss: 4400.8134765625\n",
      "Epoch [18/250], Training Loss: 3093.4673757663213, Validation Loss: 4779.74072265625\n",
      "Epoch [19/250], Training Loss: 6549.682113096882, Validation Loss: 32546.115234375\n",
      "Epoch [20/250], Training Loss: 7101.1448694125575, Validation Loss: 28104.796875\n",
      "Epoch [21/250], Training Loss: 6036.938611882265, Validation Loss: 7805.6181640625\n",
      "Epoch [22/250], Training Loss: 5579.329524725335, Validation Loss: 8948.669921875\n",
      "Epoch [23/250], Training Loss: 5309.210896261438, Validation Loss: 8000.65185546875\n",
      "Epoch [24/250], Training Loss: 1706.092373879806, Validation Loss: 4748.75732421875\n",
      "Epoch [25/250], Training Loss: 2081.124473507468, Validation Loss: 4083.734375\n",
      "Epoch [26/250], Training Loss: 1884.3024074174132, Validation Loss: 4451.16259765625\n",
      "Epoch [27/250], Training Loss: 4923.180185054903, Validation Loss: 42066.96875\n",
      "Epoch [28/250], Training Loss: 4930.721034752967, Validation Loss: 21488.818359375\n",
      "Epoch [29/250], Training Loss: 4369.335823774549, Validation Loss: 8020.71533203125\n",
      "Epoch [30/250], Training Loss: 4249.7697975259425, Validation Loss: 7460.560546875\n",
      "Epoch [31/250], Training Loss: 3203.589040674099, Validation Loss: 3694.887939453125\n",
      "Epoch [32/250], Training Loss: 4050.103111197855, Validation Loss: 18995.978515625\n",
      "Epoch [33/250], Training Loss: 3786.6468738452713, Validation Loss: 7020.494140625\n",
      "Epoch [34/250], Training Loss: 1929.8231358689525, Validation Loss: 5799.85888671875\n",
      "Epoch [35/250], Training Loss: 2535.7363581640952, Validation Loss: 6359.43017578125\n",
      "Epoch [36/250], Training Loss: 3898.3493618344987, Validation Loss: 8532.4208984375\n",
      "Epoch [37/250], Training Loss: 3788.8510109679296, Validation Loss: 6496.64013671875\n",
      "Epoch [38/250], Training Loss: 3348.0598131185043, Validation Loss: 4353.3076171875\n",
      "Epoch [39/250], Training Loss: 2430.5234514532704, Validation Loss: 25229.166015625\n",
      "Epoch [40/250], Training Loss: 2573.832462859391, Validation Loss: 9016.1259765625\n",
      "Epoch [41/250], Training Loss: 2859.6807353008894, Validation Loss: 4567.568359375\n",
      "Epoch [42/250], Training Loss: 3367.786777799496, Validation Loss: 4694.67529296875\n",
      "Epoch [43/250], Training Loss: 1833.737096789424, Validation Loss: 4562.75537109375\n",
      "Epoch [44/250], Training Loss: 1048.2344290328008, Validation Loss: 4598.28076171875\n",
      "Epoch [45/250], Training Loss: 794.6716129045401, Validation Loss: 4255.54248046875\n",
      "Epoch [46/250], Training Loss: 631.0063540865716, Validation Loss: 3808.361572265625\n",
      "Epoch [47/250], Training Loss: 489.0060154942315, Validation Loss: 3131.449462890625\n",
      "Epoch [48/250], Training Loss: 390.073252740399, Validation Loss: 3139.418212890625\n",
      "Epoch [49/250], Training Loss: 326.9528266108079, Validation Loss: 3435.06298828125\n",
      "Epoch [50/250], Training Loss: 278.3137396638752, Validation Loss: 3331.495849609375\n",
      "Epoch [51/250], Training Loss: 231.0072948382587, Validation Loss: 2935.428955078125\n",
      "Epoch [52/250], Training Loss: 187.7768047633057, Validation Loss: 2280.880615234375\n",
      "Epoch [53/250], Training Loss: 146.7436066198622, Validation Loss: 1660.6849365234375\n",
      "Epoch [54/250], Training Loss: 112.26761412834892, Validation Loss: 1360.083251953125\n",
      "Epoch [55/250], Training Loss: 94.29530382829269, Validation Loss: 997.0640258789062\n",
      "Epoch [56/250], Training Loss: 84.44864734602953, Validation Loss: 921.2589111328125\n",
      "Epoch [57/250], Training Loss: 79.60416923778007, Validation Loss: 889.4635009765625\n",
      "Epoch [58/250], Training Loss: 76.77917538617444, Validation Loss: 861.0391235351562\n",
      "Epoch [59/250], Training Loss: 74.74544677821459, Validation Loss: 842.3016967773438\n",
      "Epoch [60/250], Training Loss: 73.2110672930047, Validation Loss: 832.8297119140625\n",
      "Epoch [61/250], Training Loss: 72.00360660484046, Validation Loss: 829.1575927734375\n",
      "Epoch [62/250], Training Loss: 71.02117130328408, Validation Loss: 828.43701171875\n",
      "Epoch [63/250], Training Loss: 70.2042939083057, Validation Loss: 828.9302978515625\n",
      "Epoch [64/250], Training Loss: 69.50987457843995, Validation Loss: 829.7550659179688\n",
      "Epoch [65/250], Training Loss: 68.90190301485943, Validation Loss: 830.507080078125\n",
      "Epoch [66/250], Training Loss: 68.35796641876522, Validation Loss: 831.04150390625\n",
      "Epoch [67/250], Training Loss: 67.85709000507102, Validation Loss: 831.2943725585938\n",
      "Epoch [68/250], Training Loss: 67.39009801342188, Validation Loss: 831.1499633789062\n",
      "Epoch [69/250], Training Loss: 66.94576703091403, Validation Loss: 830.4776000976562\n",
      "Epoch [70/250], Training Loss: 66.5178508259759, Validation Loss: 829.2919921875\n",
      "Epoch [71/250], Training Loss: 66.10810474853413, Validation Loss: 827.6187744140625\n",
      "Epoch [72/250], Training Loss: 65.71828874438523, Validation Loss: 825.591552734375\n",
      "Epoch [73/250], Training Loss: 65.34677537599599, Validation Loss: 823.3543701171875\n",
      "Epoch [74/250], Training Loss: 64.99838191113433, Validation Loss: 821.0940551757812\n",
      "Epoch [75/250], Training Loss: 64.66947297008791, Validation Loss: 818.94482421875\n",
      "Epoch [76/250], Training Loss: 64.36966547989175, Validation Loss: 817.0123901367188\n",
      "Epoch [77/250], Training Loss: 64.09335623043441, Validation Loss: 815.3275146484375\n",
      "Epoch [78/250], Training Loss: 63.83992420103528, Validation Loss: 813.910888671875\n",
      "Epoch [79/250], Training Loss: 63.60975644218652, Validation Loss: 812.7568359375\n",
      "Epoch [80/250], Training Loss: 63.402307344174446, Validation Loss: 811.8388061523438\n",
      "Epoch [81/250], Training Loss: 63.216206405562744, Validation Loss: 811.1102905273438\n",
      "Epoch [82/250], Training Loss: 63.049178444579425, Validation Loss: 810.515380859375\n",
      "Epoch [83/250], Training Loss: 62.896339096614604, Validation Loss: 809.9865112304688\n",
      "Epoch [84/250], Training Loss: 62.75412713714831, Validation Loss: 809.5244750976562\n",
      "Epoch [85/250], Training Loss: 62.62801256108277, Validation Loss: 809.0985107421875\n",
      "Epoch [86/250], Training Loss: 62.51018510225032, Validation Loss: 808.68212890625\n",
      "Epoch [87/250], Training Loss: 62.40243955657851, Validation Loss: 808.2443237304688\n",
      "Epoch [88/250], Training Loss: 62.30353018835395, Validation Loss: 807.7527465820312\n",
      "Epoch [89/250], Training Loss: 62.208738029625316, Validation Loss: 807.2101440429688\n",
      "Epoch [90/250], Training Loss: 62.11959244825102, Validation Loss: 806.589111328125\n",
      "Epoch [91/250], Training Loss: 62.03247574365292, Validation Loss: 805.910888671875\n",
      "Epoch [92/250], Training Loss: 61.949150688788485, Validation Loss: 805.1702270507812\n",
      "Epoch [93/250], Training Loss: 61.86850514585736, Validation Loss: 804.3745727539062\n",
      "Epoch [94/250], Training Loss: 61.79048783509831, Validation Loss: 803.5406494140625\n",
      "Epoch [95/250], Training Loss: 61.71691369576874, Validation Loss: 802.6487426757812\n",
      "Epoch [96/250], Training Loss: 61.64209251781591, Validation Loss: 801.7158813476562\n",
      "Epoch [97/250], Training Loss: 61.569345073044005, Validation Loss: 800.734619140625\n",
      "Epoch [98/250], Training Loss: 61.49646975760222, Validation Loss: 799.7117309570312\n",
      "Epoch [99/250], Training Loss: 61.42411122769277, Validation Loss: 798.6724853515625\n",
      "Epoch [100/250], Training Loss: 61.3527293999948, Validation Loss: 797.6359252929688\n",
      "Epoch [101/250], Training Loss: 61.28334498414517, Validation Loss: 796.6032104492188\n",
      "Epoch [102/250], Training Loss: 61.21422302378622, Validation Loss: 795.5585327148438\n",
      "Epoch [103/250], Training Loss: 61.14469537128177, Validation Loss: 794.5426635742188\n",
      "Epoch [104/250], Training Loss: 61.07761976072506, Validation Loss: 793.5535888671875\n",
      "Epoch [105/250], Training Loss: 61.01130377901643, Validation Loss: 792.5895385742188\n",
      "Epoch [106/250], Training Loss: 60.94556362618407, Validation Loss: 791.6629028320312\n",
      "Epoch [107/250], Training Loss: 60.88139616960048, Validation Loss: 790.7745361328125\n",
      "Epoch [108/250], Training Loss: 60.817434695404, Validation Loss: 789.9210815429688\n",
      "Epoch [109/250], Training Loss: 60.754549236740274, Validation Loss: 789.0994262695312\n",
      "Epoch [110/250], Training Loss: 60.690520484146774, Validation Loss: 788.326904296875\n",
      "Epoch [111/250], Training Loss: 60.62802105046649, Validation Loss: 787.625732421875\n",
      "Epoch [112/250], Training Loss: 60.56750209928176, Validation Loss: 786.9732055664062\n",
      "Epoch [113/250], Training Loss: 60.50729274157671, Validation Loss: 786.3737182617188\n",
      "Epoch [114/250], Training Loss: 60.446646220783826, Validation Loss: 785.848388671875\n",
      "Epoch [115/250], Training Loss: 60.389610501090125, Validation Loss: 785.3853759765625\n",
      "Epoch [116/250], Training Loss: 60.332457834416516, Validation Loss: 785.0046997070312\n",
      "Epoch [117/250], Training Loss: 60.277361777802014, Validation Loss: 784.68212890625\n",
      "Epoch [118/250], Training Loss: 60.22364277810038, Validation Loss: 784.4127197265625\n",
      "Epoch [119/250], Training Loss: 60.17048667821399, Validation Loss: 784.2030029296875\n",
      "Epoch [120/250], Training Loss: 60.118718827185255, Validation Loss: 784.0614013671875\n",
      "Epoch [121/250], Training Loss: 60.06860199158427, Validation Loss: 783.9685668945312\n",
      "Epoch [122/250], Training Loss: 60.01960535696947, Validation Loss: 783.9259643554688\n",
      "Epoch [123/250], Training Loss: 59.971042296126804, Validation Loss: 783.939208984375\n",
      "Epoch [124/250], Training Loss: 59.92216773978086, Validation Loss: 784.0238037109375\n",
      "Epoch [125/250], Training Loss: 59.876335970965584, Validation Loss: 784.1636962890625\n",
      "Epoch [126/250], Training Loss: 59.832663819413774, Validation Loss: 784.33251953125\n",
      "Epoch [127/250], Training Loss: 59.788972256798345, Validation Loss: 784.5347900390625\n",
      "Epoch [128/250], Training Loss: 59.74467993411974, Validation Loss: 784.7927856445312\n",
      "Epoch [129/250], Training Loss: 59.70170016746687, Validation Loss: 785.0924682617188\n",
      "Epoch [130/250], Training Loss: 59.659960785281044, Validation Loss: 785.4400024414062\n",
      "Epoch [131/250], Training Loss: 59.61895559364461, Validation Loss: 785.8267822265625\n",
      "Epoch [132/250], Training Loss: 59.57881570467386, Validation Loss: 786.2474365234375\n",
      "Epoch [133/250], Training Loss: 59.539831301902645, Validation Loss: 786.7116088867188\n",
      "Epoch [134/250], Training Loss: 59.50320453456903, Validation Loss: 787.201416015625\n",
      "Epoch [135/250], Training Loss: 59.467169224876145, Validation Loss: 787.730224609375\n",
      "Epoch [136/250], Training Loss: 59.43384355188622, Validation Loss: 788.2899169921875\n",
      "Epoch [137/250], Training Loss: 59.4019936525511, Validation Loss: 788.8717041015625\n",
      "Epoch [138/250], Training Loss: 59.37205878573077, Validation Loss: 789.455078125\n",
      "Epoch [139/250], Training Loss: 59.341583446716754, Validation Loss: 790.0419921875\n",
      "Epoch [140/250], Training Loss: 59.31054725332205, Validation Loss: 790.6632690429688\n",
      "Epoch [141/250], Training Loss: 59.28325366781578, Validation Loss: 791.2786865234375\n",
      "Epoch [142/250], Training Loss: 59.255047823327985, Validation Loss: 791.895751953125\n",
      "Epoch [143/250], Training Loss: 59.22643702952219, Validation Loss: 792.5072021484375\n",
      "Epoch [144/250], Training Loss: 59.19824631180587, Validation Loss: 793.1287841796875\n",
      "Epoch [145/250], Training Loss: 59.17151229655456, Validation Loss: 793.7589111328125\n",
      "Epoch [146/250], Training Loss: 59.14644553628527, Validation Loss: 794.3663330078125\n",
      "Epoch [147/250], Training Loss: 59.11993385717262, Validation Loss: 794.9703979492188\n",
      "Epoch [148/250], Training Loss: 59.09450102539092, Validation Loss: 795.5657348632812\n",
      "Epoch [149/250], Training Loss: 59.06979877464229, Validation Loss: 796.1526489257812\n",
      "Epoch [150/250], Training Loss: 59.04652917106981, Validation Loss: 796.735595703125\n",
      "Epoch [151/250], Training Loss: 59.025090028666106, Validation Loss: 797.2919921875\n",
      "Epoch [152/250], Training Loss: 59.004286713077434, Validation Loss: 797.8397216796875\n",
      "Epoch [153/250], Training Loss: 58.9851416151361, Validation Loss: 798.3566284179688\n",
      "Epoch [154/250], Training Loss: 58.965324006699674, Validation Loss: 798.8400268554688\n",
      "Epoch [155/250], Training Loss: 58.94476877805454, Validation Loss: 799.2996215820312\n",
      "Epoch [156/250], Training Loss: 58.92362082629014, Validation Loss: 799.7378540039062\n",
      "Epoch [157/250], Training Loss: 58.90324926218104, Validation Loss: 800.1471557617188\n",
      "Epoch [158/250], Training Loss: 58.88168732932124, Validation Loss: 800.5472412109375\n",
      "Epoch [159/250], Training Loss: 58.86201739112067, Validation Loss: 800.920654296875\n",
      "Epoch [160/250], Training Loss: 58.84219498667379, Validation Loss: 801.2675170898438\n",
      "Epoch [161/250], Training Loss: 58.82285989500116, Validation Loss: 801.5884399414062\n",
      "Epoch [162/250], Training Loss: 58.803419491630926, Validation Loss: 801.8777465820312\n",
      "Epoch [163/250], Training Loss: 58.78468187377955, Validation Loss: 802.160400390625\n",
      "Epoch [164/250], Training Loss: 58.76686476629105, Validation Loss: 802.400634765625\n",
      "Epoch [165/250], Training Loss: 58.747994681170354, Validation Loss: 802.6268920898438\n",
      "Epoch [166/250], Training Loss: 58.72927480236093, Validation Loss: 802.8375244140625\n",
      "Epoch [167/250], Training Loss: 58.712523513784426, Validation Loss: 803.03076171875\n",
      "Epoch [168/250], Training Loss: 58.69553856210926, Validation Loss: 803.20068359375\n",
      "Epoch [169/250], Training Loss: 58.67865727086284, Validation Loss: 803.3541259765625\n",
      "Epoch [170/250], Training Loss: 58.66221860864625, Validation Loss: 803.4976196289062\n",
      "Epoch [171/250], Training Loss: 58.64731366339913, Validation Loss: 803.6182861328125\n",
      "Epoch [172/250], Training Loss: 58.63215278127543, Validation Loss: 803.72314453125\n",
      "Epoch [173/250], Training Loss: 58.61724496243365, Validation Loss: 803.8067626953125\n",
      "Epoch [174/250], Training Loss: 58.60238110923601, Validation Loss: 803.8779296875\n",
      "Epoch [175/250], Training Loss: 58.58766665638722, Validation Loss: 803.9369506835938\n",
      "Epoch [176/250], Training Loss: 58.57381083786072, Validation Loss: 803.9818725585938\n",
      "Epoch [177/250], Training Loss: 58.5600951640697, Validation Loss: 804.0169067382812\n",
      "Epoch [178/250], Training Loss: 58.54612859212586, Validation Loss: 804.0466918945312\n",
      "Epoch [179/250], Training Loss: 58.53403467053539, Validation Loss: 804.0579833984375\n",
      "Epoch [180/250], Training Loss: 58.52004184182287, Validation Loss: 804.05126953125\n",
      "Epoch [181/250], Training Loss: 58.50594494471877, Validation Loss: 804.0360717773438\n",
      "Epoch [182/250], Training Loss: 58.49170937041874, Validation Loss: 804.0138549804688\n",
      "Epoch [183/250], Training Loss: 58.4773178858963, Validation Loss: 803.9891357421875\n",
      "Epoch [184/250], Training Loss: 58.464187969391254, Validation Loss: 803.9625854492188\n",
      "Epoch [185/250], Training Loss: 58.45130438299667, Validation Loss: 803.9259643554688\n",
      "Epoch [186/250], Training Loss: 58.43813105071924, Validation Loss: 803.8806762695312\n",
      "Epoch [187/250], Training Loss: 58.42527316385983, Validation Loss: 803.8331298828125\n",
      "Epoch [188/250], Training Loss: 58.41192984570965, Validation Loss: 803.7740478515625\n",
      "Epoch [189/250], Training Loss: 58.399833582580676, Validation Loss: 803.7288208007812\n",
      "Epoch [190/250], Training Loss: 58.38872086890069, Validation Loss: 803.679931640625\n",
      "Epoch [191/250], Training Loss: 58.37831630181271, Validation Loss: 803.623291015625\n",
      "Epoch [192/250], Training Loss: 58.367803989182235, Validation Loss: 803.5595703125\n",
      "Epoch [193/250], Training Loss: 58.35692951606567, Validation Loss: 803.4902954101562\n",
      "Epoch [194/250], Training Loss: 58.34676340701418, Validation Loss: 803.4236450195312\n",
      "Epoch [195/250], Training Loss: 58.33685810075193, Validation Loss: 803.3651733398438\n",
      "Epoch [196/250], Training Loss: 58.32731979872433, Validation Loss: 803.3006591796875\n",
      "Epoch [197/250], Training Loss: 58.31751189735358, Validation Loss: 803.2171630859375\n",
      "Epoch [198/250], Training Loss: 58.30771294779082, Validation Loss: 803.1431274414062\n",
      "Epoch [199/250], Training Loss: 58.29776109416389, Validation Loss: 803.0697021484375\n",
      "Epoch [200/250], Training Loss: 58.28840056362313, Validation Loss: 802.9974365234375\n",
      "Epoch [201/250], Training Loss: 58.27994244227547, Validation Loss: 802.9271240234375\n",
      "Epoch [202/250], Training Loss: 58.27147538196364, Validation Loss: 802.8530883789062\n",
      "Epoch [203/250], Training Loss: 58.26249796419289, Validation Loss: 802.783203125\n",
      "Epoch [204/250], Training Loss: 58.25429077928432, Validation Loss: 802.7108154296875\n",
      "Epoch [205/250], Training Loss: 58.24602947623449, Validation Loss: 802.6403198242188\n",
      "Epoch [206/250], Training Loss: 58.23766088244212, Validation Loss: 802.5723876953125\n",
      "Epoch [207/250], Training Loss: 58.229231773942836, Validation Loss: 802.5023803710938\n",
      "Epoch [208/250], Training Loss: 58.22065275064444, Validation Loss: 802.4421997070312\n",
      "Epoch [209/250], Training Loss: 58.21278534527777, Validation Loss: 802.3733520507812\n",
      "Epoch [210/250], Training Loss: 58.2044635959533, Validation Loss: 802.30712890625\n",
      "Epoch [211/250], Training Loss: 58.195991538655775, Validation Loss: 802.2430419921875\n",
      "Epoch [212/250], Training Loss: 58.1875839161341, Validation Loss: 802.1776733398438\n",
      "Epoch [213/250], Training Loss: 58.179069991899496, Validation Loss: 802.1099853515625\n",
      "Epoch [214/250], Training Loss: 58.170997652922914, Validation Loss: 802.0537719726562\n",
      "Epoch [215/250], Training Loss: 58.16292715106365, Validation Loss: 801.99853515625\n",
      "Epoch [216/250], Training Loss: 58.154191912201284, Validation Loss: 801.9337768554688\n",
      "Epoch [217/250], Training Loss: 58.145620709088604, Validation Loss: 801.8734130859375\n",
      "Epoch [218/250], Training Loss: 58.13714300811529, Validation Loss: 801.8264770507812\n",
      "Epoch [219/250], Training Loss: 58.1298746351265, Validation Loss: 801.7779541015625\n",
      "Epoch [220/250], Training Loss: 58.12287991868704, Validation Loss: 801.7296142578125\n",
      "Epoch [221/250], Training Loss: 58.11604698132473, Validation Loss: 801.6830444335938\n",
      "Epoch [222/250], Training Loss: 58.10884657717151, Validation Loss: 801.6278686523438\n",
      "Epoch [223/250], Training Loss: 58.101290349084486, Validation Loss: 801.5891723632812\n",
      "Epoch [224/250], Training Loss: 58.09534943558505, Validation Loss: 801.5513305664062\n",
      "Epoch [225/250], Training Loss: 58.08913757148319, Validation Loss: 801.5121459960938\n",
      "Epoch [226/250], Training Loss: 58.0831928851087, Validation Loss: 801.4774169921875\n",
      "Epoch [227/250], Training Loss: 58.077671802553866, Validation Loss: 801.4457397460938\n",
      "Epoch [228/250], Training Loss: 58.07234790527249, Validation Loss: 801.4100341796875\n",
      "Epoch [229/250], Training Loss: 58.06639074387055, Validation Loss: 801.3771362304688\n",
      "Epoch [230/250], Training Loss: 58.060838223565476, Validation Loss: 801.3421630859375\n",
      "Epoch [231/250], Training Loss: 58.05481735357934, Validation Loss: 801.31201171875\n",
      "Epoch [232/250], Training Loss: 58.04856528826072, Validation Loss: 801.2796020507812\n",
      "Epoch [233/250], Training Loss: 58.042620610676586, Validation Loss: 801.2501831054688\n",
      "Epoch [234/250], Training Loss: 58.037161961607815, Validation Loss: 801.22509765625\n",
      "Epoch [235/250], Training Loss: 58.031238744909956, Validation Loss: 801.1963500976562\n",
      "Epoch [236/250], Training Loss: 58.02498965324654, Validation Loss: 801.1743774414062\n",
      "Epoch [237/250], Training Loss: 58.01927816390512, Validation Loss: 801.1492309570312\n",
      "Epoch [238/250], Training Loss: 58.01313249258395, Validation Loss: 801.1296997070312\n",
      "Epoch [239/250], Training Loss: 58.00807732841364, Validation Loss: 801.1170654296875\n",
      "Epoch [240/250], Training Loss: 58.00292096486818, Validation Loss: 801.0972290039062\n",
      "Epoch [241/250], Training Loss: 57.997348991478155, Validation Loss: 801.08935546875\n",
      "Epoch [242/250], Training Loss: 57.99276174380861, Validation Loss: 801.0813598632812\n",
      "Epoch [243/250], Training Loss: 57.9885150817653, Validation Loss: 801.077880859375\n",
      "Epoch [244/250], Training Loss: 57.98394709358415, Validation Loss: 801.0682983398438\n",
      "Epoch [245/250], Training Loss: 57.97981340697627, Validation Loss: 801.06201171875\n",
      "Epoch [246/250], Training Loss: 57.97489136635948, Validation Loss: 801.0553588867188\n",
      "Epoch [247/250], Training Loss: 57.97096725748747, Validation Loss: 801.0578002929688\n",
      "Epoch [248/250], Training Loss: 57.96722970877609, Validation Loss: 801.0552368164062\n",
      "Epoch [249/250], Training Loss: 57.963863156347195, Validation Loss: 801.0482177734375\n",
      "Epoch [250/250], Training Loss: 57.959042100917614, Validation Loss: 801.0460205078125\n",
      "Test Loss: 794.2540893554688\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.005, 2)\n",
      "Epoch [1/250], Training Loss: 19161.284152285927, Validation Loss: 19833.79296875\n",
      "Epoch [2/250], Training Loss: 13927.499415723356, Validation Loss: 15713.8603515625\n",
      "Epoch [3/250], Training Loss: 10827.484819590136, Validation Loss: 15504.0107421875\n",
      "Epoch [4/250], Training Loss: 8822.304508764964, Validation Loss: 12812.9853515625\n",
      "Epoch [5/250], Training Loss: 8225.25289267358, Validation Loss: 10968.734375\n",
      "Epoch [6/250], Training Loss: 6908.988819842003, Validation Loss: 7686.5771484375\n",
      "Epoch [7/250], Training Loss: 5756.382971501028, Validation Loss: 6628.3076171875\n",
      "Epoch [8/250], Training Loss: 4807.690422629454, Validation Loss: 6845.349609375\n",
      "Epoch [9/250], Training Loss: 4041.1729190312203, Validation Loss: 6386.78857421875\n",
      "Epoch [10/250], Training Loss: 3386.278218220084, Validation Loss: 5710.22021484375\n",
      "Epoch [11/250], Training Loss: 2821.482762821465, Validation Loss: 5280.91162109375\n",
      "Epoch [12/250], Training Loss: 2365.0817214934946, Validation Loss: 5307.07373046875\n",
      "Epoch [13/250], Training Loss: 2012.7964710878234, Validation Loss: 5282.546875\n",
      "Epoch [14/250], Training Loss: 1726.8943469858675, Validation Loss: 4817.22509765625\n",
      "Epoch [15/250], Training Loss: 1487.188474819399, Validation Loss: 4537.6884765625\n",
      "Epoch [16/250], Training Loss: 1292.8264233097345, Validation Loss: 4095.873046875\n",
      "Epoch [17/250], Training Loss: 1131.4692445580731, Validation Loss: 3697.76025390625\n",
      "Epoch [18/250], Training Loss: 992.5803459142231, Validation Loss: 3257.238037109375\n",
      "Epoch [19/250], Training Loss: 871.3365234089671, Validation Loss: 2745.5234375\n",
      "Epoch [20/250], Training Loss: 766.4955610810347, Validation Loss: 2202.397705078125\n",
      "Epoch [21/250], Training Loss: 676.1802957498331, Validation Loss: 1899.7471923828125\n",
      "Epoch [22/250], Training Loss: 598.2877164799468, Validation Loss: 1680.1224365234375\n",
      "Epoch [23/250], Training Loss: 528.8995943266806, Validation Loss: 1504.831298828125\n",
      "Epoch [24/250], Training Loss: 467.50808796013064, Validation Loss: 1375.1732177734375\n",
      "Epoch [25/250], Training Loss: 415.1123796629486, Validation Loss: 1294.7171630859375\n",
      "Epoch [26/250], Training Loss: 369.56730789568024, Validation Loss: 1236.9761962890625\n",
      "Epoch [27/250], Training Loss: 329.9531731872586, Validation Loss: 1184.4290771484375\n",
      "Epoch [28/250], Training Loss: 294.59459350917905, Validation Loss: 1150.599609375\n",
      "Epoch [29/250], Training Loss: 263.75012980563304, Validation Loss: 1141.3365478515625\n",
      "Epoch [30/250], Training Loss: 237.26582837987075, Validation Loss: 1148.7049560546875\n",
      "Epoch [31/250], Training Loss: 213.5732327062036, Validation Loss: 1177.17041015625\n",
      "Epoch [32/250], Training Loss: 193.1520710002576, Validation Loss: 1246.76318359375\n",
      "Epoch [33/250], Training Loss: 176.19635676462607, Validation Loss: 1354.3857421875\n",
      "Epoch [34/250], Training Loss: 160.814881777612, Validation Loss: 1493.1473388671875\n",
      "Epoch [35/250], Training Loss: 147.2014220991823, Validation Loss: 1670.0335693359375\n",
      "Epoch [36/250], Training Loss: 135.71393070954386, Validation Loss: 1853.7047119140625\n",
      "Epoch [37/250], Training Loss: 125.73935867975456, Validation Loss: 2015.11328125\n",
      "Epoch [38/250], Training Loss: 116.89226145182826, Validation Loss: 2156.015380859375\n",
      "Epoch [39/250], Training Loss: 108.82504264493751, Validation Loss: 2299.7470703125\n",
      "Epoch [40/250], Training Loss: 101.39339552937878, Validation Loss: 2455.887939453125\n",
      "Epoch [41/250], Training Loss: 94.71354823551287, Validation Loss: 2617.000244140625\n",
      "Epoch [42/250], Training Loss: 88.96242962626283, Validation Loss: 2752.6953125\n",
      "Epoch [43/250], Training Loss: 83.83641491333847, Validation Loss: 2855.130615234375\n",
      "Epoch [44/250], Training Loss: 79.0055102607993, Validation Loss: 2933.6669921875\n",
      "Epoch [45/250], Training Loss: 74.42159855194313, Validation Loss: 3001.94677734375\n",
      "Epoch [46/250], Training Loss: 70.26314443467061, Validation Loss: 3062.80029296875\n",
      "Epoch [47/250], Training Loss: 66.69086345581168, Validation Loss: 3114.477294921875\n",
      "Epoch [48/250], Training Loss: 63.64931500597983, Validation Loss: 3158.558837890625\n",
      "Epoch [49/250], Training Loss: 61.0007364253606, Validation Loss: 3197.277587890625\n",
      "Epoch [50/250], Training Loss: 58.66751277247683, Validation Loss: 3231.538818359375\n",
      "Epoch [51/250], Training Loss: 56.657732393111715, Validation Loss: 3262.07470703125\n",
      "Epoch [52/250], Training Loss: 54.95253540567064, Validation Loss: 3288.18896484375\n",
      "Epoch [53/250], Training Loss: 53.44130399261598, Validation Loss: 3310.239013671875\n",
      "Epoch [54/250], Training Loss: 51.97186977095489, Validation Loss: 3328.133544921875\n",
      "Epoch [55/250], Training Loss: 50.44844979033525, Validation Loss: 3342.494873046875\n",
      "Epoch [56/250], Training Loss: 48.87205329225156, Validation Loss: 3352.903076171875\n",
      "Epoch [57/250], Training Loss: 47.30548149588446, Validation Loss: 3360.60400390625\n",
      "Epoch [58/250], Training Loss: 45.819729889781904, Validation Loss: 3371.884765625\n",
      "Epoch [59/250], Training Loss: 44.40857907535896, Validation Loss: 3383.95263671875\n",
      "Epoch [60/250], Training Loss: 43.02859283262881, Validation Loss: 3403.792724609375\n",
      "Epoch [61/250], Training Loss: 41.646668158946184, Validation Loss: 3426.065185546875\n",
      "Epoch [62/250], Training Loss: 40.324746430150746, Validation Loss: 3448.536865234375\n",
      "Epoch [63/250], Training Loss: 39.20720538693282, Validation Loss: 3475.84814453125\n",
      "Epoch [64/250], Training Loss: 38.29467833161597, Validation Loss: 3503.985107421875\n",
      "Epoch [65/250], Training Loss: 37.53713341765973, Validation Loss: 3533.442138671875\n",
      "Epoch [66/250], Training Loss: 36.916376999516444, Validation Loss: 3563.9345703125\n",
      "Epoch [67/250], Training Loss: 36.42613961293409, Validation Loss: 3591.47314453125\n",
      "Epoch [68/250], Training Loss: 36.071680285481456, Validation Loss: 3612.068359375\n",
      "Epoch [69/250], Training Loss: 35.816485135931345, Validation Loss: 3624.193603515625\n",
      "Epoch [70/250], Training Loss: 35.62071967703102, Validation Loss: 3630.048828125\n",
      "Epoch [71/250], Training Loss: 35.46164593479472, Validation Loss: 3633.898193359375\n",
      "Epoch [72/250], Training Loss: 35.29417836417932, Validation Loss: 3643.1474609375\n",
      "Epoch [73/250], Training Loss: 35.18050389174666, Validation Loss: 3654.0732421875\n",
      "Epoch [74/250], Training Loss: 35.14132746180938, Validation Loss: 3664.078369140625\n",
      "Epoch [75/250], Training Loss: 35.16526791376457, Validation Loss: 3672.348388671875\n",
      "Epoch [76/250], Training Loss: 35.22030519166256, Validation Loss: 3677.88427734375\n",
      "Epoch [77/250], Training Loss: 35.28547282042098, Validation Loss: 3680.90869140625\n",
      "Epoch [78/250], Training Loss: 35.35322732818818, Validation Loss: 3681.268798828125\n",
      "Epoch [79/250], Training Loss: 35.412716641940875, Validation Loss: 3679.87548828125\n",
      "Epoch [80/250], Training Loss: 35.445011602004904, Validation Loss: 3676.981201171875\n",
      "Epoch [81/250], Training Loss: 35.44510235085948, Validation Loss: 3673.035400390625\n",
      "Epoch [82/250], Training Loss: 35.41865693600968, Validation Loss: 3668.29150390625\n",
      "Epoch [83/250], Training Loss: 35.376230183592725, Validation Loss: 3662.941650390625\n",
      "Epoch [84/250], Training Loss: 35.32047451033479, Validation Loss: 3657.59033203125\n",
      "Epoch [85/250], Training Loss: 35.25463598527262, Validation Loss: 3651.85400390625\n",
      "Epoch [86/250], Training Loss: 35.18023350462895, Validation Loss: 3645.22314453125\n",
      "Epoch [87/250], Training Loss: 35.098607922826055, Validation Loss: 3638.178955078125\n",
      "Epoch [88/250], Training Loss: 35.00885427588362, Validation Loss: 3631.55908203125\n",
      "Epoch [89/250], Training Loss: 34.9118228298825, Validation Loss: 3623.900146484375\n",
      "Epoch [90/250], Training Loss: 34.80853200006394, Validation Loss: 3616.314697265625\n",
      "Epoch [91/250], Training Loss: 34.69879100324278, Validation Loss: 3609.585693359375\n",
      "Epoch [92/250], Training Loss: 34.58297264501487, Validation Loss: 3602.39453125\n",
      "Epoch [93/250], Training Loss: 34.463013656850194, Validation Loss: 3595.337158203125\n",
      "Epoch [94/250], Training Loss: 34.33826224648451, Validation Loss: 3588.313720703125\n",
      "Epoch [95/250], Training Loss: 34.20900655363623, Validation Loss: 3581.77490234375\n",
      "Epoch [96/250], Training Loss: 34.07492939338466, Validation Loss: 3575.464599609375\n",
      "Epoch [97/250], Training Loss: 33.93549943338036, Validation Loss: 3569.799560546875\n",
      "Epoch [98/250], Training Loss: 33.79144189118726, Validation Loss: 3564.229736328125\n",
      "Epoch [99/250], Training Loss: 33.643183108997086, Validation Loss: 3558.99267578125\n",
      "Epoch [100/250], Training Loss: 33.49196912232119, Validation Loss: 3553.664306640625\n",
      "Epoch [101/250], Training Loss: 33.34279361638978, Validation Loss: 3547.601806640625\n",
      "Epoch [102/250], Training Loss: 33.197328918976034, Validation Loss: 3541.34228515625\n",
      "Epoch [103/250], Training Loss: 33.06182878975306, Validation Loss: 3535.378173828125\n",
      "Epoch [104/250], Training Loss: 32.93657675977719, Validation Loss: 3530.841064453125\n",
      "Epoch [105/250], Training Loss: 32.80902331411197, Validation Loss: 3527.164306640625\n",
      "Epoch [106/250], Training Loss: 32.66004764801504, Validation Loss: 3524.664306640625\n",
      "Epoch [107/250], Training Loss: 32.47620550108696, Validation Loss: 3523.02490234375\n",
      "Epoch [108/250], Training Loss: 32.25674118032502, Validation Loss: 3521.795654296875\n",
      "Epoch [109/250], Training Loss: 32.009704893583354, Validation Loss: 3521.30810546875\n",
      "Epoch [110/250], Training Loss: 31.74863741252678, Validation Loss: 3521.366455078125\n",
      "Epoch [111/250], Training Loss: 31.485868351755716, Validation Loss: 3521.494140625\n",
      "Epoch [112/250], Training Loss: 31.230247020713264, Validation Loss: 3522.169189453125\n",
      "Epoch [113/250], Training Loss: 30.9877900673853, Validation Loss: 3522.928955078125\n",
      "Epoch [114/250], Training Loss: 30.758092515558207, Validation Loss: 3524.010009765625\n",
      "Epoch [115/250], Training Loss: 30.536559605403735, Validation Loss: 3525.3037109375\n",
      "Epoch [116/250], Training Loss: 30.316967827966288, Validation Loss: 3526.707763671875\n",
      "Epoch [117/250], Training Loss: 30.099659396150763, Validation Loss: 3528.423583984375\n",
      "Epoch [118/250], Training Loss: 29.887989522443227, Validation Loss: 3530.16064453125\n",
      "Epoch [119/250], Training Loss: 29.67450207038951, Validation Loss: 3531.52490234375\n",
      "Epoch [120/250], Training Loss: 29.44856839865971, Validation Loss: 3532.618408203125\n",
      "Epoch [121/250], Training Loss: 29.206094733661907, Validation Loss: 3533.14599609375\n",
      "Epoch [122/250], Training Loss: 28.947597170355976, Validation Loss: 3532.4150390625\n",
      "Epoch [123/250], Training Loss: 28.676678405653167, Validation Loss: 3531.35107421875\n",
      "Epoch [124/250], Training Loss: 28.39810423352603, Validation Loss: 3530.01025390625\n",
      "Epoch [125/250], Training Loss: 28.115990612404012, Validation Loss: 3528.74609375\n",
      "Epoch [126/250], Training Loss: 27.835166693374116, Validation Loss: 3527.751953125\n",
      "Epoch [127/250], Training Loss: 27.559073111844352, Validation Loss: 3527.62939453125\n",
      "Epoch [128/250], Training Loss: 27.290517366840493, Validation Loss: 3528.426513671875\n",
      "Epoch [129/250], Training Loss: 27.02844299890546, Validation Loss: 3529.387451171875\n",
      "Epoch [130/250], Training Loss: 26.769928022171218, Validation Loss: 3529.69970703125\n",
      "Epoch [131/250], Training Loss: 26.51033898630086, Validation Loss: 3527.58740234375\n",
      "Epoch [132/250], Training Loss: 26.250356846125893, Validation Loss: 3522.106689453125\n",
      "Epoch [133/250], Training Loss: 25.998459791465514, Validation Loss: 3514.221923828125\n",
      "Epoch [134/250], Training Loss: 25.766538384410797, Validation Loss: 3504.965087890625\n",
      "Epoch [135/250], Training Loss: 25.559373603094976, Validation Loss: 3496.557861328125\n",
      "Epoch [136/250], Training Loss: 25.37625239959803, Validation Loss: 3489.51025390625\n",
      "Epoch [137/250], Training Loss: 25.21272032430309, Validation Loss: 3484.237060546875\n",
      "Epoch [138/250], Training Loss: 25.06532685837473, Validation Loss: 3480.49853515625\n",
      "Epoch [139/250], Training Loss: 24.930834409321495, Validation Loss: 3477.81396484375\n",
      "Epoch [140/250], Training Loss: 24.805760532685596, Validation Loss: 3475.94482421875\n",
      "Epoch [141/250], Training Loss: 24.6888069566673, Validation Loss: 3474.593994140625\n",
      "Epoch [142/250], Training Loss: 24.57823657063328, Validation Loss: 3473.54296875\n",
      "Epoch [143/250], Training Loss: 24.47280740030523, Validation Loss: 3472.496826171875\n",
      "Epoch [144/250], Training Loss: 24.371901433911273, Validation Loss: 3471.621337890625\n",
      "Epoch [145/250], Training Loss: 24.27441013379823, Validation Loss: 3470.63720703125\n",
      "Epoch [146/250], Training Loss: 24.179844022686865, Validation Loss: 3469.54150390625\n",
      "Epoch [147/250], Training Loss: 24.08767572660553, Validation Loss: 3468.47216796875\n",
      "Epoch [148/250], Training Loss: 23.9958843558425, Validation Loss: 3467.530517578125\n",
      "Epoch [149/250], Training Loss: 23.905472106528716, Validation Loss: 3467.2294921875\n",
      "Epoch [150/250], Training Loss: 23.81754542128926, Validation Loss: 3467.7109375\n",
      "Epoch [151/250], Training Loss: 23.73519162571405, Validation Loss: 3469.356201171875\n",
      "Epoch [152/250], Training Loss: 23.659424161101978, Validation Loss: 3472.40771484375\n",
      "Epoch [153/250], Training Loss: 23.592187321487526, Validation Loss: 3475.94140625\n",
      "Epoch [154/250], Training Loss: 23.533357475584424, Validation Loss: 3479.4443359375\n",
      "Epoch [155/250], Training Loss: 23.48192924926412, Validation Loss: 3482.592529296875\n",
      "Epoch [156/250], Training Loss: 23.435906841614635, Validation Loss: 3485.78759765625\n",
      "Epoch [157/250], Training Loss: 23.393641522910364, Validation Loss: 3489.728515625\n",
      "Epoch [158/250], Training Loss: 23.353466425757468, Validation Loss: 3494.1328125\n",
      "Epoch [159/250], Training Loss: 23.313341650964063, Validation Loss: 3499.582275390625\n",
      "Epoch [160/250], Training Loss: 23.274524816259373, Validation Loss: 3505.459228515625\n",
      "Epoch [161/250], Training Loss: 23.23807312488787, Validation Loss: 3510.924072265625\n",
      "Epoch [162/250], Training Loss: 23.204266253337146, Validation Loss: 3516.04931640625\n",
      "Epoch [163/250], Training Loss: 23.17381729998257, Validation Loss: 3520.1181640625\n",
      "Epoch [164/250], Training Loss: 23.145106657025256, Validation Loss: 3523.415283203125\n",
      "Epoch [165/250], Training Loss: 23.118827950836508, Validation Loss: 3526.12353515625\n",
      "Epoch [166/250], Training Loss: 23.09453561268525, Validation Loss: 3528.225341796875\n",
      "Epoch [167/250], Training Loss: 23.07106241456609, Validation Loss: 3529.71044921875\n",
      "Epoch [168/250], Training Loss: 23.048519892759554, Validation Loss: 3531.111572265625\n",
      "Epoch [169/250], Training Loss: 23.027269186323732, Validation Loss: 3532.152099609375\n",
      "Epoch [170/250], Training Loss: 23.007174802785332, Validation Loss: 3533.01220703125\n",
      "Epoch [171/250], Training Loss: 22.987778745830145, Validation Loss: 3533.87353515625\n",
      "Epoch [172/250], Training Loss: 22.969533481948098, Validation Loss: 3534.3837890625\n",
      "Epoch [173/250], Training Loss: 22.951871287500826, Validation Loss: 3534.89794921875\n",
      "Epoch [174/250], Training Loss: 22.93475949150181, Validation Loss: 3535.328857421875\n",
      "Epoch [175/250], Training Loss: 22.918182262515145, Validation Loss: 3535.82177734375\n",
      "Epoch [176/250], Training Loss: 22.9030435937944, Validation Loss: 3536.590087890625\n",
      "Epoch [177/250], Training Loss: 22.888849233882883, Validation Loss: 3536.97216796875\n",
      "Epoch [178/250], Training Loss: 22.87578995862483, Validation Loss: 3537.12060546875\n",
      "Epoch [179/250], Training Loss: 22.863295096998, Validation Loss: 3537.4921875\n",
      "Epoch [180/250], Training Loss: 22.852011816070966, Validation Loss: 3537.8095703125\n",
      "Epoch [181/250], Training Loss: 22.840921066207578, Validation Loss: 3538.087158203125\n",
      "Epoch [182/250], Training Loss: 22.830813311253607, Validation Loss: 3538.02001953125\n",
      "Epoch [183/250], Training Loss: 22.821290840274813, Validation Loss: 3537.81298828125\n",
      "Epoch [184/250], Training Loss: 22.812561573430475, Validation Loss: 3537.60693359375\n",
      "Epoch [185/250], Training Loss: 22.80483256690429, Validation Loss: 3537.511962890625\n",
      "Epoch [186/250], Training Loss: 22.797328235639306, Validation Loss: 3537.0673828125\n",
      "Epoch [187/250], Training Loss: 22.790744287437484, Validation Loss: 3536.822021484375\n",
      "Epoch [188/250], Training Loss: 22.784270868064585, Validation Loss: 3536.398193359375\n",
      "Epoch [189/250], Training Loss: 22.77801105602919, Validation Loss: 3536.147216796875\n",
      "Epoch [190/250], Training Loss: 22.772023160091386, Validation Loss: 3535.846435546875\n",
      "Epoch [191/250], Training Loss: 22.76639742777827, Validation Loss: 3535.62841796875\n",
      "Epoch [192/250], Training Loss: 22.761193611526704, Validation Loss: 3535.321533203125\n",
      "Epoch [193/250], Training Loss: 22.756004549579153, Validation Loss: 3535.1123046875\n",
      "Epoch [194/250], Training Loss: 22.75126707825789, Validation Loss: 3534.981201171875\n",
      "Epoch [195/250], Training Loss: 22.746344425530175, Validation Loss: 3534.776611328125\n",
      "Epoch [196/250], Training Loss: 22.741444012450735, Validation Loss: 3534.45751953125\n",
      "Epoch [197/250], Training Loss: 22.736709268622025, Validation Loss: 3534.29150390625\n",
      "Epoch [198/250], Training Loss: 22.73200855185691, Validation Loss: 3534.0810546875\n",
      "Epoch [199/250], Training Loss: 22.727594227500976, Validation Loss: 3533.832275390625\n",
      "Epoch [200/250], Training Loss: 22.723140325850224, Validation Loss: 3533.6328125\n",
      "Epoch [201/250], Training Loss: 22.718615538515994, Validation Loss: 3533.398681640625\n",
      "Epoch [202/250], Training Loss: 22.71424272917376, Validation Loss: 3533.20751953125\n",
      "Epoch [203/250], Training Loss: 22.710395468774777, Validation Loss: 3533.171630859375\n",
      "Epoch [204/250], Training Loss: 22.706285353737563, Validation Loss: 3533.12255859375\n",
      "Epoch [205/250], Training Loss: 22.702136420870247, Validation Loss: 3533.076416015625\n",
      "Epoch [206/250], Training Loss: 22.697461260945353, Validation Loss: 3533.189208984375\n",
      "Epoch [207/250], Training Loss: 22.692831354149966, Validation Loss: 3533.3564453125\n",
      "Epoch [208/250], Training Loss: 22.6882230485452, Validation Loss: 3533.6328125\n",
      "Epoch [209/250], Training Loss: 22.68362844519257, Validation Loss: 3533.844970703125\n",
      "Epoch [210/250], Training Loss: 22.678801533490713, Validation Loss: 3534.272216796875\n",
      "Epoch [211/250], Training Loss: 22.673905824744317, Validation Loss: 3534.686767578125\n",
      "Epoch [212/250], Training Loss: 22.668599662596343, Validation Loss: 3535.11083984375\n",
      "Epoch [213/250], Training Loss: 22.663434692361808, Validation Loss: 3535.72265625\n",
      "Epoch [214/250], Training Loss: 22.657832820589597, Validation Loss: 3536.1572265625\n",
      "Epoch [215/250], Training Loss: 22.6525650476511, Validation Loss: 3536.720947265625\n",
      "Epoch [216/250], Training Loss: 22.64678957681133, Validation Loss: 3537.259033203125\n",
      "Epoch [217/250], Training Loss: 22.64108687346075, Validation Loss: 3538.2373046875\n",
      "Epoch [218/250], Training Loss: 22.635860003640605, Validation Loss: 3539.01220703125\n",
      "Epoch [219/250], Training Loss: 22.63154516419987, Validation Loss: 3539.737060546875\n",
      "Epoch [220/250], Training Loss: 22.627603684867587, Validation Loss: 3540.58642578125\n",
      "Epoch [221/250], Training Loss: 22.623681374181327, Validation Loss: 3541.281494140625\n",
      "Epoch [222/250], Training Loss: 22.620219486429633, Validation Loss: 3542.25048828125\n",
      "Epoch [223/250], Training Loss: 22.617197619275576, Validation Loss: 3543.030517578125\n",
      "Epoch [224/250], Training Loss: 22.61482273179494, Validation Loss: 3543.875732421875\n",
      "Epoch [225/250], Training Loss: 22.612724390355762, Validation Loss: 3544.51220703125\n",
      "Epoch [226/250], Training Loss: 22.611406172181734, Validation Loss: 3545.122314453125\n",
      "Epoch [227/250], Training Loss: 22.609913113750604, Validation Loss: 3545.425048828125\n",
      "Epoch [228/250], Training Loss: 22.608307814618936, Validation Loss: 3545.9052734375\n",
      "Epoch [229/250], Training Loss: 22.606989915719137, Validation Loss: 3546.109619140625\n",
      "Epoch [230/250], Training Loss: 22.605297717329517, Validation Loss: 3546.246337890625\n",
      "Epoch [231/250], Training Loss: 22.603233598913427, Validation Loss: 3546.177490234375\n",
      "Epoch [232/250], Training Loss: 22.601294068803945, Validation Loss: 3546.155517578125\n",
      "Epoch [233/250], Training Loss: 22.59919758843291, Validation Loss: 3546.2841796875\n",
      "Epoch [234/250], Training Loss: 22.597037563943154, Validation Loss: 3546.360595703125\n",
      "Epoch [235/250], Training Loss: 22.594873490044673, Validation Loss: 3546.251708984375\n",
      "Epoch [236/250], Training Loss: 22.592855769824585, Validation Loss: 3546.56884765625\n",
      "Epoch [237/250], Training Loss: 22.590809771554788, Validation Loss: 3546.734375\n",
      "Epoch [238/250], Training Loss: 22.588611434488016, Validation Loss: 3546.876708984375\n",
      "Epoch [239/250], Training Loss: 22.586330603647014, Validation Loss: 3547.031005859375\n",
      "Epoch [240/250], Training Loss: 22.58423817763583, Validation Loss: 3547.283203125\n",
      "Epoch [241/250], Training Loss: 22.58220910402226, Validation Loss: 3547.435302734375\n",
      "Epoch [242/250], Training Loss: 22.57997542674792, Validation Loss: 3547.528076171875\n",
      "Epoch [243/250], Training Loss: 22.57785935511572, Validation Loss: 3547.641357421875\n",
      "Epoch [244/250], Training Loss: 22.57587305488317, Validation Loss: 3547.902587890625\n",
      "Epoch [245/250], Training Loss: 22.573957025902015, Validation Loss: 3548.11181640625\n",
      "Epoch [246/250], Training Loss: 22.571639102824246, Validation Loss: 3548.435791015625\n",
      "Epoch [247/250], Training Loss: 22.56945752928979, Validation Loss: 3548.80908203125\n",
      "Epoch [248/250], Training Loss: 22.567372215050852, Validation Loss: 3549.28759765625\n",
      "Epoch [249/250], Training Loss: 22.565415079310096, Validation Loss: 3549.715087890625\n",
      "Epoch [250/250], Training Loss: 22.563700199272365, Validation Loss: 3550.091064453125\n",
      "Test Loss: 1432.1573486328125\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.005, 5)\n",
      "Epoch [1/250], Training Loss: 19151.796003683212, Validation Loss: 19773.541015625\n",
      "Epoch [2/250], Training Loss: 13997.433116395396, Validation Loss: 16017.2568359375\n",
      "Epoch [3/250], Training Loss: 10896.877538835917, Validation Loss: 15821.908203125\n",
      "Epoch [4/250], Training Loss: 8849.073553040153, Validation Loss: 14845.3291015625\n",
      "Epoch [5/250], Training Loss: 7317.52142728402, Validation Loss: 15856.533203125\n",
      "Epoch [6/250], Training Loss: 6165.5291380185845, Validation Loss: 22341.470703125\n",
      "Epoch [7/250], Training Loss: 5362.00772901355, Validation Loss: 18763.56640625\n",
      "Epoch [8/250], Training Loss: 4273.668317108848, Validation Loss: 18560.76953125\n",
      "Epoch [9/250], Training Loss: 3696.003012307988, Validation Loss: 20193.46484375\n",
      "Epoch [10/250], Training Loss: 2956.1554839448886, Validation Loss: 21853.1875\n",
      "Epoch [11/250], Training Loss: 2484.6309007039445, Validation Loss: 20783.41015625\n",
      "Epoch [12/250], Training Loss: 2243.1152656536974, Validation Loss: 22929.9296875\n",
      "Epoch [13/250], Training Loss: 1812.419837264207, Validation Loss: 20868.3984375\n",
      "Epoch [14/250], Training Loss: 6431.341180660795, Validation Loss: 18197.646484375\n",
      "Epoch [15/250], Training Loss: 1922.8614953840158, Validation Loss: 26201.203125\n",
      "Epoch [16/250], Training Loss: 6040.675513924495, Validation Loss: 17062.97265625\n",
      "Epoch [17/250], Training Loss: 1188.1544826351526, Validation Loss: 25331.671875\n",
      "Epoch [18/250], Training Loss: 1118.9391448776098, Validation Loss: 15142.8291015625\n",
      "Epoch [19/250], Training Loss: 1014.8908611069301, Validation Loss: 15766.3330078125\n",
      "Epoch [20/250], Training Loss: 900.4742081403426, Validation Loss: 15724.4482421875\n",
      "Epoch [21/250], Training Loss: 887.038328134217, Validation Loss: 16529.8828125\n",
      "Epoch [22/250], Training Loss: 882.1305292621975, Validation Loss: 15212.7421875\n",
      "Epoch [23/250], Training Loss: 993.8330862574609, Validation Loss: 13755.1357421875\n",
      "Epoch [24/250], Training Loss: 1450.1243414917215, Validation Loss: 13413.099609375\n",
      "Epoch [25/250], Training Loss: 1048.7949811989736, Validation Loss: 13676.6328125\n",
      "Epoch [26/250], Training Loss: 725.2875558642144, Validation Loss: 13965.3681640625\n",
      "Epoch [27/250], Training Loss: 901.7812428928631, Validation Loss: 14861.3486328125\n",
      "Epoch [28/250], Training Loss: 690.5248748376299, Validation Loss: 15737.3564453125\n",
      "Epoch [29/250], Training Loss: 602.6194274316832, Validation Loss: 16291.7451171875\n",
      "Epoch [30/250], Training Loss: 636.034150398698, Validation Loss: 15441.470703125\n",
      "Epoch [31/250], Training Loss: 463.5296998384982, Validation Loss: 14860.4609375\n",
      "Epoch [32/250], Training Loss: 2148.906765278395, Validation Loss: 17133.2734375\n",
      "Epoch [33/250], Training Loss: 561.000000183321, Validation Loss: 13210.2099609375\n",
      "Epoch [34/250], Training Loss: 651.729491430279, Validation Loss: 14690.6650390625\n",
      "Epoch [35/250], Training Loss: 1787.5795529594373, Validation Loss: 11554.681640625\n",
      "Epoch [36/250], Training Loss: 1424.8175904462512, Validation Loss: 28798.83984375\n",
      "Epoch [37/250], Training Loss: 6326.33897526414, Validation Loss: 29106.26953125\n",
      "Epoch [38/250], Training Loss: 1461.9635020396365, Validation Loss: 39698.703125\n",
      "Epoch [39/250], Training Loss: 11470.187744385537, Validation Loss: 15478.7216796875\n",
      "Epoch [40/250], Training Loss: 1387.807179871067, Validation Loss: 21641.27734375\n",
      "Epoch [41/250], Training Loss: 1249.6032887292586, Validation Loss: 38059.65234375\n",
      "Epoch [42/250], Training Loss: 3874.272696204079, Validation Loss: 12987.7470703125\n",
      "Epoch [43/250], Training Loss: 1122.7242050729155, Validation Loss: 14468.2880859375\n",
      "Epoch [44/250], Training Loss: 883.2782750562442, Validation Loss: 24559.990234375\n",
      "Epoch [45/250], Training Loss: 1072.3018687446895, Validation Loss: 14728.6396484375\n",
      "Epoch [46/250], Training Loss: 521.2098294895045, Validation Loss: 15263.9716796875\n",
      "Epoch [47/250], Training Loss: 1005.3195121000603, Validation Loss: 16563.455078125\n",
      "Epoch [48/250], Training Loss: 640.5126299363978, Validation Loss: 33846.875\n",
      "Epoch [49/250], Training Loss: 1885.3101229512806, Validation Loss: 13791.5009765625\n",
      "Epoch [50/250], Training Loss: 2001.0144753604059, Validation Loss: 9471.3876953125\n",
      "Epoch [51/250], Training Loss: 1009.1042713918036, Validation Loss: 9403.0263671875\n",
      "Epoch [52/250], Training Loss: 894.5239284055355, Validation Loss: 9276.900390625\n",
      "Epoch [53/250], Training Loss: 1174.5372893967133, Validation Loss: 9792.841796875\n",
      "Epoch [54/250], Training Loss: 1266.5915940968118, Validation Loss: 11754.615234375\n",
      "Epoch [55/250], Training Loss: 2356.4670592244383, Validation Loss: 14293.888671875\n",
      "Epoch [56/250], Training Loss: 1767.4874053803114, Validation Loss: 10820.130859375\n",
      "Epoch [57/250], Training Loss: 2972.981467388586, Validation Loss: 10472.76953125\n",
      "Epoch [58/250], Training Loss: 4742.409225154475, Validation Loss: 19968.310546875\n",
      "Epoch [59/250], Training Loss: 4193.451090919623, Validation Loss: 39203.35546875\n",
      "Epoch [60/250], Training Loss: 1592.6614557401604, Validation Loss: 19096.201171875\n",
      "Epoch [61/250], Training Loss: 588.6285334232425, Validation Loss: 19386.416015625\n",
      "Epoch [62/250], Training Loss: 858.2426794010008, Validation Loss: 11296.59375\n",
      "Epoch [63/250], Training Loss: 285.7773896858837, Validation Loss: 13413.65234375\n",
      "Epoch [64/250], Training Loss: 217.5812361485662, Validation Loss: 15739.20703125\n",
      "Epoch [65/250], Training Loss: 227.7870781901411, Validation Loss: 14172.3720703125\n",
      "Epoch [66/250], Training Loss: 238.89220190485966, Validation Loss: 13791.2744140625\n",
      "Epoch [67/250], Training Loss: 268.3506694284347, Validation Loss: 19353.876953125\n",
      "Epoch [68/250], Training Loss: 306.48842437876584, Validation Loss: 13535.6474609375\n",
      "Epoch [69/250], Training Loss: 622.9087150771755, Validation Loss: 24711.609375\n",
      "Epoch [70/250], Training Loss: 235.52689231214316, Validation Loss: 31398.76953125\n",
      "Epoch [71/250], Training Loss: 623.4952901725865, Validation Loss: 15346.466796875\n",
      "Epoch [72/250], Training Loss: 167.52339824122643, Validation Loss: 32059.943359375\n",
      "Epoch [73/250], Training Loss: 1047.0284770771, Validation Loss: 18940.802734375\n",
      "Epoch [74/250], Training Loss: 279.7611028514737, Validation Loss: 26107.181640625\n",
      "Epoch [75/250], Training Loss: 367.48539193170456, Validation Loss: 56508.05078125\n",
      "Epoch [76/250], Training Loss: 2695.75982524225, Validation Loss: 16007.9423828125\n",
      "Epoch [77/250], Training Loss: 128.2564984606127, Validation Loss: 16055.4794921875\n",
      "Epoch [78/250], Training Loss: 163.12737398749763, Validation Loss: 17454.2578125\n",
      "Epoch [79/250], Training Loss: 233.6821979485177, Validation Loss: 15850.7734375\n",
      "Epoch [80/250], Training Loss: 207.7052651216205, Validation Loss: 16417.11328125\n",
      "Epoch [81/250], Training Loss: 152.79475313441338, Validation Loss: 14600.046875\n",
      "Epoch [82/250], Training Loss: 212.8336092744963, Validation Loss: 34652.06640625\n",
      "Epoch [83/250], Training Loss: 216.69181729107075, Validation Loss: 14267.6923828125\n",
      "Epoch [84/250], Training Loss: 285.47058575614426, Validation Loss: 15679.9951171875\n",
      "Epoch [85/250], Training Loss: 392.3457178485478, Validation Loss: 24157.955078125\n",
      "Epoch [86/250], Training Loss: 512.0020658204519, Validation Loss: 56232.9453125\n",
      "Epoch [87/250], Training Loss: 1511.4262214277487, Validation Loss: 23288.666015625\n",
      "Epoch [88/250], Training Loss: 215.71048017848554, Validation Loss: 24083.1484375\n",
      "Epoch [89/250], Training Loss: 343.9331445587893, Validation Loss: 50259.77734375\n",
      "Epoch [90/250], Training Loss: 848.6782394641564, Validation Loss: 16117.0908203125\n",
      "Epoch [91/250], Training Loss: 245.49791195764234, Validation Loss: 14250.931640625\n",
      "Epoch [92/250], Training Loss: 144.9882303421099, Validation Loss: 17273.77734375\n",
      "Epoch [93/250], Training Loss: 2011.2865098375423, Validation Loss: 21182.423828125\n",
      "Epoch [94/250], Training Loss: 298.3074416633987, Validation Loss: 17029.61328125\n",
      "Epoch [95/250], Training Loss: 136.3396696517006, Validation Loss: 15184.6416015625\n",
      "Epoch [96/250], Training Loss: 85.8466844154905, Validation Loss: 17633.859375\n",
      "Epoch [97/250], Training Loss: 130.9393788951553, Validation Loss: 15449.0966796875\n",
      "Epoch [98/250], Training Loss: 647.1459704729626, Validation Loss: 24297.02734375\n",
      "Epoch [99/250], Training Loss: 312.75161908093446, Validation Loss: 17157.921875\n",
      "Epoch [100/250], Training Loss: 93.90746282882537, Validation Loss: 24630.986328125\n",
      "Epoch [101/250], Training Loss: 78.2266363924931, Validation Loss: 17164.921875\n",
      "Epoch [102/250], Training Loss: 99.55449607323555, Validation Loss: 16261.3427734375\n",
      "Epoch [103/250], Training Loss: 143.31158524561775, Validation Loss: 18953.423828125\n",
      "Epoch [104/250], Training Loss: 151.8898763669393, Validation Loss: 22020.609375\n",
      "Epoch [105/250], Training Loss: 228.83519670336705, Validation Loss: 17812.89453125\n",
      "Epoch [106/250], Training Loss: 96.49827297836083, Validation Loss: 31751.41796875\n",
      "Epoch [107/250], Training Loss: 230.35074718757195, Validation Loss: 17845.275390625\n",
      "Epoch [108/250], Training Loss: 223.39806587744283, Validation Loss: 18843.05859375\n",
      "Epoch [109/250], Training Loss: 82.46634100415389, Validation Loss: 20272.51953125\n",
      "Epoch [110/250], Training Loss: 92.50940278131363, Validation Loss: 17465.59375\n",
      "Epoch [111/250], Training Loss: 86.74827048636084, Validation Loss: 16567.1328125\n",
      "Epoch [112/250], Training Loss: 95.81289348481009, Validation Loss: 17412.1875\n",
      "Epoch [113/250], Training Loss: 179.00444346901264, Validation Loss: 27418.701171875\n",
      "Epoch [114/250], Training Loss: 314.7954194359326, Validation Loss: 20157.181640625\n",
      "Epoch [115/250], Training Loss: 838.8992294316962, Validation Loss: 22017.89453125\n",
      "Epoch [116/250], Training Loss: 248.36185233358435, Validation Loss: 17880.548828125\n",
      "Epoch [117/250], Training Loss: 141.38707864057102, Validation Loss: 19016.76953125\n",
      "Epoch [118/250], Training Loss: 205.32408080647332, Validation Loss: 21325.01171875\n",
      "Epoch [119/250], Training Loss: 73.4610111094268, Validation Loss: 19175.494140625\n",
      "Epoch [120/250], Training Loss: 75.40272399663131, Validation Loss: 19087.064453125\n",
      "Epoch [121/250], Training Loss: 584.3006880415564, Validation Loss: 20146.240234375\n",
      "Epoch [122/250], Training Loss: 422.37101089660086, Validation Loss: 14993.4794921875\n",
      "Epoch [123/250], Training Loss: 207.60185487171435, Validation Loss: 22302.6953125\n",
      "Epoch [124/250], Training Loss: 123.64571051323406, Validation Loss: 21455.509765625\n",
      "Epoch [125/250], Training Loss: 124.01283499511287, Validation Loss: 22124.39453125\n",
      "Epoch [126/250], Training Loss: 265.5654478183714, Validation Loss: 22685.494140625\n",
      "Epoch [127/250], Training Loss: 178.50228870500172, Validation Loss: 17457.28125\n",
      "Epoch [128/250], Training Loss: 278.5692247143739, Validation Loss: 19255.001953125\n",
      "Epoch [129/250], Training Loss: 226.98445611304362, Validation Loss: 19103.10546875\n",
      "Epoch [130/250], Training Loss: 295.92545574461747, Validation Loss: 23002.423828125\n",
      "Epoch [131/250], Training Loss: 148.96853481464927, Validation Loss: 19941.095703125\n",
      "Epoch [132/250], Training Loss: 578.8716199064365, Validation Loss: 16717.1484375\n",
      "Epoch [133/250], Training Loss: 1057.6312968374511, Validation Loss: 17578.837890625\n",
      "Epoch [134/250], Training Loss: 149.53569398197916, Validation Loss: 19540.30078125\n",
      "Epoch [135/250], Training Loss: 510.34784305810706, Validation Loss: 21729.341796875\n",
      "Epoch [136/250], Training Loss: 682.8308621520683, Validation Loss: 21059.41796875\n",
      "Epoch [137/250], Training Loss: 345.5295784460509, Validation Loss: 12696.3759765625\n",
      "Epoch [138/250], Training Loss: 257.8048376466648, Validation Loss: 23950.990234375\n",
      "Epoch [139/250], Training Loss: 84.04543759806545, Validation Loss: 20387.142578125\n",
      "Epoch [140/250], Training Loss: 704.360491524352, Validation Loss: 31864.01171875\n",
      "Epoch [141/250], Training Loss: 233.64624973092668, Validation Loss: 24133.8515625\n",
      "Epoch [142/250], Training Loss: 223.24455300681976, Validation Loss: 23690.005859375\n",
      "Epoch [143/250], Training Loss: 149.094603026333, Validation Loss: 23596.099609375\n",
      "Epoch [144/250], Training Loss: 117.18577316037803, Validation Loss: 22541.712890625\n",
      "Epoch [145/250], Training Loss: 139.9468045105376, Validation Loss: 24115.30078125\n",
      "Epoch [146/250], Training Loss: 86.01208892905237, Validation Loss: 21732.26171875\n",
      "Epoch [147/250], Training Loss: 104.9919997250115, Validation Loss: 22857.76953125\n",
      "Epoch [148/250], Training Loss: 92.40429099352131, Validation Loss: 21972.748046875\n",
      "Epoch [149/250], Training Loss: 114.24301130569789, Validation Loss: 22339.166015625\n",
      "Epoch [150/250], Training Loss: 118.11085865545132, Validation Loss: 19397.376953125\n",
      "Epoch [151/250], Training Loss: 163.9749389372421, Validation Loss: 21088.33984375\n",
      "Epoch [152/250], Training Loss: 146.21930627646512, Validation Loss: 22332.04296875\n",
      "Epoch [153/250], Training Loss: 123.16189494500819, Validation Loss: 23180.509765625\n",
      "Epoch [154/250], Training Loss: 179.61541677514845, Validation Loss: 23391.435546875\n",
      "Epoch [155/250], Training Loss: 162.39723300658878, Validation Loss: 22108.01171875\n",
      "Epoch [156/250], Training Loss: 215.88022587886792, Validation Loss: 22077.92578125\n",
      "Epoch [157/250], Training Loss: 146.12205879350705, Validation Loss: 23007.416015625\n",
      "Epoch [158/250], Training Loss: 179.63437619702546, Validation Loss: 22966.205078125\n",
      "Epoch [159/250], Training Loss: 81.44788210315387, Validation Loss: 23273.189453125\n",
      "Epoch [160/250], Training Loss: 124.749048720642, Validation Loss: 54140.2109375\n",
      "Epoch [161/250], Training Loss: 226.7768698739687, Validation Loss: 103844.9140625\n",
      "Epoch [162/250], Training Loss: 993.2395876027032, Validation Loss: 23247.30078125\n",
      "Epoch [163/250], Training Loss: 368.69986768615627, Validation Loss: 25802.208984375\n",
      "Epoch [164/250], Training Loss: 334.5646997847052, Validation Loss: 23315.947265625\n",
      "Epoch [165/250], Training Loss: 213.34277063789077, Validation Loss: 23692.783203125\n",
      "Epoch [166/250], Training Loss: 128.48703542596445, Validation Loss: 23897.599609375\n",
      "Epoch [167/250], Training Loss: 157.18192711082813, Validation Loss: 23329.095703125\n",
      "Epoch [168/250], Training Loss: 262.5137932205315, Validation Loss: 18403.908203125\n",
      "Epoch [169/250], Training Loss: 317.61574083016194, Validation Loss: 37652.25\n",
      "Epoch [170/250], Training Loss: 145.84854227666608, Validation Loss: 22312.712890625\n",
      "Epoch [171/250], Training Loss: 72.9115949313835, Validation Loss: 23274.08984375\n",
      "Epoch [172/250], Training Loss: 264.6137627704794, Validation Loss: 25578.814453125\n",
      "Epoch [173/250], Training Loss: 118.07916166298786, Validation Loss: 23451.615234375\n",
      "Epoch [174/250], Training Loss: 87.77424862826412, Validation Loss: 23084.400390625\n",
      "Epoch [175/250], Training Loss: 213.48026687045237, Validation Loss: 22221.884765625\n",
      "Epoch [176/250], Training Loss: 367.9668500497795, Validation Loss: 20481.212890625\n",
      "Epoch [177/250], Training Loss: 264.6163360717335, Validation Loss: 20631.70703125\n",
      "Epoch [178/250], Training Loss: 69.7337411915377, Validation Loss: 21583.09375\n",
      "Epoch [179/250], Training Loss: 66.17116457290054, Validation Loss: 22284.341796875\n",
      "Epoch [180/250], Training Loss: 65.9680172201049, Validation Loss: 21626.376953125\n",
      "Epoch [181/250], Training Loss: 79.18062296252593, Validation Loss: 22197.576171875\n",
      "Epoch [182/250], Training Loss: 75.3987846753178, Validation Loss: 22077.798828125\n",
      "Epoch [183/250], Training Loss: 85.6816884864002, Validation Loss: 21361.87109375\n",
      "Epoch [184/250], Training Loss: 76.4676239762713, Validation Loss: 22340.537109375\n",
      "Epoch [185/250], Training Loss: 81.81717506593942, Validation Loss: 23290.42578125\n",
      "Epoch [186/250], Training Loss: 81.95382569543911, Validation Loss: 21018.85546875\n",
      "Epoch [187/250], Training Loss: 97.27614357378536, Validation Loss: 21458.73046875\n",
      "Epoch [188/250], Training Loss: 115.59307113065678, Validation Loss: 22689.052734375\n",
      "Epoch [189/250], Training Loss: 84.4517844608886, Validation Loss: 22177.283203125\n",
      "Epoch [190/250], Training Loss: 99.79964051396645, Validation Loss: 21270.771484375\n",
      "Epoch [191/250], Training Loss: 150.23588379891595, Validation Loss: 25694.2578125\n",
      "Epoch [192/250], Training Loss: 96.36536095480274, Validation Loss: 23862.626953125\n",
      "Epoch [193/250], Training Loss: 123.74057683744574, Validation Loss: 23445.8046875\n",
      "Epoch [194/250], Training Loss: 82.20816851244474, Validation Loss: 23326.97265625\n",
      "Epoch [195/250], Training Loss: 84.92726934037283, Validation Loss: 21825.291015625\n",
      "Epoch [196/250], Training Loss: 80.63943489929194, Validation Loss: 22709.875\n",
      "Epoch [197/250], Training Loss: 81.62753222874146, Validation Loss: 21973.544921875\n",
      "Epoch [198/250], Training Loss: 107.72412738407003, Validation Loss: 19229.7109375\n",
      "Epoch [199/250], Training Loss: 111.56879889228233, Validation Loss: 20972.501953125\n",
      "Epoch [200/250], Training Loss: 164.9704020132085, Validation Loss: 24338.787109375\n",
      "Epoch [201/250], Training Loss: 64.81587231498683, Validation Loss: 21079.3671875\n",
      "Epoch [202/250], Training Loss: 260.0517389793931, Validation Loss: 25545.177734375\n",
      "Epoch [203/250], Training Loss: 805.0113177415377, Validation Loss: 22975.701171875\n",
      "Epoch [204/250], Training Loss: 742.1101582429004, Validation Loss: 22070.76953125\n",
      "Epoch [205/250], Training Loss: 245.14521780790727, Validation Loss: 18262.32421875\n",
      "Epoch [206/250], Training Loss: 245.06542570472644, Validation Loss: 24111.021484375\n",
      "Epoch [207/250], Training Loss: 124.24738779567961, Validation Loss: 20032.501953125\n",
      "Epoch [208/250], Training Loss: 125.73138325833774, Validation Loss: 19714.083984375\n",
      "Epoch [209/250], Training Loss: 91.24024489645805, Validation Loss: 20506.00390625\n",
      "Epoch [210/250], Training Loss: 495.5945644026934, Validation Loss: 22696.33984375\n",
      "Epoch [211/250], Training Loss: 382.72147219428115, Validation Loss: 21330.947265625\n",
      "Epoch [212/250], Training Loss: 288.4003992502381, Validation Loss: 20357.95703125\n",
      "Epoch [213/250], Training Loss: 152.15045188187122, Validation Loss: 20061.166015625\n",
      "Epoch [214/250], Training Loss: 128.4807738075086, Validation Loss: 25352.259765625\n",
      "Epoch [215/250], Training Loss: 68.26615390766598, Validation Loss: 20459.45703125\n",
      "Epoch [216/250], Training Loss: 125.16029116277218, Validation Loss: 22042.228515625\n",
      "Epoch [217/250], Training Loss: 295.04042535805416, Validation Loss: 21600.21875\n",
      "Epoch [218/250], Training Loss: 300.3462171840721, Validation Loss: 17072.380859375\n",
      "Epoch [219/250], Training Loss: 113.20360810909753, Validation Loss: 21265.23046875\n",
      "Epoch [220/250], Training Loss: 126.69069736515759, Validation Loss: 22146.005859375\n",
      "Epoch [221/250], Training Loss: 226.18369091639548, Validation Loss: 23118.583984375\n",
      "Epoch [222/250], Training Loss: 177.07609903740143, Validation Loss: 22100.470703125\n",
      "Epoch [223/250], Training Loss: 313.9499985128624, Validation Loss: 23450.48828125\n",
      "Epoch [224/250], Training Loss: 358.0316267553782, Validation Loss: 24209.283203125\n",
      "Epoch [225/250], Training Loss: 345.1249843310925, Validation Loss: 22185.88671875\n",
      "Epoch [226/250], Training Loss: 357.8816403099061, Validation Loss: 22350.62109375\n",
      "Epoch [227/250], Training Loss: 250.2391338429211, Validation Loss: 24290.4296875\n",
      "Epoch [228/250], Training Loss: 96.32566398387154, Validation Loss: 22627.51171875\n",
      "Epoch [229/250], Training Loss: 130.90696283154878, Validation Loss: 21753.96484375\n",
      "Epoch [230/250], Training Loss: 303.64451075791254, Validation Loss: 22180.701171875\n",
      "Epoch [231/250], Training Loss: 125.086099230254, Validation Loss: 21637.697265625\n",
      "Epoch [232/250], Training Loss: 134.3745387710545, Validation Loss: 22555.302734375\n",
      "Epoch [233/250], Training Loss: 105.76930871981295, Validation Loss: 19838.61328125\n",
      "Epoch [234/250], Training Loss: 124.69382408507992, Validation Loss: 19190.619140625\n",
      "Epoch [235/250], Training Loss: 133.66800635480251, Validation Loss: 30422.08203125\n",
      "Epoch [236/250], Training Loss: 117.82441815105372, Validation Loss: 21123.34375\n",
      "Epoch [237/250], Training Loss: 150.64809131912048, Validation Loss: 24469.919921875\n",
      "Epoch [238/250], Training Loss: 137.61701692249312, Validation Loss: 20432.734375\n",
      "Epoch [239/250], Training Loss: 293.5844329278519, Validation Loss: 25898.017578125\n",
      "Epoch [240/250], Training Loss: 128.53907228966708, Validation Loss: 22030.59375\n",
      "Epoch [241/250], Training Loss: 103.15563280455736, Validation Loss: 21888.697265625\n",
      "Epoch [242/250], Training Loss: 211.16616472243797, Validation Loss: 21536.41796875\n",
      "Epoch [243/250], Training Loss: 123.69731473551397, Validation Loss: 22597.77734375\n",
      "Epoch [244/250], Training Loss: 96.10180470682756, Validation Loss: 20865.951171875\n",
      "Epoch [245/250], Training Loss: 110.05731117920732, Validation Loss: 21719.556640625\n",
      "Epoch [246/250], Training Loss: 134.2011386979886, Validation Loss: 22435.22265625\n",
      "Epoch [247/250], Training Loss: 89.55026885174071, Validation Loss: 22216.912109375\n",
      "Epoch [248/250], Training Loss: 95.08719882216057, Validation Loss: 22022.43359375\n",
      "Epoch [249/250], Training Loss: 127.01896031970382, Validation Loss: 18594.701171875\n",
      "Epoch [250/250], Training Loss: 202.7791769060757, Validation Loss: 18052.84765625\n",
      "Test Loss: 16823.150390625\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.005, 10)\n",
      "Epoch [1/250], Training Loss: 19141.332052599057, Validation Loss: 19764.435546875\n",
      "Epoch [2/250], Training Loss: 14412.253789271257, Validation Loss: 18126.962890625\n",
      "Epoch [3/250], Training Loss: 11035.647845226657, Validation Loss: 14067.259765625\n",
      "Epoch [4/250], Training Loss: 8721.736298338383, Validation Loss: 16816.796875\n",
      "Epoch [5/250], Training Loss: 7091.116642680038, Validation Loss: 16253.1318359375\n",
      "Epoch [6/250], Training Loss: 5767.560092611686, Validation Loss: 16421.4765625\n",
      "Epoch [7/250], Training Loss: 4692.069805632097, Validation Loss: 16075.2021484375\n",
      "Epoch [8/250], Training Loss: 3827.412678110276, Validation Loss: 15992.19140625\n",
      "Epoch [9/250], Training Loss: 3106.465937646452, Validation Loss: 16403.94921875\n",
      "Epoch [10/250], Training Loss: 2537.478321171079, Validation Loss: 17489.90234375\n",
      "Epoch [11/250], Training Loss: 2116.1058834641763, Validation Loss: 18088.376953125\n",
      "Epoch [12/250], Training Loss: 1768.1317688655713, Validation Loss: 17704.490234375\n",
      "Epoch [13/250], Training Loss: 1491.1366209535565, Validation Loss: 16892.0\n",
      "Epoch [14/250], Training Loss: 1276.0321312837748, Validation Loss: 16788.521484375\n",
      "Epoch [15/250], Training Loss: 1096.665656528385, Validation Loss: 16946.3828125\n",
      "Epoch [16/250], Training Loss: 942.3551812101695, Validation Loss: 17052.87890625\n",
      "Epoch [17/250], Training Loss: 811.912825268417, Validation Loss: 17371.5546875\n",
      "Epoch [18/250], Training Loss: 696.385224038507, Validation Loss: 17607.4140625\n",
      "Epoch [19/250], Training Loss: 595.6928265669642, Validation Loss: 17974.810546875\n",
      "Epoch [20/250], Training Loss: 510.20553786514176, Validation Loss: 18320.478515625\n",
      "Epoch [21/250], Training Loss: 435.24835651336144, Validation Loss: 18653.875\n",
      "Epoch [22/250], Training Loss: 370.20251319862433, Validation Loss: 19052.994140625\n",
      "Epoch [23/250], Training Loss: 312.3411048915153, Validation Loss: 19148.3046875\n",
      "Epoch [24/250], Training Loss: 260.91956462792837, Validation Loss: 19118.529296875\n",
      "Epoch [25/250], Training Loss: 219.70365246185503, Validation Loss: 19659.5703125\n",
      "Epoch [26/250], Training Loss: 188.26007574618302, Validation Loss: 18612.36328125\n",
      "Epoch [27/250], Training Loss: 170.39465348257554, Validation Loss: 18725.4609375\n",
      "Epoch [28/250], Training Loss: 149.73061382757692, Validation Loss: 18968.322265625\n",
      "Epoch [29/250], Training Loss: 133.497977857639, Validation Loss: 19715.65234375\n",
      "Epoch [30/250], Training Loss: 115.78522083023778, Validation Loss: 19338.265625\n",
      "Epoch [31/250], Training Loss: 98.39407154627011, Validation Loss: 18936.64453125\n",
      "Epoch [32/250], Training Loss: 78.11824115674789, Validation Loss: 18823.57421875\n",
      "Epoch [33/250], Training Loss: 66.15120662497398, Validation Loss: 18853.255859375\n",
      "Epoch [34/250], Training Loss: 58.70548348006263, Validation Loss: 18928.83203125\n",
      "Epoch [35/250], Training Loss: 50.89335194699821, Validation Loss: 19244.861328125\n",
      "Epoch [36/250], Training Loss: 44.80548433576586, Validation Loss: 19591.68359375\n",
      "Epoch [37/250], Training Loss: 39.74518245499174, Validation Loss: 19761.412109375\n",
      "Epoch [38/250], Training Loss: 35.50622336898865, Validation Loss: 19938.6640625\n",
      "Epoch [39/250], Training Loss: 31.810907644297714, Validation Loss: 20175.3828125\n",
      "Epoch [40/250], Training Loss: 1731.1285992008454, Validation Loss: 13739.05078125\n",
      "Epoch [41/250], Training Loss: 1642.347621872806, Validation Loss: 22128.958984375\n",
      "Epoch [42/250], Training Loss: 846.0594637963575, Validation Loss: 15585.705078125\n",
      "Epoch [43/250], Training Loss: 86.58135311961229, Validation Loss: 18506.091796875\n",
      "Epoch [44/250], Training Loss: 48.99677046155628, Validation Loss: 18307.400390625\n",
      "Epoch [45/250], Training Loss: 32.22089672167994, Validation Loss: 18740.08203125\n",
      "Epoch [46/250], Training Loss: 28.655455468682344, Validation Loss: 19601.138671875\n",
      "Epoch [47/250], Training Loss: 24.54440279806029, Validation Loss: 20228.529296875\n",
      "Epoch [48/250], Training Loss: 21.83124486777809, Validation Loss: 20642.923828125\n",
      "Epoch [49/250], Training Loss: 19.577298674387723, Validation Loss: 20919.0703125\n",
      "Epoch [50/250], Training Loss: 17.863751319296988, Validation Loss: 21069.966796875\n",
      "Epoch [51/250], Training Loss: 17.215670430657458, Validation Loss: 21221.68359375\n",
      "Epoch [52/250], Training Loss: 16.8531827629461, Validation Loss: 21447.431640625\n",
      "Epoch [53/250], Training Loss: 17.442281183232705, Validation Loss: 21429.681640625\n",
      "Epoch [54/250], Training Loss: 18.643037958630984, Validation Loss: 21260.47265625\n",
      "Epoch [55/250], Training Loss: 584.8080159253271, Validation Loss: 21792.888671875\n",
      "Epoch [56/250], Training Loss: 338.85531774010724, Validation Loss: 18884.115234375\n",
      "Epoch [57/250], Training Loss: 338.2829014681368, Validation Loss: 21872.353515625\n",
      "Epoch [58/250], Training Loss: 52.428091918107235, Validation Loss: 20136.15234375\n",
      "Epoch [59/250], Training Loss: 31.881404576722442, Validation Loss: 20963.1328125\n",
      "Epoch [60/250], Training Loss: 18.142982652448797, Validation Loss: 21232.24609375\n",
      "Epoch [61/250], Training Loss: 15.296730285297897, Validation Loss: 21464.765625\n",
      "Epoch [62/250], Training Loss: 13.59508530639255, Validation Loss: 21664.037109375\n",
      "Epoch [63/250], Training Loss: 147.15289017011838, Validation Loss: 21729.09375\n",
      "Epoch [64/250], Training Loss: 20.48971042185802, Validation Loss: 21564.716796875\n",
      "Epoch [65/250], Training Loss: 20.908810784526516, Validation Loss: 20959.759765625\n",
      "Epoch [66/250], Training Loss: 14.140291063150068, Validation Loss: 22088.587890625\n",
      "Epoch [67/250], Training Loss: 11.23999905583871, Validation Loss: 21371.958984375\n",
      "Epoch [68/250], Training Loss: 9.482952240876442, Validation Loss: 21826.787109375\n",
      "Epoch [69/250], Training Loss: 95.87380434148388, Validation Loss: 21679.310546875\n",
      "Epoch [70/250], Training Loss: 19.036350114708927, Validation Loss: 21490.517578125\n",
      "Epoch [71/250], Training Loss: 13.923243242274001, Validation Loss: 21167.953125\n",
      "Epoch [72/250], Training Loss: 11.973432551971607, Validation Loss: 21370.01953125\n",
      "Epoch [73/250], Training Loss: 9.849683972474493, Validation Loss: 21284.875\n",
      "Epoch [74/250], Training Loss: 8.76187465579605, Validation Loss: 21548.732421875\n",
      "Epoch [75/250], Training Loss: 9.655495098756656, Validation Loss: 22133.0859375\n",
      "Epoch [76/250], Training Loss: 9.95577303601715, Validation Loss: 22290.4765625\n",
      "Epoch [77/250], Training Loss: 41.16346308132285, Validation Loss: 20969.1328125\n",
      "Epoch [78/250], Training Loss: 14.527121841849592, Validation Loss: 21370.72265625\n",
      "Epoch [79/250], Training Loss: 11.014097240135655, Validation Loss: 21108.259765625\n",
      "Epoch [80/250], Training Loss: 9.753871367544757, Validation Loss: 20964.037109375\n",
      "Epoch [81/250], Training Loss: 9.298651345763005, Validation Loss: 21048.21875\n",
      "Epoch [82/250], Training Loss: 9.353130510525151, Validation Loss: 20939.33203125\n",
      "Epoch [83/250], Training Loss: 8.848173017278839, Validation Loss: 20591.380859375\n",
      "Epoch [84/250], Training Loss: 8.118067880700004, Validation Loss: 21794.60546875\n",
      "Epoch [85/250], Training Loss: 8.283676896876061, Validation Loss: 22386.369140625\n",
      "Epoch [86/250], Training Loss: 8.503051641186392, Validation Loss: 22491.21484375\n",
      "Epoch [87/250], Training Loss: 8.896154485215291, Validation Loss: 22728.04296875\n",
      "Epoch [88/250], Training Loss: 9.039763203931088, Validation Loss: 22975.009765625\n",
      "Epoch [89/250], Training Loss: 9.246435935703511, Validation Loss: 22895.705078125\n",
      "Epoch [90/250], Training Loss: 10.818657790949096, Validation Loss: 23164.513671875\n",
      "Epoch [91/250], Training Loss: 10.445588969583918, Validation Loss: 23136.751953125\n",
      "Epoch [92/250], Training Loss: 9.097569922801362, Validation Loss: 23184.76171875\n",
      "Epoch [93/250], Training Loss: 8.877819620981816, Validation Loss: 23254.080078125\n",
      "Epoch [94/250], Training Loss: 8.804909804207607, Validation Loss: 23361.90234375\n",
      "Epoch [95/250], Training Loss: 9.00704907975935, Validation Loss: 23444.724609375\n",
      "Epoch [96/250], Training Loss: 9.340207985932759, Validation Loss: 23488.9375\n",
      "Epoch [97/250], Training Loss: 9.668077316625771, Validation Loss: 23358.890625\n",
      "Epoch [98/250], Training Loss: 10.431222512482679, Validation Loss: 23471.154296875\n",
      "Epoch [99/250], Training Loss: 10.628811738124757, Validation Loss: 23357.47265625\n",
      "Epoch [100/250], Training Loss: 12.826662137399213, Validation Loss: 23375.439453125\n",
      "Epoch [101/250], Training Loss: 12.793106260141359, Validation Loss: 23123.32421875\n",
      "Epoch [102/250], Training Loss: 10.394367610658701, Validation Loss: 23158.48046875\n",
      "Epoch [103/250], Training Loss: 12.092553634018017, Validation Loss: 23210.76171875\n",
      "Epoch [104/250], Training Loss: 12.462843493677346, Validation Loss: 22732.279296875\n",
      "Epoch [105/250], Training Loss: 12.098560739157271, Validation Loss: 23194.08203125\n",
      "Epoch [106/250], Training Loss: 13.435131564814913, Validation Loss: 23114.318359375\n",
      "Epoch [107/250], Training Loss: 13.33798391931634, Validation Loss: 22877.3984375\n",
      "Epoch [108/250], Training Loss: 14.385248004335239, Validation Loss: 23105.994140625\n",
      "Epoch [109/250], Training Loss: 13.837815212166987, Validation Loss: 23155.5859375\n",
      "Epoch [110/250], Training Loss: 13.45649268199459, Validation Loss: 22793.521484375\n",
      "Epoch [111/250], Training Loss: 13.134970625593864, Validation Loss: 22922.7265625\n",
      "Epoch [112/250], Training Loss: 13.647552707910185, Validation Loss: 22916.62109375\n",
      "Epoch [113/250], Training Loss: 14.220173854808394, Validation Loss: 22940.998046875\n",
      "Epoch [114/250], Training Loss: 14.531671133506936, Validation Loss: 22865.228515625\n",
      "Epoch [115/250], Training Loss: 14.865748599535209, Validation Loss: 22863.83984375\n",
      "Epoch [116/250], Training Loss: 15.732256422105527, Validation Loss: 23268.375\n",
      "Epoch [117/250], Training Loss: 15.744710560026949, Validation Loss: 22891.298828125\n",
      "Epoch [118/250], Training Loss: 15.329047562217477, Validation Loss: 22771.5\n",
      "Epoch [119/250], Training Loss: 14.875535875363015, Validation Loss: 22350.04296875\n",
      "Epoch [120/250], Training Loss: 14.312456478854667, Validation Loss: 22696.92578125\n",
      "Epoch [121/250], Training Loss: 14.397814696988506, Validation Loss: 22382.810546875\n",
      "Epoch [122/250], Training Loss: 14.48256233264425, Validation Loss: 22306.666015625\n",
      "Epoch [123/250], Training Loss: 14.415187259275065, Validation Loss: 22265.779296875\n",
      "Epoch [124/250], Training Loss: 14.471686064792417, Validation Loss: 22599.5\n",
      "Epoch [125/250], Training Loss: 14.9086672358394, Validation Loss: 22256.138671875\n",
      "Epoch [126/250], Training Loss: 14.519857199543265, Validation Loss: 22419.560546875\n",
      "Epoch [127/250], Training Loss: 14.711928742429105, Validation Loss: 22562.708984375\n",
      "Epoch [128/250], Training Loss: 15.679702377028102, Validation Loss: 22208.025390625\n",
      "Epoch [129/250], Training Loss: 15.515468006930151, Validation Loss: 22166.736328125\n",
      "Epoch [130/250], Training Loss: 16.53884760641584, Validation Loss: 22090.703125\n",
      "Epoch [131/250], Training Loss: 16.065633627651263, Validation Loss: 22308.15234375\n",
      "Epoch [132/250], Training Loss: 15.841379586264015, Validation Loss: 22419.537109375\n",
      "Epoch [133/250], Training Loss: 15.947155657869022, Validation Loss: 22207.9296875\n",
      "Epoch [134/250], Training Loss: 15.864577086658691, Validation Loss: 22309.251953125\n",
      "Epoch [135/250], Training Loss: 16.400458389299725, Validation Loss: 22266.677734375\n",
      "Epoch [136/250], Training Loss: 16.469722039268067, Validation Loss: 22222.1796875\n",
      "Epoch [137/250], Training Loss: 16.44707475331475, Validation Loss: 22217.80078125\n",
      "Epoch [138/250], Training Loss: 16.582320647963787, Validation Loss: 22199.681640625\n",
      "Epoch [139/250], Training Loss: 16.61452524186587, Validation Loss: 22249.19921875\n",
      "Epoch [140/250], Training Loss: 16.572653859019912, Validation Loss: 22242.40234375\n",
      "Epoch [141/250], Training Loss: 16.898970942250592, Validation Loss: 22217.677734375\n",
      "Epoch [142/250], Training Loss: 16.827322452320406, Validation Loss: 22274.375\n",
      "Epoch [143/250], Training Loss: 16.920447834925557, Validation Loss: 22243.716796875\n",
      "Epoch [144/250], Training Loss: 17.09403069483011, Validation Loss: 22247.533203125\n",
      "Epoch [145/250], Training Loss: 17.190692648601047, Validation Loss: 22282.421875\n",
      "Epoch [146/250], Training Loss: 17.304812656249844, Validation Loss: 22162.478515625\n",
      "Epoch [147/250], Training Loss: 17.73435698544883, Validation Loss: 22120.150390625\n",
      "Epoch [148/250], Training Loss: 17.849545391514514, Validation Loss: 22146.16015625\n",
      "Epoch [149/250], Training Loss: 17.972076046357248, Validation Loss: 22448.92578125\n",
      "Epoch [150/250], Training Loss: 17.701999737678996, Validation Loss: 21976.55078125\n",
      "Epoch [151/250], Training Loss: 17.436463361003856, Validation Loss: 22063.85546875\n",
      "Epoch [152/250], Training Loss: 17.664004354327044, Validation Loss: 22055.529296875\n",
      "Epoch [153/250], Training Loss: 17.38337184117343, Validation Loss: 22100.43359375\n",
      "Epoch [154/250], Training Loss: 17.681104733230335, Validation Loss: 22099.4921875\n",
      "Epoch [155/250], Training Loss: 17.320333926080263, Validation Loss: 22144.28515625\n",
      "Epoch [156/250], Training Loss: 17.528401217473135, Validation Loss: 22153.69140625\n",
      "Epoch [157/250], Training Loss: 17.465640895701807, Validation Loss: 22196.01953125\n",
      "Epoch [158/250], Training Loss: 17.28910611768035, Validation Loss: 22224.478515625\n",
      "Epoch [159/250], Training Loss: 17.400582275392015, Validation Loss: 22205.8828125\n",
      "Epoch [160/250], Training Loss: 17.409700839544648, Validation Loss: 22181.931640625\n",
      "Epoch [161/250], Training Loss: 17.512562835790803, Validation Loss: 22159.60546875\n",
      "Epoch [162/250], Training Loss: 17.435410511143797, Validation Loss: 22178.001953125\n",
      "Epoch [163/250], Training Loss: 17.409189976822308, Validation Loss: 22213.078125\n",
      "Epoch [164/250], Training Loss: 17.25868423510841, Validation Loss: 22227.01953125\n",
      "Epoch [165/250], Training Loss: 17.37750361566578, Validation Loss: 22236.169921875\n",
      "Epoch [166/250], Training Loss: 17.27097515125167, Validation Loss: 22225.359375\n",
      "Epoch [167/250], Training Loss: 17.41323154251761, Validation Loss: 22194.984375\n",
      "Epoch [168/250], Training Loss: 17.326896981650982, Validation Loss: 22208.79296875\n",
      "Epoch [169/250], Training Loss: 17.406262627761937, Validation Loss: 22223.9296875\n",
      "Epoch [170/250], Training Loss: 17.197242296518812, Validation Loss: 22242.5234375\n",
      "Epoch [171/250], Training Loss: 17.310790167444328, Validation Loss: 22216.8203125\n",
      "Epoch [172/250], Training Loss: 17.264644608418962, Validation Loss: 22240.90625\n",
      "Epoch [173/250], Training Loss: 17.191750574796544, Validation Loss: 22225.970703125\n",
      "Epoch [174/250], Training Loss: 17.108034055098184, Validation Loss: 22233.083984375\n",
      "Epoch [175/250], Training Loss: 17.100236354207944, Validation Loss: 22216.603515625\n",
      "Epoch [176/250], Training Loss: 16.997767307762604, Validation Loss: 22257.328125\n",
      "Epoch [177/250], Training Loss: 16.914487373271374, Validation Loss: 22242.33203125\n",
      "Epoch [178/250], Training Loss: 16.818429690066512, Validation Loss: 22285.318359375\n",
      "Epoch [179/250], Training Loss: 16.74792509001946, Validation Loss: 22277.455078125\n",
      "Epoch [180/250], Training Loss: 16.729719245638407, Validation Loss: 22293.345703125\n",
      "Epoch [181/250], Training Loss: 16.653942992367853, Validation Loss: 22292.568359375\n",
      "Epoch [182/250], Training Loss: 16.61162438222153, Validation Loss: 22310.103515625\n",
      "Epoch [183/250], Training Loss: 16.539317643821704, Validation Loss: 22321.70703125\n",
      "Epoch [184/250], Training Loss: 16.49296866013553, Validation Loss: 22349.37890625\n",
      "Epoch [185/250], Training Loss: 16.446556527011992, Validation Loss: 22379.185546875\n",
      "Epoch [186/250], Training Loss: 16.48193903799733, Validation Loss: 22384.791015625\n",
      "Epoch [187/250], Training Loss: 16.611747917453076, Validation Loss: 22358.099609375\n",
      "Epoch [188/250], Training Loss: 17.05472716666462, Validation Loss: 22221.978515625\n",
      "Epoch [189/250], Training Loss: 16.85055557041644, Validation Loss: 22309.142578125\n",
      "Epoch [190/250], Training Loss: 17.805309567578252, Validation Loss: 21960.537109375\n",
      "Epoch [191/250], Training Loss: 17.386406156493155, Validation Loss: 21888.353515625\n",
      "Epoch [192/250], Training Loss: 17.29869422909875, Validation Loss: 21862.44140625\n",
      "Epoch [193/250], Training Loss: 17.260991220159003, Validation Loss: 21818.1015625\n",
      "Epoch [194/250], Training Loss: 17.265174820357426, Validation Loss: 21773.59765625\n",
      "Epoch [195/250], Training Loss: 17.245950486513124, Validation Loss: 21727.921875\n",
      "Epoch [196/250], Training Loss: 17.228940837625153, Validation Loss: 21699.1171875\n",
      "Epoch [197/250], Training Loss: 17.199715629966956, Validation Loss: 21684.765625\n",
      "Epoch [198/250], Training Loss: 17.175411390224326, Validation Loss: 21683.443359375\n",
      "Epoch [199/250], Training Loss: 17.085720085773488, Validation Loss: 21706.296875\n",
      "Epoch [200/250], Training Loss: 17.096110042880852, Validation Loss: 21718.75390625\n",
      "Epoch [201/250], Training Loss: 16.966850018994112, Validation Loss: 21772.318359375\n",
      "Epoch [202/250], Training Loss: 17.119149525904476, Validation Loss: 21762.552734375\n",
      "Epoch [203/250], Training Loss: 16.857320670580133, Validation Loss: 21874.87109375\n",
      "Epoch [204/250], Training Loss: 17.286317003475006, Validation Loss: 21765.9375\n",
      "Epoch [205/250], Training Loss: 16.68589688852389, Validation Loss: 21996.181640625\n",
      "Epoch [206/250], Training Loss: 26.40341464382092, Validation Loss: 21442.76953125\n",
      "Epoch [207/250], Training Loss: 22.882014009528316, Validation Loss: 21540.134765625\n",
      "Epoch [208/250], Training Loss: 24.575406771334848, Validation Loss: 21152.765625\n",
      "Epoch [209/250], Training Loss: 23.726476948212888, Validation Loss: 21011.9375\n",
      "Epoch [210/250], Training Loss: 28.33704859326121, Validation Loss: 20718.90234375\n",
      "Epoch [211/250], Training Loss: 24.043377191076868, Validation Loss: 20933.025390625\n",
      "Epoch [212/250], Training Loss: 118.86484789032028, Validation Loss: 20499.94140625\n",
      "Epoch [213/250], Training Loss: 28.907708345180147, Validation Loss: 20803.552734375\n",
      "Epoch [214/250], Training Loss: 23.33807572504478, Validation Loss: 21193.3359375\n",
      "Epoch [215/250], Training Loss: 87.31550208098568, Validation Loss: 20792.12109375\n",
      "Epoch [216/250], Training Loss: 24.8986914112683, Validation Loss: 20872.87890625\n",
      "Epoch [217/250], Training Loss: 393.79572836505776, Validation Loss: 21404.857421875\n",
      "Epoch [218/250], Training Loss: 27.352767182738877, Validation Loss: 21213.36328125\n",
      "Epoch [219/250], Training Loss: 24.84622767165392, Validation Loss: 21629.5\n",
      "Epoch [220/250], Training Loss: 74.70909671340392, Validation Loss: 20499.453125\n",
      "Epoch [221/250], Training Loss: 26.22837376143112, Validation Loss: 21252.18359375\n",
      "Epoch [222/250], Training Loss: 28.564174250474945, Validation Loss: 20468.91796875\n",
      "Epoch [223/250], Training Loss: 26.92502466769252, Validation Loss: 20978.74609375\n",
      "Epoch [224/250], Training Loss: 125.30239796506491, Validation Loss: 19950.873046875\n",
      "Epoch [225/250], Training Loss: 28.796887975164218, Validation Loss: 20407.1796875\n",
      "Epoch [226/250], Training Loss: 268.32804160472955, Validation Loss: 20393.912109375\n",
      "Epoch [227/250], Training Loss: 25.54085996438923, Validation Loss: 21521.72265625\n",
      "Epoch [228/250], Training Loss: 239.58761812378214, Validation Loss: 19521.6484375\n",
      "Epoch [229/250], Training Loss: 95.07445628102171, Validation Loss: 21347.896484375\n",
      "Epoch [230/250], Training Loss: 24.332821495190245, Validation Loss: 20890.408203125\n",
      "Epoch [231/250], Training Loss: 99.20799815178195, Validation Loss: 20403.5390625\n",
      "Epoch [232/250], Training Loss: 29.163307749234345, Validation Loss: 20548.9140625\n",
      "Epoch [233/250], Training Loss: 26.89775279299523, Validation Loss: 21211.494140625\n",
      "Epoch [234/250], Training Loss: 26.341222199835308, Validation Loss: 20765.466796875\n",
      "Epoch [235/250], Training Loss: 72.47855365277631, Validation Loss: 20389.849609375\n",
      "Epoch [236/250], Training Loss: 26.05359279602067, Validation Loss: 20957.8359375\n",
      "Epoch [237/250], Training Loss: 45.39109710436232, Validation Loss: 19574.234375\n",
      "Epoch [238/250], Training Loss: 50.13815377780457, Validation Loss: 21217.931640625\n",
      "Epoch [239/250], Training Loss: 22.63573690320743, Validation Loss: 21404.6171875\n",
      "Epoch [240/250], Training Loss: 25.661950214802758, Validation Loss: 21029.564453125\n",
      "Epoch [241/250], Training Loss: 27.114772688180413, Validation Loss: 20374.021484375\n",
      "Epoch [242/250], Training Loss: 25.695107550498697, Validation Loss: 20795.328125\n",
      "Epoch [243/250], Training Loss: 92.80334384599514, Validation Loss: 20763.390625\n",
      "Epoch [244/250], Training Loss: 26.55057447940433, Validation Loss: 20887.373046875\n",
      "Epoch [245/250], Training Loss: 25.143101520108676, Validation Loss: 21221.267578125\n",
      "Epoch [246/250], Training Loss: 107.56653387386142, Validation Loss: 20513.056640625\n",
      "Epoch [247/250], Training Loss: 25.802425477510067, Validation Loss: 20872.498046875\n",
      "Epoch [248/250], Training Loss: 34.331271785801874, Validation Loss: 20818.548828125\n",
      "Epoch [249/250], Training Loss: 25.99016453433102, Validation Loss: 20879.265625\n",
      "Epoch [250/250], Training Loss: 93.54385157966102, Validation Loss: 21013.07421875\n",
      "Test Loss: 18637.21875\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.005, 15)\n",
      "Epoch [1/250], Training Loss: 19073.470641475873, Validation Loss: 18370.37109375\n",
      "Epoch [2/250], Training Loss: 13835.777094148187, Validation Loss: 15324.892578125\n",
      "Epoch [3/250], Training Loss: 10675.876319424373, Validation Loss: 12758.3662109375\n",
      "Epoch [4/250], Training Loss: 8571.265042084675, Validation Loss: 11228.708984375\n",
      "Epoch [5/250], Training Loss: 6976.74865793304, Validation Loss: 11788.0185546875\n",
      "Epoch [6/250], Training Loss: 5692.319172900845, Validation Loss: 10735.990234375\n",
      "Epoch [7/250], Training Loss: 4653.042323788815, Validation Loss: 9895.130859375\n",
      "Epoch [8/250], Training Loss: 3832.732612181802, Validation Loss: 9309.6630859375\n",
      "Epoch [9/250], Training Loss: 3125.472812457739, Validation Loss: 8564.09765625\n",
      "Epoch [10/250], Training Loss: 2569.854056813012, Validation Loss: 8036.49365234375\n",
      "Epoch [11/250], Training Loss: 2152.232023237658, Validation Loss: 7695.98486328125\n",
      "Epoch [12/250], Training Loss: 1829.4174965874918, Validation Loss: 7536.33642578125\n",
      "Epoch [13/250], Training Loss: 1554.4547003525286, Validation Loss: 7138.29150390625\n",
      "Epoch [14/250], Training Loss: 1339.3609703315979, Validation Loss: 6700.701171875\n",
      "Epoch [15/250], Training Loss: 1170.4283735335227, Validation Loss: 6241.96923828125\n",
      "Epoch [16/250], Training Loss: 1031.3868260386705, Validation Loss: 5889.8056640625\n",
      "Epoch [17/250], Training Loss: 900.2324208822903, Validation Loss: 5459.52197265625\n",
      "Epoch [18/250], Training Loss: 787.2392074558901, Validation Loss: 5113.486328125\n",
      "Epoch [19/250], Training Loss: 684.2942598444008, Validation Loss: 4795.74609375\n",
      "Epoch [20/250], Training Loss: 594.473248723285, Validation Loss: 4803.87109375\n",
      "Epoch [21/250], Training Loss: 507.69711094800886, Validation Loss: 4873.29736328125\n",
      "Epoch [22/250], Training Loss: 440.08504283469006, Validation Loss: 5160.0908203125\n",
      "Epoch [23/250], Training Loss: 384.1822735858271, Validation Loss: 5758.25341796875\n",
      "Epoch [24/250], Training Loss: 341.52648054230895, Validation Loss: 6544.4169921875\n",
      "Epoch [25/250], Training Loss: 301.04356524210573, Validation Loss: 7606.96728515625\n",
      "Epoch [26/250], Training Loss: 263.8765207724562, Validation Loss: 8175.326171875\n",
      "Epoch [27/250], Training Loss: 235.4059182350202, Validation Loss: 8530.70703125\n",
      "Epoch [28/250], Training Loss: 217.8391736586211, Validation Loss: 9096.8076171875\n",
      "Epoch [29/250], Training Loss: 189.25525409065793, Validation Loss: 9284.5673828125\n",
      "Epoch [30/250], Training Loss: 172.38165405975025, Validation Loss: 9603.0048828125\n",
      "Epoch [31/250], Training Loss: 154.33856118258433, Validation Loss: 9468.0751953125\n",
      "Epoch [32/250], Training Loss: 138.44144220913992, Validation Loss: 9686.650390625\n",
      "Epoch [33/250], Training Loss: 122.67184349284184, Validation Loss: 9940.80078125\n",
      "Epoch [34/250], Training Loss: 111.39444862896711, Validation Loss: 10560.7607421875\n",
      "Epoch [35/250], Training Loss: 100.97434016466357, Validation Loss: 10509.6416015625\n",
      "Epoch [36/250], Training Loss: 93.54057465712029, Validation Loss: 10572.23046875\n",
      "Epoch [37/250], Training Loss: 88.37612738920963, Validation Loss: 9840.6767578125\n",
      "Epoch [38/250], Training Loss: 82.74949825278257, Validation Loss: 10352.1982421875\n",
      "Epoch [39/250], Training Loss: 79.49791348322628, Validation Loss: 10760.7734375\n",
      "Epoch [40/250], Training Loss: 77.70724618125895, Validation Loss: 10722.3359375\n",
      "Epoch [41/250], Training Loss: 83.95757745767253, Validation Loss: 14318.3818359375\n",
      "Epoch [42/250], Training Loss: 76.03913011229585, Validation Loss: 12319.8740234375\n",
      "Epoch [43/250], Training Loss: 73.99083728628202, Validation Loss: 13542.0810546875\n",
      "Epoch [44/250], Training Loss: 72.48762856614587, Validation Loss: 13549.0419921875\n",
      "Epoch [45/250], Training Loss: 73.29735419390484, Validation Loss: 12749.1640625\n",
      "Epoch [46/250], Training Loss: 66.95030586452134, Validation Loss: 12765.20703125\n",
      "Epoch [47/250], Training Loss: 64.43600026578125, Validation Loss: 12270.4033203125\n",
      "Epoch [48/250], Training Loss: 60.763107385412916, Validation Loss: 11956.1162109375\n",
      "Epoch [49/250], Training Loss: 59.46950561440415, Validation Loss: 11972.5947265625\n",
      "Epoch [50/250], Training Loss: 65.04410000133257, Validation Loss: 13130.9052734375\n",
      "Epoch [51/250], Training Loss: 58.84695231838752, Validation Loss: 13006.9599609375\n",
      "Epoch [52/250], Training Loss: 55.74402406727228, Validation Loss: 13565.451171875\n",
      "Epoch [53/250], Training Loss: 52.7557543282185, Validation Loss: 12631.78515625\n",
      "Epoch [54/250], Training Loss: 53.188540254403584, Validation Loss: 13356.755859375\n",
      "Epoch [55/250], Training Loss: 51.27077560572094, Validation Loss: 12568.201171875\n",
      "Epoch [56/250], Training Loss: 50.350484958377656, Validation Loss: 12568.8564453125\n",
      "Epoch [57/250], Training Loss: 48.42731897487188, Validation Loss: 11904.93359375\n",
      "Epoch [58/250], Training Loss: 47.28886571622657, Validation Loss: 11333.01953125\n",
      "Epoch [59/250], Training Loss: 51.74118542315786, Validation Loss: 12973.650390625\n",
      "Epoch [60/250], Training Loss: 45.78145684822859, Validation Loss: 12077.5029296875\n",
      "Epoch [61/250], Training Loss: 42.297277635903306, Validation Loss: 12425.0888671875\n",
      "Epoch [62/250], Training Loss: 43.10289513262285, Validation Loss: 12748.59375\n",
      "Epoch [63/250], Training Loss: 40.45932939003607, Validation Loss: 12837.9560546875\n",
      "Epoch [64/250], Training Loss: 37.10219762994836, Validation Loss: 13202.630859375\n",
      "Epoch [65/250], Training Loss: 34.221357557246776, Validation Loss: 13384.4853515625\n",
      "Epoch [66/250], Training Loss: 46.07836889117973, Validation Loss: 13915.478515625\n",
      "Epoch [67/250], Training Loss: 41.29805152273364, Validation Loss: 14292.6484375\n",
      "Epoch [68/250], Training Loss: 36.46979104971371, Validation Loss: 14439.2919921875\n",
      "Epoch [69/250], Training Loss: 45.948533643298596, Validation Loss: 15292.73046875\n",
      "Epoch [70/250], Training Loss: 42.040401940405715, Validation Loss: 15358.841796875\n",
      "Epoch [71/250], Training Loss: 63.40982542921654, Validation Loss: 15902.0068359375\n",
      "Epoch [72/250], Training Loss: 46.072168960521225, Validation Loss: 16161.5859375\n",
      "Epoch [73/250], Training Loss: 58.22160190851669, Validation Loss: 16302.12890625\n",
      "Epoch [74/250], Training Loss: 45.808769925343874, Validation Loss: 16115.5888671875\n",
      "Epoch [75/250], Training Loss: 43.92494732125375, Validation Loss: 15988.8037109375\n",
      "Epoch [76/250], Training Loss: 40.694055260444784, Validation Loss: 16257.650390625\n",
      "Epoch [77/250], Training Loss: 49.91909082469097, Validation Loss: 15912.5283203125\n",
      "Epoch [78/250], Training Loss: 42.82552307606323, Validation Loss: 16293.845703125\n",
      "Epoch [79/250], Training Loss: 53.10400573934919, Validation Loss: 15862.6025390625\n",
      "Epoch [80/250], Training Loss: 46.108594517328164, Validation Loss: 15818.3427734375\n",
      "Epoch [81/250], Training Loss: 52.31262055971155, Validation Loss: 16132.8203125\n",
      "Epoch [82/250], Training Loss: 62.511841297574016, Validation Loss: 16324.787109375\n",
      "Epoch [83/250], Training Loss: 110.8688669362494, Validation Loss: 16649.21875\n",
      "Epoch [84/250], Training Loss: 139.75503301544578, Validation Loss: 16728.890625\n",
      "Epoch [85/250], Training Loss: 113.45048099764362, Validation Loss: 17092.541015625\n",
      "Epoch [86/250], Training Loss: 103.35962395203981, Validation Loss: 17383.439453125\n",
      "Epoch [87/250], Training Loss: 91.40443720282944, Validation Loss: 17348.189453125\n",
      "Epoch [88/250], Training Loss: 85.2533709024975, Validation Loss: 17335.568359375\n",
      "Epoch [89/250], Training Loss: 84.00319051175757, Validation Loss: 17221.6328125\n",
      "Epoch [90/250], Training Loss: 83.22094674658892, Validation Loss: 17306.48046875\n",
      "Epoch [91/250], Training Loss: 84.27027021507175, Validation Loss: 17048.888671875\n",
      "Epoch [92/250], Training Loss: 77.93714408283554, Validation Loss: 16840.904296875\n",
      "Epoch [93/250], Training Loss: 72.47341981216479, Validation Loss: 16608.03125\n",
      "Epoch [94/250], Training Loss: 72.9018596758094, Validation Loss: 16666.42578125\n",
      "Epoch [95/250], Training Loss: 53.51585338630159, Validation Loss: 15971.353515625\n",
      "Epoch [96/250], Training Loss: 47.50286095459406, Validation Loss: 16077.9765625\n",
      "Epoch [97/250], Training Loss: 56.56450230522756, Validation Loss: 16477.5859375\n",
      "Epoch [98/250], Training Loss: 78.84868082171548, Validation Loss: 16934.55078125\n",
      "Epoch [99/250], Training Loss: 43.71665679869544, Validation Loss: 16179.6484375\n",
      "Epoch [100/250], Training Loss: 37.36545754323377, Validation Loss: 16160.716796875\n",
      "Epoch [101/250], Training Loss: 43.7671414100551, Validation Loss: 16667.78125\n",
      "Epoch [102/250], Training Loss: 58.60278152319584, Validation Loss: 17032.78125\n",
      "Epoch [103/250], Training Loss: 53.121268547748016, Validation Loss: 17234.6015625\n",
      "Epoch [104/250], Training Loss: 55.8585772182963, Validation Loss: 17567.5\n",
      "Epoch [105/250], Training Loss: 51.77501713967688, Validation Loss: 17824.82421875\n",
      "Epoch [106/250], Training Loss: 53.10665217421595, Validation Loss: 18154.095703125\n",
      "Epoch [107/250], Training Loss: 50.22468466177745, Validation Loss: 18397.220703125\n",
      "Epoch [108/250], Training Loss: 50.89374753473555, Validation Loss: 18512.138671875\n",
      "Epoch [109/250], Training Loss: 48.721389634230576, Validation Loss: 18634.685546875\n",
      "Epoch [110/250], Training Loss: 49.89898161180244, Validation Loss: 18677.521484375\n",
      "Epoch [111/250], Training Loss: 46.71332280518397, Validation Loss: 18790.14453125\n",
      "Epoch [112/250], Training Loss: 50.12097022375769, Validation Loss: 18761.712890625\n",
      "Epoch [113/250], Training Loss: 44.970554033744094, Validation Loss: 18862.11328125\n",
      "Epoch [114/250], Training Loss: 49.13654362237083, Validation Loss: 18691.49609375\n",
      "Epoch [115/250], Training Loss: 44.89708204810488, Validation Loss: 18890.041015625\n",
      "Epoch [116/250], Training Loss: 47.69350512162695, Validation Loss: 18750.568359375\n",
      "Epoch [117/250], Training Loss: 46.968941486169484, Validation Loss: 18763.619140625\n",
      "Epoch [118/250], Training Loss: 47.60213167004528, Validation Loss: 18763.75390625\n",
      "Epoch [119/250], Training Loss: 45.633397678021616, Validation Loss: 18566.009765625\n",
      "Epoch [120/250], Training Loss: 43.57317356064977, Validation Loss: 18429.470703125\n",
      "Epoch [121/250], Training Loss: 41.737198153223616, Validation Loss: 18423.48046875\n",
      "Epoch [122/250], Training Loss: 40.36951957543986, Validation Loss: 18441.66015625\n",
      "Epoch [123/250], Training Loss: 39.81354000982836, Validation Loss: 18495.978515625\n",
      "Epoch [124/250], Training Loss: 39.12335190273499, Validation Loss: 18372.501953125\n",
      "Epoch [125/250], Training Loss: 38.931352797301564, Validation Loss: 18229.328125\n",
      "Epoch [126/250], Training Loss: 39.64673503971995, Validation Loss: 18258.748046875\n",
      "Epoch [127/250], Training Loss: 39.97408632227938, Validation Loss: 18195.802734375\n",
      "Epoch [128/250], Training Loss: 37.7179183340682, Validation Loss: 17859.080078125\n",
      "Epoch [129/250], Training Loss: 35.600864566998816, Validation Loss: 17806.478515625\n",
      "Epoch [130/250], Training Loss: 35.21356096182821, Validation Loss: 17783.755859375\n",
      "Epoch [131/250], Training Loss: 35.31503823827226, Validation Loss: 17711.9609375\n",
      "Epoch [132/250], Training Loss: 35.04335952161272, Validation Loss: 17619.005859375\n",
      "Epoch [133/250], Training Loss: 34.69761355179961, Validation Loss: 17560.0859375\n",
      "Epoch [134/250], Training Loss: 34.39597608751999, Validation Loss: 17533.095703125\n",
      "Epoch [135/250], Training Loss: 34.15507070597843, Validation Loss: 17536.21875\n",
      "Epoch [136/250], Training Loss: 33.96970763833126, Validation Loss: 17559.27734375\n",
      "Epoch [137/250], Training Loss: 33.77083206467883, Validation Loss: 17596.677734375\n",
      "Epoch [138/250], Training Loss: 33.52582223563475, Validation Loss: 17647.091796875\n",
      "Epoch [139/250], Training Loss: 33.24569008065427, Validation Loss: 17710.62890625\n",
      "Epoch [140/250], Training Loss: 32.96992474426964, Validation Loss: 17784.5078125\n",
      "Epoch [141/250], Training Loss: 32.73851099324352, Validation Loss: 17860.390625\n",
      "Epoch [142/250], Training Loss: 32.58273605831788, Validation Loss: 17927.458984375\n",
      "Epoch [143/250], Training Loss: 32.504469057070246, Validation Loss: 17981.029296875\n",
      "Epoch [144/250], Training Loss: 32.470405256782506, Validation Loss: 18023.44140625\n",
      "Epoch [145/250], Training Loss: 32.450227425677845, Validation Loss: 18052.595703125\n",
      "Epoch [146/250], Training Loss: 32.449822666079754, Validation Loss: 18067.123046875\n",
      "Epoch [147/250], Training Loss: 32.475335254901694, Validation Loss: 18068.283203125\n",
      "Epoch [148/250], Training Loss: 32.5283668293322, Validation Loss: 18057.712890625\n",
      "Epoch [149/250], Training Loss: 32.59934812228315, Validation Loss: 18044.16796875\n",
      "Epoch [150/250], Training Loss: 32.68592951718846, Validation Loss: 18029.955078125\n",
      "Epoch [151/250], Training Loss: 32.76207143755015, Validation Loss: 18043.896484375\n",
      "Epoch [152/250], Training Loss: 32.86413460696555, Validation Loss: 18043.76953125\n",
      "Epoch [153/250], Training Loss: 32.88406225218844, Validation Loss: 18181.169921875\n",
      "Epoch [154/250], Training Loss: 33.10486304709973, Validation Loss: 18032.09765625\n",
      "Epoch [155/250], Training Loss: 34.38725878020806, Validation Loss: 17952.943359375\n",
      "Epoch [156/250], Training Loss: 35.517440318195106, Validation Loss: 18260.802734375\n",
      "Epoch [157/250], Training Loss: 33.447936607395725, Validation Loss: 18350.349609375\n",
      "Epoch [158/250], Training Loss: 33.923916897830246, Validation Loss: 18392.880859375\n",
      "Epoch [159/250], Training Loss: 42.657017249178935, Validation Loss: 18662.07421875\n",
      "Epoch [160/250], Training Loss: 33.18029548756989, Validation Loss: 18268.5078125\n",
      "Epoch [161/250], Training Loss: 33.744808540548114, Validation Loss: 18401.33984375\n",
      "Epoch [162/250], Training Loss: 32.07154473618444, Validation Loss: 18527.544921875\n",
      "Epoch [163/250], Training Loss: 36.6278533257949, Validation Loss: 18601.451171875\n",
      "Epoch [164/250], Training Loss: 34.86511002880811, Validation Loss: 18550.78125\n",
      "Epoch [165/250], Training Loss: 35.178592350965836, Validation Loss: 18380.314453125\n",
      "Epoch [166/250], Training Loss: 48.73008525361303, Validation Loss: 17761.703125\n",
      "Epoch [167/250], Training Loss: 35.345959009812084, Validation Loss: 17704.904296875\n",
      "Epoch [168/250], Training Loss: 33.77210729554426, Validation Loss: 17814.79296875\n",
      "Epoch [169/250], Training Loss: 33.19807890500756, Validation Loss: 18078.884765625\n",
      "Epoch [170/250], Training Loss: 33.52382952547432, Validation Loss: 18171.505859375\n",
      "Epoch [171/250], Training Loss: 33.20068291944061, Validation Loss: 18223.890625\n",
      "Epoch [172/250], Training Loss: 33.63651351843864, Validation Loss: 18173.140625\n",
      "Epoch [173/250], Training Loss: 31.694112633740925, Validation Loss: 18292.47265625\n",
      "Epoch [174/250], Training Loss: 36.07340973815974, Validation Loss: 18237.78515625\n",
      "Epoch [175/250], Training Loss: 32.95397298254016, Validation Loss: 18274.943359375\n",
      "Epoch [176/250], Training Loss: 32.54050827029674, Validation Loss: 18317.8203125\n",
      "Epoch [177/250], Training Loss: 33.94331099009554, Validation Loss: 18207.439453125\n",
      "Epoch [178/250], Training Loss: 32.68312246203171, Validation Loss: 18217.041015625\n",
      "Epoch [179/250], Training Loss: 32.378218831044904, Validation Loss: 18459.732421875\n",
      "Epoch [180/250], Training Loss: 34.67495173865372, Validation Loss: 18266.53125\n",
      "Epoch [181/250], Training Loss: 34.51300548196288, Validation Loss: 18216.87109375\n",
      "Epoch [182/250], Training Loss: 33.55014349900271, Validation Loss: 18458.8671875\n",
      "Epoch [183/250], Training Loss: 32.89205770175516, Validation Loss: 18580.248046875\n",
      "Epoch [184/250], Training Loss: 33.478316539650095, Validation Loss: 18587.87890625\n",
      "Epoch [185/250], Training Loss: 31.298460844655764, Validation Loss: 18560.470703125\n",
      "Epoch [186/250], Training Loss: 32.03536558651393, Validation Loss: 18687.970703125\n",
      "Epoch [187/250], Training Loss: 35.078459252091626, Validation Loss: 18494.8828125\n",
      "Epoch [188/250], Training Loss: 33.75845119556808, Validation Loss: 18523.14453125\n",
      "Epoch [189/250], Training Loss: 31.48681568387421, Validation Loss: 18768.544921875\n",
      "Epoch [190/250], Training Loss: 35.804220585179856, Validation Loss: 18605.427734375\n",
      "Epoch [191/250], Training Loss: 34.46559003084587, Validation Loss: 18550.873046875\n",
      "Epoch [192/250], Training Loss: 38.024371651807066, Validation Loss: 18528.724609375\n",
      "Epoch [193/250], Training Loss: 36.589665915171764, Validation Loss: 18422.1015625\n",
      "Epoch [194/250], Training Loss: 30.764439282076886, Validation Loss: 18672.537109375\n",
      "Epoch [195/250], Training Loss: 28.9536130442135, Validation Loss: 18666.443359375\n",
      "Epoch [196/250], Training Loss: 29.404419346000815, Validation Loss: 18736.861328125\n",
      "Epoch [197/250], Training Loss: 30.038095838197556, Validation Loss: 18601.3359375\n",
      "Epoch [198/250], Training Loss: 32.355431935204564, Validation Loss: 18671.94921875\n",
      "Epoch [199/250], Training Loss: 31.292011735265596, Validation Loss: 18537.248046875\n",
      "Epoch [200/250], Training Loss: 33.765370071917346, Validation Loss: 18566.775390625\n",
      "Epoch [201/250], Training Loss: 34.57492021828354, Validation Loss: 18547.03515625\n",
      "Epoch [202/250], Training Loss: 34.36042706977179, Validation Loss: 18507.771484375\n",
      "Epoch [203/250], Training Loss: 33.388830109592845, Validation Loss: 18675.8671875\n",
      "Epoch [204/250], Training Loss: 30.244241172326664, Validation Loss: 18531.1796875\n",
      "Epoch [205/250], Training Loss: 33.01519415234375, Validation Loss: 18655.70703125\n",
      "Epoch [206/250], Training Loss: 31.19712422277625, Validation Loss: 18689.306640625\n",
      "Epoch [207/250], Training Loss: 35.582408237836376, Validation Loss: 18499.404296875\n",
      "Epoch [208/250], Training Loss: 32.5314536815573, Validation Loss: 18838.17578125\n",
      "Epoch [209/250], Training Loss: 30.595505328874143, Validation Loss: 18833.423828125\n",
      "Epoch [210/250], Training Loss: 31.8272663475526, Validation Loss: 18720.994140625\n",
      "Epoch [211/250], Training Loss: 58.45719735271864, Validation Loss: 18117.17578125\n",
      "Epoch [212/250], Training Loss: 42.19648064796888, Validation Loss: 18549.275390625\n",
      "Epoch [213/250], Training Loss: 27.617732771893987, Validation Loss: 18591.83984375\n",
      "Epoch [214/250], Training Loss: 28.28536483529274, Validation Loss: 18678.37109375\n",
      "Epoch [215/250], Training Loss: 33.44576401852877, Validation Loss: 18545.33203125\n",
      "Epoch [216/250], Training Loss: 31.791825481251642, Validation Loss: 18545.78125\n",
      "Epoch [217/250], Training Loss: 30.66359360816342, Validation Loss: 18727.5546875\n",
      "Epoch [218/250], Training Loss: 32.19038386713748, Validation Loss: 18455.837890625\n",
      "Epoch [219/250], Training Loss: 33.94818583623345, Validation Loss: 18548.951171875\n",
      "Epoch [220/250], Training Loss: 32.114568213470235, Validation Loss: 18656.86328125\n",
      "Epoch [221/250], Training Loss: 32.556307373118315, Validation Loss: 18764.396484375\n",
      "Epoch [222/250], Training Loss: 33.41153663653185, Validation Loss: 18666.291015625\n",
      "Epoch [223/250], Training Loss: 33.94698496524442, Validation Loss: 18521.1015625\n",
      "Epoch [224/250], Training Loss: 33.53388077736216, Validation Loss: 18522.966796875\n",
      "Epoch [225/250], Training Loss: 30.433977911725023, Validation Loss: 18856.189453125\n",
      "Epoch [226/250], Training Loss: 35.62388865782775, Validation Loss: 18385.6484375\n",
      "Epoch [227/250], Training Loss: 36.527861109379195, Validation Loss: 18620.4375\n",
      "Epoch [228/250], Training Loss: 34.653937509540555, Validation Loss: 18431.27734375\n",
      "Epoch [229/250], Training Loss: 39.57707852147378, Validation Loss: 18415.6953125\n",
      "Epoch [230/250], Training Loss: 35.3059897704235, Validation Loss: 18353.0859375\n",
      "Epoch [231/250], Training Loss: 33.47649531049503, Validation Loss: 18546.30078125\n",
      "Epoch [232/250], Training Loss: 34.61271154534081, Validation Loss: 18112.669921875\n",
      "Epoch [233/250], Training Loss: 37.295842383310315, Validation Loss: 18204.45703125\n",
      "Epoch [234/250], Training Loss: 37.64718830934576, Validation Loss: 18333.16796875\n",
      "Epoch [235/250], Training Loss: 36.10663895537727, Validation Loss: 18489.927734375\n",
      "Epoch [236/250], Training Loss: 30.186702912980266, Validation Loss: 18478.361328125\n",
      "Epoch [237/250], Training Loss: 31.892071960970387, Validation Loss: 17836.1015625\n",
      "Epoch [238/250], Training Loss: 38.17888548272203, Validation Loss: 18143.181640625\n",
      "Epoch [239/250], Training Loss: 35.3267106864344, Validation Loss: 18142.537109375\n",
      "Epoch [240/250], Training Loss: 34.58781342754068, Validation Loss: 16806.263671875\n",
      "Epoch [241/250], Training Loss: 30.61571958198104, Validation Loss: 17211.068359375\n",
      "Epoch [242/250], Training Loss: 33.103569029893585, Validation Loss: 17579.669921875\n",
      "Epoch [243/250], Training Loss: 35.695386096806416, Validation Loss: 17696.90625\n",
      "Epoch [244/250], Training Loss: 34.65274380839393, Validation Loss: 18095.099609375\n",
      "Epoch [245/250], Training Loss: 33.184959764194105, Validation Loss: 17974.400390625\n",
      "Epoch [246/250], Training Loss: 29.710415263571033, Validation Loss: 17946.90234375\n",
      "Epoch [247/250], Training Loss: 35.2563497252183, Validation Loss: 17798.69921875\n",
      "Epoch [248/250], Training Loss: 34.69579952244182, Validation Loss: 18216.662109375\n",
      "Epoch [249/250], Training Loss: 33.10532819770896, Validation Loss: 18383.12109375\n",
      "Epoch [250/250], Training Loss: 47.08378065437529, Validation Loss: 17783.232421875\n",
      "Test Loss: 15919.548828125\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.005, 20)\n",
      "Epoch [1/250], Training Loss: 19099.529805664646, Validation Loss: 18207.798828125\n",
      "Epoch [2/250], Training Loss: 14496.608566215984, Validation Loss: 15562.720703125\n",
      "Epoch [3/250], Training Loss: 12326.319278095152, Validation Loss: 11802.001953125\n",
      "Epoch [4/250], Training Loss: 10942.614359453268, Validation Loss: 10528.9072265625\n",
      "Epoch [5/250], Training Loss: 9903.082826554597, Validation Loss: 9645.1015625\n",
      "Epoch [6/250], Training Loss: 8954.989785866099, Validation Loss: 10373.1572265625\n",
      "Epoch [7/250], Training Loss: 8047.626064800169, Validation Loss: 9254.14453125\n",
      "Epoch [8/250], Training Loss: 7403.266504156339, Validation Loss: 8257.466796875\n",
      "Epoch [9/250], Training Loss: 6836.84445320487, Validation Loss: 7456.40576171875\n",
      "Epoch [10/250], Training Loss: 6270.905771081802, Validation Loss: 6797.8251953125\n",
      "Epoch [11/250], Training Loss: 5716.138503341309, Validation Loss: 6218.75537109375\n",
      "Epoch [12/250], Training Loss: 5275.836597574032, Validation Loss: 5871.0205078125\n",
      "Epoch [13/250], Training Loss: 4806.2674823923835, Validation Loss: 5536.41015625\n",
      "Epoch [14/250], Training Loss: 4420.776773067055, Validation Loss: 5289.72509765625\n",
      "Epoch [15/250], Training Loss: 4122.796502869644, Validation Loss: 5105.8681640625\n",
      "Epoch [16/250], Training Loss: 3824.6039269099088, Validation Loss: 4927.17333984375\n",
      "Epoch [17/250], Training Loss: 3526.082850770557, Validation Loss: 4714.609375\n",
      "Epoch [18/250], Training Loss: 3224.64608709273, Validation Loss: 4277.73681640625\n",
      "Epoch [19/250], Training Loss: 2963.8942573715212, Validation Loss: 4079.64697265625\n",
      "Epoch [20/250], Training Loss: 2729.8290789036523, Validation Loss: 3930.649169921875\n",
      "Epoch [21/250], Training Loss: 2519.009173370059, Validation Loss: 3802.57470703125\n",
      "Epoch [22/250], Training Loss: 2329.227229436201, Validation Loss: 3686.7421875\n",
      "Epoch [23/250], Training Loss: 2158.598864440917, Validation Loss: 3579.657470703125\n",
      "Epoch [24/250], Training Loss: 6813.498438576941, Validation Loss: 4618.6865234375\n",
      "Epoch [25/250], Training Loss: 2146.413280443429, Validation Loss: 5216.10107421875\n",
      "Epoch [26/250], Training Loss: 1500.7345912461087, Validation Loss: 2900.695068359375\n",
      "Epoch [27/250], Training Loss: 1285.2166955358791, Validation Loss: 3427.24853515625\n",
      "Epoch [28/250], Training Loss: 1086.0022372673043, Validation Loss: 6554.01953125\n",
      "Epoch [29/250], Training Loss: 928.0274454608196, Validation Loss: 12662.9453125\n",
      "Epoch [30/250], Training Loss: 944.7601275923904, Validation Loss: 24678.833984375\n",
      "Epoch [31/250], Training Loss: 765.4522157831527, Validation Loss: 19584.943359375\n",
      "Epoch [32/250], Training Loss: 655.2995371395124, Validation Loss: 7611.7216796875\n",
      "Epoch [33/250], Training Loss: 585.9482755143465, Validation Loss: 10611.322265625\n",
      "Epoch [34/250], Training Loss: 530.6119761793169, Validation Loss: 19780.353515625\n",
      "Epoch [35/250], Training Loss: 527.8790236301184, Validation Loss: 28131.8515625\n",
      "Epoch [36/250], Training Loss: 462.62152676603904, Validation Loss: 10670.837890625\n",
      "Epoch [37/250], Training Loss: 369.5978026130771, Validation Loss: 21793.048828125\n",
      "Epoch [38/250], Training Loss: 334.31212572415694, Validation Loss: 12180.92578125\n",
      "Epoch [39/250], Training Loss: 291.0181627040345, Validation Loss: 27997.609375\n",
      "Epoch [40/250], Training Loss: 321.92428197306754, Validation Loss: 9279.271484375\n",
      "Epoch [41/250], Training Loss: 253.01049668146396, Validation Loss: 6574.94580078125\n",
      "Epoch [42/250], Training Loss: 228.65492726812622, Validation Loss: 10515.806640625\n",
      "Epoch [43/250], Training Loss: 173.30485776877205, Validation Loss: 9055.541015625\n",
      "Epoch [44/250], Training Loss: 170.4269172469385, Validation Loss: 10277.095703125\n",
      "Epoch [45/250], Training Loss: 161.5509370868127, Validation Loss: 9362.0830078125\n",
      "Epoch [46/250], Training Loss: 153.47066472766832, Validation Loss: 7501.5302734375\n",
      "Epoch [47/250], Training Loss: 145.0964145367683, Validation Loss: 7356.0166015625\n",
      "Epoch [48/250], Training Loss: 142.37479214582746, Validation Loss: 7347.47412109375\n",
      "Epoch [49/250], Training Loss: 130.73615985290348, Validation Loss: 6126.77978515625\n",
      "Epoch [50/250], Training Loss: 131.66690082571816, Validation Loss: 6544.13134765625\n",
      "Epoch [51/250], Training Loss: 124.01039788361604, Validation Loss: 5727.16357421875\n",
      "Epoch [52/250], Training Loss: 121.77185834186106, Validation Loss: 6008.1962890625\n",
      "Epoch [53/250], Training Loss: 114.11469658238559, Validation Loss: 5702.234375\n",
      "Epoch [54/250], Training Loss: 110.28156129249216, Validation Loss: 5690.98779296875\n",
      "Epoch [55/250], Training Loss: 108.3859168985025, Validation Loss: 5846.00341796875\n",
      "Epoch [56/250], Training Loss: 106.57776842127261, Validation Loss: 6551.3720703125\n",
      "Epoch [57/250], Training Loss: 108.6674136180348, Validation Loss: 6690.36279296875\n",
      "Epoch [58/250], Training Loss: 191.0237240283939, Validation Loss: 6285.9306640625\n",
      "Epoch [59/250], Training Loss: 110.23398737589774, Validation Loss: 5348.54638671875\n",
      "Epoch [60/250], Training Loss: 101.30452348341461, Validation Loss: 5991.65234375\n",
      "Epoch [61/250], Training Loss: 101.35979846940405, Validation Loss: 6189.17431640625\n",
      "Epoch [62/250], Training Loss: 98.39012857464738, Validation Loss: 6827.13623046875\n",
      "Epoch [63/250], Training Loss: 11693.57400968017, Validation Loss: 20455.333984375\n",
      "Epoch [64/250], Training Loss: 13964.43738335942, Validation Loss: 12242.2119140625\n",
      "Epoch [65/250], Training Loss: 676.4975804642152, Validation Loss: 8223.044921875\n",
      "Epoch [66/250], Training Loss: 286.4258467037441, Validation Loss: 10116.3955078125\n",
      "Epoch [67/250], Training Loss: 278.05420161476405, Validation Loss: 9371.7548828125\n",
      "Epoch [68/250], Training Loss: 252.29218260345073, Validation Loss: 8739.697265625\n",
      "Epoch [69/250], Training Loss: 224.62676944202303, Validation Loss: 11774.751953125\n",
      "Epoch [70/250], Training Loss: 213.35326922042358, Validation Loss: 9474.81640625\n",
      "Epoch [71/250], Training Loss: 216.05375868861833, Validation Loss: 11006.8876953125\n",
      "Epoch [72/250], Training Loss: 209.27671238046537, Validation Loss: 12909.10546875\n",
      "Epoch [73/250], Training Loss: 145.542263901655, Validation Loss: 8378.39453125\n",
      "Epoch [74/250], Training Loss: 107.52822715285839, Validation Loss: 10630.3125\n",
      "Epoch [75/250], Training Loss: 112.67841792251136, Validation Loss: 11497.04296875\n",
      "Epoch [76/250], Training Loss: 105.5100832659808, Validation Loss: 12373.5654296875\n",
      "Epoch [77/250], Training Loss: 103.32062963418124, Validation Loss: 11706.546875\n",
      "Epoch [78/250], Training Loss: 192.5309204141071, Validation Loss: 11917.50390625\n",
      "Epoch [79/250], Training Loss: 94.61163955550917, Validation Loss: 11649.4091796875\n",
      "Epoch [80/250], Training Loss: 96.41851086731089, Validation Loss: 14213.0712890625\n",
      "Epoch [81/250], Training Loss: 119.94368798388612, Validation Loss: 13963.8662109375\n",
      "Epoch [82/250], Training Loss: 119.9301769036168, Validation Loss: 14171.0068359375\n",
      "Epoch [83/250], Training Loss: 112.5809679599906, Validation Loss: 14774.30859375\n",
      "Epoch [84/250], Training Loss: 259.7207458252057, Validation Loss: 15129.8544921875\n",
      "Epoch [85/250], Training Loss: 98.90177089053869, Validation Loss: 14598.796875\n",
      "Epoch [86/250], Training Loss: 95.69705464913696, Validation Loss: 13519.607421875\n",
      "Epoch [87/250], Training Loss: 359.86494763510694, Validation Loss: 15015.841796875\n",
      "Epoch [88/250], Training Loss: 111.39894148212427, Validation Loss: 14790.6240234375\n",
      "Epoch [89/250], Training Loss: 60.16188718283459, Validation Loss: 13155.30078125\n",
      "Epoch [90/250], Training Loss: 356.89052712133923, Validation Loss: 13952.4404296875\n",
      "Epoch [91/250], Training Loss: 535.5030179294668, Validation Loss: 13158.7548828125\n",
      "Epoch [92/250], Training Loss: 106.14813962694387, Validation Loss: 13501.6572265625\n",
      "Epoch [93/250], Training Loss: 32.48879925948286, Validation Loss: 12655.5\n",
      "Epoch [94/250], Training Loss: 29.084590249682126, Validation Loss: 11580.482421875\n",
      "Epoch [95/250], Training Loss: 28.855114229076662, Validation Loss: 12847.703125\n",
      "Epoch [96/250], Training Loss: 117.88765910813082, Validation Loss: 12652.6611328125\n",
      "Epoch [97/250], Training Loss: 334.4512139440349, Validation Loss: 12054.720703125\n",
      "Epoch [98/250], Training Loss: 43.34133320595841, Validation Loss: 11563.865234375\n",
      "Epoch [99/250], Training Loss: 26.837618748490573, Validation Loss: 12946.29296875\n",
      "Epoch [100/250], Training Loss: 31.96077781044762, Validation Loss: 12138.6650390625\n",
      "Epoch [101/250], Training Loss: 23.52808059347631, Validation Loss: 12702.6240234375\n",
      "Epoch [102/250], Training Loss: 23.35137207009965, Validation Loss: 12657.0830078125\n",
      "Epoch [103/250], Training Loss: 22.46264150294701, Validation Loss: 12690.2548828125\n",
      "Epoch [104/250], Training Loss: 22.202689301302932, Validation Loss: 12543.708984375\n",
      "Epoch [105/250], Training Loss: 21.6978973481143, Validation Loss: 12309.673828125\n",
      "Epoch [106/250], Training Loss: 21.098044944618188, Validation Loss: 12060.6513671875\n",
      "Epoch [107/250], Training Loss: 20.60866798265735, Validation Loss: 11760.0146484375\n",
      "Epoch [108/250], Training Loss: 20.185082201666873, Validation Loss: 11428.1875\n",
      "Epoch [109/250], Training Loss: 19.80579581664296, Validation Loss: 11105.5302734375\n",
      "Epoch [110/250], Training Loss: 19.47484505255273, Validation Loss: 10821.0810546875\n",
      "Epoch [111/250], Training Loss: 19.19889131806817, Validation Loss: 10575.529296875\n",
      "Epoch [112/250], Training Loss: 19.073297864282647, Validation Loss: 10297.4619140625\n",
      "Epoch [113/250], Training Loss: 19.598958342147736, Validation Loss: 9535.0107421875\n",
      "Epoch [114/250], Training Loss: 19.679821916356346, Validation Loss: 7334.4736328125\n",
      "Epoch [115/250], Training Loss: 20.971369958947747, Validation Loss: 6349.7958984375\n",
      "Epoch [116/250], Training Loss: 18.767575295806925, Validation Loss: 5431.44873046875\n",
      "Epoch [117/250], Training Loss: 20.04279910121051, Validation Loss: 4877.0107421875\n",
      "Epoch [118/250], Training Loss: 20.544380504749718, Validation Loss: 4393.71142578125\n",
      "Epoch [119/250], Training Loss: 21.46259543170866, Validation Loss: 4015.770263671875\n",
      "Epoch [120/250], Training Loss: 21.0882250519592, Validation Loss: 3784.234130859375\n",
      "Epoch [121/250], Training Loss: 20.225095884470562, Validation Loss: 3781.408203125\n",
      "Epoch [122/250], Training Loss: 19.167802039292326, Validation Loss: 3826.23828125\n",
      "Epoch [123/250], Training Loss: 18.21435909774552, Validation Loss: 3831.462890625\n",
      "Epoch [124/250], Training Loss: 17.560674785612015, Validation Loss: 3772.556640625\n",
      "Epoch [125/250], Training Loss: 17.194884153522942, Validation Loss: 3774.9541015625\n",
      "Epoch [126/250], Training Loss: 16.85195843652807, Validation Loss: 3701.21826171875\n",
      "Epoch [127/250], Training Loss: 16.516706000184094, Validation Loss: 3610.02587890625\n",
      "Epoch [128/250], Training Loss: 16.11642521361171, Validation Loss: 3512.177490234375\n",
      "Epoch [129/250], Training Loss: 15.75420374422162, Validation Loss: 3425.31103515625\n",
      "Epoch [130/250], Training Loss: 15.304756977225184, Validation Loss: 3287.29052734375\n",
      "Epoch [131/250], Training Loss: 14.61266362456999, Validation Loss: 3132.451416015625\n",
      "Epoch [132/250], Training Loss: 14.300907945639212, Validation Loss: 3053.916015625\n",
      "Epoch [133/250], Training Loss: 13.951961719804773, Validation Loss: 3065.053466796875\n",
      "Epoch [134/250], Training Loss: 13.723558207289736, Validation Loss: 3006.358154296875\n",
      "Epoch [135/250], Training Loss: 13.099293739982222, Validation Loss: 3040.95556640625\n",
      "Epoch [136/250], Training Loss: 171.74365555909685, Validation Loss: 3876.28564453125\n",
      "Epoch [137/250], Training Loss: 92.26490343782955, Validation Loss: 3299.36767578125\n",
      "Epoch [138/250], Training Loss: 20.689409058311302, Validation Loss: 3595.219970703125\n",
      "Epoch [139/250], Training Loss: 15.955821301183171, Validation Loss: 2151.76953125\n",
      "Epoch [140/250], Training Loss: 15.895118550734091, Validation Loss: 2361.191162109375\n",
      "Epoch [141/250], Training Loss: 15.048227379603432, Validation Loss: 2198.236572265625\n",
      "Epoch [142/250], Training Loss: 15.120521676282934, Validation Loss: 2410.135986328125\n",
      "Epoch [143/250], Training Loss: 14.546315006202589, Validation Loss: 2383.261962890625\n",
      "Epoch [144/250], Training Loss: 14.084658785440983, Validation Loss: 2432.340087890625\n",
      "Epoch [145/250], Training Loss: 13.678340809450416, Validation Loss: 2447.42626953125\n",
      "Epoch [146/250], Training Loss: 16.3962765483225, Validation Loss: 3537.383056640625\n",
      "Epoch [147/250], Training Loss: 10.432866638578195, Validation Loss: 2127.296630859375\n",
      "Epoch [148/250], Training Loss: 11.969068611612352, Validation Loss: 2511.061279296875\n",
      "Epoch [149/250], Training Loss: 4060.246247548889, Validation Loss: 8259.4765625\n",
      "Epoch [150/250], Training Loss: 434.67160288969023, Validation Loss: 7067.92822265625\n",
      "Epoch [151/250], Training Loss: 4827.107117062183, Validation Loss: 7027.11572265625\n",
      "Epoch [152/250], Training Loss: 209.90060772282544, Validation Loss: 12515.4404296875\n",
      "Epoch [153/250], Training Loss: 15.002272345815724, Validation Loss: 8100.68994140625\n",
      "Epoch [154/250], Training Loss: 5.8188108803709495, Validation Loss: 2711.27685546875\n",
      "Epoch [155/250], Training Loss: 5.297381354326986, Validation Loss: 2975.324462890625\n",
      "Epoch [156/250], Training Loss: 5.133707293258115, Validation Loss: 3270.1640625\n",
      "Epoch [157/250], Training Loss: 4.832911920159643, Validation Loss: 3513.873291015625\n",
      "Epoch [158/250], Training Loss: 4.667924749829477, Validation Loss: 3721.61376953125\n",
      "Epoch [159/250], Training Loss: 4.860653529722693, Validation Loss: 3975.7294921875\n",
      "Epoch [160/250], Training Loss: 4.868296305280218, Validation Loss: 4258.24755859375\n",
      "Epoch [161/250], Training Loss: 4.979685802340276, Validation Loss: 4485.9306640625\n",
      "Epoch [162/250], Training Loss: 4.713283973550549, Validation Loss: 4768.580078125\n",
      "Epoch [163/250], Training Loss: 4.86106174473865, Validation Loss: 4628.99951171875\n",
      "Epoch [164/250], Training Loss: 4.985149748924997, Validation Loss: 4856.3505859375\n",
      "Epoch [165/250], Training Loss: 5.5141309025160545, Validation Loss: 5030.599609375\n",
      "Epoch [166/250], Training Loss: 6.435202815224842, Validation Loss: 4789.78857421875\n",
      "Epoch [167/250], Training Loss: 6.024128895736676, Validation Loss: 5821.09765625\n",
      "Epoch [168/250], Training Loss: 6.860635434809903, Validation Loss: 6650.8466796875\n",
      "Epoch [169/250], Training Loss: 6.510782625609507, Validation Loss: 6812.361328125\n",
      "Epoch [170/250], Training Loss: 6.46359830250022, Validation Loss: 5433.2421875\n",
      "Epoch [171/250], Training Loss: 6.267883748981317, Validation Loss: 4997.76904296875\n",
      "Epoch [172/250], Training Loss: 6.277515036036306, Validation Loss: 4801.49560546875\n",
      "Epoch [173/250], Training Loss: 6.4098375500873805, Validation Loss: 4814.62841796875\n",
      "Epoch [174/250], Training Loss: 6.84175673602421, Validation Loss: 5024.8642578125\n",
      "Epoch [175/250], Training Loss: 6.923331381525819, Validation Loss: 5455.267578125\n",
      "Epoch [176/250], Training Loss: 7.376531488850962, Validation Loss: 6007.6689453125\n",
      "Epoch [177/250], Training Loss: 7.141677952983104, Validation Loss: 5515.833984375\n",
      "Epoch [178/250], Training Loss: 7.137138182115795, Validation Loss: 5584.857421875\n",
      "Epoch [179/250], Training Loss: 7.777313147530487, Validation Loss: 6126.26611328125\n",
      "Epoch [180/250], Training Loss: 6.816437771012775, Validation Loss: 4734.75732421875\n",
      "Epoch [181/250], Training Loss: 7.141140303797696, Validation Loss: 4902.25390625\n",
      "Epoch [182/250], Training Loss: 7.200098304919001, Validation Loss: 5030.087890625\n",
      "Epoch [183/250], Training Loss: 7.148801495247504, Validation Loss: 4522.91552734375\n",
      "Epoch [184/250], Training Loss: 6.89315201841497, Validation Loss: 3979.24462890625\n",
      "Epoch [185/250], Training Loss: 7.1378519403293135, Validation Loss: 3946.032958984375\n",
      "Epoch [186/250], Training Loss: 7.02235468287086, Validation Loss: 3807.13671875\n",
      "Epoch [187/250], Training Loss: 6.658139496427599, Validation Loss: 3963.9130859375\n",
      "Epoch [188/250], Training Loss: 6.564323847414016, Validation Loss: 3876.788330078125\n",
      "Epoch [189/250], Training Loss: 6.523985120745593, Validation Loss: 3843.7744140625\n",
      "Epoch [190/250], Training Loss: 6.563282703667234, Validation Loss: 3833.48291015625\n",
      "Epoch [191/250], Training Loss: 6.272962204901042, Validation Loss: 3807.296630859375\n",
      "Epoch [192/250], Training Loss: 6.184027401190344, Validation Loss: 3820.6416015625\n",
      "Epoch [193/250], Training Loss: 6.256839598387861, Validation Loss: 3930.5341796875\n",
      "Epoch [194/250], Training Loss: 6.3544446441399165, Validation Loss: 4079.913330078125\n",
      "Epoch [195/250], Training Loss: 6.118520152080441, Validation Loss: 4125.9443359375\n",
      "Epoch [196/250], Training Loss: 6.037104047573337, Validation Loss: 4172.220703125\n",
      "Epoch [197/250], Training Loss: 5.9792879328612925, Validation Loss: 4216.58447265625\n",
      "Epoch [198/250], Training Loss: 6.115080222809587, Validation Loss: 4434.2392578125\n",
      "Epoch [199/250], Training Loss: 5.8200019390484075, Validation Loss: 4488.26806640625\n",
      "Epoch [200/250], Training Loss: 5.832244824714548, Validation Loss: 4542.24951171875\n",
      "Epoch [201/250], Training Loss: 5.759623975966527, Validation Loss: 4579.724609375\n",
      "Epoch [202/250], Training Loss: 5.688358978539774, Validation Loss: 4609.81689453125\n",
      "Epoch [203/250], Training Loss: 5.6023904005121405, Validation Loss: 4590.005859375\n",
      "Epoch [204/250], Training Loss: 5.601761226607954, Validation Loss: 4538.79150390625\n",
      "Epoch [205/250], Training Loss: 5.468415457847984, Validation Loss: 4183.9677734375\n",
      "Epoch [206/250], Training Loss: 5.4296850904340985, Validation Loss: 4012.064208984375\n",
      "Epoch [207/250], Training Loss: 5.416495389704114, Validation Loss: 3857.4794921875\n",
      "Epoch [208/250], Training Loss: 5.395454600055544, Validation Loss: 3691.852294921875\n",
      "Epoch [209/250], Training Loss: 5.3236929234698716, Validation Loss: 3488.051025390625\n",
      "Epoch [210/250], Training Loss: 5.238794902059489, Validation Loss: 3423.068603515625\n",
      "Epoch [211/250], Training Loss: 5.432184387777884, Validation Loss: 3990.283935546875\n",
      "Epoch [212/250], Training Loss: 5.30897673866513, Validation Loss: 3490.228759765625\n",
      "Epoch [213/250], Training Loss: 5.258359756223953, Validation Loss: 3605.6826171875\n",
      "Epoch [214/250], Training Loss: 5.566818216641617, Validation Loss: 3932.87744140625\n",
      "Epoch [215/250], Training Loss: 5.711822805604801, Validation Loss: 4550.970703125\n",
      "Epoch [216/250], Training Loss: 5.608434814773918, Validation Loss: 4695.85400390625\n",
      "Epoch [217/250], Training Loss: 5.045479275954092, Validation Loss: 4465.5478515625\n",
      "Epoch [218/250], Training Loss: 5.373675669735539, Validation Loss: 4383.36572265625\n",
      "Epoch [219/250], Training Loss: 5.5406582661400705, Validation Loss: 4465.376953125\n",
      "Epoch [220/250], Training Loss: 5.596323783231479, Validation Loss: 4476.8505859375\n",
      "Epoch [221/250], Training Loss: 5.813249686498162, Validation Loss: 4773.22119140625\n",
      "Epoch [222/250], Training Loss: 5.82804532559211, Validation Loss: 5036.31640625\n",
      "Epoch [223/250], Training Loss: 5.21010978824427, Validation Loss: 4780.3583984375\n",
      "Epoch [224/250], Training Loss: 5.405002162486878, Validation Loss: 4929.53125\n",
      "Epoch [225/250], Training Loss: 6.072409945767736, Validation Loss: 4976.03759765625\n",
      "Epoch [226/250], Training Loss: 5.835292338398925, Validation Loss: 4837.32568359375\n",
      "Epoch [227/250], Training Loss: 5.985727645493221, Validation Loss: 5413.3720703125\n",
      "Epoch [228/250], Training Loss: 6.074300578688451, Validation Loss: 5436.35595703125\n",
      "Epoch [229/250], Training Loss: 6.126157556610948, Validation Loss: 5304.751953125\n",
      "Epoch [230/250], Training Loss: 5.88435527401927, Validation Loss: 5424.49365234375\n",
      "Epoch [231/250], Training Loss: 5.700846879679983, Validation Loss: 4734.435546875\n",
      "Epoch [232/250], Training Loss: 4.963390513498498, Validation Loss: 4721.34619140625\n",
      "Epoch [233/250], Training Loss: 6.320425498690414, Validation Loss: 5438.203125\n",
      "Epoch [234/250], Training Loss: 5.745432105795724, Validation Loss: 4525.96875\n",
      "Epoch [235/250], Training Loss: 7.19644636047978, Validation Loss: 6103.40478515625\n",
      "Epoch [236/250], Training Loss: 4.99544844368351, Validation Loss: 4204.47412109375\n",
      "Epoch [237/250], Training Loss: 6.005368877558053, Validation Loss: 5027.10888671875\n",
      "Epoch [238/250], Training Loss: 5.76578403603666, Validation Loss: 4533.9140625\n",
      "Epoch [239/250], Training Loss: 7.833092966606711, Validation Loss: 6814.740234375\n",
      "Epoch [240/250], Training Loss: 5.213254588345235, Validation Loss: 4453.015625\n",
      "Epoch [241/250], Training Loss: 5.805112426765863, Validation Loss: 5867.51123046875\n",
      "Epoch [242/250], Training Loss: 5.788909783013273, Validation Loss: 4073.27197265625\n",
      "Epoch [243/250], Training Loss: 6.799173997791038, Validation Loss: 5701.93212890625\n",
      "Epoch [244/250], Training Loss: 6.010034058461369, Validation Loss: 4401.02734375\n",
      "Epoch [245/250], Training Loss: 6.992565700993631, Validation Loss: 6564.92626953125\n",
      "Epoch [246/250], Training Loss: 5.980685665822482, Validation Loss: 4651.509765625\n",
      "Epoch [247/250], Training Loss: 7.413206327736057, Validation Loss: 7627.69287109375\n",
      "Epoch [248/250], Training Loss: 5.957028979124917, Validation Loss: 4502.45361328125\n",
      "Epoch [249/250], Training Loss: 7.098569380463025, Validation Loss: 7295.486328125\n",
      "Epoch [250/250], Training Loss: 6.1676034604915415, Validation Loss: 4903.43359375\n",
      "Test Loss: 4592.64306640625\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.005, 25)\n",
      "Epoch [1/250], Training Loss: 18680.05832389816, Validation Loss: 18032.708984375\n",
      "Epoch [2/250], Training Loss: 13584.340304396988, Validation Loss: 14407.193359375\n",
      "Epoch [3/250], Training Loss: 10511.598467667167, Validation Loss: 12509.2421875\n",
      "Epoch [4/250], Training Loss: 8395.237940177329, Validation Loss: 11444.4267578125\n",
      "Epoch [5/250], Training Loss: 6830.281001555007, Validation Loss: 10801.796875\n",
      "Epoch [6/250], Training Loss: 5549.251147375793, Validation Loss: 10284.8740234375\n",
      "Epoch [7/250], Training Loss: 4537.681748939138, Validation Loss: 9568.8857421875\n",
      "Epoch [8/250], Training Loss: 3718.4038999093605, Validation Loss: 8935.982421875\n",
      "Epoch [9/250], Training Loss: 3025.595494640835, Validation Loss: 8516.3701171875\n",
      "Epoch [10/250], Training Loss: 2484.6020523609304, Validation Loss: 8225.232421875\n",
      "Epoch [11/250], Training Loss: 2090.7017257898833, Validation Loss: 8049.009765625\n",
      "Epoch [12/250], Training Loss: 1768.1070043180791, Validation Loss: 7875.388671875\n",
      "Epoch [13/250], Training Loss: 1502.7244695486872, Validation Loss: 7537.171875\n",
      "Epoch [14/250], Training Loss: 1299.8034335662028, Validation Loss: 7079.5693359375\n",
      "Epoch [15/250], Training Loss: 1136.5139534700616, Validation Loss: 6588.89208984375\n",
      "Epoch [16/250], Training Loss: 1000.7420004566347, Validation Loss: 6135.3818359375\n",
      "Epoch [17/250], Training Loss: 870.8337579528544, Validation Loss: 5762.58837890625\n",
      "Epoch [18/250], Training Loss: 751.2669104348847, Validation Loss: 5356.59814453125\n",
      "Epoch [19/250], Training Loss: 652.5932016244258, Validation Loss: 4934.0068359375\n",
      "Epoch [20/250], Training Loss: 565.0023152145129, Validation Loss: 4509.98193359375\n",
      "Epoch [21/250], Training Loss: 491.4536086971934, Validation Loss: 4063.010986328125\n",
      "Epoch [22/250], Training Loss: 427.8987182767904, Validation Loss: 3804.772705078125\n",
      "Epoch [23/250], Training Loss: 374.1141291497128, Validation Loss: 3616.736083984375\n",
      "Epoch [24/250], Training Loss: 327.4000764698217, Validation Loss: 3352.165283203125\n",
      "Epoch [25/250], Training Loss: 284.93804131407893, Validation Loss: 2986.843017578125\n",
      "Epoch [26/250], Training Loss: 251.5187211295383, Validation Loss: 3078.336181640625\n",
      "Epoch [27/250], Training Loss: 214.29216899359344, Validation Loss: 3349.296630859375\n",
      "Epoch [28/250], Training Loss: 190.4547376782047, Validation Loss: 3626.41015625\n",
      "Epoch [29/250], Training Loss: 180.91058343098922, Validation Loss: 4904.7734375\n",
      "Epoch [30/250], Training Loss: 202.67871020911022, Validation Loss: 5911.11279296875\n",
      "Epoch [31/250], Training Loss: 220.77253402236806, Validation Loss: 6806.33203125\n",
      "Epoch [32/250], Training Loss: 234.65157508939038, Validation Loss: 8666.5302734375\n",
      "Epoch [33/250], Training Loss: 207.58572754370232, Validation Loss: 9080.697265625\n",
      "Epoch [34/250], Training Loss: 164.85442914954862, Validation Loss: 9049.9267578125\n",
      "Epoch [35/250], Training Loss: 158.9891824407814, Validation Loss: 9460.873046875\n",
      "Epoch [36/250], Training Loss: 145.46135054745832, Validation Loss: 9378.50390625\n",
      "Epoch [37/250], Training Loss: 137.25296619343655, Validation Loss: 9426.91015625\n",
      "Epoch [38/250], Training Loss: 131.51546439057591, Validation Loss: 9492.0966796875\n",
      "Epoch [39/250], Training Loss: 120.84371772191575, Validation Loss: 9770.7626953125\n",
      "Epoch [40/250], Training Loss: 109.04230370021347, Validation Loss: 9641.27734375\n",
      "Epoch [41/250], Training Loss: 104.63254565610605, Validation Loss: 9251.015625\n",
      "Epoch [42/250], Training Loss: 102.87345647216026, Validation Loss: 9043.4501953125\n",
      "Epoch [43/250], Training Loss: 100.71606054752962, Validation Loss: 8955.560546875\n",
      "Epoch [44/250], Training Loss: 98.80945700895631, Validation Loss: 8863.146484375\n",
      "Epoch [45/250], Training Loss: 97.14118616253093, Validation Loss: 8810.486328125\n",
      "Epoch [46/250], Training Loss: 96.19465945195772, Validation Loss: 8868.201171875\n",
      "Epoch [47/250], Training Loss: 95.76873353620213, Validation Loss: 8981.04296875\n",
      "Epoch [48/250], Training Loss: 95.91271090945473, Validation Loss: 9072.6962890625\n",
      "Epoch [49/250], Training Loss: 97.8936650776367, Validation Loss: 9189.29296875\n",
      "Epoch [50/250], Training Loss: 98.12710699951575, Validation Loss: 9302.263671875\n",
      "Epoch [51/250], Training Loss: 519.1207184822248, Validation Loss: 12415.8466796875\n",
      "Epoch [52/250], Training Loss: 282.40652305709557, Validation Loss: 15298.79296875\n",
      "Epoch [53/250], Training Loss: 114.85686756797932, Validation Loss: 13428.58984375\n",
      "Epoch [54/250], Training Loss: 468.6501043481611, Validation Loss: 14560.7197265625\n",
      "Epoch [55/250], Training Loss: 345.644776406113, Validation Loss: 16525.443359375\n",
      "Epoch [56/250], Training Loss: 99.8198527375731, Validation Loss: 15646.607421875\n",
      "Epoch [57/250], Training Loss: 97.02051149538973, Validation Loss: 13307.5966796875\n",
      "Epoch [58/250], Training Loss: 147.73769382566496, Validation Loss: 13124.23046875\n",
      "Epoch [59/250], Training Loss: 94.998107016051, Validation Loss: 12737.8388671875\n",
      "Epoch [60/250], Training Loss: 99.69624770251197, Validation Loss: 12477.1083984375\n",
      "Epoch [61/250], Training Loss: 94.58082173141197, Validation Loss: 12419.7216796875\n",
      "Epoch [62/250], Training Loss: 479.8348615714699, Validation Loss: 13182.8876953125\n",
      "Epoch [63/250], Training Loss: 106.08366897739283, Validation Loss: 13564.7470703125\n",
      "Epoch [64/250], Training Loss: 102.9466833381614, Validation Loss: 12640.044921875\n",
      "Epoch [65/250], Training Loss: 151.4383778006469, Validation Loss: 12193.732421875\n",
      "Epoch [66/250], Training Loss: 97.93941883853313, Validation Loss: 12507.9248046875\n",
      "Epoch [67/250], Training Loss: 176.35527789671235, Validation Loss: 12397.9443359375\n",
      "Epoch [68/250], Training Loss: 100.32471379497771, Validation Loss: 12641.57421875\n",
      "Epoch [69/250], Training Loss: 96.83566205952052, Validation Loss: 12352.7314453125\n",
      "Epoch [70/250], Training Loss: 108.33250697069747, Validation Loss: 11867.888671875\n",
      "Epoch [71/250], Training Loss: 97.08638299537607, Validation Loss: 12338.2255859375\n",
      "Epoch [72/250], Training Loss: 121.40109814468525, Validation Loss: 11738.6552734375\n",
      "Epoch [73/250], Training Loss: 99.37480297790874, Validation Loss: 12500.77734375\n",
      "Epoch [74/250], Training Loss: 97.29420643039126, Validation Loss: 12157.8984375\n",
      "Epoch [75/250], Training Loss: 98.91057504828217, Validation Loss: 12009.841796875\n",
      "Epoch [76/250], Training Loss: 99.40825910263213, Validation Loss: 11943.1806640625\n",
      "Epoch [77/250], Training Loss: 99.89222115144499, Validation Loss: 11918.8857421875\n",
      "Epoch [78/250], Training Loss: 100.658723201208, Validation Loss: 11969.9990234375\n",
      "Epoch [79/250], Training Loss: 102.0182136543575, Validation Loss: 12081.708984375\n",
      "Epoch [80/250], Training Loss: 103.41798304733807, Validation Loss: 12216.7705078125\n",
      "Epoch [81/250], Training Loss: 105.91940814823673, Validation Loss: 12314.880859375\n",
      "Epoch [82/250], Training Loss: 108.72289311172693, Validation Loss: 12442.69921875\n",
      "Epoch [83/250], Training Loss: 110.19658609009765, Validation Loss: 12622.3642578125\n",
      "Epoch [84/250], Training Loss: 110.82092259337318, Validation Loss: 12717.4228515625\n",
      "Epoch [85/250], Training Loss: 109.13253545426228, Validation Loss: 12970.46484375\n",
      "Epoch [86/250], Training Loss: 106.82967873296462, Validation Loss: 13303.951171875\n",
      "Epoch [87/250], Training Loss: 104.27076790862358, Validation Loss: 13694.2744140625\n",
      "Epoch [88/250], Training Loss: 101.26584185689646, Validation Loss: 14164.4326171875\n",
      "Epoch [89/250], Training Loss: 98.5098680204573, Validation Loss: 14694.416015625\n",
      "Epoch [90/250], Training Loss: 97.02462884446528, Validation Loss: 15431.0703125\n",
      "Epoch [91/250], Training Loss: 96.59791007235602, Validation Loss: 16654.26171875\n",
      "Epoch [92/250], Training Loss: 97.2313415403564, Validation Loss: 17963.6015625\n",
      "Epoch [93/250], Training Loss: 98.71296663082862, Validation Loss: 18512.58984375\n",
      "Epoch [94/250], Training Loss: 98.75995051933782, Validation Loss: 18591.27734375\n",
      "Epoch [95/250], Training Loss: 100.13386007525715, Validation Loss: 18773.67578125\n",
      "Epoch [96/250], Training Loss: 107.35599695718852, Validation Loss: 18906.330078125\n",
      "Epoch [97/250], Training Loss: 109.4920592521931, Validation Loss: 18889.451171875\n",
      "Epoch [98/250], Training Loss: 117.09752228011362, Validation Loss: 18811.04296875\n",
      "Epoch [99/250], Training Loss: 116.57180837735622, Validation Loss: 18680.06640625\n",
      "Epoch [100/250], Training Loss: 116.48635391635172, Validation Loss: 18603.826171875\n",
      "Epoch [101/250], Training Loss: 116.03307898103456, Validation Loss: 18576.484375\n",
      "Epoch [102/250], Training Loss: 115.44638654735161, Validation Loss: 18440.271484375\n",
      "Epoch [103/250], Training Loss: 114.71786737713316, Validation Loss: 18419.7421875\n",
      "Epoch [104/250], Training Loss: 113.73762875459667, Validation Loss: 18418.3828125\n",
      "Epoch [105/250], Training Loss: 111.9779699656138, Validation Loss: 18541.2734375\n",
      "Epoch [106/250], Training Loss: 109.52245030758179, Validation Loss: 18602.396484375\n",
      "Epoch [107/250], Training Loss: 108.87767121875098, Validation Loss: 18788.90234375\n",
      "Epoch [108/250], Training Loss: 108.87731227354554, Validation Loss: 18568.87109375\n",
      "Epoch [109/250], Training Loss: 107.18009340229523, Validation Loss: 18831.880859375\n",
      "Epoch [110/250], Training Loss: 2069.1533699392808, Validation Loss: 48450.46484375\n",
      "Epoch [111/250], Training Loss: 4057.6053227925136, Validation Loss: 54521.046875\n",
      "Epoch [112/250], Training Loss: 3641.0863785170995, Validation Loss: 87683.1796875\n",
      "Epoch [113/250], Training Loss: 4376.236861468034, Validation Loss: 55026.59375\n",
      "Epoch [114/250], Training Loss: 3579.283528734773, Validation Loss: 67544.7734375\n",
      "Epoch [115/250], Training Loss: 2476.368482045898, Validation Loss: 14348.3486328125\n",
      "Epoch [116/250], Training Loss: 464.5244198501268, Validation Loss: 15238.466796875\n",
      "Epoch [117/250], Training Loss: 244.38434875638856, Validation Loss: 16482.9375\n",
      "Epoch [118/250], Training Loss: 176.18810117873238, Validation Loss: 17212.33984375\n",
      "Epoch [119/250], Training Loss: 141.63378842366902, Validation Loss: 17535.51171875\n",
      "Epoch [120/250], Training Loss: 128.32367598302056, Validation Loss: 17589.455078125\n",
      "Epoch [121/250], Training Loss: 118.48959382951243, Validation Loss: 17603.525390625\n",
      "Epoch [122/250], Training Loss: 109.62139788191894, Validation Loss: 17531.630859375\n",
      "Epoch [123/250], Training Loss: 103.2709997702077, Validation Loss: 17412.556640625\n",
      "Epoch [124/250], Training Loss: 98.48685674467949, Validation Loss: 17250.828125\n",
      "Epoch [125/250], Training Loss: 95.13619523669273, Validation Loss: 17036.326171875\n",
      "Epoch [126/250], Training Loss: 94.88332345182083, Validation Loss: 16863.087890625\n",
      "Epoch [127/250], Training Loss: 96.38433438039617, Validation Loss: 16786.5703125\n",
      "Epoch [128/250], Training Loss: 101.57406491792749, Validation Loss: 16789.759765625\n",
      "Epoch [129/250], Training Loss: 110.99379782315204, Validation Loss: 16945.109375\n",
      "Epoch [130/250], Training Loss: 110.40602408597185, Validation Loss: 17011.830078125\n",
      "Epoch [131/250], Training Loss: 110.78597994462893, Validation Loss: 17091.265625\n",
      "Epoch [132/250], Training Loss: 111.41685419335343, Validation Loss: 17163.083984375\n",
      "Epoch [133/250], Training Loss: 112.15275916487742, Validation Loss: 17215.224609375\n",
      "Epoch [134/250], Training Loss: 110.99134473562042, Validation Loss: 17198.283203125\n",
      "Epoch [135/250], Training Loss: 109.00301733672737, Validation Loss: 17183.482421875\n",
      "Epoch [136/250], Training Loss: 107.1744191963382, Validation Loss: 17175.0546875\n",
      "Epoch [137/250], Training Loss: 105.59072999767629, Validation Loss: 17173.658203125\n",
      "Epoch [138/250], Training Loss: 104.34186175234915, Validation Loss: 17179.998046875\n",
      "Epoch [139/250], Training Loss: 103.77798430463663, Validation Loss: 17187.630859375\n",
      "Epoch [140/250], Training Loss: 103.56061844418267, Validation Loss: 17195.0\n",
      "Epoch [141/250], Training Loss: 103.12612899624891, Validation Loss: 17206.470703125\n",
      "Epoch [142/250], Training Loss: 102.25895475325946, Validation Loss: 17225.119140625\n",
      "Epoch [143/250], Training Loss: 101.15889084186392, Validation Loss: 17244.283203125\n",
      "Epoch [144/250], Training Loss: 100.34641581526418, Validation Loss: 17254.376953125\n",
      "Epoch [145/250], Training Loss: 100.32758555505623, Validation Loss: 17247.248046875\n",
      "Epoch [146/250], Training Loss: 101.21955553776839, Validation Loss: 17225.283203125\n",
      "Epoch [147/250], Training Loss: 102.43206853428434, Validation Loss: 17223.478515625\n",
      "Epoch [148/250], Training Loss: 102.93604912841704, Validation Loss: 17243.86328125\n",
      "Epoch [149/250], Training Loss: 102.42140937612739, Validation Loss: 17264.888671875\n",
      "Epoch [150/250], Training Loss: 101.36944753007276, Validation Loss: 17273.357421875\n",
      "Epoch [151/250], Training Loss: 100.35511225114954, Validation Loss: 17266.98046875\n",
      "Epoch [152/250], Training Loss: 99.90837819572899, Validation Loss: 17244.611328125\n",
      "Epoch [153/250], Training Loss: 99.01084463014101, Validation Loss: 17254.2421875\n",
      "Epoch [154/250], Training Loss: 97.17150791705579, Validation Loss: 17294.197265625\n",
      "Epoch [155/250], Training Loss: 97.18823180464985, Validation Loss: 17366.5390625\n",
      "Epoch [156/250], Training Loss: 94.69738637569334, Validation Loss: 17340.47265625\n",
      "Epoch [157/250], Training Loss: 91.81808933379493, Validation Loss: 17414.76171875\n",
      "Epoch [158/250], Training Loss: 84.48927057306204, Validation Loss: 18022.57421875\n",
      "Epoch [159/250], Training Loss: 79.01487075354086, Validation Loss: 18337.103515625\n",
      "Epoch [160/250], Training Loss: 70.19770621552655, Validation Loss: 18103.767578125\n",
      "Epoch [161/250], Training Loss: 67.57767900357243, Validation Loss: 18169.353515625\n",
      "Epoch [162/250], Training Loss: 68.83626123859418, Validation Loss: 17947.9140625\n",
      "Epoch [163/250], Training Loss: 58.96811721201065, Validation Loss: 17974.1015625\n",
      "Epoch [164/250], Training Loss: 68.28491048379323, Validation Loss: 18238.91015625\n",
      "Epoch [165/250], Training Loss: 63.96483140786, Validation Loss: 18196.431640625\n",
      "Epoch [166/250], Training Loss: 64.74668729749915, Validation Loss: 18270.669921875\n",
      "Epoch [167/250], Training Loss: 64.1685338191542, Validation Loss: 18288.150390625\n",
      "Epoch [168/250], Training Loss: 63.08766028241265, Validation Loss: 18270.494140625\n",
      "Epoch [169/250], Training Loss: 63.1860301070917, Validation Loss: 18354.501953125\n",
      "Epoch [170/250], Training Loss: 58.47279970441376, Validation Loss: 18204.734375\n",
      "Epoch [171/250], Training Loss: 61.4997086180934, Validation Loss: 18579.830078125\n",
      "Epoch [172/250], Training Loss: 53.92060328443998, Validation Loss: 18293.662109375\n",
      "Epoch [173/250], Training Loss: 54.92686740771032, Validation Loss: 18542.0546875\n",
      "Epoch [174/250], Training Loss: 54.052863475757604, Validation Loss: 18472.18359375\n",
      "Epoch [175/250], Training Loss: 50.55273502651431, Validation Loss: 18268.33203125\n",
      "Epoch [176/250], Training Loss: 55.28236717574015, Validation Loss: 18935.59765625\n",
      "Epoch [177/250], Training Loss: 50.68006437864946, Validation Loss: 18756.830078125\n",
      "Epoch [178/250], Training Loss: 46.29988389516694, Validation Loss: 18330.68359375\n",
      "Epoch [179/250], Training Loss: 49.35902914033778, Validation Loss: 19039.423828125\n",
      "Epoch [180/250], Training Loss: 43.25542191686205, Validation Loss: 18735.474609375\n",
      "Epoch [181/250], Training Loss: 43.555984057707576, Validation Loss: 18299.30859375\n",
      "Epoch [182/250], Training Loss: 43.946749772778404, Validation Loss: 18815.71484375\n",
      "Epoch [183/250], Training Loss: 42.767161124303904, Validation Loss: 18697.439453125\n",
      "Epoch [184/250], Training Loss: 42.10557733022812, Validation Loss: 18561.1015625\n",
      "Epoch [185/250], Training Loss: 39.9608021193191, Validation Loss: 18508.083984375\n",
      "Epoch [186/250], Training Loss: 39.94065316584296, Validation Loss: 18789.857421875\n",
      "Epoch [187/250], Training Loss: 43.4184121786617, Validation Loss: 18520.896484375\n",
      "Epoch [188/250], Training Loss: 44.39480658729529, Validation Loss: 18266.91796875\n",
      "Epoch [189/250], Training Loss: 41.96273834249239, Validation Loss: 18010.240234375\n",
      "Epoch [190/250], Training Loss: 41.74626556402289, Validation Loss: 18036.345703125\n",
      "Epoch [191/250], Training Loss: 38.622573563831395, Validation Loss: 18148.849609375\n",
      "Epoch [192/250], Training Loss: 39.57185917918631, Validation Loss: 18626.591796875\n",
      "Epoch [193/250], Training Loss: 37.76986699992223, Validation Loss: 18280.560546875\n",
      "Epoch [194/250], Training Loss: 38.152623360138314, Validation Loss: 18764.177734375\n",
      "Epoch [195/250], Training Loss: 37.8740580125583, Validation Loss: 18491.2734375\n",
      "Epoch [196/250], Training Loss: 34.95144085126248, Validation Loss: 18264.5625\n",
      "Epoch [197/250], Training Loss: 34.42233156301145, Validation Loss: 18475.59765625\n",
      "Epoch [198/250], Training Loss: 33.911745649932094, Validation Loss: 18666.966796875\n",
      "Epoch [199/250], Training Loss: 33.93336111355721, Validation Loss: 18736.0859375\n",
      "Epoch [200/250], Training Loss: 34.20138840624706, Validation Loss: 18575.212890625\n",
      "Epoch [201/250], Training Loss: 33.37363811405383, Validation Loss: 18347.650390625\n",
      "Epoch [202/250], Training Loss: 32.77163808691676, Validation Loss: 18649.09765625\n",
      "Epoch [203/250], Training Loss: 33.4793731040855, Validation Loss: 18610.552734375\n",
      "Epoch [204/250], Training Loss: 33.52278251798085, Validation Loss: 18445.3515625\n",
      "Epoch [205/250], Training Loss: 32.993646045965804, Validation Loss: 18425.810546875\n",
      "Epoch [206/250], Training Loss: 33.25719456824904, Validation Loss: 18372.94140625\n",
      "Epoch [207/250], Training Loss: 30.41192712789766, Validation Loss: 18672.912109375\n",
      "Epoch [208/250], Training Loss: 31.39970028784929, Validation Loss: 18772.22265625\n",
      "Epoch [209/250], Training Loss: 31.40062714129826, Validation Loss: 18568.033203125\n",
      "Epoch [210/250], Training Loss: 30.985734397498177, Validation Loss: 18630.892578125\n",
      "Epoch [211/250], Training Loss: 31.314395232562287, Validation Loss: 18596.662109375\n",
      "Epoch [212/250], Training Loss: 34.93765193936305, Validation Loss: 18446.53125\n",
      "Epoch [213/250], Training Loss: 34.460669276135754, Validation Loss: 18388.478515625\n",
      "Epoch [214/250], Training Loss: 31.99038334797817, Validation Loss: 18297.3671875\n",
      "Epoch [215/250], Training Loss: 31.49699262137792, Validation Loss: 18470.96484375\n",
      "Epoch [216/250], Training Loss: 30.83351589086139, Validation Loss: 18419.517578125\n",
      "Epoch [217/250], Training Loss: 31.023623970620623, Validation Loss: 18553.279296875\n",
      "Epoch [218/250], Training Loss: 29.809269454877068, Validation Loss: 18568.86328125\n",
      "Epoch [219/250], Training Loss: 31.21096342779559, Validation Loss: 18701.087890625\n",
      "Epoch [220/250], Training Loss: 31.413416556310935, Validation Loss: 18586.06640625\n",
      "Epoch [221/250], Training Loss: 30.40297188792827, Validation Loss: 18613.60546875\n",
      "Epoch [222/250], Training Loss: 29.598917576464903, Validation Loss: 18684.748046875\n",
      "Epoch [223/250], Training Loss: 28.479849950866804, Validation Loss: 18550.2265625\n",
      "Epoch [224/250], Training Loss: 28.832194729290823, Validation Loss: 18685.978515625\n",
      "Epoch [225/250], Training Loss: 29.200824064628655, Validation Loss: 18666.93359375\n",
      "Epoch [226/250], Training Loss: 29.088296333650923, Validation Loss: 18665.419921875\n",
      "Epoch [227/250], Training Loss: 28.928415051660565, Validation Loss: 18668.177734375\n",
      "Epoch [228/250], Training Loss: 28.888584468790953, Validation Loss: 18681.80078125\n",
      "Epoch [229/250], Training Loss: 28.927557204135937, Validation Loss: 18683.22265625\n",
      "Epoch [230/250], Training Loss: 28.96625731695736, Validation Loss: 18685.806640625\n",
      "Epoch [231/250], Training Loss: 28.97038013933783, Validation Loss: 18678.037109375\n",
      "Epoch [232/250], Training Loss: 29.094221006140884, Validation Loss: 18736.314453125\n",
      "Epoch [233/250], Training Loss: 28.03313655486643, Validation Loss: 18748.3046875\n",
      "Epoch [234/250], Training Loss: 30.72298577569126, Validation Loss: 18598.7890625\n",
      "Epoch [235/250], Training Loss: 32.56128542408122, Validation Loss: 18695.009765625\n",
      "Epoch [236/250], Training Loss: 32.195509420212204, Validation Loss: 18590.51171875\n",
      "Epoch [237/250], Training Loss: 29.96905107498816, Validation Loss: 18648.81640625\n",
      "Epoch [238/250], Training Loss: 27.286248805382296, Validation Loss: 18769.453125\n",
      "Epoch [239/250], Training Loss: 26.506363230226775, Validation Loss: 18742.271484375\n",
      "Epoch [240/250], Training Loss: 28.831326129604566, Validation Loss: 18776.01953125\n",
      "Epoch [241/250], Training Loss: 30.473219935341174, Validation Loss: 18872.06640625\n",
      "Epoch [242/250], Training Loss: 30.21580935345438, Validation Loss: 18640.703125\n",
      "Epoch [243/250], Training Loss: 29.955840212655946, Validation Loss: 18796.12890625\n",
      "Epoch [244/250], Training Loss: 32.03533691887851, Validation Loss: 18477.728515625\n",
      "Epoch [245/250], Training Loss: 31.137435102491935, Validation Loss: 18831.64453125\n",
      "Epoch [246/250], Training Loss: 28.271717186231154, Validation Loss: 18601.90234375\n",
      "Epoch [247/250], Training Loss: 28.013081996058993, Validation Loss: 18784.837890625\n",
      "Epoch [248/250], Training Loss: 29.292486396750448, Validation Loss: 18803.224609375\n",
      "Epoch [249/250], Training Loss: 28.10189801636645, Validation Loss: 18876.0234375\n",
      "Epoch [250/250], Training Loss: 27.96049685468147, Validation Loss: 18670.08984375\n",
      "Test Loss: 15917.0205078125\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.005, 30)\n",
      "Epoch [1/250], Training Loss: 18852.94735320787, Validation Loss: 17918.18359375\n",
      "Epoch [2/250], Training Loss: 13621.29834836061, Validation Loss: 16594.587890625\n",
      "Epoch [3/250], Training Loss: 10527.198336256684, Validation Loss: 12797.93359375\n",
      "Epoch [4/250], Training Loss: 8441.418373617156, Validation Loss: 11182.6611328125\n",
      "Epoch [5/250], Training Loss: 6865.318334931325, Validation Loss: 12294.865234375\n",
      "Epoch [6/250], Training Loss: 5579.996250527444, Validation Loss: 13824.8623046875\n",
      "Epoch [7/250], Training Loss: 4558.849185347624, Validation Loss: 12292.5341796875\n",
      "Epoch [8/250], Training Loss: 3743.671669722662, Validation Loss: 10272.732421875\n",
      "Epoch [9/250], Training Loss: 3046.163791444594, Validation Loss: 9135.419921875\n",
      "Epoch [10/250], Training Loss: 2495.335550973928, Validation Loss: 8135.6318359375\n",
      "Epoch [11/250], Training Loss: 2098.5197513243324, Validation Loss: 7830.44189453125\n",
      "Epoch [12/250], Training Loss: 1770.3212770342834, Validation Loss: 7202.41455078125\n",
      "Epoch [13/250], Training Loss: 1508.2799352776665, Validation Loss: 6854.0087890625\n",
      "Epoch [14/250], Training Loss: 1302.5918248081337, Validation Loss: 6259.24169921875\n",
      "Epoch [15/250], Training Loss: 1136.818208852894, Validation Loss: 5488.171875\n",
      "Epoch [16/250], Training Loss: 983.9517701006487, Validation Loss: 5209.87841796875\n",
      "Epoch [17/250], Training Loss: 843.9011599095348, Validation Loss: 4529.0341796875\n",
      "Epoch [18/250], Training Loss: 742.7520126370407, Validation Loss: 4226.1162109375\n",
      "Epoch [19/250], Training Loss: 652.6810281484287, Validation Loss: 4145.404296875\n",
      "Epoch [20/250], Training Loss: 573.0978972003497, Validation Loss: 3816.00732421875\n",
      "Epoch [21/250], Training Loss: 503.51822710559765, Validation Loss: 4770.220703125\n",
      "Epoch [22/250], Training Loss: 467.1350060290431, Validation Loss: 4442.265625\n",
      "Epoch [23/250], Training Loss: 399.1716003039376, Validation Loss: 4303.84716796875\n",
      "Epoch [24/250], Training Loss: 350.74779210930404, Validation Loss: 4314.66259765625\n",
      "Epoch [25/250], Training Loss: 315.5464230049249, Validation Loss: 4823.70166015625\n",
      "Epoch [26/250], Training Loss: 272.1219962417377, Validation Loss: 4913.2353515625\n",
      "Epoch [27/250], Training Loss: 225.90098309134527, Validation Loss: 3608.518310546875\n",
      "Epoch [28/250], Training Loss: 205.32006002031895, Validation Loss: 3596.367919921875\n",
      "Epoch [29/250], Training Loss: 194.90821301563076, Validation Loss: 5058.65234375\n",
      "Epoch [30/250], Training Loss: 211.787293390421, Validation Loss: 6747.92626953125\n",
      "Epoch [31/250], Training Loss: 176.31681753170704, Validation Loss: 7856.97705078125\n",
      "Epoch [32/250], Training Loss: 152.72852140308018, Validation Loss: 8279.4541015625\n",
      "Epoch [33/250], Training Loss: 135.75838640090467, Validation Loss: 8718.2177734375\n",
      "Epoch [34/250], Training Loss: 121.38386697310027, Validation Loss: 8981.244140625\n",
      "Epoch [35/250], Training Loss: 106.3701749490956, Validation Loss: 9050.0068359375\n",
      "Epoch [36/250], Training Loss: 95.99407039282077, Validation Loss: 8999.6982421875\n",
      "Epoch [37/250], Training Loss: 91.73838099190004, Validation Loss: 8784.4814453125\n",
      "Epoch [38/250], Training Loss: 87.94918109891186, Validation Loss: 8471.2294921875\n",
      "Epoch [39/250], Training Loss: 83.43648909730076, Validation Loss: 8520.2314453125\n",
      "Epoch [40/250], Training Loss: 80.69733049992983, Validation Loss: 8860.564453125\n",
      "Epoch [41/250], Training Loss: 78.9127427332547, Validation Loss: 9015.8515625\n",
      "Epoch [42/250], Training Loss: 77.17609870135183, Validation Loss: 9223.73828125\n",
      "Epoch [43/250], Training Loss: 75.28526653835608, Validation Loss: 9318.982421875\n",
      "Epoch [44/250], Training Loss: 73.53225702813953, Validation Loss: 9492.8515625\n",
      "Epoch [45/250], Training Loss: 71.95072949728821, Validation Loss: 9836.837890625\n",
      "Epoch [46/250], Training Loss: 70.97461392298621, Validation Loss: 10102.63671875\n",
      "Epoch [47/250], Training Loss: 69.43692899691894, Validation Loss: 10317.09375\n",
      "Epoch [48/250], Training Loss: 68.57534538518154, Validation Loss: 10438.5712890625\n",
      "Epoch [49/250], Training Loss: 66.44272727398956, Validation Loss: 10472.6845703125\n",
      "Epoch [50/250], Training Loss: 65.21681769475929, Validation Loss: 10639.9228515625\n",
      "Epoch [51/250], Training Loss: 64.15812808501965, Validation Loss: 10739.4560546875\n",
      "Epoch [52/250], Training Loss: 63.646751153332005, Validation Loss: 10853.7021484375\n",
      "Epoch [53/250], Training Loss: 72.06934500800536, Validation Loss: 12317.6416015625\n",
      "Epoch [54/250], Training Loss: 74.42446231646544, Validation Loss: 12253.9169921875\n",
      "Epoch [55/250], Training Loss: 68.29730590775549, Validation Loss: 11633.7021484375\n",
      "Epoch [56/250], Training Loss: 66.40030194983198, Validation Loss: 11524.232421875\n",
      "Epoch [57/250], Training Loss: 79.51670647971093, Validation Loss: 11082.5615234375\n",
      "Epoch [58/250], Training Loss: 61.11136827053541, Validation Loss: 12087.251953125\n",
      "Epoch [59/250], Training Loss: 56.745601388927845, Validation Loss: 11307.6005859375\n",
      "Epoch [60/250], Training Loss: 54.02977642697066, Validation Loss: 11045.654296875\n",
      "Epoch [61/250], Training Loss: 65.35223301328443, Validation Loss: 12383.650390625\n",
      "Epoch [62/250], Training Loss: 51.718254497844484, Validation Loss: 10868.7890625\n",
      "Epoch [63/250], Training Loss: 48.951579902809854, Validation Loss: 11520.453125\n",
      "Epoch [64/250], Training Loss: 46.427309041569444, Validation Loss: 10234.3486328125\n",
      "Epoch [65/250], Training Loss: 45.479177460510456, Validation Loss: 10378.234375\n",
      "Epoch [66/250], Training Loss: 44.16427100044992, Validation Loss: 10800.6884765625\n",
      "Epoch [67/250], Training Loss: 41.87896169901134, Validation Loss: 10519.599609375\n",
      "Epoch [68/250], Training Loss: 39.15556030471163, Validation Loss: 10603.123046875\n",
      "Epoch [69/250], Training Loss: 36.85848294482656, Validation Loss: 10557.0341796875\n",
      "Epoch [70/250], Training Loss: 35.06084586963032, Validation Loss: 10540.0244140625\n",
      "Epoch [71/250], Training Loss: 33.85022043022682, Validation Loss: 10661.4306640625\n",
      "Epoch [72/250], Training Loss: 33.02719123987922, Validation Loss: 10932.896484375\n",
      "Epoch [73/250], Training Loss: 32.84348775882876, Validation Loss: 11294.138671875\n",
      "Epoch [74/250], Training Loss: 32.39504467876392, Validation Loss: 11625.3095703125\n",
      "Epoch [75/250], Training Loss: 31.984082477171405, Validation Loss: 11811.6337890625\n",
      "Epoch [76/250], Training Loss: 31.677121816527993, Validation Loss: 11951.3349609375\n",
      "Epoch [77/250], Training Loss: 31.95912417743945, Validation Loss: 12104.93359375\n",
      "Epoch [78/250], Training Loss: 31.410982500943582, Validation Loss: 12495.353515625\n",
      "Epoch [79/250], Training Loss: 31.353424286332675, Validation Loss: 12852.896484375\n",
      "Epoch [80/250], Training Loss: 31.848774091523318, Validation Loss: 13827.9560546875\n",
      "Epoch [81/250], Training Loss: 33.56402078646374, Validation Loss: 13856.7822265625\n",
      "Epoch [82/250], Training Loss: 32.689159291452206, Validation Loss: 14203.83984375\n",
      "Epoch [83/250], Training Loss: 33.387348676089395, Validation Loss: 14598.8046875\n",
      "Epoch [84/250], Training Loss: 37.135362342904735, Validation Loss: 15685.658203125\n",
      "Epoch [85/250], Training Loss: 3798.436097065963, Validation Loss: 10583.6796875\n",
      "Epoch [86/250], Training Loss: 300.5349660192309, Validation Loss: 4472.1787109375\n",
      "Epoch [87/250], Training Loss: 78.25407049474487, Validation Loss: 4440.685546875\n",
      "Epoch [88/250], Training Loss: 61.11979018153934, Validation Loss: 5475.6220703125\n",
      "Epoch [89/250], Training Loss: 3579.477709038564, Validation Loss: 6715.43994140625\n",
      "Epoch [90/250], Training Loss: 1392.2242301575502, Validation Loss: 4393.45947265625\n",
      "Epoch [91/250], Training Loss: 226.05593195732578, Validation Loss: 3140.82177734375\n",
      "Epoch [92/250], Training Loss: 176.8140244870487, Validation Loss: 5404.28173828125\n",
      "Epoch [93/250], Training Loss: 104.78346638300486, Validation Loss: 6260.07373046875\n",
      "Epoch [94/250], Training Loss: 86.2544465801256, Validation Loss: 5448.376953125\n",
      "Epoch [95/250], Training Loss: 78.41004654912696, Validation Loss: 4120.64306640625\n",
      "Epoch [96/250], Training Loss: 67.49114767946737, Validation Loss: 4185.99853515625\n",
      "Epoch [97/250], Training Loss: 201.09029284220705, Validation Loss: 4658.029296875\n",
      "Epoch [98/250], Training Loss: 77.15217200804193, Validation Loss: 4421.85791015625\n",
      "Epoch [99/250], Training Loss: 71.40847256842399, Validation Loss: 4354.59423828125\n",
      "Epoch [100/250], Training Loss: 268.0515200574567, Validation Loss: 4801.72412109375\n",
      "Epoch [101/250], Training Loss: 108.55412403293532, Validation Loss: 4506.2568359375\n",
      "Epoch [102/250], Training Loss: 79.1437896299689, Validation Loss: 4317.96728515625\n",
      "Epoch [103/250], Training Loss: 80.86599002549859, Validation Loss: 4424.42041015625\n",
      "Epoch [104/250], Training Loss: 79.677254521952, Validation Loss: 4372.853515625\n",
      "Epoch [105/250], Training Loss: 81.53026752701555, Validation Loss: 4266.31689453125\n",
      "Epoch [106/250], Training Loss: 3219.6756440479853, Validation Loss: 7674.548828125\n",
      "Epoch [107/250], Training Loss: 2970.9345800090496, Validation Loss: 6822.0322265625\n",
      "Epoch [108/250], Training Loss: 2699.695596861859, Validation Loss: 9749.5498046875\n",
      "Epoch [109/250], Training Loss: 4440.134037376841, Validation Loss: 30313.466796875\n",
      "Epoch [110/250], Training Loss: 1716.44361780567, Validation Loss: 13728.8291015625\n",
      "Epoch [111/250], Training Loss: 1196.8180614407402, Validation Loss: 9937.5888671875\n",
      "Epoch [112/250], Training Loss: 583.4553607629832, Validation Loss: 9412.666015625\n",
      "Epoch [113/250], Training Loss: 1347.2139304151283, Validation Loss: 12426.951171875\n",
      "Epoch [114/250], Training Loss: 1344.7204567489139, Validation Loss: 8454.845703125\n",
      "Epoch [115/250], Training Loss: 1160.4754963907008, Validation Loss: 10090.0048828125\n",
      "Epoch [116/250], Training Loss: 1289.7644448193291, Validation Loss: 6317.11474609375\n",
      "Epoch [117/250], Training Loss: 1047.050639481042, Validation Loss: 6131.94140625\n",
      "Epoch [118/250], Training Loss: 1174.351281514876, Validation Loss: 5860.70654296875\n",
      "Epoch [119/250], Training Loss: 1030.4043782999815, Validation Loss: 5553.2216796875\n",
      "Epoch [120/250], Training Loss: 865.1519783806822, Validation Loss: 5623.34423828125\n",
      "Epoch [121/250], Training Loss: 959.5222872233433, Validation Loss: 5049.2822265625\n",
      "Epoch [122/250], Training Loss: 1286.1426658506537, Validation Loss: 6189.32080078125\n",
      "Epoch [123/250], Training Loss: 835.6559248627958, Validation Loss: 5246.53173828125\n",
      "Epoch [124/250], Training Loss: 251.8270846002522, Validation Loss: 6813.59326171875\n",
      "Epoch [125/250], Training Loss: 785.6597267749898, Validation Loss: 4692.357421875\n",
      "Epoch [126/250], Training Loss: 873.4544926746048, Validation Loss: 4593.744140625\n",
      "Epoch [127/250], Training Loss: 1431.4225470473436, Validation Loss: 4935.99609375\n",
      "Epoch [128/250], Training Loss: 477.9379174508409, Validation Loss: 4840.814453125\n",
      "Epoch [129/250], Training Loss: 1316.4333605656707, Validation Loss: 6568.18896484375\n",
      "Epoch [130/250], Training Loss: 4550.477682472472, Validation Loss: 18063.123046875\n",
      "Epoch [131/250], Training Loss: 1123.8749872744115, Validation Loss: 8341.568359375\n",
      "Epoch [132/250], Training Loss: 535.7250826365814, Validation Loss: 6160.21484375\n",
      "Epoch [133/250], Training Loss: 1373.5780417167434, Validation Loss: 6130.68359375\n",
      "Epoch [134/250], Training Loss: 5741.355445757867, Validation Loss: 5212.53271484375\n",
      "Epoch [135/250], Training Loss: 2465.208222101447, Validation Loss: 5889.0205078125\n",
      "Epoch [136/250], Training Loss: 5358.292743223162, Validation Loss: 28218.5\n",
      "Epoch [137/250], Training Loss: 24851.758348733685, Validation Loss: 29495.947265625\n",
      "Epoch [138/250], Training Loss: 27714.771813458203, Validation Loss: 22704.701171875\n",
      "Epoch [139/250], Training Loss: 17273.683724631213, Validation Loss: 24087.025390625\n",
      "Epoch [140/250], Training Loss: 4067.2750107953766, Validation Loss: 18592.708984375\n",
      "Epoch [141/250], Training Loss: 1155.4269760012735, Validation Loss: 24492.255859375\n",
      "Epoch [142/250], Training Loss: 6266.875164020533, Validation Loss: 24809.97265625\n",
      "Epoch [143/250], Training Loss: 1922.9257983663836, Validation Loss: 19522.095703125\n",
      "Epoch [144/250], Training Loss: 2640.257655782446, Validation Loss: 20799.583984375\n",
      "Epoch [145/250], Training Loss: 1225.1130157414557, Validation Loss: 21941.169921875\n",
      "Epoch [146/250], Training Loss: 768.2427531774921, Validation Loss: 25002.154296875\n",
      "Epoch [147/250], Training Loss: 896.7229029890584, Validation Loss: 19430.744140625\n",
      "Epoch [148/250], Training Loss: 1503.0815760930075, Validation Loss: 12223.921875\n",
      "Epoch [149/250], Training Loss: 8555.79220513883, Validation Loss: 12317.265625\n",
      "Epoch [150/250], Training Loss: 7938.573661456611, Validation Loss: 12597.029296875\n",
      "Epoch [151/250], Training Loss: 7358.0018238637, Validation Loss: 12936.8427734375\n",
      "Epoch [152/250], Training Loss: 6165.662854701581, Validation Loss: 15380.677734375\n",
      "Epoch [153/250], Training Loss: 6084.95622842255, Validation Loss: 22671.685546875\n",
      "Epoch [154/250], Training Loss: 3721.6374276874603, Validation Loss: 13519.6728515625\n",
      "Epoch [155/250], Training Loss: 3510.76308029518, Validation Loss: 14165.08203125\n",
      "Epoch [156/250], Training Loss: 4896.607445801309, Validation Loss: 17584.94140625\n",
      "Epoch [157/250], Training Loss: 3291.631181336386, Validation Loss: 14013.177734375\n",
      "Epoch [158/250], Training Loss: 1347.0923470494638, Validation Loss: 12276.783203125\n",
      "Epoch [159/250], Training Loss: 406.46645597264364, Validation Loss: 11830.78125\n",
      "Epoch [160/250], Training Loss: 2052.7535052430553, Validation Loss: 7884.06884765625\n",
      "Epoch [161/250], Training Loss: 1139.7259981056727, Validation Loss: 8755.99609375\n",
      "Epoch [162/250], Training Loss: 727.3208772023503, Validation Loss: 8138.42626953125\n",
      "Epoch [163/250], Training Loss: 409.2476572164605, Validation Loss: 7977.1572265625\n",
      "Epoch [164/250], Training Loss: 286.38101491458394, Validation Loss: 7436.12060546875\n",
      "Epoch [165/250], Training Loss: 163.189707865313, Validation Loss: 7891.748046875\n",
      "Epoch [166/250], Training Loss: 426.89165970468764, Validation Loss: 7635.3037109375\n",
      "Epoch [167/250], Training Loss: 377.2802465248155, Validation Loss: 7420.2900390625\n",
      "Epoch [168/250], Training Loss: 313.3308884194198, Validation Loss: 7645.568359375\n",
      "Epoch [169/250], Training Loss: 274.7811971788185, Validation Loss: 7480.03515625\n",
      "Epoch [170/250], Training Loss: 312.1422838933727, Validation Loss: 7259.81005859375\n",
      "Epoch [171/250], Training Loss: 352.016515191831, Validation Loss: 7064.322265625\n",
      "Epoch [172/250], Training Loss: 596.2073112885897, Validation Loss: 7048.78173828125\n",
      "Epoch [173/250], Training Loss: 223.63344396556246, Validation Loss: 6466.02392578125\n",
      "Epoch [174/250], Training Loss: 306.32606029665294, Validation Loss: 6176.1455078125\n",
      "Epoch [175/250], Training Loss: 276.4819931735582, Validation Loss: 6368.591796875\n",
      "Epoch [176/250], Training Loss: 298.31964785967153, Validation Loss: 6286.86962890625\n",
      "Epoch [177/250], Training Loss: 218.57164371037354, Validation Loss: 7090.1259765625\n",
      "Epoch [178/250], Training Loss: 182.39835657490258, Validation Loss: 6881.39404296875\n",
      "Epoch [179/250], Training Loss: 161.42997663849926, Validation Loss: 7081.22607421875\n",
      "Epoch [180/250], Training Loss: 149.23983644769592, Validation Loss: 7367.17822265625\n",
      "Epoch [181/250], Training Loss: 142.93774348612789, Validation Loss: 7778.3291015625\n",
      "Epoch [182/250], Training Loss: 123.81084960985005, Validation Loss: 8225.66796875\n",
      "Epoch [183/250], Training Loss: 127.39124684898582, Validation Loss: 8750.4794921875\n",
      "Epoch [184/250], Training Loss: 140.75499560744888, Validation Loss: 9462.9306640625\n",
      "Epoch [185/250], Training Loss: 126.6495990684576, Validation Loss: 10631.056640625\n",
      "Epoch [186/250], Training Loss: 121.4335260549882, Validation Loss: 10416.2529296875\n",
      "Epoch [187/250], Training Loss: 123.42519867641367, Validation Loss: 11973.025390625\n",
      "Epoch [188/250], Training Loss: 97.91852951561467, Validation Loss: 13164.6611328125\n",
      "Epoch [189/250], Training Loss: 133.7049858252968, Validation Loss: 13655.72265625\n",
      "Epoch [190/250], Training Loss: 108.83210603990668, Validation Loss: 15242.19140625\n",
      "Epoch [191/250], Training Loss: 107.54092926429387, Validation Loss: 13623.07421875\n",
      "Epoch [192/250], Training Loss: 845.8283632144394, Validation Loss: 8882.87890625\n",
      "Epoch [193/250], Training Loss: 265.8791936653761, Validation Loss: 18843.115234375\n",
      "Epoch [194/250], Training Loss: 102.11420538872237, Validation Loss: 18782.603515625\n",
      "Epoch [195/250], Training Loss: 101.62424618132343, Validation Loss: 16642.328125\n",
      "Epoch [196/250], Training Loss: 141.93344208038368, Validation Loss: 17660.70703125\n",
      "Epoch [197/250], Training Loss: 97.05633742249034, Validation Loss: 17340.740234375\n",
      "Epoch [198/250], Training Loss: 119.18755494986536, Validation Loss: 15136.390625\n",
      "Epoch [199/250], Training Loss: 126.95123314187404, Validation Loss: 16957.1015625\n",
      "Epoch [200/250], Training Loss: 122.31818709727094, Validation Loss: 23919.361328125\n",
      "Epoch [201/250], Training Loss: 120.67100123482494, Validation Loss: 20405.896484375\n",
      "Epoch [202/250], Training Loss: 111.77382833426715, Validation Loss: 17985.671875\n",
      "Epoch [203/250], Training Loss: 122.75856746117913, Validation Loss: 16769.888671875\n",
      "Epoch [204/250], Training Loss: 121.71831023047878, Validation Loss: 24321.798828125\n",
      "Epoch [205/250], Training Loss: 150.86755651381813, Validation Loss: 17297.91015625\n",
      "Epoch [206/250], Training Loss: 107.91744779741367, Validation Loss: 17674.2890625\n",
      "Epoch [207/250], Training Loss: 114.43618637494293, Validation Loss: 16557.025390625\n",
      "Epoch [208/250], Training Loss: 89.74150284392351, Validation Loss: 16292.5361328125\n",
      "Epoch [209/250], Training Loss: 112.67357795810209, Validation Loss: 18559.451171875\n",
      "Epoch [210/250], Training Loss: 124.76738710390775, Validation Loss: 15314.97265625\n",
      "Epoch [211/250], Training Loss: 87.70887868542069, Validation Loss: 13852.8349609375\n",
      "Epoch [212/250], Training Loss: 109.88416799394874, Validation Loss: 18630.51953125\n",
      "Epoch [213/250], Training Loss: 118.26831592557427, Validation Loss: 15492.1865234375\n",
      "Epoch [214/250], Training Loss: 103.77903422828784, Validation Loss: 17440.408203125\n",
      "Epoch [215/250], Training Loss: 183.85042209112976, Validation Loss: 16789.80078125\n",
      "Epoch [216/250], Training Loss: 88.82438035831814, Validation Loss: 16911.314453125\n",
      "Epoch [217/250], Training Loss: 122.33949218343676, Validation Loss: 16077.7529296875\n",
      "Epoch [218/250], Training Loss: 78.45664111634954, Validation Loss: 13612.904296875\n",
      "Epoch [219/250], Training Loss: 101.7283493793277, Validation Loss: 14637.4775390625\n",
      "Epoch [220/250], Training Loss: 87.85088118769198, Validation Loss: 11848.1396484375\n",
      "Epoch [221/250], Training Loss: 94.61729682477915, Validation Loss: 16118.28125\n",
      "Epoch [222/250], Training Loss: 107.18535071875023, Validation Loss: 12570.2138671875\n",
      "Epoch [223/250], Training Loss: 81.54126025471153, Validation Loss: 11940.0712890625\n",
      "Epoch [224/250], Training Loss: 93.52949463286386, Validation Loss: 13446.6884765625\n",
      "Epoch [225/250], Training Loss: 86.35322930310895, Validation Loss: 12237.6611328125\n",
      "Epoch [226/250], Training Loss: 65.57135905119465, Validation Loss: 6641.65966796875\n",
      "Epoch [227/250], Training Loss: 118.58481199695723, Validation Loss: 13070.8232421875\n",
      "Epoch [228/250], Training Loss: 86.79289122698754, Validation Loss: 10125.9072265625\n",
      "Epoch [229/250], Training Loss: 77.60319627500064, Validation Loss: 14442.4619140625\n",
      "Epoch [230/250], Training Loss: 76.73391246925672, Validation Loss: 7324.85546875\n",
      "Epoch [231/250], Training Loss: 110.05435257137415, Validation Loss: 13978.43359375\n",
      "Epoch [232/250], Training Loss: 91.51297584201984, Validation Loss: 11619.6572265625\n",
      "Epoch [233/250], Training Loss: 90.49850921892465, Validation Loss: 13082.3984375\n",
      "Epoch [234/250], Training Loss: 152.89195073971592, Validation Loss: 12979.587890625\n",
      "Epoch [235/250], Training Loss: 246.97913201349394, Validation Loss: 15624.869140625\n",
      "Epoch [236/250], Training Loss: 289.18572831846376, Validation Loss: 10357.71484375\n",
      "Epoch [237/250], Training Loss: 90.60832994129072, Validation Loss: 18341.5625\n",
      "Epoch [238/250], Training Loss: 79.20658304657286, Validation Loss: 13523.560546875\n",
      "Epoch [239/250], Training Loss: 83.53025308649221, Validation Loss: 16417.9921875\n",
      "Epoch [240/250], Training Loss: 74.14079763042768, Validation Loss: 14773.923828125\n",
      "Epoch [241/250], Training Loss: 79.58071025637908, Validation Loss: 14256.3955078125\n",
      "Epoch [242/250], Training Loss: 126.23701846757567, Validation Loss: 20807.265625\n",
      "Epoch [243/250], Training Loss: 74.98898642128023, Validation Loss: 9256.2841796875\n",
      "Epoch [244/250], Training Loss: 93.49154866153984, Validation Loss: 18322.4296875\n",
      "Epoch [245/250], Training Loss: 94.15831967814522, Validation Loss: 12616.3017578125\n",
      "Epoch [246/250], Training Loss: 87.49834053723004, Validation Loss: 13370.7138671875\n",
      "Epoch [247/250], Training Loss: 80.88651763376048, Validation Loss: 15159.521484375\n",
      "Epoch [248/250], Training Loss: 128.74555002102286, Validation Loss: 11421.083984375\n",
      "Epoch [249/250], Training Loss: 97.66229139814612, Validation Loss: 14889.58203125\n",
      "Epoch [250/250], Training Loss: 96.7492036912804, Validation Loss: 13975.2451171875\n",
      "Test Loss: 14664.544921875\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.001, 1)\n",
      "Epoch [1/250], Training Loss: 27107.263305805714, Validation Loss: 28092.4765625\n",
      "Epoch [2/250], Training Loss: 24699.646292203826, Validation Loss: 25878.388671875\n",
      "Epoch [3/250], Training Loss: 22927.86562626858, Validation Loss: 24204.240234375\n",
      "Epoch [4/250], Training Loss: 21365.456337465843, Validation Loss: 22686.2890625\n",
      "Epoch [5/250], Training Loss: 19990.484928809965, Validation Loss: 21278.615234375\n",
      "Epoch [6/250], Training Loss: 18762.646988822908, Validation Loss: 20035.283203125\n",
      "Epoch [7/250], Training Loss: 17663.45570689279, Validation Loss: 18894.533203125\n",
      "Epoch [8/250], Training Loss: 16671.066285011242, Validation Loss: 17869.46484375\n",
      "Epoch [9/250], Training Loss: 15775.38474519673, Validation Loss: 16928.421875\n",
      "Epoch [10/250], Training Loss: 14965.388087916053, Validation Loss: 16092.8271484375\n",
      "Epoch [11/250], Training Loss: 14230.556983590044, Validation Loss: 15378.8837890625\n",
      "Epoch [12/250], Training Loss: 13561.092996104939, Validation Loss: 14792.4638671875\n",
      "Epoch [13/250], Training Loss: 12946.728412745037, Validation Loss: 14455.1015625\n",
      "Epoch [14/250], Training Loss: 12379.474143888505, Validation Loss: 13862.2841796875\n",
      "Epoch [15/250], Training Loss: 11849.314596942286, Validation Loss: 13317.03125\n",
      "Epoch [16/250], Training Loss: 11360.548457385956, Validation Loss: 12793.166015625\n",
      "Epoch [17/250], Training Loss: 10909.972897875125, Validation Loss: 12309.353515625\n",
      "Epoch [18/250], Training Loss: 10494.765306281402, Validation Loss: 11865.921875\n",
      "Epoch [19/250], Training Loss: 10111.618667420104, Validation Loss: 11463.8671875\n",
      "Epoch [20/250], Training Loss: 9758.294843413401, Validation Loss: 11105.47265625\n",
      "Epoch [21/250], Training Loss: 9432.664221000263, Validation Loss: 10792.9716796875\n",
      "Epoch [22/250], Training Loss: 9132.500964646195, Validation Loss: 10533.7900390625\n",
      "Epoch [23/250], Training Loss: 8855.036966790265, Validation Loss: 10405.666015625\n",
      "Epoch [24/250], Training Loss: 8576.525842033025, Validation Loss: 12956.43359375\n",
      "Epoch [25/250], Training Loss: 8355.044030632915, Validation Loss: 11569.5712890625\n",
      "Epoch [26/250], Training Loss: 8042.668702855324, Validation Loss: 10565.23046875\n",
      "Epoch [27/250], Training Loss: 7777.9720628985015, Validation Loss: 9854.046875\n",
      "Epoch [28/250], Training Loss: 7535.510018179721, Validation Loss: 9344.302734375\n",
      "Epoch [29/250], Training Loss: 7300.037335616382, Validation Loss: 8942.8076171875\n",
      "Epoch [30/250], Training Loss: 7069.13647428998, Validation Loss: 8590.953125\n",
      "Epoch [31/250], Training Loss: 6842.269590711017, Validation Loss: 8260.119140625\n",
      "Epoch [32/250], Training Loss: 6619.755159298728, Validation Loss: 7933.9619140625\n",
      "Epoch [33/250], Training Loss: 6404.949676707723, Validation Loss: 7610.89404296875\n",
      "Epoch [34/250], Training Loss: 6200.039514887774, Validation Loss: 7308.18798828125\n",
      "Epoch [35/250], Training Loss: 6004.058288931841, Validation Loss: 7037.5283203125\n",
      "Epoch [36/250], Training Loss: 5815.76358743841, Validation Loss: 6797.77587890625\n",
      "Epoch [37/250], Training Loss: 5634.242335128861, Validation Loss: 6581.59521484375\n",
      "Epoch [38/250], Training Loss: 5458.812492365868, Validation Loss: 6381.775390625\n",
      "Epoch [39/250], Training Loss: 5289.049484808197, Validation Loss: 6193.6962890625\n",
      "Epoch [40/250], Training Loss: 5124.688579121383, Validation Loss: 6014.77392578125\n",
      "Epoch [41/250], Training Loss: 4965.609291576145, Validation Loss: 5843.63720703125\n",
      "Epoch [42/250], Training Loss: 4811.699507775415, Validation Loss: 5679.38330078125\n",
      "Epoch [43/250], Training Loss: 4662.802776204497, Validation Loss: 5521.46728515625\n",
      "Epoch [44/250], Training Loss: 4518.7582109398845, Validation Loss: 5369.42626953125\n",
      "Epoch [45/250], Training Loss: 4379.392593375005, Validation Loss: 5222.7783203125\n",
      "Epoch [46/250], Training Loss: 4244.435145757851, Validation Loss: 5081.15087890625\n",
      "Epoch [47/250], Training Loss: 4113.665717410849, Validation Loss: 4944.17041015625\n",
      "Epoch [48/250], Training Loss: 3986.878573201897, Validation Loss: 4811.568359375\n",
      "Epoch [49/250], Training Loss: 3863.840810293173, Validation Loss: 4683.0126953125\n",
      "Epoch [50/250], Training Loss: 3744.3233921163733, Validation Loss: 4558.21484375\n",
      "Epoch [51/250], Training Loss: 3628.1870788789674, Validation Loss: 4436.99560546875\n",
      "Epoch [52/250], Training Loss: 3515.3194053969437, Validation Loss: 4319.2509765625\n",
      "Epoch [53/250], Training Loss: 3405.6773372850853, Validation Loss: 4204.947265625\n",
      "Epoch [54/250], Training Loss: 3299.221202358814, Validation Loss: 4094.026123046875\n",
      "Epoch [55/250], Training Loss: 3195.942846481873, Validation Loss: 3986.538818359375\n",
      "Epoch [56/250], Training Loss: 3095.808136426616, Validation Loss: 3882.44189453125\n",
      "Epoch [57/250], Training Loss: 2998.8522580985796, Validation Loss: 3781.843017578125\n",
      "Epoch [58/250], Training Loss: 2905.080302519576, Validation Loss: 3684.694580078125\n",
      "Epoch [59/250], Training Loss: 2814.55770370349, Validation Loss: 3591.060546875\n",
      "Epoch [60/250], Training Loss: 2727.187405248571, Validation Loss: 3500.776123046875\n",
      "Epoch [61/250], Training Loss: 2643.0209812281914, Validation Loss: 3413.855224609375\n",
      "Epoch [62/250], Training Loss: 2561.9857905401236, Validation Loss: 3330.145751953125\n",
      "Epoch [63/250], Training Loss: 2484.010853035235, Validation Loss: 3249.533447265625\n",
      "Epoch [64/250], Training Loss: 2409.052899710563, Validation Loss: 3171.947021484375\n",
      "Epoch [65/250], Training Loss: 2336.9892329593695, Validation Loss: 3097.221923828125\n",
      "Epoch [66/250], Training Loss: 2267.723612738044, Validation Loss: 3025.206787109375\n",
      "Epoch [67/250], Training Loss: 2201.13275331226, Validation Loss: 2955.802978515625\n",
      "Epoch [68/250], Training Loss: 2137.119377414295, Validation Loss: 2888.89208984375\n",
      "Epoch [69/250], Training Loss: 2075.5257055170696, Validation Loss: 2824.32080078125\n",
      "Epoch [70/250], Training Loss: 2016.234334108357, Validation Loss: 2762.0029296875\n",
      "Epoch [71/250], Training Loss: 1959.1117534913228, Validation Loss: 2701.806640625\n",
      "Epoch [72/250], Training Loss: 1904.041428709609, Validation Loss: 2643.644287109375\n",
      "Epoch [73/250], Training Loss: 1850.8956990574604, Validation Loss: 2587.421875\n",
      "Epoch [74/250], Training Loss: 1799.5741232614191, Validation Loss: 2533.043701171875\n",
      "Epoch [75/250], Training Loss: 1749.996219690561, Validation Loss: 2480.482421875\n",
      "Epoch [76/250], Training Loss: 1702.065246493094, Validation Loss: 2429.665771484375\n",
      "Epoch [77/250], Training Loss: 1655.696623952956, Validation Loss: 2380.553466796875\n",
      "Epoch [78/250], Training Loss: 1610.8379158857786, Validation Loss: 2333.096923828125\n",
      "Epoch [79/250], Training Loss: 1567.4292988429813, Validation Loss: 2287.260498046875\n",
      "Epoch [80/250], Training Loss: 1525.4396068157669, Validation Loss: 2243.033935546875\n",
      "Epoch [81/250], Training Loss: 1484.8009605715572, Validation Loss: 2200.3603515625\n",
      "Epoch [82/250], Training Loss: 1445.4911279023584, Validation Loss: 2159.21337890625\n",
      "Epoch [83/250], Training Loss: 1407.4778593393921, Validation Loss: 2119.604248046875\n",
      "Epoch [84/250], Training Loss: 1370.7054807970735, Validation Loss: 2081.456787109375\n",
      "Epoch [85/250], Training Loss: 1335.1317079531382, Validation Loss: 2044.73583984375\n",
      "Epoch [86/250], Training Loss: 1300.7342416929482, Validation Loss: 2009.438232421875\n",
      "Epoch [87/250], Training Loss: 1267.4687457529703, Validation Loss: 1975.4920654296875\n",
      "Epoch [88/250], Training Loss: 1235.2758790430803, Validation Loss: 1942.802978515625\n",
      "Epoch [89/250], Training Loss: 1204.1250480001263, Validation Loss: 1911.361328125\n",
      "Epoch [90/250], Training Loss: 1173.978682592359, Validation Loss: 1881.0799560546875\n",
      "Epoch [91/250], Training Loss: 1144.7635903906798, Validation Loss: 1851.8758544921875\n",
      "Epoch [92/250], Training Loss: 1116.4546890378322, Validation Loss: 1823.7266845703125\n",
      "Epoch [93/250], Training Loss: 1089.009976461678, Validation Loss: 1796.536376953125\n",
      "Epoch [94/250], Training Loss: 1062.3946505901904, Validation Loss: 1770.2552490234375\n",
      "Epoch [95/250], Training Loss: 1036.5706178383803, Validation Loss: 1744.851318359375\n",
      "Epoch [96/250], Training Loss: 1011.512635486366, Validation Loss: 1720.266845703125\n",
      "Epoch [97/250], Training Loss: 987.1723192030768, Validation Loss: 1696.4554443359375\n",
      "Epoch [98/250], Training Loss: 963.5175568387439, Validation Loss: 1673.3905029296875\n",
      "Epoch [99/250], Training Loss: 940.5249566355615, Validation Loss: 1651.0263671875\n",
      "Epoch [100/250], Training Loss: 918.153842249532, Validation Loss: 1629.342529296875\n",
      "Epoch [101/250], Training Loss: 896.383860892789, Validation Loss: 1608.274169921875\n",
      "Epoch [102/250], Training Loss: 875.2194836701974, Validation Loss: 1587.8421630859375\n",
      "Epoch [103/250], Training Loss: 854.6188231492038, Validation Loss: 1568.003662109375\n",
      "Epoch [104/250], Training Loss: 834.5803589745536, Validation Loss: 1548.75048828125\n",
      "Epoch [105/250], Training Loss: 815.1012383943988, Validation Loss: 1530.0655517578125\n",
      "Epoch [106/250], Training Loss: 796.1435384651625, Validation Loss: 1511.9541015625\n",
      "Epoch [107/250], Training Loss: 777.696438036997, Validation Loss: 1494.3843994140625\n",
      "Epoch [108/250], Training Loss: 759.7356462834058, Validation Loss: 1477.2965087890625\n",
      "Epoch [109/250], Training Loss: 742.2667598225651, Validation Loss: 1460.6998291015625\n",
      "Epoch [110/250], Training Loss: 725.2685093782144, Validation Loss: 1444.5758056640625\n",
      "Epoch [111/250], Training Loss: 708.7253368969419, Validation Loss: 1428.9151611328125\n",
      "Epoch [112/250], Training Loss: 692.6317518846764, Validation Loss: 1413.6983642578125\n",
      "Epoch [113/250], Training Loss: 676.9874768013403, Validation Loss: 1398.8831787109375\n",
      "Epoch [114/250], Training Loss: 661.7679509776685, Validation Loss: 1384.471435546875\n",
      "Epoch [115/250], Training Loss: 646.9654859033828, Validation Loss: 1370.435546875\n",
      "Epoch [116/250], Training Loss: 632.578432351547, Validation Loss: 1356.770263671875\n",
      "Epoch [117/250], Training Loss: 618.5867940108258, Validation Loss: 1343.4283447265625\n",
      "Epoch [118/250], Training Loss: 604.9801792349394, Validation Loss: 1330.3929443359375\n",
      "Epoch [119/250], Training Loss: 591.7526016752242, Validation Loss: 1317.6419677734375\n",
      "Epoch [120/250], Training Loss: 578.9038628394791, Validation Loss: 1305.1636962890625\n",
      "Epoch [121/250], Training Loss: 566.4023909444838, Validation Loss: 1292.9100341796875\n",
      "Epoch [122/250], Training Loss: 554.246463349205, Validation Loss: 1280.8568115234375\n",
      "Epoch [123/250], Training Loss: 542.4362164872226, Validation Loss: 1268.97021484375\n",
      "Epoch [124/250], Training Loss: 530.9615812747694, Validation Loss: 1257.2545166015625\n",
      "Epoch [125/250], Training Loss: 519.8063533620202, Validation Loss: 1245.6759033203125\n",
      "Epoch [126/250], Training Loss: 508.94625828630234, Validation Loss: 1234.1954345703125\n",
      "Epoch [127/250], Training Loss: 498.37053415463186, Validation Loss: 1222.7979736328125\n",
      "Epoch [128/250], Training Loss: 488.0756960234189, Validation Loss: 1211.4429931640625\n",
      "Epoch [129/250], Training Loss: 478.04689056522506, Validation Loss: 1200.1055908203125\n",
      "Epoch [130/250], Training Loss: 468.27952415817117, Validation Loss: 1188.8182373046875\n",
      "Epoch [131/250], Training Loss: 458.7550606660697, Validation Loss: 1177.5482177734375\n",
      "Epoch [132/250], Training Loss: 449.46460961722414, Validation Loss: 1166.31103515625\n",
      "Epoch [133/250], Training Loss: 440.38868389220573, Validation Loss: 1155.1341552734375\n",
      "Epoch [134/250], Training Loss: 431.51386257633646, Validation Loss: 1144.0009765625\n",
      "Epoch [135/250], Training Loss: 422.84040706180724, Validation Loss: 1132.9359130859375\n",
      "Epoch [136/250], Training Loss: 414.3529689123734, Validation Loss: 1121.990234375\n",
      "Epoch [137/250], Training Loss: 406.04063498864974, Validation Loss: 1111.1611328125\n",
      "Epoch [138/250], Training Loss: 397.89826362171874, Validation Loss: 1100.4610595703125\n",
      "Epoch [139/250], Training Loss: 389.9344148925543, Validation Loss: 1089.938720703125\n",
      "Epoch [140/250], Training Loss: 382.13850482152964, Validation Loss: 1079.6243896484375\n",
      "Epoch [141/250], Training Loss: 374.50022918879284, Validation Loss: 1069.4998779296875\n",
      "Epoch [142/250], Training Loss: 367.0175587537068, Validation Loss: 1059.5745849609375\n",
      "Epoch [143/250], Training Loss: 359.70959913394546, Validation Loss: 1049.8226318359375\n",
      "Epoch [144/250], Training Loss: 352.57356219845207, Validation Loss: 1040.243408203125\n",
      "Epoch [145/250], Training Loss: 345.59780631644054, Validation Loss: 1030.787353515625\n",
      "Epoch [146/250], Training Loss: 338.781763532367, Validation Loss: 1021.4356689453125\n",
      "Epoch [147/250], Training Loss: 332.1253147628166, Validation Loss: 1012.1412353515625\n",
      "Epoch [148/250], Training Loss: 325.63588088455793, Validation Loss: 1002.9026489257812\n",
      "Epoch [149/250], Training Loss: 319.31501656624863, Validation Loss: 993.6920776367188\n",
      "Epoch [150/250], Training Loss: 313.15183262750435, Validation Loss: 984.4912109375\n",
      "Epoch [151/250], Training Loss: 307.13681614095685, Validation Loss: 975.2634887695312\n",
      "Epoch [152/250], Training Loss: 301.2875046028339, Validation Loss: 966.0496215820312\n",
      "Epoch [153/250], Training Loss: 295.5797175995928, Validation Loss: 956.8123779296875\n",
      "Epoch [154/250], Training Loss: 290.01853748714916, Validation Loss: 947.5684204101562\n",
      "Epoch [155/250], Training Loss: 284.60979731082716, Validation Loss: 938.4144897460938\n",
      "Epoch [156/250], Training Loss: 279.3306471031192, Validation Loss: 929.3300170898438\n",
      "Epoch [157/250], Training Loss: 274.18164803850675, Validation Loss: 920.3382568359375\n",
      "Epoch [158/250], Training Loss: 269.1712547084017, Validation Loss: 911.4892578125\n",
      "Epoch [159/250], Training Loss: 264.27517257867174, Validation Loss: 902.7758178710938\n",
      "Epoch [160/250], Training Loss: 259.50125499102444, Validation Loss: 894.2324829101562\n",
      "Epoch [161/250], Training Loss: 254.83995125744622, Validation Loss: 885.8488159179688\n",
      "Epoch [162/250], Training Loss: 250.2964666586177, Validation Loss: 877.6557006835938\n",
      "Epoch [163/250], Training Loss: 245.85827017202828, Validation Loss: 869.6502685546875\n",
      "Epoch [164/250], Training Loss: 241.5217887619872, Validation Loss: 861.822998046875\n",
      "Epoch [165/250], Training Loss: 237.28525710380828, Validation Loss: 854.1514892578125\n",
      "Epoch [166/250], Training Loss: 233.1463190236263, Validation Loss: 846.6632690429688\n",
      "Epoch [167/250], Training Loss: 229.09993647902743, Validation Loss: 839.317626953125\n",
      "Epoch [168/250], Training Loss: 225.14619377553754, Validation Loss: 832.120361328125\n",
      "Epoch [169/250], Training Loss: 221.28040126448428, Validation Loss: 825.0489501953125\n",
      "Epoch [170/250], Training Loss: 217.50413713311073, Validation Loss: 818.0648803710938\n",
      "Epoch [171/250], Training Loss: 213.8186486300688, Validation Loss: 811.2023315429688\n",
      "Epoch [172/250], Training Loss: 210.21170709220075, Validation Loss: 804.3822021484375\n",
      "Epoch [173/250], Training Loss: 206.6911987615216, Validation Loss: 797.6281127929688\n",
      "Epoch [174/250], Training Loss: 203.25632257252877, Validation Loss: 790.9349365234375\n",
      "Epoch [175/250], Training Loss: 199.9008506738808, Validation Loss: 784.2393798828125\n",
      "Epoch [176/250], Training Loss: 196.62842626301037, Validation Loss: 777.5504150390625\n",
      "Epoch [177/250], Training Loss: 193.43730918299778, Validation Loss: 770.845703125\n",
      "Epoch [178/250], Training Loss: 190.3267809590073, Validation Loss: 764.1300048828125\n",
      "Epoch [179/250], Training Loss: 187.29251894602993, Validation Loss: 757.3734741210938\n",
      "Epoch [180/250], Training Loss: 184.32984406141804, Validation Loss: 750.5595703125\n",
      "Epoch [181/250], Training Loss: 181.4398926769418, Validation Loss: 743.6807250976562\n",
      "Epoch [182/250], Training Loss: 178.6255064813223, Validation Loss: 736.770751953125\n",
      "Epoch [183/250], Training Loss: 175.885597380136, Validation Loss: 729.8252563476562\n",
      "Epoch [184/250], Training Loss: 173.21218230655845, Validation Loss: 722.8450927734375\n",
      "Epoch [185/250], Training Loss: 170.60880403793362, Validation Loss: 715.8779296875\n",
      "Epoch [186/250], Training Loss: 168.07048629145655, Validation Loss: 708.9266967773438\n",
      "Epoch [187/250], Training Loss: 165.59779013125566, Validation Loss: 702.02685546875\n",
      "Epoch [188/250], Training Loss: 163.18391828799201, Validation Loss: 695.184326171875\n",
      "Epoch [189/250], Training Loss: 160.83056643487325, Validation Loss: 688.4332885742188\n",
      "Epoch [190/250], Training Loss: 158.5329733602367, Validation Loss: 681.7839965820312\n",
      "Epoch [191/250], Training Loss: 156.2940555634007, Validation Loss: 675.2727661132812\n",
      "Epoch [192/250], Training Loss: 154.10901095853154, Validation Loss: 668.9088745117188\n",
      "Epoch [193/250], Training Loss: 151.97361330756098, Validation Loss: 662.6959838867188\n",
      "Epoch [194/250], Training Loss: 149.88828700733387, Validation Loss: 656.6450805664062\n",
      "Epoch [195/250], Training Loss: 147.84805660358333, Validation Loss: 650.7623901367188\n",
      "Epoch [196/250], Training Loss: 145.84887843415174, Validation Loss: 645.047607421875\n",
      "Epoch [197/250], Training Loss: 143.89469197440778, Validation Loss: 639.5066528320312\n",
      "Epoch [198/250], Training Loss: 141.98155071733777, Validation Loss: 634.1431274414062\n",
      "Epoch [199/250], Training Loss: 140.10766019323094, Validation Loss: 628.9612426757812\n",
      "Epoch [200/250], Training Loss: 138.27333124844898, Validation Loss: 623.9567260742188\n",
      "Epoch [201/250], Training Loss: 136.47673578573577, Validation Loss: 619.1348266601562\n",
      "Epoch [202/250], Training Loss: 134.71280774235723, Validation Loss: 614.4813232421875\n",
      "Epoch [203/250], Training Loss: 132.9842628914245, Validation Loss: 609.9991455078125\n",
      "Epoch [204/250], Training Loss: 131.2898087377558, Validation Loss: 605.6866455078125\n",
      "Epoch [205/250], Training Loss: 129.62659941123727, Validation Loss: 601.5370483398438\n",
      "Epoch [206/250], Training Loss: 127.99500883217699, Validation Loss: 597.55224609375\n",
      "Epoch [207/250], Training Loss: 126.39769871856333, Validation Loss: 593.7291259765625\n",
      "Epoch [208/250], Training Loss: 124.82482992907347, Validation Loss: 590.050048828125\n",
      "Epoch [209/250], Training Loss: 123.28347584908455, Validation Loss: 586.5177612304688\n",
      "Epoch [210/250], Training Loss: 121.76887284819053, Validation Loss: 583.1210327148438\n",
      "Epoch [211/250], Training Loss: 120.28122657864976, Validation Loss: 579.861328125\n",
      "Epoch [212/250], Training Loss: 118.81878813424268, Validation Loss: 576.7155151367188\n",
      "Epoch [213/250], Training Loss: 117.38284282765544, Validation Loss: 573.6832885742188\n",
      "Epoch [214/250], Training Loss: 115.9747855965815, Validation Loss: 570.7603149414062\n",
      "Epoch [215/250], Training Loss: 114.59269646097044, Validation Loss: 567.9396362304688\n",
      "Epoch [216/250], Training Loss: 113.23737306076816, Validation Loss: 565.2136840820312\n",
      "Epoch [217/250], Training Loss: 111.90623366590646, Validation Loss: 562.568359375\n",
      "Epoch [218/250], Training Loss: 110.60111598877852, Validation Loss: 559.9998779296875\n",
      "Epoch [219/250], Training Loss: 109.32184685705116, Validation Loss: 557.504150390625\n",
      "Epoch [220/250], Training Loss: 108.06616547292082, Validation Loss: 555.0728149414062\n",
      "Epoch [221/250], Training Loss: 106.83775204459094, Validation Loss: 552.7028198242188\n",
      "Epoch [222/250], Training Loss: 105.6323827681836, Validation Loss: 550.3901977539062\n",
      "Epoch [223/250], Training Loss: 104.45139571224877, Validation Loss: 548.135498046875\n",
      "Epoch [224/250], Training Loss: 103.29900683488512, Validation Loss: 545.9359741210938\n",
      "Epoch [225/250], Training Loss: 102.1695027026161, Validation Loss: 543.7952270507812\n",
      "Epoch [226/250], Training Loss: 101.06513381762781, Validation Loss: 541.7091064453125\n",
      "Epoch [227/250], Training Loss: 99.98635952973726, Validation Loss: 539.683837890625\n",
      "Epoch [228/250], Training Loss: 98.93187585446763, Validation Loss: 537.7200317382812\n",
      "Epoch [229/250], Training Loss: 97.90258134389387, Validation Loss: 535.8129272460938\n",
      "Epoch [230/250], Training Loss: 96.89677207530183, Validation Loss: 533.96826171875\n",
      "Epoch [231/250], Training Loss: 95.91297801050632, Validation Loss: 532.1839599609375\n",
      "Epoch [232/250], Training Loss: 94.94877590268746, Validation Loss: 530.4607543945312\n",
      "Epoch [233/250], Training Loss: 94.0055063540004, Validation Loss: 528.8001708984375\n",
      "Epoch [234/250], Training Loss: 93.08257299301101, Validation Loss: 527.2039794921875\n",
      "Epoch [235/250], Training Loss: 92.17888406967342, Validation Loss: 525.6765747070312\n",
      "Epoch [236/250], Training Loss: 91.29358525535773, Validation Loss: 524.2219848632812\n",
      "Epoch [237/250], Training Loss: 90.43039797237209, Validation Loss: 522.8483276367188\n",
      "Epoch [238/250], Training Loss: 89.58600167374185, Validation Loss: 521.5537719726562\n",
      "Epoch [239/250], Training Loss: 88.75810218700204, Validation Loss: 520.3396606445312\n",
      "Epoch [240/250], Training Loss: 87.9496532546575, Validation Loss: 519.2085571289062\n",
      "Epoch [241/250], Training Loss: 87.15897178585715, Validation Loss: 518.1500244140625\n",
      "Epoch [242/250], Training Loss: 86.38281458022415, Validation Loss: 517.1729736328125\n",
      "Epoch [243/250], Training Loss: 85.62269162624139, Validation Loss: 516.2671508789062\n",
      "Epoch [244/250], Training Loss: 84.87795521415414, Validation Loss: 515.4339599609375\n",
      "Epoch [245/250], Training Loss: 84.14743007512844, Validation Loss: 514.6658935546875\n",
      "Epoch [246/250], Training Loss: 83.43071782416757, Validation Loss: 513.9714965820312\n",
      "Epoch [247/250], Training Loss: 82.72939911035495, Validation Loss: 513.3441162109375\n",
      "Epoch [248/250], Training Loss: 82.04155927961793, Validation Loss: 512.7841186523438\n",
      "Epoch [249/250], Training Loss: 81.36769906992194, Validation Loss: 512.296142578125\n",
      "Epoch [250/250], Training Loss: 80.70505259091952, Validation Loss: 511.8687744140625\n",
      "Test Loss: 486.8566589355469\n",
      "\n",
      "Hyperparameters: (7, 1, 1, 0.001, 2)\n",
      "Epoch [1/250], Training Loss: 26539.20194696237, Validation Loss: 27426.388671875\n",
      "Epoch [2/250], Training Loss: 24046.46383529169, Validation Loss: 25173.8125\n",
      "Epoch [3/250], Training Loss: 22190.09093624594, Validation Loss: 23501.875\n",
      "Epoch [4/250], Training Loss: 20559.48541403123, Validation Loss: 22117.263671875\n",
      "Epoch [5/250], Training Loss: 19137.65947580553, Validation Loss: 20954.8984375\n",
      "Epoch [6/250], Training Loss: 17878.76300001006, Validation Loss: 19942.935546875\n",
      "Epoch [7/250], Training Loss: 16753.62031006754, Validation Loss: 19089.015625\n",
      "Epoch [8/250], Training Loss: 15746.898675073831, Validation Loss: 18408.51953125\n",
      "Epoch [9/250], Training Loss: 14845.721062757722, Validation Loss: 18007.36328125\n",
      "Epoch [10/250], Training Loss: 14031.431372019279, Validation Loss: 17837.58984375\n",
      "Epoch [11/250], Training Loss: 13291.994510338474, Validation Loss: 18038.19140625\n",
      "Epoch [12/250], Training Loss: 12610.200147823229, Validation Loss: 18001.052734375\n",
      "Epoch [13/250], Training Loss: 13541.18667329516, Validation Loss: 18236.974609375\n",
      "Epoch [14/250], Training Loss: 14803.65564917832, Validation Loss: 17961.96484375\n",
      "Epoch [15/250], Training Loss: 14559.92536272346, Validation Loss: 15593.6240234375\n",
      "Epoch [16/250], Training Loss: 14278.586775217522, Validation Loss: 14919.2919921875\n",
      "Epoch [17/250], Training Loss: 13673.929294068368, Validation Loss: 15763.87109375\n",
      "Epoch [18/250], Training Loss: 13049.551141526736, Validation Loss: 15399.8408203125\n",
      "Epoch [19/250], Training Loss: 12078.606653759569, Validation Loss: 15241.1796875\n",
      "Epoch [20/250], Training Loss: 11315.29126768679, Validation Loss: 14950.2373046875\n",
      "Epoch [21/250], Training Loss: 10660.354807328762, Validation Loss: 14465.87109375\n",
      "Epoch [22/250], Training Loss: 10163.143093816403, Validation Loss: 14208.796875\n",
      "Epoch [23/250], Training Loss: 10053.317486009155, Validation Loss: 14532.212890625\n",
      "Epoch [24/250], Training Loss: 10723.81435842035, Validation Loss: 15750.8486328125\n",
      "Epoch [25/250], Training Loss: 10672.170185162226, Validation Loss: 16078.1337890625\n",
      "Epoch [26/250], Training Loss: 9860.843112356451, Validation Loss: 14714.158203125\n",
      "Epoch [27/250], Training Loss: 10450.96064699806, Validation Loss: 15057.2939453125\n",
      "Epoch [28/250], Training Loss: 9143.556241717068, Validation Loss: 12717.462890625\n",
      "Epoch [29/250], Training Loss: 8299.558148024751, Validation Loss: 11843.0048828125\n",
      "Epoch [30/250], Training Loss: 7428.358936789685, Validation Loss: 17014.900390625\n",
      "Epoch [31/250], Training Loss: 7462.289958532417, Validation Loss: 17735.365234375\n",
      "Epoch [32/250], Training Loss: 6796.121280475955, Validation Loss: 21993.609375\n",
      "Epoch [33/250], Training Loss: 9186.480144570813, Validation Loss: 15579.4609375\n",
      "Epoch [34/250], Training Loss: 8685.998459312119, Validation Loss: 14530.1728515625\n",
      "Epoch [35/250], Training Loss: 8260.03901708392, Validation Loss: 14578.9130859375\n",
      "Epoch [36/250], Training Loss: 7906.289451212898, Validation Loss: 14610.1806640625\n",
      "Epoch [37/250], Training Loss: 7629.955904788834, Validation Loss: 14808.8525390625\n",
      "Epoch [38/250], Training Loss: 7396.664544304602, Validation Loss: 14877.23828125\n",
      "Epoch [39/250], Training Loss: 7183.57638752097, Validation Loss: 14876.1494140625\n",
      "Epoch [40/250], Training Loss: 7003.276785689544, Validation Loss: 14815.033203125\n",
      "Epoch [41/250], Training Loss: 6834.173528538691, Validation Loss: 14955.6884765625\n",
      "Epoch [42/250], Training Loss: 6653.700117086596, Validation Loss: 15115.6806640625\n",
      "Epoch [43/250], Training Loss: 6504.701897530424, Validation Loss: 15267.0634765625\n",
      "Epoch [44/250], Training Loss: 6610.627437262536, Validation Loss: 16013.7861328125\n",
      "Epoch [45/250], Training Loss: 4727.587396615004, Validation Loss: 14327.6103515625\n",
      "Epoch [46/250], Training Loss: 4261.943632968631, Validation Loss: 16137.958984375\n",
      "Epoch [47/250], Training Loss: 4082.464577565353, Validation Loss: 15728.775390625\n",
      "Epoch [48/250], Training Loss: 3928.454235695164, Validation Loss: 15465.228515625\n",
      "Epoch [49/250], Training Loss: 3787.532361227615, Validation Loss: 15007.3466796875\n",
      "Epoch [50/250], Training Loss: 3656.507690472887, Validation Loss: 14603.5546875\n",
      "Epoch [51/250], Training Loss: 3532.3741506261604, Validation Loss: 14378.75390625\n",
      "Epoch [52/250], Training Loss: 3413.187729155334, Validation Loss: 14323.9130859375\n",
      "Epoch [53/250], Training Loss: 3298.104399498377, Validation Loss: 14405.490234375\n",
      "Epoch [54/250], Training Loss: 3187.137701545659, Validation Loss: 14546.794921875\n",
      "Epoch [55/250], Training Loss: 3080.3768187869896, Validation Loss: 14695.314453125\n",
      "Epoch [56/250], Training Loss: 2977.561515697788, Validation Loss: 14869.630859375\n",
      "Epoch [57/250], Training Loss: 2878.5748052889476, Validation Loss: 15089.7744140625\n",
      "Epoch [58/250], Training Loss: 2783.314475113246, Validation Loss: 15351.73046875\n",
      "Epoch [59/250], Training Loss: 2691.821538107009, Validation Loss: 15641.5126953125\n",
      "Epoch [60/250], Training Loss: 2604.1274877347923, Validation Loss: 15945.748046875\n",
      "Epoch [61/250], Training Loss: 2520.2125648127762, Validation Loss: 16260.517578125\n",
      "Epoch [62/250], Training Loss: 2439.891732757602, Validation Loss: 16922.853515625\n",
      "Epoch [63/250], Training Loss: 2362.3055875649197, Validation Loss: 17538.142578125\n",
      "Epoch [64/250], Training Loss: 2284.662463339068, Validation Loss: 18432.314453125\n",
      "Epoch [65/250], Training Loss: 2211.970126437886, Validation Loss: 19064.328125\n",
      "Epoch [66/250], Training Loss: 2143.6352026662403, Validation Loss: 19853.9296875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 45\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\optim\\optimizer.py:461\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[1;32m--> 461\u001b[0m         \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [1, 2, 4, 8, 16, 32]\n",
    "num_layers_list = [1, 2, 3, 4, 5, 6 ,7, 8]\n",
    "learning_rates = [0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001 ]\n",
    "window_sizes = [1, 2, 5, 10, 15, 20, 25, 30]\n",
    "num_epochs = 250\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "best_validation_loss = float('inf')\n",
    "best_test_loss = float('inf')\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for hyperparameters in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparameters\n",
    "\n",
    "    print()\n",
    "    print(\"Hyperparameters:\", hyperparameters)\n",
    "\n",
    "    # Initialize the model with current hyperparameters\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "            avg_train_loss = running_loss / len(x_train_tensor)\n",
    "            avg_val_loss = val_loss / len(x_val_tensor)\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    avg_test_loss = test_loss / len(x_test_tensor)\n",
    "    print(f'Test Loss: {avg_test_loss}')\n",
    "\n",
    "    # Update best hyperparameters if test loss is lower\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        best_hyperparameters = hyperparameters\n",
    "\n",
    "print()\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best Test Loss:\", best_test_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
