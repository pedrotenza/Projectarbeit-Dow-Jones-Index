{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "#print(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "y_data = data[\"close\"].values.reshape(-1, 1)  # Reshape y_data if it's 1D\n",
    "\n",
    "# Initialize the scalers\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both training and test data\n",
    "x_data_scaled = scaler_x.fit_transform(x_data)\n",
    "y_data_scaled = scaler_y.fit_transform(y_data)\n",
    "\n",
    "# Convert scaled data to Tensors\n",
    "x_feature_tensors = torch.tensor(x_data_scaled, dtype=torch.float32)\n",
    "y_feature_tensors = torch.tensor(y_data_scaled, dtype=torch.float32)\n",
    "\n",
    "# Split the training data into training and temporary sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_feature_tensors, y_feature_tensors, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert scaled labels back to numpy arrays\n",
    "y_train = y_train.numpy()\n",
    "y_val = y_val.numpy()\n",
    "y_test = y_test.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Now you can use x_train_scaled, x_val, x_test, y_train_scaled, y_val_scaled, y_test_scaled for your model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 4\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [10/8587], Loss: 0.0000\n",
      "Epoch [1/1], Step [20/8587], Loss: 0.0000\n",
      "Epoch [1/1], Step [30/8587], Loss: 0.0023\n",
      "Epoch [1/1], Step [40/8587], Loss: 0.3514\n",
      "Epoch [1/1], Step [50/8587], Loss: 0.3130\n",
      "Epoch [1/1], Step [60/8587], Loss: 0.3411\n",
      "Epoch [1/1], Step [70/8587], Loss: 0.4858\n",
      "Epoch [1/1], Step [80/8587], Loss: 0.5881\n",
      "Epoch [1/1], Step [90/8587], Loss: 0.0832\n",
      "Epoch [1/1], Step [100/8587], Loss: 0.0226\n",
      "Epoch [1/1], Step [110/8587], Loss: 0.5159\n",
      "Epoch [1/1], Step [120/8587], Loss: 0.1984\n",
      "Epoch [1/1], Step [130/8587], Loss: 0.5195\n",
      "Epoch [1/1], Step [140/8587], Loss: 0.5664\n",
      "Epoch [1/1], Step [150/8587], Loss: 0.0289\n",
      "Epoch [1/1], Step [160/8587], Loss: 0.2892\n",
      "Epoch [1/1], Step [170/8587], Loss: 4.7869\n",
      "Epoch [1/1], Step [180/8587], Loss: 0.0366\n",
      "Epoch [1/1], Step [190/8587], Loss: 0.3257\n",
      "Epoch [1/1], Step [200/8587], Loss: 0.1999\n",
      "Epoch [1/1], Step [210/8587], Loss: 0.0235\n",
      "Epoch [1/1], Step [220/8587], Loss: 0.1390\n",
      "Epoch [1/1], Step [230/8587], Loss: 0.4613\n",
      "Epoch [1/1], Step [240/8587], Loss: 0.4831\n",
      "Epoch [1/1], Step [250/8587], Loss: 0.5469\n",
      "Epoch [1/1], Step [260/8587], Loss: 0.4157\n",
      "Epoch [1/1], Step [270/8587], Loss: 0.3590\n",
      "Epoch [1/1], Step [280/8587], Loss: 0.2995\n",
      "Epoch [1/1], Step [290/8587], Loss: 0.5496\n",
      "Epoch [1/1], Step [300/8587], Loss: 0.4836\n",
      "Epoch [1/1], Step [310/8587], Loss: 0.3987\n",
      "Epoch [1/1], Step [320/8587], Loss: 0.0007\n",
      "Epoch [1/1], Step [330/8587], Loss: 0.3382\n",
      "Epoch [1/1], Step [340/8587], Loss: 0.1156\n",
      "Epoch [1/1], Step [350/8587], Loss: 0.1926\n",
      "Epoch [1/1], Step [360/8587], Loss: 0.2663\n",
      "Epoch [1/1], Step [370/8587], Loss: 0.4304\n",
      "Epoch [1/1], Step [380/8587], Loss: 0.5771\n",
      "Epoch [1/1], Step [390/8587], Loss: 0.4948\n",
      "Epoch [1/1], Step [400/8587], Loss: 0.0295\n",
      "Epoch [1/1], Step [410/8587], Loss: 0.5193\n",
      "Epoch [1/1], Step [420/8587], Loss: 0.3896\n",
      "Epoch [1/1], Step [430/8587], Loss: 0.1877\n",
      "Epoch [1/1], Step [440/8587], Loss: 0.4927\n",
      "Epoch [1/1], Step [450/8587], Loss: 0.0012\n",
      "Epoch [1/1], Step [460/8587], Loss: 0.1833\n",
      "Epoch [1/1], Step [470/8587], Loss: 0.1951\n",
      "Epoch [1/1], Step [480/8587], Loss: 0.5387\n",
      "Epoch [1/1], Step [490/8587], Loss: 0.5429\n",
      "Epoch [1/1], Step [500/8587], Loss: 0.0126\n",
      "Epoch [1/1], Step [510/8587], Loss: 0.4040\n",
      "Epoch [1/1], Step [520/8587], Loss: 0.1093\n",
      "Epoch [1/1], Step [530/8587], Loss: 0.4197\n",
      "Epoch [1/1], Step [540/8587], Loss: 0.2559\n",
      "Epoch [1/1], Step [550/8587], Loss: 0.5656\n",
      "Epoch [1/1], Step [560/8587], Loss: 0.0147\n",
      "Epoch [1/1], Step [570/8587], Loss: 0.2932\n",
      "Epoch [1/1], Step [580/8587], Loss: 0.0689\n",
      "Epoch [1/1], Step [590/8587], Loss: 0.4437\n",
      "Epoch [1/1], Step [600/8587], Loss: 0.2200\n",
      "Epoch [1/1], Step [610/8587], Loss: 1.2784\n",
      "Epoch [1/1], Step [620/8587], Loss: 0.2261\n",
      "Epoch [1/1], Step [630/8587], Loss: 0.4717\n",
      "Epoch [1/1], Step [640/8587], Loss: 0.0037\n",
      "Epoch [1/1], Step [650/8587], Loss: 0.7101\n",
      "Epoch [1/1], Step [660/8587], Loss: 0.3475\n",
      "Epoch [1/1], Step [670/8587], Loss: 0.0696\n",
      "Epoch [1/1], Step [680/8587], Loss: 0.2956\n",
      "Epoch [1/1], Step [690/8587], Loss: 0.4951\n",
      "Epoch [1/1], Step [700/8587], Loss: 0.0084\n",
      "Epoch [1/1], Step [710/8587], Loss: 0.0225\n",
      "Epoch [1/1], Step [720/8587], Loss: 1.9143\n",
      "Epoch [1/1], Step [730/8587], Loss: 0.5119\n",
      "Epoch [1/1], Step [740/8587], Loss: 0.3344\n",
      "Epoch [1/1], Step [750/8587], Loss: 0.0094\n",
      "Epoch [1/1], Step [760/8587], Loss: 2.8869\n",
      "Epoch [1/1], Step [770/8587], Loss: 0.3918\n",
      "Epoch [1/1], Step [780/8587], Loss: 8.8646\n",
      "Epoch [1/1], Step [790/8587], Loss: 0.3073\n",
      "Epoch [1/1], Step [800/8587], Loss: 0.3415\n",
      "Epoch [1/1], Step [810/8587], Loss: 2.3969\n",
      "Epoch [1/1], Step [820/8587], Loss: 0.0011\n",
      "Epoch [1/1], Step [830/8587], Loss: 0.0058\n",
      "Epoch [1/1], Step [840/8587], Loss: 0.1508\n",
      "Epoch [1/1], Step [850/8587], Loss: 0.0031\n",
      "Epoch [1/1], Step [860/8587], Loss: 0.4273\n",
      "Epoch [1/1], Step [870/8587], Loss: 0.0994\n",
      "Epoch [1/1], Step [880/8587], Loss: 0.3462\n",
      "Epoch [1/1], Step [890/8587], Loss: 0.2195\n",
      "Epoch [1/1], Step [900/8587], Loss: 0.1933\n",
      "Epoch [1/1], Step [910/8587], Loss: 0.2145\n",
      "Epoch [1/1], Step [920/8587], Loss: 0.0633\n",
      "Epoch [1/1], Step [930/8587], Loss: 0.0204\n",
      "Epoch [1/1], Step [940/8587], Loss: 0.0081\n",
      "Epoch [1/1], Step [950/8587], Loss: 0.0019\n",
      "Epoch [1/1], Step [960/8587], Loss: 0.1142\n",
      "Epoch [1/1], Step [970/8587], Loss: 0.3435\n",
      "Epoch [1/1], Step [980/8587], Loss: 0.4196\n",
      "Epoch [1/1], Step [990/8587], Loss: 0.1064\n",
      "Epoch [1/1], Step [1000/8587], Loss: 0.4045\n",
      "Epoch [1/1], Step [1010/8587], Loss: 0.1999\n",
      "Epoch [1/1], Step [1020/8587], Loss: 0.1937\n",
      "Epoch [1/1], Step [1030/8587], Loss: 0.0005\n",
      "Epoch [1/1], Step [1040/8587], Loss: 0.5367\n",
      "Epoch [1/1], Step [1050/8587], Loss: 0.2853\n",
      "Epoch [1/1], Step [1060/8587], Loss: 0.1344\n",
      "Epoch [1/1], Step [1070/8587], Loss: 0.1558\n",
      "Epoch [1/1], Step [1080/8587], Loss: 0.0083\n",
      "Epoch [1/1], Step [1090/8587], Loss: 0.2299\n",
      "Epoch [1/1], Step [1100/8587], Loss: 0.3093\n",
      "Epoch [1/1], Step [1110/8587], Loss: 0.2698\n",
      "Epoch [1/1], Step [1120/8587], Loss: 0.3756\n",
      "Epoch [1/1], Step [1130/8587], Loss: 0.1912\n",
      "Epoch [1/1], Step [1140/8587], Loss: 0.1612\n",
      "Epoch [1/1], Step [1150/8587], Loss: 0.1991\n",
      "Epoch [1/1], Step [1160/8587], Loss: 12.2885\n",
      "Epoch [1/1], Step [1170/8587], Loss: 0.2706\n",
      "Epoch [1/1], Step [1180/8587], Loss: 0.0109\n",
      "Epoch [1/1], Step [1190/8587], Loss: 0.4468\n",
      "Epoch [1/1], Step [1200/8587], Loss: 0.2024\n",
      "Epoch [1/1], Step [1210/8587], Loss: 0.1460\n",
      "Epoch [1/1], Step [1220/8587], Loss: 0.4500\n",
      "Epoch [1/1], Step [1230/8587], Loss: 0.5446\n",
      "Epoch [1/1], Step [1240/8587], Loss: 0.1751\n",
      "Epoch [1/1], Step [1250/8587], Loss: 12.2815\n",
      "Epoch [1/1], Step [1260/8587], Loss: 0.0376\n",
      "Epoch [1/1], Step [1270/8587], Loss: 0.1941\n",
      "Epoch [1/1], Step [1280/8587], Loss: 0.3467\n",
      "Epoch [1/1], Step [1290/8587], Loss: 0.4984\n",
      "Epoch [1/1], Step [1300/8587], Loss: 0.5570\n",
      "Epoch [1/1], Step [1310/8587], Loss: 0.3876\n",
      "Epoch [1/1], Step [1320/8587], Loss: 0.6681\n",
      "Epoch [1/1], Step [1330/8587], Loss: 0.0129\n",
      "Epoch [1/1], Step [1340/8587], Loss: 0.3617\n",
      "Epoch [1/1], Step [1350/8587], Loss: 0.3642\n",
      "Epoch [1/1], Step [1360/8587], Loss: 0.3553\n",
      "Epoch [1/1], Step [1370/8587], Loss: 0.3559\n",
      "Epoch [1/1], Step [1380/8587], Loss: 0.3603\n",
      "Epoch [1/1], Step [1390/8587], Loss: 0.3797\n",
      "Epoch [1/1], Step [1400/8587], Loss: 0.2488\n",
      "Epoch [1/1], Step [1410/8587], Loss: 0.1124\n",
      "Epoch [1/1], Step [1420/8587], Loss: 0.4436\n",
      "Epoch [1/1], Step [1430/8587], Loss: 0.2447\n",
      "Epoch [1/1], Step [1440/8587], Loss: 0.2720\n",
      "Epoch [1/1], Step [1450/8587], Loss: 0.2508\n",
      "Epoch [1/1], Step [1460/8587], Loss: 0.4933\n",
      "Epoch [1/1], Step [1470/8587], Loss: 5.7776\n",
      "Epoch [1/1], Step [1480/8587], Loss: 0.1486\n",
      "Epoch [1/1], Step [1490/8587], Loss: 0.4745\n",
      "Epoch [1/1], Step [1500/8587], Loss: 0.1715\n",
      "Epoch [1/1], Step [1510/8587], Loss: 0.1135\n",
      "Epoch [1/1], Step [1520/8587], Loss: 0.4262\n",
      "Epoch [1/1], Step [1530/8587], Loss: 0.1002\n",
      "Epoch [1/1], Step [1540/8587], Loss: 0.1215\n",
      "Epoch [1/1], Step [1550/8587], Loss: 0.0075\n",
      "Epoch [1/1], Step [1560/8587], Loss: 3.8531\n",
      "Epoch [1/1], Step [1570/8587], Loss: 0.2985\n",
      "Epoch [1/1], Step [1580/8587], Loss: 0.2413\n",
      "Epoch [1/1], Step [1590/8587], Loss: 0.4529\n",
      "Epoch [1/1], Step [1600/8587], Loss: 0.0101\n",
      "Epoch [1/1], Step [1610/8587], Loss: 0.5709\n",
      "Epoch [1/1], Step [1620/8587], Loss: 0.3158\n",
      "Epoch [1/1], Step [1630/8587], Loss: 0.0537\n",
      "Epoch [1/1], Step [1640/8587], Loss: 0.1847\n",
      "Epoch [1/1], Step [1650/8587], Loss: 0.0004\n",
      "Epoch [1/1], Step [1660/8587], Loss: 6.2877\n",
      "Epoch [1/1], Step [1670/8587], Loss: 0.1583\n",
      "Epoch [1/1], Step [1680/8587], Loss: 0.2176\n",
      "Epoch [1/1], Step [1690/8587], Loss: 0.0113\n",
      "Epoch [1/1], Step [1700/8587], Loss: 0.4203\n",
      "Epoch [1/1], Step [1710/8587], Loss: 0.2871\n",
      "Epoch [1/1], Step [1720/8587], Loss: 1.2799\n",
      "Epoch [1/1], Step [1730/8587], Loss: 2.1558\n",
      "Epoch [1/1], Step [1740/8587], Loss: 0.4858\n",
      "Epoch [1/1], Step [1750/8587], Loss: 0.2564\n",
      "Epoch [1/1], Step [1760/8587], Loss: 0.4835\n",
      "Epoch [1/1], Step [1770/8587], Loss: 0.1776\n",
      "Epoch [1/1], Step [1780/8587], Loss: 0.0289\n",
      "Epoch [1/1], Step [1790/8587], Loss: 0.0049\n",
      "Epoch [1/1], Step [1800/8587], Loss: 0.4500\n",
      "Epoch [1/1], Step [1810/8587], Loss: 0.1188\n",
      "Epoch [1/1], Step [1820/8587], Loss: 0.5215\n",
      "Epoch [1/1], Step [1830/8587], Loss: 14.2026\n",
      "Epoch [1/1], Step [1840/8587], Loss: 0.1712\n",
      "Epoch [1/1], Step [1850/8587], Loss: 0.4240\n",
      "Epoch [1/1], Step [1860/8587], Loss: 10.6015\n",
      "Epoch [1/1], Step [1870/8587], Loss: 0.2961\n",
      "Epoch [1/1], Step [1880/8587], Loss: 6.2230\n",
      "Epoch [1/1], Step [1890/8587], Loss: 0.0002\n",
      "Epoch [1/1], Step [1900/8587], Loss: 0.3045\n",
      "Epoch [1/1], Step [1910/8587], Loss: 0.4830\n",
      "Epoch [1/1], Step [1920/8587], Loss: 4.4541\n",
      "Epoch [1/1], Step [1930/8587], Loss: 0.0116\n",
      "Epoch [1/1], Step [1940/8587], Loss: 8.0893\n",
      "Epoch [1/1], Step [1950/8587], Loss: 0.2193\n",
      "Epoch [1/1], Step [1960/8587], Loss: 0.5465\n",
      "Epoch [1/1], Step [1970/8587], Loss: 0.1301\n",
      "Epoch [1/1], Step [1980/8587], Loss: 0.0246\n",
      "Epoch [1/1], Step [1990/8587], Loss: 0.4007\n",
      "Epoch [1/1], Step [2000/8587], Loss: 0.2547\n",
      "Epoch [1/1], Step [2010/8587], Loss: 0.3122\n",
      "Epoch [1/1], Step [2020/8587], Loss: 0.2616\n",
      "Epoch [1/1], Step [2030/8587], Loss: 0.1290\n",
      "Epoch [1/1], Step [2040/8587], Loss: 0.3731\n",
      "Epoch [1/1], Step [2050/8587], Loss: 0.5058\n",
      "Epoch [1/1], Step [2060/8587], Loss: 0.5078\n",
      "Epoch [1/1], Step [2070/8587], Loss: 0.0020\n",
      "Epoch [1/1], Step [2080/8587], Loss: 0.4248\n",
      "Epoch [1/1], Step [2090/8587], Loss: 0.1079\n",
      "Epoch [1/1], Step [2100/8587], Loss: 0.0180\n",
      "Epoch [1/1], Step [2110/8587], Loss: 0.2847\n",
      "Epoch [1/1], Step [2120/8587], Loss: 0.0375\n",
      "Epoch [1/1], Step [2130/8587], Loss: 0.4788\n",
      "Epoch [1/1], Step [2140/8587], Loss: 0.4537\n",
      "Epoch [1/1], Step [2150/8587], Loss: 0.4532\n",
      "Epoch [1/1], Step [2160/8587], Loss: 0.4866\n",
      "Epoch [1/1], Step [2170/8587], Loss: 0.3625\n",
      "Epoch [1/1], Step [2180/8587], Loss: 0.5569\n",
      "Epoch [1/1], Step [2190/8587], Loss: 0.3350\n",
      "Epoch [1/1], Step [2200/8587], Loss: 0.1882\n",
      "Epoch [1/1], Step [2210/8587], Loss: 0.3939\n",
      "Epoch [1/1], Step [2220/8587], Loss: 0.2347\n",
      "Epoch [1/1], Step [2230/8587], Loss: 0.2729\n",
      "Epoch [1/1], Step [2240/8587], Loss: 0.3552\n",
      "Epoch [1/1], Step [2250/8587], Loss: 0.6606\n",
      "Epoch [1/1], Step [2260/8587], Loss: 0.0070\n",
      "Epoch [1/1], Step [2270/8587], Loss: 3.3138\n",
      "Epoch [1/1], Step [2280/8587], Loss: 0.0164\n",
      "Epoch [1/1], Step [2290/8587], Loss: 4.8802\n",
      "Epoch [1/1], Step [2300/8587], Loss: 0.3402\n",
      "Epoch [1/1], Step [2310/8587], Loss: 0.0697\n",
      "Epoch [1/1], Step [2320/8587], Loss: 0.2649\n",
      "Epoch [1/1], Step [2330/8587], Loss: 0.0221\n",
      "Epoch [1/1], Step [2340/8587], Loss: 0.4158\n",
      "Epoch [1/1], Step [2350/8587], Loss: 0.2248\n",
      "Epoch [1/1], Step [2360/8587], Loss: 0.0320\n",
      "Epoch [1/1], Step [2370/8587], Loss: 0.4994\n",
      "Epoch [1/1], Step [2380/8587], Loss: 0.2971\n",
      "Epoch [1/1], Step [2390/8587], Loss: 0.2578\n",
      "Epoch [1/1], Step [2400/8587], Loss: 0.2376\n",
      "Epoch [1/1], Step [2410/8587], Loss: 0.2432\n",
      "Epoch [1/1], Step [2420/8587], Loss: 0.0840\n",
      "Epoch [1/1], Step [2430/8587], Loss: 0.4595\n",
      "Epoch [1/1], Step [2440/8587], Loss: 0.3694\n",
      "Epoch [1/1], Step [2450/8587], Loss: 0.0039\n",
      "Epoch [1/1], Step [2460/8587], Loss: 0.5540\n",
      "Epoch [1/1], Step [2470/8587], Loss: 0.2329\n",
      "Epoch [1/1], Step [2480/8587], Loss: 0.0173\n",
      "Epoch [1/1], Step [2490/8587], Loss: 0.4263\n",
      "Epoch [1/1], Step [2500/8587], Loss: 0.4709\n",
      "Epoch [1/1], Step [2510/8587], Loss: 0.0032\n",
      "Epoch [1/1], Step [2520/8587], Loss: 0.3540\n",
      "Epoch [1/1], Step [2530/8587], Loss: 0.3830\n",
      "Epoch [1/1], Step [2540/8587], Loss: 0.5360\n",
      "Epoch [1/1], Step [2550/8587], Loss: 0.2794\n",
      "Epoch [1/1], Step [2560/8587], Loss: 3.6267\n",
      "Epoch [1/1], Step [2570/8587], Loss: 0.3324\n",
      "Epoch [1/1], Step [2580/8587], Loss: 0.3820\n",
      "Epoch [1/1], Step [2590/8587], Loss: 0.0178\n",
      "Epoch [1/1], Step [2600/8587], Loss: 0.0616\n",
      "Epoch [1/1], Step [2610/8587], Loss: 8.1811\n",
      "Epoch [1/1], Step [2620/8587], Loss: 0.2135\n",
      "Epoch [1/1], Step [2630/8587], Loss: 0.4804\n",
      "Epoch [1/1], Step [2640/8587], Loss: 0.3529\n",
      "Epoch [1/1], Step [2650/8587], Loss: 0.5384\n",
      "Epoch [1/1], Step [2660/8587], Loss: 0.0002\n",
      "Epoch [1/1], Step [2670/8587], Loss: 3.2181\n",
      "Epoch [1/1], Step [2680/8587], Loss: 0.1681\n",
      "Epoch [1/1], Step [2690/8587], Loss: 0.4962\n",
      "Epoch [1/1], Step [2700/8587], Loss: 0.0145\n",
      "Epoch [1/1], Step [2710/8587], Loss: 0.5261\n",
      "Epoch [1/1], Step [2720/8587], Loss: 0.4989\n",
      "Epoch [1/1], Step [2730/8587], Loss: 0.2198\n",
      "Epoch [1/1], Step [2740/8587], Loss: 0.2180\n",
      "Epoch [1/1], Step [2750/8587], Loss: 0.0167\n",
      "Epoch [1/1], Step [2760/8587], Loss: 0.0041\n",
      "Epoch [1/1], Step [2770/8587], Loss: 0.2846\n",
      "Epoch [1/1], Step [2780/8587], Loss: 0.2885\n",
      "Epoch [1/1], Step [2790/8587], Loss: 0.4142\n",
      "Epoch [1/1], Step [2800/8587], Loss: 12.0688\n",
      "Epoch [1/1], Step [2810/8587], Loss: 0.1127\n",
      "Epoch [1/1], Step [2820/8587], Loss: 0.4331\n",
      "Epoch [1/1], Step [2830/8587], Loss: 0.5078\n",
      "Epoch [1/1], Step [2840/8587], Loss: 0.5168\n",
      "Epoch [1/1], Step [2850/8587], Loss: 6.7099\n",
      "Epoch [1/1], Step [2860/8587], Loss: 0.3087\n",
      "Epoch [1/1], Step [2870/8587], Loss: 0.4681\n",
      "Epoch [1/1], Step [2880/8587], Loss: 0.3141\n",
      "Epoch [1/1], Step [2890/8587], Loss: 0.5503\n",
      "Epoch [1/1], Step [2900/8587], Loss: 0.4267\n",
      "Epoch [1/1], Step [2910/8587], Loss: 0.5170\n",
      "Epoch [1/1], Step [2920/8587], Loss: 0.0717\n",
      "Epoch [1/1], Step [2930/8587], Loss: 1.0574\n",
      "Epoch [1/1], Step [2940/8587], Loss: 0.0122\n",
      "Epoch [1/1], Step [2950/8587], Loss: 0.4367\n",
      "Epoch [1/1], Step [2960/8587], Loss: 0.0182\n",
      "Epoch [1/1], Step [2970/8587], Loss: 0.3976\n",
      "Epoch [1/1], Step [2980/8587], Loss: 0.3534\n",
      "Epoch [1/1], Step [2990/8587], Loss: 0.3110\n",
      "Epoch [1/1], Step [3000/8587], Loss: 0.1410\n",
      "Epoch [1/1], Step [3010/8587], Loss: 0.1859\n",
      "Epoch [1/1], Step [3020/8587], Loss: 0.1462\n",
      "Epoch [1/1], Step [3030/8587], Loss: 0.4556\n",
      "Epoch [1/1], Step [3040/8587], Loss: 0.0002\n",
      "Epoch [1/1], Step [3050/8587], Loss: 0.2374\n",
      "Epoch [1/1], Step [3060/8587], Loss: 0.4633\n",
      "Epoch [1/1], Step [3070/8587], Loss: 5.6863\n",
      "Epoch [1/1], Step [3080/8587], Loss: 0.3497\n",
      "Epoch [1/1], Step [3090/8587], Loss: 0.3214\n",
      "Epoch [1/1], Step [3100/8587], Loss: 0.2705\n",
      "Epoch [1/1], Step [3110/8587], Loss: 0.3878\n",
      "Epoch [1/1], Step [3120/8587], Loss: 0.3730\n",
      "Epoch [1/1], Step [3130/8587], Loss: 0.1918\n",
      "Epoch [1/1], Step [3140/8587], Loss: 0.2742\n",
      "Epoch [1/1], Step [3150/8587], Loss: 0.2358\n",
      "Epoch [1/1], Step [3160/8587], Loss: 0.7660\n",
      "Epoch [1/1], Step [3170/8587], Loss: 0.2389\n",
      "Epoch [1/1], Step [3180/8587], Loss: 0.2231\n",
      "Epoch [1/1], Step [3190/8587], Loss: 0.4192\n",
      "Epoch [1/1], Step [3200/8587], Loss: 0.1665\n",
      "Epoch [1/1], Step [3210/8587], Loss: 19.0429\n",
      "Epoch [1/1], Step [3220/8587], Loss: 0.3943\n",
      "Epoch [1/1], Step [3230/8587], Loss: 2.1759\n",
      "Epoch [1/1], Step [3240/8587], Loss: 0.0022\n",
      "Epoch [1/1], Step [3250/8587], Loss: 0.0006\n",
      "Epoch [1/1], Step [3260/8587], Loss: 0.3374\n",
      "Epoch [1/1], Step [3270/8587], Loss: 0.1343\n",
      "Epoch [1/1], Step [3280/8587], Loss: 0.0096\n",
      "Epoch [1/1], Step [3290/8587], Loss: 0.4754\n",
      "Epoch [1/1], Step [3300/8587], Loss: 1.1222\n",
      "Epoch [1/1], Step [3310/8587], Loss: 0.4370\n",
      "Epoch [1/1], Step [3320/8587], Loss: 0.1822\n",
      "Epoch [1/1], Step [3330/8587], Loss: 0.2947\n",
      "Epoch [1/1], Step [3340/8587], Loss: 1.3477\n",
      "Epoch [1/1], Step [3350/8587], Loss: 0.4571\n",
      "Epoch [1/1], Step [3360/8587], Loss: 0.4384\n",
      "Epoch [1/1], Step [3370/8587], Loss: 0.1208\n",
      "Epoch [1/1], Step [3380/8587], Loss: 0.1536\n",
      "Epoch [1/1], Step [3390/8587], Loss: 0.4360\n",
      "Epoch [1/1], Step [3400/8587], Loss: 0.0950\n",
      "Epoch [1/1], Step [3410/8587], Loss: 10.8794\n",
      "Epoch [1/1], Step [3420/8587], Loss: 0.0007\n",
      "Epoch [1/1], Step [3430/8587], Loss: 0.5100\n",
      "Epoch [1/1], Step [3440/8587], Loss: 9.3647\n",
      "Epoch [1/1], Step [3450/8587], Loss: 0.1940\n",
      "Epoch [1/1], Step [3460/8587], Loss: 0.3433\n",
      "Epoch [1/1], Step [3470/8587], Loss: 0.5264\n",
      "Epoch [1/1], Step [3480/8587], Loss: 0.4981\n",
      "Epoch [1/1], Step [3490/8587], Loss: 0.0007\n",
      "Epoch [1/1], Step [3500/8587], Loss: 0.0119\n",
      "Epoch [1/1], Step [3510/8587], Loss: 0.3570\n",
      "Epoch [1/1], Step [3520/8587], Loss: 0.0186\n",
      "Epoch [1/1], Step [3530/8587], Loss: 0.2845\n",
      "Epoch [1/1], Step [3540/8587], Loss: 0.4104\n",
      "Epoch [1/1], Step [3550/8587], Loss: 0.5475\n",
      "Epoch [1/1], Step [3560/8587], Loss: 5.1110\n",
      "Epoch [1/1], Step [3570/8587], Loss: 0.0465\n",
      "Epoch [1/1], Step [3580/8587], Loss: 0.0275\n",
      "Epoch [1/1], Step [3590/8587], Loss: 0.5235\n",
      "Epoch [1/1], Step [3600/8587], Loss: 0.5603\n",
      "Epoch [1/1], Step [3610/8587], Loss: 0.3062\n",
      "Epoch [1/1], Step [3620/8587], Loss: 0.3345\n",
      "Epoch [1/1], Step [3630/8587], Loss: 0.1536\n",
      "Epoch [1/1], Step [3640/8587], Loss: 0.1848\n",
      "Epoch [1/1], Step [3650/8587], Loss: 3.5505\n",
      "Epoch [1/1], Step [3660/8587], Loss: 0.0736\n",
      "Epoch [1/1], Step [3670/8587], Loss: 0.5912\n",
      "Epoch [1/1], Step [3680/8587], Loss: 2.8485\n",
      "Epoch [1/1], Step [3690/8587], Loss: 0.4979\n",
      "Epoch [1/1], Step [3700/8587], Loss: 0.1922\n",
      "Epoch [1/1], Step [3710/8587], Loss: 0.5758\n",
      "Epoch [1/1], Step [3720/8587], Loss: 0.3453\n",
      "Epoch [1/1], Step [3730/8587], Loss: 0.5501\n",
      "Epoch [1/1], Step [3740/8587], Loss: 0.0501\n",
      "Epoch [1/1], Step [3750/8587], Loss: 0.0628\n",
      "Epoch [1/1], Step [3760/8587], Loss: 0.0034\n",
      "Epoch [1/1], Step [3770/8587], Loss: 0.0827\n",
      "Epoch [1/1], Step [3780/8587], Loss: 0.6150\n",
      "Epoch [1/1], Step [3790/8587], Loss: 0.5686\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 32\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Print training information\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m batch_evaluation_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\optim\\adam.py:332\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m param\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mand\u001b[39;00m step_t\u001b[38;5;241m.\u001b[39mis_cuda, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf capturable=True, params and state_steps must be CUDA tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    335\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the LSTM model\n",
    "model = LSTMModel(input_size=input_size, hidden_dim=hidden_dim, n_layers=n_layers, output_size=output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "window_size = 10\n",
    "epochs = 1\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Walk-Forward Validation\n",
    "    for i in range(len(x_train) - window_size):\n",
    "        # Initialize hidden state for each iteration\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        # Prepare the input and target sequences\n",
    "        x_sequence = x_train[i:i + window_size]\n",
    "        y_true = y_train[i + window_size:i + window_size + 1]\n",
    "\n",
    "        # Convert sequences to PyTorch Tensors\n",
    "        x_sequence = torch.unsqueeze(x_sequence, 0)\n",
    "        y_true = torch.tensor(y_true, dtype=torch.float32)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred, hidden = model(x_sequence, hidden)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training information\n",
    "        if (i + 1) % batch_evaluation_frequency == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(x_train) - window_size}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "val_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(x_val) - window_size):\n",
    "        x_val_sequence = x_val[i:i + window_size]\n",
    "        y_val_true = y_val[i + window_size:i + window_size + 1]\n",
    "\n",
    "        x_val_sequence = torch.unsqueeze(x_val_sequence, 0)\n",
    "        y_val_true = torch.tensor(y_val_true, dtype=torch.float32)\n",
    "\n",
    "        y_val_pred, _ = model(x_val_sequence, hidden)\n",
    "        val_loss = criterion(y_val_pred, y_val_true)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "# Calculate and print the mean validation loss\n",
    "mean_val_loss = np.mean(val_losses)\n",
    "print(f'Mean Validation Loss: {mean_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assume new_data is your input data for the next day\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Make sure to preprocess and scale new_data appropriately\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert new_data to a PyTorch tensor\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m new_data_scaled \u001b[38;5;241m=\u001b[39m scaler_x\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mnew_data\u001b[49m)\n\u001b[0;32m      6\u001b[0m new_data_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(new_data_scaled, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Assume new_data is your input data for the next day\n",
    "# Make sure to preprocess and scale new_data appropriately\n",
    "\n",
    "# Convert new_data to a PyTorch tensor\n",
    "new_data_scaled = scaler_x.transform(new_data)\n",
    "new_data_tensor = torch.tensor(new_data_scaled, dtype=torch.float32)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize hidden state\n",
    "hidden = model.init_hidden(1)  # Assuming batch size is 1\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    y_pred, _ = model(new_data_tensor.unsqueeze(0), hidden)\n",
    "\n",
    "# Inverse transform the predicted close value\n",
    "predicted_close_scaled = y_pred.item()\n",
    "predicted_close = scaler_y.inverse_transform(np.array([[predicted_close_scaled]]))\n",
    "\n",
    "print(f'Predicted Close Value for the Next Day: {predicted_close[0, 0]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "can you predict the \"close value\" for the next day after the existing data using the trained method? I dont have input for the future. I have to create it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 50 values of x_train:\n",
      "[[-3.8239661e-01 -3.6861533e-01 -3.8869005e-01 -1.2194055e-01\n",
      "  -4.4869837e-01 -4.7490220e+00 -5.5267133e-02]\n",
      " [ 1.3212335e+00  1.3480754e+00  1.3426359e+00  3.5530314e-01\n",
      "  -2.3301007e-01  1.0631542e-01  7.1244866e-01]\n",
      " [ 2.8130737e-01  2.7440774e-01  2.7848962e-01  3.2699594e+00\n",
      "   3.3431654e+00 -1.9936468e-01 -8.7569481e-01]\n",
      " [ 3.7296879e+00  3.7091699e+00  3.7638538e+00 -4.2365277e-01\n",
      "   3.8722478e-02  5.2999120e-02 -3.0757835e-02]\n",
      " [ 2.5817108e-01  2.6726300e-01  2.6891381e-01  1.9915811e+00\n",
      "   3.3593824e+00  6.9990349e-01 -9.1779560e-01]\n",
      " [ 2.6575676e-01  2.7613753e-01  2.7013949e-01  2.1285069e+00\n",
      "   3.3952315e+00  4.8663834e-01 -8.5606867e-01]\n",
      " [ 1.6491501e-02  1.4939867e-02  2.5764246e-02  8.5789776e-01\n",
      "   2.1987750e-01  5.0441045e-01 -4.7077781e-01]\n",
      " [ 2.9192731e-01  3.1374159e-01  3.0568498e-01  2.4002156e+00\n",
      "   3.5360012e+00  6.4303279e-01 -9.1319573e-01]\n",
      " [-6.6645885e-01 -6.6945368e-01 -6.6689479e-01 -5.9105599e-01\n",
      "  -4.5095354e-01 -7.3608208e-01 -1.6022661e-01]\n",
      " [-3.9414916e-02 -3.0711440e-02 -2.9698979e-02  4.6100032e-01\n",
      "   1.8280879e-01  6.0037977e-01 -5.6120032e-01]\n",
      " [-6.6488707e-01 -6.6941160e-01 -6.6831052e-01 -6.2165546e-01\n",
      "  -4.5563254e-01 -9.4223839e-01 -6.6907114e-01]\n",
      " [-5.3498393e-01 -5.2941626e-01 -5.3326213e-01 -3.2808262e-01\n",
      "  -4.5213690e-01  2.8147833e+00 -4.2526370e-01]\n",
      " [-5.4356486e-01 -5.4154581e-01 -5.3909653e-01 -6.5940601e-01\n",
      "  -4.5662388e-01  3.9066899e-01 -4.0332752e-01]\n",
      " [-6.9241399e-01 -6.9569230e-01 -6.9216275e-01 -6.6586500e-01\n",
      "  -4.5845082e-01 -4.3395638e-01 -6.4969575e-01]\n",
      " [ 1.8899314e+00  1.9636505e+00  1.8712249e+00  7.4721110e-01\n",
      "  -1.6606891e-01  2.0541375e+00  1.3669976e+00]\n",
      " [-5.1128024e-01 -5.1711822e-01 -5.0932407e-01 -6.5236229e-01\n",
      "  -4.5219660e-01  1.5607730e-01 -5.7965624e-01]\n",
      " [-5.0558794e-01 -5.0962150e-01 -5.1507264e-01 -5.8531141e-01\n",
      "  -4.5141545e-01 -8.2068831e-02  1.4661809e+00]\n",
      " [-4.4492683e-01 -4.5133221e-01 -4.4995087e-01 -6.4098048e-01\n",
      "  -4.5038000e-01 -5.3348011e-01 -5.8470786e-01]\n",
      " [-4.8664197e-01 -4.8704702e-01 -4.8444226e-01 -6.6250408e-01\n",
      "  -4.5128831e-01 -1.3183071e-01 -7.4988645e-01]\n",
      " [-7.0566767e-01 -7.0984346e-01 -7.0280188e-01 -6.7931217e-01\n",
      "  -4.5859876e-01 -1.1910478e+00 -1.0864532e+00]\n",
      " [ 2.5841663e+00  2.5659981e+00  2.5313385e+00  1.1487279e+00\n",
      "   2.3720062e+00 -8.4626907e-01 -5.9577173e-01]\n",
      " [ 2.2047026e-01  2.2116043e-01  2.3306188e-01  1.7926458e-01\n",
      "   4.0459564e-01  4.0488666e-01 -8.7061417e-01]\n",
      " [-4.4875002e-01 -4.5326957e-01 -4.5381185e-01 -6.2509406e-01\n",
      "  -4.5053831e-01 -7.0764673e-01 -6.2901628e-01]\n",
      " [-5.4169577e-01 -5.4440975e-01 -5.4570305e-01 -5.2263558e-01\n",
      "  -4.5307115e-01 -5.9035087e-01 -4.7064787e-01]\n",
      " [ 2.3738823e+00  2.3433731e+00  2.3657086e+00  4.0051218e-02\n",
      "  -1.2956853e-01 -6.2234062e-01 -1.4938398e-01]\n",
      " [-4.5631140e-01 -4.5040563e-01 -4.4995087e-01 -6.0992199e-01\n",
      "  -4.5034885e-01  9.0250546e-01 -7.3811591e-02]\n",
      " [ 3.3106941e-01  3.1802845e-01  3.2184899e-01  7.0457095e-01\n",
      "   4.7860330e-01 -6.5077597e-01 -7.2433746e-01]\n",
      " [ 1.3907599e-01  1.2872970e-01  1.4167012e-01  9.6419454e-01\n",
      "   3.0028474e-01  1.2764193e-01 -6.2979352e-01]\n",
      " [-3.5739726e-01 -3.5381138e-01 -3.5198933e-01  3.6684042e-01\n",
      "  -4.0820929e-01  7.3189330e-01  3.4466000e+00]\n",
      " [-6.1561054e-01 -6.0362554e-01 -6.1468583e-01 -5.6353980e-01\n",
      "  -4.5395350e-01  3.1915517e+00 -3.8250202e-01]\n",
      " [-5.9759915e-01 -5.8770549e-01 -5.9357917e-01 -4.2231286e-01\n",
      "  -4.5338255e-01  1.8266546e+00  7.7047157e-01]\n",
      " [-7.3693281e-01 -7.3898810e-01 -7.3334646e-01 -6.7597723e-01\n",
      "  -4.5895690e-01  8.7407011e-01 -7.6385033e-01]\n",
      " [-3.6627552e-01 -3.7244794e-01 -3.6745468e-01  2.6654729e-01\n",
      "  -4.1044110e-01 -6.6143924e-01  1.4002181e+00]\n",
      " [-7.3982143e-01 -7.4088335e-01 -7.4106842e-01  2.2458450e-01\n",
      "  -4.5467234e-01 -5.6518445e+00  4.3665373e-01]\n",
      " [-6.9224405e-01 -6.9240719e-01 -6.8821597e-01 -6.5042275e-01\n",
      "  -4.5190334e-01  5.6483555e-01 -2.9349744e-01]\n",
      " [-5.4169577e-01 -5.3404903e-01 -5.3617930e-01 -6.5564543e-01\n",
      "  -4.5247167e-01  8.1719935e-01 -4.4281268e-01]\n",
      " [ 4.1231498e-01  4.2911381e-01  4.0918058e-01  6.9389242e-01\n",
      "  -3.2649413e-01  3.4801596e-01  2.2611580e+00]\n",
      " [ 2.5642636e-01  2.5936612e-01  2.6745826e-01  1.7385830e+00\n",
      "   3.3592501e+00 -1.6026606e-01 -8.9791834e-01]\n",
      " [-6.6301799e-01 -6.6385221e-01 -6.6058856e-01 -6.6264474e-01\n",
      "  -4.5805898e-01  3.3379829e-01 -7.2589749e-01]\n",
      " [-4.8094967e-01 -4.7575977e-01 -4.7680610e-01 -6.1911261e-01\n",
      "  -4.5148033e-01  1.3830519e-01 -7.4675798e-01]\n",
      " [ 3.4243646e+00  3.4171972e+00  3.4508574e+00 -3.1603456e-01\n",
      "  -1.6541453e-04 -9.6286505e-02 -6.2321063e-02]\n",
      " [-7.4300742e-01 -7.4269432e-01 -7.3948115e-01 -3.1436154e-01\n",
      "  -4.5455298e-01  4.3332204e-01 -3.6702842e-01]\n",
      " [-6.3837969e-01 -6.3891923e-01 -6.3433391e-01 -6.2202930e-01\n",
      "  -4.5477355e-01  1.0986984e-01 -4.1896978e-01]\n",
      " [-6.7627168e-01 -6.7598176e-01 -6.7208570e-01 -6.6829681e-01\n",
      "  -4.5819393e-01  7.2123003e-01 -6.6320914e-01]\n",
      " [-4.5845664e-01 -4.6316695e-01 -4.6106189e-01  2.1222554e-01\n",
      "  -4.1994703e-01 -3.1310612e-01  9.7024411e-01]\n",
      " [-6.8238878e-01 -6.8187809e-01 -6.7937863e-01 -5.0995094e-01\n",
      "  -4.5141026e-01  6.2526071e-01 -4.2625615e-01]\n",
      " [-6.7593181e-01 -6.7387593e-01 -6.7260045e-01 -4.5360079e-01\n",
      "  -4.5104435e-01  2.6270989e-01  1.1823960e-01]\n",
      " [-6.5741068e-01 -6.6199905e-01 -6.5732819e-01 -4.3575636e-01\n",
      "  -4.5047602e-01 -6.4011276e-01 -1.4562306e-01]\n",
      " [-6.1374146e-01 -6.0455215e-01 -6.0893726e-01 -6.4175034e-01\n",
      "  -4.5735049e-01  1.9723858e+00 -3.6080033e-01]\n",
      " [ 5.7335579e-01  5.7532132e-01  5.8652806e-01  1.1599284e-01\n",
      "  -3.1082219e-01  7.2478443e-01  6.9778591e-01]]\n",
      "\n",
      "Last window (x_sequence):\n",
      "[[-8.4252313e-02 -8.0727823e-02 -9.7572483e-02  1.0598351e+00\n",
      "  -3.8234112e-01 -1.4860647e+00  4.9526243e+00]\n",
      " [-3.8239661e-01 -3.6861533e-01 -3.8869005e-01 -1.2194055e-01\n",
      "  -4.4869837e-01 -4.7490220e+00 -5.5267133e-02]\n",
      " [ 1.3212335e+00  1.3480754e+00  1.3426359e+00  3.5530314e-01\n",
      "  -2.3301007e-01  1.0631542e-01  7.1244866e-01]\n",
      " [ 2.8130737e-01  2.7440774e-01  2.7848962e-01  3.2699594e+00\n",
      "   3.3431654e+00 -1.9936468e-01 -8.7569481e-01]\n",
      " [ 3.7296879e+00  3.7091699e+00  3.7638538e+00 -4.2365277e-01\n",
      "   3.8722478e-02  5.2999120e-02 -3.0757835e-02]\n",
      " [ 2.5817108e-01  2.6726300e-01  2.6891381e-01  1.9915811e+00\n",
      "   3.3593824e+00  6.9990349e-01 -9.1779560e-01]\n",
      " [ 2.6575676e-01  2.7613753e-01  2.7013949e-01  2.1285069e+00\n",
      "   3.3952315e+00  4.8663834e-01 -8.5606867e-01]\n",
      " [ 1.6491501e-02  1.4939867e-02  2.5764246e-02  8.5789776e-01\n",
      "   2.1987750e-01  5.0441045e-01 -4.7077781e-01]\n",
      " [ 2.9192731e-01  3.1374159e-01  3.0568498e-01  2.4002156e+00\n",
      "   3.5360012e+00  6.4303279e-01 -9.1319573e-01]\n",
      " [-6.6645885e-01 -6.6945368e-01 -6.6689479e-01 -5.9105599e-01\n",
      "  -4.5095354e-01 -7.3608208e-01 -1.6022661e-01]\n",
      " [-3.9414916e-02 -3.0711440e-02 -2.9698979e-02  4.6100032e-01\n",
      "   1.8280879e-01  6.0037977e-01 -5.6120032e-01]\n",
      " [-6.6488707e-01 -6.6941160e-01 -6.6831052e-01 -6.2165546e-01\n",
      "  -4.5563254e-01 -9.4223839e-01 -6.6907114e-01]\n",
      " [-5.3498393e-01 -5.2941626e-01 -5.3326213e-01 -3.2808262e-01\n",
      "  -4.5213690e-01  2.8147833e+00 -4.2526370e-01]\n",
      " [-5.4356486e-01 -5.4154581e-01 -5.3909653e-01 -6.5940601e-01\n",
      "  -4.5662388e-01  3.9066899e-01 -4.0332752e-01]\n",
      " [-6.9241399e-01 -6.9569230e-01 -6.9216275e-01 -6.6586500e-01\n",
      "  -4.5845082e-01 -4.3395638e-01 -6.4969575e-01]\n",
      " [ 1.8899314e+00  1.9636505e+00  1.8712249e+00  7.4721110e-01\n",
      "  -1.6606891e-01  2.0541375e+00  1.3669976e+00]\n",
      " [-5.1128024e-01 -5.1711822e-01 -5.0932407e-01 -6.5236229e-01\n",
      "  -4.5219660e-01  1.5607730e-01 -5.7965624e-01]\n",
      " [-5.0558794e-01 -5.0962150e-01 -5.1507264e-01 -5.8531141e-01\n",
      "  -4.5141545e-01 -8.2068831e-02  1.4661809e+00]\n",
      " [-4.4492683e-01 -4.5133221e-01 -4.4995087e-01 -6.4098048e-01\n",
      "  -4.5038000e-01 -5.3348011e-01 -5.8470786e-01]\n",
      " [-4.8664197e-01 -4.8704702e-01 -4.8444226e-01 -6.6250408e-01\n",
      "  -4.5128831e-01 -1.3183071e-01 -7.4988645e-01]\n",
      " [-7.0566767e-01 -7.0984346e-01 -7.0280188e-01 -6.7931217e-01\n",
      "  -4.5859876e-01 -1.1910478e+00 -1.0864532e+00]\n",
      " [ 2.5841663e+00  2.5659981e+00  2.5313385e+00  1.1487279e+00\n",
      "   2.3720062e+00 -8.4626907e-01 -5.9577173e-01]\n",
      " [ 2.2047026e-01  2.2116043e-01  2.3306188e-01  1.7926458e-01\n",
      "   4.0459564e-01  4.0488666e-01 -8.7061417e-01]\n",
      " [-4.4875002e-01 -4.5326957e-01 -4.5381185e-01 -6.2509406e-01\n",
      "  -4.5053831e-01 -7.0764673e-01 -6.2901628e-01]\n",
      " [-5.4169577e-01 -5.4440975e-01 -5.4570305e-01 -5.2263558e-01\n",
      "  -4.5307115e-01 -5.9035087e-01 -4.7064787e-01]\n",
      " [ 2.3738823e+00  2.3433731e+00  2.3657086e+00  4.0051218e-02\n",
      "  -1.2956853e-01 -6.2234062e-01 -1.4938398e-01]\n",
      " [-4.5631140e-01 -4.5040563e-01 -4.4995087e-01 -6.0992199e-01\n",
      "  -4.5034885e-01  9.0250546e-01 -7.3811591e-02]\n",
      " [ 3.3106941e-01  3.1802845e-01  3.2184899e-01  7.0457095e-01\n",
      "   4.7860330e-01 -6.5077597e-01 -7.2433746e-01]\n",
      " [ 1.3907599e-01  1.2872970e-01  1.4167012e-01  9.6419454e-01\n",
      "   3.0028474e-01  1.2764193e-01 -6.2979352e-01]\n",
      " [-3.5739726e-01 -3.5381138e-01 -3.5198933e-01  3.6684042e-01\n",
      "  -4.0820929e-01  7.3189330e-01  3.4466000e+00]\n",
      " [-6.1561054e-01 -6.0362554e-01 -6.1468583e-01 -5.6353980e-01\n",
      "  -4.5395350e-01  3.1915517e+00 -3.8250202e-01]\n",
      " [-5.9759915e-01 -5.8770549e-01 -5.9357917e-01 -4.2231286e-01\n",
      "  -4.5338255e-01  1.8266546e+00  7.7047157e-01]\n",
      " [-7.3693281e-01 -7.3898810e-01 -7.3334646e-01 -6.7597723e-01\n",
      "  -4.5895690e-01  8.7407011e-01 -7.6385033e-01]\n",
      " [-3.6627552e-01 -3.7244794e-01 -3.6745468e-01  2.6654729e-01\n",
      "  -4.1044110e-01 -6.6143924e-01  1.4002181e+00]\n",
      " [-7.3982143e-01 -7.4088335e-01 -7.4106842e-01  2.2458450e-01\n",
      "  -4.5467234e-01 -5.6518445e+00  4.3665373e-01]\n",
      " [-6.9224405e-01 -6.9240719e-01 -6.8821597e-01 -6.5042275e-01\n",
      "  -4.5190334e-01  5.6483555e-01 -2.9349744e-01]\n",
      " [-5.4169577e-01 -5.3404903e-01 -5.3617930e-01 -6.5564543e-01\n",
      "  -4.5247167e-01  8.1719935e-01 -4.4281268e-01]\n",
      " [ 4.1231498e-01  4.2911381e-01  4.0918058e-01  6.9389242e-01\n",
      "  -3.2649413e-01  3.4801596e-01  2.2611580e+00]\n",
      " [ 2.5642636e-01  2.5936612e-01  2.6745826e-01  1.7385830e+00\n",
      "   3.3592501e+00 -1.6026606e-01 -8.9791834e-01]\n",
      " [-6.6301799e-01 -6.6385221e-01 -6.6058856e-01 -6.6264474e-01\n",
      "  -4.5805898e-01  3.3379829e-01 -7.2589749e-01]\n",
      " [-4.8094967e-01 -4.7575977e-01 -4.7680610e-01 -6.1911261e-01\n",
      "  -4.5148033e-01  1.3830519e-01 -7.4675798e-01]\n",
      " [ 3.4243646e+00  3.4171972e+00  3.4508574e+00 -3.1603456e-01\n",
      "  -1.6541453e-04 -9.6286505e-02 -6.2321063e-02]\n",
      " [-7.4300742e-01 -7.4269432e-01 -7.3948115e-01 -3.1436154e-01\n",
      "  -4.5455298e-01  4.3332204e-01 -3.6702842e-01]\n",
      " [-6.3837969e-01 -6.3891923e-01 -6.3433391e-01 -6.2202930e-01\n",
      "  -4.5477355e-01  1.0986984e-01 -4.1896978e-01]\n",
      " [-6.7627168e-01 -6.7598176e-01 -6.7208570e-01 -6.6829681e-01\n",
      "  -4.5819393e-01  7.2123003e-01 -6.6320914e-01]\n",
      " [-4.5845664e-01 -4.6316695e-01 -4.6106189e-01  2.1222554e-01\n",
      "  -4.1994703e-01 -3.1310612e-01  9.7024411e-01]\n",
      " [-6.8238878e-01 -6.8187809e-01 -6.7937863e-01 -5.0995094e-01\n",
      "  -4.5141026e-01  6.2526071e-01 -4.2625615e-01]\n",
      " [-6.7593181e-01 -6.7387593e-01 -6.7260045e-01 -4.5360079e-01\n",
      "  -4.5104435e-01  2.6270989e-01  1.1823960e-01]\n",
      " [-6.5741068e-01 -6.6199905e-01 -6.5732819e-01 -4.3575636e-01\n",
      "  -4.5047602e-01 -6.4011276e-01 -1.4562306e-01]\n",
      " [-6.1374146e-01 -6.0455215e-01 -6.0893726e-01 -6.4175034e-01\n",
      "  -4.5735049e-01  1.9723858e+00 -3.6080033e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Print the last 50 values of x_train\n",
    "print(\"Last 50 values of x_train:\")\n",
    "print(x_train[-50:].numpy())\n",
    "\n",
    "# Print the last window\n",
    "print(\"\\nLast window (x_sequence):\")\n",
    "print(x_sequence[0].numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
