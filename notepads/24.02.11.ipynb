{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "#print(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "y_data = data[\"close\"].values.reshape(-1, 1)  # Reshape y_data if it's 1D\n",
    "\n",
    "# Initialize the scalers\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both training and test data\n",
    "x_data_scaled = scaler_x.fit_transform(x_data)\n",
    "y_data_scaled = scaler_y.fit_transform(y_data)\n",
    "\n",
    "# Convert scaled data to Tensors\n",
    "x_feature_tensors = torch.tensor(x_data_scaled, dtype=torch.float32)\n",
    "y_feature_tensors = torch.tensor(y_data_scaled, dtype=torch.float32)\n",
    "\n",
    "# Split the training data into training and temporary sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_feature_tensors, y_feature_tensors, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert scaled labels back to numpy arrays\n",
    "y_train = y_train.numpy()\n",
    "y_val = y_val.numpy()\n",
    "y_test = y_test.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Now you can use x_train_scaled, x_val, x_test, y_train_scaled, y_val_scaled, y_test_scaled for your model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 4\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_10592\\600017660.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_10592\\600017660.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000, Train Loss: 1.0489604473114014, Validation Loss: 2.2498157024383545\n",
      "Iteration 2000, Train Loss: 2.075005292892456, Validation Loss: 2.2356631755828857\n",
      "Iteration 3000, Train Loss: 0.23920011520385742, Validation Loss: 2.2373318672180176\n",
      "Iteration 4000, Train Loss: 0.378330796957016, Validation Loss: 2.2188894748687744\n",
      "Iteration 5000, Train Loss: 0.3043554723262787, Validation Loss: 2.221320152282715\n",
      "Iteration 6000, Train Loss: 0.16968679428100586, Validation Loss: 2.2248661518096924\n",
      "Iteration 7000, Train Loss: 0.23000600934028625, Validation Loss: 2.2298691272735596\n",
      "Iteration 8000, Train Loss: 0.3340556025505066, Validation Loss: 2.222750186920166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_10592\\600017660.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.41795605421066284\n",
      "Iteration 3000, Train Loss: 0.23816022276878357, Validation Loss: 2.237905979156494\n",
      "Iteration 6000, Train Loss: 0.16921117901802063, Validation Loss: 2.225233554840088\n",
      "Test Loss: 0.4182061553001404\n",
      "Iteration 1000, Train Loss: 1.0484105348587036, Validation Loss: 2.249269962310791\n",
      "Iteration 2000, Train Loss: 2.0730135440826416, Validation Loss: 2.234947681427002\n",
      "Iteration 3000, Train Loss: 0.24075278639793396, Validation Loss: 2.2364869117736816\n",
      "Iteration 4000, Train Loss: 0.3784767985343933, Validation Loss: 2.2188034057617188\n",
      "Iteration 5000, Train Loss: 0.3053213953971863, Validation Loss: 2.2208151817321777\n",
      "Iteration 6000, Train Loss: 0.1702841818332672, Validation Loss: 2.2244038581848145\n",
      "Iteration 7000, Train Loss: 0.23146991431713104, Validation Loss: 2.229116201400757\n",
      "Iteration 8000, Train Loss: 0.3345987796783447, Validation Loss: 2.2225394248962402\n",
      "Test Loss: 0.4171517789363861\n",
      "Iteration 1000, Train Loss: 1.0492819547653198, Validation Loss: 2.2500827312469482\n",
      "Iteration 2000, Train Loss: 2.0746877193450928, Validation Loss: 2.2355494499206543\n",
      "Iteration 3000, Train Loss: 0.23966994881629944, Validation Loss: 2.2370731830596924\n",
      "Iteration 4000, Train Loss: 0.37848764657974243, Validation Loss: 2.218801259994507\n",
      "Iteration 5000, Train Loss: 0.30481231212615967, Validation Loss: 2.2210805416107178\n",
      "Iteration 6000, Train Loss: 0.1700035035610199, Validation Loss: 2.2246196269989014\n",
      "Iteration 7000, Train Loss: 0.23094606399536133, Validation Loss: 2.229384183883667\n",
      "Iteration 8000, Train Loss: 0.33419618010520935, Validation Loss: 2.2226879596710205\n",
      "Test Loss: 0.41731032729148865\n",
      "Iteration 3000, Train Loss: 0.23816879093647003, Validation Loss: 2.2378952503204346\n",
      "Iteration 6000, Train Loss: 0.16935057938098907, Validation Loss: 2.225125789642334\n",
      "Test Loss: 0.4178198277950287\n",
      "Iteration 1000, Train Loss: 1.0502890348434448, Validation Loss: 2.2511229515075684\n",
      "Iteration 2000, Train Loss: 2.078746795654297, Validation Loss: 2.237013339996338\n",
      "Iteration 3000, Train Loss: 0.23750808835029602, Validation Loss: 2.238243579864502\n",
      "Iteration 4000, Train Loss: 0.3773832321166992, Validation Loss: 2.219416856765747\n",
      "Iteration 5000, Train Loss: 0.30311840772628784, Validation Loss: 2.2219889163970947\n",
      "Iteration 6000, Train Loss: 0.16878899931907654, Validation Loss: 2.225562572479248\n",
      "Iteration 7000, Train Loss: 0.22930188477039337, Validation Loss: 2.230231285095215\n",
      "Iteration 8000, Train Loss: 0.3322398066520691, Validation Loss: 2.223414897918701\n",
      "Test Loss: 0.41792693734169006\n",
      "Iteration 1000, Train Loss: 1.0484809875488281, Validation Loss: 2.2489466667175293\n",
      "Iteration 2000, Train Loss: 2.0703988075256348, Validation Loss: 2.234032154083252\n",
      "Iteration 3000, Train Loss: 0.23939892649650574, Validation Loss: 2.237295389175415\n",
      "Iteration 4000, Train Loss: 0.3838200569152832, Validation Loss: 2.2159781455993652\n",
      "Iteration 5000, Train Loss: 0.3081146478652954, Validation Loss: 2.2193000316619873\n",
      "Iteration 6000, Train Loss: 0.17182640731334686, Validation Loss: 2.2232508659362793\n",
      "Iteration 7000, Train Loss: 0.23016004264354706, Validation Loss: 2.2298030853271484\n",
      "Iteration 8000, Train Loss: 0.3398820757865906, Validation Loss: 2.220691204071045\n",
      "Test Loss: 0.4194057881832123\n",
      "Iteration 1000, Train Loss: 1.048867106437683, Validation Loss: 2.249703884124756\n",
      "Iteration 2000, Train Loss: 2.0740151405334473, Validation Loss: 2.235306978225708\n",
      "Iteration 3000, Train Loss: 0.24013106524944305, Validation Loss: 2.236820936203003\n",
      "Iteration 4000, Train Loss: 0.3781944215297699, Validation Loss: 2.218961238861084\n",
      "Iteration 5000, Train Loss: 0.3048059940338135, Validation Loss: 2.2210869789123535\n",
      "Iteration 6000, Train Loss: 0.16996052861213684, Validation Loss: 2.224652051925659\n",
      "Iteration 7000, Train Loss: 0.23119719326496124, Validation Loss: 2.229254722595215\n",
      "Iteration 8000, Train Loss: 0.33400291204452515, Validation Loss: 2.222754955291748\n",
      "Test Loss: 0.4171012043952942\n",
      "Iteration 1000, Train Loss: 1.0477174520492554, Validation Loss: 2.248563289642334\n",
      "Iteration 2000, Train Loss: 2.0697805881500244, Validation Loss: 2.233795166015625\n",
      "Iteration 3000, Train Loss: 0.24227488040924072, Validation Loss: 2.235686779022217\n",
      "Iteration 4000, Train Loss: 0.3794187903404236, Validation Loss: 2.2182865142822266\n",
      "Iteration 5000, Train Loss: 0.30662643909454346, Validation Loss: 2.220123767852783\n",
      "Iteration 6000, Train Loss: 0.17050372064113617, Validation Loss: 2.2242331504821777\n",
      "Iteration 7000, Train Loss: 0.2321644127368927, Validation Loss: 2.2287633419036865\n",
      "Iteration 8000, Train Loss: 0.33538907766342163, Validation Loss: 2.222252607345581\n",
      "Test Loss: 0.41702309250831604\n",
      "Iteration 1000, Train Loss: 1.0477354526519775, Validation Loss: 2.248354911804199\n",
      "Iteration 2000, Train Loss: 2.0690009593963623, Validation Loss: 2.233527898788452\n",
      "Iteration 3000, Train Loss: 0.24133355915546417, Validation Loss: 2.2362236976623535\n",
      "Iteration 4000, Train Loss: 0.381846159696579, Validation Loss: 2.2169957160949707\n",
      "Iteration 5000, Train Loss: 0.3075650930404663, Validation Loss: 2.2196125984191895\n",
      "Iteration 6000, Train Loss: 0.1707763671875, Validation Loss: 2.224029779434204\n",
      "Iteration 7000, Train Loss: 0.2314443588256836, Validation Loss: 2.2291340827941895\n",
      "Iteration 8000, Train Loss: 0.33667683601379395, Validation Loss: 2.221802234649658\n",
      "Test Loss: 0.41780057549476624\n",
      "Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 0.41702309250831604}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled, x_test_scaled, y_test_scaled,\n",
    "# input_size, output_size, test_window_size, scaler are available\n",
    "\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'learning_rate': [0.00005],\n",
    "    'window_size': [ 5],\n",
    "    'hidden_dim': [256],\n",
    "    'n_layers': [11],\n",
    "    'batch_evaluation_frequency': [3, 4, 5]\n",
    "}\n",
    "\n",
    "#Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256,\n",
    "#                  'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 6463.20556640625}\n",
    "\n",
    "\n",
    "# Number of random search iterations\n",
    "num_iterations = 10\n",
    "\n",
    "best_params = None\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    params = {\n",
    "        'learning_rate': random.choice(search_space['learning_rate']),\n",
    "        'window_size': random.choice(search_space['window_size']),\n",
    "        'hidden_dim': random.choice(search_space['hidden_dim']),\n",
    "        'n_layers': random.choice(search_space['n_layers']),\n",
    "        'batch_evaluation_frequency': random.choice(search_space['batch_evaluation_frequency'])\n",
    "    }\n",
    "\n",
    "    model = LSTMModel(input_size, params['hidden_dim'], params['n_layers'], output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Training using Walk-Forward Validation\n",
    "    for i in range(params['window_size'], len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_idx = i - params['window_size']\n",
    "        end_idx = i\n",
    "        x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        y_window = torch.tensor(y_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        x_window = x_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        outputs, hidden = model(x_window, hidden)\n",
    "        loss = criterion(outputs, y_window)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % params['batch_evaluation_frequency'] == 0:\n",
    "            with torch.no_grad():\n",
    "                x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n",
    "                y_val_window = torch.tensor(y_val[:params['window_size']], dtype=torch.float32)\n",
    "                x_val_window = x_val_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "                hidden = model.init_hidden(1)\n",
    "                val_outputs, _ = model(x_val_window, hidden)\n",
    "                val_loss = criterion(val_outputs, y_val_window)\n",
    "\n",
    "                if i % 1000 == 0:  # Print every 1000 iterations\n",
    "                    print(f\"Iteration {i}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # Test set evaluation\n",
    "    with torch.no_grad():\n",
    "        x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n",
    "        y_test_window = torch.tensor(y_test[:params['window_size']], dtype=torch.float32)\n",
    "        x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        test_outputs, _ = model(x_test_window, hidden)\n",
    "        test_loss = criterion(test_outputs, y_test_window)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "    # Update the best_params if the current model performs better on the test set\n",
    "    if best_params is None:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "    elif test_loss < best_params['test_loss']:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date    open    high     low   close      volume  adjusted_close  \\\n",
      "10742  2023-07-25  193.33  194.44  192.92  193.62  37283200.0        193.3589   \n",
      "10743  2023-07-26  193.67  195.64  193.32  194.50  47471900.0        194.2377   \n",
      "10744  2023-07-27  196.02  197.20  192.55  193.22  47460200.0        192.9594   \n",
      "10745  2023-07-28  194.67  196.63  194.14  195.83  48291400.0        195.5659   \n",
      "10746  2023-07-31  196.06  196.49  195.26  196.45  38824100.0        196.1851   \n",
      "\n",
      "       change_percent  avg_vol_20d  \n",
      "10742            0.45   52369165.0  \n",
      "10743            0.45   52206220.0  \n",
      "10744           -0.66   52018390.0  \n",
      "10745            1.35   52115595.0  \n",
      "10746            0.32   49803320.0  \n"
     ]
    }
   ],
   "source": [
    "# Assuming 'data' is your original DataFrame containing the time series data\n",
    "print(data.tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Rolling Window Values (Original): [[ 2.21983979e+01  2.28479971e+01  2.20528002e+01  4.69919920e+06\n",
      "   3.42700308e-01  1.87000004e+00  2.07898037e+08]\n",
      " [ 2.30496039e+01  2.39120000e+01  2.29376033e+01  6.22159998e+06\n",
      "   3.56800340e-01  8.50000004e-01  3.56143759e+08]\n",
      " [ 2.54911971e+01  2.54912027e+01  2.49311975e+01  6.70369970e+06\n",
      "   3.78700292e-01 -1.69000008e+00  2.84303878e+08]\n",
      " [ 3.12479993e+01  3.31295967e+01  3.12480012e+01  1.13840055e+06\n",
      "   1.13800448e-01  5.65999996e+00  2.25719198e+08]\n",
      " [ 1.87739997e+02  1.90010800e+02  1.87300398e+02  2.16101999e+07\n",
      "   5.76009959e+00  2.14999995e+00  5.13932433e+08]]\n",
      "Initial Rolling Window Dimensions: torch.Size([1, 5, 7])\n",
      "Predicted Close Value for the First Day after Existing Data (Original): 116.24349\n",
      "Dimensions Before Concatenation - Rolling Window: torch.Size([1, 5, 7])\n",
      "Dimensions Before Concatenation - Prediction: (1, 1)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 7 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensions Before Concatenation - Prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction_np\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Update the rolling window for the next iteration\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m rolling_window \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrolling_window\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_np\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Print the entire rolling window\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRolling Window After Prediction (Original):\u001b[39m\u001b[38;5;124m\"\u001b[39m, scaler_x\u001b[38;5;241m.\u001b[39minverse_transform(rolling_window\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_size)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 7 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Assuming 'window_size' is a parameter in your code\n",
    "params['window_size'] = 5  # Change the window_size to 5\n",
    "\n",
    "# Assuming x_train, y_train, x_val, y_val, x_test, y_test, input_size, output_size, LSTMModel are defined\n",
    "\n",
    "# Set the model's mode to evaluation before the rolling predictions\n",
    "model.eval()\n",
    "\n",
    "# Number of future days to predict\n",
    "num_future_days = 5  # You can adjust this based on your requirement\n",
    "\n",
    "# Initialize the window with the last values from the training set\n",
    "rolling_window = x_train[-params['window_size']:]\n",
    "rolling_window = rolling_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "# Print the initial rolling window values and dimensions\n",
    "print(\"Initial Rolling Window Values (Original):\", scaler_x.inverse_transform(rolling_window.view(-1, input_size)))\n",
    "print(\"Initial Rolling Window Dimensions:\", rolling_window.shape)\n",
    "\n",
    "# Perform rolling predictions\n",
    "for _ in range(num_future_days):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next day's close value\n",
    "        hidden = model.init_hidden(1)\n",
    "        prediction, _ = model(rolling_window, hidden)\n",
    "\n",
    "        # Convert the prediction to a numpy array\n",
    "        prediction_np = prediction.numpy()\n",
    "\n",
    "        # Print the predicted close value for the first day after existing data\n",
    "        predicted_close = scaler_y.inverse_transform(prediction_np)[0, 0]\n",
    "        print(\"Predicted Close Value for the First Day after Existing Data (Original):\", predicted_close)\n",
    "\n",
    "        # Print dimensions before concatenation\n",
    "        print(\"Dimensions Before Concatenation - Rolling Window:\", rolling_window.shape)\n",
    "        print(\"Dimensions Before Concatenation - Prediction:\", prediction_np.shape)\n",
    "\n",
    "        # Update the rolling window for the next iteration\n",
    "        rolling_window = torch.cat((rolling_window[:, 1:, :], torch.tensor(prediction_np).view(1, 1, -1)), dim=1)\n",
    "\n",
    "        # Print the entire rolling window\n",
    "        print(\"Rolling Window After Prediction (Original):\", scaler_x.inverse_transform(rolling_window.view(-1, input_size)))\n",
    "\n",
    "        # Print the first rolling window after starting with predictions\n",
    "        if _ == 0:\n",
    "            print(\"First Rolling Window After Starting with Predictions (Original):\", scaler_x.inverse_transform(rolling_window.view(-1, input_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Rolling Window Values: tensor([[[-0.6824, -0.6819, -0.6794, -0.5100, -0.4514,  0.6253, -0.4263],\n",
      "         [-0.6759, -0.6739, -0.6726, -0.4536, -0.4510,  0.2627,  0.1182],\n",
      "         [-0.6574, -0.6620, -0.6573, -0.4358, -0.4505, -0.6401, -0.1456],\n",
      "         [-0.6137, -0.6046, -0.6089, -0.6418, -0.4574,  1.9724, -0.3608],\n",
      "         [ 0.5734,  0.5753,  0.5865,  0.1160, -0.3108,  0.7248,  0.6978]]])\n",
      "Initial Rolling Window Dimensions: torch.Size([1, 5, 7])\n",
      "Predicted Close Value for the First Day after Existing Data: 116.24349\n",
      "Dimensions Before Concatenation - Rolling Window: torch.Size([1, 5, 7])\n",
      "Dimensions Before Concatenation - Prediction: (1, 1)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 7 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensions Before Concatenation - Prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction_np\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Update the rolling window for the next iteration\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m rolling_window \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrolling_window\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_np\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Print the entire rolling window\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRolling Window After Prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rolling_window)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 7 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Assuming 'window_size' is a parameter in your code\n",
    "params['window_size'] = 5  # Change the window_size to 5\n",
    "\n",
    "# Assuming x_train, y_train, x_val, y_val, x_test, y_test, input_size, output_size, LSTMModel are defined\n",
    "\n",
    "# Set the model's mode to evaluation before the rolling predictions\n",
    "model.eval()\n",
    "\n",
    "# Number of future days to predict\n",
    "num_future_days = 5  # You can adjust this based on your requirement\n",
    "\n",
    "# Initialize the window with the last values from the training set\n",
    "rolling_window = x_train[-params['window_size']:]\n",
    "rolling_window = rolling_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "# Print the initial rolling window values and dimensions\n",
    "print(\"Initial Rolling Window Values:\", rolling_window)\n",
    "print(\"Initial Rolling Window Dimensions:\", rolling_window.shape)\n",
    "\n",
    "# Perform rolling predictions\n",
    "for _ in range(num_future_days):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next day's close value\n",
    "        hidden = model.init_hidden(1)\n",
    "        prediction, _ = model(rolling_window, hidden)\n",
    "\n",
    "        # Convert the prediction to a numpy array\n",
    "        prediction_np = prediction.numpy()\n",
    "\n",
    "        # Print the predicted close value for the first day after existing data\n",
    "        print(\"Predicted Close Value for the First Day after Existing Data:\", scaler_y.inverse_transform(prediction_np)[0, 0])\n",
    "\n",
    "        # Print dimensions before concatenation\n",
    "        print(\"Dimensions Before Concatenation - Rolling Window:\", rolling_window.shape)\n",
    "        print(\"Dimensions Before Concatenation - Prediction:\", prediction_np.shape)\n",
    "\n",
    "        # Update the rolling window for the next iteration\n",
    "        rolling_window = torch.cat((rolling_window[:, 1:, :], torch.tensor(prediction_np).view(1, 1, -1)), dim=1)\n",
    "\n",
    "        # Print the entire rolling window\n",
    "        print(\"Rolling Window After Prediction:\", rolling_window)\n",
    "\n",
    "        # Print the first rolling window after starting with predictions\n",
    "        if _ == 0:\n",
    "            print(\"First Rolling Window After Starting with Predictions:\", rolling_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Close Value for the First Day after Existing Data: 116.24349\n",
      "Dimensions Before Concatenation - Rolling Window: torch.Size([1, 5, 7])\n",
      "Dimensions Before Concatenation - Prediction: (1, 1)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 7 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensions Before Concatenation - Prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction_np\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Update the rolling window for the next iteration\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m rolling_window \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrolling_window\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_np\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Print the entire rolling window\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRolling Window After Prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rolling_window)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 7 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Assuming 'window_size' is a parameter in your code\n",
    "params['window_size'] = 5  # Change the window_size to 5\n",
    "\n",
    "# Assuming x_train, y_train, x_val, y_val, x_test, y_test, input_size, output_size, LSTMModel are defined\n",
    "\n",
    "# Set the model's mode to evaluation before the rolling predictions\n",
    "model.eval()\n",
    "\n",
    "# Number of future days to predict\n",
    "num_future_days = 5  # You can adjust this based on your requirement\n",
    "\n",
    "# Initialize the window with the last values from the training set\n",
    "rolling_window = x_train[-params['window_size']:]\n",
    "rolling_window = rolling_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "# Perform rolling predictions\n",
    "for _ in range(num_future_days):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next day's close value\n",
    "        hidden = model.init_hidden(1)\n",
    "        prediction, _ = model(rolling_window, hidden)\n",
    "\n",
    "        # Convert the prediction to a numpy array\n",
    "        prediction_np = prediction.numpy()\n",
    "\n",
    "        # Print the predicted close value for the first day after existing data\n",
    "        print(\"Predicted Close Value for the First Day after Existing Data:\", scaler_y.inverse_transform(prediction_np)[0, 0])\n",
    "\n",
    "        # Print dimensions before concatenation\n",
    "        print(\"Dimensions Before Concatenation - Rolling Window:\", rolling_window.shape)\n",
    "        print(\"Dimensions Before Concatenation - Prediction:\", prediction_np.shape)\n",
    "\n",
    "        # Update the rolling window for the next iteration\n",
    "        rolling_window = torch.cat((rolling_window[:, 1:, :], torch.tensor(prediction_np).view(1, 1, -1)), dim=1)\n",
    "\n",
    "        # Print the entire rolling window\n",
    "        print(\"Rolling Window After Prediction:\", rolling_window)\n",
    "\n",
    "        # Print the first rolling window after starting with predictions\n",
    "        if _ == 0:\n",
    "            print(\"First Rolling Window After Starting with Predictions:\", rolling_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 7 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m prediction_np \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Add the predicted value to the rolling window\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m rolling_window \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrolling_window\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_np\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Print or store the predicted value\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Close Value:\u001b[39m\u001b[38;5;124m\"\u001b[39m, scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(prediction_np)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 7 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Assuming x_train, y_train, x_val, y_val, x_test, y_test, input_size, output_size, LSTMModel are defined\n",
    "\n",
    "# Set the model's mode to evaluation before the rolling predictions\n",
    "model.eval()\n",
    "\n",
    "# Number of future days to predict\n",
    "num_future_days = 5  # You can adjust this based on your requirement\n",
    "\n",
    "# Initialize the window with the last values from the training set\n",
    "rolling_window = x_train[-params['window_size']:]\n",
    "rolling_window = rolling_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "# Perform rolling predictions\n",
    "for _ in range(num_future_days):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next day's close value\n",
    "        hidden = model.init_hidden(1)\n",
    "        prediction, _ = model(rolling_window, hidden)\n",
    "\n",
    "        # Convert the prediction to a numpy array\n",
    "        prediction_np = prediction.numpy()\n",
    "\n",
    "        # Add the predicted value to the rolling window\n",
    "        rolling_window = torch.cat((rolling_window[:, 1:, :], torch.tensor(prediction_np).view(1, 1, -1)), dim=1)\n",
    "\n",
    "        # Print or store the predicted value\n",
    "        print(\"Predicted Close Value:\", scaler_y.inverse_transform(prediction_np)[0, 0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
