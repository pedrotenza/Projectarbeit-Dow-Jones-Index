{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the CSV file.\n",
    "Truncating the data to 10747 rows.\n",
    "Handling missing values by filling them with zeros.\n",
    "Converting specific columns to float data type.\n",
    "\n",
    "\n",
    "#Standardizing selected columns using StandardScaler from sklearn.preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date    open    high     low   close      volume  adjusted_close  \\\n",
      "10746  2023-07-31  196.06  196.49  195.26  196.45  38824100.0        196.1851   \n",
      "\n",
      "       change_percent  avg_vol_20d  \n",
      "10746            0.32   49803320.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "print(data.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert date to torch: date_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with a 'date' column containing dates from 1980-12-12 to 2023-07-31\n",
    "data2 = pd.DataFrame({'date': pd.date_range(start='1980-12-12', end='2023-07-31')})\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "data2['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Extract day, month, and year from the date column\n",
    "data['day'] = data2['date'].dt.day\n",
    "data['month'] = data2['date'].dt.month\n",
    "data['year'] = data2['date'].dt.year\n",
    "\n",
    "# Define the embedding dimensions\n",
    "embedding_dim = 1  # You can adjust this dimension as needed\n",
    "\n",
    "# Create embedding layers for day, month, and year\n",
    "day_embedding = nn.Embedding(32, embedding_dim)  # 0-31 days\n",
    "month_embedding = nn.Embedding(13, embedding_dim)  # 1-12 months\n",
    "year_embedding = nn.Embedding(44, embedding_dim)  # Embedding for years from 1980 to 2023\n",
    "\n",
    "# Convert day, month, and year to tensors with Long data type\n",
    "day_tensor = torch.LongTensor(data['day'].values)\n",
    "month_tensor = torch.LongTensor(data['month'].values)\n",
    "year_tensor = torch.LongTensor(data['year'].values - 1980)  # Convert years to an index from 0 to 43\n",
    "\n",
    "\n",
    "# Pass tensors through embedding layers to get embeddings\n",
    "day_embeddings = day_embedding(day_tensor)\n",
    "month_embeddings = month_embedding(month_tensor)\n",
    "year_embeddings = year_embedding(year_tensor)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "date_embeddings = torch.cat((day_embeddings, month_embeddings, year_embeddings), dim=1)\n",
    "\n",
    "# Print the resulting embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate Date Embeddings - Features\n",
    "Split\n",
    "Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "y_data = data[\"close\"].values.reshape(-1, 1)  # Reshape y_data if it's 1D\n",
    "\n",
    "# Concatenate date embeddings with data\n",
    "x_feature_tensors = torch.tensor(x_data, dtype=torch.float32)\n",
    "x_combined = torch.cat((x_feature_tensors, date_embeddings), dim=1)\n",
    "\n",
    "y_feature_tensors = torch.tensor(y_data, dtype=torch.float32)\n",
    "y_combined = torch.cat((y_feature_tensors, date_embeddings), dim=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(x_combined) * 0.67)\n",
    "x_train, x_test = x_combined[:train_size], x_combined[train_size:]\n",
    "y_train, y_test = y_combined[:train_size], y_combined[train_size:]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler only on the training data and transform both training and test data\n",
    "# fitting the MinMaxScaler to your training data using fit_transform and then\n",
    "# applying the same transformation (based on the statistics learned) to your test data using transform\n",
    "x_train_scaled = scaler.fit_transform(x_train.detach().numpy())\n",
    "x_test_scaled = scaler.transform(x_test.detach().numpy())  # Use transform, not fit_transform\n",
    "\n",
    "y_train_scaled = scaler.fit_transform(y_train.detach().numpy())\n",
    "y_test_scaled = scaler.transform(y_test.detach().numpy())  # Use transform, not fit_transform\n",
    "\n",
    "# Convert back to tensors if needed\n",
    "x_train_scaled = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_test_scaled = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "\n",
    "y_train_scaled = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "y_test_scaled = torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model Definition\n",
    "Initialize the model, loss, and optimizer\n",
    "Training using Walk-Forward Validation\n",
    "Backward pass and optimize\n",
    "Evaluate the model on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3284\\1154295661.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_window = torch.tensor(x_train_scaled[start_idx:end_idx], dtype=torch.float32)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3284\\1154295661.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_window = torch.tensor(y_train_scaled[start_idx:end_idx], dtype=torch.float32)\n",
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([50, 4])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3284\\1154295661.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_window = torch.tensor(x_test_scaled[:test_window_size], dtype=torch.float32)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3284\\1154295661.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_window = torch.tensor(y_test_scaled[:test_window_size], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, Train Loss: 0.1081152930855751, Test Loss: 0.259853333234787\n",
      "Iteration 60, Train Loss: 0.1157444417476654, Test Loss: 0.1605028212070465\n",
      "Iteration 70, Train Loss: 0.13963651657104492, Test Loss: 0.1759324073791504\n",
      "Iteration 80, Train Loss: 0.1515057384967804, Test Loss: 0.12344531714916229\n",
      "Iteration 90, Train Loss: 0.14506983757019043, Test Loss: 0.13202723860740662\n",
      "Iteration 100, Train Loss: 0.13277091085910797, Test Loss: 0.13700547814369202\n",
      "Iteration 110, Train Loss: 0.11867053806781769, Test Loss: 0.15371064841747284\n",
      "Iteration 120, Train Loss: 0.09899081289768219, Test Loss: 0.16038472950458527\n",
      "Iteration 130, Train Loss: 0.08744464814662933, Test Loss: 0.19435039162635803\n",
      "Iteration 140, Train Loss: 0.0831906795501709, Test Loss: 0.2209063023328781\n",
      "Iteration 150, Train Loss: 0.0960044115781784, Test Loss: 0.24022653698921204\n",
      "Iteration 160, Train Loss: 0.10042592883110046, Test Loss: 0.2109508514404297\n",
      "Iteration 170, Train Loss: 0.11298396438360214, Test Loss: 0.21619199216365814\n",
      "Iteration 180, Train Loss: 0.11901361495256424, Test Loss: 0.16818545758724213\n",
      "Iteration 190, Train Loss: 0.11896107345819473, Test Loss: 0.14743930101394653\n",
      "Iteration 200, Train Loss: 0.10765507817268372, Test Loss: 0.18115971982479095\n",
      "Iteration 210, Train Loss: 0.09639899432659149, Test Loss: 0.20167845487594604\n",
      "Iteration 220, Train Loss: 0.07995808124542236, Test Loss: 0.22221100330352783\n",
      "Iteration 230, Train Loss: 0.06711207330226898, Test Loss: 0.25740405917167664\n",
      "Iteration 240, Train Loss: 0.06901086121797562, Test Loss: 0.2521727681159973\n",
      "Iteration 250, Train Loss: 0.06854851543903351, Test Loss: 0.23224548995494843\n",
      "Iteration 260, Train Loss: 0.06957462430000305, Test Loss: 0.2343708872795105\n",
      "Iteration 270, Train Loss: 0.07397949695587158, Test Loss: 0.23197557032108307\n",
      "Iteration 280, Train Loss: 0.08801545947790146, Test Loss: 0.1895621120929718\n",
      "Iteration 290, Train Loss: 0.1023048385977745, Test Loss: 0.1817084103822708\n",
      "Iteration 300, Train Loss: 0.12077707052230835, Test Loss: 0.13244447112083435\n",
      "Iteration 310, Train Loss: 0.12995009124279022, Test Loss: 0.10180607438087463\n",
      "Iteration 320, Train Loss: 0.13574275374412537, Test Loss: 0.07673726230859756\n",
      "Iteration 330, Train Loss: 0.1364266723394394, Test Loss: 0.060409024357795715\n",
      "Iteration 340, Train Loss: 0.13305653631687164, Test Loss: 0.05502331629395485\n",
      "Iteration 350, Train Loss: 0.1283246874809265, Test Loss: 0.05637844651937485\n",
      "Iteration 360, Train Loss: 0.12390520423650742, Test Loss: 0.06467343866825104\n",
      "Iteration 370, Train Loss: 0.11300700902938843, Test Loss: 0.06951936334371567\n",
      "Iteration 380, Train Loss: 0.11839338392019272, Test Loss: 0.06738481670618057\n",
      "Iteration 390, Train Loss: 0.12515512108802795, Test Loss: 0.08302248269319534\n",
      "Iteration 400, Train Loss: 0.13206203281879425, Test Loss: 0.10201115906238556\n",
      "Iteration 410, Train Loss: 0.13448019325733185, Test Loss: 0.09685566276311874\n",
      "Iteration 420, Train Loss: 0.1406005620956421, Test Loss: 0.09781654179096222\n",
      "Iteration 430, Train Loss: 0.1314391940832138, Test Loss: 0.08319717645645142\n",
      "Iteration 440, Train Loss: 0.12428611516952515, Test Loss: 0.05656613036990166\n",
      "Iteration 450, Train Loss: 0.12110665440559387, Test Loss: 0.06026896461844444\n",
      "Iteration 460, Train Loss: 0.11983609944581985, Test Loss: 0.07884121686220169\n",
      "Iteration 470, Train Loss: 0.11487636715173721, Test Loss: 0.09074585884809494\n",
      "Iteration 480, Train Loss: 0.10763866454362869, Test Loss: 0.11483696103096008\n",
      "Iteration 490, Train Loss: 0.10214746743440628, Test Loss: 0.10844216495752335\n",
      "Iteration 500, Train Loss: 0.09792111068964005, Test Loss: 0.09254331886768341\n",
      "Iteration 510, Train Loss: 0.09675642102956772, Test Loss: 0.07576033473014832\n",
      "Iteration 520, Train Loss: 0.09496608376502991, Test Loss: 0.07717268168926239\n",
      "Iteration 530, Train Loss: 0.091493159532547, Test Loss: 0.08296625316143036\n",
      "Iteration 540, Train Loss: 0.08407416194677353, Test Loss: 0.10071419924497604\n",
      "Iteration 550, Train Loss: 0.08645060658454895, Test Loss: 0.12057700753211975\n",
      "Iteration 560, Train Loss: 0.08316861093044281, Test Loss: 0.10417522490024567\n",
      "Iteration 570, Train Loss: 0.0858687087893486, Test Loss: 0.09606298804283142\n",
      "Iteration 580, Train Loss: 0.08624961972236633, Test Loss: 0.07176921516656876\n",
      "Iteration 590, Train Loss: 0.08607830852270126, Test Loss: 0.04655411094427109\n",
      "Iteration 600, Train Loss: 0.07798442244529724, Test Loss: 0.053305674344301224\n",
      "Iteration 610, Train Loss: 0.07414651662111282, Test Loss: 0.0620453916490078\n",
      "Iteration 620, Train Loss: 0.059416837990283966, Test Loss: 0.06816580146551132\n",
      "Iteration 630, Train Loss: 0.054628804326057434, Test Loss: 0.04977400228381157\n",
      "Iteration 640, Train Loss: 0.05773432180285454, Test Loss: 0.055219441652297974\n",
      "Iteration 650, Train Loss: 0.06512836366891861, Test Loss: 0.12327781319618225\n",
      "Iteration 660, Train Loss: 0.07093217968940735, Test Loss: 0.12151000648736954\n",
      "Iteration 670, Train Loss: 0.07749444246292114, Test Loss: 0.11520718783140182\n",
      "Iteration 680, Train Loss: 0.07942483574151993, Test Loss: 0.100738525390625\n",
      "Iteration 690, Train Loss: 0.07546979188919067, Test Loss: 0.08291560411453247\n",
      "Iteration 700, Train Loss: 0.07522827386856079, Test Loss: 0.05728871375322342\n",
      "Iteration 710, Train Loss: 0.07567806541919708, Test Loss: 0.07740193605422974\n",
      "Iteration 720, Train Loss: 0.07639113068580627, Test Loss: 0.09623301774263382\n",
      "Iteration 730, Train Loss: 0.0713590681552887, Test Loss: 0.11133413016796112\n",
      "Iteration 740, Train Loss: 0.07014110684394836, Test Loss: 0.14431160688400269\n",
      "Iteration 750, Train Loss: 0.06764562427997589, Test Loss: 0.12967047095298767\n",
      "Iteration 760, Train Loss: 0.0689619779586792, Test Loss: 0.09486522525548935\n",
      "Iteration 770, Train Loss: 0.06822964549064636, Test Loss: 0.11131337285041809\n",
      "Iteration 780, Train Loss: 0.06970169395208359, Test Loss: 0.11642565578222275\n",
      "Iteration 790, Train Loss: 0.06492205709218979, Test Loss: 0.1239851787686348\n",
      "Iteration 800, Train Loss: 0.073025181889534, Test Loss: 0.17902319133281708\n",
      "Iteration 810, Train Loss: 0.08129597455263138, Test Loss: 0.13743458688259125\n",
      "Iteration 820, Train Loss: 0.08913011848926544, Test Loss: 0.13666675984859467\n",
      "Iteration 830, Train Loss: 0.09850863367319107, Test Loss: 0.11803992092609406\n",
      "Iteration 840, Train Loss: 0.10503926128149033, Test Loss: 0.07629063725471497\n",
      "Iteration 850, Train Loss: 0.09805743396282196, Test Loss: 0.07836727052927017\n",
      "Iteration 860, Train Loss: 0.09092487394809723, Test Loss: 0.09648500382900238\n",
      "Iteration 870, Train Loss: 0.08005694299936295, Test Loss: 0.1016971692442894\n",
      "Iteration 880, Train Loss: 0.06728045642375946, Test Loss: 0.10303377360105515\n",
      "Iteration 890, Train Loss: 0.06545086950063705, Test Loss: 0.07958479225635529\n",
      "Iteration 900, Train Loss: 0.07352890819311142, Test Loss: 0.08857294172048569\n",
      "Iteration 910, Train Loss: 0.08148276060819626, Test Loss: 0.17086105048656464\n",
      "Iteration 920, Train Loss: 0.08391696959733963, Test Loss: 0.15302957594394684\n",
      "Iteration 930, Train Loss: 0.09022117406129837, Test Loss: 0.12302877753973007\n",
      "Iteration 940, Train Loss: 0.08319776505231857, Test Loss: 0.11021941900253296\n",
      "Iteration 950, Train Loss: 0.08034005761146545, Test Loss: 0.05080951377749443\n",
      "Iteration 960, Train Loss: 0.07563606649637222, Test Loss: 0.07138978689908981\n",
      "Iteration 970, Train Loss: 0.07156520336866379, Test Loss: 0.0856354683637619\n",
      "Iteration 980, Train Loss: 0.06395388394594193, Test Loss: 0.1004052385687828\n",
      "Iteration 990, Train Loss: 0.05649792030453682, Test Loss: 0.14355126023292542\n",
      "Iteration 1000, Train Loss: 0.05568757653236389, Test Loss: 0.13589218258857727\n",
      "Iteration 1010, Train Loss: 0.057893719524145126, Test Loss: 0.13206903636455536\n",
      "Iteration 1020, Train Loss: 0.059423502534627914, Test Loss: 0.10168840736150742\n",
      "Iteration 1030, Train Loss: 0.059829231351614, Test Loss: 0.12331771850585938\n",
      "Iteration 1040, Train Loss: 0.06337077915668488, Test Loss: 0.11391815543174744\n",
      "Iteration 1050, Train Loss: 0.0663842260837555, Test Loss: 0.12613040208816528\n",
      "Iteration 1060, Train Loss: 0.08175063133239746, Test Loss: 0.14541420340538025\n",
      "Iteration 1070, Train Loss: 0.09053972363471985, Test Loss: 0.1309354454278946\n",
      "Iteration 1080, Train Loss: 0.10470704734325409, Test Loss: 0.11439523845911026\n",
      "Iteration 1090, Train Loss: 0.11024566739797592, Test Loss: 0.09888975322246552\n",
      "Iteration 1100, Train Loss: 0.10744976997375488, Test Loss: 0.050526995211839676\n",
      "Iteration 1110, Train Loss: 0.09965147823095322, Test Loss: 0.06970485299825668\n",
      "Iteration 1120, Train Loss: 0.0939083844423294, Test Loss: 0.07998678088188171\n",
      "Iteration 1130, Train Loss: 0.08001477271318436, Test Loss: 0.09368998557329178\n",
      "Iteration 1140, Train Loss: 0.08644319325685501, Test Loss: 0.04109060764312744\n",
      "Iteration 1150, Train Loss: 0.09154371917247772, Test Loss: 0.054674696177244186\n",
      "Iteration 1160, Train Loss: 0.10442798584699631, Test Loss: 0.1297767609357834\n",
      "Iteration 1170, Train Loss: 0.10328194499015808, Test Loss: 0.10336035490036011\n",
      "Iteration 1180, Train Loss: 0.10946822166442871, Test Loss: 0.11942946165800095\n",
      "Iteration 1190, Train Loss: 0.10187461972236633, Test Loss: 0.082462377846241\n",
      "Iteration 1200, Train Loss: 0.0997438132762909, Test Loss: 0.08796950429677963\n",
      "Iteration 1210, Train Loss: 0.09575854986906052, Test Loss: 0.07729067653417587\n",
      "Iteration 1220, Train Loss: 0.09408992528915405, Test Loss: 0.08502200245857239\n",
      "Iteration 1230, Train Loss: 0.0880807489156723, Test Loss: 0.10148316621780396\n",
      "Iteration 1240, Train Loss: 0.08056233078241348, Test Loss: 0.11166542023420334\n",
      "Iteration 1250, Train Loss: 0.07718127965927124, Test Loss: 0.14038170874118805\n",
      "Iteration 1260, Train Loss: 0.0742352083325386, Test Loss: 0.11339320242404938\n",
      "Iteration 1270, Train Loss: 0.07552892714738846, Test Loss: 0.09159531444311142\n",
      "Iteration 1280, Train Loss: 0.07550109177827835, Test Loss: 0.09833238273859024\n",
      "Iteration 1290, Train Loss: 0.07207804918289185, Test Loss: 0.09904639422893524\n",
      "Iteration 1300, Train Loss: 0.06785330921411514, Test Loss: 0.10803131014108658\n",
      "Iteration 1310, Train Loss: 0.07917375862598419, Test Loss: 0.15594463050365448\n",
      "Iteration 1320, Train Loss: 0.08740803599357605, Test Loss: 0.14089663326740265\n",
      "Iteration 1330, Train Loss: 0.09886371344327927, Test Loss: 0.12436416745185852\n",
      "Iteration 1340, Train Loss: 0.10672523826360703, Test Loss: 0.09706154465675354\n",
      "Iteration 1350, Train Loss: 0.10375022888183594, Test Loss: 0.04666135460138321\n",
      "Iteration 1360, Train Loss: 0.09106037765741348, Test Loss: 0.06623716652393341\n",
      "Iteration 1370, Train Loss: 0.08064477890729904, Test Loss: 0.07492808997631073\n",
      "Iteration 1380, Train Loss: 0.06296619027853012, Test Loss: 0.07995671033859253\n",
      "Iteration 1390, Train Loss: 0.0611795037984848, Test Loss: 0.11585432291030884\n",
      "Iteration 1400, Train Loss: 0.06167546659708023, Test Loss: 0.12558816373348236\n",
      "Iteration 1410, Train Loss: 0.06857950985431671, Test Loss: 0.12424525618553162\n",
      "Iteration 1420, Train Loss: 0.07331584393978119, Test Loss: 0.1302834004163742\n",
      "Iteration 1430, Train Loss: 0.07908584922552109, Test Loss: 0.12753231823444366\n",
      "Iteration 1440, Train Loss: 0.07755416631698608, Test Loss: 0.10822361707687378\n",
      "Iteration 1450, Train Loss: 0.07330566644668579, Test Loss: 0.08509807288646698\n",
      "Iteration 1460, Train Loss: 0.0700295940041542, Test Loss: 0.0722292959690094\n",
      "Iteration 1470, Train Loss: 0.06609369069337845, Test Loss: 0.0970396026968956\n",
      "Iteration 1480, Train Loss: 0.059987328946590424, Test Loss: 0.10957972705364227\n",
      "Iteration 1490, Train Loss: 0.051263805478811264, Test Loss: 0.11955561488866806\n",
      "Iteration 1500, Train Loss: 0.04923611134290695, Test Loss: 0.15820662677288055\n",
      "Iteration 1510, Train Loss: 0.04729516804218292, Test Loss: 0.14176656305789948\n",
      "Iteration 1520, Train Loss: 0.04817874729633331, Test Loss: 0.09531936794519424\n",
      "Iteration 1530, Train Loss: 0.045391377061605453, Test Loss: 0.09924425184726715\n",
      "Iteration 1540, Train Loss: 0.04791998490691185, Test Loss: 0.10741820931434631\n",
      "Iteration 1550, Train Loss: 0.045926302671432495, Test Loss: 0.09698733687400818\n",
      "Iteration 1560, Train Loss: 0.06076829135417938, Test Loss: 0.13608652353286743\n",
      "Iteration 1570, Train Loss: 0.06417669355869293, Test Loss: 0.11864031851291656\n",
      "Iteration 1580, Train Loss: 0.07470455765724182, Test Loss: 0.10081412643194199\n",
      "Iteration 1590, Train Loss: 0.07620719075202942, Test Loss: 0.07110973447561264\n",
      "Iteration 1600, Train Loss: 0.07134725153446198, Test Loss: 0.039254914969205856\n",
      "Iteration 1610, Train Loss: 0.05908562242984772, Test Loss: 0.03761296719312668\n",
      "Iteration 1620, Train Loss: 0.05492856353521347, Test Loss: 0.0470614954829216\n",
      "Iteration 1630, Train Loss: 0.04128974303603172, Test Loss: 0.04959869384765625\n",
      "Iteration 1640, Train Loss: 0.040968116372823715, Test Loss: 0.038241952657699585\n",
      "Iteration 1650, Train Loss: 0.05054424703121185, Test Loss: 0.037728894501924515\n",
      "Iteration 1660, Train Loss: 0.062213487923145294, Test Loss: 0.03886771574616432\n",
      "Iteration 1670, Train Loss: 0.06813639402389526, Test Loss: 0.06771977990865707\n",
      "Iteration 1680, Train Loss: 0.07545456290245056, Test Loss: 0.07568729668855667\n",
      "Iteration 1690, Train Loss: 0.0779707133769989, Test Loss: 0.06594976782798767\n",
      "Iteration 1700, Train Loss: 0.06832478195428848, Test Loss: 0.05579037964344025\n",
      "Iteration 1710, Train Loss: 0.06681722402572632, Test Loss: 0.05466211214661598\n",
      "Iteration 1720, Train Loss: 0.0619877465069294, Test Loss: 0.07494448870420456\n",
      "Iteration 1730, Train Loss: 0.05777096375823021, Test Loss: 0.09178556501865387\n",
      "Iteration 1740, Train Loss: 0.05298586189746857, Test Loss: 0.12350192666053772\n",
      "Iteration 1750, Train Loss: 0.053661052137613297, Test Loss: 0.10210459679365158\n",
      "Iteration 1760, Train Loss: 0.049925170838832855, Test Loss: 0.1108180582523346\n",
      "Iteration 1770, Train Loss: 0.054987963289022446, Test Loss: 0.16651035845279694\n",
      "Iteration 1780, Train Loss: 0.05606333911418915, Test Loss: 0.17135688662528992\n",
      "Iteration 1790, Train Loss: 0.05914488434791565, Test Loss: 0.1541900485754013\n",
      "Iteration 1800, Train Loss: 0.05810804292559624, Test Loss: 0.15511250495910645\n",
      "Iteration 1810, Train Loss: 0.06814970076084137, Test Loss: 0.09177222847938538\n",
      "Iteration 1820, Train Loss: 0.07605941593647003, Test Loss: 0.10081024467945099\n",
      "Iteration 1830, Train Loss: 0.08344421535730362, Test Loss: 0.06594158709049225\n",
      "Iteration 1840, Train Loss: 0.08826177567243576, Test Loss: 0.05678427219390869\n",
      "Iteration 1850, Train Loss: 0.09026595205068588, Test Loss: 0.05010658875107765\n",
      "Iteration 1860, Train Loss: 0.08623684197664261, Test Loss: 0.046798743307590485\n",
      "Iteration 1870, Train Loss: 0.08037775754928589, Test Loss: 0.051100198179483414\n",
      "Iteration 1880, Train Loss: 0.07445524632930756, Test Loss: 0.0559852309525013\n",
      "Iteration 1890, Train Loss: 0.06704285740852356, Test Loss: 0.06541028618812561\n",
      "Iteration 1900, Train Loss: 0.06878164410591125, Test Loss: 0.0830545499920845\n",
      "Iteration 1910, Train Loss: 0.07409786432981491, Test Loss: 0.08749092370271683\n",
      "Iteration 1920, Train Loss: 0.0812913253903389, Test Loss: 0.07733529061079025\n",
      "Iteration 1930, Train Loss: 0.08257798850536346, Test Loss: 0.09045825898647308\n",
      "Iteration 1940, Train Loss: 0.08507544547319412, Test Loss: 0.07766636461019516\n",
      "Iteration 1950, Train Loss: 0.07609327882528305, Test Loss: 0.06619924306869507\n",
      "Iteration 1960, Train Loss: 0.07113195210695267, Test Loss: 0.058598943054676056\n",
      "Iteration 1970, Train Loss: 0.0700203999876976, Test Loss: 0.06198413670063019\n",
      "Iteration 1980, Train Loss: 0.0722779706120491, Test Loss: 0.0745309516787529\n",
      "Iteration 1990, Train Loss: 0.06803347915410995, Test Loss: 0.08297589421272278\n",
      "Iteration 2000, Train Loss: 0.06469002366065979, Test Loss: 0.09779000282287598\n",
      "Iteration 2010, Train Loss: 0.06172221526503563, Test Loss: 0.10125045478343964\n",
      "Iteration 2020, Train Loss: 0.061012253165245056, Test Loss: 0.08820570260286331\n",
      "Iteration 2030, Train Loss: 0.061346616595983505, Test Loss: 0.07757984101772308\n",
      "Iteration 2040, Train Loss: 0.06192266568541527, Test Loss: 0.07907746732234955\n",
      "Iteration 2050, Train Loss: 0.062015876173973083, Test Loss: 0.08480378240346909\n",
      "Iteration 2060, Train Loss: 0.06497517228126526, Test Loss: 0.09280089288949966\n",
      "Iteration 2070, Train Loss: 0.07474308460950851, Test Loss: 0.11521411687135696\n",
      "Iteration 2080, Train Loss: 0.08104380965232849, Test Loss: 0.08623505383729935\n",
      "Iteration 2090, Train Loss: 0.08860334753990173, Test Loss: 0.07493198662996292\n",
      "Iteration 2100, Train Loss: 0.09433507919311523, Test Loss: 0.06244027614593506\n",
      "Iteration 2110, Train Loss: 0.0905916690826416, Test Loss: 0.04544215276837349\n",
      "Iteration 2120, Train Loss: 0.08326789736747742, Test Loss: 0.048681024461984634\n",
      "Iteration 2130, Train Loss: 0.07636464387178421, Test Loss: 0.06020418554544449\n",
      "Iteration 2140, Train Loss: 0.062285248190164566, Test Loss: 0.06562718003988266\n",
      "Iteration 2150, Train Loss: 0.05949747934937477, Test Loss: 0.04687448963522911\n",
      "Iteration 2160, Train Loss: 0.06520412862300873, Test Loss: 0.043675851076841354\n",
      "Iteration 2170, Train Loss: 0.07507166266441345, Test Loss: 0.052594881504774094\n",
      "Iteration 2180, Train Loss: 0.07707792520523071, Test Loss: 0.07501109689474106\n",
      "Iteration 2190, Train Loss: 0.08272548019886017, Test Loss: 0.08517731726169586\n",
      "Iteration 2200, Train Loss: 0.07756456732749939, Test Loss: 0.06928987801074982\n",
      "Iteration 2210, Train Loss: 0.07063062489032745, Test Loss: 0.0594598725438118\n",
      "Iteration 2220, Train Loss: 0.06672704964876175, Test Loss: 0.04187987744808197\n",
      "Iteration 2230, Train Loss: 0.06584210693836212, Test Loss: 0.040002401918172836\n",
      "Iteration 2240, Train Loss: 0.0625496506690979, Test Loss: 0.0402231402695179\n",
      "Iteration 2250, Train Loss: 0.05672747269272804, Test Loss: 0.04061685502529144\n",
      "Iteration 2260, Train Loss: 0.05317199230194092, Test Loss: 0.04073890671133995\n",
      "Iteration 2270, Train Loss: 0.049129098653793335, Test Loss: 0.04092719405889511\n",
      "Iteration 2280, Train Loss: 0.05266571417450905, Test Loss: 0.04125789552927017\n",
      "Iteration 2290, Train Loss: 0.05506060644984245, Test Loss: 0.04160211980342865\n",
      "Iteration 2300, Train Loss: 0.06003427132964134, Test Loss: 0.04177432507276535\n",
      "Iteration 2310, Train Loss: 0.06275688856840134, Test Loss: 0.04182188957929611\n",
      "Iteration 2320, Train Loss: 0.07964683324098587, Test Loss: 0.04501412808895111\n",
      "Iteration 2330, Train Loss: 0.08608382195234299, Test Loss: 0.05441639572381973\n",
      "Iteration 2340, Train Loss: 0.09685131907463074, Test Loss: 0.06264308840036392\n",
      "Iteration 2350, Train Loss: 0.09929594397544861, Test Loss: 0.05029303580522537\n",
      "Iteration 2360, Train Loss: 0.09699979424476624, Test Loss: 0.04487185925245285\n",
      "Iteration 2370, Train Loss: 0.08883111923933029, Test Loss: 0.04413710907101631\n",
      "Iteration 2380, Train Loss: 0.08462344110012054, Test Loss: 0.04617584869265556\n",
      "Iteration 2390, Train Loss: 0.07081259042024612, Test Loss: 0.04949193447828293\n",
      "Iteration 2400, Train Loss: 0.07022502273321152, Test Loss: 0.055491141974925995\n",
      "Iteration 2410, Train Loss: 0.07612480968236923, Test Loss: 0.05138687789440155\n",
      "Iteration 2420, Train Loss: 0.08417432010173798, Test Loss: 0.053614746779203415\n",
      "Iteration 2430, Train Loss: 0.08748585730791092, Test Loss: 0.05605495348572731\n",
      "Iteration 2440, Train Loss: 0.09383542835712433, Test Loss: 0.0553591363132\n",
      "Iteration 2450, Train Loss: 0.08802243322134018, Test Loss: 0.06989046186208725\n",
      "Iteration 2460, Train Loss: 0.07925436645746231, Test Loss: 0.05370885878801346\n",
      "Iteration 2470, Train Loss: 0.07990554720163345, Test Loss: 0.06740327924489975\n",
      "Iteration 2480, Train Loss: 0.08158297836780548, Test Loss: 0.06607314199209213\n",
      "Iteration 2490, Train Loss: 0.08189476281404495, Test Loss: 0.07428698986768723\n",
      "Iteration 2500, Train Loss: 0.07758575677871704, Test Loss: 0.08370713144540787\n",
      "Iteration 2510, Train Loss: 0.07423047721385956, Test Loss: 0.085222527384758\n",
      "Iteration 2520, Train Loss: 0.06958770006895065, Test Loss: 0.0911988765001297\n",
      "Iteration 2530, Train Loss: 0.07148180902004242, Test Loss: 0.09656981378793716\n",
      "Iteration 2540, Train Loss: 0.06903651356697083, Test Loss: 0.08347413688898087\n",
      "Iteration 2550, Train Loss: 0.06596769392490387, Test Loss: 0.07759211212396622\n",
      "Iteration 2560, Train Loss: 0.059698525816202164, Test Loss: 0.08412878215312958\n",
      "Iteration 2570, Train Loss: 0.06161035597324371, Test Loss: 0.0848580151796341\n",
      "Iteration 2580, Train Loss: 0.0628991648554802, Test Loss: 0.10641759634017944\n",
      "Iteration 2590, Train Loss: 0.06983324885368347, Test Loss: 0.08101730048656464\n",
      "Iteration 2600, Train Loss: 0.07643555849790573, Test Loss: 0.06567560136318207\n",
      "Iteration 2610, Train Loss: 0.07415786385536194, Test Loss: 0.061829447746276855\n",
      "Iteration 2620, Train Loss: 0.06658371537923813, Test Loss: 0.051505859941244125\n",
      "Iteration 2630, Train Loss: 0.060620494186878204, Test Loss: 0.06011519581079483\n",
      "Iteration 2640, Train Loss: 0.04942069575190544, Test Loss: 0.06335708498954773\n",
      "Iteration 2650, Train Loss: 0.044262174516916275, Test Loss: 0.07695488631725311\n",
      "Iteration 2660, Train Loss: 0.05303976684808731, Test Loss: 0.08626297861337662\n",
      "Iteration 2670, Train Loss: 0.06084973365068436, Test Loss: 0.08558641374111176\n",
      "Iteration 2680, Train Loss: 0.0672905221581459, Test Loss: 0.09361857175827026\n",
      "Iteration 2690, Train Loss: 0.07023460417985916, Test Loss: 0.11584202945232391\n",
      "Iteration 2700, Train Loss: 0.07184325158596039, Test Loss: 0.09692812711000443\n",
      "Iteration 2710, Train Loss: 0.06460123509168625, Test Loss: 0.07435733079910278\n",
      "Iteration 2720, Train Loss: 0.06167863309383392, Test Loss: 0.07649119198322296\n",
      "Iteration 2730, Train Loss: 0.05618840083479881, Test Loss: 0.07007806748151779\n",
      "Iteration 2740, Train Loss: 0.05291483923792839, Test Loss: 0.07516364753246307\n",
      "Iteration 2750, Train Loss: 0.04371025040745735, Test Loss: 0.083038330078125\n",
      "Iteration 2760, Train Loss: 0.0367533378303051, Test Loss: 0.08905881643295288\n",
      "Iteration 2770, Train Loss: 0.035337578505277634, Test Loss: 0.09658568352460861\n",
      "Iteration 2780, Train Loss: 0.03685859590768814, Test Loss: 0.09736219048500061\n",
      "Iteration 2790, Train Loss: 0.037154752761125565, Test Loss: 0.08308941125869751\n",
      "Iteration 2800, Train Loss: 0.03840593993663788, Test Loss: 0.07826395332813263\n",
      "Iteration 2810, Train Loss: 0.03933016583323479, Test Loss: 0.07727518677711487\n",
      "Iteration 2820, Train Loss: 0.04396698996424675, Test Loss: 0.07901442795991898\n",
      "Iteration 2830, Train Loss: 0.054931048303842545, Test Loss: 0.09981472790241241\n",
      "Iteration 2840, Train Loss: 0.06265290081501007, Test Loss: 0.07190972566604614\n",
      "Iteration 2850, Train Loss: 0.07108982652425766, Test Loss: 0.06266042590141296\n",
      "Iteration 2860, Train Loss: 0.07587825506925583, Test Loss: 0.05500892549753189\n",
      "Iteration 2870, Train Loss: 0.06923631578683853, Test Loss: 0.04446278512477875\n",
      "Iteration 2880, Train Loss: 0.06237896531820297, Test Loss: 0.04408164322376251\n",
      "Iteration 2890, Train Loss: 0.05544300004839897, Test Loss: 0.05406702682375908\n",
      "Iteration 2900, Train Loss: 0.04408874362707138, Test Loss: 0.0602373369038105\n",
      "Iteration 2910, Train Loss: 0.047168515622615814, Test Loss: 0.05393528565764427\n",
      "Iteration 2920, Train Loss: 0.05235230550169945, Test Loss: 0.04949095845222473\n",
      "Iteration 2930, Train Loss: 0.06301511079072952, Test Loss: 0.050856221467256546\n",
      "Iteration 2940, Train Loss: 0.06481028348207474, Test Loss: 0.09260811656713486\n",
      "Iteration 2950, Train Loss: 0.0735890120267868, Test Loss: 0.09880591183900833\n",
      "Iteration 2960, Train Loss: 0.06754916906356812, Test Loss: 0.07297773659229279\n",
      "Iteration 2970, Train Loss: 0.0616135410964489, Test Loss: 0.07519366592168808\n",
      "Iteration 2980, Train Loss: 0.060202423483133316, Test Loss: 0.0762510895729065\n",
      "Iteration 2990, Train Loss: 0.05764664337038994, Test Loss: 0.08310876786708832\n",
      "Iteration 3000, Train Loss: 0.050274334847927094, Test Loss: 0.08478214591741562\n",
      "Iteration 3010, Train Loss: 0.04294965788722038, Test Loss: 0.09032095223665237\n",
      "Iteration 3020, Train Loss: 0.04037269577383995, Test Loss: 0.09391956031322479\n",
      "Iteration 3030, Train Loss: 0.03923003003001213, Test Loss: 0.09510187059640884\n",
      "Iteration 3040, Train Loss: 0.03962564095854759, Test Loss: 0.08557799458503723\n",
      "Iteration 3050, Train Loss: 0.03649817779660225, Test Loss: 0.07935085147619247\n",
      "Iteration 3060, Train Loss: 0.039024706929922104, Test Loss: 0.07799932360649109\n",
      "Iteration 3070, Train Loss: 0.03959380462765694, Test Loss: 0.07997264713048935\n",
      "Iteration 3080, Train Loss: 0.05165382847189903, Test Loss: 0.0870928019285202\n",
      "Iteration 3090, Train Loss: 0.05986731871962547, Test Loss: 0.08686532825231552\n",
      "Iteration 3100, Train Loss: 0.071499764919281, Test Loss: 0.0660386011004448\n",
      "Iteration 3110, Train Loss: 0.07752545177936554, Test Loss: 0.05618808791041374\n",
      "Iteration 3120, Train Loss: 0.07643133401870728, Test Loss: 0.05195783078670502\n",
      "Iteration 3130, Train Loss: 0.06802701205015182, Test Loss: 0.04415544494986534\n",
      "Iteration 3140, Train Loss: 0.06497834622859955, Test Loss: 0.04600376635789871\n",
      "Iteration 3150, Train Loss: 0.05095893517136574, Test Loss: 0.06479857116937637\n",
      "Iteration 3160, Train Loss: 0.048584286123514175, Test Loss: 0.06240595877170563\n",
      "Iteration 3170, Train Loss: 0.05501481518149376, Test Loss: 0.05060349032282829\n",
      "Iteration 3180, Train Loss: 0.06461930274963379, Test Loss: 0.051755815744400024\n",
      "Iteration 3190, Train Loss: 0.0696532353758812, Test Loss: 0.09217914938926697\n",
      "Iteration 3200, Train Loss: 0.07954297959804535, Test Loss: 0.09930114448070526\n",
      "Iteration 3210, Train Loss: 0.08122842013835907, Test Loss: 0.08470064401626587\n",
      "Iteration 3220, Train Loss: 0.07968086004257202, Test Loss: 0.07222101837396622\n",
      "Iteration 3230, Train Loss: 0.07718238979578018, Test Loss: 0.07307054102420807\n",
      "Iteration 3240, Train Loss: 0.07225345820188522, Test Loss: 0.07390191406011581\n",
      "Iteration 3250, Train Loss: 0.06767239421606064, Test Loss: 0.07929331064224243\n",
      "Iteration 3260, Train Loss: 0.05792561173439026, Test Loss: 0.08136365562677383\n",
      "Iteration 3270, Train Loss: 0.054665546864271164, Test Loss: 0.08672523498535156\n",
      "Iteration 3280, Train Loss: 0.05030818283557892, Test Loss: 0.08821699768304825\n",
      "Iteration 3290, Train Loss: 0.05460968613624573, Test Loss: 0.08310243487358093\n",
      "Iteration 3300, Train Loss: 0.051471151411533356, Test Loss: 0.07184000313282013\n",
      "Iteration 3310, Train Loss: 0.0558687224984169, Test Loss: 0.07299740612506866\n",
      "Iteration 3320, Train Loss: 0.051755379885435104, Test Loss: 0.07596471160650253\n",
      "Iteration 3330, Train Loss: 0.06572052836418152, Test Loss: 0.07905104011297226\n",
      "Iteration 3340, Train Loss: 0.07599855214357376, Test Loss: 0.1043504923582077\n",
      "Iteration 3350, Train Loss: 0.09457417577505112, Test Loss: 0.11303652822971344\n",
      "Iteration 3360, Train Loss: 0.10757450014352798, Test Loss: 0.08343757688999176\n",
      "Iteration 3370, Train Loss: 0.11359018087387085, Test Loss: 0.0870368555188179\n",
      "Iteration 3380, Train Loss: 0.10808900743722916, Test Loss: 0.06678294390439987\n",
      "Iteration 3390, Train Loss: 0.10087470710277557, Test Loss: 0.06175709143280983\n",
      "Iteration 3400, Train Loss: 0.08706555515527725, Test Loss: 0.07474193722009659\n",
      "Iteration 3410, Train Loss: 0.07436297088861465, Test Loss: 0.10941065102815628\n",
      "Iteration 3420, Train Loss: 0.0715038999915123, Test Loss: 0.10406563431024551\n",
      "Iteration 3430, Train Loss: 0.07333751022815704, Test Loss: 0.10233820229768753\n",
      "Iteration 3440, Train Loss: 0.08213850110769272, Test Loss: 0.1102038025856018\n",
      "Iteration 3450, Train Loss: 0.08723309636116028, Test Loss: 0.16112948954105377\n",
      "Iteration 3460, Train Loss: 0.09129707515239716, Test Loss: 0.14629346132278442\n",
      "Iteration 3470, Train Loss: 0.08911418169736862, Test Loss: 0.14882925152778625\n",
      "Iteration 3480, Train Loss: 0.08426084369421005, Test Loss: 0.10989232361316681\n",
      "Iteration 3490, Train Loss: 0.07233235239982605, Test Loss: 0.13902956247329712\n",
      "Iteration 3500, Train Loss: 0.06622391194105148, Test Loss: 0.16122326254844666\n",
      "Iteration 3510, Train Loss: 0.053174614906311035, Test Loss: 0.18528959155082703\n",
      "Iteration 3520, Train Loss: 0.04590628668665886, Test Loss: 0.21449635922908783\n",
      "Iteration 3530, Train Loss: 0.045564550906419754, Test Loss: 0.2045927494764328\n",
      "Iteration 3540, Train Loss: 0.04766956716775894, Test Loss: 0.16278602182865143\n",
      "Iteration 3550, Train Loss: 0.04481174424290657, Test Loss: 0.1652347445487976\n",
      "Iteration 3560, Train Loss: 0.046887658536434174, Test Loss: 0.1711309403181076\n",
      "Iteration 3570, Train Loss: 0.045565709471702576, Test Loss: 0.17680330574512482\n",
      "Iteration 3580, Train Loss: 0.05348486453294754, Test Loss: 0.25420546531677246\n",
      "Iteration 3590, Train Loss: 0.06509087234735489, Test Loss: 0.23526357114315033\n",
      "Iteration 3600, Train Loss: 0.07813393324613571, Test Loss: 0.22693805396556854\n",
      "Iteration 3610, Train Loss: 0.08845274895429611, Test Loss: 0.21327762305736542\n",
      "Iteration 3620, Train Loss: 0.09705457091331482, Test Loss: 0.18976165354251862\n",
      "Iteration 3630, Train Loss: 0.09141895174980164, Test Loss: 0.1277744621038437\n",
      "Iteration 3640, Train Loss: 0.08516567945480347, Test Loss: 0.13261841237545013\n",
      "Iteration 3650, Train Loss: 0.07377305626869202, Test Loss: 0.14049409329891205\n",
      "Iteration 3660, Train Loss: 0.06064464896917343, Test Loss: 0.0901813805103302\n",
      "Iteration 3670, Train Loss: 0.055063799023628235, Test Loss: 0.05519070103764534\n",
      "Iteration 3680, Train Loss: 0.05924229323863983, Test Loss: 0.07563448697328568\n",
      "Iteration 3690, Train Loss: 0.06754989922046661, Test Loss: 0.20980863273143768\n",
      "Iteration 3700, Train Loss: 0.06937109678983688, Test Loss: 0.1784781664609909\n",
      "Iteration 3710, Train Loss: 0.07569817453622818, Test Loss: 0.14948241412639618\n",
      "Iteration 3720, Train Loss: 0.0697002112865448, Test Loss: 0.12296530604362488\n",
      "Iteration 3730, Train Loss: 0.06818987429141998, Test Loss: 0.03746457397937775\n",
      "Iteration 3740, Train Loss: 0.06268301606178284, Test Loss: 0.038069989532232285\n",
      "Iteration 3750, Train Loss: 0.05887807533144951, Test Loss: 0.046057067811489105\n",
      "Iteration 3760, Train Loss: 0.05086369439959526, Test Loss: 0.05255558341741562\n",
      "Iteration 3770, Train Loss: 0.04545651748776436, Test Loss: 0.061337754130363464\n",
      "Iteration 3780, Train Loss: 0.04399779811501503, Test Loss: 0.0632910504937172\n",
      "Iteration 3790, Train Loss: 0.045320797711610794, Test Loss: 0.07525397092103958\n",
      "Iteration 3800, Train Loss: 0.04572051391005516, Test Loss: 0.06708338111639023\n",
      "Iteration 3810, Train Loss: 0.045381952077150345, Test Loss: 0.07724689692258835\n",
      "Iteration 3820, Train Loss: 0.0471835620701313, Test Loss: 0.07617505639791489\n",
      "Iteration 3830, Train Loss: 0.052761200815439224, Test Loss: 0.08257120102643967\n",
      "Iteration 3840, Train Loss: 0.07207474857568741, Test Loss: 0.14102709293365479\n",
      "Iteration 3850, Train Loss: 0.0799170657992363, Test Loss: 0.10583574324846268\n",
      "Iteration 3860, Train Loss: 0.09527131915092468, Test Loss: 0.09100338071584702\n",
      "Iteration 3870, Train Loss: 0.10170754790306091, Test Loss: 0.08486156165599823\n",
      "Iteration 3880, Train Loss: 0.1002504751086235, Test Loss: 0.04185009375214577\n",
      "Iteration 3890, Train Loss: 0.09220779687166214, Test Loss: 0.03978276997804642\n",
      "Iteration 3900, Train Loss: 0.08638119697570801, Test Loss: 0.045166511088609695\n",
      "Iteration 3910, Train Loss: 0.07084669917821884, Test Loss: 0.05167499929666519\n",
      "Iteration 3920, Train Loss: 0.07294721901416779, Test Loss: 0.03947792947292328\n",
      "Iteration 3930, Train Loss: 0.07519600540399551, Test Loss: 0.038236744701862335\n",
      "Iteration 3940, Train Loss: 0.0864647775888443, Test Loss: 0.08162534981966019\n",
      "Iteration 3950, Train Loss: 0.0899350643157959, Test Loss: 0.11175879836082458\n",
      "Iteration 3960, Train Loss: 0.09411449730396271, Test Loss: 0.10248909890651703\n",
      "Iteration 3970, Train Loss: 0.09169846773147583, Test Loss: 0.09294254332780838\n",
      "Iteration 3980, Train Loss: 0.0874798521399498, Test Loss: 0.06756652146577835\n",
      "Iteration 3990, Train Loss: 0.08337265998125076, Test Loss: 0.043914083391427994\n",
      "Iteration 4000, Train Loss: 0.07901779562234879, Test Loss: 0.04339626803994179\n",
      "Iteration 4010, Train Loss: 0.07357890903949738, Test Loss: 0.04271484911441803\n",
      "Iteration 4020, Train Loss: 0.06284588575363159, Test Loss: 0.0425434485077858\n",
      "Iteration 4030, Train Loss: 0.06035759299993515, Test Loss: 0.04218205809593201\n",
      "Iteration 4040, Train Loss: 0.05874820053577423, Test Loss: 0.04259457439184189\n",
      "Iteration 4050, Train Loss: 0.059989720582962036, Test Loss: 0.042375124990940094\n",
      "Iteration 4060, Train Loss: 0.05974993109703064, Test Loss: 0.04159872233867645\n",
      "Iteration 4070, Train Loss: 0.0712926834821701, Test Loss: 0.041358646005392075\n",
      "Iteration 4080, Train Loss: 0.07598885148763657, Test Loss: 0.041397999972105026\n",
      "Iteration 4090, Train Loss: 0.09740781038999557, Test Loss: 0.05214843153953552\n",
      "Iteration 4100, Train Loss: 0.11056681722402573, Test Loss: 0.06911134719848633\n",
      "Iteration 4110, Train Loss: 0.12820391356945038, Test Loss: 0.06504789739847183\n",
      "Iteration 4120, Train Loss: 0.13350267708301544, Test Loss: 0.060804080218076706\n",
      "Iteration 4130, Train Loss: 0.1288812905550003, Test Loss: 0.049219418317079544\n",
      "Iteration 4140, Train Loss: 0.11795775592327118, Test Loss: 0.045661311596632004\n",
      "Iteration 4150, Train Loss: 0.11356445401906967, Test Loss: 0.04896336421370506\n",
      "Iteration 4160, Train Loss: 0.09911905974149704, Test Loss: 0.05301659554243088\n",
      "Iteration 4170, Train Loss: 0.09790266305208206, Test Loss: 0.049215398728847504\n",
      "Iteration 4180, Train Loss: 0.10826427489519119, Test Loss: 0.04158337041735649\n",
      "Iteration 4190, Train Loss: 0.11553293466567993, Test Loss: 0.06489792466163635\n",
      "Iteration 4200, Train Loss: 0.12045333534479141, Test Loss: 0.09454652667045593\n",
      "Iteration 4210, Train Loss: 0.12471754103899002, Test Loss: 0.09588820487260818\n",
      "Iteration 4220, Train Loss: 0.12069001793861389, Test Loss: 0.07817152887582779\n",
      "Iteration 4230, Train Loss: 0.11011221259832382, Test Loss: 0.06539057195186615\n",
      "Iteration 4240, Train Loss: 0.10616851598024368, Test Loss: 0.06632921099662781\n",
      "Iteration 4250, Train Loss: 0.103022500872612, Test Loss: 0.07820319384336472\n",
      "Iteration 4260, Train Loss: 0.1008160188794136, Test Loss: 0.09045177698135376\n",
      "Iteration 4270, Train Loss: 0.0958249643445015, Test Loss: 0.09982068091630936\n",
      "Iteration 4280, Train Loss: 0.0948878601193428, Test Loss: 0.11619427055120468\n",
      "Iteration 4290, Train Loss: 0.0922185629606247, Test Loss: 0.09843386709690094\n",
      "Iteration 4300, Train Loss: 0.09725680947303772, Test Loss: 0.10329794138669968\n",
      "Iteration 4310, Train Loss: 0.09655001014471054, Test Loss: 0.1083621233701706\n",
      "Iteration 4320, Train Loss: 0.09336208552122116, Test Loss: 0.1199897974729538\n",
      "Iteration 4330, Train Loss: 0.08449827879667282, Test Loss: 0.14174041152000427\n",
      "Iteration 4340, Train Loss: 0.08744879812002182, Test Loss: 0.14608460664749146\n",
      "Iteration 4350, Train Loss: 0.0892140194773674, Test Loss: 0.13292068243026733\n",
      "Iteration 4360, Train Loss: 0.10012080520391464, Test Loss: 0.12281884253025055\n",
      "Iteration 4370, Train Loss: 0.11066889762878418, Test Loss: 0.1009531244635582\n",
      "Iteration 4380, Train Loss: 0.11273238062858582, Test Loss: 0.09067923575639725\n",
      "Iteration 4390, Train Loss: 0.10128269344568253, Test Loss: 0.08143829554319382\n",
      "Iteration 4400, Train Loss: 0.09164723753929138, Test Loss: 0.09640555083751678\n",
      "Iteration 4410, Train Loss: 0.07649439573287964, Test Loss: 0.09843866527080536\n",
      "Iteration 4420, Train Loss: 0.0653967633843422, Test Loss: 0.1133098378777504\n",
      "Iteration 4430, Train Loss: 0.06423443555831909, Test Loss: 0.14016640186309814\n",
      "Iteration 4440, Train Loss: 0.06770141422748566, Test Loss: 0.16632215678691864\n",
      "Iteration 4450, Train Loss: 0.07492589950561523, Test Loss: 0.13594400882720947\n",
      "Iteration 4460, Train Loss: 0.07803037762641907, Test Loss: 0.13615964353084564\n",
      "Iteration 4470, Train Loss: 0.08204349875450134, Test Loss: 0.12655076384544373\n",
      "Iteration 4480, Train Loss: 0.0764666199684143, Test Loss: 0.09166890382766724\n",
      "Iteration 4490, Train Loss: 0.07243220508098602, Test Loss: 0.08578603714704514\n",
      "Iteration 4500, Train Loss: 0.06550773978233337, Test Loss: 0.09881025552749634\n",
      "Iteration 4510, Train Loss: 0.05878305435180664, Test Loss: 0.12051776796579361\n",
      "Iteration 4520, Train Loss: 0.04916128143668175, Test Loss: 0.1297660768032074\n",
      "Iteration 4530, Train Loss: 0.0423932708799839, Test Loss: 0.15151849389076233\n",
      "Iteration 4540, Train Loss: 0.04137369245290756, Test Loss: 0.15346792340278625\n",
      "Iteration 4550, Train Loss: 0.04299022629857063, Test Loss: 0.1412496715784073\n",
      "Iteration 4560, Train Loss: 0.04336161911487579, Test Loss: 0.13735298812389374\n",
      "Iteration 4570, Train Loss: 0.042826853692531586, Test Loss: 0.1407642811536789\n",
      "Iteration 4580, Train Loss: 0.043337877839803696, Test Loss: 0.14199769496917725\n",
      "Iteration 4590, Train Loss: 0.051441118121147156, Test Loss: 0.14378955960273743\n",
      "Iteration 4600, Train Loss: 0.06435196846723557, Test Loss: 0.13771900534629822\n",
      "Iteration 4610, Train Loss: 0.07806167006492615, Test Loss: 0.10836203396320343\n",
      "Iteration 4620, Train Loss: 0.0923425555229187, Test Loss: 0.0883355364203453\n",
      "Iteration 4630, Train Loss: 0.09620796889066696, Test Loss: 0.076777882874012\n",
      "Iteration 4640, Train Loss: 0.09181690961122513, Test Loss: 0.06787531822919846\n",
      "Iteration 4650, Train Loss: 0.0840315893292427, Test Loss: 0.07954242080450058\n",
      "Iteration 4660, Train Loss: 0.07188016176223755, Test Loss: 0.08381077647209167\n",
      "Iteration 4670, Train Loss: 0.05847056210041046, Test Loss: 0.09280166029930115\n",
      "Iteration 4680, Train Loss: 0.05627146735787392, Test Loss: 0.08743470907211304\n",
      "Iteration 4690, Train Loss: 0.05771878361701965, Test Loss: 0.08824702352285385\n",
      "Iteration 4700, Train Loss: 0.06420855224132538, Test Loss: 0.11700694262981415\n",
      "Iteration 4710, Train Loss: 0.06614559888839722, Test Loss: 0.12344681471586227\n",
      "Iteration 4720, Train Loss: 0.069911889731884, Test Loss: 0.11332479119300842\n",
      "Iteration 4730, Train Loss: 0.06405805796384811, Test Loss: 0.08799498528242111\n",
      "Iteration 4740, Train Loss: 0.056460920721292496, Test Loss: 0.06118648499250412\n",
      "Iteration 4750, Train Loss: 0.05059020221233368, Test Loss: 0.05553636699914932\n",
      "Iteration 4760, Train Loss: 0.0476159043610096, Test Loss: 0.05275280773639679\n",
      "Iteration 4770, Train Loss: 0.03860555589199066, Test Loss: 0.051578398793935776\n",
      "Iteration 4780, Train Loss: 0.03148071840405464, Test Loss: 0.052705615758895874\n",
      "Iteration 4790, Train Loss: 0.029927005991339684, Test Loss: 0.058294877409935\n",
      "Iteration 4800, Train Loss: 0.030847342684864998, Test Loss: 0.058904752135276794\n",
      "Iteration 4810, Train Loss: 0.03013523295521736, Test Loss: 0.05767042189836502\n",
      "Iteration 4820, Train Loss: 0.03202222287654877, Test Loss: 0.05955566093325615\n",
      "Iteration 4830, Train Loss: 0.03558511659502983, Test Loss: 0.061821866780519485\n",
      "Iteration 4840, Train Loss: 0.04025929793715477, Test Loss: 0.06687059253454208\n",
      "Iteration 4850, Train Loss: 0.046473320573568344, Test Loss: 0.08195123821496964\n",
      "Iteration 4860, Train Loss: 0.05047326534986496, Test Loss: 0.0677729994058609\n",
      "Iteration 4870, Train Loss: 0.05027065798640251, Test Loss: 0.052730560302734375\n",
      "Iteration 4880, Train Loss: 0.04862803593277931, Test Loss: 0.043505486100912094\n",
      "Iteration 4890, Train Loss: 0.044447723776102066, Test Loss: 0.0389094240963459\n",
      "Iteration 4900, Train Loss: 0.039975687861442566, Test Loss: 0.039175957441329956\n",
      "Iteration 4910, Train Loss: 0.03827551752328873, Test Loss: 0.04278586059808731\n",
      "Iteration 4920, Train Loss: 0.031283020973205566, Test Loss: 0.050277676433324814\n",
      "Iteration 4930, Train Loss: 0.034936994314193726, Test Loss: 0.037644825875759125\n",
      "Iteration 4940, Train Loss: 0.04441523924469948, Test Loss: 0.03797193989157677\n",
      "Iteration 4950, Train Loss: 0.0569155178964138, Test Loss: 0.03818348050117493\n",
      "Iteration 4960, Train Loss: 0.059328459203243256, Test Loss: 0.1117817685008049\n",
      "Iteration 4970, Train Loss: 0.06704670190811157, Test Loss: 0.12955836951732635\n",
      "Iteration 4980, Train Loss: 0.06472218781709671, Test Loss: 0.12849174439907074\n",
      "Iteration 4990, Train Loss: 0.060887109488248825, Test Loss: 0.12788096070289612\n",
      "Iteration 5000, Train Loss: 0.05492803454399109, Test Loss: 0.03742794692516327\n",
      "Iteration 5010, Train Loss: 0.056530848145484924, Test Loss: 0.03929070383310318\n",
      "Iteration 5020, Train Loss: 0.056334253400564194, Test Loss: 0.040531471371650696\n",
      "Iteration 5030, Train Loss: 0.05279455706477165, Test Loss: 0.041104067116975784\n",
      "Iteration 5040, Train Loss: 0.054077744483947754, Test Loss: 0.04133491963148117\n",
      "Iteration 5050, Train Loss: 0.05539337545633316, Test Loss: 0.04131067916750908\n",
      "Iteration 5060, Train Loss: 0.06066426262259483, Test Loss: 0.04088415578007698\n",
      "Iteration 5070, Train Loss: 0.06222417205572128, Test Loss: 0.040769707411527634\n",
      "Iteration 5080, Train Loss: 0.06393273174762726, Test Loss: 0.04054580256342888\n",
      "Iteration 5090, Train Loss: 0.0633268654346466, Test Loss: 0.04034154489636421\n",
      "Iteration 5100, Train Loss: 0.08097674697637558, Test Loss: 0.03920480236411095\n",
      "Iteration 5110, Train Loss: 0.08986464142799377, Test Loss: 0.21666719019412994\n",
      "Iteration 5120, Train Loss: 0.10427899658679962, Test Loss: 0.22725240886211395\n",
      "Iteration 5130, Train Loss: 0.11209574341773987, Test Loss: 0.22504711151123047\n",
      "Iteration 5140, Train Loss: 0.11245355755090714, Test Loss: 0.22073376178741455\n",
      "Iteration 5150, Train Loss: 0.10261141508817673, Test Loss: 0.2318883091211319\n",
      "Iteration 5160, Train Loss: 0.0947762131690979, Test Loss: 0.22570686042308807\n",
      "Iteration 5170, Train Loss: 0.07852240651845932, Test Loss: 0.22810833156108856\n",
      "Iteration 5180, Train Loss: 0.08115142583847046, Test Loss: 0.07162550836801529\n",
      "Iteration 5190, Train Loss: 0.07865267992019653, Test Loss: 0.1384020745754242\n",
      "Iteration 5200, Train Loss: 0.08600367605686188, Test Loss: 0.10091456770896912\n",
      "Iteration 5210, Train Loss: 0.09096302092075348, Test Loss: 0.09632593393325806\n",
      "Iteration 5220, Train Loss: 0.09854090958833694, Test Loss: 0.09815698862075806\n",
      "Iteration 5230, Train Loss: 0.09611639380455017, Test Loss: 0.0787263885140419\n",
      "Iteration 5240, Train Loss: 0.09229350835084915, Test Loss: 0.06461333483457565\n",
      "Iteration 5250, Train Loss: 0.08729778975248337, Test Loss: 0.07737993448972702\n",
      "Iteration 5260, Train Loss: 0.08481421321630478, Test Loss: 0.08342422544956207\n",
      "Iteration 5270, Train Loss: 0.07837998121976852, Test Loss: 0.11435433477163315\n",
      "Iteration 5280, Train Loss: 0.07001560926437378, Test Loss: 0.1307043582201004\n",
      "Iteration 5290, Train Loss: 0.06678533554077148, Test Loss: 0.13161197304725647\n",
      "Iteration 5300, Train Loss: 0.06750813126564026, Test Loss: 0.11971404403448105\n",
      "Iteration 5310, Train Loss: 0.0689912959933281, Test Loss: 0.12853212654590607\n",
      "Iteration 5320, Train Loss: 0.06936757266521454, Test Loss: 0.127776637673378\n",
      "Iteration 5330, Train Loss: 0.07301043719053268, Test Loss: 0.11942537128925323\n",
      "Iteration 5340, Train Loss: 0.07635685801506042, Test Loss: 0.12638185918331146\n",
      "Iteration 5350, Train Loss: 0.09219261258840561, Test Loss: 0.10443540662527084\n",
      "Iteration 5360, Train Loss: 0.10072210431098938, Test Loss: 0.07186427712440491\n",
      "Iteration 5370, Train Loss: 0.11279252171516418, Test Loss: 0.05618778243660927\n",
      "Iteration 5380, Train Loss: 0.11482822149991989, Test Loss: 0.04749263450503349\n",
      "Iteration 5390, Train Loss: 0.10966426879167557, Test Loss: 0.053601838648319244\n",
      "Iteration 5400, Train Loss: 0.10050849616527557, Test Loss: 0.05642801150679588\n",
      "Iteration 5410, Train Loss: 0.09338905662298203, Test Loss: 0.05766608193516731\n",
      "Iteration 5420, Train Loss: 0.07951287180185318, Test Loss: 0.05821802467107773\n",
      "Iteration 5430, Train Loss: 0.08717230707406998, Test Loss: 0.07463106513023376\n",
      "Iteration 5440, Train Loss: 0.09436126798391342, Test Loss: 0.105190709233284\n",
      "Iteration 5450, Train Loss: 0.10457444936037064, Test Loss: 0.08887002617120743\n",
      "Iteration 5460, Train Loss: 0.10689973086118698, Test Loss: 0.0795123279094696\n",
      "Iteration 5470, Train Loss: 0.11411384493112564, Test Loss: 0.08743715286254883\n",
      "Iteration 5480, Train Loss: 0.10725590586662292, Test Loss: 0.07383541762828827\n",
      "Iteration 5490, Train Loss: 0.11268629133701324, Test Loss: 0.05744708701968193\n",
      "Iteration 5500, Train Loss: 0.10273239761590958, Test Loss: 0.054962944239377975\n",
      "Iteration 5510, Train Loss: 0.10096310824155807, Test Loss: 0.06391086429357529\n",
      "Iteration 5520, Train Loss: 0.09593507647514343, Test Loss: 0.0706462487578392\n",
      "Iteration 5530, Train Loss: 0.08996107429265976, Test Loss: 0.07838186621665955\n",
      "Iteration 5540, Train Loss: 0.08561334013938904, Test Loss: 0.12814992666244507\n",
      "Iteration 5550, Train Loss: 0.08472150564193726, Test Loss: 0.1261356621980667\n",
      "Iteration 5560, Train Loss: 0.08577657490968704, Test Loss: 0.10386648029088974\n",
      "Iteration 5570, Train Loss: 0.08588055521249771, Test Loss: 0.10987895727157593\n",
      "Iteration 5580, Train Loss: 0.08675757050514221, Test Loss: 0.119206503033638\n",
      "Iteration 5590, Train Loss: 0.07969484478235245, Test Loss: 0.11981171369552612\n",
      "Iteration 5600, Train Loss: 0.08976104110479355, Test Loss: 0.12989908456802368\n",
      "Iteration 5610, Train Loss: 0.09801769256591797, Test Loss: 0.11776262521743774\n",
      "Iteration 5620, Train Loss: 0.1099160686135292, Test Loss: 0.08873149007558823\n",
      "Iteration 5630, Train Loss: 0.11948850005865097, Test Loss: 0.06821872293949127\n",
      "Iteration 5640, Train Loss: 0.1182728111743927, Test Loss: 0.06432260572910309\n",
      "Iteration 5650, Train Loss: 0.10790779441595078, Test Loss: 0.062169093638658524\n",
      "Iteration 5660, Train Loss: 0.09718768298625946, Test Loss: 0.06464695930480957\n",
      "Iteration 5670, Train Loss: 0.08053673803806305, Test Loss: 0.06521590054035187\n",
      "Iteration 5680, Train Loss: 0.07672149688005447, Test Loss: 0.06696867942810059\n",
      "Iteration 5690, Train Loss: 0.07691839337348938, Test Loss: 0.07172149419784546\n",
      "Iteration 5700, Train Loss: 0.0834786668419838, Test Loss: 0.07476240396499634\n",
      "Iteration 5710, Train Loss: 0.08803348243236542, Test Loss: 0.07405487447977066\n",
      "Iteration 5720, Train Loss: 0.09844925254583359, Test Loss: 0.12176772952079773\n",
      "Iteration 5730, Train Loss: 0.0928376317024231, Test Loss: 0.12765391170978546\n",
      "Iteration 5740, Train Loss: 0.08979827910661697, Test Loss: 0.1122230812907219\n",
      "Iteration 5750, Train Loss: 0.08290048688650131, Test Loss: 0.08214651048183441\n",
      "Iteration 5760, Train Loss: 0.07876814156770706, Test Loss: 0.08276799321174622\n",
      "Iteration 5770, Train Loss: 0.07107675820589066, Test Loss: 0.08783576637506485\n",
      "Iteration 5780, Train Loss: 0.06113642826676369, Test Loss: 0.09024999290704727\n",
      "Iteration 5790, Train Loss: 0.0620601624250412, Test Loss: 0.11288094520568848\n",
      "Iteration 5800, Train Loss: 0.05861561745405197, Test Loss: 0.1504073590040207\n",
      "Iteration 5810, Train Loss: 0.06044011190533638, Test Loss: 0.14219148457050323\n",
      "Iteration 5820, Train Loss: 0.058693625032901764, Test Loss: 0.13040538132190704\n",
      "Iteration 5830, Train Loss: 0.05939904600381851, Test Loss: 0.13170529901981354\n",
      "Iteration 5840, Train Loss: 0.054673932492733, Test Loss: 0.12928791344165802\n",
      "Iteration 5850, Train Loss: 0.07058592885732651, Test Loss: 0.13456577062606812\n",
      "Iteration 5860, Train Loss: 0.07871970534324646, Test Loss: 0.14231069386005402\n",
      "Iteration 5870, Train Loss: 0.09928152710199356, Test Loss: 0.1250002235174179\n",
      "Iteration 5880, Train Loss: 0.10859943181276321, Test Loss: 0.09311062097549438\n",
      "Iteration 5890, Train Loss: 0.11320365965366364, Test Loss: 0.06874057650566101\n",
      "Iteration 5900, Train Loss: 0.09819265455007553, Test Loss: 0.05745058134198189\n",
      "Iteration 5910, Train Loss: 0.09238944947719574, Test Loss: 0.05745347961783409\n",
      "Iteration 5920, Train Loss: 0.07572522759437561, Test Loss: 0.06009251996874809\n",
      "Iteration 5930, Train Loss: 0.0658285841345787, Test Loss: 0.06163369119167328\n",
      "Iteration 5940, Train Loss: 0.06595389544963837, Test Loss: 0.06494522839784622\n",
      "Iteration 5950, Train Loss: 0.07003608345985413, Test Loss: 0.06949068605899811\n",
      "Iteration 5960, Train Loss: 0.0759228840470314, Test Loss: 0.07068446278572083\n",
      "Iteration 5970, Train Loss: 0.09200995415449142, Test Loss: 0.06955866515636444\n",
      "Iteration 5980, Train Loss: 0.08227289468050003, Test Loss: 0.10768479108810425\n",
      "Iteration 5990, Train Loss: 0.08154034614562988, Test Loss: 0.11844511330127716\n",
      "Iteration 6000, Train Loss: 0.07548552751541138, Test Loss: 0.10309620201587677\n",
      "Iteration 6010, Train Loss: 0.06624163687229156, Test Loss: 0.09499090164899826\n",
      "Iteration 6020, Train Loss: 0.05859386920928955, Test Loss: 0.09915866702795029\n",
      "Iteration 6030, Train Loss: 0.045701414346694946, Test Loss: 0.10718891024589539\n",
      "Iteration 6040, Train Loss: 0.04187177121639252, Test Loss: 0.1197066530585289\n",
      "Iteration 6050, Train Loss: 0.035155147314071655, Test Loss: 0.14087216556072235\n",
      "Iteration 6060, Train Loss: 0.037586648017168045, Test Loss: 0.14256049692630768\n",
      "Iteration 6070, Train Loss: 0.03347976133227348, Test Loss: 0.12984149158000946\n",
      "Iteration 6080, Train Loss: 0.05870791897177696, Test Loss: 0.11801943182945251\n",
      "Iteration 6090, Train Loss: 0.07102999836206436, Test Loss: 0.1010286808013916\n",
      "Iteration 6100, Train Loss: 0.09147778153419495, Test Loss: 0.08585557341575623\n",
      "Iteration 6110, Train Loss: 0.10228150337934494, Test Loss: 0.06274691224098206\n",
      "Iteration 6120, Train Loss: 0.1126594990491867, Test Loss: 0.04097536578774452\n",
      "Iteration 6130, Train Loss: 0.10889837145805359, Test Loss: 0.0377078503370285\n",
      "Iteration 6140, Train Loss: 0.1483069807291031, Test Loss: 0.03753623366355896\n",
      "Iteration 6150, Train Loss: 0.11651839315891266, Test Loss: 0.03883755952119827\n",
      "Iteration 6160, Train Loss: 0.1273864209651947, Test Loss: 0.03827887400984764\n",
      "Iteration 6170, Train Loss: 0.1193087100982666, Test Loss: 0.037448540329933167\n",
      "Iteration 6180, Train Loss: 0.12059711664915085, Test Loss: 0.03742675483226776\n",
      "Iteration 6190, Train Loss: 0.13120310008525848, Test Loss: 0.03746383264660835\n",
      "Iteration 6200, Train Loss: 0.14152483642101288, Test Loss: 0.03933857008814812\n",
      "Iteration 6210, Train Loss: 0.14539048075675964, Test Loss: 0.04108213260769844\n",
      "Iteration 6220, Train Loss: 0.14838391542434692, Test Loss: 0.04113926738500595\n",
      "Iteration 6230, Train Loss: 0.14035312831401825, Test Loss: 0.05053862929344177\n",
      "Iteration 6240, Train Loss: 0.12665726244449615, Test Loss: 0.04974033683538437\n",
      "Iteration 6250, Train Loss: 0.1188388541340828, Test Loss: 0.040934715420007706\n",
      "Iteration 6260, Train Loss: 0.11817992478609085, Test Loss: 0.03813307732343674\n",
      "Iteration 6270, Train Loss: 0.12543632090091705, Test Loss: 0.03844626620411873\n",
      "Iteration 6280, Train Loss: 0.12194620072841644, Test Loss: 0.039864491671323776\n",
      "Iteration 6290, Train Loss: 0.12604592740535736, Test Loss: 0.04191863536834717\n",
      "Iteration 6300, Train Loss: 0.10902836173772812, Test Loss: 0.05772150680422783\n",
      "Iteration 6310, Train Loss: 0.10746658593416214, Test Loss: 0.06383001059293747\n",
      "Iteration 6320, Train Loss: 0.10217662900686264, Test Loss: 0.05668388307094574\n",
      "Iteration 6330, Train Loss: 0.09877601265907288, Test Loss: 0.05182477831840515\n",
      "Iteration 6340, Train Loss: 0.09045512229204178, Test Loss: 0.07246759533882141\n",
      "Iteration 6350, Train Loss: 0.08997420966625214, Test Loss: 0.05901752784848213\n",
      "Iteration 6360, Train Loss: 0.07723191380500793, Test Loss: 0.07920308411121368\n",
      "Iteration 6370, Train Loss: 0.07207317650318146, Test Loss: 0.09180256724357605\n",
      "Iteration 6380, Train Loss: 0.07807391881942749, Test Loss: 0.08861000835895538\n",
      "Iteration 6390, Train Loss: 0.08403651416301727, Test Loss: 0.07380445301532745\n",
      "Iteration 6400, Train Loss: 0.07774653285741806, Test Loss: 0.06071997806429863\n",
      "Iteration 6410, Train Loss: 0.07145204395055771, Test Loss: 0.05533824488520622\n",
      "Iteration 6420, Train Loss: 0.06183619424700737, Test Loss: 0.05581465736031532\n",
      "Iteration 6430, Train Loss: 0.04991583526134491, Test Loss: 0.05938387289643288\n",
      "Iteration 6440, Train Loss: 0.0556398443877697, Test Loss: 0.07188138365745544\n",
      "Iteration 6450, Train Loss: 0.05471683666110039, Test Loss: 0.10373319685459137\n",
      "Iteration 6460, Train Loss: 0.06166795268654823, Test Loss: 0.13224981725215912\n",
      "Iteration 6470, Train Loss: 0.06640447676181793, Test Loss: 0.13462276756763458\n",
      "Iteration 6480, Train Loss: 0.07286850363016129, Test Loss: 0.13219884037971497\n",
      "Iteration 6490, Train Loss: 0.06706342846155167, Test Loss: 0.12423498928546906\n",
      "Iteration 6500, Train Loss: 0.06432203203439713, Test Loss: 0.08144976943731308\n",
      "Iteration 6510, Train Loss: 0.05299617722630501, Test Loss: 0.09145546704530716\n",
      "Iteration 6520, Train Loss: 0.05113529786467552, Test Loss: 0.08614738285541534\n",
      "Iteration 6530, Train Loss: 0.04317127540707588, Test Loss: 0.09806051105260849\n",
      "Iteration 6540, Train Loss: 0.03708573803305626, Test Loss: 0.11502203345298767\n",
      "Iteration 6550, Train Loss: 0.03353216499090195, Test Loss: 0.1363627314567566\n",
      "Iteration 6560, Train Loss: 0.03455900028347969, Test Loss: 0.14467883110046387\n",
      "Iteration 6570, Train Loss: 0.03513585031032562, Test Loss: 0.13201598823070526\n",
      "Iteration 6580, Train Loss: 0.033778734505176544, Test Loss: 0.11689978837966919\n",
      "Iteration 6590, Train Loss: 0.03425585851073265, Test Loss: 0.1111772358417511\n",
      "Iteration 6600, Train Loss: 0.039703741669654846, Test Loss: 0.1183328852057457\n",
      "Iteration 6610, Train Loss: 0.05362803861498833, Test Loss: 0.1226799413561821\n",
      "Iteration 6620, Train Loss: 0.0634751245379448, Test Loss: 0.11101570725440979\n",
      "Iteration 6630, Train Loss: 0.07535716146230698, Test Loss: 0.08469080924987793\n",
      "Iteration 6640, Train Loss: 0.07771477103233337, Test Loss: 0.05291042476892471\n",
      "Iteration 6650, Train Loss: 0.0722334235906601, Test Loss: 0.06007569283246994\n",
      "Iteration 6660, Train Loss: 0.0654398649930954, Test Loss: 0.06287937611341476\n",
      "Iteration 6670, Train Loss: 0.056230802088975906, Test Loss: 0.06749670207500458\n",
      "Iteration 6680, Train Loss: 0.04432862251996994, Test Loss: 0.07158508896827698\n",
      "Iteration 6690, Train Loss: 0.047894157469272614, Test Loss: 0.08139675855636597\n",
      "Iteration 6700, Train Loss: 0.05468926578760147, Test Loss: 0.09207846969366074\n",
      "Iteration 6710, Train Loss: 0.06485292315483093, Test Loss: 0.09623020887374878\n",
      "Iteration 6720, Train Loss: 0.07096695154905319, Test Loss: 0.08971227705478668\n",
      "Iteration 6730, Train Loss: 0.07928547263145447, Test Loss: 0.07904759794473648\n",
      "Iteration 6740, Train Loss: 0.06656752526760101, Test Loss: 0.06668310612440109\n",
      "Iteration 6750, Train Loss: 0.060039617121219635, Test Loss: 0.056470561772584915\n",
      "Iteration 6760, Train Loss: 0.06038124859333038, Test Loss: 0.053166065365076065\n",
      "Iteration 6770, Train Loss: 0.06284021586179733, Test Loss: 0.06165910139679909\n",
      "Iteration 6780, Train Loss: 0.06767823547124863, Test Loss: 0.07271907478570938\n",
      "Iteration 6790, Train Loss: 0.07435917109251022, Test Loss: 0.08105304837226868\n",
      "Iteration 6800, Train Loss: 0.07402557134628296, Test Loss: 0.08919065445661545\n",
      "Iteration 6810, Train Loss: 0.08389067649841309, Test Loss: 0.09364977478981018\n",
      "Iteration 6820, Train Loss: 0.09000147879123688, Test Loss: 0.07999516278505325\n",
      "Iteration 6830, Train Loss: 0.09174484014511108, Test Loss: 0.06486333906650543\n",
      "Iteration 6840, Train Loss: 0.08780400454998016, Test Loss: 0.05638711526989937\n",
      "Iteration 6850, Train Loss: 0.08604199439287186, Test Loss: 0.05470207333564758\n",
      "Iteration 6860, Train Loss: 0.08235794305801392, Test Loss: 0.05442831665277481\n",
      "Iteration 6870, Train Loss: 0.06524692475795746, Test Loss: 0.04779168963432312\n",
      "Iteration 6880, Train Loss: 0.053498223423957825, Test Loss: 0.039285048842430115\n",
      "Iteration 6890, Train Loss: 0.04579043388366699, Test Loss: 0.03762994706630707\n",
      "Iteration 6900, Train Loss: 0.03691438212990761, Test Loss: 0.038189832121133804\n",
      "Iteration 6910, Train Loss: 0.033641938120126724, Test Loss: 0.03753753751516342\n",
      "Iteration 6920, Train Loss: 0.03311486542224884, Test Loss: 0.037472937256097794\n",
      "Iteration 6930, Train Loss: 0.03023078292608261, Test Loss: 0.03765072673559189\n",
      "Iteration 6940, Train Loss: 0.056102972477674484, Test Loss: 0.03747427463531494\n",
      "Iteration 6950, Train Loss: 0.07551439106464386, Test Loss: 0.039468180388212204\n",
      "Iteration 6960, Train Loss: 0.08239773660898209, Test Loss: 0.04134593904018402\n",
      "Iteration 6970, Train Loss: 0.07823118567466736, Test Loss: 0.03975168988108635\n",
      "Iteration 6980, Train Loss: 0.0880689024925232, Test Loss: 0.03887205198407173\n",
      "Iteration 6990, Train Loss: 0.055122073739767075, Test Loss: 0.0405607596039772\n",
      "Iteration 7000, Train Loss: 0.041907504200935364, Test Loss: 0.03777098283171654\n",
      "Iteration 7010, Train Loss: 0.047385238111019135, Test Loss: 0.03814559802412987\n",
      "Iteration 7020, Train Loss: 0.057018451392650604, Test Loss: 0.038017403334379196\n",
      "Iteration 7030, Train Loss: 0.055056896060705185, Test Loss: 0.04473908990621567\n",
      "Iteration 7040, Train Loss: 0.05223945155739784, Test Loss: 0.05309000611305237\n",
      "Iteration 7050, Train Loss: 0.04402197524905205, Test Loss: 0.062078289687633514\n",
      "Iteration 7060, Train Loss: 0.0399731770157814, Test Loss: 0.06883440166711807\n",
      "Iteration 7070, Train Loss: 0.04007066413760185, Test Loss: 0.07555831223726273\n",
      "Iteration 7080, Train Loss: 0.041008125990629196, Test Loss: 0.07884009182453156\n",
      "Iteration 7090, Train Loss: 0.04259726405143738, Test Loss: 0.07909571379423141\n",
      "Iteration 7100, Train Loss: 0.04352152720093727, Test Loss: 0.07620613276958466\n",
      "Iteration 7110, Train Loss: 0.05252576991915703, Test Loss: 0.0751822292804718\n",
      "Iteration 7120, Train Loss: 0.05472051724791527, Test Loss: 0.07298913598060608\n",
      "Iteration 7130, Train Loss: 0.060791078954935074, Test Loss: 0.06727922707796097\n",
      "Iteration 7140, Train Loss: 0.06015314161777496, Test Loss: 0.060379158705472946\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Adjustable Parameters\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train_scaled) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train_scaled.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "# LSTM Model Definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "model = LSTMModel(input_size, hidden_dim, n_layers, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training using Walk-Forward Validation\n",
    "for i in range(window_size, epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Extract training window with batch size\n",
    "    start_idx = i - window_size\n",
    "    end_idx = i\n",
    "    x_window = torch.tensor(x_train_scaled[start_idx:end_idx], dtype=torch.float32)\n",
    "    y_window = torch.tensor(y_train_scaled[start_idx:end_idx], dtype=torch.float32)\n",
    "\n",
    "    # Reshape x_window to match LSTM input shape [batch_size=1, sequence_length=window_size, input_size=input_size]\n",
    "    x_window = x_window.view(1, window_size, input_size)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(x_window)\n",
    "    loss = criterion(outputs, y_window)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    if i % batch_evaluation_frequency == 0:\n",
    "        with torch.no_grad():\n",
    "            x_test_window = torch.tensor(x_test_scaled[:test_window_size], dtype=torch.float32)\n",
    "            y_test_window = torch.tensor(y_test_scaled[:test_window_size], dtype=torch.float32)\n",
    "\n",
    "            # Reshape x_test_window to match LSTM input shape [batch_size=1, sequence_length=test_window_size, input_size=input_size]\n",
    "            x_test_window = x_test_window.view(1, test_window_size, input_size)\n",
    "\n",
    "            test_outputs = model(x_test_window)\n",
    "            test_loss = criterion(test_outputs, y_test_window)\n",
    "            print(f\"Iteration {i}, Train Loss: {loss.item()}, Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m num_forecast_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize the input sequence for forecasting\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m x_train_scaled[\u001b[38;5;241m-\u001b[39mwindow_size:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, window_size, \u001b[43minput_dim\u001b[49m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize the hidden state\u001b[39;00m\n\u001b[0;32m     17\u001b[0m hidden \u001b[38;5;241m=\u001b[39m init_hidden(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to initialize the hidden state (customize based on your model architecture)\n",
    "def init_hidden(batch_size):\n",
    "    return (torch.zeros(1, batch_size, 50), torch.zeros(1, batch_size, 50))  # Assuming 50 hidden units\n",
    "\n",
    "# Define the number of future steps you want to forecast\n",
    "num_forecast_steps = 1\n",
    "\n",
    "# Initialize the input sequence for forecasting\n",
    "input_sequence = x_train_scaled[-window_size:].view(1, window_size, input_dim)\n",
    "\n",
    "# Initialize the hidden state\n",
    "hidden = init_hidden(batch_size=1)\n",
    "\n",
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value and update the hidden state\n",
    "        forecasted_value, hidden = model(input_sequence, hidden)\n",
    "\n",
    "        # Reshape the forecasted value to have the same sequence length as input_sequence\n",
    "        forecasted_value = forecasted_value.view(1, 1, input_dim)\n",
    "\n",
    "        # Update the input sequence by removing the first element and adding the forecasted value at the end\n",
    "        input_sequence = torch.cat((input_sequence[:, 1:, :], forecasted_value), dim=1)\n",
    "\n",
    "# Convert the forecasted value back to the original scale using the scaler\n",
    "forecasted_value_unscaled = scaler.inverse_transform(forecasted_value.cpu().numpy())\n",
    "\n",
    "print(f\"Forecasted value of 'close' for the next time step: {forecasted_value_unscaled[0][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_forecast_steps):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# Predict the next value and update the hidden state\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m         forecasted_value, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assuming your model's forward() method takes input and hidden\u001b[39;00m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;66;03m# Reshape the forecasted value to have the same sequence length as input_sequence\u001b[39;00m\n\u001b[0;32m     27\u001b[0m         forecasted_value \u001b[38;5;241m=\u001b[39m forecasted_value\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, input_dim)\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to initialize the hidden state\n",
    "def init_hidden(batch_size):\n",
    "    # Initialize with zeros\n",
    "    return (torch.zeros(1, batch_size, 50), torch.zeros(1, batch_size, 50))  # Assuming 50 hidden units\n",
    "\n",
    "# Define the number of future steps you want to forecast\n",
    "num_forecast_steps = 1\n",
    "\n",
    "# Initialize the input sequence for forecasting\n",
    "input_sequence = x_train_scaled[-window_size:].view(1, window_size, input_dim)\n",
    "\n",
    "# Initialize the hidden state\n",
    "hidden = init_hidden(batch_size=1)\n",
    "\n",
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value and update the hidden state\n",
    "        forecasted_value, hidden = model(input_sequence, hidden)  # Assuming your model's forward() method takes input and hidden\n",
    "\n",
    "        # Reshape the forecasted value to have the same sequence length as input_sequence\n",
    "        forecasted_value = forecasted_value.view(1, 1, input_dim)\n",
    "\n",
    "        # Update the input sequence by removing the first element and adding the forecasted value at the end\n",
    "        input_sequence = torch.cat((input_sequence[:, 1:, :], forecasted_value), dim=1)\n",
    "\n",
    "# Convert the forecasted value back to the original scale using the scaler\n",
    "forecasted_value_unscaled = scaler.inverse_transform(forecasted_value.cpu().numpy())\n",
    "\n",
    "print(f\"Forecasted value of 'close' for the next time step: {forecasted_value_unscaled[0][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_forecast_steps):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;66;03m# Predict the next value and update the hidden state\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m         forecasted_value, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;66;03m# Reshape the forecasted value to have the same sequence length as input_sequence\u001b[39;00m\n\u001b[0;32m     28\u001b[0m         forecasted_value \u001b[38;5;241m=\u001b[39m forecasted_value\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, input_dim)\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to initialize the hidden state (customize based on your model architecture)\n",
    "def init_hidden(batch_size):\n",
    "    # For simplicity, assuming it's a single LSTM layer\n",
    "    # Initialize with zeros\n",
    "    return (torch.zeros(1, batch_size, 50), torch.zeros(1, batch_size, 50))  # Assuming 50 hidden units\n",
    "\n",
    "# Define the number of future steps you want to forecast\n",
    "num_forecast_steps = 1\n",
    "\n",
    "# Initialize the input sequence for forecasting\n",
    "input_sequence = x_train_scaled[-window_size:].view(1, window_size, input_dim)\n",
    "\n",
    "# Initialize the hidden state\n",
    "hidden = init_hidden(batch_size=1)\n",
    "\n",
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value and update the hidden state\n",
    "        forecasted_value, hidden = model(input_sequence, hidden)\n",
    "\n",
    "        # Reshape the forecasted value to have the same sequence length as input_sequence\n",
    "        forecasted_value = forecasted_value.view(1, 1, input_dim)\n",
    "\n",
    "        # Update the input sequence by removing the first element and adding the forecasted value at the end\n",
    "        input_sequence = torch.cat((input_sequence[:, 1:, :], forecasted_value), dim=1)\n",
    "\n",
    "# Convert the forecasted value back to the original scale using the scaler\n",
    "forecasted_value_unscaled = scaler.inverse_transform(forecasted_value.cpu().numpy())\n",
    "\n",
    "print(f\"Forecasted value of 'close' for the next time step: {forecasted_value_unscaled[0][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 10 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m         forecasted_value \u001b[38;5;241m=\u001b[39m forecasted_value\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m# Update the input sequence by removing the first element and adding the forecasted value at the end\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m         input_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecasted_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Convert the forecasted value back to the original scale using the scaler\u001b[39;00m\n\u001b[0;32m     23\u001b[0m forecasted_value_unscaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(forecasted_value\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 10 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the number of future steps you want to forecast\n",
    "num_forecast_steps = 1\n",
    "\n",
    "# Initialize the input sequence for forecasting\n",
    "input_sequence = x_train_scaled[-window_size:].view(1, window_size, input_dim)  # Assuming 'device' is set properly\n",
    "\n",
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value\n",
    "        forecasted_value = model(input_sequence)\n",
    "\n",
    "        # Reshape the forecasted value for concatenation\n",
    "        forecasted_value = forecasted_value.view(1, 1, 1)\n",
    "\n",
    "        # Update the input sequence by removing the first element and adding the forecasted value at the end\n",
    "        input_sequence = torch.cat((input_sequence[:, 1:, :], forecasted_value), dim=1)\n",
    "\n",
    "# Convert the forecasted value back to the original scale using the scaler\n",
    "forecasted_value_unscaled = scaler.inverse_transform(forecasted_value.cpu().numpy())\n",
    "\n",
    "print(f\"Forecasted value of 'close' for the next time step: {forecasted_value_unscaled[0][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 10])\n",
      "torch.Size([1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(input_sequence.shape)\n",
    "print(forecasted_value.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
