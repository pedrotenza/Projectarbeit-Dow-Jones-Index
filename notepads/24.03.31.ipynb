{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# x_train_tensor inverse\\n\\nx_test_original = scaler.inverse_transform(x_train_tensor.numpy())\\nprint(\"\\nFirst row of x_test_original:\")\\nprint(x_test_original[0])\\n\\nprint(\"\\nFirst row of x_train:\")\\nprint(x_train.head(1))\\n\\n\\n\\nprint(\"\\nLast row of x_test_original:\")\\nprint(x_test_original[-1])\\n\\nprint(\"\\nLast row of x_train:\")\\nprint(x_train.tail(1))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# size of the window for data preparation\n",
    "split_window_size = 20\n",
    "\n",
    "# Initialize lists to store training and temporary sets\n",
    "x_train_list, y_train_list, x_temp_list, y_temp_list = [], [], [], []\n",
    "\n",
    "# Iterate through the data with the specified window size\n",
    "for i in range(0, len(x_data) - split_window_size, split_window_size + 1):\n",
    "    x_train_temp = x_data.iloc[i:i+split_window_size+1]\n",
    "    y_train_temp = y_data.iloc[i:i+split_window_size+1]\n",
    "\n",
    "    # Separate the last row for the temporary set\n",
    "    x_train = x_train_temp.iloc[:-1]\n",
    "    y_train = y_train_temp.iloc[:-1]\n",
    "\n",
    "    x_temp = x_train_temp.iloc[-1:]\n",
    "    y_temp = y_train_temp.iloc[-1:]\n",
    "\n",
    "    x_train_list.append(x_train)\n",
    "    y_train_list.append(y_train)\n",
    "    x_temp_list.append(x_temp)\n",
    "    y_temp_list.append(y_temp)\n",
    "\n",
    "# Concatenate the lists into pandas DataFrames\n",
    "x_train = pd.concat(x_train_list)\n",
    "y_train = pd.concat(y_train_list)\n",
    "x_temp = pd.concat(x_temp_list)\n",
    "y_temp = pd.concat(y_temp_list)\n",
    "\n",
    "# print(y_train.head(50))\n",
    "x_temp_train, x_temp_val, y_temp_train, y_temp_val = train_test_split(x_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Split x_temp and y_temp into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.0011, window_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Training Loss: 20343.56775972129, Validation Loss: 20772.96484375, Test Loss: 18378.20703125\n",
      "Epoch [2/500], Training Loss: 15975.709533204068, Validation Loss: 18510.458984375, Test Loss: 16460.921875\n",
      "Epoch [3/500], Training Loss: 12701.79457816684, Validation Loss: 13764.28125, Test Loss: 11978.0341796875\n",
      "Epoch [4/500], Training Loss: 10496.436667058597, Validation Loss: 11254.5087890625, Test Loss: 9714.1279296875\n",
      "Epoch [5/500], Training Loss: 8904.020835812396, Validation Loss: 9800.4111328125, Test Loss: 8429.302734375\n",
      "Epoch [6/500], Training Loss: 7566.5063295737855, Validation Loss: 11722.322265625, Test Loss: 10213.236328125\n",
      "Epoch [7/500], Training Loss: 8899.490160372994, Validation Loss: 10694.3330078125, Test Loss: 9186.943359375\n",
      "Epoch [8/500], Training Loss: 5591.344159374201, Validation Loss: 7925.54052734375, Test Loss: 6865.27978515625\n",
      "Epoch [9/500], Training Loss: 4684.559803394485, Validation Loss: 5554.3984375, Test Loss: 4653.3447265625\n",
      "Epoch [10/500], Training Loss: 10700.356388746613, Validation Loss: 8022.603515625, Test Loss: 7035.7099609375\n",
      "Epoch [11/500], Training Loss: 5822.8543331152805, Validation Loss: 5818.3525390625, Test Loss: 5164.20654296875\n",
      "Epoch [12/500], Training Loss: 3840.6818423185127, Validation Loss: 5037.93212890625, Test Loss: 4517.96875\n",
      "Epoch [13/500], Training Loss: 2708.820625783103, Validation Loss: 4649.3017578125, Test Loss: 3999.46728515625\n",
      "Epoch [14/500], Training Loss: 2263.713287737098, Validation Loss: 3101.1337890625, Test Loss: 2503.526123046875\n",
      "Epoch [15/500], Training Loss: 1901.7179362635702, Validation Loss: 2511.360595703125, Test Loss: 2034.1982421875\n",
      "Epoch [16/500], Training Loss: 1555.0800110614846, Validation Loss: 2100.5087890625, Test Loss: 1706.7161865234375\n",
      "Epoch [17/500], Training Loss: 1343.3782904908612, Validation Loss: 1810.478759765625, Test Loss: 1483.2039794921875\n",
      "Epoch [18/500], Training Loss: 1176.9974206158327, Validation Loss: 1555.266845703125, Test Loss: 1269.031982421875\n",
      "Epoch [19/500], Training Loss: 1032.5269489899142, Validation Loss: 1337.3974609375, Test Loss: 1081.330322265625\n",
      "Epoch [20/500], Training Loss: 906.9324735544062, Validation Loss: 1195.014892578125, Test Loss: 969.0355834960938\n",
      "Epoch [21/500], Training Loss: 796.9862799815394, Validation Loss: 1070.827880859375, Test Loss: 865.271240234375\n",
      "Epoch [22/500], Training Loss: 700.9934485059139, Validation Loss: 969.2130126953125, Test Loss: 783.1397705078125\n",
      "Epoch [23/500], Training Loss: 614.4347964784865, Validation Loss: 854.711669921875, Test Loss: 690.8464965820312\n",
      "Epoch [24/500], Training Loss: 537.3690122055356, Validation Loss: 761.8290405273438, Test Loss: 620.5107421875\n",
      "Epoch [25/500], Training Loss: 469.0814541416597, Validation Loss: 667.263427734375, Test Loss: 547.1310424804688\n",
      "Epoch [26/500], Training Loss: 408.60335709361084, Validation Loss: 601.1935424804688, Test Loss: 500.4267883300781\n",
      "Epoch [27/500], Training Loss: 354.76763594453644, Validation Loss: 528.6245727539062, Test Loss: 448.7744140625\n",
      "Epoch [28/500], Training Loss: 307.3627407753504, Validation Loss: 458.99066162109375, Test Loss: 393.912109375\n",
      "Epoch [29/500], Training Loss: 265.5089109230839, Validation Loss: 389.9151306152344, Test Loss: 335.2493591308594\n",
      "Epoch [30/500], Training Loss: 229.22943798574062, Validation Loss: 332.7012023925781, Test Loss: 286.0763854980469\n",
      "Epoch [31/500], Training Loss: 198.50230845276644, Validation Loss: 293.5772399902344, Test Loss: 253.111083984375\n",
      "Epoch [32/500], Training Loss: 172.3735661542532, Validation Loss: 260.091064453125, Test Loss: 223.379638671875\n",
      "Epoch [33/500], Training Loss: 149.90955845948625, Validation Loss: 249.46273803710938, Test Loss: 212.49127197265625\n",
      "Epoch [34/500], Training Loss: 130.38741054459038, Validation Loss: 227.2133331298828, Test Loss: 197.62371826171875\n",
      "Epoch [35/500], Training Loss: 113.75783082598944, Validation Loss: 204.98916625976562, Test Loss: 180.64073181152344\n",
      "Epoch [36/500], Training Loss: 99.34098891882626, Validation Loss: 187.8059844970703, Test Loss: 167.5341796875\n",
      "Epoch [37/500], Training Loss: 86.77486676560456, Validation Loss: 176.49920654296875, Test Loss: 159.26136779785156\n",
      "Epoch [38/500], Training Loss: 75.865298165275, Validation Loss: 168.50042724609375, Test Loss: 153.47409057617188\n",
      "Epoch [39/500], Training Loss: 66.60799366611015, Validation Loss: 159.79791259765625, Test Loss: 146.68431091308594\n",
      "Epoch [40/500], Training Loss: 58.84099731455801, Validation Loss: 150.24871826171875, Test Loss: 138.96441650390625\n",
      "Epoch [41/500], Training Loss: 52.1357475886878, Validation Loss: 139.18389892578125, Test Loss: 130.07742309570312\n",
      "Epoch [42/500], Training Loss: 46.45568976825401, Validation Loss: 126.13321685791016, Test Loss: 119.8846435546875\n",
      "Epoch [43/500], Training Loss: 41.36391682933785, Validation Loss: 112.17511749267578, Test Loss: 108.93123626708984\n",
      "Epoch [44/500], Training Loss: 36.96014752369314, Validation Loss: 98.80921936035156, Test Loss: 97.74242401123047\n",
      "Epoch [45/500], Training Loss: 33.2367302459252, Validation Loss: 86.37535858154297, Test Loss: 86.96395874023438\n",
      "Epoch [46/500], Training Loss: 30.029951159154965, Validation Loss: 77.77840423583984, Test Loss: 80.14769744873047\n",
      "Epoch [47/500], Training Loss: 27.093343594971323, Validation Loss: 72.52571105957031, Test Loss: 76.7765121459961\n",
      "Epoch [48/500], Training Loss: 24.469875432776448, Validation Loss: 68.20915222167969, Test Loss: 73.97028350830078\n",
      "Epoch [49/500], Training Loss: 22.366362838061118, Validation Loss: 63.48299789428711, Test Loss: 70.41120147705078\n",
      "Epoch [50/500], Training Loss: 20.738290007841083, Validation Loss: 59.92717742919922, Test Loss: 67.90789031982422\n",
      "Epoch [51/500], Training Loss: 19.65757416404357, Validation Loss: 57.73343276977539, Test Loss: 66.93621826171875\n",
      "Epoch [52/500], Training Loss: 18.631305834630343, Validation Loss: 57.032920837402344, Test Loss: 67.13987731933594\n",
      "Epoch [53/500], Training Loss: 17.491505960919362, Validation Loss: 56.050445556640625, Test Loss: 66.46527099609375\n",
      "Epoch [54/500], Training Loss: 16.451899943485433, Validation Loss: 54.08685302734375, Test Loss: 64.48287200927734\n",
      "Epoch [55/500], Training Loss: 15.486691765618321, Validation Loss: 51.255531311035156, Test Loss: 61.416866302490234\n",
      "Epoch [56/500], Training Loss: 14.609608753737044, Validation Loss: 48.08026885986328, Test Loss: 57.844749450683594\n",
      "Epoch [57/500], Training Loss: 13.821392884319101, Validation Loss: 44.90812683105469, Test Loss: 54.23512649536133\n",
      "Epoch [58/500], Training Loss: 13.099064929894602, Validation Loss: 41.839996337890625, Test Loss: 50.78642272949219\n",
      "Epoch [59/500], Training Loss: 12.425493052668923, Validation Loss: 39.11688995361328, Test Loss: 47.650390625\n",
      "Epoch [60/500], Training Loss: 11.792420677824806, Validation Loss: 36.98428726196289, Test Loss: 45.04880905151367\n",
      "Epoch [61/500], Training Loss: 11.195850529129702, Validation Loss: 35.69256591796875, Test Loss: 43.313045501708984\n",
      "Epoch [62/500], Training Loss: 10.641597136170212, Validation Loss: 35.29694366455078, Test Loss: 42.5201530456543\n",
      "Epoch [63/500], Training Loss: 10.134486731465547, Validation Loss: 35.597755432128906, Test Loss: 42.447120666503906\n",
      "Epoch [64/500], Training Loss: 9.670790152620599, Validation Loss: 36.267822265625, Test Loss: 42.757328033447266\n",
      "Epoch [65/500], Training Loss: 9.24849278967515, Validation Loss: 36.91362762451172, Test Loss: 43.066349029541016\n",
      "Epoch [66/500], Training Loss: 8.864549262531918, Validation Loss: 37.03339767456055, Test Loss: 42.866825103759766\n",
      "Epoch [67/500], Training Loss: 8.52422715448349, Validation Loss: 36.95133972167969, Test Loss: 42.36777114868164\n",
      "Epoch [68/500], Training Loss: 8.237268383821384, Validation Loss: 37.34248733520508, Test Loss: 42.29964065551758\n",
      "Epoch [69/500], Training Loss: 8.0002213494446, Validation Loss: 37.92336654663086, Test Loss: 42.554622650146484\n",
      "Epoch [70/500], Training Loss: 7.799579972385223, Validation Loss: 38.50960922241211, Test Loss: 42.933799743652344\n",
      "Epoch [71/500], Training Loss: 7.623973089674957, Validation Loss: 39.085323333740234, Test Loss: 43.39365005493164\n",
      "Epoch [72/500], Training Loss: 7.4726529775074635, Validation Loss: 39.641387939453125, Test Loss: 43.90098190307617\n",
      "Epoch [73/500], Training Loss: 7.341658844502016, Validation Loss: 40.14576721191406, Test Loss: 44.397274017333984\n",
      "Epoch [74/500], Training Loss: 7.22621867572739, Validation Loss: 40.576969146728516, Test Loss: 44.83462905883789\n",
      "Epoch [75/500], Training Loss: 7.122493735738578, Validation Loss: 40.89638900756836, Test Loss: 45.15208435058594\n",
      "Epoch [76/500], Training Loss: 7.027787749019597, Validation Loss: 41.09578323364258, Test Loss: 45.323028564453125\n",
      "Epoch [77/500], Training Loss: 6.940356092316919, Validation Loss: 41.16553497314453, Test Loss: 45.326419830322266\n",
      "Epoch [78/500], Training Loss: 6.859506191273719, Validation Loss: 41.11188507080078, Test Loss: 45.16888427734375\n",
      "Epoch [79/500], Training Loss: 6.786112220840754, Validation Loss: 40.95978927612305, Test Loss: 44.88834762573242\n",
      "Epoch [80/500], Training Loss: 6.721051364135044, Validation Loss: 40.73650360107422, Test Loss: 44.53636169433594\n",
      "Epoch [81/500], Training Loss: 6.664563829825337, Validation Loss: 40.45599365234375, Test Loss: 44.145057678222656\n",
      "Early stopping at epoch 81, validation loss: 10223.9208984375\n",
      "Final Test Loss: 44.145057678222656\n",
      "Hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.001, window_size=1\n",
      "Epoch [1/500], Training Loss: 20862.046434951062, Validation Loss: 21220.734375, Test Loss: 18772.16796875\n",
      "Epoch [2/500], Training Loss: 16840.952068601833, Validation Loss: 18999.5078125, Test Loss: 16894.287109375\n",
      "Epoch [3/500], Training Loss: 13512.532264321757, Validation Loss: 14606.9560546875, Test Loss: 12720.2939453125\n",
      "Epoch [4/500], Training Loss: 11249.809005717325, Validation Loss: 11976.0341796875, Test Loss: 10346.9833984375\n",
      "Epoch [5/500], Training Loss: 9574.013869998587, Validation Loss: 10536.31640625, Test Loss: 9061.794921875\n",
      "Epoch [6/500], Training Loss: 8351.822647867446, Validation Loss: 9322.505859375, Test Loss: 8059.5087890625\n",
      "Epoch [7/500], Training Loss: 7189.02781949551, Validation Loss: 11617.806640625, Test Loss: 10117.4189453125\n",
      "Epoch [8/500], Training Loss: 6213.984211596567, Validation Loss: 8681.8271484375, Test Loss: 7459.39990234375\n",
      "Epoch [9/500], Training Loss: 5306.5680916630145, Validation Loss: 6944.90234375, Test Loss: 5947.0283203125\n",
      "Epoch [10/500], Training Loss: 4581.896032383057, Validation Loss: 5672.40234375, Test Loss: 4789.04150390625\n",
      "Epoch [11/500], Training Loss: 3956.254825038039, Validation Loss: 4754.2451171875, Test Loss: 3955.56005859375\n",
      "Epoch [12/500], Training Loss: 3402.3407466773338, Validation Loss: 4092.322998046875, Test Loss: 3378.083984375\n",
      "Epoch [13/500], Training Loss: 2922.8974688474927, Validation Loss: 3502.65576171875, Test Loss: 2871.65283203125\n",
      "Epoch [14/500], Training Loss: 2517.9190078343695, Validation Loss: 2982.890380859375, Test Loss: 2426.640869140625\n",
      "Epoch [15/500], Training Loss: 2190.655649815109, Validation Loss: 2569.1279296875, Test Loss: 2076.734375\n",
      "Epoch [16/500], Training Loss: 1913.4327066209146, Validation Loss: 2232.873291015625, Test Loss: 1792.7322998046875\n",
      "Epoch [17/500], Training Loss: 1672.1625736664641, Validation Loss: 1945.527587890625, Test Loss: 1550.2445068359375\n",
      "Epoch [18/500], Training Loss: 1468.2533291930533, Validation Loss: 1710.0164794921875, Test Loss: 1352.019287109375\n",
      "Epoch [19/500], Training Loss: 1296.7903693418987, Validation Loss: 1518.143798828125, Test Loss: 1191.0196533203125\n",
      "Epoch [20/500], Training Loss: 1147.433522191439, Validation Loss: 1376.3333740234375, Test Loss: 1074.968017578125\n",
      "Epoch [21/500], Training Loss: 1018.4548674031389, Validation Loss: 1240.6595458984375, Test Loss: 963.4747924804688\n",
      "Epoch [22/500], Training Loss: 906.0247844856176, Validation Loss: 1114.8934326171875, Test Loss: 862.349853515625\n",
      "Epoch [23/500], Training Loss: 806.5402241340457, Validation Loss: 992.700927734375, Test Loss: 766.3388061523438\n",
      "Epoch [24/500], Training Loss: 717.425402119068, Validation Loss: 882.905517578125, Test Loss: 684.8689575195312\n",
      "Epoch [25/500], Training Loss: 637.1369975196164, Validation Loss: 795.8597412109375, Test Loss: 620.0473022460938\n",
      "Epoch [26/500], Training Loss: 564.1603686699233, Validation Loss: 706.7150268554688, Test Loss: 552.8781127929688\n",
      "Epoch [27/500], Training Loss: 498.6555941842183, Validation Loss: 619.2927856445312, Test Loss: 484.5148010253906\n",
      "Epoch [28/500], Training Loss: 440.14417832971174, Validation Loss: 548.106201171875, Test Loss: 434.6629638671875\n",
      "Epoch [29/500], Training Loss: 387.8485797724971, Validation Loss: 485.7417907714844, Test Loss: 388.5950927734375\n",
      "Epoch [30/500], Training Loss: 341.2945982240928, Validation Loss: 428.4975891113281, Test Loss: 343.125244140625\n",
      "Epoch [31/500], Training Loss: 300.0936690427178, Validation Loss: 379.6343078613281, Test Loss: 305.2931823730469\n",
      "Epoch [32/500], Training Loss: 263.77969393852993, Validation Loss: 340.4201354980469, Test Loss: 277.22796630859375\n",
      "Epoch [33/500], Training Loss: 232.1643226441601, Validation Loss: 310.2190856933594, Test Loss: 256.7455139160156\n",
      "Epoch [34/500], Training Loss: 204.2846489671558, Validation Loss: 272.7101135253906, Test Loss: 225.86827087402344\n",
      "Epoch [35/500], Training Loss: 179.8778270571383, Validation Loss: 240.6126251220703, Test Loss: 198.8063507080078\n",
      "Epoch [36/500], Training Loss: 158.9353376303834, Validation Loss: 221.13796997070312, Test Loss: 180.90234375\n",
      "Epoch [37/500], Training Loss: 141.32544109195544, Validation Loss: 217.9652862548828, Test Loss: 180.9339599609375\n",
      "Epoch [38/500], Training Loss: 126.90103016622426, Validation Loss: 169.207275390625, Test Loss: 138.65945434570312\n",
      "Epoch [39/500], Training Loss: 112.91791468684032, Validation Loss: 147.9542236328125, Test Loss: 127.3363265991211\n",
      "Epoch [40/500], Training Loss: 100.005952579091, Validation Loss: 145.55111694335938, Test Loss: 135.8804931640625\n",
      "Epoch [41/500], Training Loss: 89.14872566020212, Validation Loss: 139.79351806640625, Test Loss: 131.947509765625\n",
      "Epoch [42/500], Training Loss: 79.8199482843124, Validation Loss: 126.2328872680664, Test Loss: 120.35897827148438\n",
      "Epoch [43/500], Training Loss: 71.53466128708561, Validation Loss: 122.40731811523438, Test Loss: 117.78712463378906\n",
      "Epoch [44/500], Training Loss: 64.13822840933715, Validation Loss: 115.98866271972656, Test Loss: 112.57617950439453\n",
      "Epoch [45/500], Training Loss: 57.553438052335785, Validation Loss: 100.6499252319336, Test Loss: 99.4235610961914\n",
      "Epoch [46/500], Training Loss: 52.00096889651978, Validation Loss: 83.6361312866211, Test Loss: 84.55425262451172\n",
      "Epoch [47/500], Training Loss: 47.467211396611546, Validation Loss: 71.3192367553711, Test Loss: 73.32129669189453\n",
      "Epoch [48/500], Training Loss: 43.651389473871625, Validation Loss: 64.18901062011719, Test Loss: 66.7728271484375\n",
      "Epoch [49/500], Training Loss: 40.45643694004502, Validation Loss: 59.73918533325195, Test Loss: 62.44810485839844\n",
      "Epoch [50/500], Training Loss: 37.836320913826015, Validation Loss: 56.28139877319336, Test Loss: 59.338619232177734\n",
      "Epoch [51/500], Training Loss: 35.35945204667924, Validation Loss: 53.24862289428711, Test Loss: 57.070556640625\n",
      "Epoch [52/500], Training Loss: 32.97763572685546, Validation Loss: 50.99550247192383, Test Loss: 55.37356185913086\n",
      "Epoch [53/500], Training Loss: 30.670994875346462, Validation Loss: 49.2110595703125, Test Loss: 53.74672317504883\n",
      "Epoch [54/500], Training Loss: 28.458195560219195, Validation Loss: 47.706050872802734, Test Loss: 52.28424072265625\n",
      "Epoch [55/500], Training Loss: 26.45921093827196, Validation Loss: 46.3114128112793, Test Loss: 50.93840408325195\n",
      "Epoch [56/500], Training Loss: 24.747674464800973, Validation Loss: 45.163856506347656, Test Loss: 49.762229919433594\n",
      "Epoch [57/500], Training Loss: 23.253755167252862, Validation Loss: 43.68076705932617, Test Loss: 48.40335464477539\n",
      "Epoch [58/500], Training Loss: 21.861132463338723, Validation Loss: 41.85895919799805, Test Loss: 46.8388557434082\n",
      "Epoch [59/500], Training Loss: 20.49972250630802, Validation Loss: 40.080810546875, Test Loss: 45.29350280761719\n",
      "Epoch [60/500], Training Loss: 19.19262527097872, Validation Loss: 38.579280853271484, Test Loss: 43.9408073425293\n",
      "Epoch [61/500], Training Loss: 17.99325195744091, Validation Loss: 37.41731643676758, Test Loss: 42.88777160644531\n",
      "Epoch [62/500], Training Loss: 16.924864773136907, Validation Loss: 36.50704574584961, Test Loss: 42.09025955200195\n",
      "Epoch [63/500], Training Loss: 15.984208753389007, Validation Loss: 35.70738983154297, Test Loss: 41.414207458496094\n",
      "Epoch [64/500], Training Loss: 15.156145100298614, Validation Loss: 34.90946960449219, Test Loss: 40.7558479309082\n",
      "Epoch [65/500], Training Loss: 14.420370017792777, Validation Loss: 34.05786895751953, Test Loss: 40.06991958618164\n",
      "Epoch [66/500], Training Loss: 13.760214905139785, Validation Loss: 33.12627410888672, Test Loss: 39.326393127441406\n",
      "Epoch [67/500], Training Loss: 13.16264092777449, Validation Loss: 32.10649871826172, Test Loss: 38.46967315673828\n",
      "Epoch [68/500], Training Loss: 12.616362406742663, Validation Loss: 30.964488983154297, Test Loss: 37.4424934387207\n",
      "Epoch [69/500], Training Loss: 12.110599152060647, Validation Loss: 29.68343734741211, Test Loss: 36.220970153808594\n",
      "Epoch [70/500], Training Loss: 11.638896395867135, Validation Loss: 28.317235946655273, Test Loss: 34.852481842041016\n",
      "Epoch [71/500], Training Loss: 11.199753057038048, Validation Loss: 26.999296188354492, Test Loss: 33.46498107910156\n",
      "Epoch [72/500], Training Loss: 10.794709410586583, Validation Loss: 25.89438819885254, Test Loss: 32.224815368652344\n",
      "Epoch [73/500], Training Loss: 10.42545823104652, Validation Loss: 25.1296329498291, Test Loss: 31.268098831176758\n",
      "Epoch [74/500], Training Loss: 10.092569910472086, Validation Loss: 24.757204055786133, Test Loss: 30.66138458251953\n",
      "Epoch [75/500], Training Loss: 9.793917409885177, Validation Loss: 24.7457218170166, Test Loss: 30.386999130249023\n",
      "Epoch [76/500], Training Loss: 9.525145996745602, Validation Loss: 25.01955223083496, Test Loss: 30.381362915039062\n",
      "Epoch [77/500], Training Loss: 9.281375066204166, Validation Loss: 25.503183364868164, Test Loss: 30.57855987548828\n",
      "Epoch [78/500], Training Loss: 9.05760625936862, Validation Loss: 26.13113021850586, Test Loss: 30.920345306396484\n",
      "Epoch [79/500], Training Loss: 8.850578418090869, Validation Loss: 26.855993270874023, Test Loss: 31.36377716064453\n",
      "Epoch [80/500], Training Loss: 8.65819186098636, Validation Loss: 27.626951217651367, Test Loss: 31.86042594909668\n",
      "Epoch [81/500], Training Loss: 8.478714044268147, Validation Loss: 28.377235412597656, Test Loss: 32.34445571899414\n",
      "Epoch [82/500], Training Loss: 8.311154176860136, Validation Loss: 29.040029525756836, Test Loss: 32.74946212768555\n",
      "Epoch [83/500], Training Loss: 8.154104934868148, Validation Loss: 29.553897857666016, Test Loss: 33.0151481628418\n",
      "Epoch [84/500], Training Loss: 8.00645927194301, Validation Loss: 29.88447380065918, Test Loss: 33.11035919189453\n",
      "Epoch [85/500], Training Loss: 7.866846421500846, Validation Loss: 30.01593589782715, Test Loss: 33.02579879760742\n",
      "Epoch [86/500], Training Loss: 7.734501331870888, Validation Loss: 29.961095809936523, Test Loss: 32.78266143798828\n",
      "Epoch [87/500], Training Loss: 7.608804303968292, Validation Loss: 29.751375198364258, Test Loss: 32.42121505737305\n",
      "Epoch [88/500], Training Loss: 7.4892043836139655, Validation Loss: 29.432096481323242, Test Loss: 31.994503021240234\n",
      "Epoch [89/500], Training Loss: 7.375678191779699, Validation Loss: 29.065805435180664, Test Loss: 31.57051658630371\n",
      "Epoch [90/500], Training Loss: 7.268800213366448, Validation Loss: 28.69649314880371, Test Loss: 31.195341110229492\n",
      "Epoch [91/500], Training Loss: 7.16900544557099, Validation Loss: 28.3631649017334, Test Loss: 30.90629005432129\n",
      "Epoch [92/500], Training Loss: 7.076897213311355, Validation Loss: 28.084062576293945, Test Loss: 30.715293884277344\n",
      "Epoch [93/500], Training Loss: 6.99278599021096, Validation Loss: 27.860435485839844, Test Loss: 30.61498260498047\n",
      "Epoch [94/500], Training Loss: 6.916886146237534, Validation Loss: 27.68064308166504, Test Loss: 30.58223533630371\n",
      "Early stopping at epoch 94, validation loss: 7019.23974609375\n",
      "Final Test Loss: 30.58223533630371\n",
      "Hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.0009, window_size=1\n",
      "Epoch [1/500], Training Loss: 21430.821649147365, Validation Loss: 21742.42578125, Test Loss: 19236.68359375\n",
      "Epoch [2/500], Training Loss: 17473.66998926855, Validation Loss: 19332.966796875, Test Loss: 17151.66796875\n",
      "Epoch [3/500], Training Loss: 14248.965008932968, Validation Loss: 15596.5166015625, Test Loss: 13621.904296875\n",
      "Epoch [4/500], Training Loss: 11994.77825719417, Validation Loss: 12833.01953125, Test Loss: 11106.095703125\n",
      "Epoch [5/500], Training Loss: 10294.427969273775, Validation Loss: 11152.2158203125, Test Loss: 9597.43359375\n",
      "Epoch [6/500], Training Loss: 8992.866306109625, Validation Loss: 9930.958984375, Test Loss: 8525.5556640625\n",
      "Epoch [7/500], Training Loss: 7851.270590960141, Validation Loss: 11240.796875, Test Loss: 9796.099609375\n",
      "Epoch [8/500], Training Loss: 8083.355688925134, Validation Loss: 10475.16796875, Test Loss: 9099.7294921875\n",
      "Epoch [9/500], Training Loss: 6927.923598134022, Validation Loss: 9805.6669921875, Test Loss: 8681.9140625\n",
      "Epoch [10/500], Training Loss: 5400.680545860459, Validation Loss: 7689.822265625, Test Loss: 6612.28662109375\n",
      "Epoch [11/500], Training Loss: 4618.473166685028, Validation Loss: 6051.416015625, Test Loss: 5151.0537109375\n",
      "Epoch [12/500], Training Loss: 22073.800408436215, Validation Loss: 23665.349609375, Test Loss: 20981.552734375\n",
      "Epoch [13/500], Training Loss: 21306.632163768838, Validation Loss: 22807.638671875, Test Loss: 20168.73046875\n",
      "Epoch [14/500], Training Loss: 20584.945084938547, Validation Loss: 22070.458984375, Test Loss: 19486.1328125\n",
      "Epoch [15/500], Training Loss: 19908.500691218327, Validation Loss: 21385.142578125, Test Loss: 18852.974609375\n",
      "Epoch [16/500], Training Loss: 19271.851984492296, Validation Loss: 20712.3046875, Test Loss: 18228.548828125\n",
      "Epoch [17/500], Training Loss: 18670.743769654953, Validation Loss: 20075.90625, Test Loss: 17639.107421875\n",
      "Epoch [18/500], Training Loss: 18100.882163508886, Validation Loss: 19503.244140625, Test Loss: 17112.576171875\n",
      "Epoch [19/500], Training Loss: 17559.473847402864, Validation Loss: 18944.9921875, Test Loss: 16599.439453125\n",
      "Epoch [20/500], Training Loss: 17044.680583919115, Validation Loss: 18415.37890625, Test Loss: 16114.1298828125\n",
      "Epoch [21/500], Training Loss: 16555.406854867557, Validation Loss: 17909.009765625, Test Loss: 15650.9951171875\n",
      "Epoch [22/500], Training Loss: 16090.89253628804, Validation Loss: 17413.9375, Test Loss: 15197.9921875\n",
      "Epoch [23/500], Training Loss: 15649.046873567371, Validation Loss: 16927.060546875, Test Loss: 14751.982421875\n",
      "Epoch [24/500], Training Loss: 15227.424100225062, Validation Loss: 16485.486328125, Test Loss: 14350.0849609375\n",
      "Epoch [25/500], Training Loss: 14824.480816669684, Validation Loss: 16066.4033203125, Test Loss: 13969.7060546875\n",
      "Epoch [26/500], Training Loss: 14439.056209523793, Validation Loss: 15655.12109375, Test Loss: 13596.333984375\n",
      "Epoch [27/500], Training Loss: 14069.395998760121, Validation Loss: 15287.2021484375, Test Loss: 13266.0703125\n",
      "Epoch [28/500], Training Loss: 13714.449295868277, Validation Loss: 14932.55859375, Test Loss: 12946.87109375\n",
      "Epoch [29/500], Training Loss: 13374.294815870277, Validation Loss: 14581.27734375, Test Loss: 12631.5390625\n",
      "Epoch [30/500], Training Loss: 13048.154098457357, Validation Loss: 14237.5498046875, Test Loss: 12323.8359375\n",
      "Early stopping at epoch 30, validation loss: 3546761.0\n",
      "Final Test Loss: 12323.8359375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [8]\n",
    "num_layers_list = [2]\n",
    "learning_rates = [0.0011, 0.001, 0.0009]\n",
    "window_sizes = [1]\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 20  # Number of epochs to wait for improvement\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}, validation loss: {val_loss}')\n",
    "                break\n",
    "\n",
    "        # Test the model\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_test_tensor)):\n",
    "                window_end = min(i + window_size, len(x_test_tensor))\n",
    "                inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                test_loss += criterion(outputs, labels)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}, Test Loss: {test_loss / len(x_test_tensor)}')\n",
    "\n",
    "    print(f'Final Test Loss: {test_loss / len(x_test_tensor)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ä' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mä\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ä' is not defined"
     ]
    }
   ],
   "source": [
    "ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.0011, window_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Training Loss: 20306.313814295034, Validation Loss: 20753.515625\n",
      "Epoch [2/500], Training Loss: 16048.840531120357, Validation Loss: 18560.318359375\n",
      "Epoch [3/500], Training Loss: 12729.05149925699, Validation Loss: 13655.046875\n",
      "Epoch [4/500], Training Loss: 10512.95328698902, Validation Loss: 11163.7900390625\n",
      "Epoch [5/500], Training Loss: 9026.591069200795, Validation Loss: 9856.130859375\n",
      "Epoch [6/500], Training Loss: 13646.906094545182, Validation Loss: 10473.8955078125\n",
      "Epoch [7/500], Training Loss: 7557.543504811881, Validation Loss: 11543.12109375\n",
      "Epoch [8/500], Training Loss: 6201.650715865325, Validation Loss: 12255.1552734375\n",
      "Epoch [9/500], Training Loss: 5044.054255530144, Validation Loss: 9074.212890625\n",
      "Epoch [10/500], Training Loss: 4239.9348373606435, Validation Loss: 6048.8974609375\n",
      "Epoch [11/500], Training Loss: 3588.590451966964, Validation Loss: 4562.951171875\n",
      "Epoch [12/500], Training Loss: 3035.274334255149, Validation Loss: 3664.72119140625\n",
      "Epoch [13/500], Training Loss: 2571.5453118034443, Validation Loss: 3027.253662109375\n",
      "Epoch [14/500], Training Loss: 2202.1031464911193, Validation Loss: 2576.518310546875\n",
      "Epoch [15/500], Training Loss: 1896.3312964625093, Validation Loss: 2233.352294921875\n",
      "Epoch [16/500], Training Loss: 1634.8303793169034, Validation Loss: 1954.1929931640625\n",
      "Epoch [17/500], Training Loss: 1417.5079770555355, Validation Loss: 1723.86181640625\n",
      "Epoch [18/500], Training Loss: 1237.7030514400712, Validation Loss: 1541.0244140625\n",
      "Epoch [19/500], Training Loss: 1083.0962921068153, Validation Loss: 1382.0693359375\n",
      "Epoch [20/500], Training Loss: 949.1011968277963, Validation Loss: 1241.124755859375\n",
      "Epoch [21/500], Training Loss: 832.4682100172557, Validation Loss: 1103.00390625\n",
      "Epoch [22/500], Training Loss: 730.2684239513505, Validation Loss: 985.1192626953125\n",
      "Epoch [23/500], Training Loss: 639.573685904687, Validation Loss: 885.59326171875\n",
      "Epoch [24/500], Training Loss: 558.9527798938045, Validation Loss: 786.1063842773438\n",
      "Epoch [25/500], Training Loss: 487.5530342982745, Validation Loss: 687.8968505859375\n",
      "Epoch [26/500], Training Loss: 423.9989964407251, Validation Loss: 612.4187622070312\n",
      "Epoch [27/500], Training Loss: 367.3628847594504, Validation Loss: 550.1456909179688\n",
      "Epoch [28/500], Training Loss: 317.71796978953904, Validation Loss: 489.4126892089844\n",
      "Epoch [29/500], Training Loss: 273.8595447213598, Validation Loss: 433.7425537109375\n",
      "Epoch [30/500], Training Loss: 235.76989658039605, Validation Loss: 381.9217224121094\n",
      "Epoch [31/500], Training Loss: 203.44688723620564, Validation Loss: 330.3912048339844\n",
      "Epoch [32/500], Training Loss: 176.05617896484742, Validation Loss: 283.1123352050781\n",
      "Epoch [33/500], Training Loss: 153.0439741347938, Validation Loss: 248.21063232421875\n",
      "Epoch [34/500], Training Loss: 133.32289461433143, Validation Loss: 217.3267364501953\n",
      "Epoch [35/500], Training Loss: 116.08114454528085, Validation Loss: 185.8241424560547\n",
      "Epoch [36/500], Training Loss: 101.03686280292992, Validation Loss: 162.8748016357422\n",
      "Epoch [37/500], Training Loss: 87.99972835605426, Validation Loss: 142.90066528320312\n",
      "Epoch [38/500], Training Loss: 77.01484150262792, Validation Loss: 121.59912109375\n",
      "Epoch [39/500], Training Loss: 67.59325983533635, Validation Loss: 105.4186019897461\n",
      "Epoch [40/500], Training Loss: 59.60254915538501, Validation Loss: 93.1181411743164\n",
      "Epoch [41/500], Training Loss: 52.86069514148157, Validation Loss: 84.47799682617188\n",
      "Epoch [42/500], Training Loss: 47.169023183903754, Validation Loss: 79.04499816894531\n",
      "Epoch [43/500], Training Loss: 42.21671849344101, Validation Loss: 75.24322509765625\n",
      "Epoch [44/500], Training Loss: 37.76059590401332, Validation Loss: 71.00411987304688\n",
      "Epoch [45/500], Training Loss: 33.83882460520044, Validation Loss: 67.56465148925781\n",
      "Epoch [46/500], Training Loss: 30.547380181430423, Validation Loss: 64.22176361083984\n",
      "Epoch [47/500], Training Loss: 27.73575432292964, Validation Loss: 60.741153717041016\n",
      "Epoch [48/500], Training Loss: 25.156646245212464, Validation Loss: 56.81822967529297\n",
      "Epoch [49/500], Training Loss: 22.938358613515177, Validation Loss: 52.67428970336914\n",
      "Epoch [50/500], Training Loss: 21.14933676519757, Validation Loss: 49.01112747192383\n",
      "Epoch [51/500], Training Loss: 19.62532972296315, Validation Loss: 46.10961151123047\n",
      "Epoch [52/500], Training Loss: 18.301936512153315, Validation Loss: 43.8857307434082\n",
      "Epoch [53/500], Training Loss: 17.11906070748275, Validation Loss: 42.31598663330078\n",
      "Epoch [54/500], Training Loss: 16.047378615178264, Validation Loss: 41.18120193481445\n",
      "Epoch [55/500], Training Loss: 15.076713488711077, Validation Loss: 40.20206832885742\n",
      "Epoch [56/500], Training Loss: 14.201589210801878, Validation Loss: 39.263755798339844\n",
      "Epoch [57/500], Training Loss: 13.414613519564655, Validation Loss: 38.378787994384766\n",
      "Epoch [58/500], Training Loss: 12.706689900968154, Validation Loss: 37.57329177856445\n",
      "Epoch [59/500], Training Loss: 12.067325503417123, Validation Loss: 36.859291076660156\n",
      "Epoch [60/500], Training Loss: 11.48822881745559, Validation Loss: 36.22650146484375\n",
      "Epoch [61/500], Training Loss: 10.961605262452291, Validation Loss: 35.66766357421875\n",
      "Epoch [62/500], Training Loss: 10.481459792796423, Validation Loss: 35.18159866333008\n",
      "Epoch [63/500], Training Loss: 10.04280935332659, Validation Loss: 34.770877838134766\n",
      "Epoch [64/500], Training Loss: 9.641326593053765, Validation Loss: 34.438663482666016\n",
      "Epoch [65/500], Training Loss: 9.273302143602159, Validation Loss: 34.18846130371094\n",
      "Epoch [66/500], Training Loss: 8.935677011585028, Validation Loss: 34.01883316040039\n",
      "Epoch [67/500], Training Loss: 8.625243555404316, Validation Loss: 33.924129486083984\n",
      "Epoch [68/500], Training Loss: 8.339250182427934, Validation Loss: 33.89653778076172\n",
      "Epoch [69/500], Training Loss: 8.074950713900712, Validation Loss: 33.92750930786133\n",
      "Epoch [70/500], Training Loss: 7.830014600149436, Validation Loss: 34.01179504394531\n",
      "Epoch [71/500], Training Loss: 7.602223060114253, Validation Loss: 34.13839340209961\n",
      "Epoch [72/500], Training Loss: 7.389323985624862, Validation Loss: 34.30694580078125\n",
      "Epoch [73/500], Training Loss: 7.189770901805075, Validation Loss: 34.517822265625\n",
      "Epoch [74/500], Training Loss: 7.002008694860233, Validation Loss: 34.775062561035156\n",
      "Epoch [75/500], Training Loss: 6.824737776854493, Validation Loss: 35.08465576171875\n",
      "Epoch [76/500], Training Loss: 6.656998899372872, Validation Loss: 35.45521545410156\n",
      "Epoch [77/500], Training Loss: 6.49811323990361, Validation Loss: 35.90217208862305\n",
      "Epoch [78/500], Training Loss: 6.347321488488201, Validation Loss: 36.440826416015625\n",
      "Epoch [79/500], Training Loss: 6.204344022584192, Validation Loss: 37.084190368652344\n",
      "Epoch [80/500], Training Loss: 6.06905606608569, Validation Loss: 37.84940719604492\n",
      "Epoch [81/500], Training Loss: 5.941026500371896, Validation Loss: 38.74355697631836\n",
      "Epoch [82/500], Training Loss: 5.819953107209814, Validation Loss: 39.7706413269043\n",
      "Epoch [83/500], Training Loss: 5.705522534940732, Validation Loss: 40.92522048950195\n",
      "Epoch [84/500], Training Loss: 5.597068665317035, Validation Loss: 42.203163146972656\n",
      "Epoch [85/500], Training Loss: 5.494282920639116, Validation Loss: 43.600067138671875\n",
      "Epoch [86/500], Training Loss: 5.396672121938406, Validation Loss: 45.117584228515625\n",
      "Epoch [87/500], Training Loss: 5.304018100974319, Validation Loss: 46.762733459472656\n",
      "Epoch [88/500], Training Loss: 5.216142990019782, Validation Loss: 48.533905029296875\n",
      "Early stopping! Validation loss has not improved in the last 20 epochs.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with hyperparameters: input_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, hidden_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_layers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, learning_rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, window_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m model \u001b[38;5;241m=\u001b[39m train_lstm_model(input_size, hidden_size, num_layers, learning_rate, window_size, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor)\n\u001b[1;32m---> 95\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_loss \u001b[38;5;241m<\u001b[39m best_test_loss:\n",
      "Cell \u001b[1;32mIn[12], line 83\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, x_test_tensor, y_test_tensor, window_size)\u001b[0m\n\u001b[0;32m     80\u001b[0m         labels \u001b[38;5;241m=\u001b[39m y_test_tensor[window_end \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     82\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 83\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m(outputs, labels)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(x_test_tensor)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'criterion' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [8]\n",
    "num_layers_list = [2]\n",
    "learning_rates = [0.0011]\n",
    "window_sizes = [1]\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 20  # Number of epochs to wait for improvement\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Function to create and train LSTM model with given hyperparameters\n",
    "def train_lstm_model(input_size, hidden_size, num_layers, learning_rate, window_size, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor):\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement_counter = 0\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "\n",
    "        if no_improvement_counter >= patience:\n",
    "            print(\"Early stopping! Validation loss has not improved in the last {} epochs.\".format(patience))\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model on test data\n",
    "def evaluate_model(model, x_test_tensor, y_test_tensor, window_size):\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    return test_loss / len(x_test_tensor)\n",
    "\n",
    "best_hyperparameters = None\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "# Iterate over hyperparameter combinations and train models\n",
    "for input_size, hidden_size, num_layers, learning_rate, window_size in hyperparameter_combinations:\n",
    "    print(f\"Training with hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    model = train_lstm_model(input_size, hidden_size, num_layers, learning_rate, window_size, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor)\n",
    "    test_loss = evaluate_model(model, x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        best_hyperparameters = (input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    if no_improvement_counter >= patience:\n",
    "        print(\"Early stopping! Validation loss has not improved in the last {} epochs.\".format(patience))\n",
    "        break\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"input_size={best_hyperparameters[0]}, hidden_size={best_hyperparameters[1]}, num_layers={best_hyperparameters[2]}, learning_rate={best_hyperparameters[3]}, window_size={best_hyperparameters[4]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.0015, window_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Training Loss: 18770.752336328915, Validation Loss: 19579.3359375\n",
      "Epoch [2/500], Training Loss: 13740.519708273014, Validation Loss: 16766.568359375\n",
      "Epoch [3/500], Training Loss: 10378.604738698396, Validation Loss: 11175.2041015625\n",
      "Epoch [4/500], Training Loss: 8209.93255598469, Validation Loss: 9167.1962890625\n",
      "Epoch [5/500], Training Loss: 13574.830220232927, Validation Loss: 10389.9453125\n",
      "Epoch [6/500], Training Loss: 9492.488707595063, Validation Loss: 8144.0302734375\n",
      "Epoch [7/500], Training Loss: 5810.206425079688, Validation Loss: 6087.51318359375\n",
      "Epoch [8/500], Training Loss: 5467.8671059176595, Validation Loss: 5999.220703125\n",
      "Epoch [9/500], Training Loss: 2988.2715256312536, Validation Loss: 4043.23828125\n",
      "Epoch [10/500], Training Loss: 2367.885133884168, Validation Loss: 3482.7080078125\n",
      "Epoch [11/500], Training Loss: 1921.6371661279293, Validation Loss: 3057.561279296875\n",
      "Epoch [12/500], Training Loss: 1564.506560606458, Validation Loss: 2137.929931640625\n",
      "Epoch [13/500], Training Loss: 1294.21016361008, Validation Loss: 1604.314697265625\n",
      "Epoch [14/500], Training Loss: 1080.287960667981, Validation Loss: 1305.397216796875\n",
      "Epoch [15/500], Training Loss: 902.076000724118, Validation Loss: 1082.8265380859375\n",
      "Epoch [16/500], Training Loss: 756.6191421001364, Validation Loss: 926.2649536132812\n",
      "Epoch [17/500], Training Loss: 632.2871722789613, Validation Loss: 809.0403442382812\n",
      "Epoch [18/500], Training Loss: 525.2893928389267, Validation Loss: 674.413818359375\n",
      "Epoch [19/500], Training Loss: 434.73630660208454, Validation Loss: 600.6495361328125\n",
      "Epoch [20/500], Training Loss: 359.2921359174709, Validation Loss: 515.4691772460938\n",
      "Epoch [21/500], Training Loss: 296.8971139167035, Validation Loss: 437.3340759277344\n",
      "Epoch [22/500], Training Loss: 243.28387015212388, Validation Loss: 392.11639404296875\n",
      "Epoch [23/500], Training Loss: 199.90866786967769, Validation Loss: 336.61676025390625\n",
      "Epoch [24/500], Training Loss: 164.75023113500094, Validation Loss: 302.7597961425781\n",
      "Epoch [25/500], Training Loss: 136.84426071655068, Validation Loss: 285.7470397949219\n",
      "Epoch [26/500], Training Loss: 115.12243505402608, Validation Loss: 230.96832275390625\n",
      "Epoch [27/500], Training Loss: 95.80354950187238, Validation Loss: 173.40086364746094\n",
      "Epoch [28/500], Training Loss: 80.29714341335513, Validation Loss: 141.05419921875\n",
      "Epoch [29/500], Training Loss: 67.64232583099941, Validation Loss: 121.30191040039062\n",
      "Epoch [30/500], Training Loss: 57.741540585382666, Validation Loss: 109.08834838867188\n",
      "Epoch [31/500], Training Loss: 50.03693775925208, Validation Loss: 101.75579071044922\n",
      "Epoch [32/500], Training Loss: 44.102917790353665, Validation Loss: 96.1486587524414\n",
      "Epoch [33/500], Training Loss: 39.292229338726415, Validation Loss: 90.0340576171875\n",
      "Epoch [34/500], Training Loss: 35.058087198741745, Validation Loss: 84.53654479980469\n",
      "Epoch [35/500], Training Loss: 31.167957787313945, Validation Loss: 79.2562484741211\n",
      "Epoch [36/500], Training Loss: 27.592345187618502, Validation Loss: 73.39373016357422\n",
      "Epoch [37/500], Training Loss: 24.6502898306776, Validation Loss: 68.96914672851562\n",
      "Epoch [38/500], Training Loss: 22.0909283720314, Validation Loss: 66.29305267333984\n",
      "Epoch [39/500], Training Loss: 19.937018599348953, Validation Loss: 64.55963134765625\n",
      "Epoch [40/500], Training Loss: 18.221203291524475, Validation Loss: 62.591278076171875\n",
      "Epoch [41/500], Training Loss: 16.80292841946392, Validation Loss: 60.717960357666016\n",
      "Epoch [42/500], Training Loss: 15.605777005177147, Validation Loss: 59.24691390991211\n",
      "Epoch [43/500], Training Loss: 14.582950649842408, Validation Loss: 58.2706298828125\n",
      "Epoch [44/500], Training Loss: 13.710926858107442, Validation Loss: 57.7541389465332\n",
      "Epoch [45/500], Training Loss: 12.961548069782728, Validation Loss: 57.6030387878418\n",
      "Epoch [46/500], Training Loss: 12.304982069745536, Validation Loss: 57.56807327270508\n",
      "Epoch [47/500], Training Loss: 11.725556706020935, Validation Loss: 57.357025146484375\n",
      "Epoch [48/500], Training Loss: 11.21143477541909, Validation Loss: 56.82589340209961\n",
      "Epoch [49/500], Training Loss: 10.745657503883765, Validation Loss: 55.98225021362305\n",
      "Epoch [50/500], Training Loss: 10.310883628466474, Validation Loss: 54.891571044921875\n",
      "Epoch [51/500], Training Loss: 9.898645221276848, Validation Loss: 53.5904541015625\n",
      "Epoch [52/500], Training Loss: 9.50964826030339, Validation Loss: 52.12941360473633\n",
      "Epoch [53/500], Training Loss: 9.149907916834913, Validation Loss: 50.71538162231445\n",
      "Epoch [54/500], Training Loss: 8.829504036608558, Validation Loss: 49.81485366821289\n",
      "Epoch [55/500], Training Loss: 8.554736795753673, Validation Loss: 49.60148620605469\n",
      "Epoch [56/500], Training Loss: 8.313056455742858, Validation Loss: 49.491329193115234\n",
      "Epoch [57/500], Training Loss: 8.097887906917833, Validation Loss: 49.287010192871094\n",
      "Epoch [58/500], Training Loss: 7.907540601342761, Validation Loss: 49.21182632446289\n",
      "Epoch [59/500], Training Loss: 7.739797230587132, Validation Loss: 49.34838104248047\n",
      "Epoch [60/500], Training Loss: 7.5911831193394725, Validation Loss: 49.698028564453125\n",
      "Epoch [61/500], Training Loss: 7.459937877459897, Validation Loss: 50.25242233276367\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_size, hidden_size, num_layers, learning_rate, window_size \u001b[38;5;129;01min\u001b[39;00m hyperparameter_combinations:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with hyperparameters: input_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, hidden_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_layers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, learning_rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, window_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lstm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m evaluate_model(model, x_test_tensor, y_test_tensor, window_size)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 36\u001b[0m, in \u001b[0;36mtrain_lstm_model\u001b[1;34m(input_size, hidden_size, num_layers, learning_rate, window_size, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor)\u001b[0m\n\u001b[0;32m     33\u001b[0m inputs \u001b[38;5;241m=\u001b[39m x_train_tensor[i:window_end]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     34\u001b[0m labels \u001b[38;5;241m=\u001b[39m y_train_tensor[window_end \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     13\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 15\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    816\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [8]\n",
    "num_layers_list = [2]\n",
    "learning_rates = [0.0015]\n",
    "window_sizes = [1]\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 20  # Number of epochs to wait for improvement\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Function to create and train LSTM model with given hyperparameters\n",
    "def train_lstm_model(input_size, hidden_size, num_layers, learning_rate, window_size, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor):\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement_counter = 0\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "\n",
    "        if no_improvement_counter >= patience:\n",
    "            print(\"Early stopping! Validation loss has not improved in the last {} epochs.\".format(patience))\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model on test data\n",
    "def evaluate_model(model, x_test_tensor, y_test_tensor, window_size):\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    return test_loss / len(x_test_tensor)\n",
    "\n",
    "best_hyperparameters = None\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "# Iterate over hyperparameter combinations and train models\n",
    "for input_size, hidden_size, num_layers, learning_rate, window_size in hyperparameter_combinations:\n",
    "    print(f\"Training with hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    model = train_lstm_model(input_size, hidden_size, num_layers, learning_rate, window_size, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor)\n",
    "    test_loss = evaluate_model(model, x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        best_hyperparameters = (input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    if no_improvement_counter >= patience:\n",
    "        print(\"Early stopping! Validation loss has not improved in the last {} epochs.\".format(patience))\n",
    "        break\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"input_size={best_hyperparameters[0]}, hidden_size={best_hyperparameters[1]}, num_layers={best_hyperparameters[2]}, learning_rate={best_hyperparameters[3]}, window_size={best_hyperparameters[4]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.001, window_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Training Loss: 20835.642038314178, Validation Loss: 21205.44140625\n",
      "Epoch [2/500], Training Loss: 16828.648456067745, Validation Loss: 18972.677734375\n",
      "Epoch [3/500], Training Loss: 13498.493071557354, Validation Loss: 14851.197265625\n",
      "Epoch [4/500], Training Loss: 11239.545385571631, Validation Loss: 11871.763671875\n",
      "Epoch [5/500], Training Loss: 9568.744962493936, Validation Loss: 10662.0888671875\n",
      "Epoch [6/500], Training Loss: 23455.901811745836, Validation Loss: 22218.861328125\n",
      "Epoch [7/500], Training Loss: 12995.45392429937, Validation Loss: 9855.7158203125\n",
      "Epoch [8/500], Training Loss: 7643.12833366478, Validation Loss: 9261.47265625\n",
      "Epoch [9/500], Training Loss: 6493.353465517648, Validation Loss: 17180.90234375\n",
      "Epoch [10/500], Training Loss: 5612.694482069061, Validation Loss: 9053.916015625\n",
      "Epoch [11/500], Training Loss: 4809.324778606503, Validation Loss: 6324.97802734375\n",
      "Epoch [12/500], Training Loss: 4146.258019157851, Validation Loss: 5033.62255859375\n",
      "Epoch [13/500], Training Loss: 3569.6700166214187, Validation Loss: 4180.3916015625\n",
      "Epoch [14/500], Training Loss: 3063.7140590422987, Validation Loss: 3532.136962890625\n",
      "Epoch [15/500], Training Loss: 2640.185508277992, Validation Loss: 3045.728759765625\n",
      "Epoch [16/500], Training Loss: 2291.4896000259396, Validation Loss: 2682.6533203125\n",
      "Epoch [17/500], Training Loss: 2003.371928127355, Validation Loss: 2388.605712890625\n",
      "Epoch [18/500], Training Loss: 1764.3347934103545, Validation Loss: 2153.998291015625\n",
      "Epoch [19/500], Training Loss: 1560.7817781318995, Validation Loss: 1949.5999755859375\n",
      "Epoch [20/500], Training Loss: 1381.1236623297034, Validation Loss: 1770.3848876953125\n",
      "Epoch [21/500], Training Loss: 1222.221259843304, Validation Loss: 1622.1114501953125\n",
      "Epoch [22/500], Training Loss: 1086.3041983285802, Validation Loss: 1466.8465576171875\n",
      "Epoch [23/500], Training Loss: 968.1808644463744, Validation Loss: 1321.689697265625\n",
      "Epoch [24/500], Training Loss: 862.7665648733032, Validation Loss: 1178.19580078125\n",
      "Epoch [25/500], Training Loss: 767.5178857570916, Validation Loss: 1044.6053466796875\n",
      "Epoch [26/500], Training Loss: 681.5145264695453, Validation Loss: 919.7523803710938\n",
      "Epoch [27/500], Training Loss: 603.8019739911476, Validation Loss: 809.5982055664062\n",
      "Epoch [28/500], Training Loss: 533.9569936968799, Validation Loss: 704.8402099609375\n",
      "Epoch [29/500], Training Loss: 471.3297744299451, Validation Loss: 614.5498657226562\n",
      "Epoch [30/500], Training Loss: 415.65849894168514, Validation Loss: 538.6119995117188\n",
      "Epoch [31/500], Training Loss: 365.94579257325154, Validation Loss: 481.91864013671875\n",
      "Epoch [32/500], Training Loss: 321.3204058648503, Validation Loss: 412.8558044433594\n",
      "Epoch [33/500], Training Loss: 282.0964488880113, Validation Loss: 360.31927490234375\n",
      "Epoch [34/500], Training Loss: 247.663465690396, Validation Loss: 315.44317626953125\n",
      "Epoch [35/500], Training Loss: 217.85080904625164, Validation Loss: 277.5035095214844\n",
      "Epoch [36/500], Training Loss: 191.94396770760446, Validation Loss: 246.20899963378906\n",
      "Epoch [37/500], Training Loss: 169.40081757281072, Validation Loss: 224.70059204101562\n",
      "Epoch [38/500], Training Loss: 149.7617637265798, Validation Loss: 209.21224975585938\n",
      "Epoch [39/500], Training Loss: 132.65824916951388, Validation Loss: 186.14439392089844\n",
      "Epoch [40/500], Training Loss: 117.7625395315299, Validation Loss: 159.5968780517578\n",
      "Epoch [41/500], Training Loss: 104.28627352777033, Validation Loss: 138.89500427246094\n",
      "Epoch [42/500], Training Loss: 91.84387181566852, Validation Loss: 124.08869934082031\n",
      "Epoch [43/500], Training Loss: 80.95717941221409, Validation Loss: 112.14521026611328\n",
      "Epoch [44/500], Training Loss: 71.59625859650227, Validation Loss: 99.15866088867188\n",
      "Epoch [45/500], Training Loss: 63.693688986880666, Validation Loss: 86.81018829345703\n",
      "Epoch [46/500], Training Loss: 56.94077678087241, Validation Loss: 77.08137512207031\n",
      "Epoch [47/500], Training Loss: 51.15990965015846, Validation Loss: 70.38822937011719\n",
      "Epoch [48/500], Training Loss: 46.29587059861987, Validation Loss: 70.96869659423828\n",
      "Epoch [49/500], Training Loss: 42.11677938829387, Validation Loss: 75.51758575439453\n",
      "Epoch [50/500], Training Loss: 38.35660361486213, Validation Loss: 64.85977172851562\n",
      "Epoch [51/500], Training Loss: 34.86159803625702, Validation Loss: 54.565494537353516\n",
      "Epoch [52/500], Training Loss: 31.70640508311334, Validation Loss: 48.43714904785156\n",
      "Epoch [53/500], Training Loss: 28.915873563760588, Validation Loss: 45.82322692871094\n",
      "Epoch [54/500], Training Loss: 26.48943994129029, Validation Loss: 45.88468551635742\n",
      "Epoch [55/500], Training Loss: 24.40019475829858, Validation Loss: 46.057708740234375\n",
      "Epoch [56/500], Training Loss: 22.580642207906482, Validation Loss: 43.9905891418457\n",
      "Epoch [57/500], Training Loss: 20.94728837387844, Validation Loss: 40.79071807861328\n",
      "Epoch [58/500], Training Loss: 19.470456011362973, Validation Loss: 37.898468017578125\n",
      "Epoch [59/500], Training Loss: 18.15914354825083, Validation Loss: 35.083316802978516\n",
      "Epoch [60/500], Training Loss: 17.003191788851062, Validation Loss: 32.21046447753906\n",
      "Epoch [61/500], Training Loss: 15.978163364080576, Validation Loss: 29.456655502319336\n",
      "Epoch [62/500], Training Loss: 15.064556857010068, Validation Loss: 26.965240478515625\n",
      "Epoch [63/500], Training Loss: 14.249535468143407, Validation Loss: 24.80801773071289\n",
      "Epoch [64/500], Training Loss: 13.52281852172881, Validation Loss: 23.027400970458984\n",
      "Epoch [65/500], Training Loss: 12.875207702846351, Validation Loss: 21.655736923217773\n",
      "Epoch [66/500], Training Loss: 12.299082261690677, Validation Loss: 20.714881896972656\n",
      "Epoch [67/500], Training Loss: 11.788563892404808, Validation Loss: 20.24496841430664\n",
      "Epoch [68/500], Training Loss: 11.339492274853725, Validation Loss: 20.36859893798828\n",
      "Epoch [69/500], Training Loss: 10.94779581261016, Validation Loss: 21.389917373657227\n",
      "Epoch [70/500], Training Loss: 10.604950548433512, Validation Loss: 23.831314086914062\n",
      "Epoch [71/500], Training Loss: 10.283242572698503, Validation Loss: 27.49867057800293\n",
      "Epoch [72/500], Training Loss: 9.93511217014446, Validation Loss: 29.159854888916016\n",
      "Epoch [73/500], Training Loss: 9.557573768193937, Validation Loss: 28.36147689819336\n",
      "Epoch [74/500], Training Loss: 9.165189381165083, Validation Loss: 27.107952117919922\n",
      "Epoch [75/500], Training Loss: 8.776326400854298, Validation Loss: 25.776233673095703\n",
      "Epoch [76/500], Training Loss: 8.422139549734943, Validation Loss: 25.096158981323242\n",
      "Epoch [77/500], Training Loss: 8.10780916598809, Validation Loss: 25.373817443847656\n",
      "Early stopping! Validation loss has not improved in the last 10 epochs.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with hyperparameters: input_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, hidden_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_layers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, learning_rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, window_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m model \u001b[38;5;241m=\u001b[39m train_lstm_model(input_size, hidden_size, num_layers, learning_rate, window_size, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor)\n\u001b[1;32m---> 94\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_loss \u001b[38;5;241m<\u001b[39m best_test_loss:\n",
      "Cell \u001b[1;32mIn[3], line 82\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, x_test_tensor, y_test_tensor, window_size)\u001b[0m\n\u001b[0;32m     79\u001b[0m         labels \u001b[38;5;241m=\u001b[39m y_test_tensor[window_end \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     81\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 82\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m(outputs, labels)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(x_test_tensor)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'criterion' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "\n",
    "hidden_sizes = [8]\n",
    "num_layers_list = [2]\n",
    "learning_rates = [0.001]\n",
    "window_sizes = [1]\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Function to create and train LSTM model with given hyperparameters\n",
    "def train_lstm_model(input_size, hidden_size, num_layers, learning_rate, window_size, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor):\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement_counter = 0\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "\n",
    "        if no_improvement_counter >= patience:\n",
    "            print(\"Early stopping! Validation loss has not improved in the last {} epochs.\".format(patience))\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model on test data\n",
    "def evaluate_model(model, x_test_tensor, y_test_tensor, window_size):\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    return test_loss / len(x_test_tensor)\n",
    "\n",
    "best_hyperparameters = None\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "# Iterate over hyperparameter combinations and train models\n",
    "for input_size, hidden_size, num_layers, learning_rate, window_size in hyperparameter_combinations:\n",
    "    print(f\"Training with hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    model = train_lstm_model(input_size, hidden_size, num_layers, learning_rate, window_size, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor)\n",
    "    test_loss = evaluate_model(model, x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        best_hyperparameters = (input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"input_size={best_hyperparameters[0]}, hidden_size={best_hyperparameters[1]}, num_layers={best_hyperparameters[2]}, learning_rate={best_hyperparameters[3]}, window_size={best_hyperparameters[4]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
