{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters: input_size=7, hidden_size=5, num_layers=2, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
    "\n",
    "Epoch 25501/150000, Train Loss: 87, Val Loss: 397,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
    "\n",
    "Test Loss: 386.94378662109375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    # Initialize lists to store training and temporary sets\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    # Iterate through the data with the specified window size\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i+split_window_size+1]\n",
    "        y_out1_out2 = y_in.iloc[i:i+split_window_size+1]\n",
    "\n",
    "        # Separate the last row for the temporary set\n",
    "        # [ :-1]: all elements except the last one\n",
    "        # [-1:]:  selects only the last element\n",
    "        # (:) is used to indicate slicing of a sequence\n",
    "        # sequence[start : end : step]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    # Concatenate the lists into pandas DataFrames\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "# Split Data to train and temp\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 3\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split temp into val and test\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 1\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob=0.5):  # Added dropout_prob\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)  # Included dropout in LSTM\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Added dropout layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=5, num_layers=2, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150000, Train Loss: 30126, Val Loss: 30623,  Lear. Rate: 0.00500, Train Grad.: 225.5\n",
      "Epoch 101/150000, Train Loss: 29390, Val Loss: 29876,  Lear. Rate: 0.00453, Train Grad.: 218.9\n",
      "Epoch 201/150000, Train Loss: 28651, Val Loss: 29128,  Lear. Rate: 0.00455, Train Grad.: 212.0\n",
      "Epoch 301/150000, Train Loss: 28002, Val Loss: 28472,  Lear. Rate: 0.00457, Train Grad.: 205.8\n",
      "Epoch 401/150000, Train Loss: 27415, Val Loss: 27878,  Lear. Rate: 0.00459, Train Grad.: 200.0\n",
      "Epoch 501/150000, Train Loss: 26865, Val Loss: 27322,  Lear. Rate: 0.00460, Train Grad.: 194.4\n",
      "Epoch 601/150000, Train Loss: 26344, Val Loss: 26794,  Lear. Rate: 0.00462, Train Grad.: 189.0\n",
      "Epoch 701/150000, Train Loss: 25846, Val Loss: 26290,  Lear. Rate: 0.00463, Train Grad.: 183.7\n",
      "Epoch 801/150000, Train Loss: 25371, Val Loss: 25808,  Lear. Rate: 0.00464, Train Grad.: 178.4\n",
      "Epoch 901/150000, Train Loss: 24916, Val Loss: 25347,  Lear. Rate: 0.00466, Train Grad.: 173.2\n",
      "Epoch 1001/150000, Train Loss: 24481, Val Loss: 24906,  Lear. Rate: 0.00467, Train Grad.: 168.1\n",
      "Epoch 1101/150000, Train Loss: 24064, Val Loss: 24483,  Lear. Rate: 0.00468, Train Grad.: 163.1\n",
      "Epoch 1201/150000, Train Loss: 23665, Val Loss: 24077,  Lear. Rate: 0.00469, Train Grad.: 158.1\n",
      "Epoch 1301/150000, Train Loss: 23282, Val Loss: 23689,  Lear. Rate: 0.00470, Train Grad.: 153.2\n",
      "Epoch 1401/150000, Train Loss: 22917, Val Loss: 23317,  Lear. Rate: 0.00471, Train Grad.: 148.4\n",
      "Epoch 1501/150000, Train Loss: 22567, Val Loss: 22962,  Lear. Rate: 0.00472, Train Grad.: 143.6\n",
      "Epoch 1601/150000, Train Loss: 22233, Val Loss: 22622,  Lear. Rate: 0.00472, Train Grad.: 138.8\n",
      "Epoch 1701/150000, Train Loss: 21914, Val Loss: 22297,  Lear. Rate: 0.00473, Train Grad.: 134.2\n",
      "Epoch 1801/150000, Train Loss: 21609, Val Loss: 21987,  Lear. Rate: 0.00474, Train Grad.: 129.5\n",
      "Epoch 1901/150000, Train Loss: 21319, Val Loss: 21691,  Lear. Rate: 0.00475, Train Grad.: 125.0\n",
      "Epoch 2001/150000, Train Loss: 21043, Val Loss: 21409,  Lear. Rate: 0.00475, Train Grad.: 120.5\n",
      "Epoch 2101/150000, Train Loss: 20780, Val Loss: 21141,  Lear. Rate: 0.00476, Train Grad.: 116.0\n",
      "Epoch 2201/150000, Train Loss: 20531, Val Loss: 20886,  Lear. Rate: 0.00476, Train Grad.: 111.7\n",
      "Epoch 2301/150000, Train Loss: 20294, Val Loss: 20644,  Lear. Rate: 0.00477, Train Grad.: 107.3\n",
      "Epoch 2401/150000, Train Loss: 20070, Val Loss: 20415,  Lear. Rate: 0.00477, Train Grad.: 103.1\n",
      "Epoch 2501/150000, Train Loss: 19858, Val Loss: 20198,  Lear. Rate: 0.00478, Train Grad.: 98.9\n",
      "Epoch 2601/150000, Train Loss: 19658, Val Loss: 19993,  Lear. Rate: 0.00478, Train Grad.: 94.7\n",
      "Epoch 2701/150000, Train Loss: 19469, Val Loss: 19799,  Lear. Rate: 0.00479, Train Grad.: 90.7\n",
      "Epoch 2801/150000, Train Loss: 19291, Val Loss: 19616,  Lear. Rate: 0.00479, Train Grad.: 86.7\n",
      "Epoch 2901/150000, Train Loss: 19125, Val Loss: 19445,  Lear. Rate: 0.00480, Train Grad.: 82.7\n",
      "Epoch 3001/150000, Train Loss: 18968, Val Loss: 19284,  Lear. Rate: 0.00480, Train Grad.: 78.9\n",
      "Epoch 3101/150000, Train Loss: 18822, Val Loss: 19133,  Lear. Rate: 0.00480, Train Grad.: 75.1\n",
      "Epoch 3201/150000, Train Loss: 18686, Val Loss: 18992,  Lear. Rate: 0.00480, Train Grad.: 71.3\n",
      "Epoch 3301/150000, Train Loss: 18559, Val Loss: 18860,  Lear. Rate: 0.00481, Train Grad.: 67.7\n",
      "Epoch 3401/150000, Train Loss: 18441, Val Loss: 18738,  Lear. Rate: 0.00481, Train Grad.: 64.1\n",
      "Epoch 3501/150000, Train Loss: 18332, Val Loss: 18625,  Lear. Rate: 0.00481, Train Grad.: 60.6\n",
      "Epoch 3601/150000, Train Loss: 16642, Val Loss: 17025,  Lear. Rate: 0.00484, Train Grad.: 113.8\n",
      "Epoch 3701/150000, Train Loss: 16278, Val Loss: 16654,  Lear. Rate: 0.00485, Train Grad.: 110.9\n",
      "Epoch 3801/150000, Train Loss: 15934, Val Loss: 16302,  Lear. Rate: 0.00486, Train Grad.: 108.4\n",
      "Epoch 3901/150000, Train Loss: 15608, Val Loss: 15966,  Lear. Rate: 0.00486, Train Grad.: 106.0\n",
      "Epoch 4001/150000, Train Loss: 15295, Val Loss: 15644,  Lear. Rate: 0.00487, Train Grad.: 103.7\n",
      "Epoch 4101/150000, Train Loss: 14996, Val Loss: 15335,  Lear. Rate: 0.00487, Train Grad.: 101.5\n",
      "Epoch 4201/150000, Train Loss: 14708, Val Loss: 15038,  Lear. Rate: 0.00488, Train Grad.: 99.4\n",
      "Epoch 4301/150000, Train Loss: 14431, Val Loss: 14752,  Lear. Rate: 0.00488, Train Grad.: 97.5\n",
      "Epoch 4401/150000, Train Loss: 14163, Val Loss: 14479,  Lear. Rate: 0.00489, Train Grad.: 95.6\n",
      "Epoch 4501/150000, Train Loss: 13904, Val Loss: 14216,  Lear. Rate: 0.00489, Train Grad.: 93.7\n",
      "Epoch 4601/150000, Train Loss: 13654, Val Loss: 13961,  Lear. Rate: 0.00489, Train Grad.: 91.9\n",
      "Epoch 4701/150000, Train Loss: 13410, Val Loss: 13711,  Lear. Rate: 0.00490, Train Grad.: 90.1\n",
      "Epoch 4801/150000, Train Loss: 13173, Val Loss: 13469,  Lear. Rate: 0.00490, Train Grad.: 88.4\n",
      "Epoch 4901/150000, Train Loss: 12943, Val Loss: 13235,  Lear. Rate: 0.00490, Train Grad.: 86.7\n",
      "Epoch 5001/150000, Train Loss: 12719, Val Loss: 13007,  Lear. Rate: 0.00491, Train Grad.: 85.1\n",
      "Epoch 5101/150000, Train Loss: 12500, Val Loss: 12786,  Lear. Rate: 0.00491, Train Grad.: 83.6\n",
      "Epoch 5201/150000, Train Loss: 12286, Val Loss: 12569,  Lear. Rate: 0.00491, Train Grad.: 82.1\n",
      "Epoch 5301/150000, Train Loss: 12078, Val Loss: 12356,  Lear. Rate: 0.00492, Train Grad.: 80.7\n",
      "Epoch 5401/150000, Train Loss: 11873, Val Loss: 12148,  Lear. Rate: 0.00492, Train Grad.: 79.3\n",
      "Epoch 5501/150000, Train Loss: 11673, Val Loss: 11944,  Lear. Rate: 0.00492, Train Grad.: 77.8\n",
      "Epoch 5601/150000, Train Loss: 11477, Val Loss: 11744,  Lear. Rate: 0.00492, Train Grad.: 76.6\n",
      "Epoch 5701/150000, Train Loss: 11285, Val Loss: 11548,  Lear. Rate: 0.00493, Train Grad.: 75.5\n",
      "Epoch 5801/150000, Train Loss: 11096, Val Loss: 11356,  Lear. Rate: 0.00493, Train Grad.: 74.0\n",
      "Epoch 5901/150000, Train Loss: 10911, Val Loss: 11168,  Lear. Rate: 0.00493, Train Grad.: 72.8\n",
      "Epoch 6001/150000, Train Loss: 10730, Val Loss: 10984,  Lear. Rate: 0.00493, Train Grad.: 71.6\n",
      "Epoch 6101/150000, Train Loss: 10552, Val Loss: 10803,  Lear. Rate: 0.00494, Train Grad.: 70.4\n",
      "Epoch 6201/150000, Train Loss: 10377, Val Loss: 10625,  Lear. Rate: 0.00494, Train Grad.: 69.4\n",
      "Epoch 6301/150000, Train Loss: 10205, Val Loss: 10450,  Lear. Rate: 0.00494, Train Grad.: 68.2\n",
      "Epoch 6401/150000, Train Loss: 10036, Val Loss: 10279,  Lear. Rate: 0.00494, Train Grad.: 66.9\n",
      "Epoch 6501/150000, Train Loss: 9870, Val Loss: 10109,  Lear. Rate: 0.00494, Train Grad.: 66.1\n",
      "Epoch 6601/150000, Train Loss: 9706, Val Loss: 9943,  Lear. Rate: 0.00495, Train Grad.: 65.1\n",
      "Epoch 6701/150000, Train Loss: 9545, Val Loss: 9780,  Lear. Rate: 0.00495, Train Grad.: 64.1\n",
      "Epoch 6801/150000, Train Loss: 9387, Val Loss: 9619,  Lear. Rate: 0.00495, Train Grad.: 63.1\n",
      "Epoch 6901/150000, Train Loss: 9231, Val Loss: 9460,  Lear. Rate: 0.00495, Train Grad.: 62.3\n",
      "Epoch 7001/150000, Train Loss: 9077, Val Loss: 9304,  Lear. Rate: 0.00495, Train Grad.: 61.2\n",
      "Epoch 7101/150000, Train Loss: 8925, Val Loss: 9150,  Lear. Rate: 0.00495, Train Grad.: 60.4\n",
      "Epoch 7201/150000, Train Loss: 8775, Val Loss: 8997,  Lear. Rate: 0.00496, Train Grad.: 59.7\n",
      "Epoch 7301/150000, Train Loss: 8627, Val Loss: 8846,  Lear. Rate: 0.00496, Train Grad.: 58.7\n",
      "Epoch 7401/150000, Train Loss: 8482, Val Loss: 8698,  Lear. Rate: 0.00496, Train Grad.: 56.8\n",
      "Epoch 7501/150000, Train Loss: 8337, Val Loss: 8550,  Lear. Rate: 0.00496, Train Grad.: 57.1\n",
      "Epoch 7601/150000, Train Loss: 8195, Val Loss: 8406,  Lear. Rate: 0.00496, Train Grad.: 56.3\n",
      "Epoch 7701/150000, Train Loss: 8054, Val Loss: 8263,  Lear. Rate: 0.00496, Train Grad.: 55.6\n",
      "Epoch 7801/150000, Train Loss: 7915, Val Loss: 8121,  Lear. Rate: 0.00496, Train Grad.: 54.8\n",
      "Epoch 7901/150000, Train Loss: 7778, Val Loss: 7981,  Lear. Rate: 0.00497, Train Grad.: 54.1\n",
      "Epoch 8001/150000, Train Loss: 7642, Val Loss: 7843,  Lear. Rate: 0.00497, Train Grad.: 53.4\n",
      "Epoch 8101/150000, Train Loss: 7508, Val Loss: 7708,  Lear. Rate: 0.00497, Train Grad.: 52.2\n",
      "Epoch 8201/150000, Train Loss: 7376, Val Loss: 7573,  Lear. Rate: 0.00497, Train Grad.: 52.0\n",
      "Epoch 8301/150000, Train Loss: 7245, Val Loss: 7441,  Lear. Rate: 0.00497, Train Grad.: 51.3\n",
      "Epoch 8401/150000, Train Loss: 7116, Val Loss: 7310,  Lear. Rate: 0.00497, Train Grad.: 50.7\n",
      "Epoch 8501/150000, Train Loss: 6989, Val Loss: 7181,  Lear. Rate: 0.00497, Train Grad.: 50.0\n",
      "Epoch 8601/150000, Train Loss: 6863, Val Loss: 7054,  Lear. Rate: 0.00497, Train Grad.: 49.5\n",
      "Epoch 8701/150000, Train Loss: 6738, Val Loss: 6928,  Lear. Rate: 0.00497, Train Grad.: 48.7\n",
      "Epoch 8801/150000, Train Loss: 6617, Val Loss: 6804,  Lear. Rate: 0.00497, Train Grad.: 50.3\n",
      "Epoch 8901/150000, Train Loss: 6494, Val Loss: 6681,  Lear. Rate: 0.00498, Train Grad.: 47.4\n",
      "Epoch 9001/150000, Train Loss: 6374, Val Loss: 6560,  Lear. Rate: 0.00498, Train Grad.: 46.8\n",
      "Epoch 9101/150000, Train Loss: 6255, Val Loss: 6440,  Lear. Rate: 0.00498, Train Grad.: 46.0\n",
      "Epoch 9201/150000, Train Loss: 6139, Val Loss: 6322,  Lear. Rate: 0.00498, Train Grad.: 45.5\n",
      "Epoch 9301/150000, Train Loss: 6023, Val Loss: 6205,  Lear. Rate: 0.00498, Train Grad.: 44.8\n",
      "Epoch 9401/150000, Train Loss: 5909, Val Loss: 6090,  Lear. Rate: 0.00498, Train Grad.: 44.3\n",
      "Epoch 9501/150000, Train Loss: 5797, Val Loss: 5977,  Lear. Rate: 0.00498, Train Grad.: 43.7\n",
      "Epoch 9601/150000, Train Loss: 5686, Val Loss: 5865,  Lear. Rate: 0.00498, Train Grad.: 43.8\n",
      "Epoch 9701/150000, Train Loss: 5577, Val Loss: 5755,  Lear. Rate: 0.00498, Train Grad.: 42.5\n",
      "Epoch 9801/150000, Train Loss: 5469, Val Loss: 5646,  Lear. Rate: 0.00498, Train Grad.: 41.9\n",
      "Epoch 9901/150000, Train Loss: 5363, Val Loss: 5539,  Lear. Rate: 0.00498, Train Grad.: 41.3\n",
      "Epoch 10001/150000, Train Loss: 5259, Val Loss: 5433,  Lear. Rate: 0.00498, Train Grad.: 40.7\n",
      "Epoch 10101/150000, Train Loss: 5156, Val Loss: 5330,  Lear. Rate: 0.00498, Train Grad.: 40.1\n",
      "Epoch 10201/150000, Train Loss: 5055, Val Loss: 5227,  Lear. Rate: 0.00499, Train Grad.: 39.5\n",
      "Epoch 10301/150000, Train Loss: 4955, Val Loss: 5126,  Lear. Rate: 0.00499, Train Grad.: 39.1\n",
      "Epoch 10401/150000, Train Loss: 4856, Val Loss: 5027,  Lear. Rate: 0.00499, Train Grad.: 38.4\n",
      "Epoch 10501/150000, Train Loss: 4759, Val Loss: 4930,  Lear. Rate: 0.00499, Train Grad.: 37.8\n",
      "Epoch 10601/150000, Train Loss: 4663, Val Loss: 4833,  Lear. Rate: 0.00499, Train Grad.: 37.4\n",
      "Epoch 10701/150000, Train Loss: 4569, Val Loss: 4739,  Lear. Rate: 0.00499, Train Grad.: 36.7\n",
      "Epoch 10801/150000, Train Loss: 4476, Val Loss: 4645,  Lear. Rate: 0.00499, Train Grad.: 36.3\n",
      "Epoch 10901/150000, Train Loss: 4384, Val Loss: 4553,  Lear. Rate: 0.00499, Train Grad.: 35.6\n",
      "Epoch 11001/150000, Train Loss: 4294, Val Loss: 4463,  Lear. Rate: 0.00499, Train Grad.: 35.3\n",
      "Epoch 11101/150000, Train Loss: 4205, Val Loss: 4373,  Lear. Rate: 0.00499, Train Grad.: 34.9\n",
      "Epoch 11201/150000, Train Loss: 4117, Val Loss: 4284,  Lear. Rate: 0.00499, Train Grad.: 34.3\n",
      "Epoch 11301/150000, Train Loss: 4030, Val Loss: 4197,  Lear. Rate: 0.00499, Train Grad.: 33.8\n",
      "Epoch 11401/150000, Train Loss: 3945, Val Loss: 4112,  Lear. Rate: 0.00499, Train Grad.: 33.3\n",
      "Epoch 11501/150000, Train Loss: 3862, Val Loss: 4028,  Lear. Rate: 0.00499, Train Grad.: 32.8\n",
      "Epoch 11601/150000, Train Loss: 3779, Val Loss: 3945,  Lear. Rate: 0.00499, Train Grad.: 32.6\n",
      "Epoch 11701/150000, Train Loss: 3698, Val Loss: 3865,  Lear. Rate: 0.00499, Train Grad.: 31.7\n",
      "Epoch 11801/150000, Train Loss: 3619, Val Loss: 3785,  Lear. Rate: 0.00499, Train Grad.: 31.3\n",
      "Epoch 11901/150000, Train Loss: 3540, Val Loss: 3707,  Lear. Rate: 0.00499, Train Grad.: 30.9\n",
      "Epoch 12001/150000, Train Loss: 3463, Val Loss: 3630,  Lear. Rate: 0.00499, Train Grad.: 30.3\n",
      "Epoch 12101/150000, Train Loss: 3388, Val Loss: 3555,  Lear. Rate: 0.00499, Train Grad.: 29.4\n",
      "Epoch 12201/150000, Train Loss: 3313, Val Loss: 3481,  Lear. Rate: 0.00499, Train Grad.: 29.3\n",
      "Epoch 12301/150000, Train Loss: 3241, Val Loss: 3408,  Lear. Rate: 0.00499, Train Grad.: 29.2\n",
      "Epoch 12401/150000, Train Loss: 3169, Val Loss: 3336,  Lear. Rate: 0.00499, Train Grad.: 28.3\n",
      "Epoch 12501/150000, Train Loss: 3099, Val Loss: 3265,  Lear. Rate: 0.00499, Train Grad.: 28.2\n",
      "Epoch 12601/150000, Train Loss: 3029, Val Loss: 3195,  Lear. Rate: 0.00499, Train Grad.: 27.4\n",
      "Epoch 12701/150000, Train Loss: 2961, Val Loss: 3126,  Lear. Rate: 0.00499, Train Grad.: 27.5\n",
      "Epoch 12801/150000, Train Loss: 2894, Val Loss: 3059,  Lear. Rate: 0.00499, Train Grad.: 26.6\n",
      "Epoch 12901/150000, Train Loss: 2829, Val Loss: 2994,  Lear. Rate: 0.00500, Train Grad.: 25.8\n",
      "Epoch 13001/150000, Train Loss: 2764, Val Loss: 2929,  Lear. Rate: 0.00500, Train Grad.: 25.7\n",
      "Epoch 13101/150000, Train Loss: 2701, Val Loss: 2866,  Lear. Rate: 0.00500, Train Grad.: 25.2\n",
      "Epoch 13201/150000, Train Loss: 2639, Val Loss: 2804,  Lear. Rate: 0.00500, Train Grad.: 24.8\n",
      "Epoch 13301/150000, Train Loss: 2578, Val Loss: 2743,  Lear. Rate: 0.00500, Train Grad.: 24.4\n",
      "Epoch 13401/150000, Train Loss: 2518, Val Loss: 2684,  Lear. Rate: 0.00500, Train Grad.: 22.9\n",
      "Epoch 13501/150000, Train Loss: 2459, Val Loss: 2625,  Lear. Rate: 0.00500, Train Grad.: 23.6\n",
      "Epoch 13601/150000, Train Loss: 2402, Val Loss: 2567,  Lear. Rate: 0.00500, Train Grad.: 24.6\n",
      "Epoch 13701/150000, Train Loss: 2344, Val Loss: 2511,  Lear. Rate: 0.00500, Train Grad.: 22.8\n",
      "Epoch 13801/150000, Train Loss: 2289, Val Loss: 2456,  Lear. Rate: 0.00500, Train Grad.: 23.0\n",
      "Epoch 13901/150000, Train Loss: 2234, Val Loss: 2402,  Lear. Rate: 0.00500, Train Grad.: 22.0\n",
      "Epoch 14001/150000, Train Loss: 2180, Val Loss: 2349,  Lear. Rate: 0.00500, Train Grad.: 21.6\n",
      "Epoch 14101/150000, Train Loss: 2127, Val Loss: 2296,  Lear. Rate: 0.00500, Train Grad.: 21.2\n",
      "Epoch 14201/150000, Train Loss: 2075, Val Loss: 2245,  Lear. Rate: 0.00500, Train Grad.: 20.9\n",
      "Epoch 14301/150000, Train Loss: 2024, Val Loss: 2194,  Lear. Rate: 0.00500, Train Grad.: 20.7\n",
      "Epoch 14401/150000, Train Loss: 1974, Val Loss: 2145,  Lear. Rate: 0.00500, Train Grad.: 20.2\n",
      "Epoch 14501/150000, Train Loss: 1925, Val Loss: 2097,  Lear. Rate: 0.00500, Train Grad.: 19.6\n",
      "Epoch 14601/150000, Train Loss: 1877, Val Loss: 2049,  Lear. Rate: 0.00500, Train Grad.: 19.5\n",
      "Epoch 14701/150000, Train Loss: 1830, Val Loss: 2002,  Lear. Rate: 0.00500, Train Grad.: 19.1\n",
      "Epoch 14801/150000, Train Loss: 1784, Val Loss: 1956,  Lear. Rate: 0.00500, Train Grad.: 18.7\n",
      "Epoch 14901/150000, Train Loss: 1738, Val Loss: 1911,  Lear. Rate: 0.00500, Train Grad.: 18.4\n",
      "Epoch 15001/150000, Train Loss: 1694, Val Loss: 1868,  Lear. Rate: 0.00500, Train Grad.: 17.7\n",
      "Epoch 15101/150000, Train Loss: 1650, Val Loss: 1825,  Lear. Rate: 0.00500, Train Grad.: 17.7\n",
      "Epoch 15201/150000, Train Loss: 1608, Val Loss: 1783,  Lear. Rate: 0.00500, Train Grad.: 17.4\n",
      "Epoch 15301/150000, Train Loss: 1566, Val Loss: 1742,  Lear. Rate: 0.00500, Train Grad.: 17.0\n",
      "Epoch 15401/150000, Train Loss: 1525, Val Loss: 1702,  Lear. Rate: 0.00500, Train Grad.: 16.7\n",
      "Epoch 15501/150000, Train Loss: 1485, Val Loss: 1662,  Lear. Rate: 0.00500, Train Grad.: 16.7\n",
      "Epoch 15601/150000, Train Loss: 1446, Val Loss: 1625,  Lear. Rate: 0.00500, Train Grad.: 16.1\n",
      "Epoch 15701/150000, Train Loss: 1408, Val Loss: 1587,  Lear. Rate: 0.00500, Train Grad.: 15.8\n",
      "Epoch 15801/150000, Train Loss: 1370, Val Loss: 1551,  Lear. Rate: 0.00500, Train Grad.: 15.5\n",
      "Epoch 15901/150000, Train Loss: 1333, Val Loss: 1516,  Lear. Rate: 0.00500, Train Grad.: 15.2\n",
      "Epoch 16001/150000, Train Loss: 1297, Val Loss: 1480,  Lear. Rate: 0.00500, Train Grad.: 14.9\n",
      "Epoch 16101/150000, Train Loss: 1262, Val Loss: 1446,  Lear. Rate: 0.00500, Train Grad.: 14.6\n",
      "Epoch 16201/150000, Train Loss: 1227, Val Loss: 1413,  Lear. Rate: 0.00500, Train Grad.: 14.3\n",
      "Epoch 16301/150000, Train Loss: 1193, Val Loss: 1380,  Lear. Rate: 0.00500, Train Grad.: 14.1\n",
      "Epoch 16401/150000, Train Loss: 1160, Val Loss: 1348,  Lear. Rate: 0.00500, Train Grad.: 13.8\n",
      "Epoch 16501/150000, Train Loss: 1127, Val Loss: 1317,  Lear. Rate: 0.00500, Train Grad.: 13.3\n",
      "Epoch 16601/150000, Train Loss: 1096, Val Loss: 1286,  Lear. Rate: 0.00500, Train Grad.: 13.2\n",
      "Epoch 16701/150000, Train Loss: 1065, Val Loss: 1257,  Lear. Rate: 0.00500, Train Grad.: 12.7\n",
      "Epoch 16801/150000, Train Loss: 1034, Val Loss: 1228,  Lear. Rate: 0.00500, Train Grad.: 12.7\n",
      "Epoch 16901/150000, Train Loss: 1005, Val Loss: 1200,  Lear. Rate: 0.00500, Train Grad.: 12.4\n",
      "Epoch 17001/150000, Train Loss: 976, Val Loss: 1172,  Lear. Rate: 0.00500, Train Grad.: 12.1\n",
      "Epoch 17101/150000, Train Loss: 948, Val Loss: 1146,  Lear. Rate: 0.00500, Train Grad.: 12.2\n",
      "Epoch 17201/150000, Train Loss: 920, Val Loss: 1121,  Lear. Rate: 0.00500, Train Grad.: 11.6\n",
      "Epoch 17301/150000, Train Loss: 893, Val Loss: 1096,  Lear. Rate: 0.00500, Train Grad.: 11.1\n",
      "Epoch 17401/150000, Train Loss: 867, Val Loss: 1071,  Lear. Rate: 0.00500, Train Grad.: 11.1\n",
      "Epoch 17501/150000, Train Loss: 842, Val Loss: 1047,  Lear. Rate: 0.00500, Train Grad.: 12.5\n",
      "Epoch 17601/150000, Train Loss: 816, Val Loss: 1024,  Lear. Rate: 0.00500, Train Grad.: 10.6\n",
      "Epoch 17701/150000, Train Loss: 792, Val Loss: 1001,  Lear. Rate: 0.00500, Train Grad.: 10.4\n",
      "Epoch 17801/150000, Train Loss: 768, Val Loss: 979,  Lear. Rate: 0.00500, Train Grad.: 10.1\n",
      "Epoch 17901/150000, Train Loss: 745, Val Loss: 958,  Lear. Rate: 0.00500, Train Grad.: 9.9\n",
      "Epoch 18001/150000, Train Loss: 722, Val Loss: 938,  Lear. Rate: 0.00500, Train Grad.: 9.5\n",
      "Epoch 18101/150000, Train Loss: 700, Val Loss: 918,  Lear. Rate: 0.00500, Train Grad.: 9.5\n",
      "Epoch 18201/150000, Train Loss: 679, Val Loss: 898,  Lear. Rate: 0.00500, Train Grad.: 9.5\n",
      "Epoch 18301/150000, Train Loss: 658, Val Loss: 879,  Lear. Rate: 0.00500, Train Grad.: 9.0\n",
      "Epoch 18401/150000, Train Loss: 637, Val Loss: 861,  Lear. Rate: 0.00500, Train Grad.: 9.1\n",
      "Epoch 18501/150000, Train Loss: 617, Val Loss: 843,  Lear. Rate: 0.00500, Train Grad.: 8.6\n",
      "Epoch 18601/150000, Train Loss: 598, Val Loss: 826,  Lear. Rate: 0.00500, Train Grad.: 8.8\n",
      "Epoch 18701/150000, Train Loss: 579, Val Loss: 809,  Lear. Rate: 0.00500, Train Grad.: 8.2\n",
      "Epoch 18801/150000, Train Loss: 563, Val Loss: 792,  Lear. Rate: 0.00500, Train Grad.: 10.4\n",
      "Epoch 18901/150000, Train Loss: 543, Val Loss: 778,  Lear. Rate: 0.00500, Train Grad.: 7.8\n",
      "Epoch 19001/150000, Train Loss: 526, Val Loss: 761,  Lear. Rate: 0.00500, Train Grad.: 8.0\n",
      "Epoch 19101/150000, Train Loss: 509, Val Loss: 746,  Lear. Rate: 0.00500, Train Grad.: 7.4\n",
      "Epoch 19201/150000, Train Loss: 493, Val Loss: 731,  Lear. Rate: 0.00500, Train Grad.: 7.1\n",
      "Epoch 19301/150000, Train Loss: 478, Val Loss: 719,  Lear. Rate: 0.00500, Train Grad.: 5.2\n",
      "Epoch 19401/150000, Train Loss: 461, Val Loss: 702,  Lear. Rate: 0.00500, Train Grad.: 6.8\n",
      "Epoch 19501/150000, Train Loss: 446, Val Loss: 689,  Lear. Rate: 0.00500, Train Grad.: 6.6\n",
      "Epoch 19601/150000, Train Loss: 432, Val Loss: 678,  Lear. Rate: 0.00500, Train Grad.: 5.9\n",
      "Epoch 19701/150000, Train Loss: 418, Val Loss: 664,  Lear. Rate: 0.00500, Train Grad.: 6.3\n",
      "Epoch 19801/150000, Train Loss: 404, Val Loss: 652,  Lear. Rate: 0.00500, Train Grad.: 6.1\n",
      "Epoch 19901/150000, Train Loss: 391, Val Loss: 640,  Lear. Rate: 0.00500, Train Grad.: 6.1\n",
      "Epoch 20001/150000, Train Loss: 378, Val Loss: 629,  Lear. Rate: 0.00500, Train Grad.: 5.9\n",
      "Epoch 20101/150000, Train Loss: 366, Val Loss: 618,  Lear. Rate: 0.00500, Train Grad.: 5.8\n",
      "Epoch 20201/150000, Train Loss: 354, Val Loss: 608,  Lear. Rate: 0.00500, Train Grad.: 5.5\n",
      "Epoch 20301/150000, Train Loss: 343, Val Loss: 594,  Lear. Rate: 0.00500, Train Grad.: 7.7\n",
      "Epoch 20401/150000, Train Loss: 329, Val Loss: 582,  Lear. Rate: 0.00500, Train Grad.: 5.3\n",
      "Epoch 20501/150000, Train Loss: 318, Val Loss: 569,  Lear. Rate: 0.00500, Train Grad.: 4.6\n",
      "Epoch 20601/150000, Train Loss: 307, Val Loss: 558,  Lear. Rate: 0.00500, Train Grad.: 5.0\n",
      "Epoch 20701/150000, Train Loss: 297, Val Loss: 547,  Lear. Rate: 0.00500, Train Grad.: 4.9\n",
      "Epoch 20801/150000, Train Loss: 287, Val Loss: 538,  Lear. Rate: 0.00500, Train Grad.: 4.6\n",
      "Epoch 20901/150000, Train Loss: 277, Val Loss: 529,  Lear. Rate: 0.00500, Train Grad.: 4.5\n",
      "Epoch 21001/150000, Train Loss: 268, Val Loss: 522,  Lear. Rate: 0.00500, Train Grad.: 4.3\n",
      "Epoch 21101/150000, Train Loss: 259, Val Loss: 515,  Lear. Rate: 0.00500, Train Grad.: 4.6\n",
      "Epoch 21201/150000, Train Loss: 251, Val Loss: 508,  Lear. Rate: 0.00500, Train Grad.: 4.0\n",
      "Epoch 21301/150000, Train Loss: 242, Val Loss: 503,  Lear. Rate: 0.00500, Train Grad.: 3.9\n",
      "Epoch 21401/150000, Train Loss: 235, Val Loss: 497,  Lear. Rate: 0.00500, Train Grad.: 3.8\n",
      "Epoch 21501/150000, Train Loss: 227, Val Loss: 493,  Lear. Rate: 0.00500, Train Grad.: 3.6\n",
      "Epoch 21601/150000, Train Loss: 220, Val Loss: 489,  Lear. Rate: 0.00500, Train Grad.: 3.5\n",
      "Epoch 21701/150000, Train Loss: 214, Val Loss: 485,  Lear. Rate: 0.00500, Train Grad.: 3.3\n",
      "Epoch 21801/150000, Train Loss: 207, Val Loss: 482,  Lear. Rate: 0.00500, Train Grad.: 2.9\n",
      "Epoch 21901/150000, Train Loss: 201, Val Loss: 478,  Lear. Rate: 0.00500, Train Grad.: 3.1\n",
      "Epoch 22001/150000, Train Loss: 196, Val Loss: 474,  Lear. Rate: 0.00500, Train Grad.: 3.0\n",
      "Epoch 22101/150000, Train Loss: 190, Val Loss: 471,  Lear. Rate: 0.00500, Train Grad.: 1.7\n",
      "Epoch 22201/150000, Train Loss: 185, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 2.8\n",
      "Epoch 22301/150000, Train Loss: 179, Val Loss: 455,  Lear. Rate: 0.00500, Train Grad.: 2.7\n",
      "Epoch 22401/150000, Train Loss: 174, Val Loss: 446,  Lear. Rate: 0.00500, Train Grad.: 2.7\n",
      "Epoch 22501/150000, Train Loss: 169, Val Loss: 442,  Lear. Rate: 0.00500, Train Grad.: 2.6\n",
      "Epoch 22601/150000, Train Loss: 164, Val Loss: 438,  Lear. Rate: 0.00500, Train Grad.: 2.5\n",
      "Epoch 22701/150000, Train Loss: 160, Val Loss: 435,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 22801/150000, Train Loss: 155, Val Loss: 432,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 22901/150000, Train Loss: 151, Val Loss: 430,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 23001/150000, Train Loss: 147, Val Loss: 427,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 23101/150000, Train Loss: 143, Val Loss: 425,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 23201/150000, Train Loss: 140, Val Loss: 423,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 23301/150000, Train Loss: 136, Val Loss: 420,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 23401/150000, Train Loss: 134, Val Loss: 417,  Lear. Rate: 0.00500, Train Grad.: 3.7\n",
      "Epoch 23501/150000, Train Loss: 130, Val Loss: 416,  Lear. Rate: 0.00500, Train Grad.: 1.7\n",
      "Epoch 23601/150000, Train Loss: 127, Val Loss: 413,  Lear. Rate: 0.00500, Train Grad.: 1.7\n",
      "Epoch 23701/150000, Train Loss: 124, Val Loss: 411,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 23801/150000, Train Loss: 121, Val Loss: 409,  Lear. Rate: 0.00500, Train Grad.: 1.6\n",
      "Epoch 23901/150000, Train Loss: 118, Val Loss: 406,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 24001/150000, Train Loss: 115, Val Loss: 406,  Lear. Rate: 0.00500, Train Grad.: 1.5\n",
      "Epoch 24101/150000, Train Loss: 113, Val Loss: 405,  Lear. Rate: 0.00500, Train Grad.: 1.4\n",
      "Epoch 24201/150000, Train Loss: 111, Val Loss: 404,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 24301/150000, Train Loss: 108, Val Loss: 402,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
      "Epoch 24401/150000, Train Loss: 106, Val Loss: 400,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 24501/150000, Train Loss: 104, Val Loss: 400,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 24601/150000, Train Loss: 102, Val Loss: 399,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 24701/150000, Train Loss: 100, Val Loss: 399,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 24801/150000, Train Loss: 98, Val Loss: 398,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 24901/150000, Train Loss: 97, Val Loss: 397,  Lear. Rate: 0.00500, Train Grad.: 0.6\n",
      "Epoch 25001/150000, Train Loss: 95, Val Loss: 397,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 25101/150000, Train Loss: 94, Val Loss: 396,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 25201/150000, Train Loss: 92, Val Loss: 396,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 25301/150000, Train Loss: 90, Val Loss: 397,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 25401/150000, Train Loss: 90, Val Loss: 399,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 25501/150000, Train Loss: 87, Val Loss: 397,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
      "Early stopping at epoch 25536 with validation loss 400.9601135253906.\n",
      "Test Loss: 386.94378662109375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np  # Added this line\n",
    "\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [5]\n",
    "num_layers_list = [2]\n",
    "\n",
    "learning_rates = [0.005]\n",
    "\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "factors = [0.5]\n",
    "patience_lr = [10]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "def set_random_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    set_random_seeds(42)\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Lear. Rate: {current_lr:.5f}, Train Grad.: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
