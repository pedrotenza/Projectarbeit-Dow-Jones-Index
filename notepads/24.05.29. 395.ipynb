{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
    "Early stopping at epoch 32556 with validation loss 457.4826965332031.\n",
    "Test Loss: 395.1988220214844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m split_window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Call the split_data_with_window function\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m x_train, y_train, x_temp, y_temp \u001b[38;5;241m=\u001b[39m \u001b[43msplit_data_with_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_window_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m# Print the last 5 rows of x_data\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Split temp into val and test\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Define your split_window_size\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 45\u001b[0m, in \u001b[0;36msplit_data_with_window\u001b[1;34m(x_in, y_in, split_window_size)\u001b[0m\n\u001b[0;32m     42\u001b[0m     y_out2_list\u001b[38;5;241m.\u001b[39mappend(y_out2)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Concatenate the lists into pandas DataFrames\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m x_out1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_out1_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m y_out1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(y_out1_list)\n\u001b[0;32m     47\u001b[0m x_out2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(x_out2_list)\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:679\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;66;03m# 1-ax to convert BlockManager axis to DataFrame axis\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     obj_labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ax]\n\u001b[1;32m--> 679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnew_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_labels\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    680\u001b[0m         indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5611\u001b[0m, in \u001b[0;36mIndex.equals\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   5552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mequals\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   5553\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5554\u001b[0m \u001b[38;5;124;03m    Determine if two Index object are equal.\u001b[39;00m\n\u001b[0;32m   5555\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5609\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[0;32m   5610\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5611\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   5612\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   5614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Index):\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:791\u001b[0m, in \u001b[0;36mIndex.is_\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    788\u001b[0m     result\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_\u001b[39m(\u001b[38;5;28mself\u001b[39m, other) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    793\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;124;03m    More flexible, faster check like ``is`` but that works through views.\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m other:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    # Initialize lists to store training and temporary sets\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    # Iterate through the data with the specified window size\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i+split_window_size+1]\n",
    "        y_out1_out2 = y_in.iloc[i:i+split_window_size+1]\n",
    "\n",
    "        # Separate the last row for the temporary set\n",
    "        # [ :-1]: all elements except the last one\n",
    "        # [-1:]:  selects only the last element\n",
    "        # (:) is used to indicate slicing of a sequence\n",
    "        # sequence[start : end : step]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    # Concatenate the lists into pandas DataFrames\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "# Split Data to train and temp\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 3\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split temp into val and test\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 1\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob=0.5):  # Added dropout_prob\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)  # Included dropout in LSTM\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Added dropout layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3]\n",
    "num_layers_list = [1]\n",
    "\n",
    "learning_rates = [0.001]\n",
    "\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "factors = [0.5]\n",
    "patience_lr = [10]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Lear. Rate: {current_lr:.5f}, Train Grad.: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150000, Train Loss: 29890, Val Loss: 30385,  Lear. Rate: 0.00500, Train Grad.: 223.4\n",
      "Epoch 101/150000, Train Loss: 29530, Val Loss: 30020,  Lear. Rate: 0.00452, Train Grad.: 220.2\n",
      "Epoch 201/150000, Train Loss: 28952, Val Loss: 29435,  Lear. Rate: 0.00454, Train Grad.: 214.9\n",
      "Epoch 301/150000, Train Loss: 28501, Val Loss: 28979,  Lear. Rate: 0.00455, Train Grad.: 210.6\n",
      "Epoch 401/150000, Train Loss: 28091, Val Loss: 28565,  Lear. Rate: 0.00457, Train Grad.: 206.7\n",
      "Epoch 501/150000, Train Loss: 27703, Val Loss: 28172,  Lear. Rate: 0.00458, Train Grad.: 202.9\n",
      "Epoch 601/150000, Train Loss: 27331, Val Loss: 27795,  Lear. Rate: 0.00459, Train Grad.: 199.2\n",
      "Epoch 701/150000, Train Loss: 26971, Val Loss: 27431,  Lear. Rate: 0.00460, Train Grad.: 195.5\n",
      "Epoch 801/150000, Train Loss: 26623, Val Loss: 27078,  Lear. Rate: 0.00461, Train Grad.: 191.9\n",
      "Epoch 901/150000, Train Loss: 26285, Val Loss: 26736,  Lear. Rate: 0.00462, Train Grad.: 188.4\n",
      "Epoch 1001/150000, Train Loss: 25956, Val Loss: 26403,  Lear. Rate: 0.00463, Train Grad.: 184.9\n",
      "Epoch 1101/150000, Train Loss: 25636, Val Loss: 26079,  Lear. Rate: 0.00464, Train Grad.: 181.4\n",
      "Epoch 1201/150000, Train Loss: 25325, Val Loss: 25763,  Lear. Rate: 0.00464, Train Grad.: 177.9\n",
      "Epoch 1301/150000, Train Loss: 25022, Val Loss: 25456,  Lear. Rate: 0.00465, Train Grad.: 174.5\n",
      "Epoch 1401/150000, Train Loss: 24728, Val Loss: 25157,  Lear. Rate: 0.00466, Train Grad.: 171.0\n",
      "Epoch 1501/150000, Train Loss: 24441, Val Loss: 24866,  Lear. Rate: 0.00467, Train Grad.: 167.7\n",
      "Epoch 1601/150000, Train Loss: 24161, Val Loss: 24583,  Lear. Rate: 0.00468, Train Grad.: 164.3\n",
      "Epoch 1701/150000, Train Loss: 23889, Val Loss: 24306,  Lear. Rate: 0.00468, Train Grad.: 160.9\n",
      "Epoch 1801/150000, Train Loss: 23624, Val Loss: 24038,  Lear. Rate: 0.00469, Train Grad.: 157.6\n",
      "Epoch 1901/150000, Train Loss: 23367, Val Loss: 23776,  Lear. Rate: 0.00470, Train Grad.: 154.3\n",
      "Epoch 2001/150000, Train Loss: 23116, Val Loss: 23521,  Lear. Rate: 0.00470, Train Grad.: 151.0\n",
      "Epoch 2101/150000, Train Loss: 22872, Val Loss: 23273,  Lear. Rate: 0.00471, Train Grad.: 147.8\n",
      "Epoch 2201/150000, Train Loss: 22635, Val Loss: 23032,  Lear. Rate: 0.00471, Train Grad.: 144.5\n",
      "Epoch 2301/150000, Train Loss: 22405, Val Loss: 22798,  Lear. Rate: 0.00472, Train Grad.: 141.3\n",
      "Epoch 2401/150000, Train Loss: 22072, Val Loss: 22480,  Lear. Rate: 0.00473, Train Grad.: 152.3\n",
      "Epoch 2501/150000, Train Loss: 21792, Val Loss: 22206,  Lear. Rate: 0.00473, Train Grad.: 148.9\n",
      "Epoch 2601/150000, Train Loss: 21531, Val Loss: 21933,  Lear. Rate: 0.00474, Train Grad.: 147.3\n",
      "Epoch 2701/150000, Train Loss: 21276, Val Loss: 21672,  Lear. Rate: 0.00475, Train Grad.: 145.5\n",
      "Epoch 2801/150000, Train Loss: 21025, Val Loss: 21417,  Lear. Rate: 0.00475, Train Grad.: 143.8\n",
      "Epoch 2901/150000, Train Loss: 20777, Val Loss: 21166,  Lear. Rate: 0.00476, Train Grad.: 142.0\n",
      "Epoch 3001/150000, Train Loss: 20533, Val Loss: 20917,  Lear. Rate: 0.00476, Train Grad.: 140.4\n",
      "Epoch 3101/150000, Train Loss: 20292, Val Loss: 20671,  Lear. Rate: 0.00477, Train Grad.: 138.7\n",
      "Epoch 3201/150000, Train Loss: 20055, Val Loss: 20429,  Lear. Rate: 0.00477, Train Grad.: 137.1\n",
      "Epoch 3301/150000, Train Loss: 19820, Val Loss: 20190,  Lear. Rate: 0.00478, Train Grad.: 135.5\n",
      "Epoch 3401/150000, Train Loss: 19588, Val Loss: 19955,  Lear. Rate: 0.00478, Train Grad.: 133.9\n",
      "Epoch 3501/150000, Train Loss: 19360, Val Loss: 19722,  Lear. Rate: 0.00479, Train Grad.: 132.3\n",
      "Epoch 3601/150000, Train Loss: 19134, Val Loss: 19493,  Lear. Rate: 0.00479, Train Grad.: 130.8\n",
      "Epoch 3701/150000, Train Loss: 18910, Val Loss: 19266,  Lear. Rate: 0.00480, Train Grad.: 129.3\n",
      "Epoch 3801/150000, Train Loss: 18690, Val Loss: 19042,  Lear. Rate: 0.00480, Train Grad.: 127.8\n",
      "Epoch 3901/150000, Train Loss: 18472, Val Loss: 18820,  Lear. Rate: 0.00481, Train Grad.: 126.3\n",
      "Epoch 4001/150000, Train Loss: 18256, Val Loss: 18601,  Lear. Rate: 0.00481, Train Grad.: 124.8\n",
      "Epoch 4101/150000, Train Loss: 18043, Val Loss: 18385,  Lear. Rate: 0.00482, Train Grad.: 123.4\n",
      "Epoch 4201/150000, Train Loss: 17833, Val Loss: 18171,  Lear. Rate: 0.00482, Train Grad.: 122.0\n",
      "Epoch 4301/150000, Train Loss: 17625, Val Loss: 17960,  Lear. Rate: 0.00483, Train Grad.: 120.5\n",
      "Epoch 4401/150000, Train Loss: 17419, Val Loss: 17752,  Lear. Rate: 0.00483, Train Grad.: 119.1\n",
      "Epoch 4501/150000, Train Loss: 17216, Val Loss: 17545,  Lear. Rate: 0.00483, Train Grad.: 117.7\n",
      "Epoch 4601/150000, Train Loss: 17015, Val Loss: 17342,  Lear. Rate: 0.00484, Train Grad.: 116.3\n",
      "Epoch 4701/150000, Train Loss: 16817, Val Loss: 17141,  Lear. Rate: 0.00484, Train Grad.: 114.9\n",
      "Epoch 4801/150000, Train Loss: 16622, Val Loss: 16942,  Lear. Rate: 0.00484, Train Grad.: 113.5\n",
      "Epoch 4901/150000, Train Loss: 16428, Val Loss: 16746,  Lear. Rate: 0.00485, Train Grad.: 112.1\n",
      "Epoch 5001/150000, Train Loss: 16238, Val Loss: 16552,  Lear. Rate: 0.00485, Train Grad.: 110.8\n",
      "Epoch 5101/150000, Train Loss: 16049, Val Loss: 16360,  Lear. Rate: 0.00485, Train Grad.: 109.4\n",
      "Epoch 5201/150000, Train Loss: 15863, Val Loss: 16171,  Lear. Rate: 0.00486, Train Grad.: 108.1\n",
      "Epoch 5301/150000, Train Loss: 15679, Val Loss: 15985,  Lear. Rate: 0.00486, Train Grad.: 106.7\n",
      "Epoch 5401/150000, Train Loss: 15498, Val Loss: 15800,  Lear. Rate: 0.00486, Train Grad.: 105.4\n",
      "Epoch 5501/150000, Train Loss: 15319, Val Loss: 15619,  Lear. Rate: 0.00487, Train Grad.: 104.1\n",
      "Epoch 5601/150000, Train Loss: 15142, Val Loss: 15439,  Lear. Rate: 0.00487, Train Grad.: 102.8\n",
      "Epoch 5701/150000, Train Loss: 14968, Val Loss: 15262,  Lear. Rate: 0.00487, Train Grad.: 101.5\n",
      "Epoch 5801/150000, Train Loss: 14795, Val Loss: 15087,  Lear. Rate: 0.00488, Train Grad.: 100.3\n",
      "Epoch 5901/150000, Train Loss: 14625, Val Loss: 14914,  Lear. Rate: 0.00488, Train Grad.: 99.1\n",
      "Epoch 6001/150000, Train Loss: 14457, Val Loss: 14744,  Lear. Rate: 0.00488, Train Grad.: 97.9\n",
      "Epoch 6101/150000, Train Loss: 14291, Val Loss: 14575,  Lear. Rate: 0.00488, Train Grad.: 96.7\n",
      "Epoch 6201/150000, Train Loss: 14126, Val Loss: 14409,  Lear. Rate: 0.00489, Train Grad.: 95.5\n",
      "Epoch 6301/150000, Train Loss: 13964, Val Loss: 14244,  Lear. Rate: 0.00489, Train Grad.: 94.3\n",
      "Epoch 6401/150000, Train Loss: 13804, Val Loss: 14082,  Lear. Rate: 0.00489, Train Grad.: 93.1\n",
      "Epoch 6501/150000, Train Loss: 13646, Val Loss: 13922,  Lear. Rate: 0.00489, Train Grad.: 92.0\n",
      "Epoch 6601/150000, Train Loss: 13490, Val Loss: 13763,  Lear. Rate: 0.00490, Train Grad.: 90.8\n",
      "Epoch 6701/150000, Train Loss: 13336, Val Loss: 13607,  Lear. Rate: 0.00490, Train Grad.: 89.7\n",
      "Epoch 6801/150000, Train Loss: 13183, Val Loss: 13452,  Lear. Rate: 0.00490, Train Grad.: 88.6\n",
      "Epoch 6901/150000, Train Loss: 13032, Val Loss: 13299,  Lear. Rate: 0.00490, Train Grad.: 87.6\n",
      "Epoch 7001/150000, Train Loss: 12883, Val Loss: 13148,  Lear. Rate: 0.00491, Train Grad.: 86.5\n",
      "Epoch 7101/150000, Train Loss: 12736, Val Loss: 12999,  Lear. Rate: 0.00491, Train Grad.: 85.5\n",
      "Epoch 7201/150000, Train Loss: 12591, Val Loss: 12852,  Lear. Rate: 0.00491, Train Grad.: 84.4\n",
      "Epoch 7301/150000, Train Loss: 12447, Val Loss: 12706,  Lear. Rate: 0.00491, Train Grad.: 83.4\n",
      "Epoch 7401/150000, Train Loss: 12305, Val Loss: 12562,  Lear. Rate: 0.00491, Train Grad.: 82.4\n",
      "Epoch 7501/150000, Train Loss: 12164, Val Loss: 12419,  Lear. Rate: 0.00492, Train Grad.: 81.4\n",
      "Epoch 7601/150000, Train Loss: 12026, Val Loss: 12279,  Lear. Rate: 0.00492, Train Grad.: 80.5\n",
      "Epoch 7701/150000, Train Loss: 11888, Val Loss: 12139,  Lear. Rate: 0.00492, Train Grad.: 79.6\n",
      "Epoch 7801/150000, Train Loss: 11752, Val Loss: 12002,  Lear. Rate: 0.00492, Train Grad.: 78.7\n",
      "Epoch 7901/150000, Train Loss: 11618, Val Loss: 11866,  Lear. Rate: 0.00492, Train Grad.: 77.7\n",
      "Epoch 8001/150000, Train Loss: 11485, Val Loss: 11731,  Lear. Rate: 0.00492, Train Grad.: 76.8\n",
      "Epoch 8101/150000, Train Loss: 11354, Val Loss: 11598,  Lear. Rate: 0.00493, Train Grad.: 76.1\n",
      "Epoch 8201/150000, Train Loss: 11223, Val Loss: 11466,  Lear. Rate: 0.00493, Train Grad.: 75.0\n",
      "Epoch 8301/150000, Train Loss: 11095, Val Loss: 11336,  Lear. Rate: 0.00493, Train Grad.: 74.2\n",
      "Epoch 8401/150000, Train Loss: 10968, Val Loss: 11207,  Lear. Rate: 0.00493, Train Grad.: 73.3\n",
      "Epoch 8501/150000, Train Loss: 10842, Val Loss: 11079,  Lear. Rate: 0.00493, Train Grad.: 72.5\n",
      "Epoch 8601/150000, Train Loss: 10718, Val Loss: 10953,  Lear. Rate: 0.00493, Train Grad.: 71.6\n",
      "Epoch 8701/150000, Train Loss: 10595, Val Loss: 10828,  Lear. Rate: 0.00494, Train Grad.: 70.8\n",
      "Epoch 8801/150000, Train Loss: 10473, Val Loss: 10705,  Lear. Rate: 0.00494, Train Grad.: 70.0\n",
      "Epoch 8901/150000, Train Loss: 10352, Val Loss: 10583,  Lear. Rate: 0.00494, Train Grad.: 69.1\n",
      "Epoch 9001/150000, Train Loss: 10233, Val Loss: 10462,  Lear. Rate: 0.00494, Train Grad.: 68.5\n",
      "Epoch 9101/150000, Train Loss: 10115, Val Loss: 10342,  Lear. Rate: 0.00494, Train Grad.: 67.7\n",
      "Epoch 9201/150000, Train Loss: 9998, Val Loss: 10224,  Lear. Rate: 0.00494, Train Grad.: 67.0\n",
      "Epoch 9301/150000, Train Loss: 9883, Val Loss: 10107,  Lear. Rate: 0.00494, Train Grad.: 66.2\n",
      "Epoch 9401/150000, Train Loss: 9768, Val Loss: 9991,  Lear. Rate: 0.00495, Train Grad.: 65.6\n",
      "Epoch 9501/150000, Train Loss: 9655, Val Loss: 9876,  Lear. Rate: 0.00495, Train Grad.: 64.8\n",
      "Epoch 9601/150000, Train Loss: 9543, Val Loss: 9762,  Lear. Rate: 0.00495, Train Grad.: 64.1\n",
      "Epoch 9701/150000, Train Loss: 9432, Val Loss: 9650,  Lear. Rate: 0.00495, Train Grad.: 63.4\n",
      "Epoch 9801/150000, Train Loss: 9322, Val Loss: 9539,  Lear. Rate: 0.00495, Train Grad.: 62.7\n",
      "Epoch 9901/150000, Train Loss: 9213, Val Loss: 9429,  Lear. Rate: 0.00495, Train Grad.: 62.1\n",
      "Epoch 10001/150000, Train Loss: 9105, Val Loss: 9319,  Lear. Rate: 0.00495, Train Grad.: 61.5\n",
      "Epoch 10101/150000, Train Loss: 8998, Val Loss: 9211,  Lear. Rate: 0.00495, Train Grad.: 60.8\n",
      "Epoch 10201/150000, Train Loss: 8892, Val Loss: 9104,  Lear. Rate: 0.00495, Train Grad.: 60.2\n",
      "Epoch 10301/150000, Train Loss: 8787, Val Loss: 8998,  Lear. Rate: 0.00496, Train Grad.: 59.7\n",
      "Epoch 10401/150000, Train Loss: 8683, Val Loss: 8892,  Lear. Rate: 0.00496, Train Grad.: 59.0\n",
      "Epoch 10501/150000, Train Loss: 8580, Val Loss: 8788,  Lear. Rate: 0.00496, Train Grad.: 58.7\n",
      "Epoch 10601/150000, Train Loss: 8478, Val Loss: 8684,  Lear. Rate: 0.00496, Train Grad.: 57.9\n",
      "Epoch 10701/150000, Train Loss: 8376, Val Loss: 8582,  Lear. Rate: 0.00496, Train Grad.: 57.0\n",
      "Epoch 10801/150000, Train Loss: 8275, Val Loss: 8480,  Lear. Rate: 0.00496, Train Grad.: 56.8\n",
      "Epoch 10901/150000, Train Loss: 8176, Val Loss: 8379,  Lear. Rate: 0.00496, Train Grad.: 56.2\n",
      "Epoch 11001/150000, Train Loss: 8077, Val Loss: 8279,  Lear. Rate: 0.00496, Train Grad.: 55.7\n",
      "Epoch 11101/150000, Train Loss: 7979, Val Loss: 8180,  Lear. Rate: 0.00496, Train Grad.: 55.2\n",
      "Epoch 11201/150000, Train Loss: 7881, Val Loss: 8081,  Lear. Rate: 0.00496, Train Grad.: 54.7\n",
      "Epoch 11301/150000, Train Loss: 7785, Val Loss: 7983,  Lear. Rate: 0.00497, Train Grad.: 54.1\n",
      "Epoch 11401/150000, Train Loss: 7689, Val Loss: 7887,  Lear. Rate: 0.00497, Train Grad.: 53.6\n",
      "Epoch 11501/150000, Train Loss: 7594, Val Loss: 7791,  Lear. Rate: 0.00497, Train Grad.: 53.3\n",
      "Epoch 11601/150000, Train Loss: 7500, Val Loss: 7696,  Lear. Rate: 0.00497, Train Grad.: 52.6\n",
      "Epoch 11701/150000, Train Loss: 7407, Val Loss: 7601,  Lear. Rate: 0.00497, Train Grad.: 52.2\n",
      "Epoch 11801/150000, Train Loss: 7315, Val Loss: 7508,  Lear. Rate: 0.00497, Train Grad.: 51.7\n",
      "Epoch 11901/150000, Train Loss: 7223, Val Loss: 7415,  Lear. Rate: 0.00497, Train Grad.: 51.2\n",
      "Epoch 12001/150000, Train Loss: 7132, Val Loss: 7323,  Lear. Rate: 0.00497, Train Grad.: 50.8\n",
      "Epoch 12101/150000, Train Loss: 7042, Val Loss: 7232,  Lear. Rate: 0.00497, Train Grad.: 50.3\n",
      "Epoch 12201/150000, Train Loss: 6953, Val Loss: 7142,  Lear. Rate: 0.00497, Train Grad.: 50.0\n",
      "Epoch 12301/150000, Train Loss: 6864, Val Loss: 7052,  Lear. Rate: 0.00497, Train Grad.: 49.3\n",
      "Epoch 12401/150000, Train Loss: 6776, Val Loss: 6964,  Lear. Rate: 0.00497, Train Grad.: 48.9\n",
      "Epoch 12501/150000, Train Loss: 6689, Val Loss: 6876,  Lear. Rate: 0.00497, Train Grad.: 48.4\n",
      "Epoch 12601/150000, Train Loss: 6603, Val Loss: 6788,  Lear. Rate: 0.00497, Train Grad.: 48.0\n",
      "Epoch 12701/150000, Train Loss: 6518, Val Loss: 6702,  Lear. Rate: 0.00498, Train Grad.: 47.5\n",
      "Epoch 12801/150000, Train Loss: 6433, Val Loss: 6616,  Lear. Rate: 0.00498, Train Grad.: 47.1\n",
      "Epoch 12901/150000, Train Loss: 6349, Val Loss: 6531,  Lear. Rate: 0.00498, Train Grad.: 46.7\n",
      "Epoch 13001/150000, Train Loss: 6266, Val Loss: 6447,  Lear. Rate: 0.00498, Train Grad.: 46.2\n",
      "Epoch 13101/150000, Train Loss: 6183, Val Loss: 6364,  Lear. Rate: 0.00498, Train Grad.: 45.8\n",
      "Epoch 13201/150000, Train Loss: 6101, Val Loss: 6281,  Lear. Rate: 0.00498, Train Grad.: 45.3\n",
      "Epoch 13301/150000, Train Loss: 6021, Val Loss: 6200,  Lear. Rate: 0.00498, Train Grad.: 44.9\n",
      "Epoch 13401/150000, Train Loss: 5940, Val Loss: 6119,  Lear. Rate: 0.00498, Train Grad.: 44.5\n",
      "Epoch 13501/150000, Train Loss: 5861, Val Loss: 6039,  Lear. Rate: 0.00498, Train Grad.: 44.1\n",
      "Epoch 13601/150000, Train Loss: 5782, Val Loss: 5959,  Lear. Rate: 0.00498, Train Grad.: 43.6\n",
      "Epoch 13701/150000, Train Loss: 5704, Val Loss: 5881,  Lear. Rate: 0.00498, Train Grad.: 43.3\n",
      "Epoch 13801/150000, Train Loss: 5627, Val Loss: 5803,  Lear. Rate: 0.00498, Train Grad.: 42.8\n",
      "Epoch 13901/150000, Train Loss: 5551, Val Loss: 5726,  Lear. Rate: 0.00498, Train Grad.: 42.5\n",
      "Epoch 14001/150000, Train Loss: 5475, Val Loss: 5650,  Lear. Rate: 0.00498, Train Grad.: 41.9\n",
      "Epoch 14101/150000, Train Loss: 5401, Val Loss: 5574,  Lear. Rate: 0.00498, Train Grad.: 41.5\n",
      "Epoch 14201/150000, Train Loss: 5327, Val Loss: 5499,  Lear. Rate: 0.00498, Train Grad.: 41.1\n",
      "Epoch 14301/150000, Train Loss: 5254, Val Loss: 5425,  Lear. Rate: 0.00498, Train Grad.: 40.7\n",
      "Epoch 14401/150000, Train Loss: 5181, Val Loss: 5352,  Lear. Rate: 0.00498, Train Grad.: 40.2\n",
      "Epoch 14501/150000, Train Loss: 5109, Val Loss: 5279,  Lear. Rate: 0.00498, Train Grad.: 39.9\n",
      "Epoch 14601/150000, Train Loss: 5038, Val Loss: 5208,  Lear. Rate: 0.00499, Train Grad.: 39.3\n",
      "Epoch 14701/150000, Train Loss: 4968, Val Loss: 5137,  Lear. Rate: 0.00499, Train Grad.: 39.1\n",
      "Epoch 14801/150000, Train Loss: 4899, Val Loss: 5067,  Lear. Rate: 0.00499, Train Grad.: 38.5\n",
      "Epoch 14901/150000, Train Loss: 4830, Val Loss: 4998,  Lear. Rate: 0.00499, Train Grad.: 38.3\n",
      "Epoch 15001/150000, Train Loss: 4761, Val Loss: 4930,  Lear. Rate: 0.00499, Train Grad.: 37.9\n",
      "Epoch 15101/150000, Train Loss: 4694, Val Loss: 4862,  Lear. Rate: 0.00499, Train Grad.: 37.5\n",
      "Epoch 15201/150000, Train Loss: 4627, Val Loss: 4795,  Lear. Rate: 0.00499, Train Grad.: 37.2\n",
      "Epoch 15301/150000, Train Loss: 4561, Val Loss: 4728,  Lear. Rate: 0.00499, Train Grad.: 36.9\n",
      "Epoch 15401/150000, Train Loss: 4496, Val Loss: 4662,  Lear. Rate: 0.00499, Train Grad.: 36.4\n",
      "Epoch 15501/150000, Train Loss: 4431, Val Loss: 4597,  Lear. Rate: 0.00499, Train Grad.: 35.9\n",
      "Epoch 15601/150000, Train Loss: 4367, Val Loss: 4533,  Lear. Rate: 0.00499, Train Grad.: 35.7\n",
      "Epoch 15701/150000, Train Loss: 4303, Val Loss: 4469,  Lear. Rate: 0.00499, Train Grad.: 35.3\n",
      "Epoch 15801/150000, Train Loss: 4240, Val Loss: 4406,  Lear. Rate: 0.00499, Train Grad.: 35.0\n",
      "Epoch 15901/150000, Train Loss: 4177, Val Loss: 4344,  Lear. Rate: 0.00499, Train Grad.: 34.6\n",
      "Epoch 16001/150000, Train Loss: 4116, Val Loss: 4283,  Lear. Rate: 0.00499, Train Grad.: 34.3\n",
      "Epoch 16101/150000, Train Loss: 4055, Val Loss: 4222,  Lear. Rate: 0.00499, Train Grad.: 33.9\n",
      "Epoch 16201/150000, Train Loss: 3995, Val Loss: 4161,  Lear. Rate: 0.00499, Train Grad.: 33.5\n",
      "Epoch 16301/150000, Train Loss: 3935, Val Loss: 4101,  Lear. Rate: 0.00499, Train Grad.: 33.0\n",
      "Epoch 16401/150000, Train Loss: 3876, Val Loss: 4043,  Lear. Rate: 0.00499, Train Grad.: 32.8\n",
      "Epoch 16501/150000, Train Loss: 3818, Val Loss: 3984,  Lear. Rate: 0.00499, Train Grad.: 32.5\n",
      "Epoch 16601/150000, Train Loss: 3761, Val Loss: 3927,  Lear. Rate: 0.00499, Train Grad.: 32.1\n",
      "Epoch 16701/150000, Train Loss: 3704, Val Loss: 3870,  Lear. Rate: 0.00499, Train Grad.: 31.8\n",
      "Epoch 16801/150000, Train Loss: 3647, Val Loss: 3814,  Lear. Rate: 0.00499, Train Grad.: 31.5\n",
      "Epoch 16901/150000, Train Loss: 3592, Val Loss: 3758,  Lear. Rate: 0.00499, Train Grad.: 31.1\n",
      "Epoch 17001/150000, Train Loss: 3537, Val Loss: 3704,  Lear. Rate: 0.00499, Train Grad.: 30.7\n",
      "Epoch 17101/150000, Train Loss: 3482, Val Loss: 3650,  Lear. Rate: 0.00499, Train Grad.: 30.4\n",
      "Epoch 17201/150000, Train Loss: 3429, Val Loss: 3597,  Lear. Rate: 0.00499, Train Grad.: 30.1\n",
      "Epoch 17301/150000, Train Loss: 3376, Val Loss: 3544,  Lear. Rate: 0.00499, Train Grad.: 29.7\n",
      "Epoch 17401/150000, Train Loss: 3323, Val Loss: 3493,  Lear. Rate: 0.00499, Train Grad.: 29.4\n",
      "Epoch 17501/150000, Train Loss: 3272, Val Loss: 3441,  Lear. Rate: 0.00499, Train Grad.: 28.8\n",
      "Epoch 17601/150000, Train Loss: 3221, Val Loss: 3390,  Lear. Rate: 0.00499, Train Grad.: 28.7\n",
      "Epoch 17701/150000, Train Loss: 3170, Val Loss: 3340,  Lear. Rate: 0.00499, Train Grad.: 28.4\n",
      "Epoch 17801/150000, Train Loss: 3120, Val Loss: 3290,  Lear. Rate: 0.00499, Train Grad.: 28.0\n",
      "Epoch 17901/150000, Train Loss: 3071, Val Loss: 3241,  Lear. Rate: 0.00499, Train Grad.: 27.7\n",
      "Epoch 18001/150000, Train Loss: 3022, Val Loss: 3193,  Lear. Rate: 0.00499, Train Grad.: 27.4\n",
      "Epoch 18101/150000, Train Loss: 2974, Val Loss: 3145,  Lear. Rate: 0.00499, Train Grad.: 27.1\n",
      "Epoch 18201/150000, Train Loss: 2927, Val Loss: 3097,  Lear. Rate: 0.00499, Train Grad.: 26.8\n",
      "Epoch 18301/150000, Train Loss: 2880, Val Loss: 3050,  Lear. Rate: 0.00499, Train Grad.: 26.5\n",
      "Epoch 18401/150000, Train Loss: 2833, Val Loss: 3003,  Lear. Rate: 0.00500, Train Grad.: 26.6\n",
      "Epoch 18501/150000, Train Loss: 2788, Val Loss: 2958,  Lear. Rate: 0.00500, Train Grad.: 25.8\n",
      "Epoch 18601/150000, Train Loss: 2742, Val Loss: 2912,  Lear. Rate: 0.00500, Train Grad.: 26.1\n",
      "Epoch 18701/150000, Train Loss: 2698, Val Loss: 2868,  Lear. Rate: 0.00500, Train Grad.: 25.2\n",
      "Epoch 18801/150000, Train Loss: 2654, Val Loss: 2824,  Lear. Rate: 0.00500, Train Grad.: 24.9\n",
      "Epoch 18901/150000, Train Loss: 2610, Val Loss: 2780,  Lear. Rate: 0.00500, Train Grad.: 24.6\n",
      "Epoch 19001/150000, Train Loss: 2567, Val Loss: 2738,  Lear. Rate: 0.00500, Train Grad.: 24.3\n",
      "Epoch 19101/150000, Train Loss: 2525, Val Loss: 2695,  Lear. Rate: 0.00500, Train Grad.: 24.1\n",
      "Epoch 19201/150000, Train Loss: 2483, Val Loss: 2654,  Lear. Rate: 0.00500, Train Grad.: 23.7\n",
      "Epoch 19301/150000, Train Loss: 2442, Val Loss: 2613,  Lear. Rate: 0.00500, Train Grad.: 23.2\n",
      "Epoch 19401/150000, Train Loss: 2401, Val Loss: 2572,  Lear. Rate: 0.00500, Train Grad.: 23.2\n",
      "Epoch 19501/150000, Train Loss: 2360, Val Loss: 2532,  Lear. Rate: 0.00500, Train Grad.: 23.1\n",
      "Epoch 19601/150000, Train Loss: 2321, Val Loss: 2493,  Lear. Rate: 0.00500, Train Grad.: 22.6\n",
      "Epoch 19701/150000, Train Loss: 2281, Val Loss: 2454,  Lear. Rate: 0.00500, Train Grad.: 22.3\n",
      "Epoch 19801/150000, Train Loss: 2243, Val Loss: 2416,  Lear. Rate: 0.00500, Train Grad.: 22.1\n",
      "Epoch 19901/150000, Train Loss: 2204, Val Loss: 2378,  Lear. Rate: 0.00500, Train Grad.: 21.8\n",
      "Epoch 20001/150000, Train Loss: 2166, Val Loss: 2340,  Lear. Rate: 0.00500, Train Grad.: 21.9\n",
      "Epoch 20101/150000, Train Loss: 2129, Val Loss: 2303,  Lear. Rate: 0.00500, Train Grad.: 21.3\n",
      "Epoch 20201/150000, Train Loss: 2092, Val Loss: 2266,  Lear. Rate: 0.00500, Train Grad.: 21.8\n",
      "Epoch 20301/150000, Train Loss: 2055, Val Loss: 2230,  Lear. Rate: 0.00500, Train Grad.: 20.8\n",
      "Epoch 20401/150000, Train Loss: 2019, Val Loss: 2195,  Lear. Rate: 0.00500, Train Grad.: 20.5\n",
      "Epoch 20501/150000, Train Loss: 1984, Val Loss: 2159,  Lear. Rate: 0.00500, Train Grad.: 20.3\n",
      "Epoch 20601/150000, Train Loss: 1949, Val Loss: 2125,  Lear. Rate: 0.00500, Train Grad.: 20.0\n",
      "Epoch 20701/150000, Train Loss: 1914, Val Loss: 2091,  Lear. Rate: 0.00500, Train Grad.: 19.7\n",
      "Epoch 20801/150000, Train Loss: 1880, Val Loss: 2057,  Lear. Rate: 0.00500, Train Grad.: 19.5\n",
      "Epoch 20901/150000, Train Loss: 1846, Val Loss: 2024,  Lear. Rate: 0.00500, Train Grad.: 19.2\n",
      "Epoch 21001/150000, Train Loss: 1813, Val Loss: 1991,  Lear. Rate: 0.00500, Train Grad.: 19.0\n",
      "Epoch 21101/150000, Train Loss: 1780, Val Loss: 1959,  Lear. Rate: 0.00500, Train Grad.: 18.8\n",
      "Epoch 21201/150000, Train Loss: 1748, Val Loss: 1927,  Lear. Rate: 0.00500, Train Grad.: 18.5\n",
      "Epoch 21301/150000, Train Loss: 1716, Val Loss: 1896,  Lear. Rate: 0.00500, Train Grad.: 18.2\n",
      "Epoch 21401/150000, Train Loss: 1684, Val Loss: 1865,  Lear. Rate: 0.00500, Train Grad.: 18.0\n",
      "Epoch 21501/150000, Train Loss: 1653, Val Loss: 1835,  Lear. Rate: 0.00500, Train Grad.: 17.7\n",
      "Epoch 21601/150000, Train Loss: 1623, Val Loss: 1805,  Lear. Rate: 0.00500, Train Grad.: 17.5\n",
      "Epoch 21701/150000, Train Loss: 1593, Val Loss: 1775,  Lear. Rate: 0.00500, Train Grad.: 17.3\n",
      "Epoch 21801/150000, Train Loss: 1563, Val Loss: 1746,  Lear. Rate: 0.00500, Train Grad.: 17.0\n",
      "Epoch 21901/150000, Train Loss: 1534, Val Loss: 1718,  Lear. Rate: 0.00500, Train Grad.: 16.8\n",
      "Epoch 22001/150000, Train Loss: 1506, Val Loss: 1690,  Lear. Rate: 0.00500, Train Grad.: 16.6\n",
      "Epoch 22101/150000, Train Loss: 1477, Val Loss: 1662,  Lear. Rate: 0.00500, Train Grad.: 16.4\n",
      "Epoch 22201/150000, Train Loss: 1449, Val Loss: 1635,  Lear. Rate: 0.00500, Train Grad.: 16.1\n",
      "Epoch 22301/150000, Train Loss: 1422, Val Loss: 1608,  Lear. Rate: 0.00500, Train Grad.: 15.9\n",
      "Epoch 22401/150000, Train Loss: 1395, Val Loss: 1582,  Lear. Rate: 0.00500, Train Grad.: 15.7\n",
      "Epoch 22501/150000, Train Loss: 1368, Val Loss: 1556,  Lear. Rate: 0.00500, Train Grad.: 15.4\n",
      "Epoch 22601/150000, Train Loss: 1342, Val Loss: 1530,  Lear. Rate: 0.00500, Train Grad.: 15.1\n",
      "Epoch 22701/150000, Train Loss: 1316, Val Loss: 1505,  Lear. Rate: 0.00500, Train Grad.: 15.0\n",
      "Epoch 22801/150000, Train Loss: 1290, Val Loss: 1481,  Lear. Rate: 0.00500, Train Grad.: 14.4\n",
      "Epoch 22901/150000, Train Loss: 1265, Val Loss: 1456,  Lear. Rate: 0.00500, Train Grad.: 14.6\n",
      "Epoch 23001/150000, Train Loss: 1240, Val Loss: 1432,  Lear. Rate: 0.00500, Train Grad.: 14.5\n",
      "Epoch 23101/150000, Train Loss: 1215, Val Loss: 1409,  Lear. Rate: 0.00500, Train Grad.: 14.3\n",
      "Epoch 23201/150000, Train Loss: 1191, Val Loss: 1386,  Lear. Rate: 0.00500, Train Grad.: 14.1\n",
      "Epoch 23301/150000, Train Loss: 1167, Val Loss: 1363,  Lear. Rate: 0.00500, Train Grad.: 13.8\n",
      "Epoch 23401/150000, Train Loss: 1144, Val Loss: 1341,  Lear. Rate: 0.00500, Train Grad.: 13.4\n",
      "Epoch 23501/150000, Train Loss: 1120, Val Loss: 1319,  Lear. Rate: 0.00500, Train Grad.: 13.3\n",
      "Epoch 23601/150000, Train Loss: 1098, Val Loss: 1298,  Lear. Rate: 0.00500, Train Grad.: 13.2\n",
      "Epoch 23701/150000, Train Loss: 1075, Val Loss: 1276,  Lear. Rate: 0.00500, Train Grad.: 13.0\n",
      "Epoch 23801/150000, Train Loss: 1053, Val Loss: 1256,  Lear. Rate: 0.00500, Train Grad.: 12.8\n",
      "Epoch 23901/150000, Train Loss: 1031, Val Loss: 1236,  Lear. Rate: 0.00500, Train Grad.: 12.5\n",
      "Epoch 24001/150000, Train Loss: 1009, Val Loss: 1216,  Lear. Rate: 0.00500, Train Grad.: 12.4\n",
      "Epoch 24101/150000, Train Loss: 987, Val Loss: 1197,  Lear. Rate: 0.00500, Train Grad.: 12.2\n",
      "Epoch 24201/150000, Train Loss: 966, Val Loss: 1178,  Lear. Rate: 0.00500, Train Grad.: 12.0\n",
      "Epoch 24301/150000, Train Loss: 946, Val Loss: 1160,  Lear. Rate: 0.00500, Train Grad.: 11.8\n",
      "Epoch 24401/150000, Train Loss: 926, Val Loss: 1142,  Lear. Rate: 0.00500, Train Grad.: 11.7\n",
      "Epoch 24501/150000, Train Loss: 906, Val Loss: 1125,  Lear. Rate: 0.00500, Train Grad.: 11.4\n",
      "Epoch 24601/150000, Train Loss: 887, Val Loss: 1107,  Lear. Rate: 0.00500, Train Grad.: 11.3\n",
      "Epoch 24701/150000, Train Loss: 866, Val Loss: 1088,  Lear. Rate: 0.00500, Train Grad.: 11.0\n",
      "Epoch 24801/150000, Train Loss: 846, Val Loss: 1067,  Lear. Rate: 0.00500, Train Grad.: 10.9\n",
      "Epoch 24901/150000, Train Loss: 826, Val Loss: 1043,  Lear. Rate: 0.00500, Train Grad.: 10.7\n",
      "Epoch 25001/150000, Train Loss: 808, Val Loss: 1027,  Lear. Rate: 0.00500, Train Grad.: 10.6\n",
      "Epoch 25101/150000, Train Loss: 801, Val Loss: 1012,  Lear. Rate: 0.00500, Train Grad.: 10.4\n",
      "Epoch 25201/150000, Train Loss: 783, Val Loss: 996,  Lear. Rate: 0.00500, Train Grad.: 10.3\n",
      "Epoch 25301/150000, Train Loss: 766, Val Loss: 980,  Lear. Rate: 0.00500, Train Grad.: 10.2\n",
      "Epoch 25401/150000, Train Loss: 749, Val Loss: 964,  Lear. Rate: 0.00500, Train Grad.: 10.0\n",
      "Epoch 25501/150000, Train Loss: 732, Val Loss: 949,  Lear. Rate: 0.00500, Train Grad.: 9.8\n",
      "Epoch 25601/150000, Train Loss: 716, Val Loss: 934,  Lear. Rate: 0.00500, Train Grad.: 9.6\n",
      "Epoch 25701/150000, Train Loss: 700, Val Loss: 919,  Lear. Rate: 0.00500, Train Grad.: 9.5\n",
      "Epoch 25801/150000, Train Loss: 684, Val Loss: 905,  Lear. Rate: 0.00500, Train Grad.: 9.3\n",
      "Epoch 25901/150000, Train Loss: 669, Val Loss: 891,  Lear. Rate: 0.00500, Train Grad.: 9.2\n",
      "Epoch 26001/150000, Train Loss: 654, Val Loss: 878,  Lear. Rate: 0.00500, Train Grad.: 9.0\n",
      "Epoch 26101/150000, Train Loss: 639, Val Loss: 864,  Lear. Rate: 0.00500, Train Grad.: 8.9\n",
      "Epoch 26201/150000, Train Loss: 624, Val Loss: 851,  Lear. Rate: 0.00500, Train Grad.: 9.0\n",
      "Epoch 26301/150000, Train Loss: 610, Val Loss: 838,  Lear. Rate: 0.00500, Train Grad.: 8.6\n",
      "Epoch 26401/150000, Train Loss: 596, Val Loss: 825,  Lear. Rate: 0.00500, Train Grad.: 8.5\n",
      "Epoch 26501/150000, Train Loss: 582, Val Loss: 813,  Lear. Rate: 0.00500, Train Grad.: 8.3\n",
      "Epoch 26601/150000, Train Loss: 568, Val Loss: 801,  Lear. Rate: 0.00500, Train Grad.: 8.1\n",
      "Epoch 26701/150000, Train Loss: 555, Val Loss: 789,  Lear. Rate: 0.00500, Train Grad.: 8.0\n",
      "Epoch 26801/150000, Train Loss: 542, Val Loss: 777,  Lear. Rate: 0.00500, Train Grad.: 7.8\n",
      "Epoch 26901/150000, Train Loss: 530, Val Loss: 766,  Lear. Rate: 0.00500, Train Grad.: 7.6\n",
      "Epoch 27001/150000, Train Loss: 517, Val Loss: 755,  Lear. Rate: 0.00500, Train Grad.: 7.5\n",
      "Epoch 27101/150000, Train Loss: 505, Val Loss: 744,  Lear. Rate: 0.00500, Train Grad.: 7.3\n",
      "Epoch 27201/150000, Train Loss: 493, Val Loss: 734,  Lear. Rate: 0.00500, Train Grad.: 7.2\n",
      "Epoch 27301/150000, Train Loss: 482, Val Loss: 725,  Lear. Rate: 0.00500, Train Grad.: 6.5\n",
      "Epoch 27401/150000, Train Loss: 470, Val Loss: 714,  Lear. Rate: 0.00500, Train Grad.: 7.0\n",
      "Epoch 27501/150000, Train Loss: 459, Val Loss: 705,  Lear. Rate: 0.00500, Train Grad.: 6.8\n",
      "Epoch 27601/150000, Train Loss: 448, Val Loss: 696,  Lear. Rate: 0.00500, Train Grad.: 6.8\n",
      "Epoch 27701/150000, Train Loss: 438, Val Loss: 687,  Lear. Rate: 0.00500, Train Grad.: 6.5\n",
      "Epoch 27801/150000, Train Loss: 427, Val Loss: 678,  Lear. Rate: 0.00500, Train Grad.: 6.4\n",
      "Epoch 27901/150000, Train Loss: 417, Val Loss: 670,  Lear. Rate: 0.00500, Train Grad.: 6.3\n",
      "Epoch 28001/150000, Train Loss: 407, Val Loss: 661,  Lear. Rate: 0.00500, Train Grad.: 6.1\n",
      "Epoch 28101/150000, Train Loss: 398, Val Loss: 653,  Lear. Rate: 0.00500, Train Grad.: 6.1\n",
      "Epoch 28201/150000, Train Loss: 388, Val Loss: 644,  Lear. Rate: 0.00500, Train Grad.: 5.9\n",
      "Epoch 28301/150000, Train Loss: 378, Val Loss: 629,  Lear. Rate: 0.00500, Train Grad.: 5.8\n",
      "Epoch 28401/150000, Train Loss: 369, Val Loss: 621,  Lear. Rate: 0.00500, Train Grad.: 5.8\n",
      "Epoch 28501/150000, Train Loss: 360, Val Loss: 614,  Lear. Rate: 0.00500, Train Grad.: 5.6\n",
      "Epoch 28601/150000, Train Loss: 351, Val Loss: 607,  Lear. Rate: 0.00500, Train Grad.: 5.5\n",
      "Epoch 28701/150000, Train Loss: 343, Val Loss: 600,  Lear. Rate: 0.00500, Train Grad.: 5.4\n",
      "Epoch 28801/150000, Train Loss: 334, Val Loss: 593,  Lear. Rate: 0.00500, Train Grad.: 5.3\n",
      "Epoch 28901/150000, Train Loss: 326, Val Loss: 587,  Lear. Rate: 0.00500, Train Grad.: 6.1\n",
      "Epoch 29001/150000, Train Loss: 318, Val Loss: 582,  Lear. Rate: 0.00500, Train Grad.: 5.0\n",
      "Epoch 29101/150000, Train Loss: 310, Val Loss: 576,  Lear. Rate: 0.00500, Train Grad.: 4.9\n",
      "Epoch 29201/150000, Train Loss: 302, Val Loss: 570,  Lear. Rate: 0.00500, Train Grad.: 4.8\n",
      "Epoch 29301/150000, Train Loss: 294, Val Loss: 560,  Lear. Rate: 0.00500, Train Grad.: 4.7\n",
      "Epoch 29401/150000, Train Loss: 287, Val Loss: 554,  Lear. Rate: 0.00500, Train Grad.: 4.6\n",
      "Epoch 29501/150000, Train Loss: 280, Val Loss: 549,  Lear. Rate: 0.00500, Train Grad.: 4.5\n",
      "Epoch 29601/150000, Train Loss: 273, Val Loss: 544,  Lear. Rate: 0.00500, Train Grad.: 4.4\n",
      "Epoch 29701/150000, Train Loss: 266, Val Loss: 538,  Lear. Rate: 0.00500, Train Grad.: 4.3\n",
      "Epoch 29801/150000, Train Loss: 259, Val Loss: 533,  Lear. Rate: 0.00500, Train Grad.: 4.0\n",
      "Epoch 29901/150000, Train Loss: 253, Val Loss: 527,  Lear. Rate: 0.00500, Train Grad.: 4.1\n",
      "Epoch 30001/150000, Train Loss: 247, Val Loss: 523,  Lear. Rate: 0.00500, Train Grad.: 3.9\n",
      "Epoch 30101/150000, Train Loss: 241, Val Loss: 517,  Lear. Rate: 0.00500, Train Grad.: 3.8\n",
      "Epoch 30201/150000, Train Loss: 235, Val Loss: 513,  Lear. Rate: 0.00500, Train Grad.: 3.7\n",
      "Epoch 30301/150000, Train Loss: 230, Val Loss: 509,  Lear. Rate: 0.00500, Train Grad.: 3.6\n",
      "Epoch 30401/150000, Train Loss: 224, Val Loss: 505,  Lear. Rate: 0.00500, Train Grad.: 3.5\n",
      "Epoch 30501/150000, Train Loss: 219, Val Loss: 502,  Lear. Rate: 0.00500, Train Grad.: 3.4\n",
      "Epoch 30601/150000, Train Loss: 214, Val Loss: 499,  Lear. Rate: 0.00500, Train Grad.: 3.3\n",
      "Epoch 30701/150000, Train Loss: 209, Val Loss: 496,  Lear. Rate: 0.00500, Train Grad.: 3.3\n",
      "Epoch 30801/150000, Train Loss: 205, Val Loss: 493,  Lear. Rate: 0.00500, Train Grad.: 3.5\n",
      "Epoch 30901/150000, Train Loss: 200, Val Loss: 491,  Lear. Rate: 0.00500, Train Grad.: 3.1\n",
      "Epoch 31001/150000, Train Loss: 196, Val Loss: 487,  Lear. Rate: 0.00500, Train Grad.: 3.0\n",
      "Epoch 31101/150000, Train Loss: 191, Val Loss: 484,  Lear. Rate: 0.00500, Train Grad.: 3.0\n",
      "Epoch 31201/150000, Train Loss: 187, Val Loss: 480,  Lear. Rate: 0.00500, Train Grad.: 2.8\n",
      "Epoch 31301/150000, Train Loss: 183, Val Loss: 477,  Lear. Rate: 0.00500, Train Grad.: 2.8\n",
      "Epoch 31401/150000, Train Loss: 179, Val Loss: 483,  Lear. Rate: 0.00500, Train Grad.: 2.7\n",
      "Epoch 31501/150000, Train Loss: 175, Val Loss: 485,  Lear. Rate: 0.00500, Train Grad.: 2.7\n",
      "Epoch 31601/150000, Train Loss: 171, Val Loss: 481,  Lear. Rate: 0.00500, Train Grad.: 2.5\n",
      "Epoch 31701/150000, Train Loss: 167, Val Loss: 476,  Lear. Rate: 0.00500, Train Grad.: 2.5\n",
      "Epoch 31801/150000, Train Loss: 163, Val Loss: 470,  Lear. Rate: 0.00500, Train Grad.: 2.5\n",
      "Epoch 31901/150000, Train Loss: 160, Val Loss: 465,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 32001/150000, Train Loss: 156, Val Loss: 461,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 32101/150000, Train Loss: 153, Val Loss: 459,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 32201/150000, Train Loss: 148, Val Loss: 461,  Lear. Rate: 0.00500, Train Grad.: 2.6\n",
      "Epoch 32301/150000, Train Loss: 144, Val Loss: 459,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 32401/150000, Train Loss: 141, Val Loss: 458,  Lear. Rate: 0.00500, Train Grad.: 1.8\n",
      "Epoch 32501/150000, Train Loss: 138, Val Loss: 457,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Early stopping at epoch 32556 with validation loss 457.4826965332031.\n",
      "Test Loss: 395.1988220214844\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3]\n",
    "num_layers_list = [1]\n",
    "\n",
    "learning_rates = [0.005]\n",
    "\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "factors = [0.5]\n",
    "patience_lr = [10]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Lear. Rate: {current_lr:.5f}, Train Grad.: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.001, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150000, Train Loss: 29890, Val Loss: 30387,  Learning Rate: 0.00100, Train Gradient: 223.4\n",
      "Epoch 101/150000, Train Loss: 29834, Val Loss: 30332,  Learning Rate: 0.00090, Train Gradient: 223.0\n",
      "Epoch 201/150000, Train Loss: 29767, Val Loss: 30263,  Learning Rate: 0.00090, Train Gradient: 222.4\n",
      "Epoch 301/150000, Train Loss: 29676, Val Loss: 30171,  Learning Rate: 0.00090, Train Gradient: 221.6\n",
      "Epoch 401/150000, Train Loss: 29586, Val Loss: 30080,  Learning Rate: 0.00090, Train Gradient: 220.7\n",
      "Epoch 501/150000, Train Loss: 29444, Val Loss: 29936,  Learning Rate: 0.00090, Train Gradient: 219.5\n",
      "Epoch 601/150000, Train Loss: 29316, Val Loss: 29806,  Learning Rate: 0.00091, Train Gradient: 218.3\n",
      "Epoch 701/150000, Train Loss: 29209, Val Loss: 29698,  Learning Rate: 0.00091, Train Gradient: 217.3\n",
      "Epoch 801/150000, Train Loss: 29111, Val Loss: 29600,  Learning Rate: 0.00091, Train Gradient: 216.4\n",
      "Epoch 901/150000, Train Loss: 29019, Val Loss: 29506,  Learning Rate: 0.00091, Train Gradient: 215.5\n",
      "Epoch 1001/150000, Train Loss: 28930, Val Loss: 29417,  Learning Rate: 0.00091, Train Gradient: 214.7\n",
      "Epoch 1101/150000, Train Loss: 28844, Val Loss: 29330,  Learning Rate: 0.00091, Train Gradient: 213.9\n",
      "Epoch 1201/150000, Train Loss: 28760, Val Loss: 29245,  Learning Rate: 0.00091, Train Gradient: 213.1\n",
      "Epoch 1301/150000, Train Loss: 28678, Val Loss: 29161,  Learning Rate: 0.00091, Train Gradient: 212.3\n",
      "Epoch 1401/150000, Train Loss: 28596, Val Loss: 29079,  Learning Rate: 0.00091, Train Gradient: 211.5\n",
      "Epoch 1501/150000, Train Loss: 28516, Val Loss: 28998,  Learning Rate: 0.00091, Train Gradient: 210.8\n",
      "Epoch 1601/150000, Train Loss: 28437, Val Loss: 28918,  Learning Rate: 0.00091, Train Gradient: 210.0\n",
      "Epoch 1701/150000, Train Loss: 28359, Val Loss: 28838,  Learning Rate: 0.00091, Train Gradient: 209.3\n",
      "Epoch 1801/150000, Train Loss: 28281, Val Loss: 28760,  Learning Rate: 0.00091, Train Gradient: 208.5\n",
      "Epoch 1901/150000, Train Loss: 28204, Val Loss: 28682,  Learning Rate: 0.00091, Train Gradient: 207.8\n",
      "Epoch 2001/150000, Train Loss: 28127, Val Loss: 28604,  Learning Rate: 0.00091, Train Gradient: 207.0\n",
      "Epoch 2101/150000, Train Loss: 28052, Val Loss: 28528,  Learning Rate: 0.00091, Train Gradient: 206.3\n",
      "Epoch 2201/150000, Train Loss: 27976, Val Loss: 28451,  Learning Rate: 0.00091, Train Gradient: 205.6\n",
      "Epoch 2301/150000, Train Loss: 27901, Val Loss: 28376,  Learning Rate: 0.00091, Train Gradient: 204.8\n",
      "Epoch 2401/150000, Train Loss: 27827, Val Loss: 28300,  Learning Rate: 0.00091, Train Gradient: 204.1\n",
      "Epoch 2501/150000, Train Loss: 27753, Val Loss: 28226,  Learning Rate: 0.00092, Train Gradient: 203.4\n",
      "Epoch 2601/150000, Train Loss: 27680, Val Loss: 28151,  Learning Rate: 0.00092, Train Gradient: 202.7\n",
      "Epoch 2701/150000, Train Loss: 27606, Val Loss: 28077,  Learning Rate: 0.00092, Train Gradient: 201.9\n",
      "Epoch 2801/150000, Train Loss: 27534, Val Loss: 28003,  Learning Rate: 0.00092, Train Gradient: 201.2\n",
      "Epoch 2901/150000, Train Loss: 27461, Val Loss: 27930,  Learning Rate: 0.00092, Train Gradient: 200.5\n",
      "Epoch 3001/150000, Train Loss: 27389, Val Loss: 27857,  Learning Rate: 0.00092, Train Gradient: 199.8\n",
      "Epoch 3101/150000, Train Loss: 27317, Val Loss: 27784,  Learning Rate: 0.00092, Train Gradient: 199.0\n",
      "Epoch 3201/150000, Train Loss: 27246, Val Loss: 27712,  Learning Rate: 0.00092, Train Gradient: 198.3\n",
      "Epoch 3301/150000, Train Loss: 27175, Val Loss: 27640,  Learning Rate: 0.00092, Train Gradient: 197.6\n",
      "Epoch 3401/150000, Train Loss: 27104, Val Loss: 27569,  Learning Rate: 0.00092, Train Gradient: 196.9\n",
      "Epoch 3501/150000, Train Loss: 27034, Val Loss: 27497,  Learning Rate: 0.00092, Train Gradient: 196.2\n",
      "Epoch 3601/150000, Train Loss: 26964, Val Loss: 27426,  Learning Rate: 0.00092, Train Gradient: 195.5\n",
      "Epoch 3701/150000, Train Loss: 26894, Val Loss: 27355,  Learning Rate: 0.00092, Train Gradient: 194.7\n",
      "Epoch 3801/150000, Train Loss: 26824, Val Loss: 27285,  Learning Rate: 0.00092, Train Gradient: 194.0\n",
      "Epoch 3901/150000, Train Loss: 26755, Val Loss: 27215,  Learning Rate: 0.00092, Train Gradient: 193.3\n",
      "Epoch 4001/150000, Train Loss: 26686, Val Loss: 27145,  Learning Rate: 0.00092, Train Gradient: 192.6\n",
      "Epoch 4101/150000, Train Loss: 26617, Val Loss: 27075,  Learning Rate: 0.00092, Train Gradient: 191.9\n",
      "Epoch 4201/150000, Train Loss: 26549, Val Loss: 27006,  Learning Rate: 0.00092, Train Gradient: 191.2\n",
      "Epoch 4301/150000, Train Loss: 26480, Val Loss: 26937,  Learning Rate: 0.00092, Train Gradient: 190.4\n",
      "Epoch 4401/150000, Train Loss: 26412, Val Loss: 26868,  Learning Rate: 0.00092, Train Gradient: 189.7\n",
      "Epoch 4501/150000, Train Loss: 26345, Val Loss: 26799,  Learning Rate: 0.00092, Train Gradient: 189.0\n",
      "Epoch 4601/150000, Train Loss: 26277, Val Loss: 26731,  Learning Rate: 0.00092, Train Gradient: 188.3\n",
      "Epoch 4701/150000, Train Loss: 26210, Val Loss: 26663,  Learning Rate: 0.00092, Train Gradient: 187.6\n",
      "Epoch 4801/150000, Train Loss: 26143, Val Loss: 26595,  Learning Rate: 0.00092, Train Gradient: 186.9\n",
      "Epoch 4901/150000, Train Loss: 26077, Val Loss: 26528,  Learning Rate: 0.00092, Train Gradient: 186.2\n",
      "Epoch 5001/150000, Train Loss: 26010, Val Loss: 26460,  Learning Rate: 0.00093, Train Gradient: 185.4\n",
      "Epoch 5101/150000, Train Loss: 25944, Val Loss: 26393,  Learning Rate: 0.00093, Train Gradient: 184.7\n",
      "Epoch 5201/150000, Train Loss: 25878, Val Loss: 26326,  Learning Rate: 0.00093, Train Gradient: 184.0\n",
      "Epoch 5301/150000, Train Loss: 25813, Val Loss: 26260,  Learning Rate: 0.00093, Train Gradient: 183.3\n",
      "Epoch 5401/150000, Train Loss: 25747, Val Loss: 26194,  Learning Rate: 0.00093, Train Gradient: 182.6\n",
      "Epoch 5501/150000, Train Loss: 25682, Val Loss: 26128,  Learning Rate: 0.00093, Train Gradient: 181.9\n",
      "Epoch 5601/150000, Train Loss: 25617, Val Loss: 26062,  Learning Rate: 0.00093, Train Gradient: 181.2\n",
      "Epoch 5701/150000, Train Loss: 25553, Val Loss: 25996,  Learning Rate: 0.00093, Train Gradient: 180.4\n",
      "Epoch 5801/150000, Train Loss: 25488, Val Loss: 25931,  Learning Rate: 0.00093, Train Gradient: 179.7\n",
      "Epoch 5901/150000, Train Loss: 25424, Val Loss: 25866,  Learning Rate: 0.00093, Train Gradient: 179.0\n",
      "Epoch 6001/150000, Train Loss: 25360, Val Loss: 25801,  Learning Rate: 0.00093, Train Gradient: 178.3\n",
      "Epoch 6101/150000, Train Loss: 25297, Val Loss: 25737,  Learning Rate: 0.00093, Train Gradient: 177.6\n",
      "Epoch 6201/150000, Train Loss: 25233, Val Loss: 25673,  Learning Rate: 0.00093, Train Gradient: 176.9\n",
      "Epoch 6301/150000, Train Loss: 25170, Val Loss: 25609,  Learning Rate: 0.00093, Train Gradient: 176.1\n",
      "Epoch 6401/150000, Train Loss: 25107, Val Loss: 25545,  Learning Rate: 0.00093, Train Gradient: 175.4\n",
      "Epoch 6501/150000, Train Loss: 25045, Val Loss: 25481,  Learning Rate: 0.00093, Train Gradient: 174.7\n",
      "Epoch 6601/150000, Train Loss: 24982, Val Loss: 25418,  Learning Rate: 0.00093, Train Gradient: 174.0\n",
      "Epoch 6701/150000, Train Loss: 24920, Val Loss: 25355,  Learning Rate: 0.00093, Train Gradient: 173.3\n",
      "Epoch 6801/150000, Train Loss: 24858, Val Loss: 25292,  Learning Rate: 0.00093, Train Gradient: 172.6\n",
      "Epoch 6901/150000, Train Loss: 24797, Val Loss: 25230,  Learning Rate: 0.00093, Train Gradient: 171.9\n",
      "Epoch 7001/150000, Train Loss: 24735, Val Loss: 25167,  Learning Rate: 0.00093, Train Gradient: 171.1\n",
      "Epoch 7101/150000, Train Loss: 24674, Val Loss: 25105,  Learning Rate: 0.00093, Train Gradient: 170.4\n",
      "Epoch 7201/150000, Train Loss: 24613, Val Loss: 25044,  Learning Rate: 0.00093, Train Gradient: 169.7\n",
      "Epoch 7301/150000, Train Loss: 24553, Val Loss: 24982,  Learning Rate: 0.00093, Train Gradient: 169.0\n",
      "Epoch 7401/150000, Train Loss: 24492, Val Loss: 24921,  Learning Rate: 0.00093, Train Gradient: 168.3\n",
      "Epoch 7501/150000, Train Loss: 24432, Val Loss: 24860,  Learning Rate: 0.00093, Train Gradient: 167.6\n",
      "Epoch 7601/150000, Train Loss: 24372, Val Loss: 24799,  Learning Rate: 0.00093, Train Gradient: 166.8\n",
      "Epoch 7701/150000, Train Loss: 24312, Val Loss: 24738,  Learning Rate: 0.00093, Train Gradient: 166.1\n",
      "Epoch 7801/150000, Train Loss: 24253, Val Loss: 24678,  Learning Rate: 0.00093, Train Gradient: 165.4\n",
      "Epoch 7901/150000, Train Loss: 24194, Val Loss: 24618,  Learning Rate: 0.00093, Train Gradient: 164.7\n",
      "Epoch 8001/150000, Train Loss: 24135, Val Loss: 24558,  Learning Rate: 0.00094, Train Gradient: 164.0\n",
      "Epoch 8101/150000, Train Loss: 24076, Val Loss: 24499,  Learning Rate: 0.00094, Train Gradient: 163.3\n",
      "Epoch 8201/150000, Train Loss: 24018, Val Loss: 24439,  Learning Rate: 0.00094, Train Gradient: 162.5\n",
      "Epoch 8301/150000, Train Loss: 23960, Val Loss: 24380,  Learning Rate: 0.00094, Train Gradient: 161.8\n",
      "Epoch 8401/150000, Train Loss: 23902, Val Loss: 24322,  Learning Rate: 0.00094, Train Gradient: 161.1\n",
      "Epoch 8501/150000, Train Loss: 23844, Val Loss: 24263,  Learning Rate: 0.00094, Train Gradient: 160.4\n",
      "Epoch 8601/150000, Train Loss: 23787, Val Loss: 24205,  Learning Rate: 0.00094, Train Gradient: 159.7\n",
      "Epoch 8701/150000, Train Loss: 23730, Val Loss: 24147,  Learning Rate: 0.00094, Train Gradient: 159.0\n",
      "Epoch 8801/150000, Train Loss: 23673, Val Loss: 24089,  Learning Rate: 0.00094, Train Gradient: 158.2\n",
      "Epoch 8901/150000, Train Loss: 23616, Val Loss: 24031,  Learning Rate: 0.00094, Train Gradient: 157.5\n",
      "Epoch 9001/150000, Train Loss: 23560, Val Loss: 23974,  Learning Rate: 0.00094, Train Gradient: 156.8\n",
      "Epoch 9101/150000, Train Loss: 23504, Val Loss: 23917,  Learning Rate: 0.00094, Train Gradient: 156.1\n",
      "Epoch 9201/150000, Train Loss: 23448, Val Loss: 23860,  Learning Rate: 0.00094, Train Gradient: 155.4\n",
      "Epoch 9301/150000, Train Loss: 23392, Val Loss: 23804,  Learning Rate: 0.00094, Train Gradient: 154.6\n",
      "Epoch 9401/150000, Train Loss: 23337, Val Loss: 23748,  Learning Rate: 0.00094, Train Gradient: 153.9\n",
      "Epoch 9501/150000, Train Loss: 23282, Val Loss: 23692,  Learning Rate: 0.00094, Train Gradient: 153.2\n",
      "Epoch 9601/150000, Train Loss: 23227, Val Loss: 23636,  Learning Rate: 0.00094, Train Gradient: 152.5\n",
      "Epoch 9701/150000, Train Loss: 23172, Val Loss: 23580,  Learning Rate: 0.00094, Train Gradient: 151.8\n",
      "Epoch 9801/150000, Train Loss: 23118, Val Loss: 23525,  Learning Rate: 0.00094, Train Gradient: 151.1\n",
      "Epoch 9901/150000, Train Loss: 23064, Val Loss: 23470,  Learning Rate: 0.00094, Train Gradient: 150.3\n",
      "Epoch 10001/150000, Train Loss: 23010, Val Loss: 23415,  Learning Rate: 0.00094, Train Gradient: 149.6\n",
      "Epoch 10101/150000, Train Loss: 22943, Val Loss: 23350,  Learning Rate: 0.00094, Train Gradient: 151.9\n",
      "Epoch 10201/150000, Train Loss: 22829, Val Loss: 23251,  Learning Rate: 0.00094, Train Gradient: 157.0\n",
      "Epoch 10301/150000, Train Loss: 22768, Val Loss: 23189,  Learning Rate: 0.00094, Train Gradient: 156.6\n",
      "Epoch 10401/150000, Train Loss: 22709, Val Loss: 23128,  Learning Rate: 0.00094, Train Gradient: 156.2\n",
      "Epoch 10501/150000, Train Loss: 22650, Val Loss: 23067,  Learning Rate: 0.00094, Train Gradient: 155.7\n",
      "Epoch 10601/150000, Train Loss: 22591, Val Loss: 23007,  Learning Rate: 0.00094, Train Gradient: 155.3\n",
      "Epoch 10701/150000, Train Loss: 22532, Val Loss: 22947,  Learning Rate: 0.00094, Train Gradient: 154.9\n",
      "Epoch 10801/150000, Train Loss: 22474, Val Loss: 22887,  Learning Rate: 0.00094, Train Gradient: 154.5\n",
      "Epoch 10901/150000, Train Loss: 22416, Val Loss: 22827,  Learning Rate: 0.00094, Train Gradient: 154.0\n",
      "Epoch 11001/150000, Train Loss: 22358, Val Loss: 22768,  Learning Rate: 0.00094, Train Gradient: 153.6\n",
      "Epoch 11101/150000, Train Loss: 22301, Val Loss: 22710,  Learning Rate: 0.00094, Train Gradient: 153.1\n",
      "Epoch 11201/150000, Train Loss: 22244, Val Loss: 22652,  Learning Rate: 0.00094, Train Gradient: 152.7\n",
      "Epoch 11301/150000, Train Loss: 22187, Val Loss: 22594,  Learning Rate: 0.00095, Train Gradient: 152.3\n",
      "Epoch 11401/150000, Train Loss: 22130, Val Loss: 22537,  Learning Rate: 0.00095, Train Gradient: 151.8\n",
      "Epoch 11501/150000, Train Loss: 22074, Val Loss: 22480,  Learning Rate: 0.00095, Train Gradient: 151.4\n",
      "Epoch 11601/150000, Train Loss: 22018, Val Loss: 22423,  Learning Rate: 0.00095, Train Gradient: 151.0\n",
      "Epoch 11701/150000, Train Loss: 21962, Val Loss: 22366,  Learning Rate: 0.00095, Train Gradient: 150.6\n",
      "Epoch 11801/150000, Train Loss: 21906, Val Loss: 22310,  Learning Rate: 0.00095, Train Gradient: 150.2\n",
      "Epoch 11901/150000, Train Loss: 21850, Val Loss: 22253,  Learning Rate: 0.00095, Train Gradient: 149.8\n",
      "Epoch 12001/150000, Train Loss: 21795, Val Loss: 22197,  Learning Rate: 0.00095, Train Gradient: 149.3\n",
      "Epoch 12101/150000, Train Loss: 21739, Val Loss: 22140,  Learning Rate: 0.00095, Train Gradient: 148.9\n",
      "Epoch 12201/150000, Train Loss: 21684, Val Loss: 22084,  Learning Rate: 0.00095, Train Gradient: 148.5\n",
      "Epoch 12301/150000, Train Loss: 21629, Val Loss: 22029,  Learning Rate: 0.00095, Train Gradient: 148.1\n",
      "Epoch 12401/150000, Train Loss: 21574, Val Loss: 21973,  Learning Rate: 0.00095, Train Gradient: 147.8\n",
      "Epoch 12501/150000, Train Loss: 21520, Val Loss: 21917,  Learning Rate: 0.00095, Train Gradient: 147.4\n",
      "Epoch 12601/150000, Train Loss: 21465, Val Loss: 21862,  Learning Rate: 0.00095, Train Gradient: 147.0\n",
      "Epoch 12701/150000, Train Loss: 21411, Val Loss: 21807,  Learning Rate: 0.00095, Train Gradient: 146.6\n",
      "Epoch 12801/150000, Train Loss: 21357, Val Loss: 21752,  Learning Rate: 0.00095, Train Gradient: 146.2\n",
      "Epoch 12901/150000, Train Loss: 21303, Val Loss: 21697,  Learning Rate: 0.00095, Train Gradient: 145.8\n",
      "Epoch 13001/150000, Train Loss: 21249, Val Loss: 21642,  Learning Rate: 0.00095, Train Gradient: 145.4\n",
      "Epoch 13101/150000, Train Loss: 21195, Val Loss: 21587,  Learning Rate: 0.00095, Train Gradient: 145.0\n",
      "Epoch 13201/150000, Train Loss: 21141, Val Loss: 21533,  Learning Rate: 0.00095, Train Gradient: 144.7\n",
      "Epoch 13301/150000, Train Loss: 21088, Val Loss: 21479,  Learning Rate: 0.00095, Train Gradient: 144.3\n",
      "Epoch 13401/150000, Train Loss: 21034, Val Loss: 21424,  Learning Rate: 0.00095, Train Gradient: 143.9\n",
      "Epoch 13501/150000, Train Loss: 20981, Val Loss: 21370,  Learning Rate: 0.00095, Train Gradient: 143.5\n",
      "Epoch 13601/150000, Train Loss: 20928, Val Loss: 21316,  Learning Rate: 0.00095, Train Gradient: 143.1\n",
      "Epoch 13701/150000, Train Loss: 20875, Val Loss: 21262,  Learning Rate: 0.00095, Train Gradient: 142.8\n",
      "Epoch 13801/150000, Train Loss: 20822, Val Loss: 21209,  Learning Rate: 0.00095, Train Gradient: 142.4\n",
      "Epoch 13901/150000, Train Loss: 20769, Val Loss: 21155,  Learning Rate: 0.00095, Train Gradient: 142.0\n",
      "Epoch 14001/150000, Train Loss: 20716, Val Loss: 21102,  Learning Rate: 0.00095, Train Gradient: 141.7\n",
      "Epoch 14101/150000, Train Loss: 20664, Val Loss: 21048,  Learning Rate: 0.00095, Train Gradient: 141.3\n",
      "Epoch 14201/150000, Train Loss: 20611, Val Loss: 20995,  Learning Rate: 0.00095, Train Gradient: 140.9\n",
      "Epoch 14301/150000, Train Loss: 20559, Val Loss: 20942,  Learning Rate: 0.00095, Train Gradient: 140.6\n",
      "Epoch 14401/150000, Train Loss: 20507, Val Loss: 20889,  Learning Rate: 0.00095, Train Gradient: 140.2\n",
      "Epoch 14501/150000, Train Loss: 20455, Val Loss: 20836,  Learning Rate: 0.00095, Train Gradient: 139.8\n",
      "Epoch 14601/150000, Train Loss: 20403, Val Loss: 20783,  Learning Rate: 0.00095, Train Gradient: 139.5\n",
      "Epoch 14701/150000, Train Loss: 20351, Val Loss: 20730,  Learning Rate: 0.00095, Train Gradient: 139.1\n",
      "Epoch 14801/150000, Train Loss: 20300, Val Loss: 20678,  Learning Rate: 0.00095, Train Gradient: 138.8\n",
      "Epoch 14901/150000, Train Loss: 20248, Val Loss: 20626,  Learning Rate: 0.00095, Train Gradient: 138.4\n",
      "Epoch 15001/150000, Train Loss: 20197, Val Loss: 20573,  Learning Rate: 0.00095, Train Gradient: 138.1\n",
      "Epoch 15101/150000, Train Loss: 20145, Val Loss: 20521,  Learning Rate: 0.00095, Train Gradient: 137.7\n",
      "Epoch 15201/150000, Train Loss: 20094, Val Loss: 20469,  Learning Rate: 0.00095, Train Gradient: 137.4\n",
      "Epoch 15301/150000, Train Loss: 20043, Val Loss: 20417,  Learning Rate: 0.00095, Train Gradient: 137.0\n",
      "Epoch 15401/150000, Train Loss: 19992, Val Loss: 20365,  Learning Rate: 0.00096, Train Gradient: 136.7\n",
      "Epoch 15501/150000, Train Loss: 19941, Val Loss: 20314,  Learning Rate: 0.00096, Train Gradient: 136.3\n",
      "Epoch 15601/150000, Train Loss: 19891, Val Loss: 20262,  Learning Rate: 0.00096, Train Gradient: 136.0\n",
      "Epoch 15701/150000, Train Loss: 19840, Val Loss: 20211,  Learning Rate: 0.00096, Train Gradient: 135.6\n",
      "Epoch 15801/150000, Train Loss: 19790, Val Loss: 20159,  Learning Rate: 0.00096, Train Gradient: 135.3\n",
      "Epoch 15901/150000, Train Loss: 19739, Val Loss: 20108,  Learning Rate: 0.00096, Train Gradient: 134.9\n",
      "Epoch 16001/150000, Train Loss: 19689, Val Loss: 20057,  Learning Rate: 0.00096, Train Gradient: 134.6\n",
      "Epoch 16101/150000, Train Loss: 19639, Val Loss: 20006,  Learning Rate: 0.00096, Train Gradient: 134.2\n",
      "Epoch 16201/150000, Train Loss: 19589, Val Loss: 19955,  Learning Rate: 0.00096, Train Gradient: 133.9\n",
      "Epoch 16301/150000, Train Loss: 19539, Val Loss: 19905,  Learning Rate: 0.00096, Train Gradient: 133.6\n",
      "Epoch 16401/150000, Train Loss: 19489, Val Loss: 19854,  Learning Rate: 0.00096, Train Gradient: 133.2\n",
      "Epoch 16501/150000, Train Loss: 19440, Val Loss: 19804,  Learning Rate: 0.00096, Train Gradient: 132.9\n",
      "Epoch 16601/150000, Train Loss: 19390, Val Loss: 19753,  Learning Rate: 0.00096, Train Gradient: 132.5\n",
      "Epoch 16701/150000, Train Loss: 19341, Val Loss: 19703,  Learning Rate: 0.00096, Train Gradient: 132.2\n",
      "Epoch 16801/150000, Train Loss: 19291, Val Loss: 19653,  Learning Rate: 0.00096, Train Gradient: 131.9\n",
      "Epoch 16901/150000, Train Loss: 19242, Val Loss: 19603,  Learning Rate: 0.00096, Train Gradient: 131.5\n",
      "Epoch 17001/150000, Train Loss: 19193, Val Loss: 19553,  Learning Rate: 0.00096, Train Gradient: 131.2\n",
      "Epoch 17101/150000, Train Loss: 19144, Val Loss: 19503,  Learning Rate: 0.00096, Train Gradient: 130.9\n",
      "Epoch 17201/150000, Train Loss: 19095, Val Loss: 19453,  Learning Rate: 0.00096, Train Gradient: 130.5\n",
      "Epoch 17301/150000, Train Loss: 19046, Val Loss: 19404,  Learning Rate: 0.00096, Train Gradient: 130.2\n",
      "Epoch 17401/150000, Train Loss: 18998, Val Loss: 19354,  Learning Rate: 0.00096, Train Gradient: 129.9\n",
      "Epoch 17501/150000, Train Loss: 18949, Val Loss: 19305,  Learning Rate: 0.00096, Train Gradient: 129.6\n",
      "Epoch 17601/150000, Train Loss: 18901, Val Loss: 19256,  Learning Rate: 0.00096, Train Gradient: 129.2\n",
      "Epoch 17701/150000, Train Loss: 18853, Val Loss: 19207,  Learning Rate: 0.00096, Train Gradient: 128.9\n",
      "Epoch 17801/150000, Train Loss: 18804, Val Loss: 19158,  Learning Rate: 0.00096, Train Gradient: 128.5\n",
      "Epoch 17901/150000, Train Loss: 18756, Val Loss: 19109,  Learning Rate: 0.00096, Train Gradient: 128.2\n",
      "Epoch 18001/150000, Train Loss: 18708, Val Loss: 19060,  Learning Rate: 0.00096, Train Gradient: 128.0\n",
      "Epoch 18101/150000, Train Loss: 18660, Val Loss: 19012,  Learning Rate: 0.00096, Train Gradient: 127.6\n",
      "Epoch 18201/150000, Train Loss: 18613, Val Loss: 18963,  Learning Rate: 0.00096, Train Gradient: 127.3\n",
      "Epoch 18301/150000, Train Loss: 18565, Val Loss: 18915,  Learning Rate: 0.00096, Train Gradient: 126.9\n",
      "Epoch 18401/150000, Train Loss: 18518, Val Loss: 18867,  Learning Rate: 0.00096, Train Gradient: 126.6\n",
      "Epoch 18501/150000, Train Loss: 18470, Val Loss: 18819,  Learning Rate: 0.00096, Train Gradient: 126.3\n",
      "Epoch 18601/150000, Train Loss: 18423, Val Loss: 18771,  Learning Rate: 0.00096, Train Gradient: 126.0\n",
      "Epoch 18701/150000, Train Loss: 18376, Val Loss: 18723,  Learning Rate: 0.00096, Train Gradient: 125.7\n",
      "Epoch 18801/150000, Train Loss: 18329, Val Loss: 18675,  Learning Rate: 0.00096, Train Gradient: 125.3\n",
      "Epoch 18901/150000, Train Loss: 18282, Val Loss: 18627,  Learning Rate: 0.00096, Train Gradient: 125.0\n",
      "Epoch 19001/150000, Train Loss: 18235, Val Loss: 18579,  Learning Rate: 0.00096, Train Gradient: 124.7\n",
      "Epoch 19101/150000, Train Loss: 18188, Val Loss: 18532,  Learning Rate: 0.00096, Train Gradient: 124.4\n",
      "Epoch 19201/150000, Train Loss: 18141, Val Loss: 18484,  Learning Rate: 0.00096, Train Gradient: 124.1\n",
      "Epoch 19301/150000, Train Loss: 18095, Val Loss: 18437,  Learning Rate: 0.00096, Train Gradient: 123.8\n",
      "Epoch 19401/150000, Train Loss: 18048, Val Loss: 18390,  Learning Rate: 0.00096, Train Gradient: 123.5\n",
      "Epoch 19501/150000, Train Loss: 18002, Val Loss: 18343,  Learning Rate: 0.00096, Train Gradient: 123.1\n",
      "Epoch 19601/150000, Train Loss: 17956, Val Loss: 18296,  Learning Rate: 0.00096, Train Gradient: 122.8\n",
      "Epoch 19701/150000, Train Loss: 17910, Val Loss: 18249,  Learning Rate: 0.00096, Train Gradient: 122.5\n",
      "Epoch 19801/150000, Train Loss: 17864, Val Loss: 18203,  Learning Rate: 0.00096, Train Gradient: 122.2\n",
      "Epoch 19901/150000, Train Loss: 17818, Val Loss: 18156,  Learning Rate: 0.00096, Train Gradient: 121.9\n",
      "Epoch 20001/150000, Train Loss: 17772, Val Loss: 18109,  Learning Rate: 0.00096, Train Gradient: 121.6\n",
      "Epoch 20101/150000, Train Loss: 17727, Val Loss: 18063,  Learning Rate: 0.00096, Train Gradient: 121.2\n",
      "Epoch 20201/150000, Train Loss: 17681, Val Loss: 18017,  Learning Rate: 0.00096, Train Gradient: 120.9\n",
      "Epoch 20301/150000, Train Loss: 17636, Val Loss: 17971,  Learning Rate: 0.00096, Train Gradient: 120.6\n",
      "Epoch 20401/150000, Train Loss: 17590, Val Loss: 17925,  Learning Rate: 0.00097, Train Gradient: 120.3\n",
      "Epoch 20501/150000, Train Loss: 17545, Val Loss: 17879,  Learning Rate: 0.00097, Train Gradient: 120.0\n",
      "Epoch 20601/150000, Train Loss: 17500, Val Loss: 17833,  Learning Rate: 0.00097, Train Gradient: 119.7\n",
      "Epoch 20701/150000, Train Loss: 17455, Val Loss: 17787,  Learning Rate: 0.00097, Train Gradient: 119.4\n",
      "Epoch 20801/150000, Train Loss: 17410, Val Loss: 17742,  Learning Rate: 0.00097, Train Gradient: 119.1\n",
      "Epoch 20901/150000, Train Loss: 17365, Val Loss: 17696,  Learning Rate: 0.00097, Train Gradient: 118.8\n",
      "Epoch 21001/150000, Train Loss: 17321, Val Loss: 17651,  Learning Rate: 0.00097, Train Gradient: 118.4\n",
      "Epoch 21101/150000, Train Loss: 17276, Val Loss: 17606,  Learning Rate: 0.00097, Train Gradient: 118.1\n",
      "Epoch 21201/150000, Train Loss: 17232, Val Loss: 17561,  Learning Rate: 0.00097, Train Gradient: 117.8\n",
      "Epoch 21301/150000, Train Loss: 17187, Val Loss: 17516,  Learning Rate: 0.00097, Train Gradient: 117.5\n",
      "Epoch 21401/150000, Train Loss: 17143, Val Loss: 17471,  Learning Rate: 0.00097, Train Gradient: 117.2\n",
      "Epoch 21501/150000, Train Loss: 17099, Val Loss: 17426,  Learning Rate: 0.00097, Train Gradient: 116.9\n",
      "Epoch 21601/150000, Train Loss: 17055, Val Loss: 17381,  Learning Rate: 0.00097, Train Gradient: 116.6\n",
      "Epoch 21701/150000, Train Loss: 17011, Val Loss: 17336,  Learning Rate: 0.00097, Train Gradient: 116.3\n",
      "Epoch 21801/150000, Train Loss: 16967, Val Loss: 17292,  Learning Rate: 0.00097, Train Gradient: 116.0\n",
      "Epoch 21901/150000, Train Loss: 16924, Val Loss: 17248,  Learning Rate: 0.00097, Train Gradient: 115.7\n",
      "Epoch 22001/150000, Train Loss: 16880, Val Loss: 17203,  Learning Rate: 0.00097, Train Gradient: 115.4\n",
      "Epoch 22101/150000, Train Loss: 16837, Val Loss: 17159,  Learning Rate: 0.00097, Train Gradient: 115.1\n",
      "Epoch 22201/150000, Train Loss: 16794, Val Loss: 17115,  Learning Rate: 0.00097, Train Gradient: 114.7\n",
      "Epoch 22301/150000, Train Loss: 16750, Val Loss: 17071,  Learning Rate: 0.00097, Train Gradient: 114.4\n",
      "Epoch 22401/150000, Train Loss: 16707, Val Loss: 17028,  Learning Rate: 0.00097, Train Gradient: 114.1\n",
      "Epoch 22501/150000, Train Loss: 16664, Val Loss: 16984,  Learning Rate: 0.00097, Train Gradient: 113.8\n",
      "Epoch 22601/150000, Train Loss: 16621, Val Loss: 16940,  Learning Rate: 0.00097, Train Gradient: 113.5\n",
      "Epoch 22701/150000, Train Loss: 16579, Val Loss: 16897,  Learning Rate: 0.00097, Train Gradient: 113.2\n",
      "Epoch 22801/150000, Train Loss: 16536, Val Loss: 16854,  Learning Rate: 0.00097, Train Gradient: 112.9\n",
      "Epoch 22901/150000, Train Loss: 16493, Val Loss: 16810,  Learning Rate: 0.00097, Train Gradient: 112.6\n",
      "Epoch 23001/150000, Train Loss: 16451, Val Loss: 16767,  Learning Rate: 0.00097, Train Gradient: 112.3\n",
      "Epoch 23101/150000, Train Loss: 16409, Val Loss: 16724,  Learning Rate: 0.00097, Train Gradient: 112.0\n",
      "Epoch 23201/150000, Train Loss: 16366, Val Loss: 16681,  Learning Rate: 0.00097, Train Gradient: 111.7\n",
      "Epoch 23301/150000, Train Loss: 16324, Val Loss: 16638,  Learning Rate: 0.00097, Train Gradient: 111.4\n",
      "Epoch 23401/150000, Train Loss: 16282, Val Loss: 16596,  Learning Rate: 0.00097, Train Gradient: 111.1\n",
      "Epoch 23501/150000, Train Loss: 16240, Val Loss: 16553,  Learning Rate: 0.00097, Train Gradient: 110.8\n",
      "Epoch 23601/150000, Train Loss: 16198, Val Loss: 16511,  Learning Rate: 0.00097, Train Gradient: 110.5\n",
      "Epoch 23701/150000, Train Loss: 16157, Val Loss: 16469,  Learning Rate: 0.00097, Train Gradient: 110.2\n",
      "Epoch 23801/150000, Train Loss: 16115, Val Loss: 16426,  Learning Rate: 0.00097, Train Gradient: 109.9\n",
      "Epoch 23901/150000, Train Loss: 16074, Val Loss: 16384,  Learning Rate: 0.00097, Train Gradient: 109.6\n",
      "Epoch 24001/150000, Train Loss: 16032, Val Loss: 16342,  Learning Rate: 0.00097, Train Gradient: 109.3\n",
      "Epoch 24101/150000, Train Loss: 15991, Val Loss: 16301,  Learning Rate: 0.00097, Train Gradient: 109.0\n",
      "Epoch 24201/150000, Train Loss: 15950, Val Loss: 16259,  Learning Rate: 0.00097, Train Gradient: 108.7\n",
      "Epoch 24301/150000, Train Loss: 15909, Val Loss: 16217,  Learning Rate: 0.00097, Train Gradient: 108.3\n",
      "Epoch 24401/150000, Train Loss: 15868, Val Loss: 16176,  Learning Rate: 0.00097, Train Gradient: 108.1\n",
      "Epoch 24501/150000, Train Loss: 15827, Val Loss: 16135,  Learning Rate: 0.00097, Train Gradient: 107.8\n",
      "Epoch 24601/150000, Train Loss: 15787, Val Loss: 16093,  Learning Rate: 0.00097, Train Gradient: 107.5\n",
      "Epoch 24701/150000, Train Loss: 15746, Val Loss: 16052,  Learning Rate: 0.00097, Train Gradient: 107.2\n",
      "Epoch 24801/150000, Train Loss: 15706, Val Loss: 16011,  Learning Rate: 0.00097, Train Gradient: 106.9\n",
      "Epoch 24901/150000, Train Loss: 15665, Val Loss: 15970,  Learning Rate: 0.00097, Train Gradient: 106.6\n",
      "Epoch 25001/150000, Train Loss: 15625, Val Loss: 15929,  Learning Rate: 0.00097, Train Gradient: 106.3\n",
      "Epoch 25101/150000, Train Loss: 15585, Val Loss: 15888,  Learning Rate: 0.00097, Train Gradient: 106.0\n",
      "Epoch 25201/150000, Train Loss: 15545, Val Loss: 15848,  Learning Rate: 0.00097, Train Gradient: 105.7\n",
      "Epoch 25301/150000, Train Loss: 15505, Val Loss: 15807,  Learning Rate: 0.00097, Train Gradient: 105.4\n",
      "Epoch 25401/150000, Train Loss: 15465, Val Loss: 15767,  Learning Rate: 0.00097, Train Gradient: 105.2\n",
      "Epoch 25501/150000, Train Loss: 15425, Val Loss: 15726,  Learning Rate: 0.00097, Train Gradient: 104.9\n",
      "Epoch 25601/150000, Train Loss: 15385, Val Loss: 15686,  Learning Rate: 0.00097, Train Gradient: 104.6\n",
      "Epoch 25701/150000, Train Loss: 15346, Val Loss: 15646,  Learning Rate: 0.00097, Train Gradient: 104.3\n",
      "Epoch 25801/150000, Train Loss: 15306, Val Loss: 15606,  Learning Rate: 0.00097, Train Gradient: 104.0\n",
      "Epoch 25901/150000, Train Loss: 15267, Val Loss: 15566,  Learning Rate: 0.00097, Train Gradient: 103.7\n",
      "Epoch 26001/150000, Train Loss: 15228, Val Loss: 15527,  Learning Rate: 0.00097, Train Gradient: 103.4\n",
      "Epoch 26101/150000, Train Loss: 15189, Val Loss: 15487,  Learning Rate: 0.00097, Train Gradient: 103.1\n",
      "Epoch 26201/150000, Train Loss: 15150, Val Loss: 15448,  Learning Rate: 0.00097, Train Gradient: 102.8\n",
      "Epoch 26301/150000, Train Loss: 15111, Val Loss: 15408,  Learning Rate: 0.00097, Train Gradient: 102.5\n",
      "Epoch 26401/150000, Train Loss: 15072, Val Loss: 15369,  Learning Rate: 0.00097, Train Gradient: 102.2\n",
      "Epoch 26501/150000, Train Loss: 15034, Val Loss: 15329,  Learning Rate: 0.00097, Train Gradient: 102.0\n",
      "Epoch 26601/150000, Train Loss: 14995, Val Loss: 15290,  Learning Rate: 0.00097, Train Gradient: 101.7\n",
      "Epoch 26701/150000, Train Loss: 14956, Val Loss: 15251,  Learning Rate: 0.00097, Train Gradient: 101.5\n",
      "Epoch 26801/150000, Train Loss: 14918, Val Loss: 15212,  Learning Rate: 0.00097, Train Gradient: 101.2\n",
      "Epoch 26901/150000, Train Loss: 14880, Val Loss: 15173,  Learning Rate: 0.00097, Train Gradient: 100.9\n",
      "Epoch 27001/150000, Train Loss: 14841, Val Loss: 15135,  Learning Rate: 0.00098, Train Gradient: 100.7\n",
      "Epoch 27101/150000, Train Loss: 14803, Val Loss: 15096,  Learning Rate: 0.00098, Train Gradient: 100.4\n",
      "Epoch 27201/150000, Train Loss: 14765, Val Loss: 15057,  Learning Rate: 0.00098, Train Gradient: 100.1\n",
      "Epoch 27301/150000, Train Loss: 14727, Val Loss: 15019,  Learning Rate: 0.00098, Train Gradient: 99.8\n",
      "Epoch 27401/150000, Train Loss: 14689, Val Loss: 14981,  Learning Rate: 0.00098, Train Gradient: 99.6\n",
      "Epoch 27501/150000, Train Loss: 14652, Val Loss: 14942,  Learning Rate: 0.00098, Train Gradient: 99.3\n",
      "Epoch 27601/150000, Train Loss: 14614, Val Loss: 14904,  Learning Rate: 0.00098, Train Gradient: 99.0\n",
      "Epoch 27701/150000, Train Loss: 14576, Val Loss: 14866,  Learning Rate: 0.00098, Train Gradient: 98.8\n",
      "Epoch 27801/150000, Train Loss: 14539, Val Loss: 14828,  Learning Rate: 0.00098, Train Gradient: 98.5\n",
      "Epoch 27901/150000, Train Loss: 14501, Val Loss: 14790,  Learning Rate: 0.00098, Train Gradient: 98.3\n",
      "Epoch 28001/150000, Train Loss: 14464, Val Loss: 14752,  Learning Rate: 0.00098, Train Gradient: 97.9\n",
      "Epoch 28101/150000, Train Loss: 14427, Val Loss: 14715,  Learning Rate: 0.00098, Train Gradient: 97.6\n",
      "Epoch 28201/150000, Train Loss: 14390, Val Loss: 14677,  Learning Rate: 0.00098, Train Gradient: 97.4\n",
      "Epoch 28301/150000, Train Loss: 14353, Val Loss: 14640,  Learning Rate: 0.00098, Train Gradient: 97.1\n",
      "Epoch 28401/150000, Train Loss: 14316, Val Loss: 14602,  Learning Rate: 0.00098, Train Gradient: 96.9\n",
      "Epoch 28501/150000, Train Loss: 14279, Val Loss: 14565,  Learning Rate: 0.00098, Train Gradient: 96.6\n",
      "Epoch 28601/150000, Train Loss: 14243, Val Loss: 14528,  Learning Rate: 0.00098, Train Gradient: 96.3\n",
      "Epoch 28701/150000, Train Loss: 14206, Val Loss: 14491,  Learning Rate: 0.00098, Train Gradient: 96.1\n",
      "Epoch 28801/150000, Train Loss: 14170, Val Loss: 14454,  Learning Rate: 0.00098, Train Gradient: 95.8\n",
      "Epoch 28901/150000, Train Loss: 14133, Val Loss: 14417,  Learning Rate: 0.00098, Train Gradient: 95.5\n",
      "Epoch 29001/150000, Train Loss: 14097, Val Loss: 14380,  Learning Rate: 0.00098, Train Gradient: 95.3\n",
      "Epoch 29101/150000, Train Loss: 14061, Val Loss: 14343,  Learning Rate: 0.00098, Train Gradient: 95.0\n",
      "Epoch 29201/150000, Train Loss: 14025, Val Loss: 14307,  Learning Rate: 0.00098, Train Gradient: 94.7\n",
      "Epoch 29301/150000, Train Loss: 13989, Val Loss: 14270,  Learning Rate: 0.00098, Train Gradient: 94.4\n",
      "Epoch 29401/150000, Train Loss: 13953, Val Loss: 14234,  Learning Rate: 0.00098, Train Gradient: 94.2\n",
      "Epoch 29501/150000, Train Loss: 13917, Val Loss: 14198,  Learning Rate: 0.00098, Train Gradient: 93.9\n",
      "Epoch 29601/150000, Train Loss: 13882, Val Loss: 14161,  Learning Rate: 0.00098, Train Gradient: 93.7\n",
      "Epoch 29701/150000, Train Loss: 13846, Val Loss: 14125,  Learning Rate: 0.00098, Train Gradient: 93.4\n",
      "Epoch 29801/150000, Train Loss: 13811, Val Loss: 14089,  Learning Rate: 0.00098, Train Gradient: 93.2\n",
      "Epoch 29901/150000, Train Loss: 13775, Val Loss: 14053,  Learning Rate: 0.00098, Train Gradient: 92.9\n",
      "Epoch 30001/150000, Train Loss: 13740, Val Loss: 14017,  Learning Rate: 0.00098, Train Gradient: 92.6\n",
      "Epoch 30101/150000, Train Loss: 13705, Val Loss: 13982,  Learning Rate: 0.00098, Train Gradient: 92.4\n",
      "Epoch 30201/150000, Train Loss: 13669, Val Loss: 13946,  Learning Rate: 0.00098, Train Gradient: 92.1\n",
      "Epoch 30301/150000, Train Loss: 13634, Val Loss: 13911,  Learning Rate: 0.00098, Train Gradient: 91.9\n",
      "Epoch 30401/150000, Train Loss: 13599, Val Loss: 13875,  Learning Rate: 0.00098, Train Gradient: 91.6\n",
      "Epoch 30501/150000, Train Loss: 13565, Val Loss: 13840,  Learning Rate: 0.00098, Train Gradient: 91.4\n",
      "Epoch 30601/150000, Train Loss: 13530, Val Loss: 13805,  Learning Rate: 0.00098, Train Gradient: 91.1\n",
      "Epoch 30701/150000, Train Loss: 13495, Val Loss: 13769,  Learning Rate: 0.00098, Train Gradient: 90.9\n",
      "Epoch 30801/150000, Train Loss: 13461, Val Loss: 13734,  Learning Rate: 0.00098, Train Gradient: 90.6\n",
      "Epoch 30901/150000, Train Loss: 13426, Val Loss: 13699,  Learning Rate: 0.00098, Train Gradient: 90.4\n",
      "Epoch 31001/150000, Train Loss: 13392, Val Loss: 13664,  Learning Rate: 0.00098, Train Gradient: 90.2\n",
      "Epoch 31101/150000, Train Loss: 13358, Val Loss: 13630,  Learning Rate: 0.00098, Train Gradient: 89.9\n",
      "Epoch 31201/150000, Train Loss: 13323, Val Loss: 13595,  Learning Rate: 0.00098, Train Gradient: 89.6\n",
      "Epoch 31301/150000, Train Loss: 13289, Val Loss: 13560,  Learning Rate: 0.00098, Train Gradient: 89.4\n",
      "Epoch 31401/150000, Train Loss: 13255, Val Loss: 13526,  Learning Rate: 0.00098, Train Gradient: 89.1\n",
      "Epoch 31501/150000, Train Loss: 13221, Val Loss: 13491,  Learning Rate: 0.00098, Train Gradient: 88.9\n",
      "Epoch 31601/150000, Train Loss: 13187, Val Loss: 13457,  Learning Rate: 0.00098, Train Gradient: 88.7\n",
      "Epoch 31701/150000, Train Loss: 13153, Val Loss: 13423,  Learning Rate: 0.00098, Train Gradient: 88.4\n",
      "Epoch 31801/150000, Train Loss: 13120, Val Loss: 13389,  Learning Rate: 0.00098, Train Gradient: 88.2\n",
      "Epoch 31901/150000, Train Loss: 13086, Val Loss: 13354,  Learning Rate: 0.00098, Train Gradient: 87.9\n",
      "Epoch 32001/150000, Train Loss: 13053, Val Loss: 13321,  Learning Rate: 0.00098, Train Gradient: 87.7\n",
      "Epoch 32101/150000, Train Loss: 13019, Val Loss: 13287,  Learning Rate: 0.00098, Train Gradient: 87.5\n",
      "Epoch 32201/150000, Train Loss: 12986, Val Loss: 13253,  Learning Rate: 0.00098, Train Gradient: 87.2\n",
      "Epoch 32301/150000, Train Loss: 12953, Val Loss: 13219,  Learning Rate: 0.00098, Train Gradient: 86.9\n",
      "Epoch 32401/150000, Train Loss: 12919, Val Loss: 13186,  Learning Rate: 0.00098, Train Gradient: 86.8\n",
      "Epoch 32501/150000, Train Loss: 12886, Val Loss: 13152,  Learning Rate: 0.00098, Train Gradient: 86.5\n",
      "Epoch 32601/150000, Train Loss: 12853, Val Loss: 13119,  Learning Rate: 0.00098, Train Gradient: 86.3\n",
      "Epoch 32701/150000, Train Loss: 12821, Val Loss: 13085,  Learning Rate: 0.00098, Train Gradient: 86.0\n",
      "Epoch 32801/150000, Train Loss: 12788, Val Loss: 13052,  Learning Rate: 0.00098, Train Gradient: 85.8\n",
      "Epoch 32901/150000, Train Loss: 12755, Val Loss: 13019,  Learning Rate: 0.00098, Train Gradient: 85.6\n",
      "Epoch 33001/150000, Train Loss: 12722, Val Loss: 12986,  Learning Rate: 0.00098, Train Gradient: 85.3\n",
      "Epoch 33101/150000, Train Loss: 12690, Val Loss: 12953,  Learning Rate: 0.00098, Train Gradient: 85.1\n",
      "Epoch 33201/150000, Train Loss: 12657, Val Loss: 12920,  Learning Rate: 0.00098, Train Gradient: 84.9\n",
      "Epoch 33301/150000, Train Loss: 12625, Val Loss: 12887,  Learning Rate: 0.00098, Train Gradient: 84.6\n",
      "Epoch 33401/150000, Train Loss: 12593, Val Loss: 12854,  Learning Rate: 0.00098, Train Gradient: 84.4\n",
      "Epoch 33501/150000, Train Loss: 12560, Val Loss: 12822,  Learning Rate: 0.00098, Train Gradient: 84.2\n",
      "Epoch 33601/150000, Train Loss: 12528, Val Loss: 12789,  Learning Rate: 0.00098, Train Gradient: 84.0\n",
      "Epoch 33701/150000, Train Loss: 12496, Val Loss: 12757,  Learning Rate: 0.00098, Train Gradient: 83.7\n",
      "Epoch 33801/150000, Train Loss: 12464, Val Loss: 12724,  Learning Rate: 0.00098, Train Gradient: 83.5\n",
      "Epoch 33901/150000, Train Loss: 12432, Val Loss: 12692,  Learning Rate: 0.00098, Train Gradient: 83.3\n",
      "Epoch 34001/150000, Train Loss: 12401, Val Loss: 12660,  Learning Rate: 0.00098, Train Gradient: 83.1\n",
      "Epoch 34101/150000, Train Loss: 12369, Val Loss: 12627,  Learning Rate: 0.00098, Train Gradient: 82.9\n",
      "Epoch 34201/150000, Train Loss: 12337, Val Loss: 12595,  Learning Rate: 0.00098, Train Gradient: 82.7\n",
      "Epoch 34301/150000, Train Loss: 12305, Val Loss: 12563,  Learning Rate: 0.00098, Train Gradient: 82.5\n",
      "Epoch 34401/150000, Train Loss: 12274, Val Loss: 12531,  Learning Rate: 0.00098, Train Gradient: 82.2\n",
      "Epoch 34501/150000, Train Loss: 12242, Val Loss: 12499,  Learning Rate: 0.00098, Train Gradient: 82.0\n",
      "Epoch 34601/150000, Train Loss: 12211, Val Loss: 12467,  Learning Rate: 0.00098, Train Gradient: 81.8\n",
      "Epoch 34701/150000, Train Loss: 12180, Val Loss: 12435,  Learning Rate: 0.00098, Train Gradient: 81.6\n",
      "Epoch 34801/150000, Train Loss: 12149, Val Loss: 12404,  Learning Rate: 0.00098, Train Gradient: 81.4\n",
      "Epoch 34901/150000, Train Loss: 12117, Val Loss: 12372,  Learning Rate: 0.00098, Train Gradient: 81.1\n",
      "Epoch 35001/150000, Train Loss: 12086, Val Loss: 12340,  Learning Rate: 0.00098, Train Gradient: 80.9\n",
      "Epoch 35101/150000, Train Loss: 12055, Val Loss: 12309,  Learning Rate: 0.00098, Train Gradient: 80.7\n",
      "Epoch 35201/150000, Train Loss: 12025, Val Loss: 12278,  Learning Rate: 0.00098, Train Gradient: 80.5\n",
      "Epoch 35301/150000, Train Loss: 11994, Val Loss: 12246,  Learning Rate: 0.00098, Train Gradient: 80.3\n",
      "Epoch 35401/150000, Train Loss: 11963, Val Loss: 12215,  Learning Rate: 0.00098, Train Gradient: 80.1\n",
      "Epoch 35501/150000, Train Loss: 11932, Val Loss: 12184,  Learning Rate: 0.00098, Train Gradient: 79.9\n",
      "Epoch 35601/150000, Train Loss: 11902, Val Loss: 12153,  Learning Rate: 0.00098, Train Gradient: 79.7\n",
      "Epoch 35701/150000, Train Loss: 11871, Val Loss: 12122,  Learning Rate: 0.00098, Train Gradient: 79.5\n",
      "Epoch 35801/150000, Train Loss: 11841, Val Loss: 12092,  Learning Rate: 0.00098, Train Gradient: 79.2\n",
      "Epoch 35901/150000, Train Loss: 11811, Val Loss: 12061,  Learning Rate: 0.00098, Train Gradient: 79.0\n",
      "Epoch 36001/150000, Train Loss: 11780, Val Loss: 12031,  Learning Rate: 0.00098, Train Gradient: 78.8\n",
      "Epoch 36101/150000, Train Loss: 11750, Val Loss: 12001,  Learning Rate: 0.00098, Train Gradient: 78.6\n",
      "Epoch 36201/150000, Train Loss: 11720, Val Loss: 11971,  Learning Rate: 0.00098, Train Gradient: 78.4\n",
      "Epoch 36301/150000, Train Loss: 11690, Val Loss: 11941,  Learning Rate: 0.00098, Train Gradient: 78.2\n",
      "Epoch 36401/150000, Train Loss: 11660, Val Loss: 11911,  Learning Rate: 0.00098, Train Gradient: 78.0\n",
      "Epoch 36501/150000, Train Loss: 11630, Val Loss: 11880,  Learning Rate: 0.00098, Train Gradient: 77.8\n",
      "Epoch 36601/150000, Train Loss: 11601, Val Loss: 11851,  Learning Rate: 0.00098, Train Gradient: 77.6\n",
      "Epoch 36701/150000, Train Loss: 11571, Val Loss: 11821,  Learning Rate: 0.00098, Train Gradient: 77.4\n",
      "Epoch 36801/150000, Train Loss: 11541, Val Loss: 11791,  Learning Rate: 0.00098, Train Gradient: 77.2\n",
      "Epoch 36901/150000, Train Loss: 11512, Val Loss: 11761,  Learning Rate: 0.00098, Train Gradient: 77.0\n",
      "Epoch 37001/150000, Train Loss: 11482, Val Loss: 11732,  Learning Rate: 0.00098, Train Gradient: 76.8\n",
      "Epoch 37101/150000, Train Loss: 11453, Val Loss: 11702,  Learning Rate: 0.00099, Train Gradient: 76.6\n",
      "Epoch 37201/150000, Train Loss: 11424, Val Loss: 11674,  Learning Rate: 0.00099, Train Gradient: 76.4\n",
      "Epoch 37301/150000, Train Loss: 11395, Val Loss: 11645,  Learning Rate: 0.00099, Train Gradient: 76.2\n",
      "Epoch 37401/150000, Train Loss: 11365, Val Loss: 11616,  Learning Rate: 0.00099, Train Gradient: 76.0\n",
      "Epoch 37501/150000, Train Loss: 11336, Val Loss: 11587,  Learning Rate: 0.00099, Train Gradient: 75.8\n",
      "Epoch 37601/150000, Train Loss: 11307, Val Loss: 11558,  Learning Rate: 0.00099, Train Gradient: 75.6\n",
      "Epoch 37701/150000, Train Loss: 11278, Val Loss: 11529,  Learning Rate: 0.00099, Train Gradient: 75.4\n",
      "Epoch 37801/150000, Train Loss: 11249, Val Loss: 11499,  Learning Rate: 0.00099, Train Gradient: 75.2\n",
      "Epoch 37901/150000, Train Loss: 11220, Val Loss: 11470,  Learning Rate: 0.00099, Train Gradient: 75.0\n",
      "Epoch 38001/150000, Train Loss: 11192, Val Loss: 11442,  Learning Rate: 0.00099, Train Gradient: 74.8\n",
      "Epoch 38101/150000, Train Loss: 11163, Val Loss: 11413,  Learning Rate: 0.00099, Train Gradient: 74.6\n",
      "Epoch 38201/150000, Train Loss: 11134, Val Loss: 11384,  Learning Rate: 0.00099, Train Gradient: 74.4\n",
      "Epoch 38301/150000, Train Loss: 11106, Val Loss: 11355,  Learning Rate: 0.00099, Train Gradient: 74.2\n",
      "Epoch 38401/150000, Train Loss: 11077, Val Loss: 11327,  Learning Rate: 0.00099, Train Gradient: 74.0\n",
      "Epoch 38501/150000, Train Loss: 11049, Val Loss: 11298,  Learning Rate: 0.00099, Train Gradient: 73.8\n",
      "Epoch 38601/150000, Train Loss: 11020, Val Loss: 11270,  Learning Rate: 0.00099, Train Gradient: 73.6\n",
      "Epoch 38701/150000, Train Loss: 10992, Val Loss: 11241,  Learning Rate: 0.00099, Train Gradient: 73.4\n",
      "Epoch 38801/150000, Train Loss: 10964, Val Loss: 11213,  Learning Rate: 0.00099, Train Gradient: 73.3\n",
      "Epoch 38901/150000, Train Loss: 10936, Val Loss: 11185,  Learning Rate: 0.00099, Train Gradient: 73.1\n",
      "Epoch 39001/150000, Train Loss: 10908, Val Loss: 11157,  Learning Rate: 0.00099, Train Gradient: 72.9\n",
      "Epoch 39101/150000, Train Loss: 10880, Val Loss: 11129,  Learning Rate: 0.00099, Train Gradient: 72.7\n",
      "Epoch 39201/150000, Train Loss: 10852, Val Loss: 11101,  Learning Rate: 0.00099, Train Gradient: 72.5\n",
      "Epoch 39301/150000, Train Loss: 10824, Val Loss: 11073,  Learning Rate: 0.00099, Train Gradient: 72.3\n",
      "Epoch 39401/150000, Train Loss: 10796, Val Loss: 11045,  Learning Rate: 0.00099, Train Gradient: 72.1\n",
      "Epoch 39501/150000, Train Loss: 10768, Val Loss: 11017,  Learning Rate: 0.00099, Train Gradient: 72.0\n",
      "Epoch 39601/150000, Train Loss: 10741, Val Loss: 10989,  Learning Rate: 0.00099, Train Gradient: 71.8\n",
      "Epoch 39701/150000, Train Loss: 10713, Val Loss: 10962,  Learning Rate: 0.00099, Train Gradient: 71.6\n",
      "Epoch 39801/150000, Train Loss: 10686, Val Loss: 10934,  Learning Rate: 0.00099, Train Gradient: 71.4\n",
      "Epoch 39901/150000, Train Loss: 10658, Val Loss: 10906,  Learning Rate: 0.00099, Train Gradient: 71.2\n",
      "Epoch 40001/150000, Train Loss: 10631, Val Loss: 10876,  Learning Rate: 0.00099, Train Gradient: 71.1\n",
      "Epoch 40101/150000, Train Loss: 10604, Val Loss: 10846,  Learning Rate: 0.00099, Train Gradient: 70.9\n",
      "Epoch 40201/150000, Train Loss: 10576, Val Loss: 10819,  Learning Rate: 0.00099, Train Gradient: 70.7\n",
      "Epoch 40301/150000, Train Loss: 10549, Val Loss: 10791,  Learning Rate: 0.00099, Train Gradient: 70.5\n",
      "Epoch 40401/150000, Train Loss: 10522, Val Loss: 10764,  Learning Rate: 0.00099, Train Gradient: 70.3\n",
      "Epoch 40501/150000, Train Loss: 10495, Val Loss: 10738,  Learning Rate: 0.00099, Train Gradient: 70.2\n",
      "Epoch 40601/150000, Train Loss: 10468, Val Loss: 10711,  Learning Rate: 0.00099, Train Gradient: 70.0\n",
      "Epoch 40701/150000, Train Loss: 10441, Val Loss: 10684,  Learning Rate: 0.00099, Train Gradient: 69.8\n",
      "Epoch 40801/150000, Train Loss: 10415, Val Loss: 10657,  Learning Rate: 0.00099, Train Gradient: 69.6\n",
      "Epoch 40901/150000, Train Loss: 10388, Val Loss: 10631,  Learning Rate: 0.00099, Train Gradient: 69.5\n",
      "Epoch 41001/150000, Train Loss: 10361, Val Loss: 10604,  Learning Rate: 0.00099, Train Gradient: 69.3\n",
      "Epoch 41101/150000, Train Loss: 10334, Val Loss: 10578,  Learning Rate: 0.00099, Train Gradient: 69.1\n",
      "Epoch 41201/150000, Train Loss: 10308, Val Loss: 10551,  Learning Rate: 0.00099, Train Gradient: 68.9\n",
      "Epoch 41301/150000, Train Loss: 10281, Val Loss: 10525,  Learning Rate: 0.00099, Train Gradient: 68.8\n",
      "Epoch 41401/150000, Train Loss: 10255, Val Loss: 10498,  Learning Rate: 0.00099, Train Gradient: 68.6\n",
      "Epoch 41501/150000, Train Loss: 10229, Val Loss: 10472,  Learning Rate: 0.00099, Train Gradient: 68.4\n",
      "Epoch 41601/150000, Train Loss: 10202, Val Loss: 10445,  Learning Rate: 0.00099, Train Gradient: 68.3\n",
      "Epoch 41701/150000, Train Loss: 10176, Val Loss: 10419,  Learning Rate: 0.00099, Train Gradient: 68.1\n",
      "Epoch 41801/150000, Train Loss: 10150, Val Loss: 10392,  Learning Rate: 0.00099, Train Gradient: 67.9\n",
      "Epoch 41901/150000, Train Loss: 10124, Val Loss: 10364,  Learning Rate: 0.00099, Train Gradient: 67.8\n",
      "Epoch 42001/150000, Train Loss: 10098, Val Loss: 10337,  Learning Rate: 0.00099, Train Gradient: 67.6\n",
      "Epoch 42101/150000, Train Loss: 10071, Val Loss: 10311,  Learning Rate: 0.00099, Train Gradient: 67.4\n",
      "Epoch 42201/150000, Train Loss: 10045, Val Loss: 10284,  Learning Rate: 0.00099, Train Gradient: 67.2\n",
      "Epoch 42301/150000, Train Loss: 10020, Val Loss: 10257,  Learning Rate: 0.00099, Train Gradient: 67.1\n",
      "Epoch 42401/150000, Train Loss: 9994, Val Loss: 10231,  Learning Rate: 0.00099, Train Gradient: 66.9\n",
      "Epoch 42501/150000, Train Loss: 9968, Val Loss: 10205,  Learning Rate: 0.00099, Train Gradient: 66.8\n",
      "Epoch 42601/150000, Train Loss: 9942, Val Loss: 10178,  Learning Rate: 0.00099, Train Gradient: 66.6\n",
      "Epoch 42701/150000, Train Loss: 9916, Val Loss: 10152,  Learning Rate: 0.00099, Train Gradient: 66.4\n",
      "Epoch 42801/150000, Train Loss: 9891, Val Loss: 10126,  Learning Rate: 0.00099, Train Gradient: 66.3\n",
      "Epoch 42901/150000, Train Loss: 9865, Val Loss: 10100,  Learning Rate: 0.00099, Train Gradient: 66.1\n",
      "Epoch 43001/150000, Train Loss: 9840, Val Loss: 10074,  Learning Rate: 0.00099, Train Gradient: 65.9\n",
      "Epoch 43101/150000, Train Loss: 9814, Val Loss: 10047,  Learning Rate: 0.00099, Train Gradient: 65.8\n",
      "Epoch 43201/150000, Train Loss: 9789, Val Loss: 10021,  Learning Rate: 0.00099, Train Gradient: 65.6\n",
      "Epoch 43301/150000, Train Loss: 9764, Val Loss: 9995,  Learning Rate: 0.00099, Train Gradient: 65.5\n",
      "Epoch 43401/150000, Train Loss: 9738, Val Loss: 9970,  Learning Rate: 0.00099, Train Gradient: 65.3\n",
      "Epoch 43501/150000, Train Loss: 9713, Val Loss: 9944,  Learning Rate: 0.00099, Train Gradient: 65.1\n",
      "Epoch 43601/150000, Train Loss: 9688, Val Loss: 9918,  Learning Rate: 0.00099, Train Gradient: 65.0\n",
      "Epoch 43701/150000, Train Loss: 9663, Val Loss: 9892,  Learning Rate: 0.00099, Train Gradient: 64.8\n",
      "Epoch 43801/150000, Train Loss: 9638, Val Loss: 9866,  Learning Rate: 0.00099, Train Gradient: 64.7\n",
      "Epoch 43901/150000, Train Loss: 9613, Val Loss: 9841,  Learning Rate: 0.00099, Train Gradient: 64.5\n",
      "Epoch 44001/150000, Train Loss: 9588, Val Loss: 9816,  Learning Rate: 0.00099, Train Gradient: 64.3\n",
      "Epoch 44101/150000, Train Loss: 9563, Val Loss: 9790,  Learning Rate: 0.00099, Train Gradient: 64.2\n",
      "Epoch 44201/150000, Train Loss: 9539, Val Loss: 9765,  Learning Rate: 0.00099, Train Gradient: 64.0\n",
      "Epoch 44301/150000, Train Loss: 9514, Val Loss: 9740,  Learning Rate: 0.00099, Train Gradient: 63.9\n",
      "Epoch 44401/150000, Train Loss: 9489, Val Loss: 9715,  Learning Rate: 0.00099, Train Gradient: 63.7\n",
      "Epoch 44501/150000, Train Loss: 9465, Val Loss: 9690,  Learning Rate: 0.00099, Train Gradient: 63.6\n",
      "Epoch 44601/150000, Train Loss: 9440, Val Loss: 9665,  Learning Rate: 0.00099, Train Gradient: 63.4\n",
      "Epoch 44701/150000, Train Loss: 9416, Val Loss: 9640,  Learning Rate: 0.00099, Train Gradient: 63.3\n",
      "Epoch 44801/150000, Train Loss: 9391, Val Loss: 9616,  Learning Rate: 0.00099, Train Gradient: 63.1\n",
      "Epoch 44901/150000, Train Loss: 9367, Val Loss: 9591,  Learning Rate: 0.00099, Train Gradient: 63.0\n",
      "Epoch 45001/150000, Train Loss: 9343, Val Loss: 9566,  Learning Rate: 0.00099, Train Gradient: 62.8\n",
      "Epoch 45101/150000, Train Loss: 9319, Val Loss: 9542,  Learning Rate: 0.00099, Train Gradient: 62.7\n",
      "Epoch 45201/150000, Train Loss: 9294, Val Loss: 9517,  Learning Rate: 0.00099, Train Gradient: 62.5\n",
      "Epoch 45301/150000, Train Loss: 9270, Val Loss: 9493,  Learning Rate: 0.00099, Train Gradient: 62.4\n",
      "Epoch 45401/150000, Train Loss: 9246, Val Loss: 9468,  Learning Rate: 0.00099, Train Gradient: 62.3\n",
      "Epoch 45501/150000, Train Loss: 9222, Val Loss: 9444,  Learning Rate: 0.00099, Train Gradient: 62.1\n",
      "Epoch 45601/150000, Train Loss: 9198, Val Loss: 9419,  Learning Rate: 0.00099, Train Gradient: 62.0\n",
      "Epoch 45701/150000, Train Loss: 9174, Val Loss: 9395,  Learning Rate: 0.00099, Train Gradient: 61.8\n",
      "Epoch 45801/150000, Train Loss: 9150, Val Loss: 9371,  Learning Rate: 0.00099, Train Gradient: 61.7\n",
      "Epoch 45901/150000, Train Loss: 9127, Val Loss: 9347,  Learning Rate: 0.00099, Train Gradient: 61.5\n",
      "Epoch 46001/150000, Train Loss: 9103, Val Loss: 9323,  Learning Rate: 0.00099, Train Gradient: 61.4\n",
      "Epoch 46101/150000, Train Loss: 9079, Val Loss: 9298,  Learning Rate: 0.00099, Train Gradient: 61.3\n",
      "Epoch 46201/150000, Train Loss: 9055, Val Loss: 9274,  Learning Rate: 0.00099, Train Gradient: 61.1\n",
      "Epoch 46301/150000, Train Loss: 9032, Val Loss: 9250,  Learning Rate: 0.00099, Train Gradient: 61.0\n",
      "Epoch 46401/150000, Train Loss: 9008, Val Loss: 9226,  Learning Rate: 0.00099, Train Gradient: 60.9\n",
      "Epoch 46501/150000, Train Loss: 8984, Val Loss: 9203,  Learning Rate: 0.00099, Train Gradient: 60.7\n",
      "Epoch 46601/150000, Train Loss: 8961, Val Loss: 9179,  Learning Rate: 0.00099, Train Gradient: 60.6\n",
      "Epoch 46701/150000, Train Loss: 8937, Val Loss: 9155,  Learning Rate: 0.00099, Train Gradient: 60.5\n",
      "Epoch 46801/150000, Train Loss: 8914, Val Loss: 9131,  Learning Rate: 0.00099, Train Gradient: 60.3\n",
      "Epoch 46901/150000, Train Loss: 8890, Val Loss: 9107,  Learning Rate: 0.00099, Train Gradient: 60.2\n",
      "Epoch 47001/150000, Train Loss: 8867, Val Loss: 9083,  Learning Rate: 0.00099, Train Gradient: 60.1\n",
      "Epoch 47101/150000, Train Loss: 8844, Val Loss: 9060,  Learning Rate: 0.00099, Train Gradient: 60.0\n",
      "Epoch 47201/150000, Train Loss: 8821, Val Loss: 9036,  Learning Rate: 0.00099, Train Gradient: 59.8\n",
      "Epoch 47301/150000, Train Loss: 8797, Val Loss: 9013,  Learning Rate: 0.00099, Train Gradient: 59.7\n",
      "Epoch 47401/150000, Train Loss: 8774, Val Loss: 8990,  Learning Rate: 0.00099, Train Gradient: 59.6\n",
      "Epoch 47501/150000, Train Loss: 8751, Val Loss: 8967,  Learning Rate: 0.00099, Train Gradient: 59.4\n",
      "Epoch 47601/150000, Train Loss: 8728, Val Loss: 8944,  Learning Rate: 0.00099, Train Gradient: 59.3\n",
      "Epoch 47701/150000, Train Loss: 8705, Val Loss: 8922,  Learning Rate: 0.00099, Train Gradient: 59.2\n",
      "Epoch 47801/150000, Train Loss: 8682, Val Loss: 8899,  Learning Rate: 0.00099, Train Gradient: 59.0\n",
      "Epoch 47901/150000, Train Loss: 8659, Val Loss: 8876,  Learning Rate: 0.00099, Train Gradient: 58.9\n",
      "Epoch 48001/150000, Train Loss: 8637, Val Loss: 8854,  Learning Rate: 0.00099, Train Gradient: 58.8\n",
      "Epoch 48101/150000, Train Loss: 8614, Val Loss: 8832,  Learning Rate: 0.00099, Train Gradient: 58.7\n",
      "Epoch 48201/150000, Train Loss: 8591, Val Loss: 8809,  Learning Rate: 0.00099, Train Gradient: 58.5\n",
      "Epoch 48301/150000, Train Loss: 8569, Val Loss: 8788,  Learning Rate: 0.00099, Train Gradient: 58.4\n",
      "Epoch 48401/150000, Train Loss: 8546, Val Loss: 8768,  Learning Rate: 0.00099, Train Gradient: 58.3\n",
      "Epoch 48501/150000, Train Loss: 8523, Val Loss: 8743,  Learning Rate: 0.00099, Train Gradient: 58.1\n",
      "Epoch 48601/150000, Train Loss: 8501, Val Loss: 8722,  Learning Rate: 0.00099, Train Gradient: 58.0\n",
      "Epoch 48701/150000, Train Loss: 8478, Val Loss: 8704,  Learning Rate: 0.00099, Train Gradient: 57.9\n",
      "Epoch 48801/150000, Train Loss: 8456, Val Loss: 8686,  Learning Rate: 0.00099, Train Gradient: 57.8\n",
      "Epoch 48901/150000, Train Loss: 8433, Val Loss: 8665,  Learning Rate: 0.00099, Train Gradient: 57.6\n",
      "Epoch 49001/150000, Train Loss: 8411, Val Loss: 8643,  Learning Rate: 0.00099, Train Gradient: 57.5\n",
      "Epoch 49101/150000, Train Loss: 8389, Val Loss: 8620,  Learning Rate: 0.00099, Train Gradient: 57.4\n",
      "Epoch 49201/150000, Train Loss: 8367, Val Loss: 8596,  Learning Rate: 0.00099, Train Gradient: 57.3\n",
      "Epoch 49301/150000, Train Loss: 8344, Val Loss: 8572,  Learning Rate: 0.00099, Train Gradient: 57.2\n",
      "Epoch 49401/150000, Train Loss: 8322, Val Loss: 8544,  Learning Rate: 0.00099, Train Gradient: 57.0\n",
      "Epoch 49501/150000, Train Loss: 8300, Val Loss: 8517,  Learning Rate: 0.00099, Train Gradient: 56.9\n",
      "Epoch 49601/150000, Train Loss: 8278, Val Loss: 8492,  Learning Rate: 0.00099, Train Gradient: 56.8\n",
      "Epoch 49701/150000, Train Loss: 8256, Val Loss: 8468,  Learning Rate: 0.00099, Train Gradient: 56.7\n",
      "Epoch 49801/150000, Train Loss: 8234, Val Loss: 8445,  Learning Rate: 0.00099, Train Gradient: 56.6\n",
      "Epoch 49901/150000, Train Loss: 8212, Val Loss: 8422,  Learning Rate: 0.00099, Train Gradient: 56.4\n",
      "Epoch 50001/150000, Train Loss: 8190, Val Loss: 8399,  Learning Rate: 0.00099, Train Gradient: 56.3\n",
      "Epoch 50101/150000, Train Loss: 8168, Val Loss: 8376,  Learning Rate: 0.00099, Train Gradient: 56.2\n",
      "Epoch 50201/150000, Train Loss: 8146, Val Loss: 8354,  Learning Rate: 0.00099, Train Gradient: 56.1\n",
      "Epoch 50301/150000, Train Loss: 8125, Val Loss: 8332,  Learning Rate: 0.00099, Train Gradient: 56.0\n",
      "Epoch 50401/150000, Train Loss: 8103, Val Loss: 8309,  Learning Rate: 0.00099, Train Gradient: 55.9\n",
      "Epoch 50501/150000, Train Loss: 8081, Val Loss: 8287,  Learning Rate: 0.00099, Train Gradient: 55.7\n",
      "Epoch 50601/150000, Train Loss: 8059, Val Loss: 8265,  Learning Rate: 0.00099, Train Gradient: 55.6\n",
      "Epoch 50701/150000, Train Loss: 8038, Val Loss: 8243,  Learning Rate: 0.00099, Train Gradient: 55.5\n",
      "Epoch 50801/150000, Train Loss: 8016, Val Loss: 8221,  Learning Rate: 0.00099, Train Gradient: 55.4\n",
      "Epoch 50901/150000, Train Loss: 7995, Val Loss: 8199,  Learning Rate: 0.00099, Train Gradient: 55.3\n",
      "Epoch 51001/150000, Train Loss: 7973, Val Loss: 8177,  Learning Rate: 0.00099, Train Gradient: 55.2\n",
      "Epoch 51101/150000, Train Loss: 7952, Val Loss: 8155,  Learning Rate: 0.00099, Train Gradient: 55.0\n",
      "Epoch 51201/150000, Train Loss: 7930, Val Loss: 8134,  Learning Rate: 0.00099, Train Gradient: 54.9\n",
      "Epoch 51301/150000, Train Loss: 7909, Val Loss: 8112,  Learning Rate: 0.00099, Train Gradient: 54.8\n",
      "Epoch 51401/150000, Train Loss: 7888, Val Loss: 8091,  Learning Rate: 0.00099, Train Gradient: 54.7\n",
      "Epoch 51501/150000, Train Loss: 7866, Val Loss: 8069,  Learning Rate: 0.00099, Train Gradient: 54.6\n",
      "Epoch 51601/150000, Train Loss: 7845, Val Loss: 8047,  Learning Rate: 0.00099, Train Gradient: 54.5\n",
      "Epoch 51701/150000, Train Loss: 7824, Val Loss: 8026,  Learning Rate: 0.00099, Train Gradient: 54.4\n",
      "Epoch 51801/150000, Train Loss: 7803, Val Loss: 8005,  Learning Rate: 0.00099, Train Gradient: 54.2\n",
      "Epoch 51901/150000, Train Loss: 7782, Val Loss: 7983,  Learning Rate: 0.00099, Train Gradient: 54.1\n",
      "Epoch 52001/150000, Train Loss: 7761, Val Loss: 7962,  Learning Rate: 0.00099, Train Gradient: 54.0\n",
      "Epoch 52101/150000, Train Loss: 7740, Val Loss: 7941,  Learning Rate: 0.00099, Train Gradient: 53.9\n",
      "Epoch 52201/150000, Train Loss: 7719, Val Loss: 7920,  Learning Rate: 0.00099, Train Gradient: 53.8\n",
      "Epoch 52301/150000, Train Loss: 7698, Val Loss: 7899,  Learning Rate: 0.00099, Train Gradient: 53.7\n",
      "Epoch 52401/150000, Train Loss: 7677, Val Loss: 7878,  Learning Rate: 0.00099, Train Gradient: 53.6\n",
      "Epoch 52501/150000, Train Loss: 7656, Val Loss: 7856,  Learning Rate: 0.00099, Train Gradient: 53.5\n",
      "Epoch 52601/150000, Train Loss: 7636, Val Loss: 7835,  Learning Rate: 0.00099, Train Gradient: 53.4\n",
      "Epoch 52701/150000, Train Loss: 7615, Val Loss: 7815,  Learning Rate: 0.00099, Train Gradient: 53.3\n",
      "Epoch 52801/150000, Train Loss: 7594, Val Loss: 7794,  Learning Rate: 0.00099, Train Gradient: 53.1\n",
      "Epoch 52901/150000, Train Loss: 7573, Val Loss: 7773,  Learning Rate: 0.00099, Train Gradient: 53.0\n",
      "Epoch 53001/150000, Train Loss: 7553, Val Loss: 7752,  Learning Rate: 0.00099, Train Gradient: 52.9\n",
      "Epoch 53101/150000, Train Loss: 7532, Val Loss: 7731,  Learning Rate: 0.00099, Train Gradient: 52.8\n",
      "Epoch 53201/150000, Train Loss: 7512, Val Loss: 7710,  Learning Rate: 0.00099, Train Gradient: 52.7\n",
      "Epoch 53301/150000, Train Loss: 7491, Val Loss: 7689,  Learning Rate: 0.00099, Train Gradient: 52.6\n",
      "Epoch 53401/150000, Train Loss: 7471, Val Loss: 7669,  Learning Rate: 0.00099, Train Gradient: 52.5\n",
      "Epoch 53501/150000, Train Loss: 7450, Val Loss: 7648,  Learning Rate: 0.00099, Train Gradient: 52.4\n",
      "Epoch 53601/150000, Train Loss: 7430, Val Loss: 7627,  Learning Rate: 0.00099, Train Gradient: 52.3\n",
      "Epoch 53701/150000, Train Loss: 7410, Val Loss: 7607,  Learning Rate: 0.00099, Train Gradient: 52.2\n",
      "Epoch 53801/150000, Train Loss: 7389, Val Loss: 7586,  Learning Rate: 0.00099, Train Gradient: 52.1\n",
      "Epoch 53901/150000, Train Loss: 7369, Val Loss: 7566,  Learning Rate: 0.00099, Train Gradient: 51.9\n",
      "Epoch 54001/150000, Train Loss: 7349, Val Loss: 7545,  Learning Rate: 0.00099, Train Gradient: 51.8\n",
      "Epoch 54101/150000, Train Loss: 7329, Val Loss: 7525,  Learning Rate: 0.00099, Train Gradient: 51.7\n",
      "Epoch 54201/150000, Train Loss: 7309, Val Loss: 7505,  Learning Rate: 0.00099, Train Gradient: 51.6\n",
      "Epoch 54301/150000, Train Loss: 7289, Val Loss: 7484,  Learning Rate: 0.00099, Train Gradient: 51.5\n",
      "Epoch 54401/150000, Train Loss: 7269, Val Loss: 7464,  Learning Rate: 0.00099, Train Gradient: 51.4\n",
      "Epoch 54501/150000, Train Loss: 7249, Val Loss: 7444,  Learning Rate: 0.00099, Train Gradient: 51.3\n",
      "Epoch 54601/150000, Train Loss: 7229, Val Loss: 7424,  Learning Rate: 0.00099, Train Gradient: 51.2\n",
      "Epoch 54701/150000, Train Loss: 7209, Val Loss: 7404,  Learning Rate: 0.00099, Train Gradient: 51.1\n",
      "Epoch 54801/150000, Train Loss: 7189, Val Loss: 7384,  Learning Rate: 0.00099, Train Gradient: 51.0\n",
      "Epoch 54901/150000, Train Loss: 7169, Val Loss: 7364,  Learning Rate: 0.00099, Train Gradient: 50.9\n",
      "Epoch 55001/150000, Train Loss: 7149, Val Loss: 7344,  Learning Rate: 0.00099, Train Gradient: 50.8\n",
      "Epoch 55101/150000, Train Loss: 7129, Val Loss: 7324,  Learning Rate: 0.00099, Train Gradient: 50.7\n",
      "Epoch 55201/150000, Train Loss: 7110, Val Loss: 7304,  Learning Rate: 0.00099, Train Gradient: 50.6\n",
      "Epoch 55301/150000, Train Loss: 7090, Val Loss: 7284,  Learning Rate: 0.00099, Train Gradient: 50.5\n",
      "Epoch 55401/150000, Train Loss: 7070, Val Loss: 7264,  Learning Rate: 0.00099, Train Gradient: 50.4\n",
      "Epoch 55501/150000, Train Loss: 7051, Val Loss: 7244,  Learning Rate: 0.00099, Train Gradient: 50.3\n",
      "Epoch 55601/150000, Train Loss: 7031, Val Loss: 7224,  Learning Rate: 0.00099, Train Gradient: 50.2\n",
      "Epoch 55701/150000, Train Loss: 7012, Val Loss: 7205,  Learning Rate: 0.00099, Train Gradient: 50.1\n",
      "Epoch 55801/150000, Train Loss: 6992, Val Loss: 7185,  Learning Rate: 0.00099, Train Gradient: 50.0\n",
      "Epoch 55901/150000, Train Loss: 6973, Val Loss: 7165,  Learning Rate: 0.00099, Train Gradient: 49.9\n",
      "Epoch 56001/150000, Train Loss: 6953, Val Loss: 7146,  Learning Rate: 0.00099, Train Gradient: 49.8\n",
      "Epoch 56101/150000, Train Loss: 6934, Val Loss: 7126,  Learning Rate: 0.00099, Train Gradient: 49.7\n",
      "Epoch 56201/150000, Train Loss: 6915, Val Loss: 7107,  Learning Rate: 0.00099, Train Gradient: 49.6\n",
      "Epoch 56301/150000, Train Loss: 6895, Val Loss: 7087,  Learning Rate: 0.00099, Train Gradient: 49.5\n",
      "Epoch 56401/150000, Train Loss: 6876, Val Loss: 7068,  Learning Rate: 0.00099, Train Gradient: 49.4\n",
      "Epoch 56501/150000, Train Loss: 6857, Val Loss: 7048,  Learning Rate: 0.00099, Train Gradient: 49.3\n",
      "Epoch 56601/150000, Train Loss: 6838, Val Loss: 7029,  Learning Rate: 0.00099, Train Gradient: 49.2\n",
      "Epoch 56701/150000, Train Loss: 6819, Val Loss: 7009,  Learning Rate: 0.00099, Train Gradient: 49.1\n",
      "Epoch 56801/150000, Train Loss: 6800, Val Loss: 6990,  Learning Rate: 0.00099, Train Gradient: 49.0\n",
      "Epoch 56901/150000, Train Loss: 6780, Val Loss: 6971,  Learning Rate: 0.00099, Train Gradient: 48.9\n",
      "Epoch 57001/150000, Train Loss: 6761, Val Loss: 6952,  Learning Rate: 0.00099, Train Gradient: 48.8\n",
      "Epoch 57101/150000, Train Loss: 6742, Val Loss: 6932,  Learning Rate: 0.00099, Train Gradient: 48.7\n",
      "Epoch 57201/150000, Train Loss: 6724, Val Loss: 6913,  Learning Rate: 0.00099, Train Gradient: 48.6\n",
      "Epoch 57301/150000, Train Loss: 6705, Val Loss: 6894,  Learning Rate: 0.00099, Train Gradient: 48.5\n",
      "Epoch 57401/150000, Train Loss: 6686, Val Loss: 6875,  Learning Rate: 0.00099, Train Gradient: 48.4\n",
      "Epoch 57501/150000, Train Loss: 6667, Val Loss: 6856,  Learning Rate: 0.00099, Train Gradient: 48.3\n",
      "Epoch 57601/150000, Train Loss: 6648, Val Loss: 6837,  Learning Rate: 0.00099, Train Gradient: 48.2\n",
      "Epoch 57701/150000, Train Loss: 6629, Val Loss: 6818,  Learning Rate: 0.00099, Train Gradient: 48.1\n",
      "Epoch 57801/150000, Train Loss: 6611, Val Loss: 6799,  Learning Rate: 0.00099, Train Gradient: 48.0\n",
      "Epoch 57901/150000, Train Loss: 6592, Val Loss: 6780,  Learning Rate: 0.00099, Train Gradient: 47.9\n",
      "Epoch 58001/150000, Train Loss: 6573, Val Loss: 6761,  Learning Rate: 0.00099, Train Gradient: 47.8\n",
      "Epoch 58101/150000, Train Loss: 6555, Val Loss: 6742,  Learning Rate: 0.00100, Train Gradient: 47.7\n",
      "Epoch 58201/150000, Train Loss: 6536, Val Loss: 6724,  Learning Rate: 0.00100, Train Gradient: 47.6\n",
      "Epoch 58301/150000, Train Loss: 6517, Val Loss: 6705,  Learning Rate: 0.00100, Train Gradient: 47.4\n",
      "Epoch 58401/150000, Train Loss: 6499, Val Loss: 6686,  Learning Rate: 0.00100, Train Gradient: 47.5\n",
      "Epoch 58501/150000, Train Loss: 6480, Val Loss: 6667,  Learning Rate: 0.00100, Train Gradient: 47.3\n",
      "Epoch 58601/150000, Train Loss: 6462, Val Loss: 6649,  Learning Rate: 0.00100, Train Gradient: 47.2\n",
      "Epoch 58701/150000, Train Loss: 6443, Val Loss: 6630,  Learning Rate: 0.00100, Train Gradient: 47.1\n",
      "Epoch 58801/150000, Train Loss: 6425, Val Loss: 6611,  Learning Rate: 0.00100, Train Gradient: 47.0\n",
      "Epoch 58901/150000, Train Loss: 6407, Val Loss: 6593,  Learning Rate: 0.00100, Train Gradient: 46.9\n",
      "Epoch 59001/150000, Train Loss: 6388, Val Loss: 6574,  Learning Rate: 0.00100, Train Gradient: 46.8\n",
      "Epoch 59101/150000, Train Loss: 6370, Val Loss: 6556,  Learning Rate: 0.00100, Train Gradient: 46.8\n",
      "Epoch 59201/150000, Train Loss: 6352, Val Loss: 6537,  Learning Rate: 0.00100, Train Gradient: 46.7\n",
      "Epoch 59301/150000, Train Loss: 6334, Val Loss: 6519,  Learning Rate: 0.00100, Train Gradient: 46.6\n",
      "Epoch 59401/150000, Train Loss: 6315, Val Loss: 6501,  Learning Rate: 0.00100, Train Gradient: 46.5\n",
      "Epoch 59501/150000, Train Loss: 6297, Val Loss: 6482,  Learning Rate: 0.00100, Train Gradient: 46.4\n",
      "Epoch 59601/150000, Train Loss: 6279, Val Loss: 6464,  Learning Rate: 0.00100, Train Gradient: 46.3\n",
      "Epoch 59701/150000, Train Loss: 6261, Val Loss: 6446,  Learning Rate: 0.00100, Train Gradient: 46.2\n",
      "Epoch 59801/150000, Train Loss: 6243, Val Loss: 6428,  Learning Rate: 0.00100, Train Gradient: 46.1\n",
      "Epoch 59901/150000, Train Loss: 6225, Val Loss: 6410,  Learning Rate: 0.00100, Train Gradient: 46.0\n",
      "Epoch 60001/150000, Train Loss: 6207, Val Loss: 6391,  Learning Rate: 0.00100, Train Gradient: 45.9\n",
      "Epoch 60101/150000, Train Loss: 6189, Val Loss: 6373,  Learning Rate: 0.00100, Train Gradient: 45.8\n",
      "Epoch 60201/150000, Train Loss: 6171, Val Loss: 6355,  Learning Rate: 0.00100, Train Gradient: 45.7\n",
      "Epoch 60301/150000, Train Loss: 6154, Val Loss: 6337,  Learning Rate: 0.00100, Train Gradient: 45.6\n",
      "Epoch 60401/150000, Train Loss: 6136, Val Loss: 6319,  Learning Rate: 0.00100, Train Gradient: 45.5\n",
      "Epoch 60501/150000, Train Loss: 6118, Val Loss: 6301,  Learning Rate: 0.00100, Train Gradient: 45.4\n",
      "Epoch 60601/150000, Train Loss: 6100, Val Loss: 6283,  Learning Rate: 0.00100, Train Gradient: 45.3\n",
      "Epoch 60701/150000, Train Loss: 6083, Val Loss: 6265,  Learning Rate: 0.00100, Train Gradient: 45.2\n",
      "Epoch 60801/150000, Train Loss: 6065, Val Loss: 6248,  Learning Rate: 0.00100, Train Gradient: 45.1\n",
      "Epoch 60901/150000, Train Loss: 6047, Val Loss: 6230,  Learning Rate: 0.00100, Train Gradient: 45.1\n",
      "Epoch 61001/150000, Train Loss: 6030, Val Loss: 6212,  Learning Rate: 0.00100, Train Gradient: 45.0\n",
      "Epoch 61101/150000, Train Loss: 6012, Val Loss: 6194,  Learning Rate: 0.00100, Train Gradient: 44.9\n",
      "Epoch 61201/150000, Train Loss: 5995, Val Loss: 6177,  Learning Rate: 0.00100, Train Gradient: 44.7\n",
      "Epoch 61301/150000, Train Loss: 5977, Val Loss: 6159,  Learning Rate: 0.00100, Train Gradient: 44.6\n",
      "Epoch 61401/150000, Train Loss: 5960, Val Loss: 6141,  Learning Rate: 0.00100, Train Gradient: 44.6\n",
      "Epoch 61501/150000, Train Loss: 5942, Val Loss: 6124,  Learning Rate: 0.00100, Train Gradient: 44.5\n",
      "Epoch 61601/150000, Train Loss: 5925, Val Loss: 6106,  Learning Rate: 0.00100, Train Gradient: 44.4\n",
      "Epoch 61701/150000, Train Loss: 5908, Val Loss: 6089,  Learning Rate: 0.00100, Train Gradient: 44.3\n",
      "Epoch 61801/150000, Train Loss: 5890, Val Loss: 6071,  Learning Rate: 0.00100, Train Gradient: 44.2\n",
      "Epoch 61901/150000, Train Loss: 5873, Val Loss: 6054,  Learning Rate: 0.00100, Train Gradient: 44.1\n",
      "Epoch 62001/150000, Train Loss: 5856, Val Loss: 6037,  Learning Rate: 0.00100, Train Gradient: 44.0\n",
      "Epoch 62101/150000, Train Loss: 5839, Val Loss: 6019,  Learning Rate: 0.00100, Train Gradient: 44.0\n",
      "Epoch 62201/150000, Train Loss: 5821, Val Loss: 6002,  Learning Rate: 0.00100, Train Gradient: 43.8\n",
      "Epoch 62301/150000, Train Loss: 5804, Val Loss: 5985,  Learning Rate: 0.00100, Train Gradient: 43.7\n",
      "Epoch 62401/150000, Train Loss: 5787, Val Loss: 5968,  Learning Rate: 0.00100, Train Gradient: 43.7\n",
      "Epoch 62501/150000, Train Loss: 5770, Val Loss: 5950,  Learning Rate: 0.00100, Train Gradient: 43.6\n",
      "Epoch 62601/150000, Train Loss: 5753, Val Loss: 5933,  Learning Rate: 0.00100, Train Gradient: 43.5\n",
      "Epoch 62701/150000, Train Loss: 5736, Val Loss: 5916,  Learning Rate: 0.00100, Train Gradient: 43.4\n",
      "Epoch 62801/150000, Train Loss: 5719, Val Loss: 5899,  Learning Rate: 0.00100, Train Gradient: 43.3\n",
      "Epoch 62901/150000, Train Loss: 5702, Val Loss: 5882,  Learning Rate: 0.00100, Train Gradient: 43.2\n",
      "Epoch 63001/150000, Train Loss: 5686, Val Loss: 5865,  Learning Rate: 0.00100, Train Gradient: 43.0\n",
      "Epoch 63101/150000, Train Loss: 5669, Val Loss: 5848,  Learning Rate: 0.00100, Train Gradient: 43.0\n",
      "Epoch 63201/150000, Train Loss: 5652, Val Loss: 5831,  Learning Rate: 0.00100, Train Gradient: 42.9\n",
      "Epoch 63301/150000, Train Loss: 5635, Val Loss: 5814,  Learning Rate: 0.00100, Train Gradient: 42.8\n",
      "Epoch 63401/150000, Train Loss: 5618, Val Loss: 5797,  Learning Rate: 0.00100, Train Gradient: 42.7\n",
      "Epoch 63501/150000, Train Loss: 5602, Val Loss: 5781,  Learning Rate: 0.00100, Train Gradient: 42.6\n",
      "Epoch 63601/150000, Train Loss: 5585, Val Loss: 5764,  Learning Rate: 0.00100, Train Gradient: 42.6\n",
      "Epoch 63701/150000, Train Loss: 5569, Val Loss: 5747,  Learning Rate: 0.00100, Train Gradient: 42.5\n",
      "Epoch 63801/150000, Train Loss: 5552, Val Loss: 5730,  Learning Rate: 0.00100, Train Gradient: 42.4\n",
      "Epoch 63901/150000, Train Loss: 5535, Val Loss: 5714,  Learning Rate: 0.00100, Train Gradient: 42.3\n",
      "Epoch 64001/150000, Train Loss: 5519, Val Loss: 5697,  Learning Rate: 0.00100, Train Gradient: 42.2\n",
      "Epoch 64101/150000, Train Loss: 5502, Val Loss: 5680,  Learning Rate: 0.00100, Train Gradient: 42.1\n",
      "Epoch 64201/150000, Train Loss: 5486, Val Loss: 5664,  Learning Rate: 0.00100, Train Gradient: 42.0\n",
      "Epoch 64301/150000, Train Loss: 5470, Val Loss: 5647,  Learning Rate: 0.00100, Train Gradient: 41.9\n",
      "Epoch 64401/150000, Train Loss: 5453, Val Loss: 5631,  Learning Rate: 0.00100, Train Gradient: 41.7\n",
      "Epoch 64501/150000, Train Loss: 5437, Val Loss: 5614,  Learning Rate: 0.00100, Train Gradient: 41.7\n",
      "Epoch 64601/150000, Train Loss: 5421, Val Loss: 5598,  Learning Rate: 0.00100, Train Gradient: 41.6\n",
      "Epoch 64701/150000, Train Loss: 5404, Val Loss: 5582,  Learning Rate: 0.00100, Train Gradient: 41.5\n",
      "Epoch 64801/150000, Train Loss: 5388, Val Loss: 5565,  Learning Rate: 0.00100, Train Gradient: 41.4\n",
      "Epoch 64901/150000, Train Loss: 5372, Val Loss: 5549,  Learning Rate: 0.00100, Train Gradient: 41.3\n",
      "Epoch 65001/150000, Train Loss: 5356, Val Loss: 5533,  Learning Rate: 0.00100, Train Gradient: 41.3\n",
      "Epoch 65101/150000, Train Loss: 5340, Val Loss: 5516,  Learning Rate: 0.00100, Train Gradient: 41.2\n",
      "Epoch 65201/150000, Train Loss: 5324, Val Loss: 5500,  Learning Rate: 0.00100, Train Gradient: 41.1\n",
      "Epoch 65301/150000, Train Loss: 5308, Val Loss: 5484,  Learning Rate: 0.00100, Train Gradient: 41.0\n",
      "Epoch 65401/150000, Train Loss: 5292, Val Loss: 5468,  Learning Rate: 0.00100, Train Gradient: 40.9\n",
      "Epoch 65501/150000, Train Loss: 5276, Val Loss: 5452,  Learning Rate: 0.00100, Train Gradient: 40.8\n",
      "Epoch 65601/150000, Train Loss: 5260, Val Loss: 5436,  Learning Rate: 0.00100, Train Gradient: 40.7\n",
      "Epoch 65701/150000, Train Loss: 5244, Val Loss: 5420,  Learning Rate: 0.00100, Train Gradient: 40.6\n",
      "Epoch 65801/150000, Train Loss: 5228, Val Loss: 5404,  Learning Rate: 0.00100, Train Gradient: 40.5\n",
      "Epoch 65901/150000, Train Loss: 5212, Val Loss: 5388,  Learning Rate: 0.00100, Train Gradient: 40.5\n",
      "Epoch 66001/150000, Train Loss: 5197, Val Loss: 5372,  Learning Rate: 0.00100, Train Gradient: 40.3\n",
      "Epoch 66101/150000, Train Loss: 5181, Val Loss: 5356,  Learning Rate: 0.00100, Train Gradient: 40.2\n",
      "Epoch 66201/150000, Train Loss: 5165, Val Loss: 5340,  Learning Rate: 0.00100, Train Gradient: 40.2\n",
      "Epoch 66301/150000, Train Loss: 5150, Val Loss: 5325,  Learning Rate: 0.00100, Train Gradient: 40.1\n",
      "Epoch 66401/150000, Train Loss: 5134, Val Loss: 5309,  Learning Rate: 0.00100, Train Gradient: 40.0\n",
      "Epoch 66501/150000, Train Loss: 5118, Val Loss: 5293,  Learning Rate: 0.00100, Train Gradient: 39.9\n",
      "Epoch 66601/150000, Train Loss: 5103, Val Loss: 5277,  Learning Rate: 0.00100, Train Gradient: 39.8\n",
      "Epoch 66701/150000, Train Loss: 5087, Val Loss: 5262,  Learning Rate: 0.00100, Train Gradient: 39.7\n",
      "Epoch 66801/150000, Train Loss: 5072, Val Loss: 5246,  Learning Rate: 0.00100, Train Gradient: 39.6\n",
      "Epoch 66901/150000, Train Loss: 5056, Val Loss: 5231,  Learning Rate: 0.00100, Train Gradient: 39.5\n",
      "Epoch 67001/150000, Train Loss: 5041, Val Loss: 5215,  Learning Rate: 0.00100, Train Gradient: 39.5\n",
      "Epoch 67101/150000, Train Loss: 5025, Val Loss: 5200,  Learning Rate: 0.00100, Train Gradient: 39.4\n",
      "Epoch 67201/150000, Train Loss: 5010, Val Loss: 5184,  Learning Rate: 0.00100, Train Gradient: 39.3\n",
      "Epoch 67301/150000, Train Loss: 4995, Val Loss: 5169,  Learning Rate: 0.00100, Train Gradient: 39.2\n",
      "Epoch 67401/150000, Train Loss: 4979, Val Loss: 5153,  Learning Rate: 0.00100, Train Gradient: 39.1\n",
      "Epoch 67501/150000, Train Loss: 4964, Val Loss: 5138,  Learning Rate: 0.00100, Train Gradient: 39.0\n",
      "Epoch 67601/150000, Train Loss: 4949, Val Loss: 5123,  Learning Rate: 0.00100, Train Gradient: 39.0\n",
      "Epoch 67701/150000, Train Loss: 4934, Val Loss: 5107,  Learning Rate: 0.00100, Train Gradient: 38.9\n",
      "Epoch 67801/150000, Train Loss: 4918, Val Loss: 5092,  Learning Rate: 0.00100, Train Gradient: 38.8\n",
      "Epoch 67901/150000, Train Loss: 4903, Val Loss: 5077,  Learning Rate: 0.00100, Train Gradient: 38.7\n",
      "Epoch 68001/150000, Train Loss: 4888, Val Loss: 5062,  Learning Rate: 0.00100, Train Gradient: 38.6\n",
      "Epoch 68101/150000, Train Loss: 4873, Val Loss: 5047,  Learning Rate: 0.00100, Train Gradient: 38.5\n",
      "Epoch 68201/150000, Train Loss: 4858, Val Loss: 5031,  Learning Rate: 0.00100, Train Gradient: 38.5\n",
      "Epoch 68301/150000, Train Loss: 4843, Val Loss: 5016,  Learning Rate: 0.00100, Train Gradient: 38.4\n",
      "Epoch 68401/150000, Train Loss: 4828, Val Loss: 5001,  Learning Rate: 0.00100, Train Gradient: 38.2\n",
      "Epoch 68501/150000, Train Loss: 4813, Val Loss: 4986,  Learning Rate: 0.00100, Train Gradient: 38.2\n",
      "Epoch 68601/150000, Train Loss: 4798, Val Loss: 4971,  Learning Rate: 0.00100, Train Gradient: 38.1\n",
      "Epoch 68701/150000, Train Loss: 4783, Val Loss: 4956,  Learning Rate: 0.00100, Train Gradient: 38.0\n",
      "Epoch 68801/150000, Train Loss: 4768, Val Loss: 4942,  Learning Rate: 0.00100, Train Gradient: 38.0\n",
      "Epoch 68901/150000, Train Loss: 4754, Val Loss: 4927,  Learning Rate: 0.00100, Train Gradient: 37.9\n",
      "Epoch 69001/150000, Train Loss: 4739, Val Loss: 4912,  Learning Rate: 0.00100, Train Gradient: 37.8\n",
      "Epoch 69101/150000, Train Loss: 4724, Val Loss: 4897,  Learning Rate: 0.00100, Train Gradient: 37.7\n",
      "Epoch 69201/150000, Train Loss: 4709, Val Loss: 4883,  Learning Rate: 0.00100, Train Gradient: 37.6\n",
      "Epoch 69301/150000, Train Loss: 4695, Val Loss: 4868,  Learning Rate: 0.00100, Train Gradient: 37.5\n",
      "Epoch 69401/150000, Train Loss: 4680, Val Loss: 4853,  Learning Rate: 0.00100, Train Gradient: 37.5\n",
      "Epoch 69501/150000, Train Loss: 4665, Val Loss: 4839,  Learning Rate: 0.00100, Train Gradient: 37.4\n",
      "Epoch 69601/150000, Train Loss: 4651, Val Loss: 4824,  Learning Rate: 0.00100, Train Gradient: 37.3\n",
      "Epoch 69701/150000, Train Loss: 4636, Val Loss: 4809,  Learning Rate: 0.00100, Train Gradient: 37.2\n",
      "Epoch 69801/150000, Train Loss: 4622, Val Loss: 4795,  Learning Rate: 0.00100, Train Gradient: 37.1\n",
      "Epoch 69901/150000, Train Loss: 4607, Val Loss: 4780,  Learning Rate: 0.00100, Train Gradient: 37.0\n",
      "Epoch 70001/150000, Train Loss: 4593, Val Loss: 4766,  Learning Rate: 0.00100, Train Gradient: 37.0\n",
      "Epoch 70101/150000, Train Loss: 4578, Val Loss: 4751,  Learning Rate: 0.00100, Train Gradient: 36.9\n",
      "Epoch 70201/150000, Train Loss: 4564, Val Loss: 4737,  Learning Rate: 0.00100, Train Gradient: 36.8\n",
      "Epoch 70301/150000, Train Loss: 4550, Val Loss: 4723,  Learning Rate: 0.00100, Train Gradient: 36.7\n",
      "Epoch 70401/150000, Train Loss: 4535, Val Loss: 4708,  Learning Rate: 0.00100, Train Gradient: 36.6\n",
      "Epoch 70501/150000, Train Loss: 4521, Val Loss: 4694,  Learning Rate: 0.00100, Train Gradient: 36.6\n",
      "Epoch 70601/150000, Train Loss: 4507, Val Loss: 4680,  Learning Rate: 0.00100, Train Gradient: 36.5\n",
      "Epoch 70701/150000, Train Loss: 4492, Val Loss: 4665,  Learning Rate: 0.00100, Train Gradient: 36.4\n",
      "Epoch 70801/150000, Train Loss: 4478, Val Loss: 4651,  Learning Rate: 0.00100, Train Gradient: 36.3\n",
      "Epoch 70901/150000, Train Loss: 4464, Val Loss: 4637,  Learning Rate: 0.00100, Train Gradient: 36.2\n",
      "Epoch 71001/150000, Train Loss: 4450, Val Loss: 4623,  Learning Rate: 0.00100, Train Gradient: 36.2\n",
      "Epoch 71101/150000, Train Loss: 4436, Val Loss: 4608,  Learning Rate: 0.00100, Train Gradient: 36.1\n",
      "Epoch 71201/150000, Train Loss: 4422, Val Loss: 4594,  Learning Rate: 0.00100, Train Gradient: 36.0\n",
      "Epoch 71301/150000, Train Loss: 4408, Val Loss: 4580,  Learning Rate: 0.00100, Train Gradient: 35.8\n",
      "Epoch 71401/150000, Train Loss: 4394, Val Loss: 4566,  Learning Rate: 0.00100, Train Gradient: 35.8\n",
      "Epoch 71501/150000, Train Loss: 4380, Val Loss: 4551,  Learning Rate: 0.00100, Train Gradient: 35.8\n",
      "Epoch 71601/150000, Train Loss: 4366, Val Loss: 4537,  Learning Rate: 0.00100, Train Gradient: 35.7\n",
      "Epoch 71701/150000, Train Loss: 4352, Val Loss: 4523,  Learning Rate: 0.00100, Train Gradient: 35.6\n",
      "Epoch 71801/150000, Train Loss: 4338, Val Loss: 4509,  Learning Rate: 0.00100, Train Gradient: 35.5\n",
      "Epoch 71901/150000, Train Loss: 4324, Val Loss: 4496,  Learning Rate: 0.00100, Train Gradient: 35.4\n",
      "Epoch 72001/150000, Train Loss: 4310, Val Loss: 4482,  Learning Rate: 0.00100, Train Gradient: 35.3\n",
      "Epoch 72101/150000, Train Loss: 4297, Val Loss: 4468,  Learning Rate: 0.00100, Train Gradient: 35.3\n",
      "Epoch 72201/150000, Train Loss: 4283, Val Loss: 4454,  Learning Rate: 0.00100, Train Gradient: 35.5\n",
      "Epoch 72301/150000, Train Loss: 4269, Val Loss: 4440,  Learning Rate: 0.00100, Train Gradient: 35.1\n",
      "Epoch 72401/150000, Train Loss: 4255, Val Loss: 4427,  Learning Rate: 0.00100, Train Gradient: 35.0\n",
      "Epoch 72501/150000, Train Loss: 4242, Val Loss: 4413,  Learning Rate: 0.00100, Train Gradient: 34.9\n",
      "Epoch 72601/150000, Train Loss: 4228, Val Loss: 4399,  Learning Rate: 0.00100, Train Gradient: 34.9\n",
      "Epoch 72701/150000, Train Loss: 4214, Val Loss: 4386,  Learning Rate: 0.00100, Train Gradient: 34.8\n",
      "Epoch 72801/150000, Train Loss: 4201, Val Loss: 4372,  Learning Rate: 0.00100, Train Gradient: 34.7\n",
      "Epoch 72901/150000, Train Loss: 4187, Val Loss: 4359,  Learning Rate: 0.00100, Train Gradient: 34.6\n",
      "Epoch 73001/150000, Train Loss: 4174, Val Loss: 4345,  Learning Rate: 0.00100, Train Gradient: 34.5\n",
      "Epoch 73101/150000, Train Loss: 4160, Val Loss: 4332,  Learning Rate: 0.00100, Train Gradient: 34.4\n",
      "Epoch 73201/150000, Train Loss: 4147, Val Loss: 4318,  Learning Rate: 0.00100, Train Gradient: 34.4\n",
      "Epoch 73301/150000, Train Loss: 4133, Val Loss: 4305,  Learning Rate: 0.00100, Train Gradient: 34.3\n",
      "Epoch 73401/150000, Train Loss: 4120, Val Loss: 4291,  Learning Rate: 0.00100, Train Gradient: 34.2\n",
      "Epoch 73501/150000, Train Loss: 4107, Val Loss: 4278,  Learning Rate: 0.00100, Train Gradient: 34.1\n",
      "Epoch 73601/150000, Train Loss: 4093, Val Loss: 4265,  Learning Rate: 0.00100, Train Gradient: 34.0\n",
      "Epoch 73701/150000, Train Loss: 4080, Val Loss: 4252,  Learning Rate: 0.00100, Train Gradient: 34.1\n",
      "Epoch 73801/150000, Train Loss: 4067, Val Loss: 4239,  Learning Rate: 0.00100, Train Gradient: 34.0\n",
      "Epoch 73901/150000, Train Loss: 4053, Val Loss: 4225,  Learning Rate: 0.00100, Train Gradient: 33.9\n",
      "Epoch 74001/150000, Train Loss: 4040, Val Loss: 4211,  Learning Rate: 0.00100, Train Gradient: 33.8\n",
      "Epoch 74101/150000, Train Loss: 4027, Val Loss: 4198,  Learning Rate: 0.00100, Train Gradient: 33.7\n",
      "Epoch 74201/150000, Train Loss: 4014, Val Loss: 4185,  Learning Rate: 0.00100, Train Gradient: 33.7\n",
      "Epoch 74301/150000, Train Loss: 4000, Val Loss: 4171,  Learning Rate: 0.00100, Train Gradient: 33.7\n",
      "Epoch 74401/150000, Train Loss: 3987, Val Loss: 4158,  Learning Rate: 0.00100, Train Gradient: 33.5\n",
      "Epoch 74501/150000, Train Loss: 3974, Val Loss: 4145,  Learning Rate: 0.00100, Train Gradient: 33.4\n",
      "Epoch 74601/150000, Train Loss: 3961, Val Loss: 4132,  Learning Rate: 0.00100, Train Gradient: 33.3\n",
      "Epoch 74701/150000, Train Loss: 3948, Val Loss: 4119,  Learning Rate: 0.00100, Train Gradient: 33.3\n",
      "Epoch 74801/150000, Train Loss: 3935, Val Loss: 4106,  Learning Rate: 0.00100, Train Gradient: 33.2\n",
      "Epoch 74901/150000, Train Loss: 3922, Val Loss: 4093,  Learning Rate: 0.00100, Train Gradient: 33.1\n",
      "Epoch 75001/150000, Train Loss: 3909, Val Loss: 4080,  Learning Rate: 0.00100, Train Gradient: 33.0\n",
      "Epoch 75101/150000, Train Loss: 3896, Val Loss: 4067,  Learning Rate: 0.00100, Train Gradient: 32.9\n",
      "Epoch 75201/150000, Train Loss: 3883, Val Loss: 4054,  Learning Rate: 0.00100, Train Gradient: 32.9\n",
      "Epoch 75301/150000, Train Loss: 3870, Val Loss: 4042,  Learning Rate: 0.00100, Train Gradient: 32.8\n",
      "Epoch 75401/150000, Train Loss: 3858, Val Loss: 4029,  Learning Rate: 0.00100, Train Gradient: 32.7\n",
      "Epoch 75501/150000, Train Loss: 3845, Val Loss: 4016,  Learning Rate: 0.00100, Train Gradient: 32.6\n",
      "Epoch 75601/150000, Train Loss: 3832, Val Loss: 4003,  Learning Rate: 0.00100, Train Gradient: 32.6\n",
      "Epoch 75701/150000, Train Loss: 3819, Val Loss: 3990,  Learning Rate: 0.00100, Train Gradient: 32.5\n",
      "Epoch 75801/150000, Train Loss: 3807, Val Loss: 3978,  Learning Rate: 0.00100, Train Gradient: 32.6\n",
      "Epoch 75901/150000, Train Loss: 3794, Val Loss: 3965,  Learning Rate: 0.00100, Train Gradient: 32.3\n",
      "Epoch 76001/150000, Train Loss: 3782, Val Loss: 3952,  Learning Rate: 0.00100, Train Gradient: 32.3\n",
      "Epoch 76101/150000, Train Loss: 3769, Val Loss: 3940,  Learning Rate: 0.00100, Train Gradient: 32.2\n",
      "Epoch 76201/150000, Train Loss: 3756, Val Loss: 3927,  Learning Rate: 0.00100, Train Gradient: 32.1\n",
      "Epoch 76301/150000, Train Loss: 3744, Val Loss: 3915,  Learning Rate: 0.00100, Train Gradient: 32.0\n",
      "Epoch 76401/150000, Train Loss: 3731, Val Loss: 3902,  Learning Rate: 0.00100, Train Gradient: 32.0\n",
      "Epoch 76501/150000, Train Loss: 3719, Val Loss: 3890,  Learning Rate: 0.00100, Train Gradient: 31.9\n",
      "Epoch 76601/150000, Train Loss: 3706, Val Loss: 3877,  Learning Rate: 0.00100, Train Gradient: 31.7\n",
      "Epoch 76701/150000, Train Loss: 3694, Val Loss: 3865,  Learning Rate: 0.00100, Train Gradient: 31.7\n",
      "Epoch 76801/150000, Train Loss: 3682, Val Loss: 3852,  Learning Rate: 0.00100, Train Gradient: 31.6\n",
      "Epoch 76901/150000, Train Loss: 3669, Val Loss: 3840,  Learning Rate: 0.00100, Train Gradient: 31.6\n",
      "Epoch 77001/150000, Train Loss: 3657, Val Loss: 3828,  Learning Rate: 0.00100, Train Gradient: 31.5\n",
      "Epoch 77101/150000, Train Loss: 3645, Val Loss: 3815,  Learning Rate: 0.00100, Train Gradient: 31.4\n",
      "Epoch 77201/150000, Train Loss: 3632, Val Loss: 3803,  Learning Rate: 0.00100, Train Gradient: 31.3\n",
      "Epoch 77301/150000, Train Loss: 3620, Val Loss: 3791,  Learning Rate: 0.00100, Train Gradient: 31.3\n",
      "Epoch 77401/150000, Train Loss: 3608, Val Loss: 3779,  Learning Rate: 0.00100, Train Gradient: 31.2\n",
      "Epoch 77501/150000, Train Loss: 3596, Val Loss: 3767,  Learning Rate: 0.00100, Train Gradient: 31.1\n",
      "Epoch 77601/150000, Train Loss: 3584, Val Loss: 3754,  Learning Rate: 0.00100, Train Gradient: 31.0\n",
      "Epoch 77701/150000, Train Loss: 3572, Val Loss: 3742,  Learning Rate: 0.00100, Train Gradient: 30.9\n",
      "Epoch 77801/150000, Train Loss: 3559, Val Loss: 3730,  Learning Rate: 0.00100, Train Gradient: 30.9\n",
      "Epoch 77901/150000, Train Loss: 3547, Val Loss: 3718,  Learning Rate: 0.00100, Train Gradient: 30.8\n",
      "Epoch 78001/150000, Train Loss: 3535, Val Loss: 3706,  Learning Rate: 0.00100, Train Gradient: 30.7\n",
      "Epoch 78101/150000, Train Loss: 3523, Val Loss: 3694,  Learning Rate: 0.00100, Train Gradient: 30.6\n",
      "Epoch 78201/150000, Train Loss: 3511, Val Loss: 3682,  Learning Rate: 0.00100, Train Gradient: 30.6\n",
      "Epoch 78301/150000, Train Loss: 3499, Val Loss: 3670,  Learning Rate: 0.00100, Train Gradient: 30.5\n",
      "Epoch 78401/150000, Train Loss: 3488, Val Loss: 3658,  Learning Rate: 0.00100, Train Gradient: 30.4\n",
      "Epoch 78501/150000, Train Loss: 3476, Val Loss: 3646,  Learning Rate: 0.00100, Train Gradient: 30.3\n",
      "Epoch 78601/150000, Train Loss: 3464, Val Loss: 3634,  Learning Rate: 0.00100, Train Gradient: 30.3\n",
      "Epoch 78701/150000, Train Loss: 3452, Val Loss: 3623,  Learning Rate: 0.00100, Train Gradient: 30.2\n",
      "Epoch 78801/150000, Train Loss: 3440, Val Loss: 3611,  Learning Rate: 0.00100, Train Gradient: 30.1\n",
      "Epoch 78901/150000, Train Loss: 3429, Val Loss: 3599,  Learning Rate: 0.00100, Train Gradient: 30.0\n",
      "Epoch 79001/150000, Train Loss: 3417, Val Loss: 3587,  Learning Rate: 0.00100, Train Gradient: 30.0\n",
      "Epoch 79101/150000, Train Loss: 3405, Val Loss: 3576,  Learning Rate: 0.00100, Train Gradient: 29.9\n",
      "Epoch 79201/150000, Train Loss: 3394, Val Loss: 3564,  Learning Rate: 0.00100, Train Gradient: 29.8\n",
      "Epoch 79301/150000, Train Loss: 3382, Val Loss: 3553,  Learning Rate: 0.00100, Train Gradient: 29.7\n",
      "Epoch 79401/150000, Train Loss: 3370, Val Loss: 3541,  Learning Rate: 0.00100, Train Gradient: 29.7\n",
      "Epoch 79501/150000, Train Loss: 3359, Val Loss: 3529,  Learning Rate: 0.00100, Train Gradient: 29.6\n",
      "Epoch 79601/150000, Train Loss: 3347, Val Loss: 3518,  Learning Rate: 0.00100, Train Gradient: 29.5\n",
      "Epoch 79701/150000, Train Loss: 3336, Val Loss: 3506,  Learning Rate: 0.00100, Train Gradient: 29.4\n",
      "Epoch 79801/150000, Train Loss: 3324, Val Loss: 3495,  Learning Rate: 0.00100, Train Gradient: 29.4\n",
      "Epoch 79901/150000, Train Loss: 3313, Val Loss: 3483,  Learning Rate: 0.00100, Train Gradient: 29.3\n",
      "Epoch 80001/150000, Train Loss: 3301, Val Loss: 3472,  Learning Rate: 0.00100, Train Gradient: 29.2\n",
      "Epoch 80101/150000, Train Loss: 3290, Val Loss: 3461,  Learning Rate: 0.00100, Train Gradient: 29.1\n",
      "Epoch 80201/150000, Train Loss: 3279, Val Loss: 3449,  Learning Rate: 0.00100, Train Gradient: 29.1\n",
      "Epoch 80301/150000, Train Loss: 3267, Val Loss: 3438,  Learning Rate: 0.00100, Train Gradient: 29.0\n",
      "Epoch 80401/150000, Train Loss: 3256, Val Loss: 3427,  Learning Rate: 0.00100, Train Gradient: 28.9\n",
      "Epoch 80501/150000, Train Loss: 3245, Val Loss: 3416,  Learning Rate: 0.00100, Train Gradient: 28.8\n",
      "Epoch 80601/150000, Train Loss: 3233, Val Loss: 3404,  Learning Rate: 0.00100, Train Gradient: 28.8\n",
      "Epoch 80701/150000, Train Loss: 3222, Val Loss: 3393,  Learning Rate: 0.00100, Train Gradient: 28.7\n",
      "Epoch 80801/150000, Train Loss: 3211, Val Loss: 3382,  Learning Rate: 0.00100, Train Gradient: 28.6\n",
      "Epoch 80901/150000, Train Loss: 3200, Val Loss: 3371,  Learning Rate: 0.00100, Train Gradient: 28.5\n",
      "Epoch 81001/150000, Train Loss: 3189, Val Loss: 3360,  Learning Rate: 0.00100, Train Gradient: 28.5\n",
      "Epoch 81101/150000, Train Loss: 3178, Val Loss: 3349,  Learning Rate: 0.00100, Train Gradient: 28.4\n",
      "Epoch 81201/150000, Train Loss: 3166, Val Loss: 3338,  Learning Rate: 0.00100, Train Gradient: 28.3\n",
      "Epoch 81301/150000, Train Loss: 3155, Val Loss: 3327,  Learning Rate: 0.00100, Train Gradient: 28.2\n",
      "Epoch 81401/150000, Train Loss: 3144, Val Loss: 3316,  Learning Rate: 0.00100, Train Gradient: 28.2\n",
      "Epoch 81501/150000, Train Loss: 3133, Val Loss: 3305,  Learning Rate: 0.00100, Train Gradient: 28.1\n",
      "Epoch 81601/150000, Train Loss: 3122, Val Loss: 3294,  Learning Rate: 0.00100, Train Gradient: 28.0\n",
      "Epoch 81701/150000, Train Loss: 3111, Val Loss: 3283,  Learning Rate: 0.00100, Train Gradient: 28.0\n",
      "Epoch 81801/150000, Train Loss: 3101, Val Loss: 3272,  Learning Rate: 0.00100, Train Gradient: 27.9\n",
      "Epoch 81901/150000, Train Loss: 3090, Val Loss: 3261,  Learning Rate: 0.00100, Train Gradient: 27.8\n",
      "Epoch 82001/150000, Train Loss: 3079, Val Loss: 3250,  Learning Rate: 0.00100, Train Gradient: 27.7\n",
      "Epoch 82101/150000, Train Loss: 3068, Val Loss: 3239,  Learning Rate: 0.00100, Train Gradient: 27.7\n",
      "Epoch 82201/150000, Train Loss: 3057, Val Loss: 3229,  Learning Rate: 0.00100, Train Gradient: 27.6\n",
      "Epoch 82301/150000, Train Loss: 3046, Val Loss: 3218,  Learning Rate: 0.00100, Train Gradient: 27.5\n",
      "Epoch 82401/150000, Train Loss: 3036, Val Loss: 3207,  Learning Rate: 0.00100, Train Gradient: 27.5\n",
      "Epoch 82501/150000, Train Loss: 3025, Val Loss: 3196,  Learning Rate: 0.00100, Train Gradient: 27.4\n",
      "Epoch 82601/150000, Train Loss: 3014, Val Loss: 3185,  Learning Rate: 0.00100, Train Gradient: 27.3\n",
      "Epoch 82701/150000, Train Loss: 3004, Val Loss: 3175,  Learning Rate: 0.00100, Train Gradient: 27.2\n",
      "Epoch 82801/150000, Train Loss: 2993, Val Loss: 3164,  Learning Rate: 0.00100, Train Gradient: 27.2\n",
      "Epoch 82901/150000, Train Loss: 2982, Val Loss: 3153,  Learning Rate: 0.00100, Train Gradient: 27.1\n",
      "Epoch 83001/150000, Train Loss: 2972, Val Loss: 3143,  Learning Rate: 0.00100, Train Gradient: 27.0\n",
      "Epoch 83101/150000, Train Loss: 2961, Val Loss: 3132,  Learning Rate: 0.00100, Train Gradient: 27.0\n",
      "Epoch 83201/150000, Train Loss: 2951, Val Loss: 3122,  Learning Rate: 0.00100, Train Gradient: 26.9\n",
      "Epoch 83301/150000, Train Loss: 2940, Val Loss: 3111,  Learning Rate: 0.00100, Train Gradient: 26.8\n",
      "Epoch 83401/150000, Train Loss: 2930, Val Loss: 3101,  Learning Rate: 0.00100, Train Gradient: 26.8\n",
      "Epoch 83501/150000, Train Loss: 2919, Val Loss: 3090,  Learning Rate: 0.00100, Train Gradient: 26.7\n",
      "Epoch 83601/150000, Train Loss: 2909, Val Loss: 3080,  Learning Rate: 0.00100, Train Gradient: 26.6\n",
      "Epoch 83701/150000, Train Loss: 2898, Val Loss: 3069,  Learning Rate: 0.00100, Train Gradient: 26.6\n",
      "Epoch 83801/150000, Train Loss: 2888, Val Loss: 3058,  Learning Rate: 0.00100, Train Gradient: 26.5\n",
      "Epoch 83901/150000, Train Loss: 2878, Val Loss: 3048,  Learning Rate: 0.00100, Train Gradient: 26.5\n",
      "Epoch 84001/150000, Train Loss: 2867, Val Loss: 3038,  Learning Rate: 0.00100, Train Gradient: 26.3\n",
      "Epoch 84101/150000, Train Loss: 2857, Val Loss: 3027,  Learning Rate: 0.00100, Train Gradient: 26.3\n",
      "Epoch 84201/150000, Train Loss: 2847, Val Loss: 3017,  Learning Rate: 0.00100, Train Gradient: 26.3\n",
      "Epoch 84301/150000, Train Loss: 2836, Val Loss: 3008,  Learning Rate: 0.00100, Train Gradient: 26.2\n",
      "Epoch 84401/150000, Train Loss: 2826, Val Loss: 2998,  Learning Rate: 0.00100, Train Gradient: 26.1\n",
      "Epoch 84501/150000, Train Loss: 2816, Val Loss: 2988,  Learning Rate: 0.00100, Train Gradient: 26.0\n",
      "Epoch 84601/150000, Train Loss: 2806, Val Loss: 2979,  Learning Rate: 0.00100, Train Gradient: 25.9\n",
      "Epoch 84701/150000, Train Loss: 2796, Val Loss: 2969,  Learning Rate: 0.00100, Train Gradient: 25.9\n",
      "Epoch 84801/150000, Train Loss: 2786, Val Loss: 2960,  Learning Rate: 0.00100, Train Gradient: 25.8\n",
      "Epoch 84901/150000, Train Loss: 2776, Val Loss: 2950,  Learning Rate: 0.00100, Train Gradient: 25.7\n",
      "Epoch 85001/150000, Train Loss: 2766, Val Loss: 2941,  Learning Rate: 0.00100, Train Gradient: 25.7\n",
      "Epoch 85101/150000, Train Loss: 2756, Val Loss: 2931,  Learning Rate: 0.00100, Train Gradient: 25.6\n",
      "Epoch 85201/150000, Train Loss: 2746, Val Loss: 2922,  Learning Rate: 0.00100, Train Gradient: 25.5\n",
      "Epoch 85301/150000, Train Loss: 2736, Val Loss: 2912,  Learning Rate: 0.00100, Train Gradient: 25.5\n",
      "Epoch 85401/150000, Train Loss: 2726, Val Loss: 2902,  Learning Rate: 0.00100, Train Gradient: 25.4\n",
      "Epoch 85501/150000, Train Loss: 2716, Val Loss: 2893,  Learning Rate: 0.00100, Train Gradient: 25.3\n",
      "Epoch 85601/150000, Train Loss: 2706, Val Loss: 2883,  Learning Rate: 0.00100, Train Gradient: 25.3\n",
      "Epoch 85701/150000, Train Loss: 2696, Val Loss: 2874,  Learning Rate: 0.00100, Train Gradient: 25.2\n",
      "Epoch 85801/150000, Train Loss: 2687, Val Loss: 2864,  Learning Rate: 0.00100, Train Gradient: 25.2\n",
      "Epoch 85901/150000, Train Loss: 2677, Val Loss: 2855,  Learning Rate: 0.00100, Train Gradient: 25.1\n",
      "Epoch 86001/150000, Train Loss: 2667, Val Loss: 2845,  Learning Rate: 0.00100, Train Gradient: 25.0\n",
      "Epoch 86101/150000, Train Loss: 2658, Val Loss: 2836,  Learning Rate: 0.00100, Train Gradient: 24.9\n",
      "Epoch 86201/150000, Train Loss: 2648, Val Loss: 2827,  Learning Rate: 0.00100, Train Gradient: 24.8\n",
      "Epoch 86301/150000, Train Loss: 2638, Val Loss: 2817,  Learning Rate: 0.00100, Train Gradient: 24.8\n",
      "Epoch 86401/150000, Train Loss: 2629, Val Loss: 2808,  Learning Rate: 0.00100, Train Gradient: 24.7\n",
      "Epoch 86501/150000, Train Loss: 2619, Val Loss: 2799,  Learning Rate: 0.00100, Train Gradient: 24.7\n",
      "Epoch 86601/150000, Train Loss: 2610, Val Loss: 2789,  Learning Rate: 0.00100, Train Gradient: 24.7\n",
      "Epoch 86701/150000, Train Loss: 2600, Val Loss: 2780,  Learning Rate: 0.00100, Train Gradient: 24.5\n",
      "Epoch 86801/150000, Train Loss: 2591, Val Loss: 2771,  Learning Rate: 0.00100, Train Gradient: 24.5\n",
      "Epoch 86901/150000, Train Loss: 2581, Val Loss: 2762,  Learning Rate: 0.00100, Train Gradient: 24.4\n",
      "Epoch 87001/150000, Train Loss: 2572, Val Loss: 2752,  Learning Rate: 0.00100, Train Gradient: 24.3\n",
      "Epoch 87101/150000, Train Loss: 2562, Val Loss: 2743,  Learning Rate: 0.00100, Train Gradient: 24.2\n",
      "Epoch 87201/150000, Train Loss: 2553, Val Loss: 2734,  Learning Rate: 0.00100, Train Gradient: 24.2\n",
      "Epoch 87301/150000, Train Loss: 2543, Val Loss: 2725,  Learning Rate: 0.00100, Train Gradient: 24.1\n",
      "Epoch 87401/150000, Train Loss: 2534, Val Loss: 2716,  Learning Rate: 0.00100, Train Gradient: 24.1\n",
      "Epoch 87501/150000, Train Loss: 2525, Val Loss: 2707,  Learning Rate: 0.00100, Train Gradient: 24.0\n",
      "Epoch 87601/150000, Train Loss: 2515, Val Loss: 2698,  Learning Rate: 0.00100, Train Gradient: 23.9\n",
      "Epoch 87701/150000, Train Loss: 2506, Val Loss: 2689,  Learning Rate: 0.00100, Train Gradient: 24.0\n",
      "Epoch 87801/150000, Train Loss: 2497, Val Loss: 2680,  Learning Rate: 0.00100, Train Gradient: 23.8\n",
      "Epoch 87901/150000, Train Loss: 2488, Val Loss: 2671,  Learning Rate: 0.00100, Train Gradient: 23.8\n",
      "Epoch 88001/150000, Train Loss: 2479, Val Loss: 2662,  Learning Rate: 0.00100, Train Gradient: 23.7\n",
      "Epoch 88101/150000, Train Loss: 2469, Val Loss: 2653,  Learning Rate: 0.00100, Train Gradient: 23.6\n",
      "Epoch 88201/150000, Train Loss: 2460, Val Loss: 2644,  Learning Rate: 0.00100, Train Gradient: 23.6\n",
      "Epoch 88301/150000, Train Loss: 2451, Val Loss: 2635,  Learning Rate: 0.00100, Train Gradient: 23.5\n",
      "Epoch 88401/150000, Train Loss: 2442, Val Loss: 2627,  Learning Rate: 0.00100, Train Gradient: 23.4\n",
      "Epoch 88501/150000, Train Loss: 2433, Val Loss: 2618,  Learning Rate: 0.00100, Train Gradient: 23.4\n",
      "Epoch 88601/150000, Train Loss: 2424, Val Loss: 2609,  Learning Rate: 0.00100, Train Gradient: 23.3\n",
      "Epoch 88701/150000, Train Loss: 2415, Val Loss: 2600,  Learning Rate: 0.00100, Train Gradient: 23.2\n",
      "Epoch 88801/150000, Train Loss: 2406, Val Loss: 2591,  Learning Rate: 0.00100, Train Gradient: 23.2\n",
      "Epoch 88901/150000, Train Loss: 2397, Val Loss: 2583,  Learning Rate: 0.00100, Train Gradient: 23.1\n",
      "Epoch 89001/150000, Train Loss: 2388, Val Loss: 2574,  Learning Rate: 0.00100, Train Gradient: 23.0\n",
      "Epoch 89101/150000, Train Loss: 2379, Val Loss: 2565,  Learning Rate: 0.00100, Train Gradient: 23.0\n",
      "Epoch 89201/150000, Train Loss: 2370, Val Loss: 2557,  Learning Rate: 0.00100, Train Gradient: 23.0\n",
      "Epoch 89301/150000, Train Loss: 2361, Val Loss: 2548,  Learning Rate: 0.00100, Train Gradient: 22.9\n",
      "Epoch 89401/150000, Train Loss: 2352, Val Loss: 2539,  Learning Rate: 0.00100, Train Gradient: 22.7\n",
      "Epoch 89501/150000, Train Loss: 2344, Val Loss: 2531,  Learning Rate: 0.00100, Train Gradient: 22.7\n",
      "Epoch 89601/150000, Train Loss: 2335, Val Loss: 2522,  Learning Rate: 0.00100, Train Gradient: 22.7\n",
      "Epoch 89701/150000, Train Loss: 2326, Val Loss: 2514,  Learning Rate: 0.00100, Train Gradient: 22.6\n",
      "Epoch 89801/150000, Train Loss: 2317, Val Loss: 2505,  Learning Rate: 0.00100, Train Gradient: 22.6\n",
      "Epoch 89901/150000, Train Loss: 2308, Val Loss: 2497,  Learning Rate: 0.00100, Train Gradient: 22.5\n",
      "Epoch 90001/150000, Train Loss: 2300, Val Loss: 2488,  Learning Rate: 0.00100, Train Gradient: 22.4\n",
      "Epoch 90101/150000, Train Loss: 2291, Val Loss: 2480,  Learning Rate: 0.00100, Train Gradient: 22.4\n",
      "Epoch 90201/150000, Train Loss: 2282, Val Loss: 2471,  Learning Rate: 0.00100, Train Gradient: 22.3\n",
      "Epoch 90301/150000, Train Loss: 2274, Val Loss: 2463,  Learning Rate: 0.00100, Train Gradient: 22.3\n",
      "Epoch 90401/150000, Train Loss: 2265, Val Loss: 2454,  Learning Rate: 0.00100, Train Gradient: 22.2\n",
      "Epoch 90501/150000, Train Loss: 2257, Val Loss: 2446,  Learning Rate: 0.00100, Train Gradient: 22.1\n",
      "Epoch 90601/150000, Train Loss: 2248, Val Loss: 2438,  Learning Rate: 0.00100, Train Gradient: 22.1\n",
      "Epoch 90701/150000, Train Loss: 2239, Val Loss: 2429,  Learning Rate: 0.00100, Train Gradient: 22.0\n",
      "Epoch 90801/150000, Train Loss: 2231, Val Loss: 2421,  Learning Rate: 0.00100, Train Gradient: 22.0\n",
      "Epoch 90901/150000, Train Loss: 2222, Val Loss: 2413,  Learning Rate: 0.00100, Train Gradient: 21.9\n",
      "Epoch 91001/150000, Train Loss: 2214, Val Loss: 2404,  Learning Rate: 0.00100, Train Gradient: 21.9\n",
      "Epoch 91101/150000, Train Loss: 2205, Val Loss: 2396,  Learning Rate: 0.00100, Train Gradient: 21.8\n",
      "Epoch 91201/150000, Train Loss: 2197, Val Loss: 2388,  Learning Rate: 0.00100, Train Gradient: 21.8\n",
      "Epoch 91301/150000, Train Loss: 2188, Val Loss: 2380,  Learning Rate: 0.00100, Train Gradient: 21.7\n",
      "Epoch 91401/150000, Train Loss: 2180, Val Loss: 2372,  Learning Rate: 0.00100, Train Gradient: 21.6\n",
      "Epoch 91501/150000, Train Loss: 2172, Val Loss: 2363,  Learning Rate: 0.00100, Train Gradient: 21.6\n",
      "Epoch 91601/150000, Train Loss: 2163, Val Loss: 2355,  Learning Rate: 0.00100, Train Gradient: 21.5\n",
      "Epoch 91701/150000, Train Loss: 2155, Val Loss: 2347,  Learning Rate: 0.00100, Train Gradient: 21.4\n",
      "Epoch 91801/150000, Train Loss: 2147, Val Loss: 2339,  Learning Rate: 0.00100, Train Gradient: 21.4\n",
      "Epoch 91901/150000, Train Loss: 2138, Val Loss: 2331,  Learning Rate: 0.00100, Train Gradient: 21.2\n",
      "Epoch 92001/150000, Train Loss: 2130, Val Loss: 2323,  Learning Rate: 0.00100, Train Gradient: 21.3\n",
      "Epoch 92101/150000, Train Loss: 2122, Val Loss: 2315,  Learning Rate: 0.00100, Train Gradient: 21.2\n",
      "Epoch 92201/150000, Train Loss: 2114, Val Loss: 2307,  Learning Rate: 0.00100, Train Gradient: 21.2\n",
      "Epoch 92301/150000, Train Loss: 2105, Val Loss: 2298,  Learning Rate: 0.00100, Train Gradient: 21.1\n",
      "Epoch 92401/150000, Train Loss: 2097, Val Loss: 2290,  Learning Rate: 0.00100, Train Gradient: 21.0\n",
      "Epoch 92501/150000, Train Loss: 2089, Val Loss: 2282,  Learning Rate: 0.00100, Train Gradient: 20.9\n",
      "Epoch 92601/150000, Train Loss: 2081, Val Loss: 2274,  Learning Rate: 0.00100, Train Gradient: 20.9\n",
      "Epoch 92701/150000, Train Loss: 2073, Val Loss: 2266,  Learning Rate: 0.00100, Train Gradient: 20.8\n",
      "Epoch 92801/150000, Train Loss: 2065, Val Loss: 2259,  Learning Rate: 0.00100, Train Gradient: 20.8\n",
      "Epoch 92901/150000, Train Loss: 2057, Val Loss: 2251,  Learning Rate: 0.00100, Train Gradient: 20.8\n",
      "Epoch 93001/150000, Train Loss: 2049, Val Loss: 2243,  Learning Rate: 0.00100, Train Gradient: 20.7\n",
      "Epoch 93101/150000, Train Loss: 2041, Val Loss: 2235,  Learning Rate: 0.00100, Train Gradient: 20.6\n",
      "Epoch 93201/150000, Train Loss: 2033, Val Loss: 2227,  Learning Rate: 0.00100, Train Gradient: 20.7\n",
      "Epoch 93301/150000, Train Loss: 2025, Val Loss: 2219,  Learning Rate: 0.00100, Train Gradient: 20.5\n",
      "Epoch 93401/150000, Train Loss: 2017, Val Loss: 2211,  Learning Rate: 0.00100, Train Gradient: 20.5\n",
      "Epoch 93501/150000, Train Loss: 2009, Val Loss: 2204,  Learning Rate: 0.00100, Train Gradient: 20.4\n",
      "Epoch 93601/150000, Train Loss: 2001, Val Loss: 2196,  Learning Rate: 0.00100, Train Gradient: 20.4\n",
      "Epoch 93701/150000, Train Loss: 1993, Val Loss: 2188,  Learning Rate: 0.00100, Train Gradient: 20.3\n",
      "Epoch 93801/150000, Train Loss: 1985, Val Loss: 2181,  Learning Rate: 0.00100, Train Gradient: 20.2\n",
      "Epoch 93901/150000, Train Loss: 1977, Val Loss: 2173,  Learning Rate: 0.00100, Train Gradient: 20.2\n",
      "Epoch 94001/150000, Train Loss: 1970, Val Loss: 2165,  Learning Rate: 0.00100, Train Gradient: 20.1\n",
      "Epoch 94101/150000, Train Loss: 1962, Val Loss: 2158,  Learning Rate: 0.00100, Train Gradient: 20.1\n",
      "Epoch 94201/150000, Train Loss: 1954, Val Loss: 2150,  Learning Rate: 0.00100, Train Gradient: 20.1\n",
      "Epoch 94301/150000, Train Loss: 1946, Val Loss: 2142,  Learning Rate: 0.00100, Train Gradient: 20.0\n",
      "Epoch 94401/150000, Train Loss: 1938, Val Loss: 2135,  Learning Rate: 0.00100, Train Gradient: 19.9\n",
      "Epoch 94501/150000, Train Loss: 1931, Val Loss: 2127,  Learning Rate: 0.00100, Train Gradient: 19.8\n",
      "Epoch 94601/150000, Train Loss: 1923, Val Loss: 2120,  Learning Rate: 0.00100, Train Gradient: 19.8\n",
      "Epoch 94701/150000, Train Loss: 1915, Val Loss: 2112,  Learning Rate: 0.00100, Train Gradient: 19.7\n",
      "Epoch 94801/150000, Train Loss: 1908, Val Loss: 2105,  Learning Rate: 0.00100, Train Gradient: 19.6\n",
      "Epoch 94901/150000, Train Loss: 1900, Val Loss: 2097,  Learning Rate: 0.00100, Train Gradient: 19.6\n",
      "Epoch 95001/150000, Train Loss: 1893, Val Loss: 2090,  Learning Rate: 0.00100, Train Gradient: 19.6\n",
      "Epoch 95101/150000, Train Loss: 1885, Val Loss: 2083,  Learning Rate: 0.00100, Train Gradient: 19.5\n",
      "Epoch 95201/150000, Train Loss: 1877, Val Loss: 2075,  Learning Rate: 0.00100, Train Gradient: 19.5\n",
      "Epoch 95301/150000, Train Loss: 1870, Val Loss: 2068,  Learning Rate: 0.00100, Train Gradient: 19.4\n",
      "Epoch 95401/150000, Train Loss: 1862, Val Loss: 2061,  Learning Rate: 0.00100, Train Gradient: 19.3\n",
      "Epoch 95501/150000, Train Loss: 1855, Val Loss: 2053,  Learning Rate: 0.00100, Train Gradient: 19.3\n",
      "Epoch 95601/150000, Train Loss: 1847, Val Loss: 2046,  Learning Rate: 0.00100, Train Gradient: 19.2\n",
      "Epoch 95701/150000, Train Loss: 1840, Val Loss: 2039,  Learning Rate: 0.00100, Train Gradient: 19.3\n",
      "Epoch 95801/150000, Train Loss: 1833, Val Loss: 2031,  Learning Rate: 0.00100, Train Gradient: 19.1\n",
      "Epoch 95901/150000, Train Loss: 1825, Val Loss: 2024,  Learning Rate: 0.00100, Train Gradient: 19.1\n",
      "Epoch 96001/150000, Train Loss: 1818, Val Loss: 2017,  Learning Rate: 0.00100, Train Gradient: 19.0\n",
      "Epoch 96101/150000, Train Loss: 1810, Val Loss: 2010,  Learning Rate: 0.00100, Train Gradient: 18.9\n",
      "Epoch 96201/150000, Train Loss: 1803, Val Loss: 2002,  Learning Rate: 0.00100, Train Gradient: 18.8\n",
      "Epoch 96301/150000, Train Loss: 1796, Val Loss: 1995,  Learning Rate: 0.00100, Train Gradient: 18.9\n",
      "Epoch 96401/150000, Train Loss: 1788, Val Loss: 1988,  Learning Rate: 0.00100, Train Gradient: 18.8\n",
      "Epoch 96501/150000, Train Loss: 1781, Val Loss: 1981,  Learning Rate: 0.00100, Train Gradient: 18.7\n",
      "Epoch 96601/150000, Train Loss: 1774, Val Loss: 1974,  Learning Rate: 0.00100, Train Gradient: 18.7\n",
      "Epoch 96701/150000, Train Loss: 1767, Val Loss: 1967,  Learning Rate: 0.00100, Train Gradient: 18.6\n",
      "Epoch 96801/150000, Train Loss: 1759, Val Loss: 1960,  Learning Rate: 0.00100, Train Gradient: 18.6\n",
      "Epoch 96901/150000, Train Loss: 1752, Val Loss: 1953,  Learning Rate: 0.00100, Train Gradient: 18.5\n",
      "Epoch 97001/150000, Train Loss: 1745, Val Loss: 1946,  Learning Rate: 0.00100, Train Gradient: 18.4\n",
      "Epoch 97101/150000, Train Loss: 1738, Val Loss: 1939,  Learning Rate: 0.00100, Train Gradient: 18.3\n",
      "Epoch 97201/150000, Train Loss: 1731, Val Loss: 1932,  Learning Rate: 0.00100, Train Gradient: 18.3\n",
      "Epoch 97301/150000, Train Loss: 1724, Val Loss: 1925,  Learning Rate: 0.00100, Train Gradient: 18.3\n",
      "Epoch 97401/150000, Train Loss: 1717, Val Loss: 1918,  Learning Rate: 0.00100, Train Gradient: 18.2\n",
      "Epoch 97501/150000, Train Loss: 1710, Val Loss: 1911,  Learning Rate: 0.00100, Train Gradient: 18.1\n",
      "Epoch 97601/150000, Train Loss: 1703, Val Loss: 1904,  Learning Rate: 0.00100, Train Gradient: 18.1\n",
      "Epoch 97701/150000, Train Loss: 1696, Val Loss: 1897,  Learning Rate: 0.00100, Train Gradient: 18.1\n",
      "Epoch 97801/150000, Train Loss: 1689, Val Loss: 1890,  Learning Rate: 0.00100, Train Gradient: 18.0\n",
      "Epoch 97901/150000, Train Loss: 1682, Val Loss: 1884,  Learning Rate: 0.00100, Train Gradient: 17.9\n",
      "Epoch 98001/150000, Train Loss: 1675, Val Loss: 1877,  Learning Rate: 0.00100, Train Gradient: 17.9\n",
      "Epoch 98101/150000, Train Loss: 1668, Val Loss: 1870,  Learning Rate: 0.00100, Train Gradient: 17.8\n",
      "Epoch 98201/150000, Train Loss: 1661, Val Loss: 1863,  Learning Rate: 0.00100, Train Gradient: 17.8\n",
      "Epoch 98301/150000, Train Loss: 1654, Val Loss: 1857,  Learning Rate: 0.00100, Train Gradient: 17.7\n",
      "Epoch 98401/150000, Train Loss: 1647, Val Loss: 1850,  Learning Rate: 0.00100, Train Gradient: 17.6\n",
      "Epoch 98501/150000, Train Loss: 1640, Val Loss: 1843,  Learning Rate: 0.00100, Train Gradient: 17.6\n",
      "Epoch 98601/150000, Train Loss: 1633, Val Loss: 1836,  Learning Rate: 0.00100, Train Gradient: 17.5\n",
      "Epoch 98701/150000, Train Loss: 1627, Val Loss: 1830,  Learning Rate: 0.00100, Train Gradient: 17.5\n",
      "Epoch 98801/150000, Train Loss: 1620, Val Loss: 1823,  Learning Rate: 0.00100, Train Gradient: 17.4\n",
      "Epoch 98901/150000, Train Loss: 1613, Val Loss: 1817,  Learning Rate: 0.00100, Train Gradient: 17.4\n",
      "Epoch 99001/150000, Train Loss: 1606, Val Loss: 1810,  Learning Rate: 0.00100, Train Gradient: 17.3\n",
      "Epoch 99101/150000, Train Loss: 1600, Val Loss: 1803,  Learning Rate: 0.00100, Train Gradient: 17.3\n",
      "Epoch 99201/150000, Train Loss: 1593, Val Loss: 1797,  Learning Rate: 0.00100, Train Gradient: 17.2\n",
      "Epoch 99301/150000, Train Loss: 1586, Val Loss: 1790,  Learning Rate: 0.00100, Train Gradient: 17.2\n",
      "Epoch 99401/150000, Train Loss: 1580, Val Loss: 1784,  Learning Rate: 0.00100, Train Gradient: 17.1\n",
      "Epoch 99501/150000, Train Loss: 1573, Val Loss: 1777,  Learning Rate: 0.00100, Train Gradient: 17.0\n",
      "Epoch 99601/150000, Train Loss: 1566, Val Loss: 1771,  Learning Rate: 0.00100, Train Gradient: 17.0\n",
      "Epoch 99701/150000, Train Loss: 1560, Val Loss: 1765,  Learning Rate: 0.00100, Train Gradient: 17.0\n",
      "Epoch 99801/150000, Train Loss: 1553, Val Loss: 1758,  Learning Rate: 0.00100, Train Gradient: 16.9\n",
      "Epoch 99901/150000, Train Loss: 1547, Val Loss: 1752,  Learning Rate: 0.00100, Train Gradient: 16.9\n",
      "Epoch 100001/150000, Train Loss: 1540, Val Loss: 1746,  Learning Rate: 0.00100, Train Gradient: 16.8\n",
      "Epoch 100101/150000, Train Loss: 1534, Val Loss: 1739,  Learning Rate: 0.00100, Train Gradient: 16.7\n",
      "Epoch 100201/150000, Train Loss: 1527, Val Loss: 1733,  Learning Rate: 0.00100, Train Gradient: 16.7\n",
      "Epoch 100301/150000, Train Loss: 1521, Val Loss: 1727,  Learning Rate: 0.00100, Train Gradient: 16.6\n",
      "Epoch 100401/150000, Train Loss: 1514, Val Loss: 1721,  Learning Rate: 0.00100, Train Gradient: 16.5\n",
      "Epoch 100501/150000, Train Loss: 1508, Val Loss: 1714,  Learning Rate: 0.00100, Train Gradient: 16.5\n",
      "Epoch 100601/150000, Train Loss: 1501, Val Loss: 1708,  Learning Rate: 0.00100, Train Gradient: 16.6\n",
      "Epoch 100701/150000, Train Loss: 1495, Val Loss: 1702,  Learning Rate: 0.00100, Train Gradient: 16.6\n",
      "Epoch 100801/150000, Train Loss: 1489, Val Loss: 1696,  Learning Rate: 0.00100, Train Gradient: 16.4\n",
      "Epoch 100901/150000, Train Loss: 1482, Val Loss: 1690,  Learning Rate: 0.00100, Train Gradient: 16.3\n",
      "Epoch 101001/150000, Train Loss: 1476, Val Loss: 1684,  Learning Rate: 0.00100, Train Gradient: 16.3\n",
      "Epoch 101101/150000, Train Loss: 1470, Val Loss: 1677,  Learning Rate: 0.00100, Train Gradient: 16.2\n",
      "Epoch 101201/150000, Train Loss: 1463, Val Loss: 1671,  Learning Rate: 0.00100, Train Gradient: 16.1\n",
      "Epoch 101301/150000, Train Loss: 1457, Val Loss: 1665,  Learning Rate: 0.00100, Train Gradient: 16.1\n",
      "Epoch 101401/150000, Train Loss: 1451, Val Loss: 1659,  Learning Rate: 0.00100, Train Gradient: 16.0\n",
      "Epoch 101501/150000, Train Loss: 1445, Val Loss: 1653,  Learning Rate: 0.00100, Train Gradient: 16.0\n",
      "Epoch 101601/150000, Train Loss: 1439, Val Loss: 1647,  Learning Rate: 0.00100, Train Gradient: 16.0\n",
      "Epoch 101701/150000, Train Loss: 1432, Val Loss: 1641,  Learning Rate: 0.00100, Train Gradient: 15.9\n",
      "Epoch 101801/150000, Train Loss: 1426, Val Loss: 1635,  Learning Rate: 0.00100, Train Gradient: 15.8\n",
      "Epoch 101901/150000, Train Loss: 1420, Val Loss: 1629,  Learning Rate: 0.00100, Train Gradient: 15.9\n",
      "Epoch 102001/150000, Train Loss: 1414, Val Loss: 1623,  Learning Rate: 0.00100, Train Gradient: 15.8\n",
      "Epoch 102101/150000, Train Loss: 1408, Val Loss: 1617,  Learning Rate: 0.00100, Train Gradient: 15.8\n",
      "Epoch 102201/150000, Train Loss: 1402, Val Loss: 1612,  Learning Rate: 0.00100, Train Gradient: 15.7\n",
      "Epoch 102301/150000, Train Loss: 1396, Val Loss: 1606,  Learning Rate: 0.00100, Train Gradient: 15.7\n",
      "Epoch 102401/150000, Train Loss: 1389, Val Loss: 1600,  Learning Rate: 0.00100, Train Gradient: 15.6\n",
      "Epoch 102501/150000, Train Loss: 1383, Val Loss: 1594,  Learning Rate: 0.00100, Train Gradient: 15.6\n",
      "Epoch 102601/150000, Train Loss: 1377, Val Loss: 1588,  Learning Rate: 0.00100, Train Gradient: 15.5\n",
      "Epoch 102701/150000, Train Loss: 1371, Val Loss: 1582,  Learning Rate: 0.00100, Train Gradient: 15.5\n",
      "Epoch 102801/150000, Train Loss: 1365, Val Loss: 1576,  Learning Rate: 0.00100, Train Gradient: 15.4\n",
      "Epoch 102901/150000, Train Loss: 1359, Val Loss: 1571,  Learning Rate: 0.00100, Train Gradient: 15.4\n",
      "Epoch 103001/150000, Train Loss: 1353, Val Loss: 1565,  Learning Rate: 0.00100, Train Gradient: 15.3\n",
      "Epoch 103101/150000, Train Loss: 1347, Val Loss: 1559,  Learning Rate: 0.00100, Train Gradient: 15.3\n",
      "Epoch 103201/150000, Train Loss: 1342, Val Loss: 1554,  Learning Rate: 0.00100, Train Gradient: 15.3\n",
      "Epoch 103301/150000, Train Loss: 1336, Val Loss: 1548,  Learning Rate: 0.00100, Train Gradient: 15.2\n",
      "Epoch 103401/150000, Train Loss: 1330, Val Loss: 1542,  Learning Rate: 0.00100, Train Gradient: 15.2\n",
      "Epoch 103501/150000, Train Loss: 1324, Val Loss: 1537,  Learning Rate: 0.00100, Train Gradient: 15.1\n",
      "Epoch 103601/150000, Train Loss: 1318, Val Loss: 1531,  Learning Rate: 0.00100, Train Gradient: 15.0\n",
      "Epoch 103701/150000, Train Loss: 1312, Val Loss: 1526,  Learning Rate: 0.00100, Train Gradient: 15.0\n",
      "Epoch 103801/150000, Train Loss: 1306, Val Loss: 1520,  Learning Rate: 0.00100, Train Gradient: 14.9\n",
      "Epoch 103901/150000, Train Loss: 1301, Val Loss: 1515,  Learning Rate: 0.00100, Train Gradient: 14.9\n",
      "Epoch 104001/150000, Train Loss: 1295, Val Loss: 1509,  Learning Rate: 0.00100, Train Gradient: 14.9\n",
      "Epoch 104101/150000, Train Loss: 1289, Val Loss: 1504,  Learning Rate: 0.00100, Train Gradient: 14.8\n",
      "Epoch 104201/150000, Train Loss: 1283, Val Loss: 1498,  Learning Rate: 0.00100, Train Gradient: 14.8\n",
      "Epoch 104301/150000, Train Loss: 1277, Val Loss: 1493,  Learning Rate: 0.00100, Train Gradient: 14.7\n",
      "Epoch 104401/150000, Train Loss: 1272, Val Loss: 1487,  Learning Rate: 0.00100, Train Gradient: 14.7\n",
      "Epoch 104501/150000, Train Loss: 1266, Val Loss: 1482,  Learning Rate: 0.00100, Train Gradient: 14.7\n",
      "Epoch 104601/150000, Train Loss: 1260, Val Loss: 1476,  Learning Rate: 0.00100, Train Gradient: 14.6\n",
      "Epoch 104701/150000, Train Loss: 1255, Val Loss: 1471,  Learning Rate: 0.00100, Train Gradient: 14.5\n",
      "Epoch 104801/150000, Train Loss: 1249, Val Loss: 1466,  Learning Rate: 0.00100, Train Gradient: 14.5\n",
      "Epoch 104901/150000, Train Loss: 1244, Val Loss: 1460,  Learning Rate: 0.00100, Train Gradient: 14.5\n",
      "Epoch 105001/150000, Train Loss: 1238, Val Loss: 1455,  Learning Rate: 0.00100, Train Gradient: 14.4\n",
      "Epoch 105101/150000, Train Loss: 1232, Val Loss: 1450,  Learning Rate: 0.00100, Train Gradient: 14.3\n",
      "Epoch 105201/150000, Train Loss: 1227, Val Loss: 1445,  Learning Rate: 0.00100, Train Gradient: 14.3\n",
      "Epoch 105301/150000, Train Loss: 1221, Val Loss: 1439,  Learning Rate: 0.00100, Train Gradient: 14.3\n",
      "Epoch 105401/150000, Train Loss: 1216, Val Loss: 1434,  Learning Rate: 0.00100, Train Gradient: 14.2\n",
      "Epoch 105501/150000, Train Loss: 1210, Val Loss: 1429,  Learning Rate: 0.00100, Train Gradient: 14.2\n",
      "Epoch 105601/150000, Train Loss: 1205, Val Loss: 1424,  Learning Rate: 0.00100, Train Gradient: 14.1\n",
      "Epoch 105701/150000, Train Loss: 1199, Val Loss: 1419,  Learning Rate: 0.00100, Train Gradient: 14.1\n",
      "Epoch 105801/150000, Train Loss: 1194, Val Loss: 1414,  Learning Rate: 0.00100, Train Gradient: 14.0\n",
      "Epoch 105901/150000, Train Loss: 1188, Val Loss: 1408,  Learning Rate: 0.00100, Train Gradient: 14.0\n",
      "Epoch 106001/150000, Train Loss: 1183, Val Loss: 1403,  Learning Rate: 0.00100, Train Gradient: 13.9\n",
      "Epoch 106101/150000, Train Loss: 1178, Val Loss: 1398,  Learning Rate: 0.00100, Train Gradient: 13.9\n",
      "Epoch 106201/150000, Train Loss: 1172, Val Loss: 1393,  Learning Rate: 0.00100, Train Gradient: 13.9\n",
      "Epoch 106301/150000, Train Loss: 1167, Val Loss: 1388,  Learning Rate: 0.00100, Train Gradient: 13.8\n",
      "Epoch 106401/150000, Train Loss: 1161, Val Loss: 1383,  Learning Rate: 0.00100, Train Gradient: 13.8\n",
      "Epoch 106501/150000, Train Loss: 1156, Val Loss: 1378,  Learning Rate: 0.00100, Train Gradient: 13.7\n",
      "Epoch 106601/150000, Train Loss: 1151, Val Loss: 1373,  Learning Rate: 0.00100, Train Gradient: 13.7\n",
      "Epoch 106701/150000, Train Loss: 1146, Val Loss: 1368,  Learning Rate: 0.00100, Train Gradient: 13.6\n",
      "Epoch 106801/150000, Train Loss: 1140, Val Loss: 1363,  Learning Rate: 0.00100, Train Gradient: 13.6\n",
      "Epoch 106901/150000, Train Loss: 1135, Val Loss: 1359,  Learning Rate: 0.00100, Train Gradient: 13.5\n",
      "Epoch 107001/150000, Train Loss: 1130, Val Loss: 1354,  Learning Rate: 0.00100, Train Gradient: 13.5\n",
      "Epoch 107101/150000, Train Loss: 1125, Val Loss: 1349,  Learning Rate: 0.00100, Train Gradient: 13.4\n",
      "Epoch 107201/150000, Train Loss: 1119, Val Loss: 1344,  Learning Rate: 0.00100, Train Gradient: 13.4\n",
      "Epoch 107301/150000, Train Loss: 1114, Val Loss: 1339,  Learning Rate: 0.00100, Train Gradient: 13.3\n",
      "Epoch 107401/150000, Train Loss: 1109, Val Loss: 1334,  Learning Rate: 0.00100, Train Gradient: 13.2\n",
      "Epoch 107501/150000, Train Loss: 1104, Val Loss: 1329,  Learning Rate: 0.00100, Train Gradient: 13.2\n",
      "Epoch 107601/150000, Train Loss: 1099, Val Loss: 1324,  Learning Rate: 0.00100, Train Gradient: 13.2\n",
      "Epoch 107701/150000, Train Loss: 1094, Val Loss: 1320,  Learning Rate: 0.00100, Train Gradient: 13.2\n",
      "Epoch 107801/150000, Train Loss: 1089, Val Loss: 1315,  Learning Rate: 0.00100, Train Gradient: 13.1\n",
      "Epoch 107901/150000, Train Loss: 1083, Val Loss: 1310,  Learning Rate: 0.00100, Train Gradient: 13.0\n",
      "Epoch 108001/150000, Train Loss: 1078, Val Loss: 1306,  Learning Rate: 0.00100, Train Gradient: 13.0\n",
      "Epoch 108101/150000, Train Loss: 1073, Val Loss: 1301,  Learning Rate: 0.00100, Train Gradient: 13.0\n",
      "Epoch 108201/150000, Train Loss: 1068, Val Loss: 1296,  Learning Rate: 0.00100, Train Gradient: 12.9\n",
      "Epoch 108301/150000, Train Loss: 1063, Val Loss: 1292,  Learning Rate: 0.00100, Train Gradient: 12.9\n",
      "Epoch 108401/150000, Train Loss: 1059, Val Loss: 1287,  Learning Rate: 0.00100, Train Gradient: 12.8\n",
      "Epoch 108501/150000, Train Loss: 1054, Val Loss: 1282,  Learning Rate: 0.00100, Train Gradient: 12.8\n",
      "Epoch 108601/150000, Train Loss: 1049, Val Loss: 1278,  Learning Rate: 0.00100, Train Gradient: 12.7\n",
      "Epoch 108701/150000, Train Loss: 1044, Val Loss: 1273,  Learning Rate: 0.00100, Train Gradient: 12.7\n",
      "Epoch 108801/150000, Train Loss: 1039, Val Loss: 1269,  Learning Rate: 0.00100, Train Gradient: 12.6\n",
      "Epoch 108901/150000, Train Loss: 1034, Val Loss: 1264,  Learning Rate: 0.00100, Train Gradient: 12.6\n",
      "Epoch 109001/150000, Train Loss: 1029, Val Loss: 1260,  Learning Rate: 0.00100, Train Gradient: 12.5\n",
      "Epoch 109101/150000, Train Loss: 1024, Val Loss: 1255,  Learning Rate: 0.00100, Train Gradient: 12.5\n",
      "Epoch 109201/150000, Train Loss: 1020, Val Loss: 1251,  Learning Rate: 0.00100, Train Gradient: 12.4\n",
      "Epoch 109301/150000, Train Loss: 1015, Val Loss: 1247,  Learning Rate: 0.00100, Train Gradient: 12.4\n",
      "Epoch 109401/150000, Train Loss: 1010, Val Loss: 1242,  Learning Rate: 0.00100, Train Gradient: 12.4\n",
      "Epoch 109501/150000, Train Loss: 1005, Val Loss: 1238,  Learning Rate: 0.00100, Train Gradient: 12.3\n",
      "Epoch 109601/150000, Train Loss: 1001, Val Loss: 1233,  Learning Rate: 0.00100, Train Gradient: 12.3\n",
      "Epoch 109701/150000, Train Loss: 996, Val Loss: 1229,  Learning Rate: 0.00100, Train Gradient: 12.2\n",
      "Epoch 109801/150000, Train Loss: 991, Val Loss: 1225,  Learning Rate: 0.00100, Train Gradient: 12.2\n",
      "Epoch 109901/150000, Train Loss: 986, Val Loss: 1220,  Learning Rate: 0.00100, Train Gradient: 12.1\n",
      "Epoch 110001/150000, Train Loss: 982, Val Loss: 1216,  Learning Rate: 0.00100, Train Gradient: 12.1\n",
      "Epoch 110101/150000, Train Loss: 977, Val Loss: 1212,  Learning Rate: 0.00100, Train Gradient: 12.0\n",
      "Epoch 110201/150000, Train Loss: 972, Val Loss: 1207,  Learning Rate: 0.00100, Train Gradient: 12.0\n",
      "Epoch 110301/150000, Train Loss: 968, Val Loss: 1203,  Learning Rate: 0.00100, Train Gradient: 12.0\n",
      "Epoch 110401/150000, Train Loss: 963, Val Loss: 1199,  Learning Rate: 0.00100, Train Gradient: 11.9\n",
      "Epoch 110501/150000, Train Loss: 959, Val Loss: 1195,  Learning Rate: 0.00100, Train Gradient: 11.9\n",
      "Epoch 110601/150000, Train Loss: 954, Val Loss: 1190,  Learning Rate: 0.00100, Train Gradient: 11.8\n",
      "Epoch 110701/150000, Train Loss: 950, Val Loss: 1186,  Learning Rate: 0.00100, Train Gradient: 11.8\n",
      "Epoch 110801/150000, Train Loss: 945, Val Loss: 1182,  Learning Rate: 0.00100, Train Gradient: 11.8\n",
      "Epoch 110901/150000, Train Loss: 941, Val Loss: 1178,  Learning Rate: 0.00100, Train Gradient: 11.6\n",
      "Epoch 111001/150000, Train Loss: 936, Val Loss: 1174,  Learning Rate: 0.00100, Train Gradient: 11.7\n",
      "Epoch 111101/150000, Train Loss: 932, Val Loss: 1170,  Learning Rate: 0.00100, Train Gradient: 11.6\n",
      "Epoch 111201/150000, Train Loss: 927, Val Loss: 1165,  Learning Rate: 0.00100, Train Gradient: 11.6\n",
      "Epoch 111301/150000, Train Loss: 923, Val Loss: 1161,  Learning Rate: 0.00100, Train Gradient: 11.5\n",
      "Epoch 111401/150000, Train Loss: 918, Val Loss: 1157,  Learning Rate: 0.00100, Train Gradient: 11.5\n",
      "Epoch 111501/150000, Train Loss: 914, Val Loss: 1153,  Learning Rate: 0.00100, Train Gradient: 11.5\n",
      "Epoch 111601/150000, Train Loss: 909, Val Loss: 1149,  Learning Rate: 0.00100, Train Gradient: 11.4\n",
      "Epoch 111701/150000, Train Loss: 905, Val Loss: 1145,  Learning Rate: 0.00100, Train Gradient: 11.4\n",
      "Epoch 111801/150000, Train Loss: 900, Val Loss: 1140,  Learning Rate: 0.00100, Train Gradient: 11.4\n",
      "Epoch 111901/150000, Train Loss: 896, Val Loss: 1136,  Learning Rate: 0.00100, Train Gradient: 11.3\n",
      "Epoch 112001/150000, Train Loss: 892, Val Loss: 1132,  Learning Rate: 0.00100, Train Gradient: 11.3\n",
      "Epoch 112101/150000, Train Loss: 887, Val Loss: 1128,  Learning Rate: 0.00100, Train Gradient: 11.2\n",
      "Epoch 112201/150000, Train Loss: 883, Val Loss: 1124,  Learning Rate: 0.00100, Train Gradient: 11.2\n",
      "Epoch 112301/150000, Train Loss: 879, Val Loss: 1120,  Learning Rate: 0.00100, Train Gradient: 11.2\n",
      "Epoch 112401/150000, Train Loss: 874, Val Loss: 1116,  Learning Rate: 0.00100, Train Gradient: 11.1\n",
      "Epoch 112501/150000, Train Loss: 870, Val Loss: 1112,  Learning Rate: 0.00100, Train Gradient: 11.0\n",
      "Epoch 112601/150000, Train Loss: 866, Val Loss: 1108,  Learning Rate: 0.00100, Train Gradient: 11.1\n",
      "Epoch 112701/150000, Train Loss: 862, Val Loss: 1104,  Learning Rate: 0.00100, Train Gradient: 11.0\n",
      "Epoch 112801/150000, Train Loss: 857, Val Loss: 1099,  Learning Rate: 0.00100, Train Gradient: 11.0\n",
      "Epoch 112901/150000, Train Loss: 853, Val Loss: 1095,  Learning Rate: 0.00100, Train Gradient: 10.9\n",
      "Epoch 113001/150000, Train Loss: 849, Val Loss: 1091,  Learning Rate: 0.00100, Train Gradient: 10.9\n",
      "Epoch 113101/150000, Train Loss: 844, Val Loss: 1087,  Learning Rate: 0.00100, Train Gradient: 10.9\n",
      "Epoch 113201/150000, Train Loss: 840, Val Loss: 1083,  Learning Rate: 0.00100, Train Gradient: 10.8\n",
      "Epoch 113301/150000, Train Loss: 836, Val Loss: 1079,  Learning Rate: 0.00100, Train Gradient: 10.8\n",
      "Epoch 113401/150000, Train Loss: 832, Val Loss: 1076,  Learning Rate: 0.00100, Train Gradient: 10.7\n",
      "Epoch 113501/150000, Train Loss: 828, Val Loss: 1072,  Learning Rate: 0.00100, Train Gradient: 10.8\n",
      "Epoch 113601/150000, Train Loss: 824, Val Loss: 1068,  Learning Rate: 0.00100, Train Gradient: 10.7\n",
      "Epoch 113701/150000, Train Loss: 820, Val Loss: 1064,  Learning Rate: 0.00100, Train Gradient: 10.6\n",
      "Epoch 113801/150000, Train Loss: 816, Val Loss: 1060,  Learning Rate: 0.00100, Train Gradient: 10.6\n",
      "Epoch 113901/150000, Train Loss: 811, Val Loss: 1056,  Learning Rate: 0.00100, Train Gradient: 10.5\n",
      "Epoch 114001/150000, Train Loss: 807, Val Loss: 1052,  Learning Rate: 0.00100, Train Gradient: 10.7\n",
      "Epoch 114101/150000, Train Loss: 803, Val Loss: 1049,  Learning Rate: 0.00100, Train Gradient: 10.5\n",
      "Epoch 114201/150000, Train Loss: 799, Val Loss: 1045,  Learning Rate: 0.00100, Train Gradient: 10.4\n",
      "Epoch 114301/150000, Train Loss: 795, Val Loss: 1041,  Learning Rate: 0.00100, Train Gradient: 10.4\n",
      "Epoch 114401/150000, Train Loss: 791, Val Loss: 1037,  Learning Rate: 0.00100, Train Gradient: 10.3\n",
      "Epoch 114501/150000, Train Loss: 787, Val Loss: 1033,  Learning Rate: 0.00100, Train Gradient: 10.3\n",
      "Epoch 114601/150000, Train Loss: 783, Val Loss: 1029,  Learning Rate: 0.00100, Train Gradient: 10.3\n",
      "Epoch 114701/150000, Train Loss: 779, Val Loss: 1025,  Learning Rate: 0.00100, Train Gradient: 10.2\n",
      "Epoch 114801/150000, Train Loss: 775, Val Loss: 1021,  Learning Rate: 0.00100, Train Gradient: 10.2\n",
      "Epoch 114901/150000, Train Loss: 772, Val Loss: 1017,  Learning Rate: 0.00100, Train Gradient: 10.2\n",
      "Epoch 115001/150000, Train Loss: 768, Val Loss: 1014,  Learning Rate: 0.00100, Train Gradient: 10.1\n",
      "Epoch 115101/150000, Train Loss: 764, Val Loss: 1010,  Learning Rate: 0.00100, Train Gradient: 10.1\n",
      "Epoch 115201/150000, Train Loss: 760, Val Loss: 1006,  Learning Rate: 0.00100, Train Gradient: 10.0\n",
      "Epoch 115301/150000, Train Loss: 756, Val Loss: 1001,  Learning Rate: 0.00100, Train Gradient: 10.0\n",
      "Epoch 115401/150000, Train Loss: 752, Val Loss: 997,  Learning Rate: 0.00100, Train Gradient: 9.9\n",
      "Epoch 115501/150000, Train Loss: 748, Val Loss: 993,  Learning Rate: 0.00100, Train Gradient: 9.9\n",
      "Epoch 115601/150000, Train Loss: 745, Val Loss: 989,  Learning Rate: 0.00100, Train Gradient: 9.9\n",
      "Epoch 115701/150000, Train Loss: 741, Val Loss: 985,  Learning Rate: 0.00100, Train Gradient: 9.8\n",
      "Epoch 115801/150000, Train Loss: 737, Val Loss: 981,  Learning Rate: 0.00100, Train Gradient: 9.8\n",
      "Epoch 115901/150000, Train Loss: 733, Val Loss: 977,  Learning Rate: 0.00100, Train Gradient: 9.8\n",
      "Epoch 116001/150000, Train Loss: 729, Val Loss: 973,  Learning Rate: 0.00100, Train Gradient: 9.7\n",
      "Epoch 116101/150000, Train Loss: 726, Val Loss: 969,  Learning Rate: 0.00100, Train Gradient: 9.7\n",
      "Epoch 116201/150000, Train Loss: 722, Val Loss: 965,  Learning Rate: 0.00100, Train Gradient: 9.6\n",
      "Epoch 116301/150000, Train Loss: 718, Val Loss: 961,  Learning Rate: 0.00100, Train Gradient: 9.6\n",
      "Epoch 116401/150000, Train Loss: 715, Val Loss: 956,  Learning Rate: 0.00100, Train Gradient: 9.6\n",
      "Epoch 116501/150000, Train Loss: 711, Val Loss: 952,  Learning Rate: 0.00100, Train Gradient: 9.5\n",
      "Epoch 116601/150000, Train Loss: 707, Val Loss: 947,  Learning Rate: 0.00100, Train Gradient: 9.5\n",
      "Epoch 116701/150000, Train Loss: 704, Val Loss: 942,  Learning Rate: 0.00100, Train Gradient: 9.4\n",
      "Epoch 116801/150000, Train Loss: 699, Val Loss: 927,  Learning Rate: 0.00100, Train Gradient: 9.5\n",
      "Epoch 116901/150000, Train Loss: 695, Val Loss: 922,  Learning Rate: 0.00100, Train Gradient: 9.4\n",
      "Epoch 117001/150000, Train Loss: 691, Val Loss: 919,  Learning Rate: 0.00100, Train Gradient: 9.4\n",
      "Epoch 117101/150000, Train Loss: 688, Val Loss: 915,  Learning Rate: 0.00100, Train Gradient: 9.3\n",
      "Epoch 117201/150000, Train Loss: 684, Val Loss: 912,  Learning Rate: 0.00100, Train Gradient: 9.3\n",
      "Epoch 117301/150000, Train Loss: 680, Val Loss: 908,  Learning Rate: 0.00100, Train Gradient: 9.3\n",
      "Epoch 117401/150000, Train Loss: 677, Val Loss: 905,  Learning Rate: 0.00100, Train Gradient: 9.2\n",
      "Epoch 117501/150000, Train Loss: 673, Val Loss: 902,  Learning Rate: 0.00100, Train Gradient: 9.2\n",
      "Epoch 117601/150000, Train Loss: 670, Val Loss: 898,  Learning Rate: 0.00100, Train Gradient: 9.2\n",
      "Epoch 117701/150000, Train Loss: 666, Val Loss: 895,  Learning Rate: 0.00100, Train Gradient: 9.1\n",
      "Epoch 117801/150000, Train Loss: 663, Val Loss: 891,  Learning Rate: 0.00100, Train Gradient: 9.1\n",
      "Epoch 117901/150000, Train Loss: 659, Val Loss: 888,  Learning Rate: 0.00100, Train Gradient: 9.1\n",
      "Epoch 118001/150000, Train Loss: 656, Val Loss: 885,  Learning Rate: 0.00100, Train Gradient: 9.0\n",
      "Epoch 118101/150000, Train Loss: 652, Val Loss: 882,  Learning Rate: 0.00100, Train Gradient: 9.0\n",
      "Epoch 118201/150000, Train Loss: 649, Val Loss: 878,  Learning Rate: 0.00100, Train Gradient: 9.0\n",
      "Epoch 118301/150000, Train Loss: 645, Val Loss: 875,  Learning Rate: 0.00100, Train Gradient: 8.9\n",
      "Epoch 118401/150000, Train Loss: 642, Val Loss: 872,  Learning Rate: 0.00100, Train Gradient: 8.9\n",
      "Epoch 118501/150000, Train Loss: 638, Val Loss: 869,  Learning Rate: 0.00100, Train Gradient: 8.8\n",
      "Epoch 118601/150000, Train Loss: 635, Val Loss: 865,  Learning Rate: 0.00100, Train Gradient: 8.8\n",
      "Epoch 118701/150000, Train Loss: 631, Val Loss: 862,  Learning Rate: 0.00100, Train Gradient: 8.8\n",
      "Epoch 118801/150000, Train Loss: 628, Val Loss: 859,  Learning Rate: 0.00100, Train Gradient: 8.7\n",
      "Epoch 118901/150000, Train Loss: 625, Val Loss: 856,  Learning Rate: 0.00100, Train Gradient: 8.7\n",
      "Epoch 119001/150000, Train Loss: 621, Val Loss: 853,  Learning Rate: 0.00100, Train Gradient: 8.7\n",
      "Epoch 119101/150000, Train Loss: 618, Val Loss: 850,  Learning Rate: 0.00100, Train Gradient: 8.6\n",
      "Epoch 119201/150000, Train Loss: 615, Val Loss: 847,  Learning Rate: 0.00100, Train Gradient: 8.6\n",
      "Epoch 119301/150000, Train Loss: 611, Val Loss: 844,  Learning Rate: 0.00100, Train Gradient: 8.6\n",
      "Epoch 119401/150000, Train Loss: 608, Val Loss: 841,  Learning Rate: 0.00100, Train Gradient: 8.5\n",
      "Epoch 119501/150000, Train Loss: 605, Val Loss: 838,  Learning Rate: 0.00100, Train Gradient: 8.5\n",
      "Epoch 119601/150000, Train Loss: 602, Val Loss: 835,  Learning Rate: 0.00100, Train Gradient: 8.5\n",
      "Epoch 119701/150000, Train Loss: 598, Val Loss: 832,  Learning Rate: 0.00100, Train Gradient: 8.4\n",
      "Epoch 119801/150000, Train Loss: 595, Val Loss: 829,  Learning Rate: 0.00100, Train Gradient: 8.4\n",
      "Epoch 119901/150000, Train Loss: 592, Val Loss: 826,  Learning Rate: 0.00100, Train Gradient: 8.4\n",
      "Epoch 120001/150000, Train Loss: 589, Val Loss: 823,  Learning Rate: 0.00100, Train Gradient: 8.3\n",
      "Epoch 120101/150000, Train Loss: 585, Val Loss: 820,  Learning Rate: 0.00100, Train Gradient: 8.3\n",
      "Epoch 120201/150000, Train Loss: 582, Val Loss: 817,  Learning Rate: 0.00100, Train Gradient: 8.3\n",
      "Epoch 120301/150000, Train Loss: 579, Val Loss: 814,  Learning Rate: 0.00100, Train Gradient: 8.1\n",
      "Epoch 120401/150000, Train Loss: 576, Val Loss: 811,  Learning Rate: 0.00100, Train Gradient: 8.2\n",
      "Epoch 120501/150000, Train Loss: 573, Val Loss: 808,  Learning Rate: 0.00100, Train Gradient: 8.2\n",
      "Epoch 120601/150000, Train Loss: 570, Val Loss: 806,  Learning Rate: 0.00100, Train Gradient: 8.1\n",
      "Epoch 120701/150000, Train Loss: 566, Val Loss: 803,  Learning Rate: 0.00100, Train Gradient: 8.1\n",
      "Epoch 120801/150000, Train Loss: 563, Val Loss: 800,  Learning Rate: 0.00100, Train Gradient: 8.0\n",
      "Epoch 120901/150000, Train Loss: 560, Val Loss: 797,  Learning Rate: 0.00100, Train Gradient: 8.1\n",
      "Epoch 121001/150000, Train Loss: 557, Val Loss: 794,  Learning Rate: 0.00100, Train Gradient: 8.0\n",
      "Epoch 121101/150000, Train Loss: 554, Val Loss: 792,  Learning Rate: 0.00100, Train Gradient: 7.9\n",
      "Epoch 121201/150000, Train Loss: 551, Val Loss: 789,  Learning Rate: 0.00100, Train Gradient: 7.9\n",
      "Epoch 121301/150000, Train Loss: 548, Val Loss: 786,  Learning Rate: 0.00100, Train Gradient: 7.9\n",
      "Epoch 121401/150000, Train Loss: 545, Val Loss: 784,  Learning Rate: 0.00100, Train Gradient: 7.8\n",
      "Epoch 121501/150000, Train Loss: 542, Val Loss: 781,  Learning Rate: 0.00100, Train Gradient: 7.8\n",
      "Epoch 121601/150000, Train Loss: 539, Val Loss: 778,  Learning Rate: 0.00100, Train Gradient: 7.8\n",
      "Epoch 121701/150000, Train Loss: 536, Val Loss: 776,  Learning Rate: 0.00100, Train Gradient: 7.7\n",
      "Epoch 121801/150000, Train Loss: 533, Val Loss: 773,  Learning Rate: 0.00100, Train Gradient: 7.7\n",
      "Epoch 121901/150000, Train Loss: 530, Val Loss: 770,  Learning Rate: 0.00100, Train Gradient: 7.7\n",
      "Epoch 122001/150000, Train Loss: 527, Val Loss: 768,  Learning Rate: 0.00100, Train Gradient: 7.6\n",
      "Epoch 122101/150000, Train Loss: 524, Val Loss: 765,  Learning Rate: 0.00100, Train Gradient: 7.6\n",
      "Epoch 122201/150000, Train Loss: 522, Val Loss: 763,  Learning Rate: 0.00100, Train Gradient: 7.6\n",
      "Epoch 122301/150000, Train Loss: 519, Val Loss: 760,  Learning Rate: 0.00100, Train Gradient: 7.5\n",
      "Epoch 122401/150000, Train Loss: 516, Val Loss: 758,  Learning Rate: 0.00100, Train Gradient: 7.5\n",
      "Epoch 122501/150000, Train Loss: 513, Val Loss: 755,  Learning Rate: 0.00100, Train Gradient: 7.5\n",
      "Epoch 122601/150000, Train Loss: 510, Val Loss: 753,  Learning Rate: 0.00100, Train Gradient: 7.4\n",
      "Epoch 122701/150000, Train Loss: 507, Val Loss: 750,  Learning Rate: 0.00100, Train Gradient: 7.4\n",
      "Epoch 122801/150000, Train Loss: 504, Val Loss: 748,  Learning Rate: 0.00100, Train Gradient: 7.4\n",
      "Epoch 122901/150000, Train Loss: 502, Val Loss: 745,  Learning Rate: 0.00100, Train Gradient: 7.3\n",
      "Epoch 123001/150000, Train Loss: 499, Val Loss: 743,  Learning Rate: 0.00100, Train Gradient: 7.4\n",
      "Epoch 123101/150000, Train Loss: 496, Val Loss: 741,  Learning Rate: 0.00100, Train Gradient: 7.3\n",
      "Epoch 123201/150000, Train Loss: 493, Val Loss: 738,  Learning Rate: 0.00100, Train Gradient: 7.3\n",
      "Epoch 123301/150000, Train Loss: 491, Val Loss: 736,  Learning Rate: 0.00100, Train Gradient: 7.2\n",
      "Epoch 123401/150000, Train Loss: 488, Val Loss: 734,  Learning Rate: 0.00100, Train Gradient: 7.0\n",
      "Epoch 123501/150000, Train Loss: 485, Val Loss: 731,  Learning Rate: 0.00100, Train Gradient: 7.1\n",
      "Epoch 123601/150000, Train Loss: 482, Val Loss: 728,  Learning Rate: 0.00100, Train Gradient: 7.1\n",
      "Epoch 123701/150000, Train Loss: 480, Val Loss: 725,  Learning Rate: 0.00100, Train Gradient: 7.1\n",
      "Epoch 123801/150000, Train Loss: 477, Val Loss: 722,  Learning Rate: 0.00100, Train Gradient: 7.0\n",
      "Epoch 123901/150000, Train Loss: 474, Val Loss: 719,  Learning Rate: 0.00100, Train Gradient: 7.1\n",
      "Epoch 124001/150000, Train Loss: 472, Val Loss: 717,  Learning Rate: 0.00100, Train Gradient: 7.0\n",
      "Epoch 124101/150000, Train Loss: 469, Val Loss: 714,  Learning Rate: 0.00100, Train Gradient: 6.9\n",
      "Epoch 124201/150000, Train Loss: 466, Val Loss: 712,  Learning Rate: 0.00100, Train Gradient: 6.9\n",
      "Epoch 124301/150000, Train Loss: 464, Val Loss: 710,  Learning Rate: 0.00100, Train Gradient: 6.9\n",
      "Epoch 124401/150000, Train Loss: 461, Val Loss: 707,  Learning Rate: 0.00100, Train Gradient: 6.9\n",
      "Epoch 124501/150000, Train Loss: 458, Val Loss: 705,  Learning Rate: 0.00100, Train Gradient: 6.8\n",
      "Epoch 124601/150000, Train Loss: 456, Val Loss: 703,  Learning Rate: 0.00100, Train Gradient: 6.8\n",
      "Epoch 124701/150000, Train Loss: 453, Val Loss: 701,  Learning Rate: 0.00100, Train Gradient: 6.8\n",
      "Epoch 124801/150000, Train Loss: 450, Val Loss: 698,  Learning Rate: 0.00100, Train Gradient: 6.8\n",
      "Epoch 124901/150000, Train Loss: 448, Val Loss: 696,  Learning Rate: 0.00100, Train Gradient: 6.8\n",
      "Epoch 125001/150000, Train Loss: 445, Val Loss: 694,  Learning Rate: 0.00100, Train Gradient: 6.7\n",
      "Epoch 125101/150000, Train Loss: 443, Val Loss: 691,  Learning Rate: 0.00100, Train Gradient: 6.6\n",
      "Epoch 125201/150000, Train Loss: 440, Val Loss: 689,  Learning Rate: 0.00100, Train Gradient: 6.6\n",
      "Epoch 125301/150000, Train Loss: 438, Val Loss: 687,  Learning Rate: 0.00100, Train Gradient: 6.6\n",
      "Epoch 125401/150000, Train Loss: 435, Val Loss: 684,  Learning Rate: 0.00100, Train Gradient: 6.6\n",
      "Epoch 125501/150000, Train Loss: 433, Val Loss: 682,  Learning Rate: 0.00100, Train Gradient: 6.5\n",
      "Epoch 125601/150000, Train Loss: 430, Val Loss: 680,  Learning Rate: 0.00100, Train Gradient: 6.5\n",
      "Epoch 125701/150000, Train Loss: 428, Val Loss: 677,  Learning Rate: 0.00100, Train Gradient: 6.5\n",
      "Epoch 125801/150000, Train Loss: 425, Val Loss: 675,  Learning Rate: 0.00100, Train Gradient: 6.4\n",
      "Epoch 125901/150000, Train Loss: 423, Val Loss: 673,  Learning Rate: 0.00100, Train Gradient: 6.4\n",
      "Epoch 126001/150000, Train Loss: 420, Val Loss: 671,  Learning Rate: 0.00100, Train Gradient: 6.4\n",
      "Epoch 126101/150000, Train Loss: 418, Val Loss: 668,  Learning Rate: 0.00100, Train Gradient: 6.4\n",
      "Epoch 126201/150000, Train Loss: 415, Val Loss: 666,  Learning Rate: 0.00100, Train Gradient: 6.3\n",
      "Epoch 126301/150000, Train Loss: 413, Val Loss: 664,  Learning Rate: 0.00100, Train Gradient: 6.3\n",
      "Epoch 126401/150000, Train Loss: 410, Val Loss: 661,  Learning Rate: 0.00100, Train Gradient: 6.3\n",
      "Epoch 126501/150000, Train Loss: 408, Val Loss: 659,  Learning Rate: 0.00100, Train Gradient: 6.3\n",
      "Epoch 126601/150000, Train Loss: 406, Val Loss: 657,  Learning Rate: 0.00100, Train Gradient: 6.2\n",
      "Epoch 126701/150000, Train Loss: 403, Val Loss: 654,  Learning Rate: 0.00100, Train Gradient: 6.2\n",
      "Epoch 126801/150000, Train Loss: 401, Val Loss: 652,  Learning Rate: 0.00100, Train Gradient: 6.2\n",
      "Epoch 126901/150000, Train Loss: 398, Val Loss: 650,  Learning Rate: 0.00100, Train Gradient: 6.1\n",
      "Epoch 127001/150000, Train Loss: 396, Val Loss: 647,  Learning Rate: 0.00100, Train Gradient: 6.1\n",
      "Epoch 127101/150000, Train Loss: 394, Val Loss: 645,  Learning Rate: 0.00100, Train Gradient: 6.1\n",
      "Epoch 127201/150000, Train Loss: 391, Val Loss: 643,  Learning Rate: 0.00100, Train Gradient: 6.1\n",
      "Epoch 127301/150000, Train Loss: 389, Val Loss: 641,  Learning Rate: 0.00100, Train Gradient: 6.0\n",
      "Epoch 127401/150000, Train Loss: 387, Val Loss: 638,  Learning Rate: 0.00100, Train Gradient: 6.0\n",
      "Epoch 127501/150000, Train Loss: 384, Val Loss: 636,  Learning Rate: 0.00100, Train Gradient: 6.0\n",
      "Epoch 127601/150000, Train Loss: 382, Val Loss: 634,  Learning Rate: 0.00100, Train Gradient: 5.9\n",
      "Epoch 127701/150000, Train Loss: 380, Val Loss: 633,  Learning Rate: 0.00100, Train Gradient: 5.9\n",
      "Epoch 127801/150000, Train Loss: 378, Val Loss: 631,  Learning Rate: 0.00100, Train Gradient: 5.8\n",
      "Epoch 127901/150000, Train Loss: 375, Val Loss: 630,  Learning Rate: 0.00100, Train Gradient: 5.8\n",
      "Epoch 128001/150000, Train Loss: 373, Val Loss: 628,  Learning Rate: 0.00100, Train Gradient: 5.8\n",
      "Epoch 128101/150000, Train Loss: 371, Val Loss: 627,  Learning Rate: 0.00100, Train Gradient: 5.8\n",
      "Epoch 128201/150000, Train Loss: 369, Val Loss: 626,  Learning Rate: 0.00100, Train Gradient: 5.7\n",
      "Epoch 128301/150000, Train Loss: 366, Val Loss: 624,  Learning Rate: 0.00100, Train Gradient: 5.7\n",
      "Epoch 128401/150000, Train Loss: 364, Val Loss: 623,  Learning Rate: 0.00100, Train Gradient: 5.7\n",
      "Epoch 128501/150000, Train Loss: 362, Val Loss: 621,  Learning Rate: 0.00100, Train Gradient: 5.7\n",
      "Epoch 128601/150000, Train Loss: 360, Val Loss: 619,  Learning Rate: 0.00100, Train Gradient: 5.7\n",
      "Epoch 128701/150000, Train Loss: 358, Val Loss: 617,  Learning Rate: 0.00100, Train Gradient: 5.6\n",
      "Epoch 128801/150000, Train Loss: 356, Val Loss: 616,  Learning Rate: 0.00100, Train Gradient: 5.7\n",
      "Epoch 128901/150000, Train Loss: 353, Val Loss: 615,  Learning Rate: 0.00100, Train Gradient: 5.6\n",
      "Epoch 129001/150000, Train Loss: 351, Val Loss: 614,  Learning Rate: 0.00100, Train Gradient: 5.5\n",
      "Epoch 129101/150000, Train Loss: 349, Val Loss: 615,  Learning Rate: 0.00100, Train Gradient: 5.5\n",
      "Epoch 129201/150000, Train Loss: 347, Val Loss: 616,  Learning Rate: 0.00100, Train Gradient: 5.5\n",
      "Epoch 129301/150000, Train Loss: 345, Val Loss: 617,  Learning Rate: 0.00100, Train Gradient: 5.4\n",
      "Epoch 129401/150000, Train Loss: 343, Val Loss: 618,  Learning Rate: 0.00100, Train Gradient: 5.4\n",
      "Epoch 129501/150000, Train Loss: 341, Val Loss: 618,  Learning Rate: 0.00100, Train Gradient: 5.4\n",
      "Epoch 129601/150000, Train Loss: 339, Val Loss: 618,  Learning Rate: 0.00100, Train Gradient: 5.4\n",
      "Epoch 129701/150000, Train Loss: 337, Val Loss: 618,  Learning Rate: 0.00100, Train Gradient: 5.3\n",
      "Epoch 129801/150000, Train Loss: 335, Val Loss: 617,  Learning Rate: 0.00100, Train Gradient: 5.3\n",
      "Epoch 129901/150000, Train Loss: 333, Val Loss: 617,  Learning Rate: 0.00100, Train Gradient: 5.3\n",
      "Epoch 130001/150000, Train Loss: 331, Val Loss: 616,  Learning Rate: 0.00100, Train Gradient: 5.3\n",
      "Epoch 130101/150000, Train Loss: 329, Val Loss: 616,  Learning Rate: 0.00100, Train Gradient: 5.2\n",
      "Epoch 130201/150000, Train Loss: 327, Val Loss: 617,  Learning Rate: 0.00100, Train Gradient: 5.2\n",
      "Epoch 130301/150000, Train Loss: 324, Val Loss: 619,  Learning Rate: 0.00100, Train Gradient: 5.2\n",
      "Epoch 130401/150000, Train Loss: 322, Val Loss: 621,  Learning Rate: 0.00100, Train Gradient: 5.2\n",
      "Epoch 130501/150000, Train Loss: 320, Val Loss: 621,  Learning Rate: 0.00100, Train Gradient: 5.1\n",
      "Epoch 130601/150000, Train Loss: 318, Val Loss: 621,  Learning Rate: 0.00100, Train Gradient: 5.1\n",
      "Epoch 130701/150000, Train Loss: 317, Val Loss: 621,  Learning Rate: 0.00100, Train Gradient: 5.1\n",
      "Epoch 130801/150000, Train Loss: 315, Val Loss: 620,  Learning Rate: 0.00100, Train Gradient: 5.0\n",
      "Epoch 130901/150000, Train Loss: 313, Val Loss: 620,  Learning Rate: 0.00100, Train Gradient: 5.0\n",
      "Epoch 131001/150000, Train Loss: 311, Val Loss: 620,  Learning Rate: 0.00100, Train Gradient: 5.0\n",
      "Epoch 131101/150000, Train Loss: 309, Val Loss: 619,  Learning Rate: 0.00100, Train Gradient: 4.9\n",
      "Epoch 131201/150000, Train Loss: 307, Val Loss: 618,  Learning Rate: 0.00100, Train Gradient: 5.0\n",
      "Epoch 131301/150000, Train Loss: 305, Val Loss: 618,  Learning Rate: 0.00100, Train Gradient: 4.9\n",
      "Epoch 131401/150000, Train Loss: 303, Val Loss: 618,  Learning Rate: 0.00100, Train Gradient: 4.9\n",
      "Epoch 131501/150000, Train Loss: 301, Val Loss: 617,  Learning Rate: 0.00100, Train Gradient: 4.9\n",
      "Epoch 131601/150000, Train Loss: 299, Val Loss: 616,  Learning Rate: 0.00100, Train Gradient: 4.9\n",
      "Epoch 131701/150000, Train Loss: 297, Val Loss: 616,  Learning Rate: 0.00100, Train Gradient: 4.8\n",
      "Epoch 131801/150000, Train Loss: 295, Val Loss: 615,  Learning Rate: 0.00100, Train Gradient: 4.8\n",
      "Epoch 131901/150000, Train Loss: 293, Val Loss: 614,  Learning Rate: 0.00100, Train Gradient: 4.8\n",
      "Epoch 132001/150000, Train Loss: 291, Val Loss: 613,  Learning Rate: 0.00100, Train Gradient: 4.8\n",
      "Epoch 132101/150000, Train Loss: 289, Val Loss: 612,  Learning Rate: 0.00100, Train Gradient: 4.8\n",
      "Epoch 132201/150000, Train Loss: 287, Val Loss: 611,  Learning Rate: 0.00100, Train Gradient: 4.8\n",
      "Epoch 132301/150000, Train Loss: 285, Val Loss: 610,  Learning Rate: 0.00100, Train Gradient: 4.6\n",
      "Epoch 132401/150000, Train Loss: 283, Val Loss: 610,  Learning Rate: 0.00100, Train Gradient: 4.7\n",
      "Epoch 132501/150000, Train Loss: 280, Val Loss: 610,  Learning Rate: 0.00100, Train Gradient: 4.7\n",
      "Epoch 132601/150000, Train Loss: 278, Val Loss: 610,  Learning Rate: 0.00100, Train Gradient: 4.6\n",
      "Epoch 132701/150000, Train Loss: 276, Val Loss: 609,  Learning Rate: 0.00100, Train Gradient: 4.6\n",
      "Epoch 132801/150000, Train Loss: 273, Val Loss: 607,  Learning Rate: 0.00100, Train Gradient: 4.6\n",
      "Epoch 132901/150000, Train Loss: 271, Val Loss: 605,  Learning Rate: 0.00100, Train Gradient: 4.6\n",
      "Epoch 133001/150000, Train Loss: 269, Val Loss: 602,  Learning Rate: 0.00100, Train Gradient: 4.5\n",
      "Epoch 133101/150000, Train Loss: 267, Val Loss: 597,  Learning Rate: 0.00100, Train Gradient: 4.6\n",
      "Epoch 133201/150000, Train Loss: 265, Val Loss: 590,  Learning Rate: 0.00100, Train Gradient: 4.5\n",
      "Epoch 133301/150000, Train Loss: 263, Val Loss: 586,  Learning Rate: 0.00100, Train Gradient: 4.4\n",
      "Epoch 133401/150000, Train Loss: 261, Val Loss: 583,  Learning Rate: 0.00100, Train Gradient: 4.4\n",
      "Epoch 133501/150000, Train Loss: 259, Val Loss: 581,  Learning Rate: 0.00100, Train Gradient: 4.4\n",
      "Epoch 133601/150000, Train Loss: 257, Val Loss: 579,  Learning Rate: 0.00100, Train Gradient: 4.3\n",
      "Epoch 133701/150000, Train Loss: 255, Val Loss: 577,  Learning Rate: 0.00100, Train Gradient: 4.3\n",
      "Epoch 133801/150000, Train Loss: 254, Val Loss: 576,  Learning Rate: 0.00100, Train Gradient: 4.3\n",
      "Epoch 133901/150000, Train Loss: 252, Val Loss: 576,  Learning Rate: 0.00100, Train Gradient: 4.2\n",
      "Epoch 134001/150000, Train Loss: 250, Val Loss: 575,  Learning Rate: 0.00100, Train Gradient: 4.2\n",
      "Epoch 134101/150000, Train Loss: 249, Val Loss: 574,  Learning Rate: 0.00100, Train Gradient: 4.0\n",
      "Epoch 134201/150000, Train Loss: 247, Val Loss: 572,  Learning Rate: 0.00100, Train Gradient: 4.1\n",
      "Epoch 134301/150000, Train Loss: 246, Val Loss: 571,  Learning Rate: 0.00100, Train Gradient: 4.1\n",
      "Epoch 134401/150000, Train Loss: 244, Val Loss: 569,  Learning Rate: 0.00100, Train Gradient: 4.1\n",
      "Early stopping at epoch 134476 with validation loss 568.4712524414062.\n",
      "Test Loss: 518.3284301757812\n",
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.0001, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 30105, Val Loss: 30606,  Learning Rate: 0.00010, Train Gradient: 225.3\n",
      "Epoch 101/150000, Train Loss: 30097, Val Loss: 30598,  Learning Rate: 0.00009, Train Gradient: 225.2\n",
      "Epoch 201/150000, Train Loss: 30090, Val Loss: 30590,  Learning Rate: 0.00009, Train Gradient: 225.1\n",
      "Epoch 301/150000, Train Loss: 30083, Val Loss: 30583,  Learning Rate: 0.00009, Train Gradient: 225.1\n",
      "Epoch 401/150000, Train Loss: 30076, Val Loss: 30576,  Learning Rate: 0.00009, Train Gradient: 225.0\n",
      "Epoch 501/150000, Train Loss: 30069, Val Loss: 30569,  Learning Rate: 0.00009, Train Gradient: 225.0\n",
      "Epoch 601/150000, Train Loss: 30061, Val Loss: 30562,  Learning Rate: 0.00009, Train Gradient: 224.9\n",
      "Epoch 701/150000, Train Loss: 30054, Val Loss: 30554,  Learning Rate: 0.00009, Train Gradient: 224.9\n",
      "Epoch 801/150000, Train Loss: 30046, Val Loss: 30546,  Learning Rate: 0.00009, Train Gradient: 224.8\n",
      "Epoch 901/150000, Train Loss: 30038, Val Loss: 30538,  Learning Rate: 0.00009, Train Gradient: 224.7\n",
      "Epoch 1001/150000, Train Loss: 30029, Val Loss: 30528,  Learning Rate: 0.00009, Train Gradient: 224.7\n",
      "Epoch 1101/150000, Train Loss: 30019, Val Loss: 30518,  Learning Rate: 0.00009, Train Gradient: 224.6\n",
      "Epoch 1201/150000, Train Loss: 30007, Val Loss: 30506,  Learning Rate: 0.00009, Train Gradient: 224.5\n",
      "Epoch 1301/150000, Train Loss: 29993, Val Loss: 30493,  Learning Rate: 0.00009, Train Gradient: 224.4\n",
      "Epoch 1401/150000, Train Loss: 29978, Val Loss: 30477,  Learning Rate: 0.00009, Train Gradient: 224.3\n",
      "Epoch 1501/150000, Train Loss: 29962, Val Loss: 30461,  Learning Rate: 0.00009, Train Gradient: 224.2\n",
      "Epoch 1601/150000, Train Loss: 29947, Val Loss: 30446,  Learning Rate: 0.00009, Train Gradient: 224.1\n",
      "Epoch 1701/150000, Train Loss: 29932, Val Loss: 30431,  Learning Rate: 0.00009, Train Gradient: 223.9\n",
      "Epoch 1801/150000, Train Loss: 29917, Val Loss: 30416,  Learning Rate: 0.00009, Train Gradient: 223.8\n",
      "Epoch 1901/150000, Train Loss: 29903, Val Loss: 30401,  Learning Rate: 0.00009, Train Gradient: 223.7\n",
      "Epoch 2001/150000, Train Loss: 29888, Val Loss: 30387,  Learning Rate: 0.00009, Train Gradient: 223.6\n",
      "Epoch 2101/150000, Train Loss: 29874, Val Loss: 30373,  Learning Rate: 0.00009, Train Gradient: 223.5\n",
      "Epoch 2201/150000, Train Loss: 29861, Val Loss: 30359,  Learning Rate: 0.00009, Train Gradient: 223.4\n",
      "Epoch 2301/150000, Train Loss: 29847, Val Loss: 30345,  Learning Rate: 0.00009, Train Gradient: 223.2\n",
      "Epoch 2401/150000, Train Loss: 29833, Val Loss: 30330,  Learning Rate: 0.00009, Train Gradient: 223.1\n",
      "Epoch 2501/150000, Train Loss: 29817, Val Loss: 30315,  Learning Rate: 0.00009, Train Gradient: 223.0\n",
      "Epoch 2601/150000, Train Loss: 29800, Val Loss: 30298,  Learning Rate: 0.00009, Train Gradient: 222.9\n",
      "Epoch 2701/150000, Train Loss: 29780, Val Loss: 30277,  Learning Rate: 0.00009, Train Gradient: 222.7\n",
      "Epoch 2801/150000, Train Loss: 29757, Val Loss: 30254,  Learning Rate: 0.00009, Train Gradient: 222.5\n",
      "Epoch 2901/150000, Train Loss: 29732, Val Loss: 30229,  Learning Rate: 0.00009, Train Gradient: 222.3\n",
      "Epoch 3001/150000, Train Loss: 29708, Val Loss: 30204,  Learning Rate: 0.00009, Train Gradient: 222.1\n",
      "Epoch 3101/150000, Train Loss: 29686, Val Loss: 30182,  Learning Rate: 0.00009, Train Gradient: 221.9\n",
      "Epoch 3201/150000, Train Loss: 29665, Val Loss: 30161,  Learning Rate: 0.00009, Train Gradient: 221.7\n",
      "Epoch 3301/150000, Train Loss: 29647, Val Loss: 30142,  Learning Rate: 0.00009, Train Gradient: 221.5\n",
      "Epoch 3401/150000, Train Loss: 29629, Val Loss: 30125,  Learning Rate: 0.00009, Train Gradient: 221.3\n",
      "Epoch 3501/150000, Train Loss: 29613, Val Loss: 30108,  Learning Rate: 0.00009, Train Gradient: 221.2\n",
      "Epoch 3601/150000, Train Loss: 29598, Val Loss: 30093,  Learning Rate: 0.00009, Train Gradient: 221.0\n",
      "Epoch 3701/150000, Train Loss: 29583, Val Loss: 30078,  Learning Rate: 0.00009, Train Gradient: 220.9\n",
      "Epoch 3801/150000, Train Loss: 29569, Val Loss: 30064,  Learning Rate: 0.00009, Train Gradient: 220.7\n",
      "Epoch 3901/150000, Train Loss: 29556, Val Loss: 30050,  Learning Rate: 0.00009, Train Gradient: 220.6\n",
      "Epoch 4001/150000, Train Loss: 29543, Val Loss: 30037,  Learning Rate: 0.00009, Train Gradient: 220.5\n",
      "Epoch 4101/150000, Train Loss: 29530, Val Loss: 30024,  Learning Rate: 0.00009, Train Gradient: 220.4\n",
      "Epoch 4201/150000, Train Loss: 29517, Val Loss: 30012,  Learning Rate: 0.00009, Train Gradient: 220.3\n",
      "Epoch 4301/150000, Train Loss: 29505, Val Loss: 29999,  Learning Rate: 0.00009, Train Gradient: 220.1\n",
      "Epoch 4401/150000, Train Loss: 29494, Val Loss: 29987,  Learning Rate: 0.00009, Train Gradient: 220.0\n",
      "Epoch 4501/150000, Train Loss: 29482, Val Loss: 29976,  Learning Rate: 0.00009, Train Gradient: 219.9\n",
      "Epoch 4601/150000, Train Loss: 29471, Val Loss: 29964,  Learning Rate: 0.00009, Train Gradient: 219.8\n",
      "Epoch 4701/150000, Train Loss: 29459, Val Loss: 29953,  Learning Rate: 0.00009, Train Gradient: 219.7\n",
      "Epoch 4801/150000, Train Loss: 29448, Val Loss: 29942,  Learning Rate: 0.00009, Train Gradient: 219.6\n",
      "Epoch 4901/150000, Train Loss: 29437, Val Loss: 29931,  Learning Rate: 0.00009, Train Gradient: 219.5\n",
      "Epoch 5001/150000, Train Loss: 29427, Val Loss: 29920,  Learning Rate: 0.00009, Train Gradient: 219.4\n",
      "Epoch 5101/150000, Train Loss: 29416, Val Loss: 29909,  Learning Rate: 0.00009, Train Gradient: 219.3\n",
      "Epoch 5201/150000, Train Loss: 29406, Val Loss: 29899,  Learning Rate: 0.00009, Train Gradient: 219.2\n",
      "Epoch 5301/150000, Train Loss: 29395, Val Loss: 29888,  Learning Rate: 0.00009, Train Gradient: 219.1\n",
      "Epoch 5401/150000, Train Loss: 29385, Val Loss: 29878,  Learning Rate: 0.00009, Train Gradient: 219.0\n",
      "Epoch 5501/150000, Train Loss: 29375, Val Loss: 29868,  Learning Rate: 0.00009, Train Gradient: 218.9\n",
      "Epoch 5601/150000, Train Loss: 29365, Val Loss: 29858,  Learning Rate: 0.00009, Train Gradient: 218.8\n",
      "Epoch 5701/150000, Train Loss: 29355, Val Loss: 29848,  Learning Rate: 0.00009, Train Gradient: 218.7\n",
      "Epoch 5801/150000, Train Loss: 29346, Val Loss: 29838,  Learning Rate: 0.00009, Train Gradient: 218.6\n",
      "Epoch 5901/150000, Train Loss: 29336, Val Loss: 29828,  Learning Rate: 0.00009, Train Gradient: 218.5\n",
      "Epoch 6001/150000, Train Loss: 29326, Val Loss: 29818,  Learning Rate: 0.00009, Train Gradient: 218.4\n",
      "Epoch 6101/150000, Train Loss: 29317, Val Loss: 29808,  Learning Rate: 0.00009, Train Gradient: 218.4\n",
      "Epoch 6201/150000, Train Loss: 29307, Val Loss: 29799,  Learning Rate: 0.00009, Train Gradient: 218.3\n",
      "Epoch 6301/150000, Train Loss: 29298, Val Loss: 29789,  Learning Rate: 0.00009, Train Gradient: 218.2\n",
      "Epoch 6401/150000, Train Loss: 29289, Val Loss: 29780,  Learning Rate: 0.00009, Train Gradient: 218.1\n",
      "Epoch 6501/150000, Train Loss: 29279, Val Loss: 29771,  Learning Rate: 0.00009, Train Gradient: 218.0\n",
      "Epoch 6601/150000, Train Loss: 29270, Val Loss: 29761,  Learning Rate: 0.00009, Train Gradient: 217.9\n",
      "Epoch 6701/150000, Train Loss: 29261, Val Loss: 29752,  Learning Rate: 0.00009, Train Gradient: 217.8\n",
      "Epoch 6801/150000, Train Loss: 29252, Val Loss: 29743,  Learning Rate: 0.00009, Train Gradient: 217.7\n",
      "Epoch 6901/150000, Train Loss: 29243, Val Loss: 29734,  Learning Rate: 0.00009, Train Gradient: 217.6\n",
      "Epoch 7001/150000, Train Loss: 29234, Val Loss: 29725,  Learning Rate: 0.00009, Train Gradient: 217.6\n",
      "Epoch 7101/150000, Train Loss: 29225, Val Loss: 29715,  Learning Rate: 0.00009, Train Gradient: 217.5\n",
      "Epoch 7201/150000, Train Loss: 29216, Val Loss: 29706,  Learning Rate: 0.00009, Train Gradient: 217.4\n",
      "Epoch 7301/150000, Train Loss: 29207, Val Loss: 29697,  Learning Rate: 0.00009, Train Gradient: 217.3\n",
      "Epoch 7401/150000, Train Loss: 29198, Val Loss: 29689,  Learning Rate: 0.00009, Train Gradient: 217.2\n",
      "Epoch 7501/150000, Train Loss: 29189, Val Loss: 29680,  Learning Rate: 0.00009, Train Gradient: 217.1\n",
      "Epoch 7601/150000, Train Loss: 29181, Val Loss: 29671,  Learning Rate: 0.00009, Train Gradient: 217.1\n",
      "Epoch 7701/150000, Train Loss: 29172, Val Loss: 29662,  Learning Rate: 0.00009, Train Gradient: 217.0\n",
      "Epoch 7801/150000, Train Loss: 29163, Val Loss: 29653,  Learning Rate: 0.00009, Train Gradient: 216.9\n",
      "Epoch 7901/150000, Train Loss: 29155, Val Loss: 29644,  Learning Rate: 0.00009, Train Gradient: 216.8\n",
      "Epoch 8001/150000, Train Loss: 29146, Val Loss: 29636,  Learning Rate: 0.00009, Train Gradient: 216.7\n",
      "Epoch 8101/150000, Train Loss: 29137, Val Loss: 29627,  Learning Rate: 0.00009, Train Gradient: 216.6\n",
      "Epoch 8201/150000, Train Loss: 29129, Val Loss: 29618,  Learning Rate: 0.00009, Train Gradient: 216.6\n",
      "Epoch 8301/150000, Train Loss: 29120, Val Loss: 29610,  Learning Rate: 0.00009, Train Gradient: 216.5\n",
      "Epoch 8401/150000, Train Loss: 29112, Val Loss: 29601,  Learning Rate: 0.00009, Train Gradient: 216.4\n",
      "Epoch 8501/150000, Train Loss: 29103, Val Loss: 29593,  Learning Rate: 0.00009, Train Gradient: 216.3\n",
      "Epoch 8601/150000, Train Loss: 29095, Val Loss: 29584,  Learning Rate: 0.00009, Train Gradient: 216.2\n",
      "Epoch 8701/150000, Train Loss: 29086, Val Loss: 29576,  Learning Rate: 0.00009, Train Gradient: 216.2\n",
      "Epoch 8801/150000, Train Loss: 29078, Val Loss: 29567,  Learning Rate: 0.00009, Train Gradient: 216.1\n",
      "Epoch 8901/150000, Train Loss: 29070, Val Loss: 29559,  Learning Rate: 0.00009, Train Gradient: 216.0\n",
      "Epoch 9001/150000, Train Loss: 29061, Val Loss: 29550,  Learning Rate: 0.00009, Train Gradient: 215.9\n",
      "Epoch 9101/150000, Train Loss: 29053, Val Loss: 29542,  Learning Rate: 0.00009, Train Gradient: 215.8\n",
      "Epoch 9201/150000, Train Loss: 29045, Val Loss: 29533,  Learning Rate: 0.00009, Train Gradient: 215.8\n",
      "Epoch 9301/150000, Train Loss: 29036, Val Loss: 29525,  Learning Rate: 0.00009, Train Gradient: 215.7\n",
      "Epoch 9401/150000, Train Loss: 29028, Val Loss: 29517,  Learning Rate: 0.00009, Train Gradient: 215.6\n",
      "Epoch 9501/150000, Train Loss: 29020, Val Loss: 29508,  Learning Rate: 0.00009, Train Gradient: 215.5\n",
      "Epoch 9601/150000, Train Loss: 29012, Val Loss: 29500,  Learning Rate: 0.00009, Train Gradient: 215.5\n",
      "Epoch 9701/150000, Train Loss: 29003, Val Loss: 29492,  Learning Rate: 0.00009, Train Gradient: 215.4\n",
      "Epoch 9801/150000, Train Loss: 28995, Val Loss: 29483,  Learning Rate: 0.00009, Train Gradient: 215.3\n",
      "Epoch 9901/150000, Train Loss: 28987, Val Loss: 29475,  Learning Rate: 0.00009, Train Gradient: 215.2\n",
      "Epoch 10001/150000, Train Loss: 28979, Val Loss: 29467,  Learning Rate: 0.00009, Train Gradient: 215.1\n",
      "Epoch 10101/150000, Train Loss: 28971, Val Loss: 29458,  Learning Rate: 0.00009, Train Gradient: 215.1\n",
      "Epoch 10201/150000, Train Loss: 28963, Val Loss: 29450,  Learning Rate: 0.00009, Train Gradient: 215.0\n",
      "Epoch 10301/150000, Train Loss: 28954, Val Loss: 29442,  Learning Rate: 0.00009, Train Gradient: 214.9\n",
      "Epoch 10401/150000, Train Loss: 28946, Val Loss: 29434,  Learning Rate: 0.00009, Train Gradient: 214.8\n",
      "Epoch 10501/150000, Train Loss: 28938, Val Loss: 29426,  Learning Rate: 0.00009, Train Gradient: 214.8\n",
      "Epoch 10601/150000, Train Loss: 28930, Val Loss: 29417,  Learning Rate: 0.00009, Train Gradient: 214.7\n",
      "Epoch 10701/150000, Train Loss: 28922, Val Loss: 29409,  Learning Rate: 0.00009, Train Gradient: 214.6\n",
      "Epoch 10801/150000, Train Loss: 28914, Val Loss: 29401,  Learning Rate: 0.00009, Train Gradient: 214.5\n",
      "Epoch 10901/150000, Train Loss: 28906, Val Loss: 29393,  Learning Rate: 0.00009, Train Gradient: 214.5\n",
      "Epoch 11001/150000, Train Loss: 28898, Val Loss: 29385,  Learning Rate: 0.00009, Train Gradient: 214.4\n",
      "Epoch 11101/150000, Train Loss: 28890, Val Loss: 29377,  Learning Rate: 0.00009, Train Gradient: 214.3\n",
      "Epoch 11201/150000, Train Loss: 28882, Val Loss: 29369,  Learning Rate: 0.00009, Train Gradient: 214.2\n",
      "Epoch 11301/150000, Train Loss: 28874, Val Loss: 29360,  Learning Rate: 0.00009, Train Gradient: 214.2\n",
      "Epoch 11401/150000, Train Loss: 28866, Val Loss: 29352,  Learning Rate: 0.00009, Train Gradient: 214.1\n",
      "Epoch 11501/150000, Train Loss: 28858, Val Loss: 29344,  Learning Rate: 0.00009, Train Gradient: 214.0\n",
      "Epoch 11601/150000, Train Loss: 28850, Val Loss: 29336,  Learning Rate: 0.00009, Train Gradient: 213.9\n",
      "Epoch 11701/150000, Train Loss: 28842, Val Loss: 29328,  Learning Rate: 0.00009, Train Gradient: 213.8\n",
      "Epoch 11801/150000, Train Loss: 28834, Val Loss: 29320,  Learning Rate: 0.00009, Train Gradient: 213.8\n",
      "Epoch 11901/150000, Train Loss: 28826, Val Loss: 29312,  Learning Rate: 0.00009, Train Gradient: 213.7\n",
      "Epoch 12001/150000, Train Loss: 28818, Val Loss: 29304,  Learning Rate: 0.00009, Train Gradient: 213.6\n",
      "Epoch 12101/150000, Train Loss: 28810, Val Loss: 29296,  Learning Rate: 0.00009, Train Gradient: 213.5\n",
      "Epoch 12201/150000, Train Loss: 28802, Val Loss: 29288,  Learning Rate: 0.00009, Train Gradient: 213.5\n",
      "Epoch 12301/150000, Train Loss: 28794, Val Loss: 29280,  Learning Rate: 0.00009, Train Gradient: 213.4\n",
      "Epoch 12401/150000, Train Loss: 28786, Val Loss: 29272,  Learning Rate: 0.00009, Train Gradient: 213.3\n",
      "Epoch 12501/150000, Train Loss: 28779, Val Loss: 29264,  Learning Rate: 0.00009, Train Gradient: 213.2\n",
      "Epoch 12601/150000, Train Loss: 28771, Val Loss: 29256,  Learning Rate: 0.00009, Train Gradient: 213.2\n",
      "Epoch 12701/150000, Train Loss: 28763, Val Loss: 29248,  Learning Rate: 0.00009, Train Gradient: 213.1\n",
      "Epoch 12801/150000, Train Loss: 28755, Val Loss: 29240,  Learning Rate: 0.00009, Train Gradient: 213.0\n",
      "Epoch 12901/150000, Train Loss: 28747, Val Loss: 29232,  Learning Rate: 0.00009, Train Gradient: 212.9\n",
      "Epoch 13001/150000, Train Loss: 28739, Val Loss: 29224,  Learning Rate: 0.00009, Train Gradient: 212.9\n",
      "Epoch 13101/150000, Train Loss: 28731, Val Loss: 29216,  Learning Rate: 0.00009, Train Gradient: 212.8\n",
      "Epoch 13201/150000, Train Loss: 28723, Val Loss: 29208,  Learning Rate: 0.00009, Train Gradient: 212.7\n",
      "Epoch 13301/150000, Train Loss: 28716, Val Loss: 29200,  Learning Rate: 0.00009, Train Gradient: 212.7\n",
      "Epoch 13401/150000, Train Loss: 28708, Val Loss: 29192,  Learning Rate: 0.00009, Train Gradient: 212.6\n",
      "Epoch 13501/150000, Train Loss: 28700, Val Loss: 29185,  Learning Rate: 0.00009, Train Gradient: 212.5\n",
      "Epoch 13601/150000, Train Loss: 28692, Val Loss: 29177,  Learning Rate: 0.00009, Train Gradient: 212.4\n",
      "Epoch 13701/150000, Train Loss: 28684, Val Loss: 29169,  Learning Rate: 0.00009, Train Gradient: 212.4\n",
      "Epoch 13801/150000, Train Loss: 28677, Val Loss: 29161,  Learning Rate: 0.00009, Train Gradient: 212.3\n",
      "Epoch 13901/150000, Train Loss: 28669, Val Loss: 29153,  Learning Rate: 0.00009, Train Gradient: 212.2\n",
      "Epoch 14001/150000, Train Loss: 28661, Val Loss: 29145,  Learning Rate: 0.00009, Train Gradient: 212.1\n",
      "Epoch 14101/150000, Train Loss: 28653, Val Loss: 29137,  Learning Rate: 0.00009, Train Gradient: 212.1\n",
      "Epoch 14201/150000, Train Loss: 28645, Val Loss: 29129,  Learning Rate: 0.00009, Train Gradient: 212.0\n",
      "Epoch 14301/150000, Train Loss: 28638, Val Loss: 29121,  Learning Rate: 0.00009, Train Gradient: 211.9\n",
      "Epoch 14401/150000, Train Loss: 28630, Val Loss: 29113,  Learning Rate: 0.00009, Train Gradient: 211.8\n",
      "Epoch 14501/150000, Train Loss: 28622, Val Loss: 29106,  Learning Rate: 0.00009, Train Gradient: 211.8\n",
      "Epoch 14601/150000, Train Loss: 28614, Val Loss: 29098,  Learning Rate: 0.00009, Train Gradient: 211.7\n",
      "Epoch 14701/150000, Train Loss: 28606, Val Loss: 29090,  Learning Rate: 0.00009, Train Gradient: 211.6\n",
      "Epoch 14801/150000, Train Loss: 28599, Val Loss: 29082,  Learning Rate: 0.00009, Train Gradient: 211.5\n",
      "Epoch 14901/150000, Train Loss: 28591, Val Loss: 29074,  Learning Rate: 0.00009, Train Gradient: 211.5\n",
      "Epoch 15001/150000, Train Loss: 28583, Val Loss: 29066,  Learning Rate: 0.00009, Train Gradient: 211.4\n",
      "Epoch 15101/150000, Train Loss: 28575, Val Loss: 29059,  Learning Rate: 0.00009, Train Gradient: 211.3\n",
      "Epoch 15201/150000, Train Loss: 28568, Val Loss: 29051,  Learning Rate: 0.00009, Train Gradient: 211.2\n",
      "Epoch 15301/150000, Train Loss: 28560, Val Loss: 29043,  Learning Rate: 0.00009, Train Gradient: 211.2\n",
      "Epoch 15401/150000, Train Loss: 28552, Val Loss: 29035,  Learning Rate: 0.00009, Train Gradient: 211.1\n",
      "Epoch 15501/150000, Train Loss: 28545, Val Loss: 29027,  Learning Rate: 0.00009, Train Gradient: 211.0\n",
      "Epoch 15601/150000, Train Loss: 28537, Val Loss: 29019,  Learning Rate: 0.00009, Train Gradient: 210.9\n",
      "Epoch 15701/150000, Train Loss: 28529, Val Loss: 29012,  Learning Rate: 0.00009, Train Gradient: 210.9\n",
      "Epoch 15801/150000, Train Loss: 28521, Val Loss: 29004,  Learning Rate: 0.00009, Train Gradient: 210.8\n",
      "Epoch 15901/150000, Train Loss: 28514, Val Loss: 28996,  Learning Rate: 0.00009, Train Gradient: 210.7\n",
      "Epoch 16001/150000, Train Loss: 28506, Val Loss: 28988,  Learning Rate: 0.00009, Train Gradient: 210.7\n",
      "Epoch 16101/150000, Train Loss: 28498, Val Loss: 28980,  Learning Rate: 0.00009, Train Gradient: 210.6\n",
      "Epoch 16201/150000, Train Loss: 28491, Val Loss: 28973,  Learning Rate: 0.00009, Train Gradient: 210.5\n",
      "Epoch 16301/150000, Train Loss: 28483, Val Loss: 28965,  Learning Rate: 0.00009, Train Gradient: 210.4\n",
      "Epoch 16401/150000, Train Loss: 28475, Val Loss: 28957,  Learning Rate: 0.00009, Train Gradient: 210.4\n",
      "Epoch 16501/150000, Train Loss: 28468, Val Loss: 28949,  Learning Rate: 0.00009, Train Gradient: 210.3\n",
      "Epoch 16601/150000, Train Loss: 28460, Val Loss: 28942,  Learning Rate: 0.00009, Train Gradient: 210.2\n",
      "Epoch 16701/150000, Train Loss: 28452, Val Loss: 28934,  Learning Rate: 0.00009, Train Gradient: 210.1\n",
      "Epoch 16801/150000, Train Loss: 28444, Val Loss: 28926,  Learning Rate: 0.00009, Train Gradient: 210.1\n",
      "Epoch 16901/150000, Train Loss: 28437, Val Loss: 28918,  Learning Rate: 0.00009, Train Gradient: 210.0\n",
      "Epoch 17001/150000, Train Loss: 28429, Val Loss: 28910,  Learning Rate: 0.00009, Train Gradient: 209.9\n",
      "Epoch 17101/150000, Train Loss: 28421, Val Loss: 28903,  Learning Rate: 0.00009, Train Gradient: 209.8\n",
      "Epoch 17201/150000, Train Loss: 28414, Val Loss: 28895,  Learning Rate: 0.00009, Train Gradient: 209.8\n",
      "Epoch 17301/150000, Train Loss: 28406, Val Loss: 28887,  Learning Rate: 0.00009, Train Gradient: 209.7\n",
      "Epoch 17401/150000, Train Loss: 28399, Val Loss: 28880,  Learning Rate: 0.00009, Train Gradient: 209.6\n",
      "Epoch 17501/150000, Train Loss: 28391, Val Loss: 28872,  Learning Rate: 0.00009, Train Gradient: 209.6\n",
      "Epoch 17601/150000, Train Loss: 28383, Val Loss: 28864,  Learning Rate: 0.00009, Train Gradient: 209.5\n",
      "Epoch 17701/150000, Train Loss: 28376, Val Loss: 28856,  Learning Rate: 0.00009, Train Gradient: 209.4\n",
      "Epoch 17801/150000, Train Loss: 28368, Val Loss: 28849,  Learning Rate: 0.00009, Train Gradient: 209.3\n",
      "Epoch 17901/150000, Train Loss: 28360, Val Loss: 28841,  Learning Rate: 0.00009, Train Gradient: 209.3\n",
      "Epoch 18001/150000, Train Loss: 28353, Val Loss: 28833,  Learning Rate: 0.00009, Train Gradient: 209.2\n",
      "Epoch 18101/150000, Train Loss: 28345, Val Loss: 28825,  Learning Rate: 0.00009, Train Gradient: 209.1\n",
      "Epoch 18201/150000, Train Loss: 28337, Val Loss: 28818,  Learning Rate: 0.00009, Train Gradient: 209.0\n",
      "Epoch 18301/150000, Train Loss: 28330, Val Loss: 28810,  Learning Rate: 0.00009, Train Gradient: 209.0\n",
      "Epoch 18401/150000, Train Loss: 28322, Val Loss: 28802,  Learning Rate: 0.00009, Train Gradient: 208.9\n",
      "Epoch 18501/150000, Train Loss: 28315, Val Loss: 28795,  Learning Rate: 0.00009, Train Gradient: 208.8\n",
      "Epoch 18601/150000, Train Loss: 28307, Val Loss: 28787,  Learning Rate: 0.00009, Train Gradient: 208.8\n",
      "Epoch 18701/150000, Train Loss: 28299, Val Loss: 28779,  Learning Rate: 0.00009, Train Gradient: 208.7\n",
      "Epoch 18801/150000, Train Loss: 28292, Val Loss: 28771,  Learning Rate: 0.00009, Train Gradient: 208.6\n",
      "Epoch 18901/150000, Train Loss: 28284, Val Loss: 28764,  Learning Rate: 0.00009, Train Gradient: 208.5\n",
      "Epoch 19001/150000, Train Loss: 28277, Val Loss: 28756,  Learning Rate: 0.00009, Train Gradient: 208.5\n",
      "Epoch 19101/150000, Train Loss: 28269, Val Loss: 28748,  Learning Rate: 0.00009, Train Gradient: 208.4\n",
      "Epoch 19201/150000, Train Loss: 28261, Val Loss: 28741,  Learning Rate: 0.00009, Train Gradient: 208.3\n",
      "Epoch 19301/150000, Train Loss: 28254, Val Loss: 28733,  Learning Rate: 0.00009, Train Gradient: 208.2\n",
      "Epoch 19401/150000, Train Loss: 28246, Val Loss: 28725,  Learning Rate: 0.00009, Train Gradient: 208.2\n",
      "Epoch 19501/150000, Train Loss: 28239, Val Loss: 28718,  Learning Rate: 0.00009, Train Gradient: 208.1\n",
      "Epoch 19601/150000, Train Loss: 28231, Val Loss: 28710,  Learning Rate: 0.00009, Train Gradient: 208.0\n",
      "Epoch 19701/150000, Train Loss: 28224, Val Loss: 28702,  Learning Rate: 0.00009, Train Gradient: 207.9\n",
      "Epoch 19801/150000, Train Loss: 28216, Val Loss: 28695,  Learning Rate: 0.00009, Train Gradient: 207.9\n",
      "Epoch 19901/150000, Train Loss: 28208, Val Loss: 28687,  Learning Rate: 0.00009, Train Gradient: 207.8\n",
      "Epoch 20001/150000, Train Loss: 28201, Val Loss: 28679,  Learning Rate: 0.00009, Train Gradient: 207.7\n",
      "Epoch 20101/150000, Train Loss: 28193, Val Loss: 28672,  Learning Rate: 0.00009, Train Gradient: 207.7\n",
      "Epoch 20201/150000, Train Loss: 28186, Val Loss: 28664,  Learning Rate: 0.00009, Train Gradient: 207.6\n",
      "Epoch 20301/150000, Train Loss: 28178, Val Loss: 28656,  Learning Rate: 0.00009, Train Gradient: 207.5\n",
      "Epoch 20401/150000, Train Loss: 28171, Val Loss: 28649,  Learning Rate: 0.00009, Train Gradient: 207.4\n",
      "Epoch 20501/150000, Train Loss: 28163, Val Loss: 28641,  Learning Rate: 0.00009, Train Gradient: 207.4\n",
      "Epoch 20601/150000, Train Loss: 28155, Val Loss: 28633,  Learning Rate: 0.00009, Train Gradient: 207.3\n",
      "Epoch 20701/150000, Train Loss: 28148, Val Loss: 28626,  Learning Rate: 0.00009, Train Gradient: 207.2\n",
      "Epoch 20801/150000, Train Loss: 28140, Val Loss: 28618,  Learning Rate: 0.00009, Train Gradient: 207.1\n",
      "Epoch 20901/150000, Train Loss: 28133, Val Loss: 28611,  Learning Rate: 0.00009, Train Gradient: 207.1\n",
      "Epoch 21001/150000, Train Loss: 28125, Val Loss: 28603,  Learning Rate: 0.00009, Train Gradient: 207.0\n",
      "Epoch 21101/150000, Train Loss: 28118, Val Loss: 28595,  Learning Rate: 0.00009, Train Gradient: 206.9\n",
      "Epoch 21201/150000, Train Loss: 28110, Val Loss: 28588,  Learning Rate: 0.00009, Train Gradient: 206.9\n",
      "Epoch 21301/150000, Train Loss: 28103, Val Loss: 28580,  Learning Rate: 0.00009, Train Gradient: 206.8\n",
      "Epoch 21401/150000, Train Loss: 28095, Val Loss: 28572,  Learning Rate: 0.00009, Train Gradient: 206.7\n",
      "Epoch 21501/150000, Train Loss: 28088, Val Loss: 28565,  Learning Rate: 0.00009, Train Gradient: 206.6\n",
      "Epoch 21601/150000, Train Loss: 28080, Val Loss: 28557,  Learning Rate: 0.00009, Train Gradient: 206.6\n",
      "Epoch 21701/150000, Train Loss: 28072, Val Loss: 28549,  Learning Rate: 0.00009, Train Gradient: 206.5\n",
      "Epoch 21801/150000, Train Loss: 28065, Val Loss: 28542,  Learning Rate: 0.00009, Train Gradient: 206.4\n",
      "Epoch 21901/150000, Train Loss: 28057, Val Loss: 28534,  Learning Rate: 0.00009, Train Gradient: 206.3\n",
      "Epoch 22001/150000, Train Loss: 28050, Val Loss: 28527,  Learning Rate: 0.00009, Train Gradient: 206.3\n",
      "Epoch 22101/150000, Train Loss: 28042, Val Loss: 28519,  Learning Rate: 0.00009, Train Gradient: 206.2\n",
      "Epoch 22201/150000, Train Loss: 28035, Val Loss: 28511,  Learning Rate: 0.00009, Train Gradient: 206.1\n",
      "Epoch 22301/150000, Train Loss: 28027, Val Loss: 28504,  Learning Rate: 0.00009, Train Gradient: 206.0\n",
      "Epoch 22401/150000, Train Loss: 28020, Val Loss: 28496,  Learning Rate: 0.00009, Train Gradient: 206.0\n",
      "Epoch 22501/150000, Train Loss: 28012, Val Loss: 28489,  Learning Rate: 0.00009, Train Gradient: 205.9\n",
      "Epoch 22601/150000, Train Loss: 28005, Val Loss: 28481,  Learning Rate: 0.00009, Train Gradient: 205.8\n",
      "Epoch 22701/150000, Train Loss: 27997, Val Loss: 28473,  Learning Rate: 0.00009, Train Gradient: 205.8\n",
      "Epoch 22801/150000, Train Loss: 27990, Val Loss: 28466,  Learning Rate: 0.00009, Train Gradient: 205.7\n",
      "Epoch 22901/150000, Train Loss: 27982, Val Loss: 28458,  Learning Rate: 0.00009, Train Gradient: 205.6\n",
      "Epoch 23001/150000, Train Loss: 27975, Val Loss: 28451,  Learning Rate: 0.00009, Train Gradient: 205.5\n",
      "Epoch 23101/150000, Train Loss: 27967, Val Loss: 28443,  Learning Rate: 0.00009, Train Gradient: 205.5\n",
      "Epoch 23201/150000, Train Loss: 27960, Val Loss: 28435,  Learning Rate: 0.00009, Train Gradient: 205.4\n",
      "Epoch 23301/150000, Train Loss: 27952, Val Loss: 28428,  Learning Rate: 0.00009, Train Gradient: 205.3\n",
      "Epoch 23401/150000, Train Loss: 27945, Val Loss: 28420,  Learning Rate: 0.00009, Train Gradient: 205.2\n",
      "Epoch 23501/150000, Train Loss: 27937, Val Loss: 28413,  Learning Rate: 0.00009, Train Gradient: 205.2\n",
      "Epoch 23601/150000, Train Loss: 27930, Val Loss: 28405,  Learning Rate: 0.00009, Train Gradient: 205.1\n",
      "Epoch 23701/150000, Train Loss: 27922, Val Loss: 28398,  Learning Rate: 0.00009, Train Gradient: 205.0\n",
      "Epoch 23801/150000, Train Loss: 27915, Val Loss: 28390,  Learning Rate: 0.00009, Train Gradient: 205.0\n",
      "Epoch 23901/150000, Train Loss: 27907, Val Loss: 28383,  Learning Rate: 0.00009, Train Gradient: 204.9\n",
      "Epoch 24001/150000, Train Loss: 27900, Val Loss: 28375,  Learning Rate: 0.00009, Train Gradient: 204.8\n",
      "Epoch 24101/150000, Train Loss: 27893, Val Loss: 28367,  Learning Rate: 0.00009, Train Gradient: 204.7\n",
      "Epoch 24201/150000, Train Loss: 27885, Val Loss: 28360,  Learning Rate: 0.00009, Train Gradient: 204.7\n",
      "Epoch 24301/150000, Train Loss: 27878, Val Loss: 28352,  Learning Rate: 0.00009, Train Gradient: 204.6\n",
      "Epoch 24401/150000, Train Loss: 27870, Val Loss: 28345,  Learning Rate: 0.00009, Train Gradient: 204.5\n",
      "Epoch 24501/150000, Train Loss: 27863, Val Loss: 28337,  Learning Rate: 0.00009, Train Gradient: 204.4\n",
      "Epoch 24601/150000, Train Loss: 27855, Val Loss: 28330,  Learning Rate: 0.00009, Train Gradient: 204.4\n",
      "Epoch 24701/150000, Train Loss: 27848, Val Loss: 28322,  Learning Rate: 0.00009, Train Gradient: 204.3\n",
      "Epoch 24801/150000, Train Loss: 27840, Val Loss: 28315,  Learning Rate: 0.00009, Train Gradient: 204.2\n",
      "Epoch 24901/150000, Train Loss: 27833, Val Loss: 28307,  Learning Rate: 0.00009, Train Gradient: 204.2\n",
      "Epoch 25001/150000, Train Loss: 27825, Val Loss: 28300,  Learning Rate: 0.00009, Train Gradient: 204.1\n",
      "Epoch 25101/150000, Train Loss: 27818, Val Loss: 28292,  Learning Rate: 0.00009, Train Gradient: 204.0\n",
      "Epoch 25201/150000, Train Loss: 27811, Val Loss: 28284,  Learning Rate: 0.00009, Train Gradient: 203.9\n",
      "Epoch 25301/150000, Train Loss: 27803, Val Loss: 28277,  Learning Rate: 0.00009, Train Gradient: 203.9\n",
      "Epoch 25401/150000, Train Loss: 27796, Val Loss: 28269,  Learning Rate: 0.00009, Train Gradient: 203.8\n",
      "Epoch 25501/150000, Train Loss: 27788, Val Loss: 28262,  Learning Rate: 0.00009, Train Gradient: 203.7\n",
      "Epoch 25601/150000, Train Loss: 27781, Val Loss: 28254,  Learning Rate: 0.00009, Train Gradient: 203.6\n",
      "Epoch 25701/150000, Train Loss: 27773, Val Loss: 28247,  Learning Rate: 0.00009, Train Gradient: 203.6\n",
      "Epoch 25801/150000, Train Loss: 27766, Val Loss: 28239,  Learning Rate: 0.00009, Train Gradient: 203.5\n",
      "Epoch 25901/150000, Train Loss: 27759, Val Loss: 28232,  Learning Rate: 0.00009, Train Gradient: 203.4\n",
      "Epoch 26001/150000, Train Loss: 27751, Val Loss: 28224,  Learning Rate: 0.00009, Train Gradient: 203.3\n",
      "Epoch 26101/150000, Train Loss: 27744, Val Loss: 28217,  Learning Rate: 0.00009, Train Gradient: 203.3\n",
      "Epoch 26201/150000, Train Loss: 27736, Val Loss: 28209,  Learning Rate: 0.00009, Train Gradient: 203.2\n",
      "Epoch 26301/150000, Train Loss: 27729, Val Loss: 28202,  Learning Rate: 0.00009, Train Gradient: 203.1\n",
      "Epoch 26401/150000, Train Loss: 27722, Val Loss: 28194,  Learning Rate: 0.00009, Train Gradient: 203.1\n",
      "Epoch 26501/150000, Train Loss: 27714, Val Loss: 28187,  Learning Rate: 0.00009, Train Gradient: 203.0\n",
      "Epoch 26601/150000, Train Loss: 27707, Val Loss: 28179,  Learning Rate: 0.00009, Train Gradient: 202.9\n",
      "Epoch 26701/150000, Train Loss: 27699, Val Loss: 28172,  Learning Rate: 0.00009, Train Gradient: 202.8\n",
      "Epoch 26801/150000, Train Loss: 27692, Val Loss: 28164,  Learning Rate: 0.00009, Train Gradient: 202.8\n",
      "Epoch 26901/150000, Train Loss: 27685, Val Loss: 28157,  Learning Rate: 0.00009, Train Gradient: 202.7\n",
      "Epoch 27001/150000, Train Loss: 27677, Val Loss: 28149,  Learning Rate: 0.00009, Train Gradient: 202.6\n",
      "Epoch 27101/150000, Train Loss: 27670, Val Loss: 28142,  Learning Rate: 0.00009, Train Gradient: 202.5\n",
      "Epoch 27201/150000, Train Loss: 27662, Val Loss: 28134,  Learning Rate: 0.00009, Train Gradient: 202.5\n",
      "Epoch 27301/150000, Train Loss: 27655, Val Loss: 28127,  Learning Rate: 0.00009, Train Gradient: 202.4\n",
      "Epoch 27401/150000, Train Loss: 27648, Val Loss: 28119,  Learning Rate: 0.00009, Train Gradient: 202.3\n",
      "Epoch 27501/150000, Train Loss: 27640, Val Loss: 28112,  Learning Rate: 0.00009, Train Gradient: 202.3\n",
      "Epoch 27601/150000, Train Loss: 27633, Val Loss: 28104,  Learning Rate: 0.00009, Train Gradient: 202.2\n",
      "Epoch 27701/150000, Train Loss: 27625, Val Loss: 28097,  Learning Rate: 0.00009, Train Gradient: 202.1\n",
      "Epoch 27801/150000, Train Loss: 27618, Val Loss: 28089,  Learning Rate: 0.00009, Train Gradient: 202.0\n",
      "Epoch 27901/150000, Train Loss: 27611, Val Loss: 28082,  Learning Rate: 0.00009, Train Gradient: 202.0\n",
      "Epoch 28001/150000, Train Loss: 27603, Val Loss: 28074,  Learning Rate: 0.00009, Train Gradient: 201.9\n",
      "Epoch 28101/150000, Train Loss: 27596, Val Loss: 28067,  Learning Rate: 0.00009, Train Gradient: 201.8\n",
      "Epoch 28201/150000, Train Loss: 27588, Val Loss: 28060,  Learning Rate: 0.00009, Train Gradient: 201.7\n",
      "Epoch 28301/150000, Train Loss: 27581, Val Loss: 28052,  Learning Rate: 0.00009, Train Gradient: 201.7\n",
      "Epoch 28401/150000, Train Loss: 27574, Val Loss: 28045,  Learning Rate: 0.00009, Train Gradient: 201.6\n",
      "Epoch 28501/150000, Train Loss: 27566, Val Loss: 28037,  Learning Rate: 0.00009, Train Gradient: 201.5\n",
      "Epoch 28601/150000, Train Loss: 27559, Val Loss: 28030,  Learning Rate: 0.00009, Train Gradient: 201.4\n",
      "Epoch 28701/150000, Train Loss: 27552, Val Loss: 28022,  Learning Rate: 0.00009, Train Gradient: 201.4\n",
      "Epoch 28801/150000, Train Loss: 27544, Val Loss: 28015,  Learning Rate: 0.00009, Train Gradient: 201.3\n",
      "Epoch 28901/150000, Train Loss: 27537, Val Loss: 28007,  Learning Rate: 0.00009, Train Gradient: 201.2\n",
      "Epoch 29001/150000, Train Loss: 27530, Val Loss: 28000,  Learning Rate: 0.00009, Train Gradient: 201.2\n",
      "Epoch 29101/150000, Train Loss: 27522, Val Loss: 27993,  Learning Rate: 0.00009, Train Gradient: 201.1\n",
      "Epoch 29201/150000, Train Loss: 27515, Val Loss: 27985,  Learning Rate: 0.00009, Train Gradient: 201.0\n",
      "Epoch 29301/150000, Train Loss: 27508, Val Loss: 27978,  Learning Rate: 0.00009, Train Gradient: 200.9\n",
      "Epoch 29401/150000, Train Loss: 27500, Val Loss: 27970,  Learning Rate: 0.00009, Train Gradient: 200.9\n",
      "Epoch 29501/150000, Train Loss: 27493, Val Loss: 27963,  Learning Rate: 0.00009, Train Gradient: 200.8\n",
      "Epoch 29601/150000, Train Loss: 27486, Val Loss: 27955,  Learning Rate: 0.00009, Train Gradient: 200.7\n",
      "Epoch 29701/150000, Train Loss: 27478, Val Loss: 27948,  Learning Rate: 0.00009, Train Gradient: 200.6\n",
      "Epoch 29801/150000, Train Loss: 27471, Val Loss: 27941,  Learning Rate: 0.00009, Train Gradient: 200.6\n",
      "Epoch 29901/150000, Train Loss: 27464, Val Loss: 27933,  Learning Rate: 0.00009, Train Gradient: 200.5\n",
      "Epoch 30001/150000, Train Loss: 27456, Val Loss: 27926,  Learning Rate: 0.00009, Train Gradient: 200.4\n",
      "Epoch 30101/150000, Train Loss: 27449, Val Loss: 27918,  Learning Rate: 0.00009, Train Gradient: 200.4\n",
      "Epoch 30201/150000, Train Loss: 27442, Val Loss: 27911,  Learning Rate: 0.00009, Train Gradient: 200.3\n",
      "Epoch 30301/150000, Train Loss: 27434, Val Loss: 27903,  Learning Rate: 0.00009, Train Gradient: 200.2\n",
      "Epoch 30401/150000, Train Loss: 27427, Val Loss: 27896,  Learning Rate: 0.00009, Train Gradient: 200.1\n",
      "Epoch 30501/150000, Train Loss: 27420, Val Loss: 27889,  Learning Rate: 0.00009, Train Gradient: 200.1\n",
      "Epoch 30601/150000, Train Loss: 27412, Val Loss: 27881,  Learning Rate: 0.00009, Train Gradient: 200.0\n",
      "Epoch 30701/150000, Train Loss: 27405, Val Loss: 27874,  Learning Rate: 0.00009, Train Gradient: 199.9\n",
      "Epoch 30801/150000, Train Loss: 27398, Val Loss: 27866,  Learning Rate: 0.00009, Train Gradient: 199.8\n",
      "Epoch 30901/150000, Train Loss: 27390, Val Loss: 27859,  Learning Rate: 0.00009, Train Gradient: 199.8\n",
      "Epoch 31001/150000, Train Loss: 27383, Val Loss: 27852,  Learning Rate: 0.00009, Train Gradient: 199.7\n",
      "Epoch 31101/150000, Train Loss: 27376, Val Loss: 27844,  Learning Rate: 0.00009, Train Gradient: 199.6\n",
      "Epoch 31201/150000, Train Loss: 27369, Val Loss: 27837,  Learning Rate: 0.00009, Train Gradient: 199.5\n",
      "Epoch 31301/150000, Train Loss: 27361, Val Loss: 27830,  Learning Rate: 0.00009, Train Gradient: 199.5\n",
      "Epoch 31401/150000, Train Loss: 27354, Val Loss: 27822,  Learning Rate: 0.00009, Train Gradient: 199.4\n",
      "Epoch 31501/150000, Train Loss: 27347, Val Loss: 27815,  Learning Rate: 0.00009, Train Gradient: 199.3\n",
      "Epoch 31601/150000, Train Loss: 27339, Val Loss: 27807,  Learning Rate: 0.00009, Train Gradient: 199.3\n",
      "Epoch 31701/150000, Train Loss: 27332, Val Loss: 27800,  Learning Rate: 0.00009, Train Gradient: 199.2\n",
      "Epoch 31801/150000, Train Loss: 27325, Val Loss: 27793,  Learning Rate: 0.00009, Train Gradient: 199.1\n",
      "Epoch 31901/150000, Train Loss: 27318, Val Loss: 27785,  Learning Rate: 0.00009, Train Gradient: 199.0\n",
      "Epoch 32001/150000, Train Loss: 27310, Val Loss: 27778,  Learning Rate: 0.00009, Train Gradient: 199.0\n",
      "Epoch 32101/150000, Train Loss: 27303, Val Loss: 27771,  Learning Rate: 0.00009, Train Gradient: 198.9\n",
      "Epoch 32201/150000, Train Loss: 27296, Val Loss: 27763,  Learning Rate: 0.00009, Train Gradient: 198.8\n",
      "Epoch 32301/150000, Train Loss: 27288, Val Loss: 27756,  Learning Rate: 0.00009, Train Gradient: 198.7\n",
      "Epoch 32401/150000, Train Loss: 27281, Val Loss: 27748,  Learning Rate: 0.00009, Train Gradient: 198.7\n",
      "Epoch 32501/150000, Train Loss: 27274, Val Loss: 27741,  Learning Rate: 0.00009, Train Gradient: 198.6\n",
      "Epoch 32601/150000, Train Loss: 27267, Val Loss: 27734,  Learning Rate: 0.00009, Train Gradient: 198.5\n",
      "Epoch 32701/150000, Train Loss: 27259, Val Loss: 27726,  Learning Rate: 0.00009, Train Gradient: 198.5\n",
      "Epoch 32801/150000, Train Loss: 27252, Val Loss: 27719,  Learning Rate: 0.00009, Train Gradient: 198.4\n",
      "Epoch 32901/150000, Train Loss: 27245, Val Loss: 27712,  Learning Rate: 0.00009, Train Gradient: 198.3\n",
      "Epoch 33001/150000, Train Loss: 27238, Val Loss: 27704,  Learning Rate: 0.00009, Train Gradient: 198.2\n",
      "Epoch 33101/150000, Train Loss: 27230, Val Loss: 27697,  Learning Rate: 0.00009, Train Gradient: 198.2\n",
      "Epoch 33201/150000, Train Loss: 27223, Val Loss: 27690,  Learning Rate: 0.00009, Train Gradient: 198.1\n",
      "Epoch 33301/150000, Train Loss: 27216, Val Loss: 27682,  Learning Rate: 0.00009, Train Gradient: 198.0\n",
      "Epoch 33401/150000, Train Loss: 27209, Val Loss: 27675,  Learning Rate: 0.00009, Train Gradient: 197.9\n",
      "Epoch 33501/150000, Train Loss: 27201, Val Loss: 27668,  Learning Rate: 0.00009, Train Gradient: 197.9\n",
      "Epoch 33601/150000, Train Loss: 27194, Val Loss: 27660,  Learning Rate: 0.00009, Train Gradient: 197.8\n",
      "Epoch 33701/150000, Train Loss: 27187, Val Loss: 27653,  Learning Rate: 0.00009, Train Gradient: 197.7\n",
      "Epoch 33801/150000, Train Loss: 27180, Val Loss: 27646,  Learning Rate: 0.00009, Train Gradient: 197.6\n",
      "Epoch 33901/150000, Train Loss: 27172, Val Loss: 27638,  Learning Rate: 0.00009, Train Gradient: 197.6\n",
      "Epoch 34001/150000, Train Loss: 27165, Val Loss: 27631,  Learning Rate: 0.00009, Train Gradient: 197.5\n",
      "Epoch 34101/150000, Train Loss: 27158, Val Loss: 27624,  Learning Rate: 0.00009, Train Gradient: 197.4\n",
      "Epoch 34201/150000, Train Loss: 27151, Val Loss: 27616,  Learning Rate: 0.00009, Train Gradient: 197.4\n",
      "Epoch 34301/150000, Train Loss: 27143, Val Loss: 27609,  Learning Rate: 0.00009, Train Gradient: 197.3\n",
      "Epoch 34401/150000, Train Loss: 27136, Val Loss: 27602,  Learning Rate: 0.00009, Train Gradient: 197.2\n",
      "Epoch 34501/150000, Train Loss: 27129, Val Loss: 27594,  Learning Rate: 0.00009, Train Gradient: 197.1\n",
      "Epoch 34601/150000, Train Loss: 27122, Val Loss: 27587,  Learning Rate: 0.00009, Train Gradient: 197.1\n",
      "Epoch 34701/150000, Train Loss: 27115, Val Loss: 27580,  Learning Rate: 0.00009, Train Gradient: 197.0\n",
      "Epoch 34801/150000, Train Loss: 27107, Val Loss: 27572,  Learning Rate: 0.00009, Train Gradient: 196.9\n",
      "Epoch 34901/150000, Train Loss: 27100, Val Loss: 27565,  Learning Rate: 0.00009, Train Gradient: 196.8\n",
      "Epoch 35001/150000, Train Loss: 27093, Val Loss: 27558,  Learning Rate: 0.00009, Train Gradient: 196.8\n",
      "Epoch 35101/150000, Train Loss: 27086, Val Loss: 27551,  Learning Rate: 0.00009, Train Gradient: 196.7\n",
      "Epoch 35201/150000, Train Loss: 27079, Val Loss: 27543,  Learning Rate: 0.00009, Train Gradient: 196.6\n",
      "Epoch 35301/150000, Train Loss: 27071, Val Loss: 27536,  Learning Rate: 0.00009, Train Gradient: 196.5\n",
      "Epoch 35401/150000, Train Loss: 27064, Val Loss: 27529,  Learning Rate: 0.00009, Train Gradient: 196.5\n",
      "Epoch 35501/150000, Train Loss: 27057, Val Loss: 27521,  Learning Rate: 0.00009, Train Gradient: 196.4\n",
      "Epoch 35601/150000, Train Loss: 27050, Val Loss: 27514,  Learning Rate: 0.00009, Train Gradient: 196.3\n",
      "Epoch 35701/150000, Train Loss: 27043, Val Loss: 27507,  Learning Rate: 0.00009, Train Gradient: 196.3\n",
      "Epoch 35801/150000, Train Loss: 27035, Val Loss: 27500,  Learning Rate: 0.00009, Train Gradient: 196.2\n",
      "Epoch 35901/150000, Train Loss: 27028, Val Loss: 27492,  Learning Rate: 0.00009, Train Gradient: 196.1\n",
      "Epoch 36001/150000, Train Loss: 27021, Val Loss: 27485,  Learning Rate: 0.00009, Train Gradient: 196.0\n",
      "Epoch 36101/150000, Train Loss: 27014, Val Loss: 27478,  Learning Rate: 0.00009, Train Gradient: 196.0\n",
      "Epoch 36201/150000, Train Loss: 27007, Val Loss: 27470,  Learning Rate: 0.00009, Train Gradient: 195.9\n",
      "Epoch 36301/150000, Train Loss: 26999, Val Loss: 27463,  Learning Rate: 0.00009, Train Gradient: 195.8\n",
      "Epoch 36401/150000, Train Loss: 26992, Val Loss: 27456,  Learning Rate: 0.00009, Train Gradient: 195.7\n",
      "Epoch 36501/150000, Train Loss: 26985, Val Loss: 27449,  Learning Rate: 0.00009, Train Gradient: 195.7\n",
      "Epoch 36601/150000, Train Loss: 26978, Val Loss: 27441,  Learning Rate: 0.00009, Train Gradient: 195.6\n",
      "Epoch 36701/150000, Train Loss: 26971, Val Loss: 27434,  Learning Rate: 0.00009, Train Gradient: 195.5\n",
      "Epoch 36801/150000, Train Loss: 26964, Val Loss: 27427,  Learning Rate: 0.00009, Train Gradient: 195.4\n",
      "Epoch 36901/150000, Train Loss: 26957, Val Loss: 27420,  Learning Rate: 0.00009, Train Gradient: 195.4\n",
      "Epoch 37001/150000, Train Loss: 26949, Val Loss: 27412,  Learning Rate: 0.00009, Train Gradient: 195.3\n",
      "Epoch 37101/150000, Train Loss: 26942, Val Loss: 27405,  Learning Rate: 0.00009, Train Gradient: 195.2\n",
      "Epoch 37201/150000, Train Loss: 26935, Val Loss: 27398,  Learning Rate: 0.00009, Train Gradient: 195.2\n",
      "Epoch 37301/150000, Train Loss: 26928, Val Loss: 27391,  Learning Rate: 0.00009, Train Gradient: 195.1\n",
      "Epoch 37401/150000, Train Loss: 26921, Val Loss: 27383,  Learning Rate: 0.00009, Train Gradient: 195.0\n",
      "Epoch 37501/150000, Train Loss: 26914, Val Loss: 27376,  Learning Rate: 0.00009, Train Gradient: 194.9\n",
      "Epoch 37601/150000, Train Loss: 26906, Val Loss: 27369,  Learning Rate: 0.00009, Train Gradient: 194.9\n",
      "Epoch 37701/150000, Train Loss: 26899, Val Loss: 27362,  Learning Rate: 0.00009, Train Gradient: 194.8\n",
      "Epoch 37801/150000, Train Loss: 26892, Val Loss: 27355,  Learning Rate: 0.00009, Train Gradient: 194.7\n",
      "Epoch 37901/150000, Train Loss: 26885, Val Loss: 27347,  Learning Rate: 0.00009, Train Gradient: 194.6\n",
      "Epoch 38001/150000, Train Loss: 26878, Val Loss: 27340,  Learning Rate: 0.00009, Train Gradient: 194.6\n",
      "Epoch 38101/150000, Train Loss: 26871, Val Loss: 27333,  Learning Rate: 0.00009, Train Gradient: 194.5\n",
      "Epoch 38201/150000, Train Loss: 26864, Val Loss: 27326,  Learning Rate: 0.00009, Train Gradient: 194.4\n",
      "Epoch 38301/150000, Train Loss: 26857, Val Loss: 27318,  Learning Rate: 0.00009, Train Gradient: 194.3\n",
      "Epoch 38401/150000, Train Loss: 26849, Val Loss: 27311,  Learning Rate: 0.00009, Train Gradient: 194.3\n",
      "Epoch 38501/150000, Train Loss: 26842, Val Loss: 27304,  Learning Rate: 0.00009, Train Gradient: 194.2\n",
      "Epoch 38601/150000, Train Loss: 26835, Val Loss: 27297,  Learning Rate: 0.00009, Train Gradient: 194.1\n",
      "Epoch 38701/150000, Train Loss: 26828, Val Loss: 27289,  Learning Rate: 0.00009, Train Gradient: 194.1\n",
      "Epoch 38801/150000, Train Loss: 26821, Val Loss: 27282,  Learning Rate: 0.00009, Train Gradient: 194.0\n",
      "Epoch 38901/150000, Train Loss: 26814, Val Loss: 27275,  Learning Rate: 0.00009, Train Gradient: 193.9\n",
      "Epoch 39001/150000, Train Loss: 26807, Val Loss: 27268,  Learning Rate: 0.00009, Train Gradient: 193.8\n",
      "Epoch 39101/150000, Train Loss: 26800, Val Loss: 27261,  Learning Rate: 0.00009, Train Gradient: 193.8\n",
      "Epoch 39201/150000, Train Loss: 26792, Val Loss: 27253,  Learning Rate: 0.00009, Train Gradient: 193.7\n",
      "Epoch 39301/150000, Train Loss: 26785, Val Loss: 27246,  Learning Rate: 0.00009, Train Gradient: 193.6\n",
      "Epoch 39401/150000, Train Loss: 26778, Val Loss: 27239,  Learning Rate: 0.00009, Train Gradient: 193.5\n",
      "Epoch 39501/150000, Train Loss: 26771, Val Loss: 27232,  Learning Rate: 0.00009, Train Gradient: 193.5\n",
      "Epoch 39601/150000, Train Loss: 26764, Val Loss: 27225,  Learning Rate: 0.00009, Train Gradient: 193.4\n",
      "Epoch 39701/150000, Train Loss: 26757, Val Loss: 27218,  Learning Rate: 0.00009, Train Gradient: 193.3\n",
      "Epoch 39801/150000, Train Loss: 26750, Val Loss: 27210,  Learning Rate: 0.00009, Train Gradient: 193.2\n",
      "Epoch 39901/150000, Train Loss: 26743, Val Loss: 27203,  Learning Rate: 0.00009, Train Gradient: 193.2\n",
      "Epoch 40001/150000, Train Loss: 26736, Val Loss: 27196,  Learning Rate: 0.00009, Train Gradient: 193.1\n",
      "Epoch 40101/150000, Train Loss: 26729, Val Loss: 27189,  Learning Rate: 0.00009, Train Gradient: 193.0\n",
      "Epoch 40201/150000, Train Loss: 26722, Val Loss: 27182,  Learning Rate: 0.00009, Train Gradient: 193.0\n",
      "Epoch 40301/150000, Train Loss: 26714, Val Loss: 27174,  Learning Rate: 0.00009, Train Gradient: 192.9\n",
      "Epoch 40401/150000, Train Loss: 26707, Val Loss: 27167,  Learning Rate: 0.00009, Train Gradient: 192.8\n",
      "Epoch 40501/150000, Train Loss: 26700, Val Loss: 27160,  Learning Rate: 0.00009, Train Gradient: 192.7\n",
      "Epoch 40601/150000, Train Loss: 26693, Val Loss: 27153,  Learning Rate: 0.00009, Train Gradient: 192.7\n",
      "Epoch 40701/150000, Train Loss: 26686, Val Loss: 27146,  Learning Rate: 0.00009, Train Gradient: 192.6\n",
      "Epoch 40801/150000, Train Loss: 26679, Val Loss: 27139,  Learning Rate: 0.00009, Train Gradient: 192.5\n",
      "Epoch 40901/150000, Train Loss: 26672, Val Loss: 27132,  Learning Rate: 0.00009, Train Gradient: 192.4\n",
      "Epoch 41001/150000, Train Loss: 26665, Val Loss: 27124,  Learning Rate: 0.00009, Train Gradient: 192.4\n",
      "Epoch 41101/150000, Train Loss: 26658, Val Loss: 27117,  Learning Rate: 0.00009, Train Gradient: 192.3\n",
      "Epoch 41201/150000, Train Loss: 26651, Val Loss: 27110,  Learning Rate: 0.00009, Train Gradient: 192.2\n",
      "Epoch 41301/150000, Train Loss: 26644, Val Loss: 27103,  Learning Rate: 0.00009, Train Gradient: 192.1\n",
      "Epoch 41401/150000, Train Loss: 26637, Val Loss: 27096,  Learning Rate: 0.00009, Train Gradient: 192.1\n",
      "Epoch 41501/150000, Train Loss: 26630, Val Loss: 27089,  Learning Rate: 0.00009, Train Gradient: 192.0\n",
      "Epoch 41601/150000, Train Loss: 26623, Val Loss: 27081,  Learning Rate: 0.00009, Train Gradient: 191.9\n",
      "Epoch 41701/150000, Train Loss: 26616, Val Loss: 27074,  Learning Rate: 0.00009, Train Gradient: 191.9\n",
      "Epoch 41801/150000, Train Loss: 26608, Val Loss: 27067,  Learning Rate: 0.00009, Train Gradient: 191.8\n",
      "Epoch 41901/150000, Train Loss: 26601, Val Loss: 27060,  Learning Rate: 0.00009, Train Gradient: 191.7\n",
      "Epoch 42001/150000, Train Loss: 26594, Val Loss: 27053,  Learning Rate: 0.00009, Train Gradient: 191.6\n",
      "Epoch 42101/150000, Train Loss: 26587, Val Loss: 27046,  Learning Rate: 0.00009, Train Gradient: 191.6\n",
      "Epoch 42201/150000, Train Loss: 26580, Val Loss: 27039,  Learning Rate: 0.00009, Train Gradient: 191.5\n",
      "Epoch 42301/150000, Train Loss: 26573, Val Loss: 27031,  Learning Rate: 0.00009, Train Gradient: 191.4\n",
      "Epoch 42401/150000, Train Loss: 26566, Val Loss: 27024,  Learning Rate: 0.00009, Train Gradient: 191.3\n",
      "Epoch 42501/150000, Train Loss: 26559, Val Loss: 27017,  Learning Rate: 0.00009, Train Gradient: 191.3\n",
      "Epoch 42601/150000, Train Loss: 26552, Val Loss: 27010,  Learning Rate: 0.00009, Train Gradient: 191.2\n",
      "Epoch 42701/150000, Train Loss: 26545, Val Loss: 27003,  Learning Rate: 0.00009, Train Gradient: 191.1\n",
      "Epoch 42801/150000, Train Loss: 26538, Val Loss: 26996,  Learning Rate: 0.00009, Train Gradient: 191.0\n",
      "Epoch 42901/150000, Train Loss: 26531, Val Loss: 26989,  Learning Rate: 0.00009, Train Gradient: 191.0\n",
      "Epoch 43001/150000, Train Loss: 26524, Val Loss: 26982,  Learning Rate: 0.00009, Train Gradient: 190.9\n",
      "Epoch 43101/150000, Train Loss: 26517, Val Loss: 26974,  Learning Rate: 0.00009, Train Gradient: 190.8\n",
      "Epoch 43201/150000, Train Loss: 26510, Val Loss: 26967,  Learning Rate: 0.00009, Train Gradient: 190.7\n",
      "Epoch 43301/150000, Train Loss: 26503, Val Loss: 26960,  Learning Rate: 0.00009, Train Gradient: 190.7\n",
      "Epoch 43401/150000, Train Loss: 26496, Val Loss: 26953,  Learning Rate: 0.00009, Train Gradient: 190.6\n",
      "Epoch 43501/150000, Train Loss: 26489, Val Loss: 26946,  Learning Rate: 0.00009, Train Gradient: 190.5\n",
      "Epoch 43601/150000, Train Loss: 26482, Val Loss: 26939,  Learning Rate: 0.00009, Train Gradient: 190.5\n",
      "Epoch 43701/150000, Train Loss: 26475, Val Loss: 26932,  Learning Rate: 0.00009, Train Gradient: 190.4\n",
      "Epoch 43801/150000, Train Loss: 26468, Val Loss: 26925,  Learning Rate: 0.00009, Train Gradient: 190.3\n",
      "Epoch 43901/150000, Train Loss: 26461, Val Loss: 26918,  Learning Rate: 0.00009, Train Gradient: 190.2\n",
      "Epoch 44001/150000, Train Loss: 26454, Val Loss: 26910,  Learning Rate: 0.00009, Train Gradient: 190.2\n",
      "Epoch 44101/150000, Train Loss: 26447, Val Loss: 26903,  Learning Rate: 0.00009, Train Gradient: 190.1\n",
      "Epoch 44201/150000, Train Loss: 26440, Val Loss: 26896,  Learning Rate: 0.00009, Train Gradient: 190.0\n",
      "Epoch 44301/150000, Train Loss: 26433, Val Loss: 26889,  Learning Rate: 0.00009, Train Gradient: 189.9\n",
      "Epoch 44401/150000, Train Loss: 26426, Val Loss: 26882,  Learning Rate: 0.00009, Train Gradient: 189.9\n",
      "Epoch 44501/150000, Train Loss: 26419, Val Loss: 26875,  Learning Rate: 0.00009, Train Gradient: 189.8\n",
      "Epoch 44601/150000, Train Loss: 26412, Val Loss: 26868,  Learning Rate: 0.00009, Train Gradient: 189.7\n",
      "Epoch 44701/150000, Train Loss: 26405, Val Loss: 26861,  Learning Rate: 0.00009, Train Gradient: 189.6\n",
      "Epoch 44801/150000, Train Loss: 26398, Val Loss: 26854,  Learning Rate: 0.00009, Train Gradient: 189.6\n",
      "Epoch 44901/150000, Train Loss: 26391, Val Loss: 26847,  Learning Rate: 0.00009, Train Gradient: 189.5\n",
      "Epoch 45001/150000, Train Loss: 26384, Val Loss: 26840,  Learning Rate: 0.00009, Train Gradient: 189.4\n",
      "Epoch 45101/150000, Train Loss: 26377, Val Loss: 26833,  Learning Rate: 0.00009, Train Gradient: 189.4\n",
      "Epoch 45201/150000, Train Loss: 26370, Val Loss: 26826,  Learning Rate: 0.00009, Train Gradient: 189.3\n",
      "Epoch 45301/150000, Train Loss: 26363, Val Loss: 26819,  Learning Rate: 0.00009, Train Gradient: 189.2\n",
      "Epoch 45401/150000, Train Loss: 26356, Val Loss: 26811,  Learning Rate: 0.00009, Train Gradient: 189.1\n",
      "Epoch 45501/150000, Train Loss: 26349, Val Loss: 26804,  Learning Rate: 0.00009, Train Gradient: 189.1\n",
      "Epoch 45601/150000, Train Loss: 26342, Val Loss: 26797,  Learning Rate: 0.00009, Train Gradient: 189.0\n",
      "Epoch 45701/150000, Train Loss: 26335, Val Loss: 26790,  Learning Rate: 0.00009, Train Gradient: 188.9\n",
      "Epoch 45801/150000, Train Loss: 26328, Val Loss: 26783,  Learning Rate: 0.00009, Train Gradient: 188.8\n",
      "Epoch 45901/150000, Train Loss: 26321, Val Loss: 26776,  Learning Rate: 0.00009, Train Gradient: 188.8\n",
      "Epoch 46001/150000, Train Loss: 26314, Val Loss: 26769,  Learning Rate: 0.00009, Train Gradient: 188.7\n",
      "Epoch 46101/150000, Train Loss: 26308, Val Loss: 26762,  Learning Rate: 0.00009, Train Gradient: 188.6\n",
      "Epoch 46201/150000, Train Loss: 26301, Val Loss: 26755,  Learning Rate: 0.00009, Train Gradient: 188.5\n",
      "Epoch 46301/150000, Train Loss: 26294, Val Loss: 26748,  Learning Rate: 0.00009, Train Gradient: 188.5\n",
      "Epoch 46401/150000, Train Loss: 26287, Val Loss: 26741,  Learning Rate: 0.00009, Train Gradient: 188.4\n",
      "Epoch 46501/150000, Train Loss: 26280, Val Loss: 26734,  Learning Rate: 0.00009, Train Gradient: 188.3\n",
      "Epoch 46601/150000, Train Loss: 26273, Val Loss: 26727,  Learning Rate: 0.00009, Train Gradient: 188.2\n",
      "Epoch 46701/150000, Train Loss: 26266, Val Loss: 26720,  Learning Rate: 0.00009, Train Gradient: 188.2\n",
      "Epoch 46801/150000, Train Loss: 26259, Val Loss: 26713,  Learning Rate: 0.00009, Train Gradient: 188.1\n",
      "Epoch 46901/150000, Train Loss: 26252, Val Loss: 26706,  Learning Rate: 0.00009, Train Gradient: 188.0\n",
      "Epoch 47001/150000, Train Loss: 26245, Val Loss: 26699,  Learning Rate: 0.00009, Train Gradient: 188.0\n",
      "Epoch 47101/150000, Train Loss: 26238, Val Loss: 26692,  Learning Rate: 0.00009, Train Gradient: 187.9\n",
      "Epoch 47201/150000, Train Loss: 26231, Val Loss: 26685,  Learning Rate: 0.00009, Train Gradient: 187.8\n",
      "Epoch 47301/150000, Train Loss: 26224, Val Loss: 26678,  Learning Rate: 0.00009, Train Gradient: 187.7\n",
      "Epoch 47401/150000, Train Loss: 26217, Val Loss: 26671,  Learning Rate: 0.00009, Train Gradient: 187.7\n",
      "Epoch 47501/150000, Train Loss: 26211, Val Loss: 26664,  Learning Rate: 0.00009, Train Gradient: 187.6\n",
      "Epoch 47601/150000, Train Loss: 26204, Val Loss: 26657,  Learning Rate: 0.00009, Train Gradient: 187.5\n",
      "Epoch 47701/150000, Train Loss: 26197, Val Loss: 26650,  Learning Rate: 0.00009, Train Gradient: 187.4\n",
      "Epoch 47801/150000, Train Loss: 26190, Val Loss: 26643,  Learning Rate: 0.00009, Train Gradient: 187.4\n",
      "Epoch 47901/150000, Train Loss: 26183, Val Loss: 26636,  Learning Rate: 0.00009, Train Gradient: 187.3\n",
      "Epoch 48001/150000, Train Loss: 26176, Val Loss: 26629,  Learning Rate: 0.00009, Train Gradient: 187.2\n",
      "Epoch 48101/150000, Train Loss: 26169, Val Loss: 26622,  Learning Rate: 0.00009, Train Gradient: 187.1\n",
      "Epoch 48201/150000, Train Loss: 26162, Val Loss: 26615,  Learning Rate: 0.00009, Train Gradient: 187.1\n",
      "Epoch 48301/150000, Train Loss: 26155, Val Loss: 26608,  Learning Rate: 0.00009, Train Gradient: 187.0\n",
      "Epoch 48401/150000, Train Loss: 26149, Val Loss: 26601,  Learning Rate: 0.00009, Train Gradient: 186.9\n",
      "Epoch 48501/150000, Train Loss: 26142, Val Loss: 26594,  Learning Rate: 0.00009, Train Gradient: 186.8\n",
      "Epoch 48601/150000, Train Loss: 26135, Val Loss: 26587,  Learning Rate: 0.00009, Train Gradient: 186.8\n",
      "Epoch 48701/150000, Train Loss: 26128, Val Loss: 26580,  Learning Rate: 0.00009, Train Gradient: 186.7\n",
      "Epoch 48801/150000, Train Loss: 26121, Val Loss: 26573,  Learning Rate: 0.00009, Train Gradient: 186.6\n",
      "Epoch 48901/150000, Train Loss: 26114, Val Loss: 26566,  Learning Rate: 0.00009, Train Gradient: 186.6\n",
      "Epoch 49001/150000, Train Loss: 26107, Val Loss: 26559,  Learning Rate: 0.00009, Train Gradient: 186.5\n",
      "Epoch 49101/150000, Train Loss: 26100, Val Loss: 26552,  Learning Rate: 0.00009, Train Gradient: 186.4\n",
      "Epoch 49201/150000, Train Loss: 26094, Val Loss: 26545,  Learning Rate: 0.00009, Train Gradient: 186.3\n",
      "Epoch 49301/150000, Train Loss: 26087, Val Loss: 26538,  Learning Rate: 0.00009, Train Gradient: 186.3\n",
      "Epoch 49401/150000, Train Loss: 26080, Val Loss: 26531,  Learning Rate: 0.00009, Train Gradient: 186.2\n",
      "Epoch 49501/150000, Train Loss: 26073, Val Loss: 26525,  Learning Rate: 0.00009, Train Gradient: 186.1\n",
      "Epoch 49601/150000, Train Loss: 26066, Val Loss: 26518,  Learning Rate: 0.00009, Train Gradient: 186.0\n",
      "Epoch 49701/150000, Train Loss: 26059, Val Loss: 26511,  Learning Rate: 0.00009, Train Gradient: 186.0\n",
      "Epoch 49801/150000, Train Loss: 26052, Val Loss: 26504,  Learning Rate: 0.00009, Train Gradient: 185.9\n",
      "Epoch 49901/150000, Train Loss: 26046, Val Loss: 26497,  Learning Rate: 0.00009, Train Gradient: 185.8\n",
      "Epoch 50001/150000, Train Loss: 26039, Val Loss: 26490,  Learning Rate: 0.00009, Train Gradient: 185.7\n",
      "Epoch 50101/150000, Train Loss: 26032, Val Loss: 26483,  Learning Rate: 0.00009, Train Gradient: 185.7\n",
      "Epoch 50201/150000, Train Loss: 26025, Val Loss: 26476,  Learning Rate: 0.00009, Train Gradient: 185.6\n",
      "Epoch 50301/150000, Train Loss: 26018, Val Loss: 26469,  Learning Rate: 0.00009, Train Gradient: 185.5\n",
      "Epoch 50401/150000, Train Loss: 26011, Val Loss: 26462,  Learning Rate: 0.00009, Train Gradient: 185.4\n",
      "Epoch 50501/150000, Train Loss: 26005, Val Loss: 26455,  Learning Rate: 0.00009, Train Gradient: 185.4\n",
      "Epoch 50601/150000, Train Loss: 25998, Val Loss: 26448,  Learning Rate: 0.00009, Train Gradient: 185.3\n",
      "Epoch 50701/150000, Train Loss: 25991, Val Loss: 26441,  Learning Rate: 0.00009, Train Gradient: 185.2\n",
      "Epoch 50801/150000, Train Loss: 25984, Val Loss: 26434,  Learning Rate: 0.00009, Train Gradient: 185.2\n",
      "Epoch 50901/150000, Train Loss: 25977, Val Loss: 26428,  Learning Rate: 0.00009, Train Gradient: 185.1\n",
      "Epoch 51001/150000, Train Loss: 25971, Val Loss: 26421,  Learning Rate: 0.00009, Train Gradient: 185.0\n",
      "Epoch 51101/150000, Train Loss: 25964, Val Loss: 26414,  Learning Rate: 0.00009, Train Gradient: 184.9\n",
      "Epoch 51201/150000, Train Loss: 25957, Val Loss: 26407,  Learning Rate: 0.00009, Train Gradient: 184.9\n",
      "Epoch 51301/150000, Train Loss: 25950, Val Loss: 26400,  Learning Rate: 0.00009, Train Gradient: 184.8\n",
      "Epoch 51401/150000, Train Loss: 25943, Val Loss: 26393,  Learning Rate: 0.00009, Train Gradient: 184.7\n",
      "Epoch 51501/150000, Train Loss: 25937, Val Loss: 26386,  Learning Rate: 0.00009, Train Gradient: 184.6\n",
      "Epoch 51601/150000, Train Loss: 25930, Val Loss: 26379,  Learning Rate: 0.00009, Train Gradient: 184.6\n",
      "Epoch 51701/150000, Train Loss: 25923, Val Loss: 26372,  Learning Rate: 0.00009, Train Gradient: 184.5\n",
      "Epoch 51801/150000, Train Loss: 25916, Val Loss: 26366,  Learning Rate: 0.00009, Train Gradient: 184.4\n",
      "Epoch 51901/150000, Train Loss: 25909, Val Loss: 26359,  Learning Rate: 0.00009, Train Gradient: 184.3\n",
      "Epoch 52001/150000, Train Loss: 25903, Val Loss: 26352,  Learning Rate: 0.00009, Train Gradient: 184.3\n",
      "Epoch 52101/150000, Train Loss: 25896, Val Loss: 26345,  Learning Rate: 0.00009, Train Gradient: 184.2\n",
      "Epoch 52201/150000, Train Loss: 25889, Val Loss: 26338,  Learning Rate: 0.00009, Train Gradient: 184.1\n",
      "Epoch 52301/150000, Train Loss: 25882, Val Loss: 26331,  Learning Rate: 0.00009, Train Gradient: 184.1\n",
      "Epoch 52401/150000, Train Loss: 25875, Val Loss: 26324,  Learning Rate: 0.00009, Train Gradient: 184.0\n",
      "Epoch 52501/150000, Train Loss: 25869, Val Loss: 26317,  Learning Rate: 0.00009, Train Gradient: 183.9\n",
      "Epoch 52601/150000, Train Loss: 25862, Val Loss: 26311,  Learning Rate: 0.00009, Train Gradient: 183.8\n",
      "Epoch 52701/150000, Train Loss: 25855, Val Loss: 26304,  Learning Rate: 0.00009, Train Gradient: 183.8\n",
      "Epoch 52801/150000, Train Loss: 25848, Val Loss: 26297,  Learning Rate: 0.00009, Train Gradient: 183.7\n",
      "Epoch 52901/150000, Train Loss: 25842, Val Loss: 26290,  Learning Rate: 0.00009, Train Gradient: 183.6\n",
      "Epoch 53001/150000, Train Loss: 25835, Val Loss: 26283,  Learning Rate: 0.00009, Train Gradient: 183.5\n",
      "Epoch 53101/150000, Train Loss: 25828, Val Loss: 26276,  Learning Rate: 0.00009, Train Gradient: 183.5\n",
      "Epoch 53201/150000, Train Loss: 25821, Val Loss: 26269,  Learning Rate: 0.00009, Train Gradient: 183.4\n",
      "Epoch 53301/150000, Train Loss: 25815, Val Loss: 26263,  Learning Rate: 0.00009, Train Gradient: 183.3\n",
      "Epoch 53401/150000, Train Loss: 25808, Val Loss: 26256,  Learning Rate: 0.00009, Train Gradient: 183.2\n",
      "Epoch 53501/150000, Train Loss: 25801, Val Loss: 26249,  Learning Rate: 0.00009, Train Gradient: 183.2\n",
      "Epoch 53601/150000, Train Loss: 25794, Val Loss: 26242,  Learning Rate: 0.00009, Train Gradient: 183.1\n",
      "Epoch 53701/150000, Train Loss: 25788, Val Loss: 26235,  Learning Rate: 0.00009, Train Gradient: 183.0\n",
      "Epoch 53801/150000, Train Loss: 25781, Val Loss: 26228,  Learning Rate: 0.00009, Train Gradient: 182.9\n",
      "Epoch 53901/150000, Train Loss: 25774, Val Loss: 26222,  Learning Rate: 0.00009, Train Gradient: 182.9\n",
      "Epoch 54001/150000, Train Loss: 25767, Val Loss: 26215,  Learning Rate: 0.00009, Train Gradient: 182.8\n",
      "Epoch 54101/150000, Train Loss: 25761, Val Loss: 26208,  Learning Rate: 0.00009, Train Gradient: 182.7\n",
      "Epoch 54201/150000, Train Loss: 25754, Val Loss: 26201,  Learning Rate: 0.00009, Train Gradient: 182.6\n",
      "Epoch 54301/150000, Train Loss: 25747, Val Loss: 26194,  Learning Rate: 0.00009, Train Gradient: 182.6\n",
      "Epoch 54401/150000, Train Loss: 25740, Val Loss: 26187,  Learning Rate: 0.00009, Train Gradient: 182.5\n",
      "Epoch 54501/150000, Train Loss: 25734, Val Loss: 26180,  Learning Rate: 0.00009, Train Gradient: 182.4\n",
      "Epoch 54601/150000, Train Loss: 25727, Val Loss: 26174,  Learning Rate: 0.00009, Train Gradient: 182.4\n",
      "Epoch 54701/150000, Train Loss: 25720, Val Loss: 26167,  Learning Rate: 0.00009, Train Gradient: 182.3\n",
      "Epoch 54801/150000, Train Loss: 25713, Val Loss: 26160,  Learning Rate: 0.00009, Train Gradient: 182.2\n",
      "Epoch 54901/150000, Train Loss: 25707, Val Loss: 26153,  Learning Rate: 0.00009, Train Gradient: 182.1\n",
      "Epoch 55001/150000, Train Loss: 25700, Val Loss: 26146,  Learning Rate: 0.00009, Train Gradient: 182.1\n",
      "Epoch 55101/150000, Train Loss: 25693, Val Loss: 26139,  Learning Rate: 0.00009, Train Gradient: 182.0\n",
      "Epoch 55201/150000, Train Loss: 25686, Val Loss: 26133,  Learning Rate: 0.00009, Train Gradient: 181.9\n",
      "Epoch 55301/150000, Train Loss: 25679, Val Loss: 26126,  Learning Rate: 0.00009, Train Gradient: 182.0\n",
      "Epoch 55401/150000, Train Loss: 25672, Val Loss: 26120,  Learning Rate: 0.00009, Train Gradient: 182.5\n",
      "Epoch 55501/150000, Train Loss: 25665, Val Loss: 26113,  Learning Rate: 0.00009, Train Gradient: 182.5\n",
      "Epoch 55601/150000, Train Loss: 25658, Val Loss: 26106,  Learning Rate: 0.00009, Train Gradient: 182.5\n",
      "Epoch 55701/150000, Train Loss: 25651, Val Loss: 26099,  Learning Rate: 0.00009, Train Gradient: 182.4\n",
      "Epoch 55801/150000, Train Loss: 25644, Val Loss: 26093,  Learning Rate: 0.00009, Train Gradient: 182.3\n",
      "Epoch 55901/150000, Train Loss: 25637, Val Loss: 26086,  Learning Rate: 0.00009, Train Gradient: 182.3\n",
      "Epoch 56001/150000, Train Loss: 25631, Val Loss: 26079,  Learning Rate: 0.00009, Train Gradient: 182.2\n",
      "Epoch 56101/150000, Train Loss: 25624, Val Loss: 26072,  Learning Rate: 0.00009, Train Gradient: 182.1\n",
      "Epoch 56201/150000, Train Loss: 25617, Val Loss: 26065,  Learning Rate: 0.00009, Train Gradient: 182.1\n",
      "Epoch 56301/150000, Train Loss: 25610, Val Loss: 26059,  Learning Rate: 0.00009, Train Gradient: 182.0\n",
      "Epoch 56401/150000, Train Loss: 25604, Val Loss: 26052,  Learning Rate: 0.00009, Train Gradient: 182.0\n",
      "Epoch 56501/150000, Train Loss: 25597, Val Loss: 26045,  Learning Rate: 0.00009, Train Gradient: 181.9\n",
      "Epoch 56601/150000, Train Loss: 25590, Val Loss: 26038,  Learning Rate: 0.00009, Train Gradient: 181.8\n",
      "Epoch 56701/150000, Train Loss: 25583, Val Loss: 26031,  Learning Rate: 0.00009, Train Gradient: 181.8\n",
      "Epoch 56801/150000, Train Loss: 25577, Val Loss: 26024,  Learning Rate: 0.00009, Train Gradient: 181.7\n",
      "Epoch 56901/150000, Train Loss: 25570, Val Loss: 26018,  Learning Rate: 0.00009, Train Gradient: 181.7\n",
      "Epoch 57001/150000, Train Loss: 25563, Val Loss: 26011,  Learning Rate: 0.00009, Train Gradient: 181.6\n",
      "Epoch 57101/150000, Train Loss: 25556, Val Loss: 26004,  Learning Rate: 0.00009, Train Gradient: 181.5\n",
      "Epoch 57201/150000, Train Loss: 25550, Val Loss: 25997,  Learning Rate: 0.00009, Train Gradient: 181.5\n",
      "Epoch 57301/150000, Train Loss: 25543, Val Loss: 25990,  Learning Rate: 0.00009, Train Gradient: 181.4\n",
      "Epoch 57401/150000, Train Loss: 25536, Val Loss: 25984,  Learning Rate: 0.00009, Train Gradient: 181.3\n",
      "Epoch 57501/150000, Train Loss: 25529, Val Loss: 25977,  Learning Rate: 0.00009, Train Gradient: 181.3\n",
      "Epoch 57601/150000, Train Loss: 25523, Val Loss: 25970,  Learning Rate: 0.00009, Train Gradient: 181.2\n",
      "Epoch 57701/150000, Train Loss: 25516, Val Loss: 25963,  Learning Rate: 0.00009, Train Gradient: 181.2\n",
      "Epoch 57801/150000, Train Loss: 25509, Val Loss: 25956,  Learning Rate: 0.00009, Train Gradient: 181.1\n",
      "Epoch 57901/150000, Train Loss: 25503, Val Loss: 25950,  Learning Rate: 0.00009, Train Gradient: 181.0\n",
      "Epoch 58001/150000, Train Loss: 25496, Val Loss: 25943,  Learning Rate: 0.00009, Train Gradient: 181.0\n",
      "Epoch 58101/150000, Train Loss: 25489, Val Loss: 25936,  Learning Rate: 0.00009, Train Gradient: 180.9\n",
      "Epoch 58201/150000, Train Loss: 25482, Val Loss: 25929,  Learning Rate: 0.00009, Train Gradient: 180.8\n",
      "Epoch 58301/150000, Train Loss: 25476, Val Loss: 25922,  Learning Rate: 0.00009, Train Gradient: 180.8\n",
      "Epoch 58401/150000, Train Loss: 25469, Val Loss: 25916,  Learning Rate: 0.00009, Train Gradient: 180.7\n",
      "Epoch 58501/150000, Train Loss: 25462, Val Loss: 25909,  Learning Rate: 0.00009, Train Gradient: 180.6\n",
      "Epoch 58601/150000, Train Loss: 25456, Val Loss: 25902,  Learning Rate: 0.00009, Train Gradient: 180.6\n",
      "Epoch 58701/150000, Train Loss: 25449, Val Loss: 25895,  Learning Rate: 0.00009, Train Gradient: 180.5\n",
      "Epoch 58801/150000, Train Loss: 25442, Val Loss: 25889,  Learning Rate: 0.00009, Train Gradient: 180.4\n",
      "Epoch 58901/150000, Train Loss: 25436, Val Loss: 25882,  Learning Rate: 0.00009, Train Gradient: 180.4\n",
      "Epoch 59001/150000, Train Loss: 25429, Val Loss: 25875,  Learning Rate: 0.00009, Train Gradient: 180.3\n",
      "Epoch 59101/150000, Train Loss: 25422, Val Loss: 25868,  Learning Rate: 0.00009, Train Gradient: 180.3\n",
      "Epoch 59201/150000, Train Loss: 25416, Val Loss: 25862,  Learning Rate: 0.00009, Train Gradient: 180.2\n",
      "Epoch 59301/150000, Train Loss: 25409, Val Loss: 25855,  Learning Rate: 0.00009, Train Gradient: 180.1\n",
      "Epoch 59401/150000, Train Loss: 25402, Val Loss: 25848,  Learning Rate: 0.00009, Train Gradient: 180.1\n",
      "Epoch 59501/150000, Train Loss: 25396, Val Loss: 25841,  Learning Rate: 0.00009, Train Gradient: 180.0\n",
      "Epoch 59601/150000, Train Loss: 25389, Val Loss: 25835,  Learning Rate: 0.00009, Train Gradient: 179.9\n",
      "Epoch 59701/150000, Train Loss: 25382, Val Loss: 25828,  Learning Rate: 0.00009, Train Gradient: 179.9\n",
      "Epoch 59801/150000, Train Loss: 25376, Val Loss: 25821,  Learning Rate: 0.00009, Train Gradient: 179.8\n",
      "Epoch 59901/150000, Train Loss: 25369, Val Loss: 25814,  Learning Rate: 0.00009, Train Gradient: 179.8\n",
      "Epoch 60001/150000, Train Loss: 25362, Val Loss: 25808,  Learning Rate: 0.00009, Train Gradient: 179.7\n",
      "Epoch 60101/150000, Train Loss: 25356, Val Loss: 25801,  Learning Rate: 0.00009, Train Gradient: 179.6\n",
      "Epoch 60201/150000, Train Loss: 25349, Val Loss: 25794,  Learning Rate: 0.00009, Train Gradient: 179.6\n",
      "Epoch 60301/150000, Train Loss: 25342, Val Loss: 25787,  Learning Rate: 0.00009, Train Gradient: 179.5\n",
      "Epoch 60401/150000, Train Loss: 25336, Val Loss: 25781,  Learning Rate: 0.00009, Train Gradient: 179.4\n",
      "Epoch 60501/150000, Train Loss: 25329, Val Loss: 25774,  Learning Rate: 0.00009, Train Gradient: 179.4\n",
      "Epoch 60601/150000, Train Loss: 25322, Val Loss: 25767,  Learning Rate: 0.00009, Train Gradient: 179.3\n",
      "Epoch 60701/150000, Train Loss: 25316, Val Loss: 25760,  Learning Rate: 0.00009, Train Gradient: 179.3\n",
      "Epoch 60801/150000, Train Loss: 25309, Val Loss: 25754,  Learning Rate: 0.00009, Train Gradient: 179.2\n",
      "Epoch 60901/150000, Train Loss: 25303, Val Loss: 25747,  Learning Rate: 0.00009, Train Gradient: 179.1\n",
      "Epoch 61001/150000, Train Loss: 25296, Val Loss: 25740,  Learning Rate: 0.00009, Train Gradient: 179.1\n",
      "Epoch 61101/150000, Train Loss: 25289, Val Loss: 25733,  Learning Rate: 0.00009, Train Gradient: 179.0\n",
      "Epoch 61201/150000, Train Loss: 25283, Val Loss: 25727,  Learning Rate: 0.00009, Train Gradient: 178.9\n",
      "Epoch 61301/150000, Train Loss: 25276, Val Loss: 25720,  Learning Rate: 0.00009, Train Gradient: 178.9\n",
      "Epoch 61401/150000, Train Loss: 25269, Val Loss: 25713,  Learning Rate: 0.00009, Train Gradient: 178.8\n",
      "Epoch 61501/150000, Train Loss: 25263, Val Loss: 25707,  Learning Rate: 0.00009, Train Gradient: 178.8\n",
      "Epoch 61601/150000, Train Loss: 25256, Val Loss: 25700,  Learning Rate: 0.00009, Train Gradient: 178.7\n",
      "Epoch 61701/150000, Train Loss: 25250, Val Loss: 25693,  Learning Rate: 0.00009, Train Gradient: 178.6\n",
      "Epoch 61801/150000, Train Loss: 25243, Val Loss: 25687,  Learning Rate: 0.00009, Train Gradient: 178.6\n",
      "Epoch 61901/150000, Train Loss: 25236, Val Loss: 25680,  Learning Rate: 0.00009, Train Gradient: 178.5\n",
      "Epoch 62001/150000, Train Loss: 25230, Val Loss: 25673,  Learning Rate: 0.00009, Train Gradient: 178.5\n",
      "Epoch 62101/150000, Train Loss: 25223, Val Loss: 25666,  Learning Rate: 0.00009, Train Gradient: 178.4\n",
      "Epoch 62201/150000, Train Loss: 25217, Val Loss: 25660,  Learning Rate: 0.00009, Train Gradient: 178.3\n",
      "Epoch 62301/150000, Train Loss: 25210, Val Loss: 25653,  Learning Rate: 0.00009, Train Gradient: 178.3\n",
      "Epoch 62401/150000, Train Loss: 25203, Val Loss: 25646,  Learning Rate: 0.00009, Train Gradient: 178.2\n",
      "Epoch 62501/150000, Train Loss: 25197, Val Loss: 25640,  Learning Rate: 0.00009, Train Gradient: 178.1\n",
      "Epoch 62601/150000, Train Loss: 25190, Val Loss: 25633,  Learning Rate: 0.00009, Train Gradient: 178.1\n",
      "Epoch 62701/150000, Train Loss: 25184, Val Loss: 25626,  Learning Rate: 0.00009, Train Gradient: 178.0\n",
      "Epoch 62801/150000, Train Loss: 25177, Val Loss: 25620,  Learning Rate: 0.00009, Train Gradient: 178.0\n",
      "Epoch 62901/150000, Train Loss: 25170, Val Loss: 25613,  Learning Rate: 0.00009, Train Gradient: 177.9\n",
      "Epoch 63001/150000, Train Loss: 25164, Val Loss: 25606,  Learning Rate: 0.00009, Train Gradient: 177.8\n",
      "Epoch 63101/150000, Train Loss: 25157, Val Loss: 25600,  Learning Rate: 0.00009, Train Gradient: 177.8\n",
      "Epoch 63201/150000, Train Loss: 25151, Val Loss: 25593,  Learning Rate: 0.00009, Train Gradient: 177.7\n",
      "Epoch 63301/150000, Train Loss: 25144, Val Loss: 25586,  Learning Rate: 0.00009, Train Gradient: 177.7\n",
      "Epoch 63401/150000, Train Loss: 25137, Val Loss: 25580,  Learning Rate: 0.00009, Train Gradient: 177.6\n",
      "Epoch 63501/150000, Train Loss: 25131, Val Loss: 25573,  Learning Rate: 0.00009, Train Gradient: 177.5\n",
      "Epoch 63601/150000, Train Loss: 25124, Val Loss: 25566,  Learning Rate: 0.00009, Train Gradient: 177.5\n",
      "Epoch 63701/150000, Train Loss: 25118, Val Loss: 25560,  Learning Rate: 0.00009, Train Gradient: 177.4\n",
      "Epoch 63801/150000, Train Loss: 25111, Val Loss: 25553,  Learning Rate: 0.00009, Train Gradient: 177.3\n",
      "Epoch 63901/150000, Train Loss: 25105, Val Loss: 25546,  Learning Rate: 0.00009, Train Gradient: 177.3\n",
      "Epoch 64001/150000, Train Loss: 25098, Val Loss: 25540,  Learning Rate: 0.00009, Train Gradient: 177.2\n",
      "Epoch 64101/150000, Train Loss: 25091, Val Loss: 25533,  Learning Rate: 0.00009, Train Gradient: 177.2\n",
      "Epoch 64201/150000, Train Loss: 25085, Val Loss: 25526,  Learning Rate: 0.00009, Train Gradient: 177.1\n",
      "Epoch 64301/150000, Train Loss: 25078, Val Loss: 25520,  Learning Rate: 0.00009, Train Gradient: 177.0\n",
      "Epoch 64401/150000, Train Loss: 25072, Val Loss: 25513,  Learning Rate: 0.00009, Train Gradient: 177.0\n",
      "Epoch 64501/150000, Train Loss: 25065, Val Loss: 25507,  Learning Rate: 0.00009, Train Gradient: 176.9\n",
      "Epoch 64601/150000, Train Loss: 25059, Val Loss: 25500,  Learning Rate: 0.00009, Train Gradient: 176.9\n",
      "Epoch 64701/150000, Train Loss: 25052, Val Loss: 25493,  Learning Rate: 0.00009, Train Gradient: 176.8\n",
      "Epoch 64801/150000, Train Loss: 25046, Val Loss: 25487,  Learning Rate: 0.00009, Train Gradient: 176.7\n",
      "Epoch 64901/150000, Train Loss: 25039, Val Loss: 25480,  Learning Rate: 0.00009, Train Gradient: 176.7\n",
      "Epoch 65001/150000, Train Loss: 25033, Val Loss: 25473,  Learning Rate: 0.00009, Train Gradient: 176.6\n",
      "Epoch 65101/150000, Train Loss: 25026, Val Loss: 25467,  Learning Rate: 0.00009, Train Gradient: 176.5\n",
      "Epoch 65201/150000, Train Loss: 25020, Val Loss: 25460,  Learning Rate: 0.00009, Train Gradient: 176.5\n",
      "Epoch 65301/150000, Train Loss: 25013, Val Loss: 25454,  Learning Rate: 0.00009, Train Gradient: 176.4\n",
      "Epoch 65401/150000, Train Loss: 25006, Val Loss: 25447,  Learning Rate: 0.00009, Train Gradient: 176.4\n",
      "Epoch 65501/150000, Train Loss: 25000, Val Loss: 25440,  Learning Rate: 0.00009, Train Gradient: 176.3\n",
      "Epoch 65601/150000, Train Loss: 24993, Val Loss: 25434,  Learning Rate: 0.00009, Train Gradient: 176.2\n",
      "Epoch 65701/150000, Train Loss: 24987, Val Loss: 25427,  Learning Rate: 0.00009, Train Gradient: 176.2\n",
      "Epoch 65801/150000, Train Loss: 24980, Val Loss: 25420,  Learning Rate: 0.00009, Train Gradient: 176.1\n",
      "Epoch 65901/150000, Train Loss: 24974, Val Loss: 25414,  Learning Rate: 0.00009, Train Gradient: 176.1\n",
      "Epoch 66001/150000, Train Loss: 24967, Val Loss: 25407,  Learning Rate: 0.00009, Train Gradient: 176.0\n",
      "Epoch 66101/150000, Train Loss: 24961, Val Loss: 25400,  Learning Rate: 0.00009, Train Gradient: 175.9\n",
      "Epoch 66201/150000, Train Loss: 24954, Val Loss: 25394,  Learning Rate: 0.00009, Train Gradient: 175.9\n",
      "Epoch 66301/150000, Train Loss: 24947, Val Loss: 25387,  Learning Rate: 0.00009, Train Gradient: 175.8\n",
      "Epoch 66401/150000, Train Loss: 24941, Val Loss: 25381,  Learning Rate: 0.00009, Train Gradient: 175.8\n",
      "Epoch 66501/150000, Train Loss: 24934, Val Loss: 25374,  Learning Rate: 0.00009, Train Gradient: 175.7\n",
      "Epoch 66601/150000, Train Loss: 24928, Val Loss: 25367,  Learning Rate: 0.00009, Train Gradient: 175.6\n",
      "Epoch 66701/150000, Train Loss: 24921, Val Loss: 25361,  Learning Rate: 0.00009, Train Gradient: 175.6\n",
      "Epoch 66801/150000, Train Loss: 24915, Val Loss: 25354,  Learning Rate: 0.00009, Train Gradient: 175.5\n",
      "Epoch 66901/150000, Train Loss: 24908, Val Loss: 25347,  Learning Rate: 0.00009, Train Gradient: 175.5\n",
      "Epoch 67001/150000, Train Loss: 24902, Val Loss: 25341,  Learning Rate: 0.00009, Train Gradient: 175.4\n",
      "Epoch 67101/150000, Train Loss: 24895, Val Loss: 25334,  Learning Rate: 0.00009, Train Gradient: 175.3\n",
      "Epoch 67201/150000, Train Loss: 24889, Val Loss: 25328,  Learning Rate: 0.00009, Train Gradient: 175.3\n",
      "Epoch 67301/150000, Train Loss: 24882, Val Loss: 25321,  Learning Rate: 0.00009, Train Gradient: 175.2\n",
      "Epoch 67401/150000, Train Loss: 24876, Val Loss: 25314,  Learning Rate: 0.00009, Train Gradient: 175.2\n",
      "Epoch 67501/150000, Train Loss: 24869, Val Loss: 25308,  Learning Rate: 0.00009, Train Gradient: 175.1\n",
      "Epoch 67601/150000, Train Loss: 24863, Val Loss: 25301,  Learning Rate: 0.00009, Train Gradient: 175.0\n",
      "Epoch 67701/150000, Train Loss: 24856, Val Loss: 25295,  Learning Rate: 0.00009, Train Gradient: 175.0\n",
      "Epoch 67801/150000, Train Loss: 24850, Val Loss: 25288,  Learning Rate: 0.00009, Train Gradient: 174.9\n",
      "Epoch 67901/150000, Train Loss: 24843, Val Loss: 25281,  Learning Rate: 0.00009, Train Gradient: 174.9\n",
      "Epoch 68001/150000, Train Loss: 24837, Val Loss: 25275,  Learning Rate: 0.00009, Train Gradient: 174.8\n",
      "Epoch 68101/150000, Train Loss: 24830, Val Loss: 25268,  Learning Rate: 0.00009, Train Gradient: 174.7\n",
      "Epoch 68201/150000, Train Loss: 24824, Val Loss: 25262,  Learning Rate: 0.00009, Train Gradient: 174.7\n",
      "Epoch 68301/150000, Train Loss: 24817, Val Loss: 25255,  Learning Rate: 0.00009, Train Gradient: 174.6\n",
      "Epoch 68401/150000, Train Loss: 24811, Val Loss: 25248,  Learning Rate: 0.00009, Train Gradient: 174.6\n",
      "Epoch 68501/150000, Train Loss: 24804, Val Loss: 25242,  Learning Rate: 0.00009, Train Gradient: 174.5\n",
      "Epoch 68601/150000, Train Loss: 24798, Val Loss: 25235,  Learning Rate: 0.00009, Train Gradient: 174.4\n",
      "Epoch 68701/150000, Train Loss: 24791, Val Loss: 25229,  Learning Rate: 0.00009, Train Gradient: 174.4\n",
      "Epoch 68801/150000, Train Loss: 24785, Val Loss: 25222,  Learning Rate: 0.00009, Train Gradient: 174.3\n",
      "Epoch 68901/150000, Train Loss: 24778, Val Loss: 25215,  Learning Rate: 0.00009, Train Gradient: 174.3\n",
      "Epoch 69001/150000, Train Loss: 24772, Val Loss: 25209,  Learning Rate: 0.00009, Train Gradient: 174.2\n",
      "Epoch 69101/150000, Train Loss: 24765, Val Loss: 25202,  Learning Rate: 0.00009, Train Gradient: 174.1\n",
      "Epoch 69201/150000, Train Loss: 24759, Val Loss: 25196,  Learning Rate: 0.00009, Train Gradient: 174.1\n",
      "Epoch 69301/150000, Train Loss: 24752, Val Loss: 25189,  Learning Rate: 0.00009, Train Gradient: 174.0\n",
      "Epoch 69401/150000, Train Loss: 24746, Val Loss: 25183,  Learning Rate: 0.00009, Train Gradient: 174.0\n",
      "Epoch 69501/150000, Train Loss: 24739, Val Loss: 25176,  Learning Rate: 0.00009, Train Gradient: 173.9\n",
      "Epoch 69601/150000, Train Loss: 24733, Val Loss: 25170,  Learning Rate: 0.00009, Train Gradient: 173.8\n",
      "Epoch 69701/150000, Train Loss: 24726, Val Loss: 25163,  Learning Rate: 0.00009, Train Gradient: 173.8\n",
      "Epoch 69801/150000, Train Loss: 24720, Val Loss: 25156,  Learning Rate: 0.00009, Train Gradient: 173.7\n",
      "Epoch 69901/150000, Train Loss: 24713, Val Loss: 25150,  Learning Rate: 0.00009, Train Gradient: 173.7\n",
      "Epoch 70001/150000, Train Loss: 24707, Val Loss: 25143,  Learning Rate: 0.00009, Train Gradient: 173.6\n",
      "Epoch 70101/150000, Train Loss: 24701, Val Loss: 25137,  Learning Rate: 0.00009, Train Gradient: 173.6\n",
      "Epoch 70201/150000, Train Loss: 24694, Val Loss: 25130,  Learning Rate: 0.00009, Train Gradient: 173.5\n",
      "Epoch 70301/150000, Train Loss: 24688, Val Loss: 25124,  Learning Rate: 0.00009, Train Gradient: 173.4\n",
      "Epoch 70401/150000, Train Loss: 24681, Val Loss: 25117,  Learning Rate: 0.00009, Train Gradient: 173.4\n",
      "Epoch 70501/150000, Train Loss: 24675, Val Loss: 25111,  Learning Rate: 0.00009, Train Gradient: 173.3\n",
      "Epoch 70601/150000, Train Loss: 24668, Val Loss: 25104,  Learning Rate: 0.00009, Train Gradient: 173.3\n",
      "Epoch 70701/150000, Train Loss: 24662, Val Loss: 25098,  Learning Rate: 0.00009, Train Gradient: 173.2\n",
      "Epoch 70801/150000, Train Loss: 24655, Val Loss: 25091,  Learning Rate: 0.00009, Train Gradient: 173.1\n",
      "Epoch 70901/150000, Train Loss: 24649, Val Loss: 25085,  Learning Rate: 0.00009, Train Gradient: 173.1\n",
      "Epoch 71001/150000, Train Loss: 24643, Val Loss: 25078,  Learning Rate: 0.00009, Train Gradient: 173.0\n",
      "Epoch 71101/150000, Train Loss: 24636, Val Loss: 25072,  Learning Rate: 0.00009, Train Gradient: 173.0\n",
      "Epoch 71201/150000, Train Loss: 24630, Val Loss: 25065,  Learning Rate: 0.00009, Train Gradient: 172.9\n",
      "Epoch 71301/150000, Train Loss: 24623, Val Loss: 25059,  Learning Rate: 0.00009, Train Gradient: 172.9\n",
      "Epoch 71401/150000, Train Loss: 24617, Val Loss: 25052,  Learning Rate: 0.00009, Train Gradient: 172.8\n",
      "Epoch 71501/150000, Train Loss: 24610, Val Loss: 25046,  Learning Rate: 0.00009, Train Gradient: 172.7\n",
      "Epoch 71601/150000, Train Loss: 24604, Val Loss: 25039,  Learning Rate: 0.00009, Train Gradient: 172.7\n",
      "Epoch 71701/150000, Train Loss: 24598, Val Loss: 25033,  Learning Rate: 0.00009, Train Gradient: 172.6\n",
      "Epoch 71801/150000, Train Loss: 24591, Val Loss: 25026,  Learning Rate: 0.00009, Train Gradient: 172.6\n",
      "Epoch 71901/150000, Train Loss: 24585, Val Loss: 25020,  Learning Rate: 0.00009, Train Gradient: 172.5\n",
      "Epoch 72001/150000, Train Loss: 24578, Val Loss: 25013,  Learning Rate: 0.00009, Train Gradient: 172.4\n",
      "Epoch 72101/150000, Train Loss: 24572, Val Loss: 25007,  Learning Rate: 0.00009, Train Gradient: 172.4\n",
      "Epoch 72201/150000, Train Loss: 24565, Val Loss: 25000,  Learning Rate: 0.00009, Train Gradient: 172.3\n",
      "Epoch 72301/150000, Train Loss: 24559, Val Loss: 24994,  Learning Rate: 0.00009, Train Gradient: 172.3\n",
      "Epoch 72401/150000, Train Loss: 24553, Val Loss: 24987,  Learning Rate: 0.00009, Train Gradient: 172.2\n",
      "Epoch 72501/150000, Train Loss: 24546, Val Loss: 24981,  Learning Rate: 0.00009, Train Gradient: 172.2\n",
      "Epoch 72601/150000, Train Loss: 24540, Val Loss: 24974,  Learning Rate: 0.00009, Train Gradient: 172.1\n",
      "Epoch 72701/150000, Train Loss: 24533, Val Loss: 24968,  Learning Rate: 0.00009, Train Gradient: 172.0\n",
      "Epoch 72801/150000, Train Loss: 24527, Val Loss: 24961,  Learning Rate: 0.00009, Train Gradient: 172.0\n",
      "Epoch 72901/150000, Train Loss: 24521, Val Loss: 24955,  Learning Rate: 0.00009, Train Gradient: 171.9\n",
      "Epoch 73001/150000, Train Loss: 24514, Val Loss: 24948,  Learning Rate: 0.00009, Train Gradient: 171.9\n",
      "Epoch 73101/150000, Train Loss: 24508, Val Loss: 24942,  Learning Rate: 0.00009, Train Gradient: 171.8\n",
      "Epoch 73201/150000, Train Loss: 24501, Val Loss: 24935,  Learning Rate: 0.00009, Train Gradient: 171.8\n",
      "Epoch 73301/150000, Train Loss: 24495, Val Loss: 24929,  Learning Rate: 0.00009, Train Gradient: 171.7\n",
      "Epoch 73401/150000, Train Loss: 24489, Val Loss: 24922,  Learning Rate: 0.00009, Train Gradient: 171.6\n",
      "Epoch 73501/150000, Train Loss: 24482, Val Loss: 24916,  Learning Rate: 0.00009, Train Gradient: 171.6\n",
      "Epoch 73601/150000, Train Loss: 24476, Val Loss: 24909,  Learning Rate: 0.00009, Train Gradient: 171.5\n",
      "Epoch 73701/150000, Train Loss: 24469, Val Loss: 24903,  Learning Rate: 0.00009, Train Gradient: 171.5\n",
      "Epoch 73801/150000, Train Loss: 24463, Val Loss: 24897,  Learning Rate: 0.00009, Train Gradient: 171.4\n",
      "Epoch 73901/150000, Train Loss: 24457, Val Loss: 24890,  Learning Rate: 0.00009, Train Gradient: 171.4\n",
      "Epoch 74001/150000, Train Loss: 24450, Val Loss: 24884,  Learning Rate: 0.00009, Train Gradient: 171.3\n",
      "Epoch 74101/150000, Train Loss: 24444, Val Loss: 24877,  Learning Rate: 0.00009, Train Gradient: 171.3\n",
      "Epoch 74201/150000, Train Loss: 24438, Val Loss: 24871,  Learning Rate: 0.00009, Train Gradient: 171.2\n",
      "Epoch 74301/150000, Train Loss: 24431, Val Loss: 24864,  Learning Rate: 0.00009, Train Gradient: 171.2\n",
      "Epoch 74401/150000, Train Loss: 24425, Val Loss: 24858,  Learning Rate: 0.00009, Train Gradient: 171.1\n",
      "Epoch 74501/150000, Train Loss: 24418, Val Loss: 24851,  Learning Rate: 0.00009, Train Gradient: 171.0\n",
      "Epoch 74601/150000, Train Loss: 24412, Val Loss: 24845,  Learning Rate: 0.00009, Train Gradient: 171.0\n",
      "Epoch 74701/150000, Train Loss: 24406, Val Loss: 24838,  Learning Rate: 0.00009, Train Gradient: 170.9\n",
      "Epoch 74801/150000, Train Loss: 24399, Val Loss: 24832,  Learning Rate: 0.00009, Train Gradient: 170.9\n",
      "Epoch 74901/150000, Train Loss: 24393, Val Loss: 24826,  Learning Rate: 0.00009, Train Gradient: 170.8\n",
      "Epoch 75001/150000, Train Loss: 24387, Val Loss: 24819,  Learning Rate: 0.00009, Train Gradient: 170.8\n",
      "Epoch 75101/150000, Train Loss: 24380, Val Loss: 24813,  Learning Rate: 0.00009, Train Gradient: 170.7\n",
      "Epoch 75201/150000, Train Loss: 24374, Val Loss: 24806,  Learning Rate: 0.00009, Train Gradient: 170.6\n",
      "Epoch 75301/150000, Train Loss: 24368, Val Loss: 24800,  Learning Rate: 0.00009, Train Gradient: 170.6\n",
      "Epoch 75401/150000, Train Loss: 24361, Val Loss: 24793,  Learning Rate: 0.00009, Train Gradient: 170.5\n",
      "Epoch 75501/150000, Train Loss: 24355, Val Loss: 24787,  Learning Rate: 0.00009, Train Gradient: 170.5\n",
      "Epoch 75601/150000, Train Loss: 24349, Val Loss: 24781,  Learning Rate: 0.00009, Train Gradient: 170.4\n",
      "Epoch 75701/150000, Train Loss: 24342, Val Loss: 24774,  Learning Rate: 0.00009, Train Gradient: 170.4\n",
      "Epoch 75801/150000, Train Loss: 24336, Val Loss: 24768,  Learning Rate: 0.00009, Train Gradient: 170.3\n",
      "Epoch 75901/150000, Train Loss: 24330, Val Loss: 24761,  Learning Rate: 0.00009, Train Gradient: 170.3\n",
      "Epoch 76001/150000, Train Loss: 24323, Val Loss: 24755,  Learning Rate: 0.00009, Train Gradient: 170.2\n",
      "Epoch 76101/150000, Train Loss: 24317, Val Loss: 24748,  Learning Rate: 0.00009, Train Gradient: 170.1\n",
      "Epoch 76201/150000, Train Loss: 24311, Val Loss: 24742,  Learning Rate: 0.00009, Train Gradient: 170.1\n",
      "Epoch 76301/150000, Train Loss: 24304, Val Loss: 24736,  Learning Rate: 0.00009, Train Gradient: 170.0\n",
      "Epoch 76401/150000, Train Loss: 24298, Val Loss: 24729,  Learning Rate: 0.00009, Train Gradient: 170.0\n",
      "Epoch 76501/150000, Train Loss: 24292, Val Loss: 24723,  Learning Rate: 0.00009, Train Gradient: 169.9\n",
      "Epoch 76601/150000, Train Loss: 24285, Val Loss: 24716,  Learning Rate: 0.00009, Train Gradient: 169.9\n",
      "Epoch 76701/150000, Train Loss: 24279, Val Loss: 24710,  Learning Rate: 0.00009, Train Gradient: 169.8\n",
      "Epoch 76801/150000, Train Loss: 24273, Val Loss: 24704,  Learning Rate: 0.00009, Train Gradient: 169.7\n",
      "Epoch 76901/150000, Train Loss: 24266, Val Loss: 24697,  Learning Rate: 0.00009, Train Gradient: 169.7\n",
      "Epoch 77001/150000, Train Loss: 24260, Val Loss: 24691,  Learning Rate: 0.00009, Train Gradient: 169.6\n",
      "Epoch 77101/150000, Train Loss: 24254, Val Loss: 24685,  Learning Rate: 0.00009, Train Gradient: 169.6\n",
      "Epoch 77201/150000, Train Loss: 24247, Val Loss: 24678,  Learning Rate: 0.00009, Train Gradient: 169.5\n",
      "Epoch 77301/150000, Train Loss: 24241, Val Loss: 24672,  Learning Rate: 0.00009, Train Gradient: 169.5\n",
      "Epoch 77401/150000, Train Loss: 24235, Val Loss: 24665,  Learning Rate: 0.00009, Train Gradient: 169.4\n",
      "Epoch 77501/150000, Train Loss: 24229, Val Loss: 24659,  Learning Rate: 0.00009, Train Gradient: 169.4\n",
      "Epoch 77601/150000, Train Loss: 24222, Val Loss: 24653,  Learning Rate: 0.00009, Train Gradient: 169.3\n",
      "Epoch 77701/150000, Train Loss: 24216, Val Loss: 24646,  Learning Rate: 0.00009, Train Gradient: 169.2\n",
      "Epoch 77801/150000, Train Loss: 24210, Val Loss: 24640,  Learning Rate: 0.00009, Train Gradient: 169.2\n",
      "Epoch 77901/150000, Train Loss: 24203, Val Loss: 24633,  Learning Rate: 0.00009, Train Gradient: 169.1\n",
      "Epoch 78001/150000, Train Loss: 24197, Val Loss: 24627,  Learning Rate: 0.00009, Train Gradient: 169.1\n",
      "Epoch 78101/150000, Train Loss: 24191, Val Loss: 24621,  Learning Rate: 0.00009, Train Gradient: 169.0\n",
      "Epoch 78201/150000, Train Loss: 24184, Val Loss: 24614,  Learning Rate: 0.00009, Train Gradient: 169.0\n",
      "Epoch 78301/150000, Train Loss: 24178, Val Loss: 24608,  Learning Rate: 0.00009, Train Gradient: 168.9\n",
      "Epoch 78401/150000, Train Loss: 24172, Val Loss: 24602,  Learning Rate: 0.00009, Train Gradient: 168.9\n",
      "Epoch 78501/150000, Train Loss: 24166, Val Loss: 24595,  Learning Rate: 0.00009, Train Gradient: 168.8\n",
      "Epoch 78601/150000, Train Loss: 24159, Val Loss: 24589,  Learning Rate: 0.00009, Train Gradient: 168.8\n",
      "Epoch 78701/150000, Train Loss: 24153, Val Loss: 24583,  Learning Rate: 0.00009, Train Gradient: 168.7\n",
      "Epoch 78801/150000, Train Loss: 24147, Val Loss: 24576,  Learning Rate: 0.00009, Train Gradient: 168.7\n",
      "Epoch 78901/150000, Train Loss: 24140, Val Loss: 24570,  Learning Rate: 0.00009, Train Gradient: 168.6\n",
      "Epoch 79001/150000, Train Loss: 24134, Val Loss: 24563,  Learning Rate: 0.00009, Train Gradient: 168.6\n",
      "Epoch 79101/150000, Train Loss: 24128, Val Loss: 24557,  Learning Rate: 0.00009, Train Gradient: 168.5\n",
      "Epoch 79201/150000, Train Loss: 24122, Val Loss: 24551,  Learning Rate: 0.00009, Train Gradient: 168.4\n",
      "Epoch 79301/150000, Train Loss: 24115, Val Loss: 24544,  Learning Rate: 0.00009, Train Gradient: 168.4\n",
      "Epoch 79401/150000, Train Loss: 24109, Val Loss: 24538,  Learning Rate: 0.00009, Train Gradient: 168.3\n",
      "Epoch 79501/150000, Train Loss: 24103, Val Loss: 24532,  Learning Rate: 0.00009, Train Gradient: 168.3\n",
      "Epoch 79601/150000, Train Loss: 24096, Val Loss: 24525,  Learning Rate: 0.00009, Train Gradient: 168.2\n",
      "Epoch 79701/150000, Train Loss: 24090, Val Loss: 24519,  Learning Rate: 0.00009, Train Gradient: 168.2\n",
      "Epoch 79801/150000, Train Loss: 24084, Val Loss: 24513,  Learning Rate: 0.00009, Train Gradient: 168.1\n",
      "Epoch 79901/150000, Train Loss: 24077, Val Loss: 24506,  Learning Rate: 0.00009, Train Gradient: 168.1\n",
      "Epoch 80001/150000, Train Loss: 24071, Val Loss: 24500,  Learning Rate: 0.00009, Train Gradient: 168.0\n",
      "Epoch 80101/150000, Train Loss: 24065, Val Loss: 24493,  Learning Rate: 0.00009, Train Gradient: 167.9\n",
      "Epoch 80201/150000, Train Loss: 24059, Val Loss: 24487,  Learning Rate: 0.00009, Train Gradient: 167.9\n",
      "Epoch 80301/150000, Train Loss: 24052, Val Loss: 24481,  Learning Rate: 0.00009, Train Gradient: 167.8\n",
      "Epoch 80401/150000, Train Loss: 24046, Val Loss: 24474,  Learning Rate: 0.00009, Train Gradient: 167.8\n",
      "Epoch 80501/150000, Train Loss: 24040, Val Loss: 24468,  Learning Rate: 0.00009, Train Gradient: 167.7\n",
      "Epoch 80601/150000, Train Loss: 24034, Val Loss: 24462,  Learning Rate: 0.00009, Train Gradient: 167.7\n",
      "Epoch 80701/150000, Train Loss: 24027, Val Loss: 24455,  Learning Rate: 0.00009, Train Gradient: 167.6\n",
      "Epoch 80801/150000, Train Loss: 24021, Val Loss: 24449,  Learning Rate: 0.00009, Train Gradient: 167.6\n",
      "Epoch 80901/150000, Train Loss: 24015, Val Loss: 24443,  Learning Rate: 0.00009, Train Gradient: 167.5\n",
      "Epoch 81001/150000, Train Loss: 24008, Val Loss: 24436,  Learning Rate: 0.00009, Train Gradient: 167.5\n",
      "Epoch 81101/150000, Train Loss: 24002, Val Loss: 24430,  Learning Rate: 0.00009, Train Gradient: 167.4\n",
      "Epoch 81201/150000, Train Loss: 23996, Val Loss: 24424,  Learning Rate: 0.00009, Train Gradient: 167.3\n",
      "Epoch 81301/150000, Train Loss: 23990, Val Loss: 24417,  Learning Rate: 0.00009, Train Gradient: 167.3\n",
      "Epoch 81401/150000, Train Loss: 23983, Val Loss: 24411,  Learning Rate: 0.00009, Train Gradient: 167.2\n",
      "Epoch 81501/150000, Train Loss: 23977, Val Loss: 24405,  Learning Rate: 0.00009, Train Gradient: 167.2\n",
      "Epoch 81601/150000, Train Loss: 23971, Val Loss: 24398,  Learning Rate: 0.00009, Train Gradient: 167.1\n",
      "Epoch 81701/150000, Train Loss: 23965, Val Loss: 24392,  Learning Rate: 0.00009, Train Gradient: 167.1\n",
      "Epoch 81801/150000, Train Loss: 23958, Val Loss: 24386,  Learning Rate: 0.00009, Train Gradient: 167.0\n",
      "Epoch 81901/150000, Train Loss: 23952, Val Loss: 24379,  Learning Rate: 0.00009, Train Gradient: 167.0\n",
      "Epoch 82001/150000, Train Loss: 23946, Val Loss: 24373,  Learning Rate: 0.00009, Train Gradient: 166.9\n",
      "Epoch 82101/150000, Train Loss: 23940, Val Loss: 24367,  Learning Rate: 0.00009, Train Gradient: 166.8\n",
      "Epoch 82201/150000, Train Loss: 23933, Val Loss: 24360,  Learning Rate: 0.00009, Train Gradient: 166.8\n",
      "Epoch 82301/150000, Train Loss: 23927, Val Loss: 24354,  Learning Rate: 0.00009, Train Gradient: 166.7\n",
      "Epoch 82401/150000, Train Loss: 23921, Val Loss: 24348,  Learning Rate: 0.00009, Train Gradient: 166.7\n",
      "Epoch 82501/150000, Train Loss: 23915, Val Loss: 24342,  Learning Rate: 0.00009, Train Gradient: 166.6\n",
      "Epoch 82601/150000, Train Loss: 23909, Val Loss: 24335,  Learning Rate: 0.00009, Train Gradient: 166.6\n",
      "Epoch 82701/150000, Train Loss: 23902, Val Loss: 24329,  Learning Rate: 0.00009, Train Gradient: 166.5\n",
      "Epoch 82801/150000, Train Loss: 23896, Val Loss: 24323,  Learning Rate: 0.00009, Train Gradient: 166.5\n",
      "Epoch 82901/150000, Train Loss: 23890, Val Loss: 24316,  Learning Rate: 0.00009, Train Gradient: 166.4\n",
      "Epoch 83001/150000, Train Loss: 23884, Val Loss: 24310,  Learning Rate: 0.00009, Train Gradient: 166.4\n",
      "Epoch 83101/150000, Train Loss: 23877, Val Loss: 24304,  Learning Rate: 0.00009, Train Gradient: 166.3\n",
      "Epoch 83201/150000, Train Loss: 23871, Val Loss: 24297,  Learning Rate: 0.00009, Train Gradient: 166.2\n",
      "Epoch 83301/150000, Train Loss: 23865, Val Loss: 24291,  Learning Rate: 0.00009, Train Gradient: 166.2\n",
      "Epoch 83401/150000, Train Loss: 23859, Val Loss: 24285,  Learning Rate: 0.00009, Train Gradient: 166.1\n",
      "Epoch 83501/150000, Train Loss: 23853, Val Loss: 24279,  Learning Rate: 0.00009, Train Gradient: 166.1\n",
      "Epoch 83601/150000, Train Loss: 23846, Val Loss: 24272,  Learning Rate: 0.00009, Train Gradient: 166.0\n",
      "Epoch 83701/150000, Train Loss: 23840, Val Loss: 24266,  Learning Rate: 0.00009, Train Gradient: 166.0\n",
      "Epoch 83801/150000, Train Loss: 23834, Val Loss: 24260,  Learning Rate: 0.00009, Train Gradient: 165.9\n",
      "Epoch 83901/150000, Train Loss: 23828, Val Loss: 24254,  Learning Rate: 0.00009, Train Gradient: 165.9\n",
      "Epoch 84001/150000, Train Loss: 23822, Val Loss: 24247,  Learning Rate: 0.00009, Train Gradient: 165.8\n",
      "Epoch 84101/150000, Train Loss: 23815, Val Loss: 24241,  Learning Rate: 0.00009, Train Gradient: 165.8\n",
      "Epoch 84201/150000, Train Loss: 23809, Val Loss: 24235,  Learning Rate: 0.00009, Train Gradient: 165.7\n",
      "Epoch 84301/150000, Train Loss: 23803, Val Loss: 24228,  Learning Rate: 0.00009, Train Gradient: 165.6\n",
      "Epoch 84401/150000, Train Loss: 23797, Val Loss: 24222,  Learning Rate: 0.00009, Train Gradient: 165.6\n",
      "Epoch 84501/150000, Train Loss: 23791, Val Loss: 24216,  Learning Rate: 0.00009, Train Gradient: 165.5\n",
      "Epoch 84601/150000, Train Loss: 23784, Val Loss: 24210,  Learning Rate: 0.00009, Train Gradient: 165.5\n",
      "Epoch 84701/150000, Train Loss: 23778, Val Loss: 24203,  Learning Rate: 0.00009, Train Gradient: 165.4\n",
      "Epoch 84801/150000, Train Loss: 23772, Val Loss: 24197,  Learning Rate: 0.00009, Train Gradient: 165.4\n",
      "Epoch 84901/150000, Train Loss: 23766, Val Loss: 24191,  Learning Rate: 0.00009, Train Gradient: 165.3\n",
      "Epoch 85001/150000, Train Loss: 23760, Val Loss: 24185,  Learning Rate: 0.00009, Train Gradient: 165.3\n",
      "Epoch 85101/150000, Train Loss: 23753, Val Loss: 24178,  Learning Rate: 0.00009, Train Gradient: 165.2\n",
      "Epoch 85201/150000, Train Loss: 23747, Val Loss: 24172,  Learning Rate: 0.00009, Train Gradient: 165.2\n",
      "Epoch 85301/150000, Train Loss: 23741, Val Loss: 24166,  Learning Rate: 0.00009, Train Gradient: 165.1\n",
      "Epoch 85401/150000, Train Loss: 23735, Val Loss: 24160,  Learning Rate: 0.00009, Train Gradient: 165.0\n",
      "Epoch 85501/150000, Train Loss: 23729, Val Loss: 24153,  Learning Rate: 0.00009, Train Gradient: 165.0\n",
      "Epoch 85601/150000, Train Loss: 23723, Val Loss: 24147,  Learning Rate: 0.00009, Train Gradient: 164.9\n",
      "Epoch 85701/150000, Train Loss: 23716, Val Loss: 24141,  Learning Rate: 0.00009, Train Gradient: 164.9\n",
      "Epoch 85801/150000, Train Loss: 23710, Val Loss: 24135,  Learning Rate: 0.00009, Train Gradient: 164.8\n",
      "Epoch 85901/150000, Train Loss: 23704, Val Loss: 24128,  Learning Rate: 0.00009, Train Gradient: 164.8\n",
      "Epoch 86001/150000, Train Loss: 23698, Val Loss: 24122,  Learning Rate: 0.00009, Train Gradient: 164.7\n",
      "Epoch 86101/150000, Train Loss: 23692, Val Loss: 24116,  Learning Rate: 0.00009, Train Gradient: 164.7\n",
      "Epoch 86201/150000, Train Loss: 23686, Val Loss: 24110,  Learning Rate: 0.00009, Train Gradient: 164.6\n",
      "Epoch 86301/150000, Train Loss: 23679, Val Loss: 24104,  Learning Rate: 0.00009, Train Gradient: 164.6\n",
      "Epoch 86401/150000, Train Loss: 23673, Val Loss: 24097,  Learning Rate: 0.00009, Train Gradient: 164.5\n",
      "Epoch 86501/150000, Train Loss: 23667, Val Loss: 24091,  Learning Rate: 0.00009, Train Gradient: 164.5\n",
      "Epoch 86601/150000, Train Loss: 23661, Val Loss: 24085,  Learning Rate: 0.00009, Train Gradient: 164.4\n",
      "Epoch 86701/150000, Train Loss: 23655, Val Loss: 24079,  Learning Rate: 0.00009, Train Gradient: 164.3\n",
      "Epoch 86801/150000, Train Loss: 23649, Val Loss: 24072,  Learning Rate: 0.00009, Train Gradient: 164.3\n",
      "Epoch 86901/150000, Train Loss: 23643, Val Loss: 24066,  Learning Rate: 0.00009, Train Gradient: 164.2\n",
      "Epoch 87001/150000, Train Loss: 23636, Val Loss: 24060,  Learning Rate: 0.00009, Train Gradient: 164.2\n",
      "Epoch 87101/150000, Train Loss: 23630, Val Loss: 24054,  Learning Rate: 0.00009, Train Gradient: 164.1\n",
      "Epoch 87201/150000, Train Loss: 23624, Val Loss: 24048,  Learning Rate: 0.00009, Train Gradient: 164.1\n",
      "Epoch 87301/150000, Train Loss: 23618, Val Loss: 24041,  Learning Rate: 0.00009, Train Gradient: 164.0\n",
      "Epoch 87401/150000, Train Loss: 23612, Val Loss: 24035,  Learning Rate: 0.00009, Train Gradient: 164.0\n",
      "Epoch 87501/150000, Train Loss: 23606, Val Loss: 24029,  Learning Rate: 0.00009, Train Gradient: 163.9\n",
      "Epoch 87601/150000, Train Loss: 23600, Val Loss: 24023,  Learning Rate: 0.00009, Train Gradient: 163.9\n",
      "Epoch 87701/150000, Train Loss: 23594, Val Loss: 24017,  Learning Rate: 0.00009, Train Gradient: 163.8\n",
      "Epoch 87801/150000, Train Loss: 23587, Val Loss: 24010,  Learning Rate: 0.00009, Train Gradient: 163.8\n",
      "Epoch 87901/150000, Train Loss: 23581, Val Loss: 24004,  Learning Rate: 0.00009, Train Gradient: 163.7\n",
      "Epoch 88001/150000, Train Loss: 23575, Val Loss: 23998,  Learning Rate: 0.00009, Train Gradient: 163.6\n",
      "Epoch 88101/150000, Train Loss: 23569, Val Loss: 23992,  Learning Rate: 0.00009, Train Gradient: 163.6\n",
      "Epoch 88201/150000, Train Loss: 23563, Val Loss: 23986,  Learning Rate: 0.00009, Train Gradient: 163.5\n",
      "Epoch 88301/150000, Train Loss: 23557, Val Loss: 23980,  Learning Rate: 0.00009, Train Gradient: 163.5\n",
      "Epoch 88401/150000, Train Loss: 23551, Val Loss: 23973,  Learning Rate: 0.00009, Train Gradient: 163.4\n",
      "Epoch 88501/150000, Train Loss: 23545, Val Loss: 23967,  Learning Rate: 0.00009, Train Gradient: 163.4\n",
      "Epoch 88601/150000, Train Loss: 23538, Val Loss: 23961,  Learning Rate: 0.00009, Train Gradient: 163.3\n",
      "Epoch 88701/150000, Train Loss: 23532, Val Loss: 23955,  Learning Rate: 0.00009, Train Gradient: 163.3\n",
      "Epoch 88801/150000, Train Loss: 23526, Val Loss: 23949,  Learning Rate: 0.00009, Train Gradient: 163.2\n",
      "Epoch 88901/150000, Train Loss: 23520, Val Loss: 23943,  Learning Rate: 0.00009, Train Gradient: 163.2\n",
      "Epoch 89001/150000, Train Loss: 23514, Val Loss: 23936,  Learning Rate: 0.00009, Train Gradient: 163.1\n",
      "Epoch 89101/150000, Train Loss: 23508, Val Loss: 23930,  Learning Rate: 0.00009, Train Gradient: 163.1\n",
      "Epoch 89201/150000, Train Loss: 23502, Val Loss: 23924,  Learning Rate: 0.00009, Train Gradient: 163.0\n",
      "Epoch 89301/150000, Train Loss: 23496, Val Loss: 23918,  Learning Rate: 0.00009, Train Gradient: 163.0\n",
      "Epoch 89401/150000, Train Loss: 23490, Val Loss: 23912,  Learning Rate: 0.00009, Train Gradient: 162.9\n",
      "Epoch 89501/150000, Train Loss: 23484, Val Loss: 23906,  Learning Rate: 0.00009, Train Gradient: 162.9\n",
      "Epoch 89601/150000, Train Loss: 23477, Val Loss: 23899,  Learning Rate: 0.00009, Train Gradient: 162.8\n",
      "Epoch 89701/150000, Train Loss: 23471, Val Loss: 23893,  Learning Rate: 0.00009, Train Gradient: 162.8\n",
      "Epoch 89801/150000, Train Loss: 23465, Val Loss: 23887,  Learning Rate: 0.00009, Train Gradient: 162.7\n",
      "Epoch 89901/150000, Train Loss: 23459, Val Loss: 23881,  Learning Rate: 0.00009, Train Gradient: 162.6\n",
      "Epoch 90001/150000, Train Loss: 23453, Val Loss: 23875,  Learning Rate: 0.00009, Train Gradient: 162.6\n",
      "Epoch 90101/150000, Train Loss: 23447, Val Loss: 23869,  Learning Rate: 0.00009, Train Gradient: 162.5\n",
      "Epoch 90201/150000, Train Loss: 23441, Val Loss: 23863,  Learning Rate: 0.00009, Train Gradient: 162.5\n",
      "Epoch 90301/150000, Train Loss: 23435, Val Loss: 23856,  Learning Rate: 0.00009, Train Gradient: 162.4\n",
      "Epoch 90401/150000, Train Loss: 23429, Val Loss: 23850,  Learning Rate: 0.00009, Train Gradient: 162.4\n",
      "Epoch 90501/150000, Train Loss: 23423, Val Loss: 23844,  Learning Rate: 0.00009, Train Gradient: 162.3\n",
      "Epoch 90601/150000, Train Loss: 23417, Val Loss: 23838,  Learning Rate: 0.00009, Train Gradient: 162.3\n",
      "Epoch 90701/150000, Train Loss: 23411, Val Loss: 23832,  Learning Rate: 0.00009, Train Gradient: 162.2\n",
      "Epoch 90801/150000, Train Loss: 23405, Val Loss: 23826,  Learning Rate: 0.00009, Train Gradient: 162.2\n",
      "Epoch 90901/150000, Train Loss: 23399, Val Loss: 23820,  Learning Rate: 0.00009, Train Gradient: 162.1\n",
      "Epoch 91001/150000, Train Loss: 23392, Val Loss: 23814,  Learning Rate: 0.00009, Train Gradient: 162.1\n",
      "Epoch 91101/150000, Train Loss: 23386, Val Loss: 23807,  Learning Rate: 0.00009, Train Gradient: 162.0\n",
      "Epoch 91201/150000, Train Loss: 23380, Val Loss: 23801,  Learning Rate: 0.00009, Train Gradient: 162.0\n",
      "Epoch 91301/150000, Train Loss: 23374, Val Loss: 23795,  Learning Rate: 0.00009, Train Gradient: 161.9\n",
      "Epoch 91401/150000, Train Loss: 23368, Val Loss: 23789,  Learning Rate: 0.00009, Train Gradient: 161.9\n",
      "Epoch 91501/150000, Train Loss: 23362, Val Loss: 23783,  Learning Rate: 0.00009, Train Gradient: 161.8\n",
      "Epoch 91601/150000, Train Loss: 23356, Val Loss: 23777,  Learning Rate: 0.00009, Train Gradient: 161.8\n",
      "Epoch 91701/150000, Train Loss: 23350, Val Loss: 23771,  Learning Rate: 0.00009, Train Gradient: 161.7\n",
      "Epoch 91801/150000, Train Loss: 23344, Val Loss: 23765,  Learning Rate: 0.00009, Train Gradient: 161.7\n",
      "Epoch 91901/150000, Train Loss: 23338, Val Loss: 23759,  Learning Rate: 0.00009, Train Gradient: 161.6\n",
      "Epoch 92001/150000, Train Loss: 23332, Val Loss: 23752,  Learning Rate: 0.00009, Train Gradient: 161.6\n",
      "Epoch 92101/150000, Train Loss: 23326, Val Loss: 23746,  Learning Rate: 0.00009, Train Gradient: 161.5\n",
      "Epoch 92201/150000, Train Loss: 23320, Val Loss: 23740,  Learning Rate: 0.00009, Train Gradient: 161.5\n",
      "Epoch 92301/150000, Train Loss: 23314, Val Loss: 23734,  Learning Rate: 0.00009, Train Gradient: 161.4\n",
      "Epoch 92401/150000, Train Loss: 23308, Val Loss: 23728,  Learning Rate: 0.00009, Train Gradient: 161.3\n",
      "Epoch 92501/150000, Train Loss: 23302, Val Loss: 23722,  Learning Rate: 0.00009, Train Gradient: 161.3\n",
      "Epoch 92601/150000, Train Loss: 23296, Val Loss: 23716,  Learning Rate: 0.00009, Train Gradient: 161.2\n",
      "Epoch 92701/150000, Train Loss: 23290, Val Loss: 23710,  Learning Rate: 0.00009, Train Gradient: 161.2\n",
      "Epoch 92801/150000, Train Loss: 23284, Val Loss: 23704,  Learning Rate: 0.00009, Train Gradient: 161.1\n",
      "Epoch 92901/150000, Train Loss: 23278, Val Loss: 23698,  Learning Rate: 0.00009, Train Gradient: 161.1\n",
      "Epoch 93001/150000, Train Loss: 23272, Val Loss: 23692,  Learning Rate: 0.00009, Train Gradient: 161.0\n",
      "Epoch 93101/150000, Train Loss: 23266, Val Loss: 23685,  Learning Rate: 0.00009, Train Gradient: 161.0\n",
      "Epoch 93201/150000, Train Loss: 23260, Val Loss: 23679,  Learning Rate: 0.00009, Train Gradient: 160.9\n",
      "Epoch 93301/150000, Train Loss: 23254, Val Loss: 23673,  Learning Rate: 0.00009, Train Gradient: 160.9\n",
      "Epoch 93401/150000, Train Loss: 23248, Val Loss: 23667,  Learning Rate: 0.00009, Train Gradient: 160.8\n",
      "Epoch 93501/150000, Train Loss: 23242, Val Loss: 23661,  Learning Rate: 0.00009, Train Gradient: 160.8\n",
      "Epoch 93601/150000, Train Loss: 23236, Val Loss: 23655,  Learning Rate: 0.00009, Train Gradient: 160.7\n",
      "Epoch 93701/150000, Train Loss: 23230, Val Loss: 23649,  Learning Rate: 0.00009, Train Gradient: 160.7\n",
      "Epoch 93801/150000, Train Loss: 23224, Val Loss: 23643,  Learning Rate: 0.00009, Train Gradient: 160.6\n",
      "Epoch 93901/150000, Train Loss: 23218, Val Loss: 23637,  Learning Rate: 0.00009, Train Gradient: 160.6\n",
      "Epoch 94001/150000, Train Loss: 23212, Val Loss: 23631,  Learning Rate: 0.00009, Train Gradient: 160.5\n",
      "Epoch 94101/150000, Train Loss: 23206, Val Loss: 23625,  Learning Rate: 0.00009, Train Gradient: 160.5\n",
      "Epoch 94201/150000, Train Loss: 23200, Val Loss: 23619,  Learning Rate: 0.00009, Train Gradient: 160.4\n",
      "Epoch 94301/150000, Train Loss: 23194, Val Loss: 23613,  Learning Rate: 0.00009, Train Gradient: 160.4\n",
      "Epoch 94401/150000, Train Loss: 23188, Val Loss: 23607,  Learning Rate: 0.00009, Train Gradient: 160.3\n",
      "Epoch 94501/150000, Train Loss: 23182, Val Loss: 23601,  Learning Rate: 0.00009, Train Gradient: 160.3\n",
      "Epoch 94601/150000, Train Loss: 23176, Val Loss: 23595,  Learning Rate: 0.00009, Train Gradient: 160.2\n",
      "Epoch 94701/150000, Train Loss: 23170, Val Loss: 23589,  Learning Rate: 0.00009, Train Gradient: 160.2\n",
      "Epoch 94801/150000, Train Loss: 23164, Val Loss: 23582,  Learning Rate: 0.00009, Train Gradient: 160.1\n",
      "Epoch 94901/150000, Train Loss: 23158, Val Loss: 23576,  Learning Rate: 0.00009, Train Gradient: 160.1\n",
      "Epoch 95001/150000, Train Loss: 23152, Val Loss: 23570,  Learning Rate: 0.00009, Train Gradient: 160.0\n",
      "Epoch 95101/150000, Train Loss: 23146, Val Loss: 23564,  Learning Rate: 0.00009, Train Gradient: 160.0\n",
      "Epoch 95201/150000, Train Loss: 23140, Val Loss: 23558,  Learning Rate: 0.00009, Train Gradient: 159.9\n",
      "Epoch 95301/150000, Train Loss: 23134, Val Loss: 23552,  Learning Rate: 0.00009, Train Gradient: 159.9\n",
      "Epoch 95401/150000, Train Loss: 23128, Val Loss: 23546,  Learning Rate: 0.00009, Train Gradient: 159.8\n",
      "Epoch 95501/150000, Train Loss: 23122, Val Loss: 23540,  Learning Rate: 0.00009, Train Gradient: 159.8\n",
      "Epoch 95601/150000, Train Loss: 23116, Val Loss: 23534,  Learning Rate: 0.00009, Train Gradient: 159.7\n",
      "Epoch 95701/150000, Train Loss: 23110, Val Loss: 23528,  Learning Rate: 0.00009, Train Gradient: 159.7\n",
      "Epoch 95801/150000, Train Loss: 23104, Val Loss: 23522,  Learning Rate: 0.00009, Train Gradient: 159.6\n",
      "Epoch 95901/150000, Train Loss: 23098, Val Loss: 23516,  Learning Rate: 0.00009, Train Gradient: 159.6\n",
      "Epoch 96001/150000, Train Loss: 23092, Val Loss: 23510,  Learning Rate: 0.00009, Train Gradient: 159.5\n",
      "Epoch 96101/150000, Train Loss: 23086, Val Loss: 23504,  Learning Rate: 0.00009, Train Gradient: 159.5\n",
      "Epoch 96201/150000, Train Loss: 23080, Val Loss: 23498,  Learning Rate: 0.00009, Train Gradient: 159.4\n",
      "Epoch 96301/150000, Train Loss: 23074, Val Loss: 23492,  Learning Rate: 0.00009, Train Gradient: 159.4\n",
      "Epoch 96401/150000, Train Loss: 23068, Val Loss: 23486,  Learning Rate: 0.00009, Train Gradient: 159.3\n",
      "Epoch 96501/150000, Train Loss: 23062, Val Loss: 23480,  Learning Rate: 0.00009, Train Gradient: 159.3\n",
      "Epoch 96601/150000, Train Loss: 23056, Val Loss: 23474,  Learning Rate: 0.00009, Train Gradient: 159.2\n",
      "Epoch 96701/150000, Train Loss: 23050, Val Loss: 23468,  Learning Rate: 0.00009, Train Gradient: 159.2\n",
      "Epoch 96801/150000, Train Loss: 23044, Val Loss: 23462,  Learning Rate: 0.00009, Train Gradient: 159.1\n",
      "Epoch 96901/150000, Train Loss: 23038, Val Loss: 23456,  Learning Rate: 0.00009, Train Gradient: 159.1\n",
      "Epoch 97001/150000, Train Loss: 23032, Val Loss: 23450,  Learning Rate: 0.00009, Train Gradient: 159.0\n",
      "Epoch 97101/150000, Train Loss: 23026, Val Loss: 23444,  Learning Rate: 0.00009, Train Gradient: 159.0\n",
      "Epoch 97201/150000, Train Loss: 23020, Val Loss: 23438,  Learning Rate: 0.00009, Train Gradient: 159.0\n",
      "Epoch 97301/150000, Train Loss: 23014, Val Loss: 23432,  Learning Rate: 0.00009, Train Gradient: 158.9\n",
      "Epoch 97401/150000, Train Loss: 23008, Val Loss: 23426,  Learning Rate: 0.00009, Train Gradient: 158.8\n",
      "Epoch 97501/150000, Train Loss: 23003, Val Loss: 23420,  Learning Rate: 0.00009, Train Gradient: 158.8\n",
      "Epoch 97601/150000, Train Loss: 22997, Val Loss: 23414,  Learning Rate: 0.00009, Train Gradient: 158.7\n",
      "Epoch 97701/150000, Train Loss: 22991, Val Loss: 23408,  Learning Rate: 0.00009, Train Gradient: 158.7\n",
      "Epoch 97801/150000, Train Loss: 22985, Val Loss: 23402,  Learning Rate: 0.00009, Train Gradient: 158.6\n",
      "Epoch 97901/150000, Train Loss: 22979, Val Loss: 23396,  Learning Rate: 0.00009, Train Gradient: 158.6\n",
      "Epoch 98001/150000, Train Loss: 22973, Val Loss: 23390,  Learning Rate: 0.00009, Train Gradient: 158.5\n",
      "Epoch 98101/150000, Train Loss: 22967, Val Loss: 23384,  Learning Rate: 0.00009, Train Gradient: 158.5\n",
      "Epoch 98201/150000, Train Loss: 22961, Val Loss: 23378,  Learning Rate: 0.00009, Train Gradient: 158.4\n",
      "Epoch 98301/150000, Train Loss: 22955, Val Loss: 23372,  Learning Rate: 0.00009, Train Gradient: 158.4\n",
      "Epoch 98401/150000, Train Loss: 22949, Val Loss: 23366,  Learning Rate: 0.00009, Train Gradient: 158.3\n",
      "Epoch 98501/150000, Train Loss: 22943, Val Loss: 23360,  Learning Rate: 0.00009, Train Gradient: 158.3\n",
      "Epoch 98601/150000, Train Loss: 22937, Val Loss: 23354,  Learning Rate: 0.00009, Train Gradient: 158.2\n",
      "Epoch 98701/150000, Train Loss: 22931, Val Loss: 23348,  Learning Rate: 0.00009, Train Gradient: 158.2\n",
      "Epoch 98801/150000, Train Loss: 22926, Val Loss: 23342,  Learning Rate: 0.00009, Train Gradient: 158.1\n",
      "Epoch 98901/150000, Train Loss: 22920, Val Loss: 23336,  Learning Rate: 0.00009, Train Gradient: 158.1\n",
      "Epoch 99001/150000, Train Loss: 22914, Val Loss: 23330,  Learning Rate: 0.00009, Train Gradient: 158.0\n",
      "Epoch 99101/150000, Train Loss: 22908, Val Loss: 23324,  Learning Rate: 0.00009, Train Gradient: 158.0\n",
      "Epoch 99201/150000, Train Loss: 22902, Val Loss: 23318,  Learning Rate: 0.00009, Train Gradient: 157.9\n",
      "Epoch 99301/150000, Train Loss: 22896, Val Loss: 23312,  Learning Rate: 0.00009, Train Gradient: 157.9\n",
      "Epoch 99401/150000, Train Loss: 22890, Val Loss: 23306,  Learning Rate: 0.00009, Train Gradient: 157.8\n",
      "Epoch 99501/150000, Train Loss: 22884, Val Loss: 23300,  Learning Rate: 0.00009, Train Gradient: 157.8\n",
      "Epoch 99601/150000, Train Loss: 22878, Val Loss: 23294,  Learning Rate: 0.00009, Train Gradient: 157.7\n",
      "Epoch 99701/150000, Train Loss: 22872, Val Loss: 23288,  Learning Rate: 0.00009, Train Gradient: 157.7\n",
      "Epoch 99801/150000, Train Loss: 22867, Val Loss: 23282,  Learning Rate: 0.00009, Train Gradient: 157.6\n",
      "Epoch 99901/150000, Train Loss: 22861, Val Loss: 23276,  Learning Rate: 0.00009, Train Gradient: 157.6\n",
      "Epoch 100001/150000, Train Loss: 22855, Val Loss: 23270,  Learning Rate: 0.00009, Train Gradient: 157.5\n",
      "Epoch 100101/150000, Train Loss: 22849, Val Loss: 23264,  Learning Rate: 0.00009, Train Gradient: 157.5\n",
      "Epoch 100201/150000, Train Loss: 22843, Val Loss: 23258,  Learning Rate: 0.00009, Train Gradient: 157.5\n",
      "Epoch 100301/150000, Train Loss: 22837, Val Loss: 23252,  Learning Rate: 0.00009, Train Gradient: 157.4\n",
      "Epoch 100401/150000, Train Loss: 22831, Val Loss: 23247,  Learning Rate: 0.00009, Train Gradient: 157.4\n",
      "Epoch 100501/150000, Train Loss: 22825, Val Loss: 23241,  Learning Rate: 0.00009, Train Gradient: 157.3\n",
      "Epoch 100601/150000, Train Loss: 22819, Val Loss: 23235,  Learning Rate: 0.00009, Train Gradient: 157.3\n",
      "Epoch 100701/150000, Train Loss: 22814, Val Loss: 23229,  Learning Rate: 0.00009, Train Gradient: 157.2\n",
      "Epoch 100801/150000, Train Loss: 22808, Val Loss: 23223,  Learning Rate: 0.00009, Train Gradient: 157.2\n",
      "Epoch 100901/150000, Train Loss: 22802, Val Loss: 23217,  Learning Rate: 0.00009, Train Gradient: 157.1\n",
      "Epoch 101001/150000, Train Loss: 22796, Val Loss: 23211,  Learning Rate: 0.00009, Train Gradient: 157.1\n",
      "Epoch 101101/150000, Train Loss: 22790, Val Loss: 23205,  Learning Rate: 0.00009, Train Gradient: 157.0\n",
      "Epoch 101201/150000, Train Loss: 22784, Val Loss: 23199,  Learning Rate: 0.00009, Train Gradient: 157.0\n",
      "Epoch 101301/150000, Train Loss: 22778, Val Loss: 23193,  Learning Rate: 0.00009, Train Gradient: 156.9\n",
      "Epoch 101401/150000, Train Loss: 22772, Val Loss: 23187,  Learning Rate: 0.00009, Train Gradient: 156.9\n",
      "Epoch 101501/150000, Train Loss: 22767, Val Loss: 23181,  Learning Rate: 0.00009, Train Gradient: 156.8\n",
      "Epoch 101601/150000, Train Loss: 22761, Val Loss: 23176,  Learning Rate: 0.00009, Train Gradient: 156.8\n",
      "Epoch 101701/150000, Train Loss: 22755, Val Loss: 23170,  Learning Rate: 0.00009, Train Gradient: 156.7\n",
      "Epoch 101801/150000, Train Loss: 22749, Val Loss: 23163,  Learning Rate: 0.00009, Train Gradient: 156.7\n",
      "Epoch 101901/150000, Train Loss: 22743, Val Loss: 23157,  Learning Rate: 0.00009, Train Gradient: 156.6\n",
      "Epoch 102001/150000, Train Loss: 22737, Val Loss: 23151,  Learning Rate: 0.00009, Train Gradient: 156.6\n",
      "Epoch 102101/150000, Train Loss: 22731, Val Loss: 23145,  Learning Rate: 0.00009, Train Gradient: 156.5\n",
      "Epoch 102201/150000, Train Loss: 22725, Val Loss: 23140,  Learning Rate: 0.00009, Train Gradient: 156.5\n",
      "Epoch 102301/150000, Train Loss: 22719, Val Loss: 23134,  Learning Rate: 0.00009, Train Gradient: 156.4\n",
      "Epoch 102401/150000, Train Loss: 22713, Val Loss: 23128,  Learning Rate: 0.00009, Train Gradient: 156.4\n",
      "Epoch 102501/150000, Train Loss: 22707, Val Loss: 23122,  Learning Rate: 0.00009, Train Gradient: 156.3\n",
      "Epoch 102601/150000, Train Loss: 22702, Val Loss: 23116,  Learning Rate: 0.00009, Train Gradient: 156.3\n",
      "Epoch 102701/150000, Train Loss: 22696, Val Loss: 23110,  Learning Rate: 0.00009, Train Gradient: 156.2\n",
      "Epoch 102801/150000, Train Loss: 22690, Val Loss: 23104,  Learning Rate: 0.00009, Train Gradient: 156.2\n",
      "Epoch 102901/150000, Train Loss: 22684, Val Loss: 23098,  Learning Rate: 0.00009, Train Gradient: 156.2\n",
      "Epoch 103001/150000, Train Loss: 22678, Val Loss: 23092,  Learning Rate: 0.00009, Train Gradient: 156.1\n",
      "Epoch 103101/150000, Train Loss: 22672, Val Loss: 23086,  Learning Rate: 0.00009, Train Gradient: 156.1\n",
      "Epoch 103201/150000, Train Loss: 22666, Val Loss: 23080,  Learning Rate: 0.00009, Train Gradient: 156.0\n",
      "Epoch 103301/150000, Train Loss: 22660, Val Loss: 23074,  Learning Rate: 0.00009, Train Gradient: 156.0\n",
      "Epoch 103401/150000, Train Loss: 22654, Val Loss: 23068,  Learning Rate: 0.00009, Train Gradient: 155.9\n",
      "Epoch 103501/150000, Train Loss: 22648, Val Loss: 23062,  Learning Rate: 0.00009, Train Gradient: 155.9\n",
      "Epoch 103601/150000, Train Loss: 22643, Val Loss: 23056,  Learning Rate: 0.00009, Train Gradient: 155.8\n",
      "Epoch 103701/150000, Train Loss: 22637, Val Loss: 23050,  Learning Rate: 0.00009, Train Gradient: 155.8\n",
      "Epoch 103801/150000, Train Loss: 22631, Val Loss: 23044,  Learning Rate: 0.00009, Train Gradient: 155.7\n",
      "Epoch 103901/150000, Train Loss: 22625, Val Loss: 23038,  Learning Rate: 0.00009, Train Gradient: 155.7\n",
      "Epoch 104001/150000, Train Loss: 22619, Val Loss: 23032,  Learning Rate: 0.00009, Train Gradient: 155.6\n",
      "Epoch 104101/150000, Train Loss: 22613, Val Loss: 23026,  Learning Rate: 0.00009, Train Gradient: 155.6\n",
      "Epoch 104201/150000, Train Loss: 22607, Val Loss: 23020,  Learning Rate: 0.00009, Train Gradient: 155.5\n",
      "Epoch 104301/150000, Train Loss: 22601, Val Loss: 23014,  Learning Rate: 0.00009, Train Gradient: 155.5\n",
      "Epoch 104401/150000, Train Loss: 22595, Val Loss: 23008,  Learning Rate: 0.00009, Train Gradient: 155.4\n",
      "Epoch 104501/150000, Train Loss: 22590, Val Loss: 23002,  Learning Rate: 0.00009, Train Gradient: 155.4\n",
      "Epoch 104601/150000, Train Loss: 22584, Val Loss: 22997,  Learning Rate: 0.00009, Train Gradient: 155.3\n",
      "Epoch 104701/150000, Train Loss: 22578, Val Loss: 22991,  Learning Rate: 0.00009, Train Gradient: 155.3\n",
      "Epoch 104801/150000, Train Loss: 22572, Val Loss: 22985,  Learning Rate: 0.00009, Train Gradient: 155.2\n",
      "Epoch 104901/150000, Train Loss: 22566, Val Loss: 22979,  Learning Rate: 0.00009, Train Gradient: 155.2\n",
      "Epoch 105001/150000, Train Loss: 22560, Val Loss: 22973,  Learning Rate: 0.00009, Train Gradient: 155.1\n",
      "Epoch 105101/150000, Train Loss: 22554, Val Loss: 22967,  Learning Rate: 0.00009, Train Gradient: 155.1\n",
      "Epoch 105201/150000, Train Loss: 22549, Val Loss: 22961,  Learning Rate: 0.00009, Train Gradient: 155.0\n",
      "Epoch 105301/150000, Train Loss: 22543, Val Loss: 22955,  Learning Rate: 0.00009, Train Gradient: 155.0\n",
      "Epoch 105401/150000, Train Loss: 22537, Val Loss: 22949,  Learning Rate: 0.00009, Train Gradient: 154.9\n",
      "Epoch 105501/150000, Train Loss: 22531, Val Loss: 22943,  Learning Rate: 0.00009, Train Gradient: 154.9\n",
      "Epoch 105601/150000, Train Loss: 22525, Val Loss: 22937,  Learning Rate: 0.00009, Train Gradient: 154.9\n",
      "Epoch 105701/150000, Train Loss: 22519, Val Loss: 22932,  Learning Rate: 0.00009, Train Gradient: 154.8\n",
      "Epoch 105801/150000, Train Loss: 22513, Val Loss: 22925,  Learning Rate: 0.00009, Train Gradient: 154.8\n",
      "Epoch 105901/150000, Train Loss: 22508, Val Loss: 22920,  Learning Rate: 0.00009, Train Gradient: 154.7\n",
      "Epoch 106001/150000, Train Loss: 22502, Val Loss: 22913,  Learning Rate: 0.00009, Train Gradient: 154.7\n",
      "Epoch 106101/150000, Train Loss: 22496, Val Loss: 22907,  Learning Rate: 0.00009, Train Gradient: 154.7\n",
      "Epoch 106201/150000, Train Loss: 22490, Val Loss: 22901,  Learning Rate: 0.00009, Train Gradient: 154.6\n",
      "Epoch 106301/150000, Train Loss: 22484, Val Loss: 22895,  Learning Rate: 0.00009, Train Gradient: 154.6\n",
      "Epoch 106401/150000, Train Loss: 22478, Val Loss: 22889,  Learning Rate: 0.00009, Train Gradient: 154.6\n",
      "Epoch 106501/150000, Train Loss: 22472, Val Loss: 22883,  Learning Rate: 0.00009, Train Gradient: 154.5\n",
      "Epoch 106601/150000, Train Loss: 22467, Val Loss: 22877,  Learning Rate: 0.00009, Train Gradient: 154.5\n",
      "Epoch 106701/150000, Train Loss: 22461, Val Loss: 22871,  Learning Rate: 0.00009, Train Gradient: 154.4\n",
      "Epoch 106801/150000, Train Loss: 22455, Val Loss: 22865,  Learning Rate: 0.00009, Train Gradient: 154.4\n",
      "Epoch 106901/150000, Train Loss: 22449, Val Loss: 22859,  Learning Rate: 0.00009, Train Gradient: 154.3\n",
      "Epoch 107001/150000, Train Loss: 22443, Val Loss: 22853,  Learning Rate: 0.00009, Train Gradient: 154.3\n",
      "Epoch 107101/150000, Train Loss: 22437, Val Loss: 22847,  Learning Rate: 0.00009, Train Gradient: 154.2\n",
      "Epoch 107201/150000, Train Loss: 22431, Val Loss: 22841,  Learning Rate: 0.00009, Train Gradient: 154.2\n",
      "Epoch 107301/150000, Train Loss: 22426, Val Loss: 22835,  Learning Rate: 0.00009, Train Gradient: 154.2\n",
      "Epoch 107401/150000, Train Loss: 22420, Val Loss: 22828,  Learning Rate: 0.00009, Train Gradient: 154.1\n",
      "Epoch 107501/150000, Train Loss: 22414, Val Loss: 22823,  Learning Rate: 0.00009, Train Gradient: 154.1\n",
      "Epoch 107601/150000, Train Loss: 22408, Val Loss: 22817,  Learning Rate: 0.00009, Train Gradient: 154.0\n",
      "Epoch 107701/150000, Train Loss: 22402, Val Loss: 22811,  Learning Rate: 0.00009, Train Gradient: 154.0\n",
      "Epoch 107801/150000, Train Loss: 22396, Val Loss: 22805,  Learning Rate: 0.00009, Train Gradient: 153.9\n",
      "Epoch 107901/150000, Train Loss: 22391, Val Loss: 22799,  Learning Rate: 0.00009, Train Gradient: 153.9\n",
      "Epoch 108001/150000, Train Loss: 22385, Val Loss: 22793,  Learning Rate: 0.00009, Train Gradient: 153.8\n",
      "Epoch 108101/150000, Train Loss: 22379, Val Loss: 22787,  Learning Rate: 0.00009, Train Gradient: 153.8\n",
      "Epoch 108201/150000, Train Loss: 22373, Val Loss: 22781,  Learning Rate: 0.00009, Train Gradient: 153.7\n",
      "Epoch 108301/150000, Train Loss: 22367, Val Loss: 22775,  Learning Rate: 0.00009, Train Gradient: 153.7\n",
      "Epoch 108401/150000, Train Loss: 22362, Val Loss: 22770,  Learning Rate: 0.00009, Train Gradient: 153.7\n",
      "Epoch 108501/150000, Train Loss: 22356, Val Loss: 22764,  Learning Rate: 0.00009, Train Gradient: 153.6\n",
      "Epoch 108601/150000, Train Loss: 22350, Val Loss: 22758,  Learning Rate: 0.00009, Train Gradient: 153.6\n",
      "Epoch 108701/150000, Train Loss: 22344, Val Loss: 22752,  Learning Rate: 0.00009, Train Gradient: 153.5\n",
      "Epoch 108801/150000, Train Loss: 22338, Val Loss: 22746,  Learning Rate: 0.00009, Train Gradient: 153.5\n",
      "Epoch 108901/150000, Train Loss: 22333, Val Loss: 22740,  Learning Rate: 0.00009, Train Gradient: 153.4\n",
      "Epoch 109001/150000, Train Loss: 22327, Val Loss: 22734,  Learning Rate: 0.00009, Train Gradient: 153.4\n",
      "Epoch 109101/150000, Train Loss: 22321, Val Loss: 22728,  Learning Rate: 0.00009, Train Gradient: 153.4\n",
      "Epoch 109201/150000, Train Loss: 22315, Val Loss: 22723,  Learning Rate: 0.00009, Train Gradient: 153.3\n",
      "Epoch 109301/150000, Train Loss: 22309, Val Loss: 22717,  Learning Rate: 0.00009, Train Gradient: 153.3\n",
      "Epoch 109401/150000, Train Loss: 22304, Val Loss: 22711,  Learning Rate: 0.00009, Train Gradient: 153.2\n",
      "Epoch 109501/150000, Train Loss: 22298, Val Loss: 22705,  Learning Rate: 0.00009, Train Gradient: 153.2\n",
      "Epoch 109601/150000, Train Loss: 22292, Val Loss: 22699,  Learning Rate: 0.00009, Train Gradient: 153.1\n",
      "Epoch 109701/150000, Train Loss: 22286, Val Loss: 22693,  Learning Rate: 0.00009, Train Gradient: 153.1\n",
      "Epoch 109801/150000, Train Loss: 22280, Val Loss: 22688,  Learning Rate: 0.00009, Train Gradient: 153.0\n",
      "Epoch 109901/150000, Train Loss: 22275, Val Loss: 22682,  Learning Rate: 0.00009, Train Gradient: 153.0\n",
      "Epoch 110001/150000, Train Loss: 22269, Val Loss: 22676,  Learning Rate: 0.00009, Train Gradient: 152.9\n",
      "Epoch 110101/150000, Train Loss: 22263, Val Loss: 22670,  Learning Rate: 0.00009, Train Gradient: 152.9\n",
      "Epoch 110201/150000, Train Loss: 22257, Val Loss: 22664,  Learning Rate: 0.00009, Train Gradient: 152.9\n",
      "Epoch 110301/150000, Train Loss: 22252, Val Loss: 22659,  Learning Rate: 0.00009, Train Gradient: 152.8\n",
      "Epoch 110401/150000, Train Loss: 22246, Val Loss: 22653,  Learning Rate: 0.00009, Train Gradient: 152.8\n",
      "Epoch 110501/150000, Train Loss: 22240, Val Loss: 22647,  Learning Rate: 0.00009, Train Gradient: 152.7\n",
      "Epoch 110601/150000, Train Loss: 22234, Val Loss: 22641,  Learning Rate: 0.00009, Train Gradient: 152.7\n",
      "Epoch 110701/150000, Train Loss: 22228, Val Loss: 22635,  Learning Rate: 0.00009, Train Gradient: 152.6\n",
      "Epoch 110801/150000, Train Loss: 22223, Val Loss: 22629,  Learning Rate: 0.00009, Train Gradient: 152.6\n",
      "Epoch 110901/150000, Train Loss: 22217, Val Loss: 22624,  Learning Rate: 0.00009, Train Gradient: 152.6\n",
      "Epoch 111001/150000, Train Loss: 22211, Val Loss: 22618,  Learning Rate: 0.00009, Train Gradient: 152.5\n",
      "Epoch 111101/150000, Train Loss: 22205, Val Loss: 22612,  Learning Rate: 0.00009, Train Gradient: 152.5\n",
      "Epoch 111201/150000, Train Loss: 22200, Val Loss: 22606,  Learning Rate: 0.00009, Train Gradient: 152.4\n",
      "Epoch 111301/150000, Train Loss: 22194, Val Loss: 22600,  Learning Rate: 0.00009, Train Gradient: 152.4\n",
      "Epoch 111401/150000, Train Loss: 22188, Val Loss: 22595,  Learning Rate: 0.00009, Train Gradient: 152.3\n",
      "Epoch 111501/150000, Train Loss: 22182, Val Loss: 22589,  Learning Rate: 0.00009, Train Gradient: 152.3\n",
      "Epoch 111601/150000, Train Loss: 22177, Val Loss: 22583,  Learning Rate: 0.00009, Train Gradient: 152.3\n",
      "Epoch 111701/150000, Train Loss: 22171, Val Loss: 22577,  Learning Rate: 0.00009, Train Gradient: 152.2\n",
      "Epoch 111801/150000, Train Loss: 22165, Val Loss: 22571,  Learning Rate: 0.00009, Train Gradient: 152.2\n",
      "Epoch 111901/150000, Train Loss: 22159, Val Loss: 22566,  Learning Rate: 0.00009, Train Gradient: 152.1\n",
      "Epoch 112001/150000, Train Loss: 22154, Val Loss: 22560,  Learning Rate: 0.00009, Train Gradient: 152.1\n",
      "Epoch 112101/150000, Train Loss: 22148, Val Loss: 22554,  Learning Rate: 0.00009, Train Gradient: 152.0\n",
      "Epoch 112201/150000, Train Loss: 22142, Val Loss: 22548,  Learning Rate: 0.00009, Train Gradient: 152.0\n",
      "Epoch 112301/150000, Train Loss: 22136, Val Loss: 22542,  Learning Rate: 0.00009, Train Gradient: 151.9\n",
      "Epoch 112401/150000, Train Loss: 22131, Val Loss: 22537,  Learning Rate: 0.00009, Train Gradient: 151.9\n",
      "Epoch 112501/150000, Train Loss: 22125, Val Loss: 22531,  Learning Rate: 0.00009, Train Gradient: 151.9\n",
      "Epoch 112601/150000, Train Loss: 22119, Val Loss: 22525,  Learning Rate: 0.00009, Train Gradient: 151.8\n",
      "Epoch 112701/150000, Train Loss: 22113, Val Loss: 22519,  Learning Rate: 0.00009, Train Gradient: 151.8\n",
      "Epoch 112801/150000, Train Loss: 22108, Val Loss: 22514,  Learning Rate: 0.00009, Train Gradient: 151.7\n",
      "Epoch 112901/150000, Train Loss: 22102, Val Loss: 22508,  Learning Rate: 0.00009, Train Gradient: 151.7\n",
      "Epoch 113001/150000, Train Loss: 22096, Val Loss: 22502,  Learning Rate: 0.00009, Train Gradient: 151.6\n",
      "Epoch 113101/150000, Train Loss: 22091, Val Loss: 22496,  Learning Rate: 0.00009, Train Gradient: 151.6\n",
      "Epoch 113201/150000, Train Loss: 22085, Val Loss: 22490,  Learning Rate: 0.00009, Train Gradient: 151.6\n",
      "Epoch 113301/150000, Train Loss: 22079, Val Loss: 22485,  Learning Rate: 0.00009, Train Gradient: 151.5\n",
      "Epoch 113401/150000, Train Loss: 22073, Val Loss: 22479,  Learning Rate: 0.00009, Train Gradient: 151.5\n",
      "Epoch 113501/150000, Train Loss: 22068, Val Loss: 22473,  Learning Rate: 0.00009, Train Gradient: 151.4\n",
      "Epoch 113601/150000, Train Loss: 22062, Val Loss: 22467,  Learning Rate: 0.00009, Train Gradient: 151.4\n",
      "Epoch 113701/150000, Train Loss: 22056, Val Loss: 22462,  Learning Rate: 0.00009, Train Gradient: 151.3\n",
      "Epoch 113801/150000, Train Loss: 22050, Val Loss: 22456,  Learning Rate: 0.00009, Train Gradient: 151.3\n",
      "Epoch 113901/150000, Train Loss: 22045, Val Loss: 22450,  Learning Rate: 0.00009, Train Gradient: 151.2\n",
      "Epoch 114001/150000, Train Loss: 22039, Val Loss: 22444,  Learning Rate: 0.00009, Train Gradient: 151.2\n",
      "Epoch 114101/150000, Train Loss: 22033, Val Loss: 22438,  Learning Rate: 0.00009, Train Gradient: 151.2\n",
      "Epoch 114201/150000, Train Loss: 22028, Val Loss: 22433,  Learning Rate: 0.00009, Train Gradient: 151.1\n",
      "Epoch 114301/150000, Train Loss: 22022, Val Loss: 22427,  Learning Rate: 0.00009, Train Gradient: 151.1\n",
      "Epoch 114401/150000, Train Loss: 22016, Val Loss: 22421,  Learning Rate: 0.00009, Train Gradient: 151.0\n",
      "Epoch 114501/150000, Train Loss: 22010, Val Loss: 22415,  Learning Rate: 0.00009, Train Gradient: 151.0\n",
      "Epoch 114601/150000, Train Loss: 22005, Val Loss: 22409,  Learning Rate: 0.00009, Train Gradient: 150.9\n",
      "Epoch 114701/150000, Train Loss: 21999, Val Loss: 22404,  Learning Rate: 0.00009, Train Gradient: 150.9\n",
      "Epoch 114801/150000, Train Loss: 21993, Val Loss: 22398,  Learning Rate: 0.00009, Train Gradient: 150.9\n",
      "Epoch 114901/150000, Train Loss: 21988, Val Loss: 22392,  Learning Rate: 0.00009, Train Gradient: 150.8\n",
      "Epoch 115001/150000, Train Loss: 21982, Val Loss: 22386,  Learning Rate: 0.00009, Train Gradient: 150.8\n",
      "Epoch 115101/150000, Train Loss: 21976, Val Loss: 22381,  Learning Rate: 0.00009, Train Gradient: 150.7\n",
      "Epoch 115201/150000, Train Loss: 21971, Val Loss: 22375,  Learning Rate: 0.00009, Train Gradient: 150.7\n",
      "Epoch 115301/150000, Train Loss: 21965, Val Loss: 22369,  Learning Rate: 0.00009, Train Gradient: 150.7\n",
      "Epoch 115401/150000, Train Loss: 21959, Val Loss: 22363,  Learning Rate: 0.00009, Train Gradient: 150.6\n",
      "Epoch 115501/150000, Train Loss: 21954, Val Loss: 22358,  Learning Rate: 0.00009, Train Gradient: 150.6\n",
      "Epoch 115601/150000, Train Loss: 21948, Val Loss: 22352,  Learning Rate: 0.00009, Train Gradient: 150.5\n",
      "Epoch 115701/150000, Train Loss: 21942, Val Loss: 22346,  Learning Rate: 0.00009, Train Gradient: 150.5\n",
      "Epoch 115801/150000, Train Loss: 21936, Val Loss: 22340,  Learning Rate: 0.00009, Train Gradient: 150.4\n",
      "Epoch 115901/150000, Train Loss: 21931, Val Loss: 22335,  Learning Rate: 0.00009, Train Gradient: 150.4\n",
      "Epoch 116001/150000, Train Loss: 21925, Val Loss: 22329,  Learning Rate: 0.00009, Train Gradient: 150.4\n",
      "Epoch 116101/150000, Train Loss: 21919, Val Loss: 22323,  Learning Rate: 0.00009, Train Gradient: 150.3\n",
      "Epoch 116201/150000, Train Loss: 21914, Val Loss: 22317,  Learning Rate: 0.00009, Train Gradient: 150.3\n",
      "Epoch 116301/150000, Train Loss: 21908, Val Loss: 22312,  Learning Rate: 0.00009, Train Gradient: 150.2\n",
      "Epoch 116401/150000, Train Loss: 21902, Val Loss: 22306,  Learning Rate: 0.00009, Train Gradient: 150.2\n",
      "Epoch 116501/150000, Train Loss: 21897, Val Loss: 22300,  Learning Rate: 0.00009, Train Gradient: 150.1\n",
      "Epoch 116601/150000, Train Loss: 21891, Val Loss: 22294,  Learning Rate: 0.00009, Train Gradient: 150.1\n",
      "Epoch 116701/150000, Train Loss: 21885, Val Loss: 22288,  Learning Rate: 0.00009, Train Gradient: 150.1\n",
      "Epoch 116801/150000, Train Loss: 21880, Val Loss: 22283,  Learning Rate: 0.00009, Train Gradient: 150.0\n",
      "Epoch 116901/150000, Train Loss: 21874, Val Loss: 22277,  Learning Rate: 0.00009, Train Gradient: 150.0\n",
      "Epoch 117001/150000, Train Loss: 21868, Val Loss: 22271,  Learning Rate: 0.00009, Train Gradient: 149.9\n",
      "Epoch 117101/150000, Train Loss: 21863, Val Loss: 22265,  Learning Rate: 0.00009, Train Gradient: 149.9\n",
      "Epoch 117201/150000, Train Loss: 21857, Val Loss: 22260,  Learning Rate: 0.00009, Train Gradient: 149.9\n",
      "Epoch 117301/150000, Train Loss: 21851, Val Loss: 22254,  Learning Rate: 0.00009, Train Gradient: 149.8\n",
      "Epoch 117401/150000, Train Loss: 21846, Val Loss: 22248,  Learning Rate: 0.00009, Train Gradient: 149.8\n",
      "Epoch 117501/150000, Train Loss: 21840, Val Loss: 22242,  Learning Rate: 0.00009, Train Gradient: 149.7\n",
      "Epoch 117601/150000, Train Loss: 21834, Val Loss: 22237,  Learning Rate: 0.00009, Train Gradient: 149.7\n",
      "Epoch 117701/150000, Train Loss: 21829, Val Loss: 22231,  Learning Rate: 0.00009, Train Gradient: 149.6\n",
      "Epoch 117801/150000, Train Loss: 21823, Val Loss: 22225,  Learning Rate: 0.00009, Train Gradient: 149.6\n",
      "Epoch 117901/150000, Train Loss: 21817, Val Loss: 22220,  Learning Rate: 0.00009, Train Gradient: 149.6\n",
      "Epoch 118001/150000, Train Loss: 21812, Val Loss: 22214,  Learning Rate: 0.00009, Train Gradient: 149.5\n",
      "Epoch 118101/150000, Train Loss: 21806, Val Loss: 22208,  Learning Rate: 0.00009, Train Gradient: 149.5\n",
      "Epoch 118201/150000, Train Loss: 21801, Val Loss: 22202,  Learning Rate: 0.00009, Train Gradient: 149.4\n",
      "Epoch 118301/150000, Train Loss: 21795, Val Loss: 22197,  Learning Rate: 0.00009, Train Gradient: 149.4\n",
      "Epoch 118401/150000, Train Loss: 21789, Val Loss: 22191,  Learning Rate: 0.00009, Train Gradient: 149.4\n",
      "Epoch 118501/150000, Train Loss: 21784, Val Loss: 22185,  Learning Rate: 0.00009, Train Gradient: 149.3\n",
      "Epoch 118601/150000, Train Loss: 21778, Val Loss: 22179,  Learning Rate: 0.00009, Train Gradient: 149.3\n",
      "Epoch 118701/150000, Train Loss: 21772, Val Loss: 22174,  Learning Rate: 0.00009, Train Gradient: 149.2\n",
      "Epoch 118801/150000, Train Loss: 21767, Val Loss: 22168,  Learning Rate: 0.00009, Train Gradient: 149.2\n",
      "Epoch 118901/150000, Train Loss: 21761, Val Loss: 22162,  Learning Rate: 0.00009, Train Gradient: 149.1\n",
      "Epoch 119001/150000, Train Loss: 21755, Val Loss: 22157,  Learning Rate: 0.00009, Train Gradient: 149.1\n",
      "Epoch 119101/150000, Train Loss: 21750, Val Loss: 22151,  Learning Rate: 0.00009, Train Gradient: 149.1\n",
      "Epoch 119201/150000, Train Loss: 21744, Val Loss: 22145,  Learning Rate: 0.00009, Train Gradient: 149.0\n",
      "Epoch 119301/150000, Train Loss: 21739, Val Loss: 22140,  Learning Rate: 0.00009, Train Gradient: 149.0\n",
      "Epoch 119401/150000, Train Loss: 21733, Val Loss: 22134,  Learning Rate: 0.00009, Train Gradient: 148.9\n",
      "Epoch 119501/150000, Train Loss: 21727, Val Loss: 22128,  Learning Rate: 0.00009, Train Gradient: 148.9\n",
      "Epoch 119601/150000, Train Loss: 21722, Val Loss: 22122,  Learning Rate: 0.00009, Train Gradient: 148.9\n",
      "Epoch 119701/150000, Train Loss: 21716, Val Loss: 22117,  Learning Rate: 0.00009, Train Gradient: 148.8\n",
      "Epoch 119801/150000, Train Loss: 21710, Val Loss: 22111,  Learning Rate: 0.00009, Train Gradient: 148.8\n",
      "Epoch 119901/150000, Train Loss: 21705, Val Loss: 22105,  Learning Rate: 0.00009, Train Gradient: 148.7\n",
      "Epoch 120001/150000, Train Loss: 21699, Val Loss: 22100,  Learning Rate: 0.00009, Train Gradient: 148.7\n",
      "Epoch 120101/150000, Train Loss: 21694, Val Loss: 22094,  Learning Rate: 0.00009, Train Gradient: 148.6\n",
      "Epoch 120201/150000, Train Loss: 21688, Val Loss: 22088,  Learning Rate: 0.00009, Train Gradient: 148.6\n",
      "Epoch 120301/150000, Train Loss: 21682, Val Loss: 22083,  Learning Rate: 0.00009, Train Gradient: 148.6\n",
      "Epoch 120401/150000, Train Loss: 21677, Val Loss: 22077,  Learning Rate: 0.00009, Train Gradient: 148.5\n",
      "Epoch 120501/150000, Train Loss: 21671, Val Loss: 22071,  Learning Rate: 0.00009, Train Gradient: 148.5\n",
      "Epoch 120601/150000, Train Loss: 21665, Val Loss: 22065,  Learning Rate: 0.00009, Train Gradient: 148.4\n",
      "Epoch 120701/150000, Train Loss: 21660, Val Loss: 22060,  Learning Rate: 0.00009, Train Gradient: 148.4\n",
      "Epoch 120801/150000, Train Loss: 21654, Val Loss: 22054,  Learning Rate: 0.00009, Train Gradient: 148.4\n",
      "Epoch 120901/150000, Train Loss: 21649, Val Loss: 22048,  Learning Rate: 0.00009, Train Gradient: 148.3\n",
      "Epoch 121001/150000, Train Loss: 21643, Val Loss: 22043,  Learning Rate: 0.00009, Train Gradient: 148.3\n",
      "Epoch 121101/150000, Train Loss: 21637, Val Loss: 22037,  Learning Rate: 0.00009, Train Gradient: 148.2\n",
      "Epoch 121201/150000, Train Loss: 21632, Val Loss: 22031,  Learning Rate: 0.00009, Train Gradient: 148.2\n",
      "Epoch 121301/150000, Train Loss: 21626, Val Loss: 22026,  Learning Rate: 0.00009, Train Gradient: 148.2\n",
      "Epoch 121401/150000, Train Loss: 21621, Val Loss: 22020,  Learning Rate: 0.00009, Train Gradient: 148.1\n",
      "Epoch 121501/150000, Train Loss: 21615, Val Loss: 22014,  Learning Rate: 0.00009, Train Gradient: 148.1\n",
      "Epoch 121601/150000, Train Loss: 21610, Val Loss: 22009,  Learning Rate: 0.00009, Train Gradient: 148.0\n",
      "Epoch 121701/150000, Train Loss: 21604, Val Loss: 22003,  Learning Rate: 0.00009, Train Gradient: 148.0\n",
      "Epoch 121801/150000, Train Loss: 21598, Val Loss: 21997,  Learning Rate: 0.00009, Train Gradient: 148.0\n",
      "Epoch 121901/150000, Train Loss: 21593, Val Loss: 21992,  Learning Rate: 0.00009, Train Gradient: 147.9\n",
      "Epoch 122001/150000, Train Loss: 21587, Val Loss: 21986,  Learning Rate: 0.00009, Train Gradient: 147.9\n",
      "Epoch 122101/150000, Train Loss: 21582, Val Loss: 21980,  Learning Rate: 0.00009, Train Gradient: 147.8\n",
      "Epoch 122201/150000, Train Loss: 21576, Val Loss: 21975,  Learning Rate: 0.00009, Train Gradient: 147.8\n",
      "Epoch 122301/150000, Train Loss: 21570, Val Loss: 21969,  Learning Rate: 0.00009, Train Gradient: 147.8\n",
      "Epoch 122401/150000, Train Loss: 21565, Val Loss: 21963,  Learning Rate: 0.00009, Train Gradient: 147.7\n",
      "Epoch 122501/150000, Train Loss: 21559, Val Loss: 21958,  Learning Rate: 0.00009, Train Gradient: 147.7\n",
      "Epoch 122601/150000, Train Loss: 21554, Val Loss: 21952,  Learning Rate: 0.00009, Train Gradient: 147.6\n",
      "Epoch 122701/150000, Train Loss: 21548, Val Loss: 21946,  Learning Rate: 0.00009, Train Gradient: 147.6\n",
      "Epoch 122801/150000, Train Loss: 21543, Val Loss: 21941,  Learning Rate: 0.00009, Train Gradient: 147.5\n",
      "Epoch 122901/150000, Train Loss: 21537, Val Loss: 21935,  Learning Rate: 0.00009, Train Gradient: 147.5\n",
      "Epoch 123001/150000, Train Loss: 21531, Val Loss: 21930,  Learning Rate: 0.00009, Train Gradient: 147.5\n",
      "Epoch 123101/150000, Train Loss: 21526, Val Loss: 21924,  Learning Rate: 0.00009, Train Gradient: 147.4\n",
      "Epoch 123201/150000, Train Loss: 21520, Val Loss: 21918,  Learning Rate: 0.00009, Train Gradient: 147.4\n",
      "Epoch 123301/150000, Train Loss: 21515, Val Loss: 21913,  Learning Rate: 0.00009, Train Gradient: 147.3\n",
      "Epoch 123401/150000, Train Loss: 21509, Val Loss: 21907,  Learning Rate: 0.00009, Train Gradient: 147.3\n",
      "Epoch 123501/150000, Train Loss: 21504, Val Loss: 21901,  Learning Rate: 0.00009, Train Gradient: 147.3\n",
      "Epoch 123601/150000, Train Loss: 21498, Val Loss: 21896,  Learning Rate: 0.00009, Train Gradient: 147.2\n",
      "Epoch 123701/150000, Train Loss: 21492, Val Loss: 21890,  Learning Rate: 0.00009, Train Gradient: 147.2\n",
      "Epoch 123801/150000, Train Loss: 21487, Val Loss: 21884,  Learning Rate: 0.00009, Train Gradient: 147.1\n",
      "Epoch 123901/150000, Train Loss: 21481, Val Loss: 21879,  Learning Rate: 0.00009, Train Gradient: 147.1\n",
      "Epoch 124001/150000, Train Loss: 21476, Val Loss: 21873,  Learning Rate: 0.00009, Train Gradient: 147.1\n",
      "Epoch 124101/150000, Train Loss: 21470, Val Loss: 21868,  Learning Rate: 0.00009, Train Gradient: 147.0\n",
      "Epoch 124201/150000, Train Loss: 21465, Val Loss: 21862,  Learning Rate: 0.00009, Train Gradient: 147.0\n",
      "Epoch 124301/150000, Train Loss: 21459, Val Loss: 21856,  Learning Rate: 0.00009, Train Gradient: 146.9\n",
      "Epoch 124401/150000, Train Loss: 21454, Val Loss: 21851,  Learning Rate: 0.00009, Train Gradient: 146.9\n",
      "Epoch 124501/150000, Train Loss: 21448, Val Loss: 21845,  Learning Rate: 0.00009, Train Gradient: 146.9\n",
      "Epoch 124601/150000, Train Loss: 21442, Val Loss: 21839,  Learning Rate: 0.00009, Train Gradient: 146.8\n",
      "Epoch 124701/150000, Train Loss: 21437, Val Loss: 21834,  Learning Rate: 0.00009, Train Gradient: 146.8\n",
      "Epoch 124801/150000, Train Loss: 21431, Val Loss: 21828,  Learning Rate: 0.00009, Train Gradient: 146.7\n",
      "Epoch 124901/150000, Train Loss: 21426, Val Loss: 21823,  Learning Rate: 0.00009, Train Gradient: 146.7\n",
      "Epoch 125001/150000, Train Loss: 21420, Val Loss: 21817,  Learning Rate: 0.00009, Train Gradient: 146.7\n",
      "Epoch 125101/150000, Train Loss: 21415, Val Loss: 21811,  Learning Rate: 0.00009, Train Gradient: 146.6\n",
      "Epoch 125201/150000, Train Loss: 21409, Val Loss: 21806,  Learning Rate: 0.00009, Train Gradient: 146.6\n",
      "Epoch 125301/150000, Train Loss: 21404, Val Loss: 21800,  Learning Rate: 0.00009, Train Gradient: 146.5\n",
      "Epoch 125401/150000, Train Loss: 21398, Val Loss: 21794,  Learning Rate: 0.00009, Train Gradient: 146.5\n",
      "Epoch 125501/150000, Train Loss: 21393, Val Loss: 21789,  Learning Rate: 0.00009, Train Gradient: 146.5\n",
      "Epoch 125601/150000, Train Loss: 21387, Val Loss: 21783,  Learning Rate: 0.00009, Train Gradient: 146.4\n",
      "Epoch 125701/150000, Train Loss: 21382, Val Loss: 21778,  Learning Rate: 0.00009, Train Gradient: 146.4\n",
      "Epoch 125801/150000, Train Loss: 21376, Val Loss: 21772,  Learning Rate: 0.00009, Train Gradient: 146.3\n",
      "Epoch 125901/150000, Train Loss: 21370, Val Loss: 21766,  Learning Rate: 0.00009, Train Gradient: 146.3\n",
      "Epoch 126001/150000, Train Loss: 21365, Val Loss: 21761,  Learning Rate: 0.00009, Train Gradient: 146.3\n",
      "Epoch 126101/150000, Train Loss: 21359, Val Loss: 21755,  Learning Rate: 0.00009, Train Gradient: 146.2\n",
      "Epoch 126201/150000, Train Loss: 21354, Val Loss: 21750,  Learning Rate: 0.00009, Train Gradient: 146.2\n",
      "Epoch 126301/150000, Train Loss: 21348, Val Loss: 21744,  Learning Rate: 0.00009, Train Gradient: 146.1\n",
      "Epoch 126401/150000, Train Loss: 21343, Val Loss: 21738,  Learning Rate: 0.00009, Train Gradient: 146.1\n",
      "Epoch 126501/150000, Train Loss: 21337, Val Loss: 21733,  Learning Rate: 0.00009, Train Gradient: 146.1\n",
      "Epoch 126601/150000, Train Loss: 21332, Val Loss: 21727,  Learning Rate: 0.00009, Train Gradient: 146.0\n",
      "Epoch 126701/150000, Train Loss: 21326, Val Loss: 21722,  Learning Rate: 0.00009, Train Gradient: 146.0\n",
      "Epoch 126801/150000, Train Loss: 21321, Val Loss: 21716,  Learning Rate: 0.00009, Train Gradient: 145.9\n",
      "Epoch 126901/150000, Train Loss: 21315, Val Loss: 21710,  Learning Rate: 0.00009, Train Gradient: 145.9\n",
      "Epoch 127001/150000, Train Loss: 21310, Val Loss: 21705,  Learning Rate: 0.00009, Train Gradient: 145.9\n",
      "Epoch 127101/150000, Train Loss: 21304, Val Loss: 21699,  Learning Rate: 0.00009, Train Gradient: 145.8\n",
      "Epoch 127201/150000, Train Loss: 21299, Val Loss: 21694,  Learning Rate: 0.00009, Train Gradient: 145.8\n",
      "Epoch 127301/150000, Train Loss: 21293, Val Loss: 21688,  Learning Rate: 0.00009, Train Gradient: 145.8\n",
      "Epoch 127401/150000, Train Loss: 21288, Val Loss: 21682,  Learning Rate: 0.00009, Train Gradient: 145.7\n",
      "Epoch 127501/150000, Train Loss: 21282, Val Loss: 21677,  Learning Rate: 0.00009, Train Gradient: 145.7\n",
      "Epoch 127601/150000, Train Loss: 21277, Val Loss: 21671,  Learning Rate: 0.00009, Train Gradient: 145.6\n",
      "Epoch 127701/150000, Train Loss: 21271, Val Loss: 21666,  Learning Rate: 0.00009, Train Gradient: 145.6\n",
      "Epoch 127801/150000, Train Loss: 21266, Val Loss: 21660,  Learning Rate: 0.00009, Train Gradient: 145.6\n",
      "Epoch 127901/150000, Train Loss: 21260, Val Loss: 21655,  Learning Rate: 0.00009, Train Gradient: 145.5\n",
      "Epoch 128001/150000, Train Loss: 21255, Val Loss: 21649,  Learning Rate: 0.00009, Train Gradient: 145.5\n",
      "Epoch 128101/150000, Train Loss: 21249, Val Loss: 21643,  Learning Rate: 0.00009, Train Gradient: 145.4\n",
      "Epoch 128201/150000, Train Loss: 21244, Val Loss: 21638,  Learning Rate: 0.00009, Train Gradient: 145.4\n",
      "Epoch 128301/150000, Train Loss: 21238, Val Loss: 21632,  Learning Rate: 0.00009, Train Gradient: 145.4\n",
      "Epoch 128401/150000, Train Loss: 21233, Val Loss: 21627,  Learning Rate: 0.00009, Train Gradient: 145.3\n",
      "Epoch 128501/150000, Train Loss: 21227, Val Loss: 21621,  Learning Rate: 0.00009, Train Gradient: 145.3\n",
      "Epoch 128601/150000, Train Loss: 21222, Val Loss: 21616,  Learning Rate: 0.00009, Train Gradient: 145.2\n",
      "Epoch 128701/150000, Train Loss: 21216, Val Loss: 21610,  Learning Rate: 0.00009, Train Gradient: 145.2\n",
      "Epoch 128801/150000, Train Loss: 21211, Val Loss: 21604,  Learning Rate: 0.00009, Train Gradient: 145.2\n",
      "Epoch 128901/150000, Train Loss: 21205, Val Loss: 21599,  Learning Rate: 0.00009, Train Gradient: 145.1\n",
      "Epoch 129001/150000, Train Loss: 21200, Val Loss: 21593,  Learning Rate: 0.00009, Train Gradient: 145.1\n",
      "Epoch 129101/150000, Train Loss: 21194, Val Loss: 21588,  Learning Rate: 0.00009, Train Gradient: 145.0\n",
      "Epoch 129201/150000, Train Loss: 21189, Val Loss: 21582,  Learning Rate: 0.00009, Train Gradient: 145.0\n",
      "Epoch 129301/150000, Train Loss: 21183, Val Loss: 21577,  Learning Rate: 0.00009, Train Gradient: 145.0\n",
      "Epoch 129401/150000, Train Loss: 21178, Val Loss: 21571,  Learning Rate: 0.00009, Train Gradient: 144.9\n",
      "Epoch 129501/150000, Train Loss: 21173, Val Loss: 21566,  Learning Rate: 0.00009, Train Gradient: 144.9\n",
      "Epoch 129601/150000, Train Loss: 21167, Val Loss: 21560,  Learning Rate: 0.00009, Train Gradient: 144.8\n",
      "Epoch 129701/150000, Train Loss: 21162, Val Loss: 21555,  Learning Rate: 0.00009, Train Gradient: 144.8\n",
      "Epoch 129801/150000, Train Loss: 21156, Val Loss: 21549,  Learning Rate: 0.00009, Train Gradient: 144.8\n",
      "Epoch 129901/150000, Train Loss: 21151, Val Loss: 21543,  Learning Rate: 0.00009, Train Gradient: 144.7\n",
      "Epoch 130001/150000, Train Loss: 21145, Val Loss: 21538,  Learning Rate: 0.00009, Train Gradient: 144.7\n",
      "Epoch 130101/150000, Train Loss: 21140, Val Loss: 21532,  Learning Rate: 0.00010, Train Gradient: 144.7\n",
      "Epoch 130201/150000, Train Loss: 21134, Val Loss: 21527,  Learning Rate: 0.00010, Train Gradient: 144.6\n",
      "Epoch 130301/150000, Train Loss: 21129, Val Loss: 21521,  Learning Rate: 0.00010, Train Gradient: 144.6\n",
      "Epoch 130401/150000, Train Loss: 21123, Val Loss: 21516,  Learning Rate: 0.00010, Train Gradient: 144.5\n",
      "Epoch 130501/150000, Train Loss: 21118, Val Loss: 21510,  Learning Rate: 0.00010, Train Gradient: 144.5\n",
      "Epoch 130601/150000, Train Loss: 21112, Val Loss: 21505,  Learning Rate: 0.00010, Train Gradient: 144.5\n",
      "Epoch 130701/150000, Train Loss: 21107, Val Loss: 21499,  Learning Rate: 0.00010, Train Gradient: 144.4\n",
      "Epoch 130801/150000, Train Loss: 21102, Val Loss: 21494,  Learning Rate: 0.00010, Train Gradient: 144.4\n",
      "Epoch 130901/150000, Train Loss: 21096, Val Loss: 21488,  Learning Rate: 0.00010, Train Gradient: 144.3\n",
      "Epoch 131001/150000, Train Loss: 21091, Val Loss: 21483,  Learning Rate: 0.00010, Train Gradient: 144.3\n",
      "Epoch 131101/150000, Train Loss: 21085, Val Loss: 21477,  Learning Rate: 0.00010, Train Gradient: 144.3\n",
      "Epoch 131201/150000, Train Loss: 21080, Val Loss: 21472,  Learning Rate: 0.00010, Train Gradient: 144.2\n",
      "Epoch 131301/150000, Train Loss: 21074, Val Loss: 21466,  Learning Rate: 0.00010, Train Gradient: 144.2\n",
      "Epoch 131401/150000, Train Loss: 21069, Val Loss: 21460,  Learning Rate: 0.00010, Train Gradient: 144.1\n",
      "Epoch 131501/150000, Train Loss: 21063, Val Loss: 21455,  Learning Rate: 0.00010, Train Gradient: 144.1\n",
      "Epoch 131601/150000, Train Loss: 21058, Val Loss: 21449,  Learning Rate: 0.00010, Train Gradient: 144.1\n",
      "Epoch 131701/150000, Train Loss: 21052, Val Loss: 21444,  Learning Rate: 0.00010, Train Gradient: 144.0\n",
      "Epoch 131801/150000, Train Loss: 21047, Val Loss: 21438,  Learning Rate: 0.00010, Train Gradient: 144.0\n",
      "Epoch 131901/150000, Train Loss: 21042, Val Loss: 21433,  Learning Rate: 0.00010, Train Gradient: 144.0\n",
      "Epoch 132001/150000, Train Loss: 21036, Val Loss: 21427,  Learning Rate: 0.00010, Train Gradient: 143.9\n",
      "Epoch 132101/150000, Train Loss: 21031, Val Loss: 21422,  Learning Rate: 0.00010, Train Gradient: 143.9\n",
      "Epoch 132201/150000, Train Loss: 21025, Val Loss: 21416,  Learning Rate: 0.00010, Train Gradient: 143.8\n",
      "Epoch 132301/150000, Train Loss: 21020, Val Loss: 21411,  Learning Rate: 0.00010, Train Gradient: 143.8\n",
      "Epoch 132401/150000, Train Loss: 21014, Val Loss: 21405,  Learning Rate: 0.00010, Train Gradient: 143.8\n",
      "Epoch 132501/150000, Train Loss: 21009, Val Loss: 21400,  Learning Rate: 0.00010, Train Gradient: 143.7\n",
      "Epoch 132601/150000, Train Loss: 21004, Val Loss: 21395,  Learning Rate: 0.00010, Train Gradient: 143.7\n",
      "Epoch 132701/150000, Train Loss: 20998, Val Loss: 21389,  Learning Rate: 0.00010, Train Gradient: 143.6\n",
      "Epoch 132801/150000, Train Loss: 20993, Val Loss: 21384,  Learning Rate: 0.00010, Train Gradient: 143.6\n",
      "Epoch 132901/150000, Train Loss: 20987, Val Loss: 21378,  Learning Rate: 0.00010, Train Gradient: 143.6\n",
      "Epoch 133001/150000, Train Loss: 20982, Val Loss: 21373,  Learning Rate: 0.00010, Train Gradient: 143.5\n",
      "Epoch 133101/150000, Train Loss: 20976, Val Loss: 21367,  Learning Rate: 0.00010, Train Gradient: 143.5\n",
      "Epoch 133201/150000, Train Loss: 20971, Val Loss: 21362,  Learning Rate: 0.00010, Train Gradient: 143.4\n",
      "Epoch 133301/150000, Train Loss: 20966, Val Loss: 21356,  Learning Rate: 0.00010, Train Gradient: 143.4\n",
      "Epoch 133401/150000, Train Loss: 20960, Val Loss: 21351,  Learning Rate: 0.00010, Train Gradient: 143.4\n",
      "Epoch 133501/150000, Train Loss: 20955, Val Loss: 21345,  Learning Rate: 0.00010, Train Gradient: 143.3\n",
      "Epoch 133601/150000, Train Loss: 20949, Val Loss: 21340,  Learning Rate: 0.00010, Train Gradient: 143.3\n",
      "Epoch 133701/150000, Train Loss: 20944, Val Loss: 21334,  Learning Rate: 0.00010, Train Gradient: 143.3\n",
      "Epoch 133801/150000, Train Loss: 20939, Val Loss: 21329,  Learning Rate: 0.00010, Train Gradient: 143.2\n",
      "Epoch 133901/150000, Train Loss: 20933, Val Loss: 21323,  Learning Rate: 0.00010, Train Gradient: 143.2\n",
      "Epoch 134001/150000, Train Loss: 20928, Val Loss: 21318,  Learning Rate: 0.00010, Train Gradient: 143.1\n",
      "Epoch 134101/150000, Train Loss: 20922, Val Loss: 21312,  Learning Rate: 0.00010, Train Gradient: 143.1\n",
      "Epoch 134201/150000, Train Loss: 20917, Val Loss: 21307,  Learning Rate: 0.00010, Train Gradient: 143.1\n",
      "Epoch 134301/150000, Train Loss: 20912, Val Loss: 21301,  Learning Rate: 0.00010, Train Gradient: 143.0\n",
      "Epoch 134401/150000, Train Loss: 20906, Val Loss: 21296,  Learning Rate: 0.00010, Train Gradient: 143.0\n",
      "Epoch 134501/150000, Train Loss: 20901, Val Loss: 21290,  Learning Rate: 0.00010, Train Gradient: 142.9\n",
      "Epoch 134601/150000, Train Loss: 20895, Val Loss: 21285,  Learning Rate: 0.00010, Train Gradient: 142.9\n",
      "Epoch 134701/150000, Train Loss: 20890, Val Loss: 21279,  Learning Rate: 0.00010, Train Gradient: 142.9\n",
      "Epoch 134801/150000, Train Loss: 20885, Val Loss: 21274,  Learning Rate: 0.00010, Train Gradient: 142.8\n",
      "Epoch 134901/150000, Train Loss: 20879, Val Loss: 21268,  Learning Rate: 0.00010, Train Gradient: 142.8\n",
      "Epoch 135001/150000, Train Loss: 20874, Val Loss: 21263,  Learning Rate: 0.00010, Train Gradient: 142.8\n",
      "Epoch 135101/150000, Train Loss: 20868, Val Loss: 21258,  Learning Rate: 0.00010, Train Gradient: 142.7\n",
      "Epoch 135201/150000, Train Loss: 20863, Val Loss: 21252,  Learning Rate: 0.00010, Train Gradient: 142.7\n",
      "Epoch 135301/150000, Train Loss: 20858, Val Loss: 21247,  Learning Rate: 0.00010, Train Gradient: 142.6\n",
      "Epoch 135401/150000, Train Loss: 20852, Val Loss: 21241,  Learning Rate: 0.00010, Train Gradient: 142.6\n",
      "Epoch 135501/150000, Train Loss: 20847, Val Loss: 21236,  Learning Rate: 0.00010, Train Gradient: 142.6\n",
      "Epoch 135601/150000, Train Loss: 20841, Val Loss: 21230,  Learning Rate: 0.00010, Train Gradient: 142.5\n",
      "Epoch 135701/150000, Train Loss: 20836, Val Loss: 21224,  Learning Rate: 0.00010, Train Gradient: 142.5\n",
      "Epoch 135801/150000, Train Loss: 20831, Val Loss: 21219,  Learning Rate: 0.00010, Train Gradient: 142.5\n",
      "Epoch 135901/150000, Train Loss: 20825, Val Loss: 21213,  Learning Rate: 0.00010, Train Gradient: 142.4\n",
      "Epoch 136001/150000, Train Loss: 20820, Val Loss: 21208,  Learning Rate: 0.00010, Train Gradient: 142.4\n",
      "Epoch 136101/150000, Train Loss: 20814, Val Loss: 21202,  Learning Rate: 0.00010, Train Gradient: 142.3\n",
      "Epoch 136201/150000, Train Loss: 20809, Val Loss: 21197,  Learning Rate: 0.00010, Train Gradient: 142.3\n",
      "Epoch 136301/150000, Train Loss: 20803, Val Loss: 21191,  Learning Rate: 0.00010, Train Gradient: 142.3\n",
      "Epoch 136401/150000, Train Loss: 20798, Val Loss: 21186,  Learning Rate: 0.00010, Train Gradient: 142.2\n",
      "Epoch 136501/150000, Train Loss: 20793, Val Loss: 21180,  Learning Rate: 0.00010, Train Gradient: 142.2\n",
      "Epoch 136601/150000, Train Loss: 20787, Val Loss: 21175,  Learning Rate: 0.00010, Train Gradient: 142.1\n",
      "Epoch 136701/150000, Train Loss: 20782, Val Loss: 21169,  Learning Rate: 0.00010, Train Gradient: 142.1\n",
      "Epoch 136801/150000, Train Loss: 20776, Val Loss: 21163,  Learning Rate: 0.00010, Train Gradient: 142.1\n",
      "Epoch 136901/150000, Train Loss: 20771, Val Loss: 21158,  Learning Rate: 0.00010, Train Gradient: 142.0\n",
      "Epoch 137001/150000, Train Loss: 20766, Val Loss: 21152,  Learning Rate: 0.00010, Train Gradient: 142.0\n",
      "Epoch 137101/150000, Train Loss: 20760, Val Loss: 21147,  Learning Rate: 0.00010, Train Gradient: 142.0\n",
      "Epoch 137201/150000, Train Loss: 20755, Val Loss: 21141,  Learning Rate: 0.00010, Train Gradient: 141.9\n",
      "Epoch 137301/150000, Train Loss: 20749, Val Loss: 21136,  Learning Rate: 0.00010, Train Gradient: 141.9\n",
      "Epoch 137401/150000, Train Loss: 20744, Val Loss: 21130,  Learning Rate: 0.00010, Train Gradient: 141.8\n",
      "Epoch 137501/150000, Train Loss: 20738, Val Loss: 21125,  Learning Rate: 0.00010, Train Gradient: 141.8\n",
      "Epoch 137601/150000, Train Loss: 20733, Val Loss: 21119,  Learning Rate: 0.00010, Train Gradient: 141.8\n",
      "Epoch 137701/150000, Train Loss: 20728, Val Loss: 21114,  Learning Rate: 0.00010, Train Gradient: 141.7\n",
      "Epoch 137801/150000, Train Loss: 20722, Val Loss: 21108,  Learning Rate: 0.00010, Train Gradient: 141.7\n",
      "Epoch 137901/150000, Train Loss: 20717, Val Loss: 21103,  Learning Rate: 0.00010, Train Gradient: 141.7\n",
      "Epoch 138001/150000, Train Loss: 20711, Val Loss: 21097,  Learning Rate: 0.00010, Train Gradient: 141.6\n",
      "Epoch 138101/150000, Train Loss: 20706, Val Loss: 21091,  Learning Rate: 0.00010, Train Gradient: 141.6\n",
      "Epoch 138201/150000, Train Loss: 20701, Val Loss: 21086,  Learning Rate: 0.00010, Train Gradient: 141.5\n",
      "Epoch 138301/150000, Train Loss: 20695, Val Loss: 21080,  Learning Rate: 0.00010, Train Gradient: 141.5\n",
      "Epoch 138401/150000, Train Loss: 20690, Val Loss: 21075,  Learning Rate: 0.00010, Train Gradient: 141.5\n",
      "Epoch 138501/150000, Train Loss: 20684, Val Loss: 21069,  Learning Rate: 0.00010, Train Gradient: 141.4\n",
      "Epoch 138601/150000, Train Loss: 20679, Val Loss: 21064,  Learning Rate: 0.00010, Train Gradient: 141.4\n",
      "Epoch 138701/150000, Train Loss: 20674, Val Loss: 21058,  Learning Rate: 0.00010, Train Gradient: 141.4\n",
      "Epoch 138801/150000, Train Loss: 20668, Val Loss: 21053,  Learning Rate: 0.00010, Train Gradient: 141.3\n",
      "Epoch 138901/150000, Train Loss: 20663, Val Loss: 21047,  Learning Rate: 0.00010, Train Gradient: 141.3\n",
      "Epoch 139001/150000, Train Loss: 20657, Val Loss: 21042,  Learning Rate: 0.00010, Train Gradient: 141.2\n",
      "Epoch 139101/150000, Train Loss: 20652, Val Loss: 21036,  Learning Rate: 0.00010, Train Gradient: 141.2\n",
      "Epoch 139201/150000, Train Loss: 20647, Val Loss: 21031,  Learning Rate: 0.00010, Train Gradient: 141.2\n",
      "Epoch 139301/150000, Train Loss: 20641, Val Loss: 21025,  Learning Rate: 0.00010, Train Gradient: 141.1\n",
      "Epoch 139401/150000, Train Loss: 20636, Val Loss: 21020,  Learning Rate: 0.00010, Train Gradient: 141.1\n",
      "Epoch 139501/150000, Train Loss: 20630, Val Loss: 21014,  Learning Rate: 0.00010, Train Gradient: 141.1\n",
      "Epoch 139601/150000, Train Loss: 20625, Val Loss: 21009,  Learning Rate: 0.00010, Train Gradient: 141.0\n",
      "Epoch 139701/150000, Train Loss: 20620, Val Loss: 21003,  Learning Rate: 0.00010, Train Gradient: 141.0\n",
      "Epoch 139801/150000, Train Loss: 20614, Val Loss: 20998,  Learning Rate: 0.00010, Train Gradient: 140.9\n",
      "Epoch 139901/150000, Train Loss: 20609, Val Loss: 20992,  Learning Rate: 0.00010, Train Gradient: 140.9\n",
      "Epoch 140001/150000, Train Loss: 20604, Val Loss: 20987,  Learning Rate: 0.00010, Train Gradient: 140.9\n",
      "Epoch 140101/150000, Train Loss: 20598, Val Loss: 20982,  Learning Rate: 0.00010, Train Gradient: 140.8\n",
      "Epoch 140201/150000, Train Loss: 20593, Val Loss: 20976,  Learning Rate: 0.00010, Train Gradient: 140.8\n",
      "Epoch 140301/150000, Train Loss: 20587, Val Loss: 20971,  Learning Rate: 0.00010, Train Gradient: 140.8\n",
      "Epoch 140401/150000, Train Loss: 20582, Val Loss: 20965,  Learning Rate: 0.00010, Train Gradient: 140.7\n",
      "Epoch 140501/150000, Train Loss: 20577, Val Loss: 20960,  Learning Rate: 0.00010, Train Gradient: 140.7\n",
      "Epoch 140601/150000, Train Loss: 20571, Val Loss: 20954,  Learning Rate: 0.00010, Train Gradient: 140.6\n",
      "Epoch 140701/150000, Train Loss: 20566, Val Loss: 20949,  Learning Rate: 0.00010, Train Gradient: 140.6\n",
      "Epoch 140801/150000, Train Loss: 20561, Val Loss: 20943,  Learning Rate: 0.00010, Train Gradient: 140.6\n",
      "Epoch 140901/150000, Train Loss: 20555, Val Loss: 20938,  Learning Rate: 0.00010, Train Gradient: 140.5\n",
      "Epoch 141001/150000, Train Loss: 20550, Val Loss: 20932,  Learning Rate: 0.00010, Train Gradient: 140.5\n",
      "Epoch 141101/150000, Train Loss: 20545, Val Loss: 20927,  Learning Rate: 0.00010, Train Gradient: 140.5\n",
      "Epoch 141201/150000, Train Loss: 20539, Val Loss: 20922,  Learning Rate: 0.00010, Train Gradient: 140.4\n",
      "Epoch 141301/150000, Train Loss: 20534, Val Loss: 20916,  Learning Rate: 0.00010, Train Gradient: 140.4\n",
      "Epoch 141401/150000, Train Loss: 20528, Val Loss: 20911,  Learning Rate: 0.00010, Train Gradient: 140.3\n",
      "Epoch 141501/150000, Train Loss: 20523, Val Loss: 20905,  Learning Rate: 0.00010, Train Gradient: 140.3\n",
      "Epoch 141601/150000, Train Loss: 20518, Val Loss: 20900,  Learning Rate: 0.00010, Train Gradient: 140.3\n",
      "Epoch 141701/150000, Train Loss: 20512, Val Loss: 20894,  Learning Rate: 0.00010, Train Gradient: 140.2\n",
      "Epoch 141801/150000, Train Loss: 20507, Val Loss: 20889,  Learning Rate: 0.00010, Train Gradient: 140.2\n",
      "Epoch 141901/150000, Train Loss: 20502, Val Loss: 20883,  Learning Rate: 0.00010, Train Gradient: 140.2\n",
      "Epoch 142001/150000, Train Loss: 20496, Val Loss: 20878,  Learning Rate: 0.00010, Train Gradient: 140.1\n",
      "Epoch 142101/150000, Train Loss: 20491, Val Loss: 20873,  Learning Rate: 0.00010, Train Gradient: 140.1\n",
      "Epoch 142201/150000, Train Loss: 20486, Val Loss: 20867,  Learning Rate: 0.00010, Train Gradient: 140.0\n",
      "Epoch 142301/150000, Train Loss: 20480, Val Loss: 20862,  Learning Rate: 0.00010, Train Gradient: 140.0\n",
      "Epoch 142401/150000, Train Loss: 20475, Val Loss: 20856,  Learning Rate: 0.00010, Train Gradient: 140.0\n",
      "Epoch 142501/150000, Train Loss: 20470, Val Loss: 20851,  Learning Rate: 0.00010, Train Gradient: 139.9\n",
      "Epoch 142601/150000, Train Loss: 20464, Val Loss: 20845,  Learning Rate: 0.00010, Train Gradient: 139.9\n",
      "Epoch 142701/150000, Train Loss: 20459, Val Loss: 20840,  Learning Rate: 0.00010, Train Gradient: 139.9\n",
      "Epoch 142801/150000, Train Loss: 20454, Val Loss: 20834,  Learning Rate: 0.00010, Train Gradient: 139.8\n",
      "Epoch 142901/150000, Train Loss: 20448, Val Loss: 20829,  Learning Rate: 0.00010, Train Gradient: 139.8\n",
      "Epoch 143001/150000, Train Loss: 20443, Val Loss: 20824,  Learning Rate: 0.00010, Train Gradient: 139.7\n",
      "Epoch 143101/150000, Train Loss: 20438, Val Loss: 20818,  Learning Rate: 0.00010, Train Gradient: 139.7\n",
      "Epoch 143201/150000, Train Loss: 20432, Val Loss: 20813,  Learning Rate: 0.00010, Train Gradient: 139.7\n",
      "Epoch 143301/150000, Train Loss: 20427, Val Loss: 20807,  Learning Rate: 0.00010, Train Gradient: 139.6\n",
      "Epoch 143401/150000, Train Loss: 20422, Val Loss: 20802,  Learning Rate: 0.00010, Train Gradient: 139.6\n",
      "Epoch 143501/150000, Train Loss: 20416, Val Loss: 20796,  Learning Rate: 0.00010, Train Gradient: 139.6\n",
      "Epoch 143601/150000, Train Loss: 20411, Val Loss: 20791,  Learning Rate: 0.00010, Train Gradient: 139.5\n",
      "Epoch 143701/150000, Train Loss: 20406, Val Loss: 20786,  Learning Rate: 0.00010, Train Gradient: 139.5\n",
      "Epoch 143801/150000, Train Loss: 20400, Val Loss: 20780,  Learning Rate: 0.00010, Train Gradient: 139.5\n",
      "Epoch 143901/150000, Train Loss: 20395, Val Loss: 20775,  Learning Rate: 0.00010, Train Gradient: 139.4\n",
      "Epoch 144001/150000, Train Loss: 20390, Val Loss: 20769,  Learning Rate: 0.00010, Train Gradient: 139.4\n",
      "Epoch 144101/150000, Train Loss: 20384, Val Loss: 20764,  Learning Rate: 0.00010, Train Gradient: 139.3\n",
      "Epoch 144201/150000, Train Loss: 20379, Val Loss: 20759,  Learning Rate: 0.00010, Train Gradient: 139.3\n",
      "Epoch 144301/150000, Train Loss: 20374, Val Loss: 20753,  Learning Rate: 0.00010, Train Gradient: 139.3\n",
      "Epoch 144401/150000, Train Loss: 20368, Val Loss: 20748,  Learning Rate: 0.00010, Train Gradient: 139.2\n",
      "Epoch 144501/150000, Train Loss: 20363, Val Loss: 20743,  Learning Rate: 0.00010, Train Gradient: 139.2\n",
      "Epoch 144601/150000, Train Loss: 20358, Val Loss: 20737,  Learning Rate: 0.00010, Train Gradient: 139.2\n",
      "Epoch 144701/150000, Train Loss: 20352, Val Loss: 20732,  Learning Rate: 0.00010, Train Gradient: 139.1\n",
      "Epoch 144801/150000, Train Loss: 20347, Val Loss: 20726,  Learning Rate: 0.00010, Train Gradient: 139.1\n",
      "Epoch 144901/150000, Train Loss: 20342, Val Loss: 20721,  Learning Rate: 0.00010, Train Gradient: 139.0\n",
      "Epoch 145001/150000, Train Loss: 20337, Val Loss: 20716,  Learning Rate: 0.00010, Train Gradient: 139.0\n",
      "Epoch 145101/150000, Train Loss: 20331, Val Loss: 20710,  Learning Rate: 0.00010, Train Gradient: 139.0\n",
      "Epoch 145201/150000, Train Loss: 20326, Val Loss: 20705,  Learning Rate: 0.00010, Train Gradient: 138.9\n",
      "Epoch 145301/150000, Train Loss: 20321, Val Loss: 20699,  Learning Rate: 0.00010, Train Gradient: 138.9\n",
      "Epoch 145401/150000, Train Loss: 20315, Val Loss: 20694,  Learning Rate: 0.00010, Train Gradient: 138.9\n",
      "Epoch 145501/150000, Train Loss: 20310, Val Loss: 20689,  Learning Rate: 0.00010, Train Gradient: 138.8\n",
      "Epoch 145601/150000, Train Loss: 20305, Val Loss: 20683,  Learning Rate: 0.00010, Train Gradient: 138.8\n",
      "Epoch 145701/150000, Train Loss: 20299, Val Loss: 20678,  Learning Rate: 0.00010, Train Gradient: 138.8\n",
      "Epoch 145801/150000, Train Loss: 20294, Val Loss: 20672,  Learning Rate: 0.00010, Train Gradient: 138.7\n",
      "Epoch 145901/150000, Train Loss: 20289, Val Loss: 20667,  Learning Rate: 0.00010, Train Gradient: 138.7\n",
      "Epoch 146001/150000, Train Loss: 20284, Val Loss: 20662,  Learning Rate: 0.00010, Train Gradient: 138.6\n",
      "Epoch 146101/150000, Train Loss: 20278, Val Loss: 20656,  Learning Rate: 0.00010, Train Gradient: 138.6\n",
      "Epoch 146201/150000, Train Loss: 20273, Val Loss: 20651,  Learning Rate: 0.00010, Train Gradient: 138.6\n",
      "Epoch 146301/150000, Train Loss: 20268, Val Loss: 20645,  Learning Rate: 0.00010, Train Gradient: 138.5\n",
      "Epoch 146401/150000, Train Loss: 20262, Val Loss: 20640,  Learning Rate: 0.00010, Train Gradient: 138.5\n",
      "Epoch 146501/150000, Train Loss: 20257, Val Loss: 20635,  Learning Rate: 0.00010, Train Gradient: 138.5\n",
      "Epoch 146601/150000, Train Loss: 20252, Val Loss: 20629,  Learning Rate: 0.00010, Train Gradient: 138.4\n",
      "Epoch 146701/150000, Train Loss: 20247, Val Loss: 20624,  Learning Rate: 0.00010, Train Gradient: 138.4\n",
      "Epoch 146801/150000, Train Loss: 20241, Val Loss: 20619,  Learning Rate: 0.00010, Train Gradient: 138.4\n",
      "Epoch 146901/150000, Train Loss: 20236, Val Loss: 20613,  Learning Rate: 0.00010, Train Gradient: 138.3\n",
      "Epoch 147001/150000, Train Loss: 20231, Val Loss: 20608,  Learning Rate: 0.00010, Train Gradient: 138.3\n",
      "Epoch 147101/150000, Train Loss: 20225, Val Loss: 20602,  Learning Rate: 0.00010, Train Gradient: 138.2\n",
      "Epoch 147201/150000, Train Loss: 20220, Val Loss: 20597,  Learning Rate: 0.00010, Train Gradient: 138.2\n",
      "Epoch 147301/150000, Train Loss: 20215, Val Loss: 20592,  Learning Rate: 0.00010, Train Gradient: 138.2\n",
      "Epoch 147401/150000, Train Loss: 20210, Val Loss: 20586,  Learning Rate: 0.00010, Train Gradient: 138.1\n",
      "Epoch 147501/150000, Train Loss: 20204, Val Loss: 20581,  Learning Rate: 0.00010, Train Gradient: 138.1\n",
      "Epoch 147601/150000, Train Loss: 20199, Val Loss: 20576,  Learning Rate: 0.00010, Train Gradient: 138.1\n",
      "Epoch 147701/150000, Train Loss: 20194, Val Loss: 20570,  Learning Rate: 0.00010, Train Gradient: 138.0\n",
      "Epoch 147801/150000, Train Loss: 20189, Val Loss: 20565,  Learning Rate: 0.00010, Train Gradient: 138.0\n",
      "Epoch 147901/150000, Train Loss: 20183, Val Loss: 20560,  Learning Rate: 0.00010, Train Gradient: 138.0\n",
      "Epoch 148001/150000, Train Loss: 20178, Val Loss: 20554,  Learning Rate: 0.00010, Train Gradient: 137.9\n",
      "Epoch 148101/150000, Train Loss: 20173, Val Loss: 20549,  Learning Rate: 0.00010, Train Gradient: 137.9\n",
      "Epoch 148201/150000, Train Loss: 20168, Val Loss: 20543,  Learning Rate: 0.00010, Train Gradient: 137.8\n",
      "Epoch 148301/150000, Train Loss: 20162, Val Loss: 20538,  Learning Rate: 0.00010, Train Gradient: 137.8\n",
      "Epoch 148401/150000, Train Loss: 20157, Val Loss: 20533,  Learning Rate: 0.00010, Train Gradient: 137.8\n",
      "Epoch 148501/150000, Train Loss: 20152, Val Loss: 20527,  Learning Rate: 0.00010, Train Gradient: 137.7\n",
      "Epoch 148601/150000, Train Loss: 20146, Val Loss: 20522,  Learning Rate: 0.00010, Train Gradient: 137.7\n",
      "Epoch 148701/150000, Train Loss: 20141, Val Loss: 20517,  Learning Rate: 0.00010, Train Gradient: 137.7\n",
      "Epoch 148801/150000, Train Loss: 20136, Val Loss: 20511,  Learning Rate: 0.00010, Train Gradient: 137.6\n",
      "Epoch 148901/150000, Train Loss: 20131, Val Loss: 20506,  Learning Rate: 0.00010, Train Gradient: 137.6\n",
      "Epoch 149001/150000, Train Loss: 20125, Val Loss: 20501,  Learning Rate: 0.00010, Train Gradient: 137.6\n",
      "Epoch 149101/150000, Train Loss: 20120, Val Loss: 20495,  Learning Rate: 0.00010, Train Gradient: 137.5\n",
      "Epoch 149201/150000, Train Loss: 20115, Val Loss: 20490,  Learning Rate: 0.00010, Train Gradient: 137.5\n",
      "Epoch 149301/150000, Train Loss: 20110, Val Loss: 20485,  Learning Rate: 0.00010, Train Gradient: 137.5\n",
      "Epoch 149401/150000, Train Loss: 20104, Val Loss: 20479,  Learning Rate: 0.00010, Train Gradient: 137.4\n",
      "Epoch 149501/150000, Train Loss: 20099, Val Loss: 20474,  Learning Rate: 0.00010, Train Gradient: 137.4\n",
      "Epoch 149601/150000, Train Loss: 20094, Val Loss: 20469,  Learning Rate: 0.00010, Train Gradient: 137.3\n",
      "Epoch 149701/150000, Train Loss: 20089, Val Loss: 20463,  Learning Rate: 0.00010, Train Gradient: 137.3\n",
      "Epoch 149801/150000, Train Loss: 20084, Val Loss: 20458,  Learning Rate: 0.00010, Train Gradient: 137.3\n",
      "Epoch 149901/150000, Train Loss: 20078, Val Loss: 20453,  Learning Rate: 0.00010, Train Gradient: 137.2\n",
      "Test Loss: 20251.505859375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3]\n",
    "num_layers_list = [1]\n",
    "\n",
    "learning_rates = [0.001, 0.0001]\n",
    "\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "factors = [0.5]\n",
    "patience_lr = [10]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Lear. Rate: {current_lr:.5f}, Train Grad.: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mp\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150000, Train Loss: 29890, Val Loss: 30385,  Learning Rate: 0.00500, Train Gradient: 223.4\n",
      "Epoch 101/150000, Train Loss: 29530, Val Loss: 30020,  Learning Rate: 0.00452, Train Gradient: 220.2\n",
      "Epoch 201/150000, Train Loss: 28952, Val Loss: 29435,  Learning Rate: 0.00454, Train Gradient: 214.9\n",
      "Epoch 301/150000, Train Loss: 28501, Val Loss: 28979,  Learning Rate: 0.00455, Train Gradient: 210.6\n",
      "Epoch 401/150000, Train Loss: 28091, Val Loss: 28565,  Learning Rate: 0.00457, Train Gradient: 206.7\n",
      "Epoch 501/150000, Train Loss: 27703, Val Loss: 28172,  Learning Rate: 0.00458, Train Gradient: 202.9\n",
      "Epoch 601/150000, Train Loss: 27331, Val Loss: 27795,  Learning Rate: 0.00459, Train Gradient: 199.2\n",
      "Epoch 701/150000, Train Loss: 26971, Val Loss: 27431,  Learning Rate: 0.00460, Train Gradient: 195.5\n",
      "Epoch 801/150000, Train Loss: 26623, Val Loss: 27078,  Learning Rate: 0.00461, Train Gradient: 191.9\n",
      "Epoch 901/150000, Train Loss: 26285, Val Loss: 26736,  Learning Rate: 0.00462, Train Gradient: 188.4\n",
      "Epoch 1001/150000, Train Loss: 25956, Val Loss: 26403,  Learning Rate: 0.00463, Train Gradient: 184.9\n",
      "Epoch 1101/150000, Train Loss: 25636, Val Loss: 26079,  Learning Rate: 0.00464, Train Gradient: 181.4\n",
      "Epoch 1201/150000, Train Loss: 25325, Val Loss: 25763,  Learning Rate: 0.00464, Train Gradient: 177.9\n",
      "Epoch 1301/150000, Train Loss: 25022, Val Loss: 25456,  Learning Rate: 0.00465, Train Gradient: 174.5\n",
      "Epoch 1401/150000, Train Loss: 24728, Val Loss: 25157,  Learning Rate: 0.00466, Train Gradient: 171.0\n",
      "Epoch 1501/150000, Train Loss: 24441, Val Loss: 24866,  Learning Rate: 0.00467, Train Gradient: 167.7\n",
      "Epoch 1601/150000, Train Loss: 24161, Val Loss: 24583,  Learning Rate: 0.00468, Train Gradient: 164.3\n",
      "Epoch 1701/150000, Train Loss: 23889, Val Loss: 24306,  Learning Rate: 0.00468, Train Gradient: 160.9\n",
      "Epoch 1801/150000, Train Loss: 23624, Val Loss: 24038,  Learning Rate: 0.00469, Train Gradient: 157.6\n",
      "Epoch 1901/150000, Train Loss: 23367, Val Loss: 23776,  Learning Rate: 0.00470, Train Gradient: 154.3\n",
      "Epoch 2001/150000, Train Loss: 23116, Val Loss: 23521,  Learning Rate: 0.00470, Train Gradient: 151.0\n",
      "Epoch 2101/150000, Train Loss: 22872, Val Loss: 23273,  Learning Rate: 0.00471, Train Gradient: 147.8\n",
      "Epoch 2201/150000, Train Loss: 22635, Val Loss: 23032,  Learning Rate: 0.00471, Train Gradient: 144.5\n",
      "Epoch 2301/150000, Train Loss: 22405, Val Loss: 22798,  Learning Rate: 0.00472, Train Gradient: 141.3\n",
      "Epoch 2401/150000, Train Loss: 22072, Val Loss: 22480,  Learning Rate: 0.00473, Train Gradient: 152.3\n",
      "Epoch 2501/150000, Train Loss: 21792, Val Loss: 22206,  Learning Rate: 0.00473, Train Gradient: 148.9\n",
      "Epoch 2601/150000, Train Loss: 21531, Val Loss: 21933,  Learning Rate: 0.00474, Train Gradient: 147.3\n",
      "Epoch 2701/150000, Train Loss: 21276, Val Loss: 21672,  Learning Rate: 0.00475, Train Gradient: 145.5\n",
      "Epoch 2801/150000, Train Loss: 21025, Val Loss: 21417,  Learning Rate: 0.00475, Train Gradient: 143.8\n",
      "Epoch 2901/150000, Train Loss: 20777, Val Loss: 21166,  Learning Rate: 0.00476, Train Gradient: 142.0\n",
      "Epoch 3001/150000, Train Loss: 20533, Val Loss: 20917,  Learning Rate: 0.00476, Train Gradient: 140.4\n",
      "Epoch 3101/150000, Train Loss: 20292, Val Loss: 20671,  Learning Rate: 0.00477, Train Gradient: 138.7\n",
      "Epoch 3201/150000, Train Loss: 20055, Val Loss: 20429,  Learning Rate: 0.00477, Train Gradient: 137.1\n",
      "Epoch 3301/150000, Train Loss: 19820, Val Loss: 20190,  Learning Rate: 0.00478, Train Gradient: 135.5\n",
      "Epoch 3401/150000, Train Loss: 19588, Val Loss: 19955,  Learning Rate: 0.00478, Train Gradient: 133.9\n",
      "Epoch 3501/150000, Train Loss: 19360, Val Loss: 19722,  Learning Rate: 0.00479, Train Gradient: 132.3\n",
      "Epoch 3601/150000, Train Loss: 19134, Val Loss: 19493,  Learning Rate: 0.00479, Train Gradient: 130.8\n",
      "Epoch 3701/150000, Train Loss: 18910, Val Loss: 19266,  Learning Rate: 0.00480, Train Gradient: 129.3\n",
      "Epoch 3801/150000, Train Loss: 18690, Val Loss: 19042,  Learning Rate: 0.00480, Train Gradient: 127.8\n",
      "Epoch 3901/150000, Train Loss: 18472, Val Loss: 18820,  Learning Rate: 0.00481, Train Gradient: 126.3\n",
      "Epoch 4001/150000, Train Loss: 18256, Val Loss: 18601,  Learning Rate: 0.00481, Train Gradient: 124.8\n",
      "Epoch 4101/150000, Train Loss: 18043, Val Loss: 18385,  Learning Rate: 0.00482, Train Gradient: 123.4\n",
      "Epoch 4201/150000, Train Loss: 17833, Val Loss: 18171,  Learning Rate: 0.00482, Train Gradient: 122.0\n",
      "Epoch 4301/150000, Train Loss: 17625, Val Loss: 17960,  Learning Rate: 0.00483, Train Gradient: 120.5\n",
      "Epoch 4401/150000, Train Loss: 17419, Val Loss: 17752,  Learning Rate: 0.00483, Train Gradient: 119.1\n",
      "Epoch 4501/150000, Train Loss: 17216, Val Loss: 17545,  Learning Rate: 0.00483, Train Gradient: 117.7\n",
      "Epoch 4601/150000, Train Loss: 17015, Val Loss: 17342,  Learning Rate: 0.00484, Train Gradient: 116.3\n",
      "Epoch 4701/150000, Train Loss: 16817, Val Loss: 17141,  Learning Rate: 0.00484, Train Gradient: 114.9\n",
      "Epoch 4801/150000, Train Loss: 16622, Val Loss: 16942,  Learning Rate: 0.00484, Train Gradient: 113.5\n",
      "Epoch 4901/150000, Train Loss: 16428, Val Loss: 16746,  Learning Rate: 0.00485, Train Gradient: 112.1\n",
      "Epoch 5001/150000, Train Loss: 16238, Val Loss: 16552,  Learning Rate: 0.00485, Train Gradient: 110.8\n",
      "Epoch 5101/150000, Train Loss: 16049, Val Loss: 16360,  Learning Rate: 0.00485, Train Gradient: 109.4\n",
      "Epoch 5201/150000, Train Loss: 15863, Val Loss: 16171,  Learning Rate: 0.00486, Train Gradient: 108.1\n",
      "Epoch 5301/150000, Train Loss: 15679, Val Loss: 15985,  Learning Rate: 0.00486, Train Gradient: 106.7\n",
      "Epoch 5401/150000, Train Loss: 15498, Val Loss: 15800,  Learning Rate: 0.00486, Train Gradient: 105.4\n",
      "Epoch 5501/150000, Train Loss: 15319, Val Loss: 15619,  Learning Rate: 0.00487, Train Gradient: 104.1\n",
      "Epoch 5601/150000, Train Loss: 15142, Val Loss: 15439,  Learning Rate: 0.00487, Train Gradient: 102.8\n",
      "Epoch 5701/150000, Train Loss: 14968, Val Loss: 15262,  Learning Rate: 0.00487, Train Gradient: 101.5\n",
      "Epoch 5801/150000, Train Loss: 14795, Val Loss: 15087,  Learning Rate: 0.00488, Train Gradient: 100.3\n",
      "Epoch 5901/150000, Train Loss: 14625, Val Loss: 14914,  Learning Rate: 0.00488, Train Gradient: 99.1\n",
      "Epoch 6001/150000, Train Loss: 14457, Val Loss: 14744,  Learning Rate: 0.00488, Train Gradient: 97.9\n",
      "Epoch 6101/150000, Train Loss: 14291, Val Loss: 14575,  Learning Rate: 0.00488, Train Gradient: 96.7\n",
      "Epoch 6201/150000, Train Loss: 14126, Val Loss: 14409,  Learning Rate: 0.00489, Train Gradient: 95.5\n",
      "Epoch 6301/150000, Train Loss: 13964, Val Loss: 14244,  Learning Rate: 0.00489, Train Gradient: 94.3\n",
      "Epoch 6401/150000, Train Loss: 13804, Val Loss: 14082,  Learning Rate: 0.00489, Train Gradient: 93.1\n",
      "Epoch 6501/150000, Train Loss: 13646, Val Loss: 13922,  Learning Rate: 0.00489, Train Gradient: 92.0\n",
      "Epoch 6601/150000, Train Loss: 13490, Val Loss: 13763,  Learning Rate: 0.00490, Train Gradient: 90.8\n",
      "Epoch 6701/150000, Train Loss: 13336, Val Loss: 13607,  Learning Rate: 0.00490, Train Gradient: 89.7\n",
      "Epoch 6801/150000, Train Loss: 13183, Val Loss: 13452,  Learning Rate: 0.00490, Train Gradient: 88.6\n",
      "Epoch 6901/150000, Train Loss: 13032, Val Loss: 13299,  Learning Rate: 0.00490, Train Gradient: 87.6\n",
      "Epoch 7001/150000, Train Loss: 12883, Val Loss: 13148,  Learning Rate: 0.00491, Train Gradient: 86.5\n",
      "Epoch 7101/150000, Train Loss: 12736, Val Loss: 12999,  Learning Rate: 0.00491, Train Gradient: 85.5\n",
      "Epoch 7201/150000, Train Loss: 12591, Val Loss: 12852,  Learning Rate: 0.00491, Train Gradient: 84.4\n",
      "Epoch 7301/150000, Train Loss: 12447, Val Loss: 12706,  Learning Rate: 0.00491, Train Gradient: 83.4\n",
      "Epoch 7401/150000, Train Loss: 12305, Val Loss: 12562,  Learning Rate: 0.00491, Train Gradient: 82.4\n",
      "Epoch 7501/150000, Train Loss: 12164, Val Loss: 12419,  Learning Rate: 0.00492, Train Gradient: 81.4\n",
      "Epoch 7601/150000, Train Loss: 12026, Val Loss: 12279,  Learning Rate: 0.00492, Train Gradient: 80.5\n",
      "Epoch 7701/150000, Train Loss: 11888, Val Loss: 12139,  Learning Rate: 0.00492, Train Gradient: 79.6\n",
      "Epoch 7801/150000, Train Loss: 11752, Val Loss: 12002,  Learning Rate: 0.00492, Train Gradient: 78.7\n",
      "Epoch 7901/150000, Train Loss: 11618, Val Loss: 11866,  Learning Rate: 0.00492, Train Gradient: 77.7\n",
      "Epoch 8001/150000, Train Loss: 11485, Val Loss: 11731,  Learning Rate: 0.00492, Train Gradient: 76.8\n",
      "Epoch 8101/150000, Train Loss: 11354, Val Loss: 11598,  Learning Rate: 0.00493, Train Gradient: 76.1\n",
      "Epoch 8201/150000, Train Loss: 11223, Val Loss: 11466,  Learning Rate: 0.00493, Train Gradient: 75.0\n",
      "Epoch 8301/150000, Train Loss: 11095, Val Loss: 11336,  Learning Rate: 0.00493, Train Gradient: 74.2\n",
      "Epoch 8401/150000, Train Loss: 10968, Val Loss: 11207,  Learning Rate: 0.00493, Train Gradient: 73.3\n",
      "Epoch 8501/150000, Train Loss: 10842, Val Loss: 11079,  Learning Rate: 0.00493, Train Gradient: 72.5\n",
      "Epoch 8601/150000, Train Loss: 10718, Val Loss: 10953,  Learning Rate: 0.00493, Train Gradient: 71.6\n",
      "Epoch 8701/150000, Train Loss: 10595, Val Loss: 10828,  Learning Rate: 0.00494, Train Gradient: 70.8\n",
      "Epoch 8801/150000, Train Loss: 10473, Val Loss: 10705,  Learning Rate: 0.00494, Train Gradient: 70.0\n",
      "Epoch 8901/150000, Train Loss: 10352, Val Loss: 10583,  Learning Rate: 0.00494, Train Gradient: 69.1\n",
      "Epoch 9001/150000, Train Loss: 10233, Val Loss: 10462,  Learning Rate: 0.00494, Train Gradient: 68.5\n",
      "Epoch 9101/150000, Train Loss: 10115, Val Loss: 10342,  Learning Rate: 0.00494, Train Gradient: 67.7\n",
      "Epoch 9201/150000, Train Loss: 9998, Val Loss: 10224,  Learning Rate: 0.00494, Train Gradient: 67.0\n",
      "Epoch 9301/150000, Train Loss: 9883, Val Loss: 10107,  Learning Rate: 0.00494, Train Gradient: 66.2\n",
      "Epoch 9401/150000, Train Loss: 9768, Val Loss: 9991,  Learning Rate: 0.00495, Train Gradient: 65.6\n",
      "Epoch 9501/150000, Train Loss: 9655, Val Loss: 9876,  Learning Rate: 0.00495, Train Gradient: 64.8\n",
      "Epoch 9601/150000, Train Loss: 9543, Val Loss: 9762,  Learning Rate: 0.00495, Train Gradient: 64.1\n",
      "Epoch 9701/150000, Train Loss: 9432, Val Loss: 9650,  Learning Rate: 0.00495, Train Gradient: 63.4\n",
      "Epoch 9801/150000, Train Loss: 9322, Val Loss: 9539,  Learning Rate: 0.00495, Train Gradient: 62.7\n",
      "Epoch 9901/150000, Train Loss: 9213, Val Loss: 9429,  Learning Rate: 0.00495, Train Gradient: 62.1\n",
      "Epoch 10001/150000, Train Loss: 9105, Val Loss: 9319,  Learning Rate: 0.00495, Train Gradient: 61.5\n",
      "Epoch 10101/150000, Train Loss: 8998, Val Loss: 9211,  Learning Rate: 0.00495, Train Gradient: 60.8\n",
      "Epoch 10201/150000, Train Loss: 8892, Val Loss: 9104,  Learning Rate: 0.00495, Train Gradient: 60.2\n",
      "Epoch 10301/150000, Train Loss: 8787, Val Loss: 8998,  Learning Rate: 0.00496, Train Gradient: 59.7\n",
      "Epoch 10401/150000, Train Loss: 8683, Val Loss: 8892,  Learning Rate: 0.00496, Train Gradient: 59.0\n",
      "Epoch 10501/150000, Train Loss: 8580, Val Loss: 8788,  Learning Rate: 0.00496, Train Gradient: 58.7\n",
      "Epoch 10601/150000, Train Loss: 8478, Val Loss: 8684,  Learning Rate: 0.00496, Train Gradient: 57.9\n",
      "Epoch 10701/150000, Train Loss: 8376, Val Loss: 8582,  Learning Rate: 0.00496, Train Gradient: 57.0\n",
      "Epoch 10801/150000, Train Loss: 8275, Val Loss: 8480,  Learning Rate: 0.00496, Train Gradient: 56.8\n",
      "Epoch 10901/150000, Train Loss: 8176, Val Loss: 8379,  Learning Rate: 0.00496, Train Gradient: 56.2\n",
      "Epoch 11001/150000, Train Loss: 8077, Val Loss: 8279,  Learning Rate: 0.00496, Train Gradient: 55.7\n",
      "Epoch 11101/150000, Train Loss: 7979, Val Loss: 8180,  Learning Rate: 0.00496, Train Gradient: 55.2\n",
      "Epoch 11201/150000, Train Loss: 7881, Val Loss: 8081,  Learning Rate: 0.00496, Train Gradient: 54.7\n",
      "Epoch 11301/150000, Train Loss: 7785, Val Loss: 7983,  Learning Rate: 0.00497, Train Gradient: 54.1\n",
      "Epoch 11401/150000, Train Loss: 7689, Val Loss: 7887,  Learning Rate: 0.00497, Train Gradient: 53.6\n",
      "Epoch 11501/150000, Train Loss: 7594, Val Loss: 7791,  Learning Rate: 0.00497, Train Gradient: 53.3\n",
      "Epoch 11601/150000, Train Loss: 7500, Val Loss: 7696,  Learning Rate: 0.00497, Train Gradient: 52.6\n",
      "Epoch 11701/150000, Train Loss: 7407, Val Loss: 7601,  Learning Rate: 0.00497, Train Gradient: 52.2\n",
      "Epoch 11801/150000, Train Loss: 7315, Val Loss: 7508,  Learning Rate: 0.00497, Train Gradient: 51.7\n",
      "Epoch 11901/150000, Train Loss: 7223, Val Loss: 7415,  Learning Rate: 0.00497, Train Gradient: 51.2\n",
      "Epoch 12001/150000, Train Loss: 7132, Val Loss: 7323,  Learning Rate: 0.00497, Train Gradient: 50.8\n",
      "Epoch 12101/150000, Train Loss: 7042, Val Loss: 7232,  Learning Rate: 0.00497, Train Gradient: 50.3\n",
      "Epoch 12201/150000, Train Loss: 6953, Val Loss: 7142,  Learning Rate: 0.00497, Train Gradient: 50.0\n",
      "Epoch 12301/150000, Train Loss: 6864, Val Loss: 7052,  Learning Rate: 0.00497, Train Gradient: 49.3\n",
      "Epoch 12401/150000, Train Loss: 6776, Val Loss: 6964,  Learning Rate: 0.00497, Train Gradient: 48.9\n",
      "Epoch 12501/150000, Train Loss: 6689, Val Loss: 6876,  Learning Rate: 0.00497, Train Gradient: 48.4\n",
      "Epoch 12601/150000, Train Loss: 6603, Val Loss: 6788,  Learning Rate: 0.00497, Train Gradient: 48.0\n",
      "Epoch 12701/150000, Train Loss: 6518, Val Loss: 6702,  Learning Rate: 0.00498, Train Gradient: 47.5\n",
      "Epoch 12801/150000, Train Loss: 6433, Val Loss: 6616,  Learning Rate: 0.00498, Train Gradient: 47.1\n",
      "Epoch 12901/150000, Train Loss: 6349, Val Loss: 6531,  Learning Rate: 0.00498, Train Gradient: 46.7\n",
      "Epoch 13001/150000, Train Loss: 6266, Val Loss: 6447,  Learning Rate: 0.00498, Train Gradient: 46.2\n",
      "Epoch 13101/150000, Train Loss: 6183, Val Loss: 6364,  Learning Rate: 0.00498, Train Gradient: 45.8\n",
      "Epoch 13201/150000, Train Loss: 6101, Val Loss: 6281,  Learning Rate: 0.00498, Train Gradient: 45.3\n",
      "Epoch 13301/150000, Train Loss: 6021, Val Loss: 6200,  Learning Rate: 0.00498, Train Gradient: 44.9\n",
      "Epoch 13401/150000, Train Loss: 5940, Val Loss: 6119,  Learning Rate: 0.00498, Train Gradient: 44.5\n",
      "Epoch 13501/150000, Train Loss: 5861, Val Loss: 6039,  Learning Rate: 0.00498, Train Gradient: 44.1\n",
      "Epoch 13601/150000, Train Loss: 5782, Val Loss: 5959,  Learning Rate: 0.00498, Train Gradient: 43.6\n",
      "Epoch 13701/150000, Train Loss: 5704, Val Loss: 5881,  Learning Rate: 0.00498, Train Gradient: 43.3\n",
      "Epoch 13801/150000, Train Loss: 5627, Val Loss: 5803,  Learning Rate: 0.00498, Train Gradient: 42.8\n",
      "Epoch 13901/150000, Train Loss: 5551, Val Loss: 5726,  Learning Rate: 0.00498, Train Gradient: 42.5\n",
      "Epoch 14001/150000, Train Loss: 5475, Val Loss: 5650,  Learning Rate: 0.00498, Train Gradient: 41.9\n",
      "Epoch 14101/150000, Train Loss: 5401, Val Loss: 5574,  Learning Rate: 0.00498, Train Gradient: 41.5\n",
      "Epoch 14201/150000, Train Loss: 5327, Val Loss: 5499,  Learning Rate: 0.00498, Train Gradient: 41.1\n",
      "Epoch 14301/150000, Train Loss: 5254, Val Loss: 5425,  Learning Rate: 0.00498, Train Gradient: 40.7\n",
      "Epoch 14401/150000, Train Loss: 5181, Val Loss: 5352,  Learning Rate: 0.00498, Train Gradient: 40.2\n",
      "Epoch 14501/150000, Train Loss: 5109, Val Loss: 5279,  Learning Rate: 0.00498, Train Gradient: 39.9\n",
      "Epoch 14601/150000, Train Loss: 5038, Val Loss: 5208,  Learning Rate: 0.00499, Train Gradient: 39.3\n",
      "Epoch 14701/150000, Train Loss: 4968, Val Loss: 5137,  Learning Rate: 0.00499, Train Gradient: 39.1\n",
      "Epoch 14801/150000, Train Loss: 4899, Val Loss: 5067,  Learning Rate: 0.00499, Train Gradient: 38.5\n",
      "Epoch 14901/150000, Train Loss: 4830, Val Loss: 4998,  Learning Rate: 0.00499, Train Gradient: 38.3\n",
      "Epoch 15001/150000, Train Loss: 4761, Val Loss: 4930,  Learning Rate: 0.00499, Train Gradient: 37.9\n",
      "Epoch 15101/150000, Train Loss: 4694, Val Loss: 4862,  Learning Rate: 0.00499, Train Gradient: 37.5\n",
      "Epoch 15201/150000, Train Loss: 4627, Val Loss: 4795,  Learning Rate: 0.00499, Train Gradient: 37.2\n",
      "Epoch 15301/150000, Train Loss: 4561, Val Loss: 4728,  Learning Rate: 0.00499, Train Gradient: 36.9\n",
      "Epoch 15401/150000, Train Loss: 4496, Val Loss: 4662,  Learning Rate: 0.00499, Train Gradient: 36.4\n",
      "Epoch 15501/150000, Train Loss: 4431, Val Loss: 4597,  Learning Rate: 0.00499, Train Gradient: 35.9\n",
      "Epoch 15601/150000, Train Loss: 4367, Val Loss: 4533,  Learning Rate: 0.00499, Train Gradient: 35.7\n",
      "Epoch 15701/150000, Train Loss: 4303, Val Loss: 4469,  Learning Rate: 0.00499, Train Gradient: 35.3\n",
      "Epoch 15801/150000, Train Loss: 4240, Val Loss: 4406,  Learning Rate: 0.00499, Train Gradient: 35.0\n",
      "Epoch 15901/150000, Train Loss: 4177, Val Loss: 4344,  Learning Rate: 0.00499, Train Gradient: 34.6\n",
      "Epoch 16001/150000, Train Loss: 4116, Val Loss: 4283,  Learning Rate: 0.00499, Train Gradient: 34.3\n",
      "Epoch 16101/150000, Train Loss: 4055, Val Loss: 4222,  Learning Rate: 0.00499, Train Gradient: 33.9\n",
      "Epoch 16201/150000, Train Loss: 3995, Val Loss: 4161,  Learning Rate: 0.00499, Train Gradient: 33.5\n",
      "Epoch 16301/150000, Train Loss: 3935, Val Loss: 4101,  Learning Rate: 0.00499, Train Gradient: 33.0\n",
      "Epoch 16401/150000, Train Loss: 3876, Val Loss: 4043,  Learning Rate: 0.00499, Train Gradient: 32.8\n",
      "Epoch 16501/150000, Train Loss: 3818, Val Loss: 3984,  Learning Rate: 0.00499, Train Gradient: 32.5\n",
      "Epoch 16601/150000, Train Loss: 3761, Val Loss: 3927,  Learning Rate: 0.00499, Train Gradient: 32.1\n",
      "Epoch 16701/150000, Train Loss: 3704, Val Loss: 3870,  Learning Rate: 0.00499, Train Gradient: 31.8\n",
      "Epoch 16801/150000, Train Loss: 3647, Val Loss: 3814,  Learning Rate: 0.00499, Train Gradient: 31.5\n",
      "Epoch 16901/150000, Train Loss: 3592, Val Loss: 3758,  Learning Rate: 0.00499, Train Gradient: 31.1\n",
      "Epoch 17001/150000, Train Loss: 3537, Val Loss: 3704,  Learning Rate: 0.00499, Train Gradient: 30.7\n",
      "Epoch 17101/150000, Train Loss: 3482, Val Loss: 3650,  Learning Rate: 0.00499, Train Gradient: 30.4\n",
      "Epoch 17201/150000, Train Loss: 3429, Val Loss: 3597,  Learning Rate: 0.00499, Train Gradient: 30.1\n",
      "Epoch 17301/150000, Train Loss: 3376, Val Loss: 3544,  Learning Rate: 0.00499, Train Gradient: 29.7\n",
      "Epoch 17401/150000, Train Loss: 3323, Val Loss: 3493,  Learning Rate: 0.00499, Train Gradient: 29.4\n",
      "Epoch 17501/150000, Train Loss: 3272, Val Loss: 3441,  Learning Rate: 0.00499, Train Gradient: 28.8\n",
      "Epoch 17601/150000, Train Loss: 3221, Val Loss: 3390,  Learning Rate: 0.00499, Train Gradient: 28.7\n",
      "Epoch 17701/150000, Train Loss: 3170, Val Loss: 3340,  Learning Rate: 0.00499, Train Gradient: 28.4\n",
      "Epoch 17801/150000, Train Loss: 3120, Val Loss: 3290,  Learning Rate: 0.00499, Train Gradient: 28.0\n",
      "Epoch 17901/150000, Train Loss: 3071, Val Loss: 3241,  Learning Rate: 0.00499, Train Gradient: 27.7\n",
      "Epoch 18001/150000, Train Loss: 3022, Val Loss: 3193,  Learning Rate: 0.00499, Train Gradient: 27.4\n",
      "Epoch 18101/150000, Train Loss: 2974, Val Loss: 3145,  Learning Rate: 0.00499, Train Gradient: 27.1\n",
      "Epoch 18201/150000, Train Loss: 2927, Val Loss: 3097,  Learning Rate: 0.00499, Train Gradient: 26.8\n",
      "Epoch 18301/150000, Train Loss: 2880, Val Loss: 3050,  Learning Rate: 0.00499, Train Gradient: 26.5\n",
      "Epoch 18401/150000, Train Loss: 2833, Val Loss: 3003,  Learning Rate: 0.00500, Train Gradient: 26.6\n",
      "Epoch 18501/150000, Train Loss: 2788, Val Loss: 2958,  Learning Rate: 0.00500, Train Gradient: 25.8\n",
      "Epoch 18601/150000, Train Loss: 2742, Val Loss: 2912,  Learning Rate: 0.00500, Train Gradient: 26.1\n",
      "Epoch 18701/150000, Train Loss: 2698, Val Loss: 2868,  Learning Rate: 0.00500, Train Gradient: 25.2\n",
      "Epoch 18801/150000, Train Loss: 2654, Val Loss: 2824,  Learning Rate: 0.00500, Train Gradient: 24.9\n",
      "Epoch 18901/150000, Train Loss: 2610, Val Loss: 2780,  Learning Rate: 0.00500, Train Gradient: 24.6\n",
      "Epoch 19001/150000, Train Loss: 2567, Val Loss: 2738,  Learning Rate: 0.00500, Train Gradient: 24.3\n",
      "Epoch 19101/150000, Train Loss: 2525, Val Loss: 2695,  Learning Rate: 0.00500, Train Gradient: 24.1\n",
      "Epoch 19201/150000, Train Loss: 2483, Val Loss: 2654,  Learning Rate: 0.00500, Train Gradient: 23.7\n",
      "Epoch 19301/150000, Train Loss: 2442, Val Loss: 2613,  Learning Rate: 0.00500, Train Gradient: 23.2\n",
      "Epoch 19401/150000, Train Loss: 2401, Val Loss: 2572,  Learning Rate: 0.00500, Train Gradient: 23.2\n",
      "Epoch 19501/150000, Train Loss: 2360, Val Loss: 2532,  Learning Rate: 0.00500, Train Gradient: 23.1\n",
      "Epoch 19601/150000, Train Loss: 2321, Val Loss: 2493,  Learning Rate: 0.00500, Train Gradient: 22.6\n",
      "Epoch 19701/150000, Train Loss: 2281, Val Loss: 2454,  Learning Rate: 0.00500, Train Gradient: 22.3\n",
      "Epoch 19801/150000, Train Loss: 2243, Val Loss: 2416,  Learning Rate: 0.00500, Train Gradient: 22.1\n",
      "Epoch 19901/150000, Train Loss: 2204, Val Loss: 2378,  Learning Rate: 0.00500, Train Gradient: 21.8\n",
      "Epoch 20001/150000, Train Loss: 2166, Val Loss: 2340,  Learning Rate: 0.00500, Train Gradient: 21.9\n",
      "Epoch 20101/150000, Train Loss: 2129, Val Loss: 2303,  Learning Rate: 0.00500, Train Gradient: 21.3\n",
      "Epoch 20201/150000, Train Loss: 2092, Val Loss: 2266,  Learning Rate: 0.00500, Train Gradient: 21.8\n",
      "Epoch 20301/150000, Train Loss: 2055, Val Loss: 2230,  Learning Rate: 0.00500, Train Gradient: 20.8\n",
      "Epoch 20401/150000, Train Loss: 2019, Val Loss: 2195,  Learning Rate: 0.00500, Train Gradient: 20.5\n",
      "Epoch 20501/150000, Train Loss: 1984, Val Loss: 2159,  Learning Rate: 0.00500, Train Gradient: 20.3\n",
      "Epoch 20601/150000, Train Loss: 1949, Val Loss: 2125,  Learning Rate: 0.00500, Train Gradient: 20.0\n",
      "Epoch 20701/150000, Train Loss: 1914, Val Loss: 2091,  Learning Rate: 0.00500, Train Gradient: 19.7\n",
      "Epoch 20801/150000, Train Loss: 1880, Val Loss: 2057,  Learning Rate: 0.00500, Train Gradient: 19.5\n",
      "Epoch 20901/150000, Train Loss: 1846, Val Loss: 2024,  Learning Rate: 0.00500, Train Gradient: 19.2\n",
      "Epoch 21001/150000, Train Loss: 1813, Val Loss: 1991,  Learning Rate: 0.00500, Train Gradient: 19.0\n",
      "Epoch 21101/150000, Train Loss: 1780, Val Loss: 1959,  Learning Rate: 0.00500, Train Gradient: 18.8\n",
      "Epoch 21201/150000, Train Loss: 1748, Val Loss: 1927,  Learning Rate: 0.00500, Train Gradient: 18.5\n",
      "Epoch 21301/150000, Train Loss: 1716, Val Loss: 1896,  Learning Rate: 0.00500, Train Gradient: 18.2\n",
      "Epoch 21401/150000, Train Loss: 1684, Val Loss: 1865,  Learning Rate: 0.00500, Train Gradient: 18.0\n",
      "Epoch 21501/150000, Train Loss: 1653, Val Loss: 1835,  Learning Rate: 0.00500, Train Gradient: 17.7\n",
      "Epoch 21601/150000, Train Loss: 1623, Val Loss: 1805,  Learning Rate: 0.00500, Train Gradient: 17.5\n",
      "Epoch 21701/150000, Train Loss: 1593, Val Loss: 1775,  Learning Rate: 0.00500, Train Gradient: 17.3\n",
      "Epoch 21801/150000, Train Loss: 1563, Val Loss: 1746,  Learning Rate: 0.00500, Train Gradient: 17.0\n",
      "Epoch 21901/150000, Train Loss: 1534, Val Loss: 1718,  Learning Rate: 0.00500, Train Gradient: 16.8\n",
      "Epoch 22001/150000, Train Loss: 1506, Val Loss: 1690,  Learning Rate: 0.00500, Train Gradient: 16.6\n",
      "Epoch 22101/150000, Train Loss: 1477, Val Loss: 1662,  Learning Rate: 0.00500, Train Gradient: 16.4\n",
      "Epoch 22201/150000, Train Loss: 1449, Val Loss: 1635,  Learning Rate: 0.00500, Train Gradient: 16.1\n",
      "Epoch 22301/150000, Train Loss: 1422, Val Loss: 1608,  Learning Rate: 0.00500, Train Gradient: 15.9\n",
      "Epoch 22401/150000, Train Loss: 1395, Val Loss: 1582,  Learning Rate: 0.00500, Train Gradient: 15.7\n",
      "Epoch 22501/150000, Train Loss: 1368, Val Loss: 1556,  Learning Rate: 0.00500, Train Gradient: 15.4\n",
      "Epoch 22601/150000, Train Loss: 1342, Val Loss: 1530,  Learning Rate: 0.00500, Train Gradient: 15.1\n",
      "Epoch 22701/150000, Train Loss: 1316, Val Loss: 1505,  Learning Rate: 0.00500, Train Gradient: 15.0\n",
      "Epoch 22801/150000, Train Loss: 1290, Val Loss: 1481,  Learning Rate: 0.00500, Train Gradient: 14.4\n",
      "Epoch 22901/150000, Train Loss: 1265, Val Loss: 1456,  Learning Rate: 0.00500, Train Gradient: 14.6\n",
      "Epoch 23001/150000, Train Loss: 1240, Val Loss: 1432,  Learning Rate: 0.00500, Train Gradient: 14.5\n",
      "Epoch 23101/150000, Train Loss: 1215, Val Loss: 1409,  Learning Rate: 0.00500, Train Gradient: 14.3\n",
      "Epoch 23201/150000, Train Loss: 1191, Val Loss: 1386,  Learning Rate: 0.00500, Train Gradient: 14.1\n",
      "Epoch 23301/150000, Train Loss: 1167, Val Loss: 1363,  Learning Rate: 0.00500, Train Gradient: 13.8\n",
      "Epoch 23401/150000, Train Loss: 1144, Val Loss: 1341,  Learning Rate: 0.00500, Train Gradient: 13.4\n",
      "Epoch 23501/150000, Train Loss: 1120, Val Loss: 1319,  Learning Rate: 0.00500, Train Gradient: 13.3\n",
      "Epoch 23601/150000, Train Loss: 1098, Val Loss: 1298,  Learning Rate: 0.00500, Train Gradient: 13.2\n",
      "Epoch 23701/150000, Train Loss: 1075, Val Loss: 1276,  Learning Rate: 0.00500, Train Gradient: 13.0\n",
      "Epoch 23801/150000, Train Loss: 1053, Val Loss: 1256,  Learning Rate: 0.00500, Train Gradient: 12.8\n",
      "Epoch 23901/150000, Train Loss: 1031, Val Loss: 1236,  Learning Rate: 0.00500, Train Gradient: 12.5\n",
      "Epoch 24001/150000, Train Loss: 1009, Val Loss: 1216,  Learning Rate: 0.00500, Train Gradient: 12.4\n",
      "Epoch 24101/150000, Train Loss: 987, Val Loss: 1197,  Learning Rate: 0.00500, Train Gradient: 12.2\n",
      "Epoch 24201/150000, Train Loss: 966, Val Loss: 1178,  Learning Rate: 0.00500, Train Gradient: 12.0\n",
      "Epoch 24301/150000, Train Loss: 946, Val Loss: 1160,  Learning Rate: 0.00500, Train Gradient: 11.8\n",
      "Epoch 24401/150000, Train Loss: 926, Val Loss: 1142,  Learning Rate: 0.00500, Train Gradient: 11.7\n",
      "Epoch 24501/150000, Train Loss: 906, Val Loss: 1125,  Learning Rate: 0.00500, Train Gradient: 11.4\n",
      "Epoch 24601/150000, Train Loss: 887, Val Loss: 1107,  Learning Rate: 0.00500, Train Gradient: 11.3\n",
      "Epoch 24701/150000, Train Loss: 866, Val Loss: 1088,  Learning Rate: 0.00500, Train Gradient: 11.0\n",
      "Epoch 24801/150000, Train Loss: 846, Val Loss: 1067,  Learning Rate: 0.00500, Train Gradient: 10.9\n",
      "Epoch 24901/150000, Train Loss: 826, Val Loss: 1043,  Learning Rate: 0.00500, Train Gradient: 10.7\n",
      "Epoch 25001/150000, Train Loss: 808, Val Loss: 1027,  Learning Rate: 0.00500, Train Gradient: 10.6\n",
      "Epoch 25101/150000, Train Loss: 801, Val Loss: 1012,  Learning Rate: 0.00500, Train Gradient: 10.4\n",
      "Epoch 25201/150000, Train Loss: 783, Val Loss: 996,  Learning Rate: 0.00500, Train Gradient: 10.3\n",
      "Epoch 25301/150000, Train Loss: 766, Val Loss: 980,  Learning Rate: 0.00500, Train Gradient: 10.2\n",
      "Epoch 25401/150000, Train Loss: 749, Val Loss: 964,  Learning Rate: 0.00500, Train Gradient: 10.0\n",
      "Epoch 25501/150000, Train Loss: 732, Val Loss: 949,  Learning Rate: 0.00500, Train Gradient: 9.8\n",
      "Epoch 25601/150000, Train Loss: 716, Val Loss: 934,  Learning Rate: 0.00500, Train Gradient: 9.6\n",
      "Epoch 25701/150000, Train Loss: 700, Val Loss: 919,  Learning Rate: 0.00500, Train Gradient: 9.5\n",
      "Epoch 25801/150000, Train Loss: 684, Val Loss: 905,  Learning Rate: 0.00500, Train Gradient: 9.3\n",
      "Epoch 25901/150000, Train Loss: 669, Val Loss: 891,  Learning Rate: 0.00500, Train Gradient: 9.2\n",
      "Epoch 26001/150000, Train Loss: 654, Val Loss: 878,  Learning Rate: 0.00500, Train Gradient: 9.0\n",
      "Epoch 26101/150000, Train Loss: 639, Val Loss: 864,  Learning Rate: 0.00500, Train Gradient: 8.9\n",
      "Epoch 26201/150000, Train Loss: 624, Val Loss: 851,  Learning Rate: 0.00500, Train Gradient: 9.0\n",
      "Epoch 26301/150000, Train Loss: 610, Val Loss: 838,  Learning Rate: 0.00500, Train Gradient: 8.6\n",
      "Epoch 26401/150000, Train Loss: 596, Val Loss: 825,  Learning Rate: 0.00500, Train Gradient: 8.5\n",
      "Epoch 26501/150000, Train Loss: 582, Val Loss: 813,  Learning Rate: 0.00500, Train Gradient: 8.3\n",
      "Epoch 26601/150000, Train Loss: 568, Val Loss: 801,  Learning Rate: 0.00500, Train Gradient: 8.1\n",
      "Epoch 26701/150000, Train Loss: 555, Val Loss: 789,  Learning Rate: 0.00500, Train Gradient: 8.0\n",
      "Epoch 26801/150000, Train Loss: 542, Val Loss: 777,  Learning Rate: 0.00500, Train Gradient: 7.8\n",
      "Epoch 26901/150000, Train Loss: 530, Val Loss: 766,  Learning Rate: 0.00500, Train Gradient: 7.6\n",
      "Epoch 27001/150000, Train Loss: 517, Val Loss: 755,  Learning Rate: 0.00500, Train Gradient: 7.5\n",
      "Epoch 27101/150000, Train Loss: 505, Val Loss: 744,  Learning Rate: 0.00500, Train Gradient: 7.3\n",
      "Epoch 27201/150000, Train Loss: 493, Val Loss: 734,  Learning Rate: 0.00500, Train Gradient: 7.2\n",
      "Epoch 27301/150000, Train Loss: 482, Val Loss: 725,  Learning Rate: 0.00500, Train Gradient: 6.5\n",
      "Epoch 27401/150000, Train Loss: 470, Val Loss: 714,  Learning Rate: 0.00500, Train Gradient: 7.0\n",
      "Epoch 27501/150000, Train Loss: 459, Val Loss: 705,  Learning Rate: 0.00500, Train Gradient: 6.8\n",
      "Epoch 27601/150000, Train Loss: 448, Val Loss: 696,  Learning Rate: 0.00500, Train Gradient: 6.8\n",
      "Epoch 27701/150000, Train Loss: 438, Val Loss: 687,  Learning Rate: 0.00500, Train Gradient: 6.5\n",
      "Epoch 27801/150000, Train Loss: 427, Val Loss: 678,  Learning Rate: 0.00500, Train Gradient: 6.4\n",
      "Epoch 27901/150000, Train Loss: 417, Val Loss: 670,  Learning Rate: 0.00500, Train Gradient: 6.3\n",
      "Epoch 28001/150000, Train Loss: 407, Val Loss: 661,  Learning Rate: 0.00500, Train Gradient: 6.1\n",
      "Epoch 28101/150000, Train Loss: 398, Val Loss: 653,  Learning Rate: 0.00500, Train Gradient: 6.1\n",
      "Epoch 28201/150000, Train Loss: 388, Val Loss: 644,  Learning Rate: 0.00500, Train Gradient: 5.9\n",
      "Epoch 28301/150000, Train Loss: 378, Val Loss: 629,  Learning Rate: 0.00500, Train Gradient: 5.8\n",
      "Epoch 28401/150000, Train Loss: 369, Val Loss: 621,  Learning Rate: 0.00500, Train Gradient: 5.8\n",
      "Epoch 28501/150000, Train Loss: 360, Val Loss: 614,  Learning Rate: 0.00500, Train Gradient: 5.6\n",
      "Epoch 28601/150000, Train Loss: 351, Val Loss: 607,  Learning Rate: 0.00500, Train Gradient: 5.5\n",
      "Epoch 28701/150000, Train Loss: 343, Val Loss: 600,  Learning Rate: 0.00500, Train Gradient: 5.4\n",
      "Epoch 28801/150000, Train Loss: 334, Val Loss: 593,  Learning Rate: 0.00500, Train Gradient: 5.3\n",
      "Epoch 28901/150000, Train Loss: 326, Val Loss: 587,  Learning Rate: 0.00500, Train Gradient: 6.1\n",
      "Epoch 29001/150000, Train Loss: 318, Val Loss: 582,  Learning Rate: 0.00500, Train Gradient: 5.0\n",
      "Epoch 29101/150000, Train Loss: 310, Val Loss: 576,  Learning Rate: 0.00500, Train Gradient: 4.9\n",
      "Epoch 29201/150000, Train Loss: 302, Val Loss: 570,  Learning Rate: 0.00500, Train Gradient: 4.8\n",
      "Epoch 29301/150000, Train Loss: 294, Val Loss: 560,  Learning Rate: 0.00500, Train Gradient: 4.7\n",
      "Epoch 29401/150000, Train Loss: 287, Val Loss: 554,  Learning Rate: 0.00500, Train Gradient: 4.6\n",
      "Epoch 29501/150000, Train Loss: 280, Val Loss: 549,  Learning Rate: 0.00500, Train Gradient: 4.5\n",
      "Epoch 29601/150000, Train Loss: 273, Val Loss: 544,  Learning Rate: 0.00500, Train Gradient: 4.4\n",
      "Epoch 29701/150000, Train Loss: 266, Val Loss: 538,  Learning Rate: 0.00500, Train Gradient: 4.3\n",
      "Epoch 29801/150000, Train Loss: 259, Val Loss: 533,  Learning Rate: 0.00500, Train Gradient: 4.0\n",
      "Epoch 29901/150000, Train Loss: 253, Val Loss: 527,  Learning Rate: 0.00500, Train Gradient: 4.1\n",
      "Epoch 30001/150000, Train Loss: 247, Val Loss: 523,  Learning Rate: 0.00500, Train Gradient: 3.9\n",
      "Epoch 30101/150000, Train Loss: 241, Val Loss: 517,  Learning Rate: 0.00500, Train Gradient: 3.8\n",
      "Epoch 30201/150000, Train Loss: 235, Val Loss: 513,  Learning Rate: 0.00500, Train Gradient: 3.7\n",
      "Epoch 30301/150000, Train Loss: 230, Val Loss: 509,  Learning Rate: 0.00500, Train Gradient: 3.6\n",
      "Epoch 30401/150000, Train Loss: 224, Val Loss: 505,  Learning Rate: 0.00500, Train Gradient: 3.5\n",
      "Epoch 30501/150000, Train Loss: 219, Val Loss: 502,  Learning Rate: 0.00500, Train Gradient: 3.4\n",
      "Epoch 30601/150000, Train Loss: 214, Val Loss: 499,  Learning Rate: 0.00500, Train Gradient: 3.3\n",
      "Epoch 30701/150000, Train Loss: 209, Val Loss: 496,  Learning Rate: 0.00500, Train Gradient: 3.3\n",
      "Epoch 30801/150000, Train Loss: 205, Val Loss: 493,  Learning Rate: 0.00500, Train Gradient: 3.5\n",
      "Epoch 30901/150000, Train Loss: 200, Val Loss: 491,  Learning Rate: 0.00500, Train Gradient: 3.1\n",
      "Epoch 31001/150000, Train Loss: 196, Val Loss: 487,  Learning Rate: 0.00500, Train Gradient: 3.0\n",
      "Epoch 31101/150000, Train Loss: 191, Val Loss: 484,  Learning Rate: 0.00500, Train Gradient: 3.0\n",
      "Epoch 31201/150000, Train Loss: 187, Val Loss: 480,  Learning Rate: 0.00500, Train Gradient: 2.8\n",
      "Epoch 31301/150000, Train Loss: 183, Val Loss: 477,  Learning Rate: 0.00500, Train Gradient: 2.8\n",
      "Epoch 31401/150000, Train Loss: 179, Val Loss: 483,  Learning Rate: 0.00500, Train Gradient: 2.7\n",
      "Epoch 31501/150000, Train Loss: 175, Val Loss: 485,  Learning Rate: 0.00500, Train Gradient: 2.7\n",
      "Epoch 31601/150000, Train Loss: 171, Val Loss: 481,  Learning Rate: 0.00500, Train Gradient: 2.5\n",
      "Epoch 31701/150000, Train Loss: 167, Val Loss: 476,  Learning Rate: 0.00500, Train Gradient: 2.5\n",
      "Epoch 31801/150000, Train Loss: 163, Val Loss: 470,  Learning Rate: 0.00500, Train Gradient: 2.5\n",
      "Epoch 31901/150000, Train Loss: 160, Val Loss: 465,  Learning Rate: 0.00500, Train Gradient: 2.3\n",
      "Epoch 32001/150000, Train Loss: 156, Val Loss: 461,  Learning Rate: 0.00500, Train Gradient: 2.3\n",
      "Epoch 32101/150000, Train Loss: 153, Val Loss: 459,  Learning Rate: 0.00500, Train Gradient: 2.3\n",
      "Epoch 32201/150000, Train Loss: 148, Val Loss: 461,  Learning Rate: 0.00500, Train Gradient: 2.6\n",
      "Epoch 32301/150000, Train Loss: 144, Val Loss: 459,  Learning Rate: 0.00500, Train Gradient: 2.3\n",
      "Epoch 32401/150000, Train Loss: 141, Val Loss: 458,  Learning Rate: 0.00500, Train Gradient: 1.8\n",
      "Epoch 32501/150000, Train Loss: 138, Val Loss: 457,  Learning Rate: 0.00500, Train Gradient: 2.0\n",
      "Early stopping at epoch 32556 with validation loss 457.4826965332031.\n",
      "Test Loss: 395.1988220214844\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3]\n",
    "num_layers_list = [1]\n",
    "\n",
    "learning_rates = [0.005]\n",
    "\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "factors = [0.5]\n",
    "patience_lr = [10]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Learning Rate: {current_lr:.5f}, Train Gradient: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.01, window_size=20, dropout_prob=0, weight_decay=0, factor=0.9, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150000, Train Loss: 29890, Val Loss: 30381,  Learning Rate: 0.01000, Train Gradient: 223.4\n",
      "Epoch 101/150000, Train Loss: 29037, Val Loss: 29514,  Learning Rate: 0.00907, Train Gradient: 215.7\n",
      "Epoch 201/150000, Train Loss: 28114, Val Loss: 28583,  Learning Rate: 0.00913, Train Gradient: 206.9\n",
      "Epoch 301/150000, Train Loss: 27333, Val Loss: 27793,  Learning Rate: 0.00918, Train Gradient: 199.2\n",
      "Epoch 401/150000, Train Loss: 26614, Val Loss: 27066,  Learning Rate: 0.00922, Train Gradient: 191.8\n",
      "Epoch 501/150000, Train Loss: 25941, Val Loss: 26384,  Learning Rate: 0.00926, Train Gradient: 184.7\n",
      "Epoch 601/150000, Train Loss: 25308, Val Loss: 25742,  Learning Rate: 0.00929, Train Gradient: 177.7\n",
      "Epoch 701/150000, Train Loss: 24711, Val Loss: 25137,  Learning Rate: 0.00932, Train Gradient: 170.9\n",
      "Epoch 801/150000, Train Loss: 24148, Val Loss: 24566,  Learning Rate: 0.00935, Train Gradient: 164.1\n",
      "Epoch 901/150000, Train Loss: 23617, Val Loss: 24027,  Learning Rate: 0.00938, Train Gradient: 157.5\n",
      "Epoch 1001/150000, Train Loss: 23117, Val Loss: 23519,  Learning Rate: 0.00941, Train Gradient: 151.0\n",
      "Epoch 1101/150000, Train Loss: 22646, Val Loss: 23041,  Learning Rate: 0.00943, Train Gradient: 144.7\n",
      "Epoch 1201/150000, Train Loss: 22203, Val Loss: 22591,  Learning Rate: 0.00945, Train Gradient: 138.4\n",
      "Epoch 1301/150000, Train Loss: 21605, Val Loss: 22009,  Learning Rate: 0.00948, Train Gradient: 146.3\n",
      "Epoch 1401/150000, Train Loss: 21112, Val Loss: 21510,  Learning Rate: 0.00950, Train Gradient: 144.0\n",
      "Epoch 1501/150000, Train Loss: 20634, Val Loss: 21026,  Learning Rate: 0.00952, Train Gradient: 140.8\n",
      "Epoch 1601/150000, Train Loss: 20169, Val Loss: 20554,  Learning Rate: 0.00954, Train Gradient: 137.7\n",
      "Epoch 1701/150000, Train Loss: 19718, Val Loss: 20094,  Learning Rate: 0.00956, Train Gradient: 134.7\n",
      "Epoch 1801/150000, Train Loss: 19278, Val Loss: 19647,  Learning Rate: 0.00958, Train Gradient: 131.7\n",
      "Epoch 1901/150000, Train Loss: 18851, Val Loss: 19212,  Learning Rate: 0.00960, Train Gradient: 128.8\n",
      "Epoch 2001/150000, Train Loss: 18434, Val Loss: 18788,  Learning Rate: 0.00962, Train Gradient: 126.0\n",
      "Epoch 2101/150000, Train Loss: 18028, Val Loss: 18375,  Learning Rate: 0.00963, Train Gradient: 123.3\n",
      "Epoch 2201/150000, Train Loss: 17632, Val Loss: 17972,  Learning Rate: 0.00965, Train Gradient: 120.6\n",
      "Epoch 2301/150000, Train Loss: 17247, Val Loss: 17579,  Learning Rate: 0.00966, Train Gradient: 117.9\n",
      "Epoch 2401/150000, Train Loss: 16871, Val Loss: 17197,  Learning Rate: 0.00968, Train Gradient: 115.3\n",
      "Epoch 2501/150000, Train Loss: 16506, Val Loss: 16826,  Learning Rate: 0.00969, Train Gradient: 112.7\n",
      "Epoch 2601/150000, Train Loss: 16150, Val Loss: 16465,  Learning Rate: 0.00971, Train Gradient: 110.1\n",
      "Epoch 2701/150000, Train Loss: 15803, Val Loss: 16114,  Learning Rate: 0.00972, Train Gradient: 107.6\n",
      "Epoch 2801/150000, Train Loss: 15466, Val Loss: 15772,  Learning Rate: 0.00973, Train Gradient: 105.2\n",
      "Epoch 2901/150000, Train Loss: 15138, Val Loss: 15439,  Learning Rate: 0.00974, Train Gradient: 102.6\n",
      "Epoch 3001/150000, Train Loss: 14818, Val Loss: 15115,  Learning Rate: 0.00975, Train Gradient: 100.4\n",
      "Epoch 3101/150000, Train Loss: 14507, Val Loss: 14797,  Learning Rate: 0.00976, Train Gradient: 98.2\n",
      "Epoch 3201/150000, Train Loss: 14203, Val Loss: 14490,  Learning Rate: 0.00977, Train Gradient: 96.0\n",
      "Epoch 3301/150000, Train Loss: 13907, Val Loss: 14189,  Learning Rate: 0.00978, Train Gradient: 93.8\n",
      "Epoch 3401/150000, Train Loss: 13619, Val Loss: 13897,  Learning Rate: 0.00979, Train Gradient: 91.6\n",
      "Epoch 3501/150000, Train Loss: 13337, Val Loss: 13611,  Learning Rate: 0.00980, Train Gradient: 89.7\n",
      "Epoch 3601/150000, Train Loss: 13062, Val Loss: 13333,  Learning Rate: 0.00981, Train Gradient: 87.6\n",
      "Epoch 3701/150000, Train Loss: 12794, Val Loss: 13061,  Learning Rate: 0.00981, Train Gradient: 85.8\n",
      "Epoch 3801/150000, Train Loss: 12532, Val Loss: 12795,  Learning Rate: 0.00982, Train Gradient: 84.0\n",
      "Epoch 3901/150000, Train Loss: 12275, Val Loss: 12535,  Learning Rate: 0.00983, Train Gradient: 82.2\n",
      "Epoch 4001/150000, Train Loss: 12025, Val Loss: 12281,  Learning Rate: 0.00984, Train Gradient: 80.5\n",
      "Epoch 4101/150000, Train Loss: 11779, Val Loss: 12032,  Learning Rate: 0.00984, Train Gradient: 78.8\n",
      "Epoch 4201/150000, Train Loss: 11539, Val Loss: 11789,  Learning Rate: 0.00985, Train Gradient: 77.1\n",
      "Epoch 4301/150000, Train Loss: 11304, Val Loss: 11552,  Learning Rate: 0.00985, Train Gradient: 75.5\n",
      "Epoch 4401/150000, Train Loss: 11074, Val Loss: 11319,  Learning Rate: 0.00986, Train Gradient: 73.9\n",
      "Epoch 4501/150000, Train Loss: 10849, Val Loss: 11090,  Learning Rate: 0.00987, Train Gradient: 72.5\n",
      "Epoch 4601/150000, Train Loss: 10628, Val Loss: 10867,  Learning Rate: 0.00987, Train Gradient: 71.0\n",
      "Epoch 4701/150000, Train Loss: 10412, Val Loss: 10649,  Learning Rate: 0.00988, Train Gradient: 69.6\n",
      "Epoch 4801/150000, Train Loss: 10200, Val Loss: 10435,  Learning Rate: 0.00988, Train Gradient: 68.1\n",
      "Epoch 4901/150000, Train Loss: 9992, Val Loss: 10224,  Learning Rate: 0.00989, Train Gradient: 66.9\n",
      "Epoch 5001/150000, Train Loss: 9788, Val Loss: 10017,  Learning Rate: 0.00989, Train Gradient: 65.3\n",
      "Epoch 5101/150000, Train Loss: 9588, Val Loss: 9814,  Learning Rate: 0.00989, Train Gradient: 64.3\n",
      "Epoch 5201/150000, Train Loss: 9391, Val Loss: 9614,  Learning Rate: 0.00990, Train Gradient: 63.3\n",
      "Epoch 5301/150000, Train Loss: 9197, Val Loss: 9418,  Learning Rate: 0.00990, Train Gradient: 62.0\n",
      "Epoch 5401/150000, Train Loss: 9007, Val Loss: 9226,  Learning Rate: 0.00991, Train Gradient: 61.9\n",
      "Epoch 5501/150000, Train Loss: 8819, Val Loss: 9036,  Learning Rate: 0.00991, Train Gradient: 59.8\n",
      "Epoch 5601/150000, Train Loss: 8635, Val Loss: 8848,  Learning Rate: 0.00991, Train Gradient: 58.8\n",
      "Epoch 5701/150000, Train Loss: 8453, Val Loss: 8664,  Learning Rate: 0.00992, Train Gradient: 57.3\n",
      "Epoch 5801/150000, Train Loss: 8274, Val Loss: 8482,  Learning Rate: 0.00992, Train Gradient: 56.8\n",
      "Epoch 5901/150000, Train Loss: 8098, Val Loss: 8303,  Learning Rate: 0.00992, Train Gradient: 55.8\n",
      "Epoch 6001/150000, Train Loss: 7924, Val Loss: 8126,  Learning Rate: 0.00993, Train Gradient: 54.6\n",
      "Epoch 6101/150000, Train Loss: 7753, Val Loss: 7953,  Learning Rate: 0.00993, Train Gradient: 54.0\n",
      "Epoch 6201/150000, Train Loss: 7584, Val Loss: 7781,  Learning Rate: 0.00993, Train Gradient: 53.1\n",
      "Epoch 6301/150000, Train Loss: 7417, Val Loss: 7612,  Learning Rate: 0.00994, Train Gradient: 52.4\n",
      "Epoch 6401/150000, Train Loss: 7253, Val Loss: 7446,  Learning Rate: 0.00994, Train Gradient: 51.3\n",
      "Epoch 6501/150000, Train Loss: 7091, Val Loss: 7283,  Learning Rate: 0.00994, Train Gradient: 50.6\n",
      "Epoch 6601/150000, Train Loss: 6932, Val Loss: 7121,  Learning Rate: 0.00994, Train Gradient: 49.7\n",
      "Epoch 6701/150000, Train Loss: 6775, Val Loss: 6963,  Learning Rate: 0.00995, Train Gradient: 48.8\n",
      "Epoch 6801/150000, Train Loss: 6620, Val Loss: 6806,  Learning Rate: 0.00995, Train Gradient: 48.1\n",
      "Epoch 6901/150000, Train Loss: 6468, Val Loss: 6652,  Learning Rate: 0.00995, Train Gradient: 47.3\n",
      "Epoch 7001/150000, Train Loss: 6318, Val Loss: 6500,  Learning Rate: 0.00995, Train Gradient: 46.5\n",
      "Epoch 7101/150000, Train Loss: 6170, Val Loss: 6351,  Learning Rate: 0.00996, Train Gradient: 45.7\n",
      "Epoch 7201/150000, Train Loss: 6025, Val Loss: 6204,  Learning Rate: 0.00996, Train Gradient: 44.9\n",
      "Epoch 7301/150000, Train Loss: 5882, Val Loss: 6059,  Learning Rate: 0.00996, Train Gradient: 44.2\n",
      "Epoch 7401/150000, Train Loss: 5741, Val Loss: 5917,  Learning Rate: 0.00996, Train Gradient: 43.4\n",
      "Epoch 7501/150000, Train Loss: 5603, Val Loss: 5777,  Learning Rate: 0.00996, Train Gradient: 42.6\n",
      "Epoch 7601/150000, Train Loss: 5467, Val Loss: 5639,  Learning Rate: 0.00997, Train Gradient: 41.9\n",
      "Epoch 7701/150000, Train Loss: 5333, Val Loss: 5505,  Learning Rate: 0.00997, Train Gradient: 41.1\n",
      "Epoch 7801/150000, Train Loss: 5202, Val Loss: 5373,  Learning Rate: 0.00997, Train Gradient: 40.4\n",
      "Epoch 7901/150000, Train Loss: 5074, Val Loss: 5243,  Learning Rate: 0.00997, Train Gradient: 39.6\n",
      "Epoch 8001/150000, Train Loss: 4948, Val Loss: 5116,  Learning Rate: 0.00997, Train Gradient: 39.0\n",
      "Epoch 8101/150000, Train Loss: 4824, Val Loss: 4991,  Learning Rate: 0.00997, Train Gradient: 38.3\n",
      "Epoch 8201/150000, Train Loss: 4702, Val Loss: 4869,  Learning Rate: 0.00997, Train Gradient: 37.9\n",
      "Epoch 8301/150000, Train Loss: 4582, Val Loss: 4748,  Learning Rate: 0.00998, Train Gradient: 36.9\n",
      "Epoch 8401/150000, Train Loss: 4464, Val Loss: 4631,  Learning Rate: 0.00998, Train Gradient: 36.3\n",
      "Epoch 8501/150000, Train Loss: 4349, Val Loss: 4515,  Learning Rate: 0.00998, Train Gradient: 35.6\n",
      "Epoch 8601/150000, Train Loss: 4236, Val Loss: 4401,  Learning Rate: 0.00998, Train Gradient: 35.0\n",
      "Epoch 8701/150000, Train Loss: 4125, Val Loss: 4290,  Learning Rate: 0.00998, Train Gradient: 34.3\n",
      "Epoch 8801/150000, Train Loss: 4016, Val Loss: 4181,  Learning Rate: 0.00998, Train Gradient: 33.7\n",
      "Epoch 8901/150000, Train Loss: 3909, Val Loss: 4073,  Learning Rate: 0.00998, Train Gradient: 33.0\n",
      "Epoch 9001/150000, Train Loss: 3804, Val Loss: 3969,  Learning Rate: 0.00998, Train Gradient: 32.4\n",
      "Epoch 9101/150000, Train Loss: 3702, Val Loss: 3866,  Learning Rate: 0.00998, Train Gradient: 31.8\n",
      "Epoch 9201/150000, Train Loss: 3601, Val Loss: 3766,  Learning Rate: 0.00998, Train Gradient: 31.1\n",
      "Epoch 9301/150000, Train Loss: 3503, Val Loss: 3667,  Learning Rate: 0.00999, Train Gradient: 30.6\n",
      "Epoch 9401/150000, Train Loss: 3407, Val Loss: 3571,  Learning Rate: 0.00999, Train Gradient: 30.2\n",
      "Epoch 9501/150000, Train Loss: 3313, Val Loss: 3477,  Learning Rate: 0.00999, Train Gradient: 29.3\n",
      "Epoch 9601/150000, Train Loss: 3221, Val Loss: 3385,  Learning Rate: 0.00999, Train Gradient: 28.7\n",
      "Epoch 9701/150000, Train Loss: 3132, Val Loss: 3296,  Learning Rate: 0.00999, Train Gradient: 28.2\n",
      "Epoch 9801/150000, Train Loss: 3044, Val Loss: 3208,  Learning Rate: 0.00999, Train Gradient: 27.5\n",
      "Epoch 9901/150000, Train Loss: 2958, Val Loss: 3122,  Learning Rate: 0.00999, Train Gradient: 27.0\n",
      "Epoch 10001/150000, Train Loss: 2874, Val Loss: 3038,  Learning Rate: 0.00999, Train Gradient: 26.0\n",
      "Epoch 10101/150000, Train Loss: 2792, Val Loss: 2955,  Learning Rate: 0.00999, Train Gradient: 25.9\n",
      "Epoch 10201/150000, Train Loss: 2712, Val Loss: 2875,  Learning Rate: 0.00999, Train Gradient: 25.3\n",
      "Epoch 10301/150000, Train Loss: 2633, Val Loss: 2797,  Learning Rate: 0.00999, Train Gradient: 24.8\n",
      "Epoch 10401/150000, Train Loss: 2557, Val Loss: 2721,  Learning Rate: 0.00999, Train Gradient: 24.3\n",
      "Epoch 10501/150000, Train Loss: 2482, Val Loss: 2646,  Learning Rate: 0.00999, Train Gradient: 23.7\n",
      "Epoch 10601/150000, Train Loss: 2409, Val Loss: 2574,  Learning Rate: 0.00999, Train Gradient: 23.4\n",
      "Epoch 10701/150000, Train Loss: 2338, Val Loss: 2503,  Learning Rate: 0.00999, Train Gradient: 22.7\n",
      "Epoch 10801/150000, Train Loss: 2268, Val Loss: 2435,  Learning Rate: 0.00999, Train Gradient: 21.5\n",
      "Epoch 10901/150000, Train Loss: 2200, Val Loss: 2367,  Learning Rate: 0.00999, Train Gradient: 21.8\n",
      "Epoch 11001/150000, Train Loss: 2133, Val Loss: 2301,  Learning Rate: 0.00999, Train Gradient: 21.3\n",
      "Epoch 11101/150000, Train Loss: 2068, Val Loss: 2236,  Learning Rate: 0.00999, Train Gradient: 20.8\n",
      "Epoch 11201/150000, Train Loss: 2004, Val Loss: 2173,  Learning Rate: 0.00999, Train Gradient: 20.4\n",
      "Epoch 11301/150000, Train Loss: 1942, Val Loss: 2112,  Learning Rate: 0.01000, Train Gradient: 20.0\n",
      "Epoch 11401/150000, Train Loss: 1882, Val Loss: 2052,  Learning Rate: 0.01000, Train Gradient: 19.5\n",
      "Epoch 11501/150000, Train Loss: 1822, Val Loss: 1994,  Learning Rate: 0.01000, Train Gradient: 19.1\n",
      "Epoch 11601/150000, Train Loss: 1765, Val Loss: 1937,  Learning Rate: 0.01000, Train Gradient: 18.6\n",
      "Epoch 11701/150000, Train Loss: 1709, Val Loss: 1882,  Learning Rate: 0.01000, Train Gradient: 18.1\n",
      "Epoch 11801/150000, Train Loss: 1654, Val Loss: 1828,  Learning Rate: 0.01000, Train Gradient: 17.7\n",
      "Epoch 11901/150000, Train Loss: 1601, Val Loss: 1775,  Learning Rate: 0.01000, Train Gradient: 17.9\n",
      "Epoch 12001/150000, Train Loss: 1549, Val Loss: 1725,  Learning Rate: 0.01000, Train Gradient: 16.9\n",
      "Epoch 12101/150000, Train Loss: 1498, Val Loss: 1675,  Learning Rate: 0.01000, Train Gradient: 16.5\n",
      "Epoch 12201/150000, Train Loss: 1449, Val Loss: 1627,  Learning Rate: 0.01000, Train Gradient: 16.1\n",
      "Epoch 12301/150000, Train Loss: 1402, Val Loss: 1581,  Learning Rate: 0.01000, Train Gradient: 15.7\n",
      "Epoch 12401/150000, Train Loss: 1355, Val Loss: 1536,  Learning Rate: 0.01000, Train Gradient: 15.4\n",
      "Epoch 12501/150000, Train Loss: 1309, Val Loss: 1492,  Learning Rate: 0.01000, Train Gradient: 15.0\n",
      "Epoch 12601/150000, Train Loss: 1265, Val Loss: 1450,  Learning Rate: 0.01000, Train Gradient: 14.7\n",
      "Epoch 12701/150000, Train Loss: 1222, Val Loss: 1409,  Learning Rate: 0.01000, Train Gradient: 14.3\n",
      "Epoch 12801/150000, Train Loss: 1180, Val Loss: 1368,  Learning Rate: 0.01000, Train Gradient: 14.0\n",
      "Epoch 12901/150000, Train Loss: 1139, Val Loss: 1329,  Learning Rate: 0.01000, Train Gradient: 13.6\n",
      "Epoch 13001/150000, Train Loss: 1100, Val Loss: 1292,  Learning Rate: 0.01000, Train Gradient: 11.8\n",
      "Epoch 13101/150000, Train Loss: 1061, Val Loss: 1255,  Learning Rate: 0.01000, Train Gradient: 12.9\n",
      "Epoch 13201/150000, Train Loss: 1023, Val Loss: 1219,  Learning Rate: 0.01000, Train Gradient: 12.5\n",
      "Epoch 13301/150000, Train Loss: 987, Val Loss: 1185,  Learning Rate: 0.01000, Train Gradient: 12.0\n",
      "Epoch 13401/150000, Train Loss: 952, Val Loss: 1152,  Learning Rate: 0.01000, Train Gradient: 11.9\n",
      "Epoch 13501/150000, Train Loss: 918, Val Loss: 1120,  Learning Rate: 0.01000, Train Gradient: 11.5\n",
      "Epoch 13601/150000, Train Loss: 884, Val Loss: 1089,  Learning Rate: 0.01000, Train Gradient: 11.3\n",
      "Epoch 13701/150000, Train Loss: 852, Val Loss: 1060,  Learning Rate: 0.01000, Train Gradient: 10.1\n",
      "Epoch 13801/150000, Train Loss: 821, Val Loss: 1030,  Learning Rate: 0.01000, Train Gradient: 10.7\n",
      "Epoch 13901/150000, Train Loss: 791, Val Loss: 1003,  Learning Rate: 0.01000, Train Gradient: 10.4\n",
      "Epoch 14001/150000, Train Loss: 761, Val Loss: 976,  Learning Rate: 0.01000, Train Gradient: 9.8\n",
      "Epoch 14101/150000, Train Loss: 733, Val Loss: 951,  Learning Rate: 0.01000, Train Gradient: 9.8\n",
      "Epoch 14201/150000, Train Loss: 705, Val Loss: 926,  Learning Rate: 0.01000, Train Gradient: 9.6\n",
      "Epoch 14301/150000, Train Loss: 679, Val Loss: 902,  Learning Rate: 0.01000, Train Gradient: 9.3\n",
      "Epoch 14401/150000, Train Loss: 653, Val Loss: 879,  Learning Rate: 0.01000, Train Gradient: 9.0\n",
      "Epoch 14501/150000, Train Loss: 628, Val Loss: 857,  Learning Rate: 0.01000, Train Gradient: 8.8\n",
      "Epoch 14601/150000, Train Loss: 604, Val Loss: 836,  Learning Rate: 0.01000, Train Gradient: 8.5\n",
      "Epoch 14701/150000, Train Loss: 581, Val Loss: 815,  Learning Rate: 0.01000, Train Gradient: 8.2\n",
      "Epoch 14801/150000, Train Loss: 558, Val Loss: 795,  Learning Rate: 0.01000, Train Gradient: 8.0\n",
      "Epoch 14901/150000, Train Loss: 536, Val Loss: 775,  Learning Rate: 0.01000, Train Gradient: 7.8\n",
      "Epoch 15001/150000, Train Loss: 515, Val Loss: 756,  Learning Rate: 0.01000, Train Gradient: 7.5\n",
      "Epoch 15101/150000, Train Loss: 495, Val Loss: 738,  Learning Rate: 0.01000, Train Gradient: 7.2\n",
      "Epoch 15201/150000, Train Loss: 476, Val Loss: 720,  Learning Rate: 0.01000, Train Gradient: 7.0\n",
      "Epoch 15301/150000, Train Loss: 457, Val Loss: 703,  Learning Rate: 0.01000, Train Gradient: 6.5\n",
      "Epoch 15401/150000, Train Loss: 439, Val Loss: 687,  Learning Rate: 0.01000, Train Gradient: 6.6\n",
      "Epoch 15501/150000, Train Loss: 422, Val Loss: 673,  Learning Rate: 0.01000, Train Gradient: 4.9\n",
      "Epoch 15601/150000, Train Loss: 404, Val Loss: 655,  Learning Rate: 0.01000, Train Gradient: 6.2\n",
      "Epoch 15701/150000, Train Loss: 388, Val Loss: 640,  Learning Rate: 0.01000, Train Gradient: 6.0\n",
      "Epoch 15801/150000, Train Loss: 372, Val Loss: 626,  Learning Rate: 0.01000, Train Gradient: 5.7\n",
      "Epoch 15901/150000, Train Loss: 357, Val Loss: 612,  Learning Rate: 0.01000, Train Gradient: 5.6\n",
      "Epoch 16001/150000, Train Loss: 343, Val Loss: 600,  Learning Rate: 0.01000, Train Gradient: 5.4\n",
      "Epoch 16101/150000, Train Loss: 329, Val Loss: 588,  Learning Rate: 0.01000, Train Gradient: 5.2\n",
      "Epoch 16201/150000, Train Loss: 315, Val Loss: 577,  Learning Rate: 0.01000, Train Gradient: 5.0\n",
      "Epoch 16301/150000, Train Loss: 303, Val Loss: 566,  Learning Rate: 0.01000, Train Gradient: 6.4\n",
      "Epoch 16401/150000, Train Loss: 290, Val Loss: 556,  Learning Rate: 0.01000, Train Gradient: 4.7\n",
      "Epoch 16501/150000, Train Loss: 279, Val Loss: 547,  Learning Rate: 0.01000, Train Gradient: 4.5\n",
      "Epoch 16601/150000, Train Loss: 268, Val Loss: 538,  Learning Rate: 0.01000, Train Gradient: 4.3\n",
      "Epoch 16701/150000, Train Loss: 257, Val Loss: 530,  Learning Rate: 0.01000, Train Gradient: 4.1\n",
      "Epoch 16801/150000, Train Loss: 247, Val Loss: 523,  Learning Rate: 0.01000, Train Gradient: 4.0\n",
      "Epoch 16901/150000, Train Loss: 237, Val Loss: 516,  Learning Rate: 0.01000, Train Gradient: 3.8\n",
      "Epoch 17001/150000, Train Loss: 229, Val Loss: 510,  Learning Rate: 0.01000, Train Gradient: 3.7\n",
      "Epoch 17101/150000, Train Loss: 220, Val Loss: 504,  Learning Rate: 0.01000, Train Gradient: 3.5\n",
      "Epoch 17201/150000, Train Loss: 210, Val Loss: 505,  Learning Rate: 0.01000, Train Gradient: 3.5\n",
      "Epoch 17301/150000, Train Loss: 201, Val Loss: 499,  Learning Rate: 0.01000, Train Gradient: 3.2\n",
      "Epoch 17401/150000, Train Loss: 198, Val Loss: 494,  Learning Rate: 0.01000, Train Gradient: 3.7\n",
      "Early stopping at epoch 17411 with validation loss 512.159423828125.\n",
      "Test Loss: 471.7339172363281\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3]\n",
    "num_layers_list = [1]\n",
    "\n",
    "learning_rates = [0.01]\n",
    "\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Learning Rate: {current_lr:.5f}, Train Gradient: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.01, window_size=20, dropout_prob=0, weight_decay=0, factor=0.9, patience=10\n",
      "Epoch 1/150000, Train Loss: 29890, Val Loss: 30388,  Learning Rate: 0.00040, Train Gradient: 223.4\n",
      "Epoch 101/150000, Train Loss: 29176, Val Loss: 29655,  Learning Rate: 0.00750, Train Gradient: 217.0\n",
      "Epoch 201/150000, Train Loss: 28391, Val Loss: 28865,  Learning Rate: 0.00727, Train Gradient: 209.6\n",
      "Epoch 301/150000, Train Loss: 27758, Val Loss: 28225,  Learning Rate: 0.00707, Train Gradient: 203.4\n",
      "Epoch 401/150000, Train Loss: 27188, Val Loss: 27649,  Learning Rate: 0.00689, Train Gradient: 197.7\n",
      "Epoch 501/150000, Train Loss: 26663, Val Loss: 27117,  Learning Rate: 0.00672, Train Gradient: 192.4\n",
      "Epoch 601/150000, Train Loss: 26175, Val Loss: 26623,  Learning Rate: 0.00656, Train Gradient: 187.2\n",
      "Epoch 701/150000, Train Loss: 25719, Val Loss: 26162,  Learning Rate: 0.00642, Train Gradient: 182.3\n",
      "Epoch 801/150000, Train Loss: 25292, Val Loss: 25729,  Learning Rate: 0.00627, Train Gradient: 177.5\n",
      "Epoch 901/150000, Train Loss: 24891, Val Loss: 25322,  Learning Rate: 0.00614, Train Gradient: 172.9\n",
      "Epoch 1001/150000, Train Loss: 24512, Val Loss: 24938,  Learning Rate: 0.00601, Train Gradient: 168.5\n",
      "Epoch 1101/150000, Train Loss: 24155, Val Loss: 24576,  Learning Rate: 0.00589, Train Gradient: 164.2\n",
      "Epoch 1201/150000, Train Loss: 23817, Val Loss: 24233,  Learning Rate: 0.00578, Train Gradient: 160.1\n",
      "Epoch 1301/150000, Train Loss: 23498, Val Loss: 23909,  Learning Rate: 0.00567, Train Gradient: 156.0\n",
      "Epoch 1401/150000, Train Loss: 23195, Val Loss: 23601,  Learning Rate: 0.00557, Train Gradient: 152.1\n",
      "Epoch 1501/150000, Train Loss: 22908, Val Loss: 23309,  Learning Rate: 0.00547, Train Gradient: 148.3\n",
      "Epoch 1601/150000, Train Loss: 22635, Val Loss: 23032,  Learning Rate: 0.00538, Train Gradient: 144.5\n",
      "Epoch 1701/150000, Train Loss: 22376, Val Loss: 22768,  Learning Rate: 0.00529, Train Gradient: 140.9\n",
      "Epoch 1801/150000, Train Loss: 22003, Val Loss: 22417,  Learning Rate: 0.00517, Train Gradient: 148.0\n",
      "Epoch 1901/150000, Train Loss: 21712, Val Loss: 22121,  Learning Rate: 0.00507, Train Gradient: 148.4\n",
      "Epoch 2001/150000, Train Loss: 21437, Val Loss: 21838,  Learning Rate: 0.00498, Train Gradient: 146.7\n",
      "Epoch 2101/150000, Train Loss: 21174, Val Loss: 21570,  Learning Rate: 0.00489, Train Gradient: 144.8\n",
      "Epoch 2201/150000, Train Loss: 20920, Val Loss: 21312,  Learning Rate: 0.00480, Train Gradient: 143.0\n",
      "Epoch 2301/150000, Train Loss: 20674, Val Loss: 21062,  Learning Rate: 0.00472, Train Gradient: 141.3\n",
      "Epoch 2401/150000, Train Loss: 20435, Val Loss: 20820,  Learning Rate: 0.00464, Train Gradient: 139.7\n",
      "Epoch 2501/150000, Train Loss: 20204, Val Loss: 20585,  Learning Rate: 0.00456, Train Gradient: 138.1\n",
      "Epoch 2601/150000, Train Loss: 19979, Val Loss: 20356,  Learning Rate: 0.00449, Train Gradient: 136.5\n",
      "Epoch 2701/150000, Train Loss: 19761, Val Loss: 20134,  Learning Rate: 0.00441, Train Gradient: 135.0\n",
      "Epoch 2801/150000, Train Loss: 19549, Val Loss: 19917,  Learning Rate: 0.00434, Train Gradient: 133.6\n",
      "Epoch 2901/150000, Train Loss: 19343, Val Loss: 19707,  Learning Rate: 0.00427, Train Gradient: 132.2\n",
      "Epoch 3001/150000, Train Loss: 19142, Val Loss: 19503,  Learning Rate: 0.00420, Train Gradient: 130.8\n",
      "Epoch 3101/150000, Train Loss: 18946, Val Loss: 19304,  Learning Rate: 0.00414, Train Gradient: 129.5\n",
      "Epoch 3201/150000, Train Loss: 18756, Val Loss: 19110,  Learning Rate: 0.00408, Train Gradient: 128.2\n",
      "Epoch 3301/150000, Train Loss: 18570, Val Loss: 18921,  Learning Rate: 0.00401, Train Gradient: 127.0\n",
      "Epoch 3401/150000, Train Loss: 18389, Val Loss: 18737,  Learning Rate: 0.00395, Train Gradient: 125.7\n",
      "Epoch 3501/150000, Train Loss: 18212, Val Loss: 18557,  Learning Rate: 0.00390, Train Gradient: 124.5\n",
      "Epoch 3601/150000, Train Loss: 18039, Val Loss: 18382,  Learning Rate: 0.00384, Train Gradient: 123.4\n",
      "Epoch 3701/150000, Train Loss: 17870, Val Loss: 18210,  Learning Rate: 0.00378, Train Gradient: 122.2\n",
      "Epoch 3801/150000, Train Loss: 17705, Val Loss: 18043,  Learning Rate: 0.00373, Train Gradient: 121.1\n",
      "Epoch 3901/150000, Train Loss: 17544, Val Loss: 17879,  Learning Rate: 0.00368, Train Gradient: 120.0\n",
      "Epoch 4001/150000, Train Loss: 17386, Val Loss: 17720,  Learning Rate: 0.00363, Train Gradient: 118.9\n",
      "Epoch 4101/150000, Train Loss: 17232, Val Loss: 17563,  Learning Rate: 0.00358, Train Gradient: 117.8\n",
      "Epoch 4201/150000, Train Loss: 17082, Val Loss: 17410,  Learning Rate: 0.00353, Train Gradient: 116.8\n",
      "Epoch 4301/150000, Train Loss: 16934, Val Loss: 17260,  Learning Rate: 0.00348, Train Gradient: 115.7\n",
      "Epoch 4401/150000, Train Loss: 16790, Val Loss: 17114,  Learning Rate: 0.00344, Train Gradient: 114.7\n",
      "Epoch 4501/150000, Train Loss: 16649, Val Loss: 16971,  Learning Rate: 0.00339, Train Gradient: 113.7\n",
      "Epoch 4601/150000, Train Loss: 16510, Val Loss: 16830,  Learning Rate: 0.00335, Train Gradient: 112.7\n",
      "Epoch 4701/150000, Train Loss: 16375, Val Loss: 16693,  Learning Rate: 0.00331, Train Gradient: 111.8\n",
      "Epoch 4801/150000, Train Loss: 16243, Val Loss: 16558,  Learning Rate: 0.00327, Train Gradient: 110.8\n",
      "Epoch 4901/150000, Train Loss: 16113, Val Loss: 16426,  Learning Rate: 0.00323, Train Gradient: 109.9\n",
      "Epoch 5001/150000, Train Loss: 15985, Val Loss: 16297,  Learning Rate: 0.00319, Train Gradient: 108.9\n",
      "Epoch 5101/150000, Train Loss: 15861, Val Loss: 16170,  Learning Rate: 0.00315, Train Gradient: 108.0\n",
      "Epoch 5201/150000, Train Loss: 15738, Val Loss: 16046,  Learning Rate: 0.00311, Train Gradient: 107.2\n",
      "Epoch 5301/150000, Train Loss: 15618, Val Loss: 15924,  Learning Rate: 0.00307, Train Gradient: 106.3\n",
      "Epoch 5401/150000, Train Loss: 15501, Val Loss: 15805,  Learning Rate: 0.00304, Train Gradient: 105.4\n",
      "Epoch 5501/150000, Train Loss: 15385, Val Loss: 15687,  Learning Rate: 0.00300, Train Gradient: 104.6\n",
      "Epoch 5601/150000, Train Loss: 15272, Val Loss: 15572,  Learning Rate: 0.00297, Train Gradient: 103.7\n",
      "Epoch 5701/150000, Train Loss: 15161, Val Loss: 15459,  Learning Rate: 0.00294, Train Gradient: 102.9\n",
      "Epoch 5801/150000, Train Loss: 15052, Val Loss: 15348,  Learning Rate: 0.00290, Train Gradient: 102.1\n",
      "Epoch 5901/150000, Train Loss: 14944, Val Loss: 15239,  Learning Rate: 0.00287, Train Gradient: 101.4\n",
      "Epoch 6001/150000, Train Loss: 14839, Val Loss: 15132,  Learning Rate: 0.00284, Train Gradient: 100.6\n",
      "Epoch 6101/150000, Train Loss: 14736, Val Loss: 15027,  Learning Rate: 0.00281, Train Gradient: 99.9\n",
      "Epoch 6201/150000, Train Loss: 14634, Val Loss: 14924,  Learning Rate: 0.00278, Train Gradient: 99.1\n",
      "Epoch 6301/150000, Train Loss: 14534, Val Loss: 14822,  Learning Rate: 0.00275, Train Gradient: 98.4\n",
      "Epoch 6401/150000, Train Loss: 14435, Val Loss: 14723,  Learning Rate: 0.00272, Train Gradient: 97.7\n",
      "Epoch 6501/150000, Train Loss: 14339, Val Loss: 14625,  Learning Rate: 0.00269, Train Gradient: 97.0\n",
      "Epoch 6601/150000, Train Loss: 14244, Val Loss: 14528,  Learning Rate: 0.00267, Train Gradient: 96.3\n",
      "Epoch 6701/150000, Train Loss: 14150, Val Loss: 14433,  Learning Rate: 0.00264, Train Gradient: 95.6\n",
      "Epoch 6801/150000, Train Loss: 14058, Val Loss: 14340,  Learning Rate: 0.00261, Train Gradient: 95.0\n",
      "Epoch 6901/150000, Train Loss: 13968, Val Loss: 14248,  Learning Rate: 0.00259, Train Gradient: 94.3\n",
      "Epoch 7001/150000, Train Loss: 13878, Val Loss: 14158,  Learning Rate: 0.00256, Train Gradient: 93.6\n",
      "Epoch 7101/150000, Train Loss: 13791, Val Loss: 14069,  Learning Rate: 0.00254, Train Gradient: 93.0\n",
      "Epoch 7201/150000, Train Loss: 13704, Val Loss: 13982,  Learning Rate: 0.00251, Train Gradient: 92.4\n",
      "Epoch 7301/150000, Train Loss: 13620, Val Loss: 13895,  Learning Rate: 0.00249, Train Gradient: 91.8\n",
      "Epoch 7401/150000, Train Loss: 13536, Val Loss: 13810,  Learning Rate: 0.00246, Train Gradient: 91.2\n",
      "Epoch 7501/150000, Train Loss: 13453, Val Loss: 13727,  Learning Rate: 0.00244, Train Gradient: 90.6\n",
      "Epoch 7601/150000, Train Loss: 13372, Val Loss: 13645,  Learning Rate: 0.00242, Train Gradient: 90.0\n",
      "Epoch 7701/150000, Train Loss: 13292, Val Loss: 13563,  Learning Rate: 0.00240, Train Gradient: 89.4\n",
      "Epoch 7801/150000, Train Loss: 13213, Val Loss: 13483,  Learning Rate: 0.00237, Train Gradient: 88.9\n",
      "Epoch 7901/150000, Train Loss: 13136, Val Loss: 13405,  Learning Rate: 0.00235, Train Gradient: 88.3\n",
      "Epoch 8001/150000, Train Loss: 13059, Val Loss: 13327,  Learning Rate: 0.00233, Train Gradient: 87.7\n",
      "Epoch 8101/150000, Train Loss: 12984, Val Loss: 13251,  Learning Rate: 0.00231, Train Gradient: 87.2\n",
      "Epoch 8201/150000, Train Loss: 12909, Val Loss: 13175,  Learning Rate: 0.00229, Train Gradient: 86.7\n",
      "Epoch 8301/150000, Train Loss: 12836, Val Loss: 13101,  Learning Rate: 0.00227, Train Gradient: 86.2\n",
      "Epoch 8401/150000, Train Loss: 12763, Val Loss: 13027,  Learning Rate: 0.00225, Train Gradient: 85.6\n",
      "Epoch 8501/150000, Train Loss: 12692, Val Loss: 12955,  Learning Rate: 0.00223, Train Gradient: 85.2\n",
      "Epoch 8601/150000, Train Loss: 12622, Val Loss: 12883,  Learning Rate: 0.00221, Train Gradient: 84.6\n",
      "Epoch 8701/150000, Train Loss: 12552, Val Loss: 12813,  Learning Rate: 0.00220, Train Gradient: 84.1\n",
      "Epoch 8801/150000, Train Loss: 12484, Val Loss: 12743,  Learning Rate: 0.00218, Train Gradient: 83.7\n",
      "Epoch 8901/150000, Train Loss: 12416, Val Loss: 12675,  Learning Rate: 0.00216, Train Gradient: 83.2\n",
      "Epoch 9001/150000, Train Loss: 12349, Val Loss: 12607,  Learning Rate: 0.00214, Train Gradient: 82.7\n",
      "Epoch 9101/150000, Train Loss: 12283, Val Loss: 12540,  Learning Rate: 0.00213, Train Gradient: 82.3\n",
      "Epoch 9201/150000, Train Loss: 12218, Val Loss: 12474,  Learning Rate: 0.00211, Train Gradient: 81.8\n",
      "Epoch 9301/150000, Train Loss: 12154, Val Loss: 12409,  Learning Rate: 0.00209, Train Gradient: 81.4\n",
      "Epoch 9401/150000, Train Loss: 12090, Val Loss: 12345,  Learning Rate: 0.00208, Train Gradient: 80.9\n",
      "Epoch 9501/150000, Train Loss: 12028, Val Loss: 12281,  Learning Rate: 0.00206, Train Gradient: 80.5\n",
      "Epoch 9601/150000, Train Loss: 11966, Val Loss: 12219,  Learning Rate: 0.00204, Train Gradient: 80.1\n",
      "Epoch 9701/150000, Train Loss: 11905, Val Loss: 12157,  Learning Rate: 0.00203, Train Gradient: 79.7\n",
      "Epoch 9801/150000, Train Loss: 11844, Val Loss: 12096,  Learning Rate: 0.00201, Train Gradient: 79.3\n",
      "Epoch 9901/150000, Train Loss: 11785, Val Loss: 12035,  Learning Rate: 0.00200, Train Gradient: 78.9\n",
      "Epoch 10001/150000, Train Loss: 11726, Val Loss: 11976,  Learning Rate: 0.00198, Train Gradient: 78.4\n",
      "Epoch 10101/150000, Train Loss: 11667, Val Loss: 11917,  Learning Rate: 0.00197, Train Gradient: 78.1\n",
      "Epoch 10201/150000, Train Loss: 11610, Val Loss: 11858,  Learning Rate: 0.00195, Train Gradient: 77.7\n",
      "Epoch 10301/150000, Train Loss: 11553, Val Loss: 11801,  Learning Rate: 0.00194, Train Gradient: 77.3\n",
      "Epoch 10401/150000, Train Loss: 11497, Val Loss: 11744,  Learning Rate: 0.00193, Train Gradient: 76.9\n",
      "Epoch 10501/150000, Train Loss: 11441, Val Loss: 11688,  Learning Rate: 0.00191, Train Gradient: 76.5\n",
      "Epoch 10601/150000, Train Loss: 11386, Val Loss: 11632,  Learning Rate: 0.00190, Train Gradient: 76.1\n",
      "Epoch 10701/150000, Train Loss: 11332, Val Loss: 11576,  Learning Rate: 0.00188, Train Gradient: 75.8\n",
      "Epoch 10801/150000, Train Loss: 11278, Val Loss: 11522,  Learning Rate: 0.00187, Train Gradient: 75.4\n",
      "Epoch 10901/150000, Train Loss: 11225, Val Loss: 11467,  Learning Rate: 0.00186, Train Gradient: 75.0\n",
      "Epoch 11001/150000, Train Loss: 11172, Val Loss: 11414,  Learning Rate: 0.00185, Train Gradient: 74.7\n",
      "Epoch 11101/150000, Train Loss: 11120, Val Loss: 11362,  Learning Rate: 0.00183, Train Gradient: 74.3\n",
      "Epoch 11201/150000, Train Loss: 11069, Val Loss: 11310,  Learning Rate: 0.00182, Train Gradient: 74.0\n",
      "Epoch 11301/150000, Train Loss: 11018, Val Loss: 11258,  Learning Rate: 0.00181, Train Gradient: 73.7\n",
      "Epoch 11401/150000, Train Loss: 10968, Val Loss: 11208,  Learning Rate: 0.00180, Train Gradient: 73.3\n",
      "Epoch 11501/150000, Train Loss: 10918, Val Loss: 11158,  Learning Rate: 0.00178, Train Gradient: 73.0\n",
      "Epoch 11601/150000, Train Loss: 10869, Val Loss: 11108,  Learning Rate: 0.00177, Train Gradient: 72.6\n",
      "Epoch 11701/150000, Train Loss: 10820, Val Loss: 11059,  Learning Rate: 0.00176, Train Gradient: 72.3\n",
      "Epoch 11801/150000, Train Loss: 10772, Val Loss: 11010,  Learning Rate: 0.00175, Train Gradient: 72.0\n",
      "Epoch 11901/150000, Train Loss: 10725, Val Loss: 10962,  Learning Rate: 0.00174, Train Gradient: 71.7\n",
      "Epoch 12001/150000, Train Loss: 10678, Val Loss: 10914,  Learning Rate: 0.00173, Train Gradient: 71.4\n",
      "Epoch 12101/150000, Train Loss: 10631, Val Loss: 10867,  Learning Rate: 0.00172, Train Gradient: 71.1\n",
      "Epoch 12201/150000, Train Loss: 10585, Val Loss: 10820,  Learning Rate: 0.00171, Train Gradient: 70.8\n",
      "Epoch 12301/150000, Train Loss: 10539, Val Loss: 10774,  Learning Rate: 0.00170, Train Gradient: 70.5\n",
      "Epoch 12401/150000, Train Loss: 10494, Val Loss: 10728,  Learning Rate: 0.00168, Train Gradient: 70.2\n",
      "Epoch 12501/150000, Train Loss: 10449, Val Loss: 10683,  Learning Rate: 0.00167, Train Gradient: 69.9\n",
      "Epoch 12601/150000, Train Loss: 10404, Val Loss: 10638,  Learning Rate: 0.00166, Train Gradient: 69.6\n",
      "Epoch 12701/150000, Train Loss: 10360, Val Loss: 10593,  Learning Rate: 0.00165, Train Gradient: 69.3\n",
      "Epoch 12801/150000, Train Loss: 10317, Val Loss: 10549,  Learning Rate: 0.00164, Train Gradient: 69.0\n",
      "Epoch 12901/150000, Train Loss: 10274, Val Loss: 10506,  Learning Rate: 0.00163, Train Gradient: 68.7\n",
      "Epoch 13001/150000, Train Loss: 10231, Val Loss: 10462,  Learning Rate: 0.00162, Train Gradient: 68.5\n",
      "Epoch 13101/150000, Train Loss: 10189, Val Loss: 10419,  Learning Rate: 0.00162, Train Gradient: 68.2\n",
      "Epoch 13201/150000, Train Loss: 10147, Val Loss: 10377,  Learning Rate: 0.00161, Train Gradient: 67.9\n",
      "Epoch 13301/150000, Train Loss: 10105, Val Loss: 10335,  Learning Rate: 0.00160, Train Gradient: 67.6\n",
      "Epoch 13401/150000, Train Loss: 10064, Val Loss: 10293,  Learning Rate: 0.00159, Train Gradient: 67.4\n",
      "Epoch 13501/150000, Train Loss: 10023, Val Loss: 10252,  Learning Rate: 0.00158, Train Gradient: 67.1\n",
      "Epoch 13601/150000, Train Loss: 9983, Val Loss: 10211,  Learning Rate: 0.00157, Train Gradient: 66.9\n",
      "Epoch 13701/150000, Train Loss: 9943, Val Loss: 10170,  Learning Rate: 0.00156, Train Gradient: 66.6\n",
      "Epoch 13801/150000, Train Loss: 9903, Val Loss: 10130,  Learning Rate: 0.00155, Train Gradient: 66.4\n",
      "Epoch 13901/150000, Train Loss: 9864, Val Loss: 10090,  Learning Rate: 0.00154, Train Gradient: 66.1\n",
      "Epoch 14001/150000, Train Loss: 9825, Val Loss: 10050,  Learning Rate: 0.00153, Train Gradient: 65.9\n",
      "Epoch 14101/150000, Train Loss: 9786, Val Loss: 10011,  Learning Rate: 0.00153, Train Gradient: 65.6\n",
      "Epoch 14201/150000, Train Loss: 9748, Val Loss: 9972,  Learning Rate: 0.00152, Train Gradient: 65.4\n",
      "Epoch 14301/150000, Train Loss: 9710, Val Loss: 9934,  Learning Rate: 0.00151, Train Gradient: 65.1\n",
      "Epoch 14401/150000, Train Loss: 9673, Val Loss: 9896,  Learning Rate: 0.00150, Train Gradient: 64.9\n",
      "Epoch 14501/150000, Train Loss: 9635, Val Loss: 9858,  Learning Rate: 0.00149, Train Gradient: 64.7\n",
      "Epoch 14601/150000, Train Loss: 9598, Val Loss: 9820,  Learning Rate: 0.00148, Train Gradient: 64.4\n",
      "Epoch 14701/150000, Train Loss: 9562, Val Loss: 9783,  Learning Rate: 0.00148, Train Gradient: 64.2\n",
      "Epoch 14801/150000, Train Loss: 9526, Val Loss: 9747,  Learning Rate: 0.00147, Train Gradient: 64.0\n",
      "Epoch 14901/150000, Train Loss: 9490, Val Loss: 9710,  Learning Rate: 0.00146, Train Gradient: 63.8\n",
      "Epoch 15001/150000, Train Loss: 9454, Val Loss: 9674,  Learning Rate: 0.00145, Train Gradient: 63.5\n",
      "Epoch 15101/150000, Train Loss: 9418, Val Loss: 9638,  Learning Rate: 0.00145, Train Gradient: 63.3\n",
      "Epoch 15201/150000, Train Loss: 9383, Val Loss: 9603,  Learning Rate: 0.00144, Train Gradient: 63.1\n",
      "Epoch 15301/150000, Train Loss: 9348, Val Loss: 9567,  Learning Rate: 0.00143, Train Gradient: 62.9\n",
      "Epoch 15401/150000, Train Loss: 9314, Val Loss: 9532,  Learning Rate: 0.00142, Train Gradient: 62.7\n",
      "Epoch 15501/150000, Train Loss: 9280, Val Loss: 9498,  Learning Rate: 0.00142, Train Gradient: 62.5\n",
      "Epoch 15601/150000, Train Loss: 9246, Val Loss: 9464,  Learning Rate: 0.00141, Train Gradient: 62.3\n",
      "Epoch 15701/150000, Train Loss: 9212, Val Loss: 9429,  Learning Rate: 0.00140, Train Gradient: 62.1\n",
      "Epoch 15801/150000, Train Loss: 9178, Val Loss: 9396,  Learning Rate: 0.00140, Train Gradient: 61.9\n",
      "Epoch 15901/150000, Train Loss: 9145, Val Loss: 9362,  Learning Rate: 0.00139, Train Gradient: 61.7\n",
      "Epoch 16001/150000, Train Loss: 9112, Val Loss: 9329,  Learning Rate: 0.00138, Train Gradient: 61.5\n",
      "Epoch 16101/150000, Train Loss: 9080, Val Loss: 9296,  Learning Rate: 0.00138, Train Gradient: 61.3\n",
      "Epoch 16201/150000, Train Loss: 9047, Val Loss: 9263,  Learning Rate: 0.00137, Train Gradient: 61.1\n",
      "Epoch 16301/150000, Train Loss: 9015, Val Loss: 9231,  Learning Rate: 0.00136, Train Gradient: 60.9\n",
      "Epoch 16401/150000, Train Loss: 8983, Val Loss: 9198,  Learning Rate: 0.00136, Train Gradient: 60.7\n",
      "Epoch 16501/150000, Train Loss: 8951, Val Loss: 9166,  Learning Rate: 0.00135, Train Gradient: 60.6\n",
      "Epoch 16601/150000, Train Loss: 8920, Val Loss: 9134,  Learning Rate: 0.00134, Train Gradient: 60.4\n",
      "Epoch 16701/150000, Train Loss: 8888, Val Loss: 9102,  Learning Rate: 0.00134, Train Gradient: 60.2\n",
      "Epoch 16801/150000, Train Loss: 8857, Val Loss: 9070,  Learning Rate: 0.00133, Train Gradient: 60.1\n",
      "Epoch 16901/150000, Train Loss: 8826, Val Loss: 9039,  Learning Rate: 0.00132, Train Gradient: 59.9\n",
      "Epoch 17001/150000, Train Loss: 8796, Val Loss: 9008,  Learning Rate: 0.00132, Train Gradient: 59.7\n",
      "Epoch 17101/150000, Train Loss: 8765, Val Loss: 8978,  Learning Rate: 0.00131, Train Gradient: 59.5\n",
      "Epoch 17201/150000, Train Loss: 8735, Val Loss: 8947,  Learning Rate: 0.00131, Train Gradient: 59.3\n",
      "Epoch 17301/150000, Train Loss: 8705, Val Loss: 8917,  Learning Rate: 0.00130, Train Gradient: 59.2\n",
      "Epoch 17401/150000, Train Loss: 8676, Val Loss: 8887,  Learning Rate: 0.00129, Train Gradient: 59.0\n",
      "Epoch 17501/150000, Train Loss: 8646, Val Loss: 8857,  Learning Rate: 0.00129, Train Gradient: 58.8\n",
      "Epoch 17601/150000, Train Loss: 8617, Val Loss: 8827,  Learning Rate: 0.00128, Train Gradient: 58.7\n",
      "Epoch 17701/150000, Train Loss: 8588, Val Loss: 8798,  Learning Rate: 0.00128, Train Gradient: 58.5\n",
      "Epoch 17801/150000, Train Loss: 8559, Val Loss: 8769,  Learning Rate: 0.00127, Train Gradient: 58.4\n",
      "Epoch 17901/150000, Train Loss: 8530, Val Loss: 8740,  Learning Rate: 0.00127, Train Gradient: 58.2\n",
      "Epoch 18001/150000, Train Loss: 8501, Val Loss: 8711,  Learning Rate: 0.00126, Train Gradient: 58.0\n",
      "Epoch 18101/150000, Train Loss: 8473, Val Loss: 8683,  Learning Rate: 0.00126, Train Gradient: 57.9\n",
      "Epoch 18201/150000, Train Loss: 8445, Val Loss: 8654,  Learning Rate: 0.00125, Train Gradient: 57.7\n",
      "Epoch 18301/150000, Train Loss: 8417, Val Loss: 8626,  Learning Rate: 0.00124, Train Gradient: 57.6\n",
      "Epoch 18401/150000, Train Loss: 8389, Val Loss: 8598,  Learning Rate: 0.00124, Train Gradient: 57.4\n",
      "Epoch 18501/150000, Train Loss: 8362, Val Loss: 8570,  Learning Rate: 0.00123, Train Gradient: 57.2\n",
      "Epoch 18601/150000, Train Loss: 8334, Val Loss: 8542,  Learning Rate: 0.00123, Train Gradient: 57.1\n",
      "Epoch 18701/150000, Train Loss: 8307, Val Loss: 8515,  Learning Rate: 0.00122, Train Gradient: 57.0\n",
      "Epoch 18801/150000, Train Loss: 8280, Val Loss: 8488,  Learning Rate: 0.00122, Train Gradient: 56.8\n",
      "Epoch 18901/150000, Train Loss: 8253, Val Loss: 8461,  Learning Rate: 0.00121, Train Gradient: 56.7\n",
      "Epoch 19001/150000, Train Loss: 8227, Val Loss: 8434,  Learning Rate: 0.00121, Train Gradient: 56.5\n",
      "Epoch 19101/150000, Train Loss: 8200, Val Loss: 8407,  Learning Rate: 0.00120, Train Gradient: 56.4\n",
      "Epoch 19201/150000, Train Loss: 8174, Val Loss: 8380,  Learning Rate: 0.00120, Train Gradient: 56.2\n",
      "Epoch 19301/150000, Train Loss: 8148, Val Loss: 8354,  Learning Rate: 0.00119, Train Gradient: 56.1\n",
      "Epoch 19401/150000, Train Loss: 8122, Val Loss: 8327,  Learning Rate: 0.00119, Train Gradient: 56.0\n",
      "Epoch 19501/150000, Train Loss: 8096, Val Loss: 8301,  Learning Rate: 0.00118, Train Gradient: 55.8\n",
      "Epoch 19601/150000, Train Loss: 8070, Val Loss: 8275,  Learning Rate: 0.00118, Train Gradient: 55.7\n",
      "Epoch 19701/150000, Train Loss: 8045, Val Loss: 8250,  Learning Rate: 0.00117, Train Gradient: 55.6\n",
      "Epoch 19801/150000, Train Loss: 8019, Val Loss: 8224,  Learning Rate: 0.00117, Train Gradient: 55.4\n",
      "Epoch 19901/150000, Train Loss: 7994, Val Loss: 8199,  Learning Rate: 0.00117, Train Gradient: 55.3\n",
      "Epoch 20001/150000, Train Loss: 7969, Val Loss: 8173,  Learning Rate: 0.00116, Train Gradient: 55.1\n",
      "Epoch 20101/150000, Train Loss: 7944, Val Loss: 8148,  Learning Rate: 0.00116, Train Gradient: 55.0\n",
      "Epoch 20201/150000, Train Loss: 7920, Val Loss: 8123,  Learning Rate: 0.00115, Train Gradient: 54.9\n",
      "Epoch 20301/150000, Train Loss: 7895, Val Loss: 8098,  Learning Rate: 0.00115, Train Gradient: 54.7\n",
      "Epoch 20401/150000, Train Loss: 7871, Val Loss: 8074,  Learning Rate: 0.00114, Train Gradient: 54.6\n",
      "Epoch 20501/150000, Train Loss: 7846, Val Loss: 8049,  Learning Rate: 0.00114, Train Gradient: 54.5\n",
      "Epoch 20601/150000, Train Loss: 7822, Val Loss: 8025,  Learning Rate: 0.00113, Train Gradient: 54.4\n",
      "Epoch 20701/150000, Train Loss: 7798, Val Loss: 8001,  Learning Rate: 0.00113, Train Gradient: 54.2\n",
      "Epoch 20801/150000, Train Loss: 7774, Val Loss: 7977,  Learning Rate: 0.00113, Train Gradient: 54.1\n",
      "Epoch 20901/150000, Train Loss: 7751, Val Loss: 7953,  Learning Rate: 0.00112, Train Gradient: 54.0\n",
      "Epoch 21001/150000, Train Loss: 7727, Val Loss: 7929,  Learning Rate: 0.00112, Train Gradient: 53.8\n",
      "Epoch 21101/150000, Train Loss: 7704, Val Loss: 7905,  Learning Rate: 0.00111, Train Gradient: 53.7\n",
      "Epoch 21201/150000, Train Loss: 7681, Val Loss: 7882,  Learning Rate: 0.00111, Train Gradient: 53.6\n",
      "Epoch 21301/150000, Train Loss: 7657, Val Loss: 7858,  Learning Rate: 0.00110, Train Gradient: 53.5\n",
      "Epoch 21401/150000, Train Loss: 7634, Val Loss: 7835,  Learning Rate: 0.00110, Train Gradient: 53.4\n",
      "Epoch 21501/150000, Train Loss: 7612, Val Loss: 7812,  Learning Rate: 0.00110, Train Gradient: 53.2\n",
      "Epoch 21601/150000, Train Loss: 7589, Val Loss: 7789,  Learning Rate: 0.00109, Train Gradient: 53.1\n",
      "Epoch 21701/150000, Train Loss: 7566, Val Loss: 7766,  Learning Rate: 0.00109, Train Gradient: 53.0\n",
      "Epoch 21801/150000, Train Loss: 7544, Val Loss: 7743,  Learning Rate: 0.00108, Train Gradient: 52.9\n",
      "Epoch 21901/150000, Train Loss: 7521, Val Loss: 7721,  Learning Rate: 0.00108, Train Gradient: 52.8\n",
      "Epoch 22001/150000, Train Loss: 7499, Val Loss: 7699,  Learning Rate: 0.00108, Train Gradient: 52.6\n",
      "Epoch 22101/150000, Train Loss: 7477, Val Loss: 7676,  Learning Rate: 0.00107, Train Gradient: 52.5\n",
      "Epoch 22201/150000, Train Loss: 7455, Val Loss: 7654,  Learning Rate: 0.00107, Train Gradient: 52.4\n",
      "Epoch 22301/150000, Train Loss: 7433, Val Loss: 7632,  Learning Rate: 0.00107, Train Gradient: 52.3\n",
      "Epoch 22401/150000, Train Loss: 7412, Val Loss: 7610,  Learning Rate: 0.00106, Train Gradient: 52.2\n",
      "Epoch 22501/150000, Train Loss: 7390, Val Loss: 7588,  Learning Rate: 0.00106, Train Gradient: 52.1\n",
      "Epoch 22601/150000, Train Loss: 7369, Val Loss: 7567,  Learning Rate: 0.00105, Train Gradient: 52.0\n",
      "Epoch 22701/150000, Train Loss: 7347, Val Loss: 7545,  Learning Rate: 0.00105, Train Gradient: 51.8\n",
      "Epoch 22801/150000, Train Loss: 7326, Val Loss: 7524,  Learning Rate: 0.00105, Train Gradient: 51.7\n",
      "Epoch 22901/150000, Train Loss: 7305, Val Loss: 7502,  Learning Rate: 0.00104, Train Gradient: 51.6\n",
      "Epoch 23001/150000, Train Loss: 7284, Val Loss: 7481,  Learning Rate: 0.00104, Train Gradient: 51.5\n",
      "Epoch 23101/150000, Train Loss: 7263, Val Loss: 7460,  Learning Rate: 0.00104, Train Gradient: 51.4\n",
      "Epoch 23201/150000, Train Loss: 7242, Val Loss: 7439,  Learning Rate: 0.00103, Train Gradient: 51.3\n",
      "Epoch 23301/150000, Train Loss: 7221, Val Loss: 7418,  Learning Rate: 0.00103, Train Gradient: 51.2\n",
      "Epoch 23401/150000, Train Loss: 7201, Val Loss: 7398,  Learning Rate: 0.00103, Train Gradient: 51.1\n",
      "Epoch 23501/150000, Train Loss: 7180, Val Loss: 7377,  Learning Rate: 0.00102, Train Gradient: 51.0\n",
      "Epoch 23601/150000, Train Loss: 7160, Val Loss: 7356,  Learning Rate: 0.00102, Train Gradient: 50.9\n",
      "Epoch 23701/150000, Train Loss: 7140, Val Loss: 7336,  Learning Rate: 0.00102, Train Gradient: 50.8\n",
      "Epoch 23801/150000, Train Loss: 7120, Val Loss: 7316,  Learning Rate: 0.00101, Train Gradient: 50.6\n",
      "Epoch 23901/150000, Train Loss: 7100, Val Loss: 7295,  Learning Rate: 0.00101, Train Gradient: 50.6\n",
      "Epoch 24001/150000, Train Loss: 7080, Val Loss: 7275,  Learning Rate: 0.00101, Train Gradient: 50.5\n",
      "Epoch 24101/150000, Train Loss: 7060, Val Loss: 7255,  Learning Rate: 0.00100, Train Gradient: 50.4\n",
      "Epoch 24201/150000, Train Loss: 7040, Val Loss: 7235,  Learning Rate: 0.00100, Train Gradient: 50.3\n",
      "Epoch 24301/150000, Train Loss: 7021, Val Loss: 7216,  Learning Rate: 0.00100, Train Gradient: 50.2\n",
      "Epoch 24401/150000, Train Loss: 7001, Val Loss: 7196,  Learning Rate: 0.00099, Train Gradient: 50.1\n",
      "Epoch 24501/150000, Train Loss: 6982, Val Loss: 7176,  Learning Rate: 0.00099, Train Gradient: 49.9\n",
      "Epoch 24601/150000, Train Loss: 6962, Val Loss: 7157,  Learning Rate: 0.00099, Train Gradient: 49.9\n",
      "Epoch 24701/150000, Train Loss: 6943, Val Loss: 7137,  Learning Rate: 0.00098, Train Gradient: 49.8\n",
      "Epoch 24801/150000, Train Loss: 6924, Val Loss: 7118,  Learning Rate: 0.00098, Train Gradient: 49.7\n",
      "Epoch 24901/150000, Train Loss: 6905, Val Loss: 7099,  Learning Rate: 0.00098, Train Gradient: 49.6\n",
      "Epoch 25001/150000, Train Loss: 6886, Val Loss: 7080,  Learning Rate: 0.00097, Train Gradient: 49.4\n",
      "Epoch 25101/150000, Train Loss: 6867, Val Loss: 7061,  Learning Rate: 0.00097, Train Gradient: 49.4\n",
      "Epoch 25201/150000, Train Loss: 6848, Val Loss: 7042,  Learning Rate: 0.00097, Train Gradient: 49.3\n",
      "Epoch 25301/150000, Train Loss: 6830, Val Loss: 7023,  Learning Rate: 0.00097, Train Gradient: 49.2\n",
      "Epoch 25401/150000, Train Loss: 6811, Val Loss: 7004,  Learning Rate: 0.00096, Train Gradient: 49.1\n",
      "Epoch 25501/150000, Train Loss: 6793, Val Loss: 6985,  Learning Rate: 0.00096, Train Gradient: 49.0\n",
      "Epoch 25601/150000, Train Loss: 6774, Val Loss: 6967,  Learning Rate: 0.00096, Train Gradient: 48.9\n",
      "Epoch 25701/150000, Train Loss: 6756, Val Loss: 6948,  Learning Rate: 0.00095, Train Gradient: 48.8\n",
      "Epoch 25801/150000, Train Loss: 6738, Val Loss: 6930,  Learning Rate: 0.00095, Train Gradient: 48.7\n",
      "Epoch 25901/150000, Train Loss: 6720, Val Loss: 6912,  Learning Rate: 0.00095, Train Gradient: 48.6\n",
      "Epoch 26001/150000, Train Loss: 6702, Val Loss: 6894,  Learning Rate: 0.00095, Train Gradient: 48.5\n",
      "Epoch 26101/150000, Train Loss: 6684, Val Loss: 6875,  Learning Rate: 0.00094, Train Gradient: 48.4\n",
      "Epoch 26201/150000, Train Loss: 6666, Val Loss: 6857,  Learning Rate: 0.00094, Train Gradient: 48.3\n",
      "Epoch 26301/150000, Train Loss: 6648, Val Loss: 6839,  Learning Rate: 0.00094, Train Gradient: 48.2\n",
      "Epoch 26401/150000, Train Loss: 6631, Val Loss: 6822,  Learning Rate: 0.00093, Train Gradient: 48.1\n",
      "Epoch 26501/150000, Train Loss: 6613, Val Loss: 6804,  Learning Rate: 0.00093, Train Gradient: 48.0\n",
      "Epoch 26601/150000, Train Loss: 6595, Val Loss: 6786,  Learning Rate: 0.00093, Train Gradient: 47.9\n",
      "Epoch 26701/150000, Train Loss: 6578, Val Loss: 6768,  Learning Rate: 0.00093, Train Gradient: 47.8\n",
      "Epoch 26801/150000, Train Loss: 6561, Val Loss: 6751,  Learning Rate: 0.00092, Train Gradient: 47.8\n",
      "Epoch 26901/150000, Train Loss: 6543, Val Loss: 6733,  Learning Rate: 0.00092, Train Gradient: 47.7\n",
      "Epoch 27001/150000, Train Loss: 6526, Val Loss: 6716,  Learning Rate: 0.00092, Train Gradient: 47.6\n",
      "Epoch 27101/150000, Train Loss: 6509, Val Loss: 6699,  Learning Rate: 0.00092, Train Gradient: 47.5\n",
      "Epoch 27201/150000, Train Loss: 6492, Val Loss: 6681,  Learning Rate: 0.00091, Train Gradient: 47.4\n",
      "Epoch 27301/150000, Train Loss: 6475, Val Loss: 6664,  Learning Rate: 0.00091, Train Gradient: 47.3\n",
      "Epoch 27401/150000, Train Loss: 6458, Val Loss: 6647,  Learning Rate: 0.00091, Train Gradient: 47.2\n",
      "Epoch 27501/150000, Train Loss: 6442, Val Loss: 6630,  Learning Rate: 0.00091, Train Gradient: 47.1\n",
      "Epoch 27601/150000, Train Loss: 6425, Val Loss: 6613,  Learning Rate: 0.00090, Train Gradient: 47.0\n",
      "Epoch 27701/150000, Train Loss: 6408, Val Loss: 6596,  Learning Rate: 0.00090, Train Gradient: 47.0\n",
      "Epoch 27801/150000, Train Loss: 6392, Val Loss: 6580,  Learning Rate: 0.00090, Train Gradient: 46.9\n",
      "Epoch 27901/150000, Train Loss: 6375, Val Loss: 6563,  Learning Rate: 0.00090, Train Gradient: 46.8\n",
      "Epoch 28001/150000, Train Loss: 6359, Val Loss: 6546,  Learning Rate: 0.00089, Train Gradient: 46.7\n",
      "Epoch 28101/150000, Train Loss: 6342, Val Loss: 6530,  Learning Rate: 0.00089, Train Gradient: 46.6\n",
      "Epoch 28201/150000, Train Loss: 6326, Val Loss: 6513,  Learning Rate: 0.00089, Train Gradient: 46.5\n",
      "Epoch 28301/150000, Train Loss: 6310, Val Loss: 6497,  Learning Rate: 0.00089, Train Gradient: 46.5\n",
      "Epoch 28401/150000, Train Loss: 6294, Val Loss: 6481,  Learning Rate: 0.00088, Train Gradient: 46.4\n",
      "Epoch 28501/150000, Train Loss: 6278, Val Loss: 6464,  Learning Rate: 0.00088, Train Gradient: 46.3\n",
      "Epoch 28601/150000, Train Loss: 6262, Val Loss: 6448,  Learning Rate: 0.00088, Train Gradient: 46.2\n",
      "Epoch 28701/150000, Train Loss: 6246, Val Loss: 6432,  Learning Rate: 0.00088, Train Gradient: 46.1\n",
      "Epoch 28801/150000, Train Loss: 6230, Val Loss: 6416,  Learning Rate: 0.00087, Train Gradient: 46.0\n",
      "Epoch 28901/150000, Train Loss: 6214, Val Loss: 6400,  Learning Rate: 0.00087, Train Gradient: 45.9\n",
      "Epoch 29001/150000, Train Loss: 6199, Val Loss: 6384,  Learning Rate: 0.00087, Train Gradient: 45.9\n",
      "Epoch 29101/150000, Train Loss: 6183, Val Loss: 6368,  Learning Rate: 0.00087, Train Gradient: 45.8\n",
      "Epoch 29201/150000, Train Loss: 6168, Val Loss: 6352,  Learning Rate: 0.00086, Train Gradient: 45.7\n",
      "Epoch 29301/150000, Train Loss: 6152, Val Loss: 6337,  Learning Rate: 0.00086, Train Gradient: 45.6\n",
      "Epoch 29401/150000, Train Loss: 6137, Val Loss: 6321,  Learning Rate: 0.00086, Train Gradient: 45.5\n",
      "Epoch 29501/150000, Train Loss: 6121, Val Loss: 6306,  Learning Rate: 0.00086, Train Gradient: 45.5\n",
      "Epoch 29601/150000, Train Loss: 6106, Val Loss: 6290,  Learning Rate: 0.00086, Train Gradient: 45.4\n",
      "Epoch 29701/150000, Train Loss: 6091, Val Loss: 6275,  Learning Rate: 0.00085, Train Gradient: 45.3\n",
      "Epoch 29801/150000, Train Loss: 6076, Val Loss: 6259,  Learning Rate: 0.00085, Train Gradient: 45.2\n",
      "Epoch 29901/150000, Train Loss: 6061, Val Loss: 6244,  Learning Rate: 0.00085, Train Gradient: 45.1\n",
      "Epoch 30001/150000, Train Loss: 6046, Val Loss: 6229,  Learning Rate: 0.00085, Train Gradient: 45.1\n",
      "Epoch 30101/150000, Train Loss: 6031, Val Loss: 6214,  Learning Rate: 0.00084, Train Gradient: 45.0\n",
      "Epoch 30201/150000, Train Loss: 6016, Val Loss: 6199,  Learning Rate: 0.00084, Train Gradient: 44.9\n",
      "Epoch 30301/150000, Train Loss: 6001, Val Loss: 6184,  Learning Rate: 0.00084, Train Gradient: 44.8\n",
      "Epoch 30401/150000, Train Loss: 5986, Val Loss: 6169,  Learning Rate: 0.00084, Train Gradient: 44.7\n",
      "Epoch 30501/150000, Train Loss: 5971, Val Loss: 6154,  Learning Rate: 0.00084, Train Gradient: 44.7\n",
      "Epoch 30601/150000, Train Loss: 5957, Val Loss: 6139,  Learning Rate: 0.00083, Train Gradient: 44.6\n",
      "Epoch 30701/150000, Train Loss: 5942, Val Loss: 6124,  Learning Rate: 0.00083, Train Gradient: 44.5\n",
      "Epoch 30801/150000, Train Loss: 5928, Val Loss: 6110,  Learning Rate: 0.00083, Train Gradient: 44.4\n",
      "Epoch 30901/150000, Train Loss: 5913, Val Loss: 6095,  Learning Rate: 0.00083, Train Gradient: 44.4\n",
      "Epoch 31001/150000, Train Loss: 5899, Val Loss: 6080,  Learning Rate: 0.00083, Train Gradient: 44.3\n",
      "Epoch 31101/150000, Train Loss: 5884, Val Loss: 6066,  Learning Rate: 0.00082, Train Gradient: 44.2\n",
      "Epoch 31201/150000, Train Loss: 5870, Val Loss: 6052,  Learning Rate: 0.00082, Train Gradient: 44.1\n",
      "Epoch 31301/150000, Train Loss: 5856, Val Loss: 6037,  Learning Rate: 0.00082, Train Gradient: 44.0\n",
      "Epoch 31401/150000, Train Loss: 5842, Val Loss: 6023,  Learning Rate: 0.00082, Train Gradient: 44.0\n",
      "Epoch 31501/150000, Train Loss: 5828, Val Loss: 6009,  Learning Rate: 0.00082, Train Gradient: 43.9\n",
      "Epoch 31601/150000, Train Loss: 5814, Val Loss: 5994,  Learning Rate: 0.00081, Train Gradient: 43.8\n",
      "Epoch 31701/150000, Train Loss: 5800, Val Loss: 5980,  Learning Rate: 0.00081, Train Gradient: 43.7\n",
      "Epoch 31801/150000, Train Loss: 5786, Val Loss: 5966,  Learning Rate: 0.00081, Train Gradient: 43.7\n",
      "Epoch 31901/150000, Train Loss: 5772, Val Loss: 5952,  Learning Rate: 0.00081, Train Gradient: 43.6\n",
      "Epoch 32001/150000, Train Loss: 5758, Val Loss: 5938,  Learning Rate: 0.00081, Train Gradient: 43.5\n",
      "Epoch 32101/150000, Train Loss: 5744, Val Loss: 5924,  Learning Rate: 0.00080, Train Gradient: 43.4\n",
      "Epoch 32201/150000, Train Loss: 5730, Val Loss: 5911,  Learning Rate: 0.00080, Train Gradient: 43.4\n",
      "Epoch 32301/150000, Train Loss: 5717, Val Loss: 5897,  Learning Rate: 0.00080, Train Gradient: 43.3\n",
      "Epoch 32401/150000, Train Loss: 5703, Val Loss: 5883,  Learning Rate: 0.00080, Train Gradient: 43.2\n",
      "Epoch 32501/150000, Train Loss: 5690, Val Loss: 5869,  Learning Rate: 0.00080, Train Gradient: 43.1\n",
      "Epoch 32601/150000, Train Loss: 5676, Val Loss: 5856,  Learning Rate: 0.00080, Train Gradient: 43.1\n",
      "Epoch 32701/150000, Train Loss: 5663, Val Loss: 5842,  Learning Rate: 0.00079, Train Gradient: 43.0\n",
      "Epoch 32801/150000, Train Loss: 5649, Val Loss: 5829,  Learning Rate: 0.00079, Train Gradient: 42.9\n",
      "Epoch 32901/150000, Train Loss: 5636, Val Loss: 5815,  Learning Rate: 0.00079, Train Gradient: 42.8\n",
      "Epoch 33001/150000, Train Loss: 5623, Val Loss: 5802,  Learning Rate: 0.00079, Train Gradient: 42.8\n",
      "Epoch 33101/150000, Train Loss: 5609, Val Loss: 5788,  Learning Rate: 0.00079, Train Gradient: 42.7\n",
      "Epoch 33201/150000, Train Loss: 5596, Val Loss: 5775,  Learning Rate: 0.00079, Train Gradient: 42.6\n",
      "Epoch 33301/150000, Train Loss: 5583, Val Loss: 5762,  Learning Rate: 0.00078, Train Gradient: 42.5\n",
      "Epoch 33401/150000, Train Loss: 5570, Val Loss: 5749,  Learning Rate: 0.00078, Train Gradient: 42.4\n",
      "Epoch 33501/150000, Train Loss: 5557, Val Loss: 5736,  Learning Rate: 0.00078, Train Gradient: 42.4\n",
      "Epoch 33601/150000, Train Loss: 5544, Val Loss: 5723,  Learning Rate: 0.00078, Train Gradient: 42.3\n",
      "Epoch 33701/150000, Train Loss: 5531, Val Loss: 5710,  Learning Rate: 0.00078, Train Gradient: 42.3\n",
      "Epoch 33801/150000, Train Loss: 5518, Val Loss: 5697,  Learning Rate: 0.00077, Train Gradient: 42.2\n",
      "Epoch 33901/150000, Train Loss: 5505, Val Loss: 5684,  Learning Rate: 0.00077, Train Gradient: 42.1\n",
      "Epoch 34001/150000, Train Loss: 5493, Val Loss: 5671,  Learning Rate: 0.00077, Train Gradient: 42.0\n",
      "Epoch 34101/150000, Train Loss: 5480, Val Loss: 5658,  Learning Rate: 0.00077, Train Gradient: 42.0\n",
      "Epoch 34201/150000, Train Loss: 5467, Val Loss: 5645,  Learning Rate: 0.00077, Train Gradient: 41.9\n",
      "Epoch 34301/150000, Train Loss: 5455, Val Loss: 5632,  Learning Rate: 0.00077, Train Gradient: 41.8\n",
      "Epoch 34401/150000, Train Loss: 5442, Val Loss: 5620,  Learning Rate: 0.00076, Train Gradient: 41.8\n",
      "Epoch 34501/150000, Train Loss: 5430, Val Loss: 5607,  Learning Rate: 0.00076, Train Gradient: 41.7\n",
      "Epoch 34601/150000, Train Loss: 5417, Val Loss: 5595,  Learning Rate: 0.00076, Train Gradient: 41.6\n",
      "Epoch 34701/150000, Train Loss: 5405, Val Loss: 5582,  Learning Rate: 0.00076, Train Gradient: 41.5\n",
      "Epoch 34801/150000, Train Loss: 5392, Val Loss: 5570,  Learning Rate: 0.00076, Train Gradient: 41.5\n",
      "Epoch 34901/150000, Train Loss: 5380, Val Loss: 5557,  Learning Rate: 0.00076, Train Gradient: 41.4\n",
      "Epoch 35001/150000, Train Loss: 5368, Val Loss: 5545,  Learning Rate: 0.00076, Train Gradient: 41.3\n",
      "Epoch 35101/150000, Train Loss: 5355, Val Loss: 5532,  Learning Rate: 0.00075, Train Gradient: 41.3\n",
      "Epoch 35201/150000, Train Loss: 5343, Val Loss: 5520,  Learning Rate: 0.00075, Train Gradient: 41.2\n",
      "Epoch 35301/150000, Train Loss: 5331, Val Loss: 5508,  Learning Rate: 0.00075, Train Gradient: 41.1\n",
      "Epoch 35401/150000, Train Loss: 5319, Val Loss: 5496,  Learning Rate: 0.00075, Train Gradient: 41.1\n",
      "Epoch 35501/150000, Train Loss: 5307, Val Loss: 5483,  Learning Rate: 0.00075, Train Gradient: 41.0\n",
      "Epoch 35601/150000, Train Loss: 5295, Val Loss: 5471,  Learning Rate: 0.00075, Train Gradient: 40.9\n",
      "Epoch 35701/150000, Train Loss: 5283, Val Loss: 5459,  Learning Rate: 0.00074, Train Gradient: 40.9\n",
      "Epoch 35801/150000, Train Loss: 5271, Val Loss: 5447,  Learning Rate: 0.00074, Train Gradient: 40.8\n",
      "Epoch 35901/150000, Train Loss: 5259, Val Loss: 5435,  Learning Rate: 0.00074, Train Gradient: 40.7\n",
      "Epoch 36001/150000, Train Loss: 5247, Val Loss: 5423,  Learning Rate: 0.00074, Train Gradient: 40.6\n",
      "Epoch 36101/150000, Train Loss: 5235, Val Loss: 5411,  Learning Rate: 0.00074, Train Gradient: 40.6\n",
      "Epoch 36201/150000, Train Loss: 5224, Val Loss: 5399,  Learning Rate: 0.00074, Train Gradient: 40.5\n",
      "Epoch 36301/150000, Train Loss: 5212, Val Loss: 5387,  Learning Rate: 0.00074, Train Gradient: 40.4\n",
      "Epoch 36401/150000, Train Loss: 5200, Val Loss: 5376,  Learning Rate: 0.00073, Train Gradient: 40.4\n",
      "Epoch 36501/150000, Train Loss: 5188, Val Loss: 5364,  Learning Rate: 0.00073, Train Gradient: 40.3\n",
      "Epoch 36601/150000, Train Loss: 5177, Val Loss: 5352,  Learning Rate: 0.00073, Train Gradient: 40.2\n",
      "Epoch 36701/150000, Train Loss: 5165, Val Loss: 5340,  Learning Rate: 0.00073, Train Gradient: 40.2\n",
      "Epoch 36801/150000, Train Loss: 5154, Val Loss: 5329,  Learning Rate: 0.00073, Train Gradient: 40.1\n",
      "Epoch 36901/150000, Train Loss: 5142, Val Loss: 5317,  Learning Rate: 0.00073, Train Gradient: 40.0\n",
      "Epoch 37001/150000, Train Loss: 5131, Val Loss: 5305,  Learning Rate: 0.00073, Train Gradient: 40.0\n",
      "Epoch 37101/150000, Train Loss: 5120, Val Loss: 5294,  Learning Rate: 0.00072, Train Gradient: 39.9\n",
      "Epoch 37201/150000, Train Loss: 5108, Val Loss: 5282,  Learning Rate: 0.00072, Train Gradient: 39.8\n",
      "Epoch 37301/150000, Train Loss: 5097, Val Loss: 5271,  Learning Rate: 0.00072, Train Gradient: 39.8\n",
      "Epoch 37401/150000, Train Loss: 5086, Val Loss: 5259,  Learning Rate: 0.00072, Train Gradient: 39.7\n",
      "Epoch 37501/150000, Train Loss: 5074, Val Loss: 5248,  Learning Rate: 0.00072, Train Gradient: 39.7\n",
      "Epoch 37601/150000, Train Loss: 5063, Val Loss: 5236,  Learning Rate: 0.00072, Train Gradient: 39.6\n",
      "Epoch 37701/150000, Train Loss: 5052, Val Loss: 5225,  Learning Rate: 0.00072, Train Gradient: 39.5\n",
      "Epoch 37801/150000, Train Loss: 5041, Val Loss: 5214,  Learning Rate: 0.00071, Train Gradient: 39.5\n",
      "Epoch 37901/150000, Train Loss: 5030, Val Loss: 5203,  Learning Rate: 0.00071, Train Gradient: 39.4\n",
      "Epoch 38001/150000, Train Loss: 5019, Val Loss: 5192,  Learning Rate: 0.00071, Train Gradient: 39.4\n",
      "Epoch 38101/150000, Train Loss: 5008, Val Loss: 5181,  Learning Rate: 0.00071, Train Gradient: 39.3\n",
      "Epoch 38201/150000, Train Loss: 4997, Val Loss: 5169,  Learning Rate: 0.00071, Train Gradient: 39.2\n",
      "Epoch 38301/150000, Train Loss: 4986, Val Loss: 5158,  Learning Rate: 0.00071, Train Gradient: 39.2\n",
      "Epoch 38401/150000, Train Loss: 4975, Val Loss: 5148,  Learning Rate: 0.00071, Train Gradient: 39.1\n",
      "Epoch 38501/150000, Train Loss: 4964, Val Loss: 5137,  Learning Rate: 0.00071, Train Gradient: 39.0\n",
      "Epoch 38601/150000, Train Loss: 4953, Val Loss: 5126,  Learning Rate: 0.00070, Train Gradient: 39.0\n",
      "Epoch 38701/150000, Train Loss: 4942, Val Loss: 5115,  Learning Rate: 0.00070, Train Gradient: 38.9\n",
      "Epoch 38801/150000, Train Loss: 4932, Val Loss: 5104,  Learning Rate: 0.00070, Train Gradient: 38.9\n",
      "Epoch 38901/150000, Train Loss: 4921, Val Loss: 5093,  Learning Rate: 0.00070, Train Gradient: 38.8\n",
      "Epoch 39001/150000, Train Loss: 4910, Val Loss: 5082,  Learning Rate: 0.00070, Train Gradient: 38.7\n",
      "Epoch 39101/150000, Train Loss: 4900, Val Loss: 5071,  Learning Rate: 0.00070, Train Gradient: 38.7\n",
      "Epoch 39201/150000, Train Loss: 4889, Val Loss: 5060,  Learning Rate: 0.00070, Train Gradient: 38.6\n",
      "Epoch 39301/150000, Train Loss: 4878, Val Loss: 5049,  Learning Rate: 0.00070, Train Gradient: 38.6\n",
      "Epoch 39401/150000, Train Loss: 4868, Val Loss: 5038,  Learning Rate: 0.00069, Train Gradient: 38.5\n",
      "Epoch 39501/150000, Train Loss: 4857, Val Loss: 5027,  Learning Rate: 0.00069, Train Gradient: 38.5\n",
      "Epoch 39601/150000, Train Loss: 4847, Val Loss: 5017,  Learning Rate: 0.00069, Train Gradient: 38.4\n",
      "Epoch 39701/150000, Train Loss: 4836, Val Loss: 5006,  Learning Rate: 0.00069, Train Gradient: 38.3\n",
      "Epoch 39801/150000, Train Loss: 4826, Val Loss: 4996,  Learning Rate: 0.00069, Train Gradient: 38.3\n",
      "Epoch 39901/150000, Train Loss: 4816, Val Loss: 4985,  Learning Rate: 0.00069, Train Gradient: 38.2\n",
      "Epoch 40001/150000, Train Loss: 4805, Val Loss: 4975,  Learning Rate: 0.00069, Train Gradient: 38.2\n",
      "Epoch 40101/150000, Train Loss: 4795, Val Loss: 4964,  Learning Rate: 0.00069, Train Gradient: 38.1\n",
      "Epoch 40201/150000, Train Loss: 4785, Val Loss: 4954,  Learning Rate: 0.00068, Train Gradient: 38.0\n",
      "Epoch 40301/150000, Train Loss: 4774, Val Loss: 4944,  Learning Rate: 0.00068, Train Gradient: 38.0\n",
      "Epoch 40401/150000, Train Loss: 4764, Val Loss: 4934,  Learning Rate: 0.00068, Train Gradient: 37.9\n",
      "Epoch 40501/150000, Train Loss: 4754, Val Loss: 4923,  Learning Rate: 0.00068, Train Gradient: 37.9\n",
      "Epoch 40601/150000, Train Loss: 4744, Val Loss: 4913,  Learning Rate: 0.00068, Train Gradient: 37.8\n",
      "Epoch 40701/150000, Train Loss: 4734, Val Loss: 4903,  Learning Rate: 0.00068, Train Gradient: 37.8\n",
      "Epoch 40801/150000, Train Loss: 4724, Val Loss: 4893,  Learning Rate: 0.00068, Train Gradient: 37.7\n",
      "Epoch 40901/150000, Train Loss: 4714, Val Loss: 4883,  Learning Rate: 0.00068, Train Gradient: 37.7\n",
      "Epoch 41001/150000, Train Loss: 4704, Val Loss: 4873,  Learning Rate: 0.00068, Train Gradient: 37.6\n",
      "Epoch 41101/150000, Train Loss: 4694, Val Loss: 4863,  Learning Rate: 0.00067, Train Gradient: 37.5\n",
      "Epoch 41201/150000, Train Loss: 4684, Val Loss: 4853,  Learning Rate: 0.00067, Train Gradient: 37.5\n",
      "Epoch 41301/150000, Train Loss: 4674, Val Loss: 4843,  Learning Rate: 0.00067, Train Gradient: 37.4\n",
      "Epoch 41401/150000, Train Loss: 4664, Val Loss: 4833,  Learning Rate: 0.00067, Train Gradient: 37.4\n",
      "Epoch 41501/150000, Train Loss: 4654, Val Loss: 4823,  Learning Rate: 0.00067, Train Gradient: 37.3\n",
      "Epoch 41601/150000, Train Loss: 4644, Val Loss: 4813,  Learning Rate: 0.00067, Train Gradient: 37.3\n",
      "Epoch 41701/150000, Train Loss: 4634, Val Loss: 4804,  Learning Rate: 0.00067, Train Gradient: 37.2\n",
      "Epoch 41801/150000, Train Loss: 4625, Val Loss: 4794,  Learning Rate: 0.00067, Train Gradient: 37.2\n",
      "Epoch 41901/150000, Train Loss: 4615, Val Loss: 4784,  Learning Rate: 0.00067, Train Gradient: 37.1\n",
      "Epoch 42001/150000, Train Loss: 4605, Val Loss: 4774,  Learning Rate: 0.00066, Train Gradient: 37.0\n",
      "Epoch 42101/150000, Train Loss: 4595, Val Loss: 4765,  Learning Rate: 0.00066, Train Gradient: 37.0\n",
      "Epoch 42201/150000, Train Loss: 4586, Val Loss: 4755,  Learning Rate: 0.00066, Train Gradient: 37.0\n",
      "Epoch 42301/150000, Train Loss: 4576, Val Loss: 4745,  Learning Rate: 0.00066, Train Gradient: 36.9\n",
      "Epoch 42401/150000, Train Loss: 4567, Val Loss: 4736,  Learning Rate: 0.00066, Train Gradient: 36.8\n",
      "Epoch 42501/150000, Train Loss: 4557, Val Loss: 4726,  Learning Rate: 0.00066, Train Gradient: 36.8\n",
      "Epoch 42601/150000, Train Loss: 4547, Val Loss: 4717,  Learning Rate: 0.00066, Train Gradient: 36.7\n",
      "Epoch 42701/150000, Train Loss: 4538, Val Loss: 4707,  Learning Rate: 0.00066, Train Gradient: 36.7\n",
      "Epoch 42801/150000, Train Loss: 4528, Val Loss: 4697,  Learning Rate: 0.00066, Train Gradient: 36.6\n",
      "Epoch 42901/150000, Train Loss: 4519, Val Loss: 4688,  Learning Rate: 0.00065, Train Gradient: 36.6\n",
      "Epoch 43001/150000, Train Loss: 4509, Val Loss: 4677,  Learning Rate: 0.00065, Train Gradient: 36.5\n",
      "Epoch 43101/150000, Train Loss: 4500, Val Loss: 4668,  Learning Rate: 0.00065, Train Gradient: 36.5\n",
      "Epoch 43201/150000, Train Loss: 4491, Val Loss: 4658,  Learning Rate: 0.00065, Train Gradient: 36.4\n",
      "Epoch 43301/150000, Train Loss: 4481, Val Loss: 4649,  Learning Rate: 0.00065, Train Gradient: 36.4\n",
      "Epoch 43401/150000, Train Loss: 4472, Val Loss: 4640,  Learning Rate: 0.00065, Train Gradient: 36.3\n",
      "Epoch 43501/150000, Train Loss: 4463, Val Loss: 4630,  Learning Rate: 0.00065, Train Gradient: 36.3\n",
      "Epoch 43601/150000, Train Loss: 4453, Val Loss: 4621,  Learning Rate: 0.00065, Train Gradient: 36.2\n",
      "Epoch 43701/150000, Train Loss: 4444, Val Loss: 4612,  Learning Rate: 0.00065, Train Gradient: 36.2\n",
      "Epoch 43801/150000, Train Loss: 4435, Val Loss: 4602,  Learning Rate: 0.00065, Train Gradient: 36.1\n",
      "Epoch 43901/150000, Train Loss: 4426, Val Loss: 4593,  Learning Rate: 0.00064, Train Gradient: 36.1\n",
      "Epoch 44001/150000, Train Loss: 4417, Val Loss: 4584,  Learning Rate: 0.00064, Train Gradient: 36.0\n",
      "Epoch 44101/150000, Train Loss: 4407, Val Loss: 4575,  Learning Rate: 0.00064, Train Gradient: 36.0\n",
      "Epoch 44201/150000, Train Loss: 4398, Val Loss: 4566,  Learning Rate: 0.00064, Train Gradient: 35.9\n",
      "Epoch 44301/150000, Train Loss: 4389, Val Loss: 4557,  Learning Rate: 0.00064, Train Gradient: 35.9\n",
      "Epoch 44401/150000, Train Loss: 4380, Val Loss: 4548,  Learning Rate: 0.00064, Train Gradient: 35.8\n",
      "Epoch 44501/150000, Train Loss: 4371, Val Loss: 4538,  Learning Rate: 0.00064, Train Gradient: 35.8\n",
      "Epoch 44601/150000, Train Loss: 4362, Val Loss: 4529,  Learning Rate: 0.00064, Train Gradient: 35.7\n",
      "Epoch 44701/150000, Train Loss: 4353, Val Loss: 4520,  Learning Rate: 0.00064, Train Gradient: 35.6\n",
      "Epoch 44801/150000, Train Loss: 4344, Val Loss: 4512,  Learning Rate: 0.00064, Train Gradient: 35.6\n",
      "Epoch 44901/150000, Train Loss: 4335, Val Loss: 4503,  Learning Rate: 0.00064, Train Gradient: 35.5\n",
      "Epoch 45001/150000, Train Loss: 4326, Val Loss: 4494,  Learning Rate: 0.00063, Train Gradient: 35.5\n",
      "Epoch 45101/150000, Train Loss: 4318, Val Loss: 4485,  Learning Rate: 0.00063, Train Gradient: 35.4\n",
      "Epoch 45201/150000, Train Loss: 4309, Val Loss: 4476,  Learning Rate: 0.00063, Train Gradient: 35.4\n",
      "Epoch 45301/150000, Train Loss: 4300, Val Loss: 4467,  Learning Rate: 0.00063, Train Gradient: 35.4\n",
      "Epoch 45401/150000, Train Loss: 4291, Val Loss: 4458,  Learning Rate: 0.00063, Train Gradient: 35.3\n",
      "Epoch 45501/150000, Train Loss: 4282, Val Loss: 4449,  Learning Rate: 0.00063, Train Gradient: 35.3\n",
      "Epoch 45601/150000, Train Loss: 4274, Val Loss: 4441,  Learning Rate: 0.00063, Train Gradient: 35.2\n",
      "Epoch 45701/150000, Train Loss: 4265, Val Loss: 4432,  Learning Rate: 0.00063, Train Gradient: 35.1\n",
      "Epoch 45801/150000, Train Loss: 4256, Val Loss: 4423,  Learning Rate: 0.00063, Train Gradient: 35.1\n",
      "Epoch 45901/150000, Train Loss: 4247, Val Loss: 4414,  Learning Rate: 0.00063, Train Gradient: 35.0\n",
      "Epoch 46001/150000, Train Loss: 4239, Val Loss: 4406,  Learning Rate: 0.00063, Train Gradient: 35.0\n",
      "Epoch 46101/150000, Train Loss: 4230, Val Loss: 4397,  Learning Rate: 0.00062, Train Gradient: 34.9\n",
      "Epoch 46201/150000, Train Loss: 4222, Val Loss: 4388,  Learning Rate: 0.00062, Train Gradient: 34.9\n",
      "Epoch 46301/150000, Train Loss: 4213, Val Loss: 4380,  Learning Rate: 0.00062, Train Gradient: 34.9\n",
      "Epoch 46401/150000, Train Loss: 4204, Val Loss: 4371,  Learning Rate: 0.00062, Train Gradient: 34.8\n",
      "Epoch 46501/150000, Train Loss: 4196, Val Loss: 4363,  Learning Rate: 0.00062, Train Gradient: 34.7\n",
      "Epoch 46601/150000, Train Loss: 4187, Val Loss: 4354,  Learning Rate: 0.00062, Train Gradient: 34.7\n",
      "Epoch 46701/150000, Train Loss: 4179, Val Loss: 4346,  Learning Rate: 0.00062, Train Gradient: 34.6\n",
      "Epoch 46801/150000, Train Loss: 4171, Val Loss: 4337,  Learning Rate: 0.00062, Train Gradient: 34.6\n",
      "Epoch 46901/150000, Train Loss: 4162, Val Loss: 4329,  Learning Rate: 0.00062, Train Gradient: 34.5\n",
      "Epoch 47001/150000, Train Loss: 4154, Val Loss: 4320,  Learning Rate: 0.00062, Train Gradient: 34.5\n",
      "Epoch 47101/150000, Train Loss: 4145, Val Loss: 4312,  Learning Rate: 0.00062, Train Gradient: 34.4\n",
      "Epoch 47201/150000, Train Loss: 4137, Val Loss: 4304,  Learning Rate: 0.00062, Train Gradient: 34.4\n",
      "Epoch 47301/150000, Train Loss: 4129, Val Loss: 4295,  Learning Rate: 0.00061, Train Gradient: 34.4\n",
      "Epoch 47401/150000, Train Loss: 4120, Val Loss: 4287,  Learning Rate: 0.00061, Train Gradient: 34.3\n",
      "Epoch 47501/150000, Train Loss: 4112, Val Loss: 4279,  Learning Rate: 0.00061, Train Gradient: 34.2\n",
      "Epoch 47601/150000, Train Loss: 4104, Val Loss: 4270,  Learning Rate: 0.00061, Train Gradient: 34.2\n",
      "Epoch 47701/150000, Train Loss: 4095, Val Loss: 4262,  Learning Rate: 0.00061, Train Gradient: 34.2\n",
      "Epoch 47801/150000, Train Loss: 4087, Val Loss: 4254,  Learning Rate: 0.00061, Train Gradient: 34.1\n",
      "Epoch 47901/150000, Train Loss: 4079, Val Loss: 4246,  Learning Rate: 0.00061, Train Gradient: 34.1\n",
      "Epoch 48001/150000, Train Loss: 4071, Val Loss: 4238,  Learning Rate: 0.00061, Train Gradient: 34.0\n",
      "Epoch 48101/150000, Train Loss: 4063, Val Loss: 4229,  Learning Rate: 0.00061, Train Gradient: 34.0\n",
      "Epoch 48201/150000, Train Loss: 4054, Val Loss: 4221,  Learning Rate: 0.00061, Train Gradient: 33.9\n",
      "Epoch 48301/150000, Train Loss: 4046, Val Loss: 4213,  Learning Rate: 0.00061, Train Gradient: 33.9\n",
      "Epoch 48401/150000, Train Loss: 4038, Val Loss: 4205,  Learning Rate: 0.00061, Train Gradient: 33.8\n",
      "Epoch 48501/150000, Train Loss: 4030, Val Loss: 4197,  Learning Rate: 0.00060, Train Gradient: 33.8\n",
      "Epoch 48601/150000, Train Loss: 4022, Val Loss: 4189,  Learning Rate: 0.00060, Train Gradient: 33.7\n",
      "Epoch 48701/150000, Train Loss: 4014, Val Loss: 4181,  Learning Rate: 0.00060, Train Gradient: 33.7\n",
      "Epoch 48801/150000, Train Loss: 4006, Val Loss: 4173,  Learning Rate: 0.00060, Train Gradient: 33.6\n",
      "Epoch 48901/150000, Train Loss: 3998, Val Loss: 4165,  Learning Rate: 0.00060, Train Gradient: 33.6\n",
      "Epoch 49001/150000, Train Loss: 3990, Val Loss: 4157,  Learning Rate: 0.00060, Train Gradient: 33.5\n",
      "Epoch 49101/150000, Train Loss: 3982, Val Loss: 4149,  Learning Rate: 0.00060, Train Gradient: 33.5\n",
      "Epoch 49201/150000, Train Loss: 3974, Val Loss: 4141,  Learning Rate: 0.00060, Train Gradient: 33.4\n",
      "Epoch 49301/150000, Train Loss: 3967, Val Loss: 4133,  Learning Rate: 0.00060, Train Gradient: 33.4\n",
      "Epoch 49401/150000, Train Loss: 3959, Val Loss: 4125,  Learning Rate: 0.00060, Train Gradient: 33.3\n",
      "Epoch 49501/150000, Train Loss: 3951, Val Loss: 4117,  Learning Rate: 0.00060, Train Gradient: 33.3\n",
      "Epoch 49601/150000, Train Loss: 3943, Val Loss: 4110,  Learning Rate: 0.00060, Train Gradient: 33.2\n",
      "Epoch 49701/150000, Train Loss: 3935, Val Loss: 4102,  Learning Rate: 0.00060, Train Gradient: 33.2\n",
      "Epoch 49801/150000, Train Loss: 3927, Val Loss: 4094,  Learning Rate: 0.00059, Train Gradient: 33.2\n",
      "Epoch 49901/150000, Train Loss: 3920, Val Loss: 4086,  Learning Rate: 0.00059, Train Gradient: 33.1\n",
      "Epoch 50001/150000, Train Loss: 3912, Val Loss: 4078,  Learning Rate: 0.00059, Train Gradient: 33.1\n",
      "Epoch 50101/150000, Train Loss: 3904, Val Loss: 4071,  Learning Rate: 0.00059, Train Gradient: 33.0\n",
      "Epoch 50201/150000, Train Loss: 3896, Val Loss: 4063,  Learning Rate: 0.00059, Train Gradient: 33.0\n",
      "Epoch 50301/150000, Train Loss: 3889, Val Loss: 4055,  Learning Rate: 0.00059, Train Gradient: 32.9\n",
      "Epoch 50401/150000, Train Loss: 3881, Val Loss: 4048,  Learning Rate: 0.00059, Train Gradient: 32.9\n",
      "Epoch 50501/150000, Train Loss: 3873, Val Loss: 4040,  Learning Rate: 0.00059, Train Gradient: 32.8\n",
      "Epoch 50601/150000, Train Loss: 3866, Val Loss: 4032,  Learning Rate: 0.00059, Train Gradient: 32.8\n",
      "Epoch 50701/150000, Train Loss: 3858, Val Loss: 4025,  Learning Rate: 0.00059, Train Gradient: 32.7\n",
      "Epoch 50801/150000, Train Loss: 3851, Val Loss: 4017,  Learning Rate: 0.00059, Train Gradient: 32.7\n",
      "Epoch 50901/150000, Train Loss: 3843, Val Loss: 4010,  Learning Rate: 0.00059, Train Gradient: 32.7\n",
      "Epoch 51001/150000, Train Loss: 3836, Val Loss: 4002,  Learning Rate: 0.00059, Train Gradient: 32.6\n",
      "Epoch 51101/150000, Train Loss: 3828, Val Loss: 3995,  Learning Rate: 0.00059, Train Gradient: 32.5\n",
      "Epoch 51201/150000, Train Loss: 3820, Val Loss: 3987,  Learning Rate: 0.00058, Train Gradient: 32.5\n",
      "Epoch 51301/150000, Train Loss: 3813, Val Loss: 3980,  Learning Rate: 0.00058, Train Gradient: 32.5\n",
      "Epoch 51401/150000, Train Loss: 3805, Val Loss: 3972,  Learning Rate: 0.00058, Train Gradient: 32.4\n",
      "Epoch 51501/150000, Train Loss: 3798, Val Loss: 3965,  Learning Rate: 0.00058, Train Gradient: 32.4\n",
      "Epoch 51601/150000, Train Loss: 3791, Val Loss: 3957,  Learning Rate: 0.00058, Train Gradient: 32.3\n",
      "Epoch 51701/150000, Train Loss: 3783, Val Loss: 3950,  Learning Rate: 0.00058, Train Gradient: 32.3\n",
      "Epoch 51801/150000, Train Loss: 3776, Val Loss: 3943,  Learning Rate: 0.00058, Train Gradient: 32.2\n",
      "Epoch 51901/150000, Train Loss: 3768, Val Loss: 3935,  Learning Rate: 0.00058, Train Gradient: 32.2\n",
      "Epoch 52001/150000, Train Loss: 3761, Val Loss: 3928,  Learning Rate: 0.00058, Train Gradient: 32.1\n",
      "Epoch 52101/150000, Train Loss: 3754, Val Loss: 3920,  Learning Rate: 0.00058, Train Gradient: 32.1\n",
      "Epoch 52201/150000, Train Loss: 3746, Val Loss: 3913,  Learning Rate: 0.00058, Train Gradient: 32.0\n",
      "Epoch 52301/150000, Train Loss: 3739, Val Loss: 3906,  Learning Rate: 0.00058, Train Gradient: 32.0\n",
      "Epoch 52401/150000, Train Loss: 3732, Val Loss: 3899,  Learning Rate: 0.00058, Train Gradient: 32.0\n",
      "Epoch 52501/150000, Train Loss: 3725, Val Loss: 3891,  Learning Rate: 0.00058, Train Gradient: 31.9\n",
      "Epoch 52601/150000, Train Loss: 3717, Val Loss: 3884,  Learning Rate: 0.00058, Train Gradient: 31.9\n",
      "Epoch 52701/150000, Train Loss: 3710, Val Loss: 3877,  Learning Rate: 0.00057, Train Gradient: 31.8\n",
      "Epoch 52801/150000, Train Loss: 3703, Val Loss: 3870,  Learning Rate: 0.00057, Train Gradient: 31.8\n",
      "Epoch 52901/150000, Train Loss: 3696, Val Loss: 3863,  Learning Rate: 0.00057, Train Gradient: 31.7\n",
      "Epoch 53001/150000, Train Loss: 3689, Val Loss: 3855,  Learning Rate: 0.00057, Train Gradient: 31.7\n",
      "Epoch 53101/150000, Train Loss: 3681, Val Loss: 3848,  Learning Rate: 0.00057, Train Gradient: 31.6\n",
      "Epoch 53201/150000, Train Loss: 3674, Val Loss: 3841,  Learning Rate: 0.00057, Train Gradient: 31.6\n",
      "Epoch 53301/150000, Train Loss: 3667, Val Loss: 3834,  Learning Rate: 0.00057, Train Gradient: 31.6\n",
      "Epoch 53401/150000, Train Loss: 3660, Val Loss: 3827,  Learning Rate: 0.00057, Train Gradient: 31.5\n",
      "Epoch 53501/150000, Train Loss: 3653, Val Loss: 3820,  Learning Rate: 0.00057, Train Gradient: 31.5\n",
      "Epoch 53601/150000, Train Loss: 3646, Val Loss: 3813,  Learning Rate: 0.00057, Train Gradient: 31.4\n",
      "Epoch 53701/150000, Train Loss: 3639, Val Loss: 3806,  Learning Rate: 0.00057, Train Gradient: 31.4\n",
      "Epoch 53801/150000, Train Loss: 3632, Val Loss: 3799,  Learning Rate: 0.00057, Train Gradient: 31.3\n",
      "Epoch 53901/150000, Train Loss: 3625, Val Loss: 3792,  Learning Rate: 0.00057, Train Gradient: 31.3\n",
      "Epoch 54001/150000, Train Loss: 3618, Val Loss: 3785,  Learning Rate: 0.00057, Train Gradient: 31.3\n",
      "Epoch 54101/150000, Train Loss: 3611, Val Loss: 3778,  Learning Rate: 0.00057, Train Gradient: 31.2\n",
      "Epoch 54201/150000, Train Loss: 3604, Val Loss: 3771,  Learning Rate: 0.00057, Train Gradient: 31.2\n",
      "Epoch 54301/150000, Train Loss: 3597, Val Loss: 3764,  Learning Rate: 0.00056, Train Gradient: 31.1\n",
      "Epoch 54401/150000, Train Loss: 3590, Val Loss: 3757,  Learning Rate: 0.00056, Train Gradient: 31.1\n",
      "Epoch 54501/150000, Train Loss: 3583, Val Loss: 3750,  Learning Rate: 0.00056, Train Gradient: 31.0\n",
      "Epoch 54601/150000, Train Loss: 3576, Val Loss: 3743,  Learning Rate: 0.00056, Train Gradient: 31.0\n",
      "Epoch 54701/150000, Train Loss: 3569, Val Loss: 3737,  Learning Rate: 0.00056, Train Gradient: 31.0\n",
      "Epoch 54801/150000, Train Loss: 3562, Val Loss: 3730,  Learning Rate: 0.00056, Train Gradient: 30.9\n",
      "Epoch 54901/150000, Train Loss: 3556, Val Loss: 3723,  Learning Rate: 0.00056, Train Gradient: 30.9\n",
      "Epoch 55001/150000, Train Loss: 3549, Val Loss: 3716,  Learning Rate: 0.00056, Train Gradient: 30.8\n",
      "Epoch 55101/150000, Train Loss: 3542, Val Loss: 3709,  Learning Rate: 0.00056, Train Gradient: 30.8\n",
      "Epoch 55201/150000, Train Loss: 3535, Val Loss: 3703,  Learning Rate: 0.00056, Train Gradient: 30.7\n",
      "Epoch 55301/150000, Train Loss: 3528, Val Loss: 3696,  Learning Rate: 0.00056, Train Gradient: 30.7\n",
      "Epoch 55401/150000, Train Loss: 3522, Val Loss: 3689,  Learning Rate: 0.00056, Train Gradient: 30.7\n",
      "Epoch 55501/150000, Train Loss: 3515, Val Loss: 3682,  Learning Rate: 0.00056, Train Gradient: 30.6\n",
      "Epoch 55601/150000, Train Loss: 3508, Val Loss: 3676,  Learning Rate: 0.00056, Train Gradient: 30.6\n",
      "Epoch 55701/150000, Train Loss: 3501, Val Loss: 3669,  Learning Rate: 0.00056, Train Gradient: 30.5\n",
      "Epoch 55801/150000, Train Loss: 3495, Val Loss: 3662,  Learning Rate: 0.00056, Train Gradient: 30.5\n",
      "Epoch 55901/150000, Train Loss: 3488, Val Loss: 3656,  Learning Rate: 0.00056, Train Gradient: 30.4\n",
      "Epoch 56001/150000, Train Loss: 3481, Val Loss: 3649,  Learning Rate: 0.00056, Train Gradient: 30.4\n",
      "Epoch 56101/150000, Train Loss: 3475, Val Loss: 3642,  Learning Rate: 0.00055, Train Gradient: 30.4\n",
      "Epoch 56201/150000, Train Loss: 3468, Val Loss: 3636,  Learning Rate: 0.00055, Train Gradient: 30.3\n",
      "Epoch 56301/150000, Train Loss: 3461, Val Loss: 3629,  Learning Rate: 0.00055, Train Gradient: 30.3\n",
      "Epoch 56401/150000, Train Loss: 3455, Val Loss: 3622,  Learning Rate: 0.00055, Train Gradient: 30.2\n",
      "Epoch 56501/150000, Train Loss: 3448, Val Loss: 3616,  Learning Rate: 0.00055, Train Gradient: 30.2\n",
      "Epoch 56601/150000, Train Loss: 3442, Val Loss: 3609,  Learning Rate: 0.00055, Train Gradient: 30.1\n",
      "Epoch 56701/150000, Train Loss: 3435, Val Loss: 3603,  Learning Rate: 0.00055, Train Gradient: 30.1\n",
      "Epoch 56801/150000, Train Loss: 3429, Val Loss: 3596,  Learning Rate: 0.00055, Train Gradient: 30.1\n",
      "Epoch 56901/150000, Train Loss: 3422, Val Loss: 3590,  Learning Rate: 0.00055, Train Gradient: 30.0\n",
      "Epoch 57001/150000, Train Loss: 3416, Val Loss: 3583,  Learning Rate: 0.00055, Train Gradient: 30.0\n",
      "Epoch 57101/150000, Train Loss: 3409, Val Loss: 3577,  Learning Rate: 0.00055, Train Gradient: 29.9\n",
      "Epoch 57201/150000, Train Loss: 3403, Val Loss: 3570,  Learning Rate: 0.00055, Train Gradient: 29.9\n",
      "Epoch 57301/150000, Train Loss: 3396, Val Loss: 3564,  Learning Rate: 0.00055, Train Gradient: 29.8\n",
      "Epoch 57401/150000, Train Loss: 3390, Val Loss: 3557,  Learning Rate: 0.00055, Train Gradient: 29.8\n",
      "Epoch 57501/150000, Train Loss: 3383, Val Loss: 3551,  Learning Rate: 0.00055, Train Gradient: 29.8\n",
      "Epoch 57601/150000, Train Loss: 3377, Val Loss: 3545,  Learning Rate: 0.00055, Train Gradient: 29.7\n",
      "Epoch 57701/150000, Train Loss: 3370, Val Loss: 3538,  Learning Rate: 0.00055, Train Gradient: 29.7\n",
      "Epoch 57801/150000, Train Loss: 3364, Val Loss: 3532,  Learning Rate: 0.00055, Train Gradient: 29.6\n",
      "Epoch 57901/150000, Train Loss: 3358, Val Loss: 3525,  Learning Rate: 0.00054, Train Gradient: 29.6\n",
      "Epoch 58001/150000, Train Loss: 3351, Val Loss: 3519,  Learning Rate: 0.00054, Train Gradient: 29.6\n",
      "Epoch 58101/150000, Train Loss: 3345, Val Loss: 3513,  Learning Rate: 0.00054, Train Gradient: 29.5\n",
      "Epoch 58201/150000, Train Loss: 3339, Val Loss: 3507,  Learning Rate: 0.00054, Train Gradient: 29.5\n",
      "Epoch 58301/150000, Train Loss: 3332, Val Loss: 3500,  Learning Rate: 0.00054, Train Gradient: 29.4\n",
      "Epoch 58401/150000, Train Loss: 3326, Val Loss: 3494,  Learning Rate: 0.00054, Train Gradient: 29.4\n",
      "Epoch 58501/150000, Train Loss: 3320, Val Loss: 3488,  Learning Rate: 0.00054, Train Gradient: 29.4\n",
      "Epoch 58601/150000, Train Loss: 3314, Val Loss: 3481,  Learning Rate: 0.00054, Train Gradient: 29.3\n",
      "Epoch 58701/150000, Train Loss: 3307, Val Loss: 3475,  Learning Rate: 0.00054, Train Gradient: 29.3\n",
      "Epoch 58801/150000, Train Loss: 3301, Val Loss: 3469,  Learning Rate: 0.00054, Train Gradient: 29.2\n",
      "Epoch 58901/150000, Train Loss: 3295, Val Loss: 3463,  Learning Rate: 0.00054, Train Gradient: 29.2\n",
      "Epoch 59001/150000, Train Loss: 3289, Val Loss: 3456,  Learning Rate: 0.00054, Train Gradient: 29.2\n",
      "Epoch 59101/150000, Train Loss: 3282, Val Loss: 3450,  Learning Rate: 0.00054, Train Gradient: 29.1\n",
      "Epoch 59201/150000, Train Loss: 3276, Val Loss: 3444,  Learning Rate: 0.00054, Train Gradient: 29.1\n",
      "Epoch 59301/150000, Train Loss: 3270, Val Loss: 3438,  Learning Rate: 0.00054, Train Gradient: 29.0\n",
      "Epoch 59401/150000, Train Loss: 3264, Val Loss: 3432,  Learning Rate: 0.00054, Train Gradient: 29.0\n",
      "Epoch 59501/150000, Train Loss: 3258, Val Loss: 3425,  Learning Rate: 0.00054, Train Gradient: 29.0\n",
      "Epoch 59601/150000, Train Loss: 3252, Val Loss: 3419,  Learning Rate: 0.00054, Train Gradient: 28.9\n",
      "Epoch 59701/150000, Train Loss: 3246, Val Loss: 3413,  Learning Rate: 0.00054, Train Gradient: 28.9\n",
      "Epoch 59801/150000, Train Loss: 3240, Val Loss: 3407,  Learning Rate: 0.00054, Train Gradient: 28.8\n",
      "Epoch 59901/150000, Train Loss: 3234, Val Loss: 3401,  Learning Rate: 0.00053, Train Gradient: 28.8\n",
      "Epoch 60001/150000, Train Loss: 3227, Val Loss: 3395,  Learning Rate: 0.00053, Train Gradient: 28.8\n",
      "Epoch 60101/150000, Train Loss: 3221, Val Loss: 3389,  Learning Rate: 0.00053, Train Gradient: 28.7\n",
      "Epoch 60201/150000, Train Loss: 3215, Val Loss: 3383,  Learning Rate: 0.00053, Train Gradient: 28.7\n",
      "Epoch 60301/150000, Train Loss: 3209, Val Loss: 3377,  Learning Rate: 0.00053, Train Gradient: 28.6\n",
      "Epoch 60401/150000, Train Loss: 3203, Val Loss: 3371,  Learning Rate: 0.00053, Train Gradient: 28.6\n",
      "Epoch 60501/150000, Train Loss: 3197, Val Loss: 3365,  Learning Rate: 0.00053, Train Gradient: 28.5\n",
      "Epoch 60601/150000, Train Loss: 3191, Val Loss: 3359,  Learning Rate: 0.00053, Train Gradient: 28.5\n",
      "Epoch 60701/150000, Train Loss: 3185, Val Loss: 3353,  Learning Rate: 0.00053, Train Gradient: 28.5\n",
      "Epoch 60801/150000, Train Loss: 3179, Val Loss: 3347,  Learning Rate: 0.00053, Train Gradient: 28.4\n",
      "Epoch 60901/150000, Train Loss: 3173, Val Loss: 3341,  Learning Rate: 0.00053, Train Gradient: 28.4\n",
      "Epoch 61001/150000, Train Loss: 3167, Val Loss: 3335,  Learning Rate: 0.00053, Train Gradient: 28.3\n",
      "Epoch 61101/150000, Train Loss: 3162, Val Loss: 3329,  Learning Rate: 0.00053, Train Gradient: 28.3\n",
      "Epoch 61201/150000, Train Loss: 3156, Val Loss: 3323,  Learning Rate: 0.00053, Train Gradient: 28.3\n",
      "Epoch 61301/150000, Train Loss: 3150, Val Loss: 3317,  Learning Rate: 0.00053, Train Gradient: 28.2\n",
      "Epoch 61401/150000, Train Loss: 3144, Val Loss: 3311,  Learning Rate: 0.00053, Train Gradient: 28.2\n",
      "Epoch 61501/150000, Train Loss: 3138, Val Loss: 3306,  Learning Rate: 0.00053, Train Gradient: 28.1\n",
      "Epoch 61601/150000, Train Loss: 3132, Val Loss: 3300,  Learning Rate: 0.00053, Train Gradient: 28.1\n",
      "Epoch 61701/150000, Train Loss: 3126, Val Loss: 3294,  Learning Rate: 0.00053, Train Gradient: 28.1\n",
      "Epoch 61801/150000, Train Loss: 3121, Val Loss: 3288,  Learning Rate: 0.00053, Train Gradient: 28.0\n",
      "Epoch 61901/150000, Train Loss: 3115, Val Loss: 3282,  Learning Rate: 0.00053, Train Gradient: 28.0\n",
      "Epoch 62001/150000, Train Loss: 3109, Val Loss: 3277,  Learning Rate: 0.00053, Train Gradient: 28.0\n",
      "Epoch 62101/150000, Train Loss: 3103, Val Loss: 3271,  Learning Rate: 0.00052, Train Gradient: 27.9\n",
      "Epoch 62201/150000, Train Loss: 3097, Val Loss: 3265,  Learning Rate: 0.00052, Train Gradient: 27.9\n",
      "Epoch 62301/150000, Train Loss: 3092, Val Loss: 3259,  Learning Rate: 0.00052, Train Gradient: 27.8\n",
      "Epoch 62401/150000, Train Loss: 3086, Val Loss: 3253,  Learning Rate: 0.00052, Train Gradient: 27.8\n",
      "Epoch 62501/150000, Train Loss: 3080, Val Loss: 3248,  Learning Rate: 0.00052, Train Gradient: 27.8\n",
      "Epoch 62601/150000, Train Loss: 3074, Val Loss: 3242,  Learning Rate: 0.00052, Train Gradient: 27.7\n",
      "Epoch 62701/150000, Train Loss: 3069, Val Loss: 3236,  Learning Rate: 0.00052, Train Gradient: 27.7\n",
      "Epoch 62801/150000, Train Loss: 3063, Val Loss: 3231,  Learning Rate: 0.00052, Train Gradient: 27.7\n",
      "Epoch 62901/150000, Train Loss: 3057, Val Loss: 3225,  Learning Rate: 0.00052, Train Gradient: 27.6\n",
      "Epoch 63001/150000, Train Loss: 3052, Val Loss: 3219,  Learning Rate: 0.00052, Train Gradient: 27.6\n",
      "Epoch 63101/150000, Train Loss: 3046, Val Loss: 3214,  Learning Rate: 0.00052, Train Gradient: 27.5\n",
      "Epoch 63201/150000, Train Loss: 3040, Val Loss: 3208,  Learning Rate: 0.00052, Train Gradient: 27.5\n",
      "Epoch 63301/150000, Train Loss: 3035, Val Loss: 3203,  Learning Rate: 0.00052, Train Gradient: 27.5\n",
      "Epoch 63401/150000, Train Loss: 3029, Val Loss: 3197,  Learning Rate: 0.00052, Train Gradient: 27.4\n",
      "Epoch 63501/150000, Train Loss: 3023, Val Loss: 3192,  Learning Rate: 0.00052, Train Gradient: 27.4\n",
      "Epoch 63601/150000, Train Loss: 3018, Val Loss: 3186,  Learning Rate: 0.00052, Train Gradient: 27.4\n",
      "Epoch 63701/150000, Train Loss: 3012, Val Loss: 3181,  Learning Rate: 0.00052, Train Gradient: 27.3\n",
      "Epoch 63801/150000, Train Loss: 3007, Val Loss: 3175,  Learning Rate: 0.00052, Train Gradient: 27.3\n",
      "Epoch 63901/150000, Train Loss: 3001, Val Loss: 3170,  Learning Rate: 0.00052, Train Gradient: 27.3\n",
      "Epoch 64001/150000, Train Loss: 2995, Val Loss: 3164,  Learning Rate: 0.00052, Train Gradient: 27.2\n",
      "Epoch 64101/150000, Train Loss: 2990, Val Loss: 3159,  Learning Rate: 0.00052, Train Gradient: 27.2\n",
      "Epoch 64201/150000, Train Loss: 2984, Val Loss: 3153,  Learning Rate: 0.00052, Train Gradient: 27.1\n",
      "Epoch 64301/150000, Train Loss: 2979, Val Loss: 3148,  Learning Rate: 0.00052, Train Gradient: 27.1\n",
      "Epoch 64401/150000, Train Loss: 2973, Val Loss: 3142,  Learning Rate: 0.00052, Train Gradient: 27.1\n",
      "Epoch 64501/150000, Train Loss: 2968, Val Loss: 3137,  Learning Rate: 0.00051, Train Gradient: 27.0\n",
      "Epoch 64601/150000, Train Loss: 2962, Val Loss: 3131,  Learning Rate: 0.00051, Train Gradient: 27.0\n",
      "Epoch 64701/150000, Train Loss: 2957, Val Loss: 3126,  Learning Rate: 0.00051, Train Gradient: 27.0\n",
      "Epoch 64801/150000, Train Loss: 2951, Val Loss: 3121,  Learning Rate: 0.00051, Train Gradient: 26.9\n",
      "Epoch 64901/150000, Train Loss: 2946, Val Loss: 3115,  Learning Rate: 0.00051, Train Gradient: 26.9\n",
      "Epoch 65001/150000, Train Loss: 2940, Val Loss: 3110,  Learning Rate: 0.00051, Train Gradient: 26.9\n",
      "Epoch 65101/150000, Train Loss: 2935, Val Loss: 3104,  Learning Rate: 0.00051, Train Gradient: 26.8\n",
      "Epoch 65201/150000, Train Loss: 2930, Val Loss: 3099,  Learning Rate: 0.00051, Train Gradient: 26.8\n",
      "Epoch 65301/150000, Train Loss: 2924, Val Loss: 3094,  Learning Rate: 0.00051, Train Gradient: 26.8\n",
      "Epoch 65401/150000, Train Loss: 2919, Val Loss: 3088,  Learning Rate: 0.00051, Train Gradient: 26.7\n",
      "Epoch 65501/150000, Train Loss: 2914, Val Loss: 3083,  Learning Rate: 0.00051, Train Gradient: 26.7\n",
      "Epoch 65601/150000, Train Loss: 2908, Val Loss: 3078,  Learning Rate: 0.00051, Train Gradient: 26.6\n",
      "Epoch 65701/150000, Train Loss: 2903, Val Loss: 3072,  Learning Rate: 0.00051, Train Gradient: 26.6\n",
      "Epoch 65801/150000, Train Loss: 2897, Val Loss: 3067,  Learning Rate: 0.00051, Train Gradient: 26.6\n",
      "Epoch 65901/150000, Train Loss: 2892, Val Loss: 3062,  Learning Rate: 0.00051, Train Gradient: 26.5\n",
      "Epoch 66001/150000, Train Loss: 2887, Val Loss: 3056,  Learning Rate: 0.00051, Train Gradient: 26.5\n",
      "Epoch 66101/150000, Train Loss: 2881, Val Loss: 3051,  Learning Rate: 0.00051, Train Gradient: 26.5\n",
      "Epoch 66201/150000, Train Loss: 2876, Val Loss: 3046,  Learning Rate: 0.00051, Train Gradient: 26.4\n",
      "Epoch 66301/150000, Train Loss: 2871, Val Loss: 3040,  Learning Rate: 0.00051, Train Gradient: 26.4\n",
      "Epoch 66401/150000, Train Loss: 2865, Val Loss: 3035,  Learning Rate: 0.00051, Train Gradient: 26.3\n",
      "Epoch 66501/150000, Train Loss: 2860, Val Loss: 3030,  Learning Rate: 0.00051, Train Gradient: 26.3\n",
      "Epoch 66601/150000, Train Loss: 2855, Val Loss: 3024,  Learning Rate: 0.00051, Train Gradient: 26.3\n",
      "Epoch 66701/150000, Train Loss: 2850, Val Loss: 3019,  Learning Rate: 0.00051, Train Gradient: 26.3\n",
      "Epoch 66801/150000, Train Loss: 2844, Val Loss: 3014,  Learning Rate: 0.00051, Train Gradient: 26.2\n",
      "Epoch 66901/150000, Train Loss: 2839, Val Loss: 3009,  Learning Rate: 0.00051, Train Gradient: 26.2\n",
      "Epoch 67001/150000, Train Loss: 2834, Val Loss: 3004,  Learning Rate: 0.00051, Train Gradient: 26.1\n",
      "Epoch 67101/150000, Train Loss: 2829, Val Loss: 2998,  Learning Rate: 0.00050, Train Gradient: 26.1\n",
      "Epoch 67201/150000, Train Loss: 2824, Val Loss: 2993,  Learning Rate: 0.00050, Train Gradient: 26.1\n",
      "Epoch 67301/150000, Train Loss: 2818, Val Loss: 2988,  Learning Rate: 0.00050, Train Gradient: 26.0\n",
      "Epoch 67401/150000, Train Loss: 2813, Val Loss: 2983,  Learning Rate: 0.00050, Train Gradient: 26.0\n",
      "Epoch 67501/150000, Train Loss: 2808, Val Loss: 2978,  Learning Rate: 0.00050, Train Gradient: 26.0\n",
      "Epoch 67601/150000, Train Loss: 2803, Val Loss: 2973,  Learning Rate: 0.00050, Train Gradient: 25.9\n",
      "Epoch 67701/150000, Train Loss: 2798, Val Loss: 2967,  Learning Rate: 0.00050, Train Gradient: 25.9\n",
      "Epoch 67801/150000, Train Loss: 2793, Val Loss: 2962,  Learning Rate: 0.00050, Train Gradient: 25.9\n",
      "Epoch 67901/150000, Train Loss: 2788, Val Loss: 2957,  Learning Rate: 0.00050, Train Gradient: 25.8\n",
      "Epoch 68001/150000, Train Loss: 2782, Val Loss: 2952,  Learning Rate: 0.00050, Train Gradient: 25.8\n",
      "Epoch 68101/150000, Train Loss: 2777, Val Loss: 2947,  Learning Rate: 0.00050, Train Gradient: 25.8\n",
      "Epoch 68201/150000, Train Loss: 2772, Val Loss: 2942,  Learning Rate: 0.00050, Train Gradient: 25.7\n",
      "Epoch 68301/150000, Train Loss: 2767, Val Loss: 2937,  Learning Rate: 0.00050, Train Gradient: 25.7\n",
      "Epoch 68401/150000, Train Loss: 2762, Val Loss: 2932,  Learning Rate: 0.00050, Train Gradient: 25.7\n",
      "Epoch 68501/150000, Train Loss: 2757, Val Loss: 2927,  Learning Rate: 0.00050, Train Gradient: 25.6\n",
      "Epoch 68601/150000, Train Loss: 2752, Val Loss: 2921,  Learning Rate: 0.00050, Train Gradient: 25.6\n",
      "Epoch 68701/150000, Train Loss: 2747, Val Loss: 2916,  Learning Rate: 0.00050, Train Gradient: 25.5\n",
      "Epoch 68801/150000, Train Loss: 2742, Val Loss: 2911,  Learning Rate: 0.00050, Train Gradient: 25.5\n",
      "Epoch 68901/150000, Train Loss: 2737, Val Loss: 2906,  Learning Rate: 0.00050, Train Gradient: 25.5\n",
      "Epoch 69001/150000, Train Loss: 2732, Val Loss: 2901,  Learning Rate: 0.00050, Train Gradient: 25.4\n",
      "Epoch 69101/150000, Train Loss: 2727, Val Loss: 2896,  Learning Rate: 0.00050, Train Gradient: 25.4\n",
      "Epoch 69201/150000, Train Loss: 2722, Val Loss: 2891,  Learning Rate: 0.00050, Train Gradient: 25.4\n",
      "Epoch 69301/150000, Train Loss: 2717, Val Loss: 2886,  Learning Rate: 0.00050, Train Gradient: 25.3\n",
      "Epoch 69401/150000, Train Loss: 2712, Val Loss: 2881,  Learning Rate: 0.00050, Train Gradient: 25.3\n",
      "Epoch 69501/150000, Train Loss: 2707, Val Loss: 2877,  Learning Rate: 0.00050, Train Gradient: 25.3\n",
      "Epoch 69601/150000, Train Loss: 2702, Val Loss: 2872,  Learning Rate: 0.00050, Train Gradient: 25.2\n",
      "Epoch 69701/150000, Train Loss: 2697, Val Loss: 2867,  Learning Rate: 0.00050, Train Gradient: 25.2\n",
      "Epoch 69801/150000, Train Loss: 2692, Val Loss: 2862,  Learning Rate: 0.00050, Train Gradient: 25.2\n",
      "Epoch 69901/150000, Train Loss: 2687, Val Loss: 2857,  Learning Rate: 0.00050, Train Gradient: 25.1\n",
      "Epoch 70001/150000, Train Loss: 2682, Val Loss: 2852,  Learning Rate: 0.00049, Train Gradient: 25.1\n",
      "Epoch 70101/150000, Train Loss: 2677, Val Loss: 2847,  Learning Rate: 0.00049, Train Gradient: 25.1\n",
      "Epoch 70201/150000, Train Loss: 2673, Val Loss: 2843,  Learning Rate: 0.00049, Train Gradient: 25.0\n",
      "Epoch 70301/150000, Train Loss: 2668, Val Loss: 2838,  Learning Rate: 0.00049, Train Gradient: 25.0\n",
      "Epoch 70401/150000, Train Loss: 2663, Val Loss: 2833,  Learning Rate: 0.00049, Train Gradient: 25.0\n",
      "Epoch 70501/150000, Train Loss: 2658, Val Loss: 2828,  Learning Rate: 0.00049, Train Gradient: 25.0\n",
      "Epoch 70601/150000, Train Loss: 2653, Val Loss: 2823,  Learning Rate: 0.00049, Train Gradient: 24.9\n",
      "Epoch 70701/150000, Train Loss: 2648, Val Loss: 2818,  Learning Rate: 0.00049, Train Gradient: 24.9\n",
      "Epoch 70801/150000, Train Loss: 2643, Val Loss: 2813,  Learning Rate: 0.00049, Train Gradient: 24.8\n",
      "Epoch 70901/150000, Train Loss: 2638, Val Loss: 2808,  Learning Rate: 0.00049, Train Gradient: 24.8\n",
      "Epoch 71001/150000, Train Loss: 2634, Val Loss: 2804,  Learning Rate: 0.00049, Train Gradient: 24.8\n",
      "Epoch 71101/150000, Train Loss: 2629, Val Loss: 2799,  Learning Rate: 0.00049, Train Gradient: 24.7\n",
      "Epoch 71201/150000, Train Loss: 2624, Val Loss: 2794,  Learning Rate: 0.00049, Train Gradient: 24.7\n",
      "Epoch 71301/150000, Train Loss: 2619, Val Loss: 2789,  Learning Rate: 0.00049, Train Gradient: 24.7\n",
      "Epoch 71401/150000, Train Loss: 2614, Val Loss: 2784,  Learning Rate: 0.00049, Train Gradient: 24.6\n",
      "Epoch 71501/150000, Train Loss: 2610, Val Loss: 2780,  Learning Rate: 0.00049, Train Gradient: 24.6\n",
      "Epoch 71601/150000, Train Loss: 2605, Val Loss: 2775,  Learning Rate: 0.00049, Train Gradient: 24.6\n",
      "Epoch 71701/150000, Train Loss: 2600, Val Loss: 2770,  Learning Rate: 0.00049, Train Gradient: 24.5\n",
      "Epoch 71801/150000, Train Loss: 2595, Val Loss: 2766,  Learning Rate: 0.00049, Train Gradient: 24.5\n",
      "Epoch 71901/150000, Train Loss: 2591, Val Loss: 2761,  Learning Rate: 0.00049, Train Gradient: 24.5\n",
      "Epoch 72001/150000, Train Loss: 2586, Val Loss: 2756,  Learning Rate: 0.00049, Train Gradient: 24.4\n",
      "Epoch 72101/150000, Train Loss: 2581, Val Loss: 2752,  Learning Rate: 0.00049, Train Gradient: 24.4\n",
      "Epoch 72201/150000, Train Loss: 2577, Val Loss: 2747,  Learning Rate: 0.00049, Train Gradient: 24.4\n",
      "Epoch 72301/150000, Train Loss: 2572, Val Loss: 2742,  Learning Rate: 0.00049, Train Gradient: 24.3\n",
      "Epoch 72401/150000, Train Loss: 2567, Val Loss: 2738,  Learning Rate: 0.00049, Train Gradient: 24.3\n",
      "Epoch 72501/150000, Train Loss: 2563, Val Loss: 2733,  Learning Rate: 0.00049, Train Gradient: 24.3\n",
      "Epoch 72601/150000, Train Loss: 2558, Val Loss: 2728,  Learning Rate: 0.00049, Train Gradient: 24.2\n",
      "Epoch 72701/150000, Train Loss: 2553, Val Loss: 2724,  Learning Rate: 0.00049, Train Gradient: 24.2\n",
      "Epoch 72801/150000, Train Loss: 2549, Val Loss: 2719,  Learning Rate: 0.00049, Train Gradient: 24.2\n",
      "Epoch 72901/150000, Train Loss: 2544, Val Loss: 2715,  Learning Rate: 0.00049, Train Gradient: 24.2\n",
      "Epoch 73001/150000, Train Loss: 2539, Val Loss: 2710,  Learning Rate: 0.00049, Train Gradient: 24.1\n",
      "Epoch 73101/150000, Train Loss: 2535, Val Loss: 2705,  Learning Rate: 0.00049, Train Gradient: 24.1\n",
      "Epoch 73201/150000, Train Loss: 2530, Val Loss: 2701,  Learning Rate: 0.00049, Train Gradient: 24.1\n",
      "Epoch 73301/150000, Train Loss: 2525, Val Loss: 2696,  Learning Rate: 0.00048, Train Gradient: 24.0\n",
      "Epoch 73401/150000, Train Loss: 2521, Val Loss: 2692,  Learning Rate: 0.00048, Train Gradient: 24.0\n",
      "Epoch 73501/150000, Train Loss: 2516, Val Loss: 2687,  Learning Rate: 0.00048, Train Gradient: 24.0\n",
      "Epoch 73601/150000, Train Loss: 2512, Val Loss: 2683,  Learning Rate: 0.00048, Train Gradient: 23.9\n",
      "Epoch 73701/150000, Train Loss: 2507, Val Loss: 2678,  Learning Rate: 0.00048, Train Gradient: 23.9\n",
      "Epoch 73801/150000, Train Loss: 2502, Val Loss: 2673,  Learning Rate: 0.00048, Train Gradient: 23.9\n",
      "Epoch 73901/150000, Train Loss: 2498, Val Loss: 2669,  Learning Rate: 0.00048, Train Gradient: 23.8\n",
      "Epoch 74001/150000, Train Loss: 2493, Val Loss: 2665,  Learning Rate: 0.00048, Train Gradient: 23.8\n",
      "Epoch 74101/150000, Train Loss: 2489, Val Loss: 2660,  Learning Rate: 0.00048, Train Gradient: 23.8\n",
      "Epoch 74201/150000, Train Loss: 2484, Val Loss: 2656,  Learning Rate: 0.00048, Train Gradient: 23.7\n",
      "Epoch 74301/150000, Train Loss: 2480, Val Loss: 2651,  Learning Rate: 0.00048, Train Gradient: 23.7\n",
      "Epoch 74401/150000, Train Loss: 2475, Val Loss: 2647,  Learning Rate: 0.00048, Train Gradient: 23.7\n",
      "Epoch 74501/150000, Train Loss: 2471, Val Loss: 2642,  Learning Rate: 0.00048, Train Gradient: 23.6\n",
      "Epoch 74601/150000, Train Loss: 2466, Val Loss: 2638,  Learning Rate: 0.00048, Train Gradient: 23.6\n",
      "Epoch 74701/150000, Train Loss: 2462, Val Loss: 2634,  Learning Rate: 0.00048, Train Gradient: 23.6\n",
      "Epoch 74801/150000, Train Loss: 2457, Val Loss: 2629,  Learning Rate: 0.00048, Train Gradient: 23.6\n",
      "Epoch 74901/150000, Train Loss: 2453, Val Loss: 2625,  Learning Rate: 0.00048, Train Gradient: 23.5\n",
      "Epoch 75001/150000, Train Loss: 2449, Val Loss: 2620,  Learning Rate: 0.00048, Train Gradient: 23.5\n",
      "Epoch 75101/150000, Train Loss: 2444, Val Loss: 2616,  Learning Rate: 0.00048, Train Gradient: 23.5\n",
      "Epoch 75201/150000, Train Loss: 2440, Val Loss: 2612,  Learning Rate: 0.00048, Train Gradient: 23.4\n",
      "Epoch 75301/150000, Train Loss: 2435, Val Loss: 2607,  Learning Rate: 0.00048, Train Gradient: 23.4\n",
      "Epoch 75401/150000, Train Loss: 2431, Val Loss: 2603,  Learning Rate: 0.00048, Train Gradient: 23.4\n",
      "Epoch 75501/150000, Train Loss: 2426, Val Loss: 2598,  Learning Rate: 0.00048, Train Gradient: 23.3\n",
      "Epoch 75601/150000, Train Loss: 2422, Val Loss: 2594,  Learning Rate: 0.00048, Train Gradient: 23.3\n",
      "Epoch 75701/150000, Train Loss: 2418, Val Loss: 2590,  Learning Rate: 0.00048, Train Gradient: 23.3\n",
      "Epoch 75801/150000, Train Loss: 2413, Val Loss: 2585,  Learning Rate: 0.00048, Train Gradient: 23.2\n",
      "Epoch 75901/150000, Train Loss: 2409, Val Loss: 2581,  Learning Rate: 0.00048, Train Gradient: 23.2\n",
      "Epoch 76001/150000, Train Loss: 2404, Val Loss: 2577,  Learning Rate: 0.00048, Train Gradient: 23.2\n",
      "Epoch 76101/150000, Train Loss: 2400, Val Loss: 2572,  Learning Rate: 0.00048, Train Gradient: 23.2\n",
      "Epoch 76201/150000, Train Loss: 2396, Val Loss: 2568,  Learning Rate: 0.00048, Train Gradient: 23.1\n",
      "Epoch 76301/150000, Train Loss: 2391, Val Loss: 2564,  Learning Rate: 0.00048, Train Gradient: 23.1\n",
      "Epoch 76401/150000, Train Loss: 2387, Val Loss: 2559,  Learning Rate: 0.00048, Train Gradient: 23.1\n",
      "Epoch 76501/150000, Train Loss: 2383, Val Loss: 2555,  Learning Rate: 0.00048, Train Gradient: 23.0\n",
      "Epoch 76601/150000, Train Loss: 2378, Val Loss: 2551,  Learning Rate: 0.00048, Train Gradient: 23.0\n",
      "Epoch 76701/150000, Train Loss: 2374, Val Loss: 2546,  Learning Rate: 0.00048, Train Gradient: 23.0\n",
      "Epoch 76801/150000, Train Loss: 2369, Val Loss: 2542,  Learning Rate: 0.00048, Train Gradient: 22.9\n",
      "Epoch 76901/150000, Train Loss: 2365, Val Loss: 2538,  Learning Rate: 0.00048, Train Gradient: 22.9\n",
      "Epoch 77001/150000, Train Loss: 2361, Val Loss: 2534,  Learning Rate: 0.00047, Train Gradient: 22.9\n",
      "Epoch 77101/150000, Train Loss: 2357, Val Loss: 2530,  Learning Rate: 0.00047, Train Gradient: 22.9\n",
      "Epoch 77201/150000, Train Loss: 2352, Val Loss: 2525,  Learning Rate: 0.00047, Train Gradient: 22.8\n",
      "Epoch 77301/150000, Train Loss: 2348, Val Loss: 2521,  Learning Rate: 0.00047, Train Gradient: 22.8\n",
      "Epoch 77401/150000, Train Loss: 2344, Val Loss: 2517,  Learning Rate: 0.00047, Train Gradient: 22.8\n",
      "Epoch 77501/150000, Train Loss: 2340, Val Loss: 2513,  Learning Rate: 0.00047, Train Gradient: 22.7\n",
      "Epoch 77601/150000, Train Loss: 2335, Val Loss: 2508,  Learning Rate: 0.00047, Train Gradient: 22.7\n",
      "Epoch 77701/150000, Train Loss: 2331, Val Loss: 2504,  Learning Rate: 0.00047, Train Gradient: 22.7\n",
      "Epoch 77801/150000, Train Loss: 2327, Val Loss: 2500,  Learning Rate: 0.00047, Train Gradient: 22.7\n",
      "Epoch 77901/150000, Train Loss: 2323, Val Loss: 2496,  Learning Rate: 0.00047, Train Gradient: 22.6\n",
      "Epoch 78001/150000, Train Loss: 2319, Val Loss: 2492,  Learning Rate: 0.00047, Train Gradient: 22.6\n",
      "Epoch 78101/150000, Train Loss: 2314, Val Loss: 2487,  Learning Rate: 0.00047, Train Gradient: 22.6\n",
      "Epoch 78201/150000, Train Loss: 2310, Val Loss: 2483,  Learning Rate: 0.00047, Train Gradient: 22.5\n",
      "Epoch 78301/150000, Train Loss: 2306, Val Loss: 2479,  Learning Rate: 0.00047, Train Gradient: 22.5\n",
      "Epoch 78401/150000, Train Loss: 2302, Val Loss: 2475,  Learning Rate: 0.00047, Train Gradient: 22.5\n",
      "Epoch 78501/150000, Train Loss: 2298, Val Loss: 2471,  Learning Rate: 0.00047, Train Gradient: 22.4\n",
      "Epoch 78601/150000, Train Loss: 2293, Val Loss: 2466,  Learning Rate: 0.00047, Train Gradient: 22.4\n",
      "Epoch 78701/150000, Train Loss: 2289, Val Loss: 2462,  Learning Rate: 0.00047, Train Gradient: 22.4\n",
      "Epoch 78801/150000, Train Loss: 2285, Val Loss: 2458,  Learning Rate: 0.00047, Train Gradient: 22.4\n",
      "Epoch 78901/150000, Train Loss: 2281, Val Loss: 2454,  Learning Rate: 0.00047, Train Gradient: 22.3\n",
      "Epoch 79001/150000, Train Loss: 2277, Val Loss: 2450,  Learning Rate: 0.00047, Train Gradient: 22.3\n",
      "Epoch 79101/150000, Train Loss: 2273, Val Loss: 2446,  Learning Rate: 0.00047, Train Gradient: 22.3\n",
      "Epoch 79201/150000, Train Loss: 2268, Val Loss: 2441,  Learning Rate: 0.00047, Train Gradient: 22.2\n",
      "Epoch 79301/150000, Train Loss: 2264, Val Loss: 2437,  Learning Rate: 0.00047, Train Gradient: 22.2\n",
      "Epoch 79401/150000, Train Loss: 2260, Val Loss: 2433,  Learning Rate: 0.00047, Train Gradient: 22.2\n",
      "Epoch 79501/150000, Train Loss: 2256, Val Loss: 2429,  Learning Rate: 0.00047, Train Gradient: 22.2\n",
      "Epoch 79601/150000, Train Loss: 2252, Val Loss: 2425,  Learning Rate: 0.00047, Train Gradient: 22.1\n",
      "Epoch 79701/150000, Train Loss: 2248, Val Loss: 2421,  Learning Rate: 0.00047, Train Gradient: 22.1\n",
      "Epoch 79801/150000, Train Loss: 2244, Val Loss: 2416,  Learning Rate: 0.00047, Train Gradient: 22.1\n",
      "Epoch 79901/150000, Train Loss: 2240, Val Loss: 2412,  Learning Rate: 0.00047, Train Gradient: 22.0\n",
      "Epoch 80001/150000, Train Loss: 2235, Val Loss: 2408,  Learning Rate: 0.00047, Train Gradient: 22.0\n",
      "Epoch 80101/150000, Train Loss: 2231, Val Loss: 2404,  Learning Rate: 0.00047, Train Gradient: 22.0\n",
      "Epoch 80201/150000, Train Loss: 2227, Val Loss: 2400,  Learning Rate: 0.00047, Train Gradient: 22.0\n",
      "Epoch 80301/150000, Train Loss: 2223, Val Loss: 2396,  Learning Rate: 0.00047, Train Gradient: 22.0\n",
      "Epoch 80401/150000, Train Loss: 2219, Val Loss: 2392,  Learning Rate: 0.00047, Train Gradient: 21.9\n",
      "Epoch 80501/150000, Train Loss: 2215, Val Loss: 2388,  Learning Rate: 0.00047, Train Gradient: 21.9\n",
      "Epoch 80601/150000, Train Loss: 2211, Val Loss: 2384,  Learning Rate: 0.00047, Train Gradient: 21.9\n",
      "Epoch 80701/150000, Train Loss: 2207, Val Loss: 2380,  Learning Rate: 0.00047, Train Gradient: 21.8\n",
      "Epoch 80801/150000, Train Loss: 2203, Val Loss: 2376,  Learning Rate: 0.00047, Train Gradient: 21.8\n",
      "Epoch 80901/150000, Train Loss: 2199, Val Loss: 2372,  Learning Rate: 0.00047, Train Gradient: 21.8\n",
      "Epoch 81001/150000, Train Loss: 2195, Val Loss: 2368,  Learning Rate: 0.00047, Train Gradient: 21.7\n",
      "Epoch 81101/150000, Train Loss: 2191, Val Loss: 2364,  Learning Rate: 0.00047, Train Gradient: 21.7\n",
      "Epoch 81201/150000, Train Loss: 2187, Val Loss: 2360,  Learning Rate: 0.00047, Train Gradient: 21.7\n",
      "Epoch 81301/150000, Train Loss: 2183, Val Loss: 2356,  Learning Rate: 0.00046, Train Gradient: 21.7\n",
      "Epoch 81401/150000, Train Loss: 2179, Val Loss: 2352,  Learning Rate: 0.00046, Train Gradient: 21.6\n",
      "Epoch 81501/150000, Train Loss: 2175, Val Loss: 2348,  Learning Rate: 0.00046, Train Gradient: 21.6\n",
      "Epoch 81601/150000, Train Loss: 2171, Val Loss: 2344,  Learning Rate: 0.00046, Train Gradient: 21.6\n",
      "Epoch 81701/150000, Train Loss: 2168, Val Loss: 2341,  Learning Rate: 0.00046, Train Gradient: 21.6\n",
      "Epoch 81801/150000, Train Loss: 2164, Val Loss: 2337,  Learning Rate: 0.00046, Train Gradient: 21.5\n",
      "Epoch 81901/150000, Train Loss: 2160, Val Loss: 2333,  Learning Rate: 0.00046, Train Gradient: 21.5\n",
      "Epoch 82001/150000, Train Loss: 2156, Val Loss: 2329,  Learning Rate: 0.00046, Train Gradient: 21.5\n",
      "Epoch 82101/150000, Train Loss: 2152, Val Loss: 2325,  Learning Rate: 0.00046, Train Gradient: 21.4\n",
      "Epoch 82201/150000, Train Loss: 2148, Val Loss: 2321,  Learning Rate: 0.00046, Train Gradient: 21.4\n",
      "Epoch 82301/150000, Train Loss: 2144, Val Loss: 2317,  Learning Rate: 0.00046, Train Gradient: 21.4\n",
      "Epoch 82401/150000, Train Loss: 2140, Val Loss: 2313,  Learning Rate: 0.00046, Train Gradient: 21.4\n",
      "Epoch 82501/150000, Train Loss: 2136, Val Loss: 2309,  Learning Rate: 0.00046, Train Gradient: 21.3\n",
      "Epoch 82601/150000, Train Loss: 2132, Val Loss: 2306,  Learning Rate: 0.00046, Train Gradient: 21.3\n",
      "Epoch 82701/150000, Train Loss: 2128, Val Loss: 2302,  Learning Rate: 0.00046, Train Gradient: 21.3\n",
      "Epoch 82801/150000, Train Loss: 2124, Val Loss: 2298,  Learning Rate: 0.00046, Train Gradient: 21.2\n",
      "Epoch 82901/150000, Train Loss: 2120, Val Loss: 2294,  Learning Rate: 0.00046, Train Gradient: 21.2\n",
      "Epoch 83001/150000, Train Loss: 2117, Val Loss: 2290,  Learning Rate: 0.00046, Train Gradient: 21.2\n",
      "Epoch 83101/150000, Train Loss: 2113, Val Loss: 2286,  Learning Rate: 0.00046, Train Gradient: 21.2\n",
      "Epoch 83201/150000, Train Loss: 2109, Val Loss: 2282,  Learning Rate: 0.00046, Train Gradient: 21.1\n",
      "Epoch 83301/150000, Train Loss: 2105, Val Loss: 2279,  Learning Rate: 0.00046, Train Gradient: 21.1\n",
      "Epoch 83401/150000, Train Loss: 2101, Val Loss: 2275,  Learning Rate: 0.00046, Train Gradient: 21.1\n",
      "Epoch 83501/150000, Train Loss: 2097, Val Loss: 2271,  Learning Rate: 0.00046, Train Gradient: 21.1\n",
      "Epoch 83601/150000, Train Loss: 2093, Val Loss: 2267,  Learning Rate: 0.00046, Train Gradient: 21.0\n",
      "Epoch 83701/150000, Train Loss: 2090, Val Loss: 2263,  Learning Rate: 0.00046, Train Gradient: 21.0\n",
      "Epoch 83801/150000, Train Loss: 2086, Val Loss: 2260,  Learning Rate: 0.00046, Train Gradient: 21.0\n",
      "Epoch 83901/150000, Train Loss: 2082, Val Loss: 2256,  Learning Rate: 0.00046, Train Gradient: 20.9\n",
      "Epoch 84001/150000, Train Loss: 2078, Val Loss: 2252,  Learning Rate: 0.00046, Train Gradient: 20.9\n",
      "Epoch 84101/150000, Train Loss: 2074, Val Loss: 2249,  Learning Rate: 0.00046, Train Gradient: 20.9\n",
      "Epoch 84201/150000, Train Loss: 2071, Val Loss: 2245,  Learning Rate: 0.00046, Train Gradient: 20.9\n",
      "Epoch 84301/150000, Train Loss: 2067, Val Loss: 2241,  Learning Rate: 0.00046, Train Gradient: 20.8\n",
      "Epoch 84401/150000, Train Loss: 2063, Val Loss: 2238,  Learning Rate: 0.00046, Train Gradient: 20.8\n",
      "Epoch 84501/150000, Train Loss: 2059, Val Loss: 2234,  Learning Rate: 0.00046, Train Gradient: 20.8\n",
      "Epoch 84601/150000, Train Loss: 2056, Val Loss: 2230,  Learning Rate: 0.00046, Train Gradient: 20.8\n",
      "Epoch 84701/150000, Train Loss: 2052, Val Loss: 2226,  Learning Rate: 0.00046, Train Gradient: 20.7\n",
      "Epoch 84801/150000, Train Loss: 2048, Val Loss: 2223,  Learning Rate: 0.00046, Train Gradient: 20.7\n",
      "Epoch 84901/150000, Train Loss: 2044, Val Loss: 2219,  Learning Rate: 0.00046, Train Gradient: 20.7\n",
      "Epoch 85001/150000, Train Loss: 2041, Val Loss: 2216,  Learning Rate: 0.00046, Train Gradient: 20.7\n",
      "Epoch 85101/150000, Train Loss: 2037, Val Loss: 2212,  Learning Rate: 0.00046, Train Gradient: 20.6\n",
      "Epoch 85201/150000, Train Loss: 2033, Val Loss: 2208,  Learning Rate: 0.00046, Train Gradient: 20.6\n",
      "Epoch 85301/150000, Train Loss: 2030, Val Loss: 2205,  Learning Rate: 0.00046, Train Gradient: 20.6\n",
      "Epoch 85401/150000, Train Loss: 2026, Val Loss: 2201,  Learning Rate: 0.00046, Train Gradient: 20.6\n",
      "Epoch 85501/150000, Train Loss: 2022, Val Loss: 2197,  Learning Rate: 0.00046, Train Gradient: 20.5\n",
      "Epoch 85601/150000, Train Loss: 2018, Val Loss: 2194,  Learning Rate: 0.00046, Train Gradient: 20.5\n",
      "Epoch 85701/150000, Train Loss: 2015, Val Loss: 2190,  Learning Rate: 0.00046, Train Gradient: 20.5\n",
      "Epoch 85801/150000, Train Loss: 2011, Val Loss: 2186,  Learning Rate: 0.00046, Train Gradient: 20.5\n",
      "Epoch 85901/150000, Train Loss: 2007, Val Loss: 2183,  Learning Rate: 0.00046, Train Gradient: 20.4\n",
      "Epoch 86001/150000, Train Loss: 2004, Val Loss: 2179,  Learning Rate: 0.00046, Train Gradient: 20.4\n",
      "Epoch 86101/150000, Train Loss: 2000, Val Loss: 2176,  Learning Rate: 0.00046, Train Gradient: 20.4\n",
      "Epoch 86201/150000, Train Loss: 1996, Val Loss: 2172,  Learning Rate: 0.00046, Train Gradient: 20.3\n",
      "Epoch 86301/150000, Train Loss: 1993, Val Loss: 2168,  Learning Rate: 0.00045, Train Gradient: 20.3\n",
      "Epoch 86401/150000, Train Loss: 1989, Val Loss: 2165,  Learning Rate: 0.00045, Train Gradient: 20.3\n",
      "Epoch 86501/150000, Train Loss: 1985, Val Loss: 2161,  Learning Rate: 0.00045, Train Gradient: 20.3\n",
      "Epoch 86601/150000, Train Loss: 1982, Val Loss: 2158,  Learning Rate: 0.00045, Train Gradient: 20.2\n",
      "Epoch 86701/150000, Train Loss: 1978, Val Loss: 2154,  Learning Rate: 0.00045, Train Gradient: 20.2\n",
      "Epoch 86801/150000, Train Loss: 1974, Val Loss: 2150,  Learning Rate: 0.00045, Train Gradient: 20.2\n",
      "Epoch 86901/150000, Train Loss: 1971, Val Loss: 2147,  Learning Rate: 0.00045, Train Gradient: 20.2\n",
      "Epoch 87001/150000, Train Loss: 1967, Val Loss: 2143,  Learning Rate: 0.00045, Train Gradient: 20.1\n",
      "Epoch 87101/150000, Train Loss: 1964, Val Loss: 2140,  Learning Rate: 0.00045, Train Gradient: 20.1\n",
      "Epoch 87201/150000, Train Loss: 1960, Val Loss: 2136,  Learning Rate: 0.00045, Train Gradient: 20.1\n",
      "Epoch 87301/150000, Train Loss: 1956, Val Loss: 2133,  Learning Rate: 0.00045, Train Gradient: 20.1\n",
      "Epoch 87401/150000, Train Loss: 1953, Val Loss: 2129,  Learning Rate: 0.00045, Train Gradient: 20.0\n",
      "Epoch 87501/150000, Train Loss: 1949, Val Loss: 2126,  Learning Rate: 0.00045, Train Gradient: 20.0\n",
      "Epoch 87601/150000, Train Loss: 1946, Val Loss: 2122,  Learning Rate: 0.00045, Train Gradient: 20.0\n",
      "Epoch 87701/150000, Train Loss: 1942, Val Loss: 2119,  Learning Rate: 0.00045, Train Gradient: 19.9\n",
      "Epoch 87801/150000, Train Loss: 1938, Val Loss: 2115,  Learning Rate: 0.00045, Train Gradient: 19.9\n",
      "Epoch 87901/150000, Train Loss: 1935, Val Loss: 2112,  Learning Rate: 0.00045, Train Gradient: 19.9\n",
      "Epoch 88001/150000, Train Loss: 1931, Val Loss: 2108,  Learning Rate: 0.00045, Train Gradient: 19.9\n",
      "Epoch 88101/150000, Train Loss: 1928, Val Loss: 2105,  Learning Rate: 0.00045, Train Gradient: 19.8\n",
      "Epoch 88201/150000, Train Loss: 1924, Val Loss: 2101,  Learning Rate: 0.00045, Train Gradient: 19.8\n",
      "Epoch 88301/150000, Train Loss: 1921, Val Loss: 2098,  Learning Rate: 0.00045, Train Gradient: 19.8\n",
      "Epoch 88401/150000, Train Loss: 1917, Val Loss: 2094,  Learning Rate: 0.00045, Train Gradient: 19.8\n",
      "Epoch 88501/150000, Train Loss: 1914, Val Loss: 2091,  Learning Rate: 0.00045, Train Gradient: 19.8\n",
      "Epoch 88601/150000, Train Loss: 1910, Val Loss: 2087,  Learning Rate: 0.00045, Train Gradient: 19.7\n",
      "Epoch 88701/150000, Train Loss: 1907, Val Loss: 2084,  Learning Rate: 0.00045, Train Gradient: 19.7\n",
      "Epoch 88801/150000, Train Loss: 1903, Val Loss: 2081,  Learning Rate: 0.00045, Train Gradient: 19.7\n",
      "Epoch 88901/150000, Train Loss: 1900, Val Loss: 2077,  Learning Rate: 0.00045, Train Gradient: 19.6\n",
      "Epoch 89001/150000, Train Loss: 1896, Val Loss: 2074,  Learning Rate: 0.00045, Train Gradient: 19.6\n",
      "Epoch 89101/150000, Train Loss: 1893, Val Loss: 2070,  Learning Rate: 0.00045, Train Gradient: 19.6\n",
      "Epoch 89201/150000, Train Loss: 1889, Val Loss: 2067,  Learning Rate: 0.00045, Train Gradient: 19.5\n",
      "Epoch 89301/150000, Train Loss: 1886, Val Loss: 2064,  Learning Rate: 0.00045, Train Gradient: 19.5\n",
      "Epoch 89401/150000, Train Loss: 1882, Val Loss: 2060,  Learning Rate: 0.00045, Train Gradient: 19.5\n",
      "Epoch 89501/150000, Train Loss: 1879, Val Loss: 2057,  Learning Rate: 0.00045, Train Gradient: 19.5\n",
      "Epoch 89601/150000, Train Loss: 1876, Val Loss: 2054,  Learning Rate: 0.00045, Train Gradient: 19.5\n",
      "Epoch 89701/150000, Train Loss: 1872, Val Loss: 2050,  Learning Rate: 0.00045, Train Gradient: 19.4\n",
      "Epoch 89801/150000, Train Loss: 1869, Val Loss: 2047,  Learning Rate: 0.00045, Train Gradient: 19.4\n",
      "Epoch 89901/150000, Train Loss: 1865, Val Loss: 2043,  Learning Rate: 0.00045, Train Gradient: 19.4\n",
      "Epoch 90001/150000, Train Loss: 1862, Val Loss: 2040,  Learning Rate: 0.00045, Train Gradient: 19.4\n",
      "Epoch 90101/150000, Train Loss: 1858, Val Loss: 2037,  Learning Rate: 0.00045, Train Gradient: 19.3\n",
      "Epoch 90201/150000, Train Loss: 1855, Val Loss: 2033,  Learning Rate: 0.00045, Train Gradient: 19.3\n",
      "Epoch 90301/150000, Train Loss: 1852, Val Loss: 2030,  Learning Rate: 0.00045, Train Gradient: 19.3\n",
      "Epoch 90401/150000, Train Loss: 1848, Val Loss: 2027,  Learning Rate: 0.00045, Train Gradient: 19.2\n",
      "Epoch 90501/150000, Train Loss: 1845, Val Loss: 2023,  Learning Rate: 0.00045, Train Gradient: 19.2\n",
      "Epoch 90601/150000, Train Loss: 1841, Val Loss: 2020,  Learning Rate: 0.00045, Train Gradient: 19.2\n",
      "Epoch 90701/150000, Train Loss: 1838, Val Loss: 2017,  Learning Rate: 0.00045, Train Gradient: 19.2\n",
      "Epoch 90801/150000, Train Loss: 1835, Val Loss: 2013,  Learning Rate: 0.00045, Train Gradient: 19.1\n",
      "Epoch 90901/150000, Train Loss: 1831, Val Loss: 2010,  Learning Rate: 0.00045, Train Gradient: 19.1\n",
      "Epoch 91001/150000, Train Loss: 1828, Val Loss: 2007,  Learning Rate: 0.00045, Train Gradient: 19.1\n",
      "Epoch 91101/150000, Train Loss: 1824, Val Loss: 2003,  Learning Rate: 0.00045, Train Gradient: 19.1\n",
      "Epoch 91201/150000, Train Loss: 1821, Val Loss: 2000,  Learning Rate: 0.00045, Train Gradient: 19.0\n",
      "Epoch 91301/150000, Train Loss: 1818, Val Loss: 1997,  Learning Rate: 0.00045, Train Gradient: 19.0\n",
      "Epoch 91401/150000, Train Loss: 1814, Val Loss: 1993,  Learning Rate: 0.00045, Train Gradient: 19.0\n",
      "Epoch 91501/150000, Train Loss: 1811, Val Loss: 1990,  Learning Rate: 0.00045, Train Gradient: 19.0\n",
      "Epoch 91601/150000, Train Loss: 1808, Val Loss: 1987,  Learning Rate: 0.00045, Train Gradient: 18.9\n",
      "Epoch 91701/150000, Train Loss: 1804, Val Loss: 1983,  Learning Rate: 0.00045, Train Gradient: 18.9\n",
      "Epoch 91801/150000, Train Loss: 1801, Val Loss: 1980,  Learning Rate: 0.00045, Train Gradient: 18.9\n",
      "Epoch 91901/150000, Train Loss: 1797, Val Loss: 1977,  Learning Rate: 0.00045, Train Gradient: 18.9\n",
      "Epoch 92001/150000, Train Loss: 1794, Val Loss: 1973,  Learning Rate: 0.00045, Train Gradient: 18.9\n",
      "Epoch 92101/150000, Train Loss: 1791, Val Loss: 1970,  Learning Rate: 0.00045, Train Gradient: 18.8\n",
      "Epoch 92201/150000, Train Loss: 1787, Val Loss: 1967,  Learning Rate: 0.00045, Train Gradient: 18.8\n",
      "Epoch 92301/150000, Train Loss: 1784, Val Loss: 1963,  Learning Rate: 0.00045, Train Gradient: 18.8\n",
      "Epoch 92401/150000, Train Loss: 1781, Val Loss: 1960,  Learning Rate: 0.00044, Train Gradient: 18.7\n",
      "Epoch 92501/150000, Train Loss: 1778, Val Loss: 1957,  Learning Rate: 0.00044, Train Gradient: 18.7\n",
      "Epoch 92601/150000, Train Loss: 1774, Val Loss: 1954,  Learning Rate: 0.00044, Train Gradient: 18.7\n",
      "Epoch 92701/150000, Train Loss: 1771, Val Loss: 1950,  Learning Rate: 0.00044, Train Gradient: 18.7\n",
      "Epoch 92801/150000, Train Loss: 1768, Val Loss: 1947,  Learning Rate: 0.00044, Train Gradient: 18.6\n",
      "Epoch 92901/150000, Train Loss: 1764, Val Loss: 1944,  Learning Rate: 0.00044, Train Gradient: 18.6\n",
      "Epoch 93001/150000, Train Loss: 1761, Val Loss: 1941,  Learning Rate: 0.00044, Train Gradient: 18.6\n",
      "Epoch 93101/150000, Train Loss: 1758, Val Loss: 1938,  Learning Rate: 0.00044, Train Gradient: 18.6\n",
      "Epoch 93201/150000, Train Loss: 1755, Val Loss: 1934,  Learning Rate: 0.00044, Train Gradient: 18.5\n",
      "Epoch 93301/150000, Train Loss: 1752, Val Loss: 1931,  Learning Rate: 0.00044, Train Gradient: 18.5\n",
      "Epoch 93401/150000, Train Loss: 1748, Val Loss: 1928,  Learning Rate: 0.00044, Train Gradient: 18.5\n",
      "Epoch 93501/150000, Train Loss: 1745, Val Loss: 1925,  Learning Rate: 0.00044, Train Gradient: 18.5\n",
      "Epoch 93601/150000, Train Loss: 1742, Val Loss: 1921,  Learning Rate: 0.00044, Train Gradient: 18.4\n",
      "Epoch 93701/150000, Train Loss: 1739, Val Loss: 1918,  Learning Rate: 0.00044, Train Gradient: 18.4\n",
      "Epoch 93801/150000, Train Loss: 1735, Val Loss: 1915,  Learning Rate: 0.00044, Train Gradient: 18.4\n",
      "Epoch 93901/150000, Train Loss: 1732, Val Loss: 1912,  Learning Rate: 0.00044, Train Gradient: 18.4\n",
      "Epoch 94001/150000, Train Loss: 1729, Val Loss: 1909,  Learning Rate: 0.00044, Train Gradient: 18.3\n",
      "Epoch 94101/150000, Train Loss: 1726, Val Loss: 1906,  Learning Rate: 0.00044, Train Gradient: 18.3\n",
      "Epoch 94201/150000, Train Loss: 1723, Val Loss: 1902,  Learning Rate: 0.00044, Train Gradient: 18.3\n",
      "Epoch 94301/150000, Train Loss: 1720, Val Loss: 1899,  Learning Rate: 0.00044, Train Gradient: 18.3\n",
      "Epoch 94401/150000, Train Loss: 1716, Val Loss: 1896,  Learning Rate: 0.00044, Train Gradient: 18.2\n",
      "Epoch 94501/150000, Train Loss: 1713, Val Loss: 1893,  Learning Rate: 0.00044, Train Gradient: 18.2\n",
      "Epoch 94601/150000, Train Loss: 1710, Val Loss: 1890,  Learning Rate: 0.00044, Train Gradient: 18.2\n",
      "Epoch 94701/150000, Train Loss: 1707, Val Loss: 1886,  Learning Rate: 0.00044, Train Gradient: 18.2\n",
      "Epoch 94801/150000, Train Loss: 1704, Val Loss: 1883,  Learning Rate: 0.00044, Train Gradient: 18.1\n",
      "Epoch 94901/150000, Train Loss: 1701, Val Loss: 1880,  Learning Rate: 0.00044, Train Gradient: 18.1\n",
      "Epoch 95001/150000, Train Loss: 1697, Val Loss: 1877,  Learning Rate: 0.00044, Train Gradient: 18.1\n",
      "Epoch 95101/150000, Train Loss: 1694, Val Loss: 1874,  Learning Rate: 0.00044, Train Gradient: 18.1\n",
      "Epoch 95201/150000, Train Loss: 1691, Val Loss: 1871,  Learning Rate: 0.00044, Train Gradient: 18.0\n",
      "Epoch 95301/150000, Train Loss: 1688, Val Loss: 1868,  Learning Rate: 0.00044, Train Gradient: 18.0\n",
      "Epoch 95401/150000, Train Loss: 1685, Val Loss: 1864,  Learning Rate: 0.00044, Train Gradient: 18.0\n",
      "Epoch 95501/150000, Train Loss: 1682, Val Loss: 1861,  Learning Rate: 0.00044, Train Gradient: 18.0\n",
      "Epoch 95601/150000, Train Loss: 1679, Val Loss: 1858,  Learning Rate: 0.00044, Train Gradient: 17.9\n",
      "Epoch 95701/150000, Train Loss: 1675, Val Loss: 1855,  Learning Rate: 0.00044, Train Gradient: 17.9\n",
      "Epoch 95801/150000, Train Loss: 1672, Val Loss: 1852,  Learning Rate: 0.00044, Train Gradient: 17.9\n",
      "Epoch 95901/150000, Train Loss: 1669, Val Loss: 1849,  Learning Rate: 0.00044, Train Gradient: 17.9\n",
      "Epoch 96001/150000, Train Loss: 1666, Val Loss: 1846,  Learning Rate: 0.00044, Train Gradient: 17.8\n",
      "Epoch 96101/150000, Train Loss: 1663, Val Loss: 1843,  Learning Rate: 0.00044, Train Gradient: 17.8\n",
      "Epoch 96201/150000, Train Loss: 1660, Val Loss: 1840,  Learning Rate: 0.00044, Train Gradient: 17.8\n",
      "Epoch 96301/150000, Train Loss: 1657, Val Loss: 1837,  Learning Rate: 0.00044, Train Gradient: 17.8\n",
      "Epoch 96401/150000, Train Loss: 1654, Val Loss: 1833,  Learning Rate: 0.00044, Train Gradient: 17.7\n",
      "Epoch 96501/150000, Train Loss: 1651, Val Loss: 1830,  Learning Rate: 0.00044, Train Gradient: 17.7\n",
      "Epoch 96601/150000, Train Loss: 1648, Val Loss: 1827,  Learning Rate: 0.00044, Train Gradient: 17.7\n",
      "Epoch 96701/150000, Train Loss: 1644, Val Loss: 1824,  Learning Rate: 0.00044, Train Gradient: 17.7\n",
      "Epoch 96801/150000, Train Loss: 1641, Val Loss: 1821,  Learning Rate: 0.00044, Train Gradient: 17.6\n",
      "Epoch 96901/150000, Train Loss: 1638, Val Loss: 1818,  Learning Rate: 0.00044, Train Gradient: 17.6\n",
      "Epoch 97001/150000, Train Loss: 1635, Val Loss: 1815,  Learning Rate: 0.00044, Train Gradient: 17.6\n",
      "Epoch 97101/150000, Train Loss: 1632, Val Loss: 1812,  Learning Rate: 0.00044, Train Gradient: 17.6\n",
      "Epoch 97201/150000, Train Loss: 1629, Val Loss: 1809,  Learning Rate: 0.00044, Train Gradient: 17.5\n",
      "Epoch 97301/150000, Train Loss: 1626, Val Loss: 1806,  Learning Rate: 0.00044, Train Gradient: 17.5\n",
      "Epoch 97401/150000, Train Loss: 1623, Val Loss: 1803,  Learning Rate: 0.00044, Train Gradient: 17.5\n",
      "Epoch 97501/150000, Train Loss: 1620, Val Loss: 1800,  Learning Rate: 0.00044, Train Gradient: 17.5\n",
      "Epoch 97601/150000, Train Loss: 1617, Val Loss: 1797,  Learning Rate: 0.00044, Train Gradient: 17.5\n",
      "Epoch 97701/150000, Train Loss: 1614, Val Loss: 1794,  Learning Rate: 0.00044, Train Gradient: 17.4\n",
      "Epoch 97801/150000, Train Loss: 1611, Val Loss: 1791,  Learning Rate: 0.00044, Train Gradient: 17.4\n",
      "Epoch 97901/150000, Train Loss: 1608, Val Loss: 1788,  Learning Rate: 0.00044, Train Gradient: 17.4\n",
      "Epoch 98001/150000, Train Loss: 1605, Val Loss: 1785,  Learning Rate: 0.00044, Train Gradient: 17.4\n",
      "Epoch 98101/150000, Train Loss: 1602, Val Loss: 1782,  Learning Rate: 0.00044, Train Gradient: 17.3\n",
      "Epoch 98201/150000, Train Loss: 1599, Val Loss: 1779,  Learning Rate: 0.00044, Train Gradient: 17.3\n",
      "Epoch 98301/150000, Train Loss: 1596, Val Loss: 1776,  Learning Rate: 0.00044, Train Gradient: 17.3\n",
      "Epoch 98401/150000, Train Loss: 1593, Val Loss: 1773,  Learning Rate: 0.00044, Train Gradient: 17.3\n",
      "Epoch 98501/150000, Train Loss: 1590, Val Loss: 1770,  Learning Rate: 0.00044, Train Gradient: 17.2\n",
      "Epoch 98601/150000, Train Loss: 1587, Val Loss: 1767,  Learning Rate: 0.00044, Train Gradient: 17.2\n",
      "Epoch 98701/150000, Train Loss: 1584, Val Loss: 1764,  Learning Rate: 0.00044, Train Gradient: 17.2\n",
      "Epoch 98801/150000, Train Loss: 1581, Val Loss: 1761,  Learning Rate: 0.00044, Train Gradient: 17.2\n",
      "Epoch 98901/150000, Train Loss: 1578, Val Loss: 1758,  Learning Rate: 0.00044, Train Gradient: 17.1\n",
      "Epoch 99001/150000, Train Loss: 1575, Val Loss: 1755,  Learning Rate: 0.00044, Train Gradient: 17.1\n",
      "Epoch 99101/150000, Train Loss: 1572, Val Loss: 1753,  Learning Rate: 0.00044, Train Gradient: 17.1\n",
      "Epoch 99201/150000, Train Loss: 1569, Val Loss: 1750,  Learning Rate: 0.00044, Train Gradient: 17.1\n",
      "Epoch 99301/150000, Train Loss: 1567, Val Loss: 1747,  Learning Rate: 0.00044, Train Gradient: 17.1\n",
      "Epoch 99401/150000, Train Loss: 1564, Val Loss: 1744,  Learning Rate: 0.00044, Train Gradient: 17.0\n",
      "Epoch 99501/150000, Train Loss: 1561, Val Loss: 1741,  Learning Rate: 0.00044, Train Gradient: 17.0\n",
      "Epoch 99601/150000, Train Loss: 1558, Val Loss: 1738,  Learning Rate: 0.00044, Train Gradient: 17.0\n",
      "Epoch 99701/150000, Train Loss: 1555, Val Loss: 1735,  Learning Rate: 0.00044, Train Gradient: 17.0\n",
      "Epoch 99801/150000, Train Loss: 1552, Val Loss: 1732,  Learning Rate: 0.00044, Train Gradient: 16.9\n",
      "Epoch 99901/150000, Train Loss: 1549, Val Loss: 1729,  Learning Rate: 0.00043, Train Gradient: 16.9\n",
      "Epoch 100001/150000, Train Loss: 1546, Val Loss: 1727,  Learning Rate: 0.00043, Train Gradient: 16.9\n",
      "Epoch 100101/150000, Train Loss: 1543, Val Loss: 1724,  Learning Rate: 0.00043, Train Gradient: 16.9\n",
      "Epoch 100201/150000, Train Loss: 1540, Val Loss: 1721,  Learning Rate: 0.00043, Train Gradient: 16.8\n",
      "Epoch 100301/150000, Train Loss: 1538, Val Loss: 1718,  Learning Rate: 0.00043, Train Gradient: 16.8\n",
      "Epoch 100401/150000, Train Loss: 1535, Val Loss: 1715,  Learning Rate: 0.00043, Train Gradient: 16.8\n",
      "Epoch 100501/150000, Train Loss: 1532, Val Loss: 1712,  Learning Rate: 0.00043, Train Gradient: 16.8\n",
      "Epoch 100601/150000, Train Loss: 1529, Val Loss: 1710,  Learning Rate: 0.00043, Train Gradient: 16.7\n",
      "Epoch 100701/150000, Train Loss: 1526, Val Loss: 1707,  Learning Rate: 0.00043, Train Gradient: 16.7\n",
      "Epoch 100801/150000, Train Loss: 1523, Val Loss: 1704,  Learning Rate: 0.00043, Train Gradient: 16.7\n",
      "Epoch 100901/150000, Train Loss: 1520, Val Loss: 1701,  Learning Rate: 0.00043, Train Gradient: 16.7\n",
      "Epoch 101001/150000, Train Loss: 1518, Val Loss: 1698,  Learning Rate: 0.00043, Train Gradient: 16.7\n",
      "Epoch 101101/150000, Train Loss: 1515, Val Loss: 1695,  Learning Rate: 0.00043, Train Gradient: 16.6\n",
      "Epoch 101201/150000, Train Loss: 1512, Val Loss: 1693,  Learning Rate: 0.00043, Train Gradient: 16.6\n",
      "Epoch 101301/150000, Train Loss: 1509, Val Loss: 1690,  Learning Rate: 0.00043, Train Gradient: 16.6\n",
      "Epoch 101401/150000, Train Loss: 1506, Val Loss: 1687,  Learning Rate: 0.00043, Train Gradient: 16.6\n",
      "Epoch 101501/150000, Train Loss: 1503, Val Loss: 1684,  Learning Rate: 0.00043, Train Gradient: 16.5\n",
      "Epoch 101601/150000, Train Loss: 1501, Val Loss: 1681,  Learning Rate: 0.00043, Train Gradient: 16.5\n",
      "Epoch 101701/150000, Train Loss: 1498, Val Loss: 1679,  Learning Rate: 0.00043, Train Gradient: 16.5\n",
      "Epoch 101801/150000, Train Loss: 1495, Val Loss: 1676,  Learning Rate: 0.00043, Train Gradient: 16.5\n",
      "Epoch 101901/150000, Train Loss: 1492, Val Loss: 1673,  Learning Rate: 0.00043, Train Gradient: 16.4\n",
      "Epoch 102001/150000, Train Loss: 1489, Val Loss: 1670,  Learning Rate: 0.00043, Train Gradient: 16.4\n",
      "Epoch 102101/150000, Train Loss: 1486, Val Loss: 1668,  Learning Rate: 0.00043, Train Gradient: 16.4\n",
      "Epoch 102201/150000, Train Loss: 1484, Val Loss: 1665,  Learning Rate: 0.00043, Train Gradient: 16.4\n",
      "Epoch 102301/150000, Train Loss: 1481, Val Loss: 1662,  Learning Rate: 0.00043, Train Gradient: 16.4\n",
      "Epoch 102401/150000, Train Loss: 1478, Val Loss: 1659,  Learning Rate: 0.00043, Train Gradient: 16.3\n",
      "Epoch 102501/150000, Train Loss: 1475, Val Loss: 1657,  Learning Rate: 0.00043, Train Gradient: 16.3\n",
      "Epoch 102601/150000, Train Loss: 1472, Val Loss: 1654,  Learning Rate: 0.00043, Train Gradient: 16.3\n",
      "Epoch 102701/150000, Train Loss: 1470, Val Loss: 1651,  Learning Rate: 0.00043, Train Gradient: 16.3\n",
      "Epoch 102801/150000, Train Loss: 1467, Val Loss: 1648,  Learning Rate: 0.00043, Train Gradient: 16.2\n",
      "Epoch 102901/150000, Train Loss: 1464, Val Loss: 1646,  Learning Rate: 0.00043, Train Gradient: 16.2\n",
      "Epoch 103001/150000, Train Loss: 1461, Val Loss: 1643,  Learning Rate: 0.00043, Train Gradient: 16.2\n",
      "Epoch 103101/150000, Train Loss: 1459, Val Loss: 1640,  Learning Rate: 0.00043, Train Gradient: 16.2\n",
      "Epoch 103201/150000, Train Loss: 1456, Val Loss: 1638,  Learning Rate: 0.00043, Train Gradient: 16.2\n",
      "Epoch 103301/150000, Train Loss: 1453, Val Loss: 1635,  Learning Rate: 0.00043, Train Gradient: 16.1\n",
      "Epoch 103401/150000, Train Loss: 1450, Val Loss: 1632,  Learning Rate: 0.00043, Train Gradient: 16.1\n",
      "Epoch 103501/150000, Train Loss: 1448, Val Loss: 1629,  Learning Rate: 0.00043, Train Gradient: 16.1\n",
      "Epoch 103601/150000, Train Loss: 1445, Val Loss: 1627,  Learning Rate: 0.00043, Train Gradient: 16.1\n",
      "Epoch 103701/150000, Train Loss: 1442, Val Loss: 1624,  Learning Rate: 0.00043, Train Gradient: 16.0\n",
      "Epoch 103801/150000, Train Loss: 1439, Val Loss: 1621,  Learning Rate: 0.00043, Train Gradient: 16.0\n",
      "Epoch 103901/150000, Train Loss: 1437, Val Loss: 1619,  Learning Rate: 0.00043, Train Gradient: 16.0\n",
      "Epoch 104001/150000, Train Loss: 1434, Val Loss: 1616,  Learning Rate: 0.00043, Train Gradient: 16.0\n",
      "Epoch 104101/150000, Train Loss: 1431, Val Loss: 1613,  Learning Rate: 0.00043, Train Gradient: 16.0\n",
      "Epoch 104201/150000, Train Loss: 1428, Val Loss: 1611,  Learning Rate: 0.00043, Train Gradient: 15.9\n",
      "Epoch 104301/150000, Train Loss: 1426, Val Loss: 1608,  Learning Rate: 0.00043, Train Gradient: 15.9\n",
      "Epoch 104401/150000, Train Loss: 1423, Val Loss: 1605,  Learning Rate: 0.00043, Train Gradient: 15.9\n",
      "Epoch 104501/150000, Train Loss: 1420, Val Loss: 1603,  Learning Rate: 0.00043, Train Gradient: 15.9\n",
      "Epoch 104601/150000, Train Loss: 1417, Val Loss: 1600,  Learning Rate: 0.00043, Train Gradient: 15.8\n",
      "Epoch 104701/150000, Train Loss: 1415, Val Loss: 1597,  Learning Rate: 0.00043, Train Gradient: 15.8\n",
      "Epoch 104801/150000, Train Loss: 1412, Val Loss: 1595,  Learning Rate: 0.00043, Train Gradient: 15.8\n",
      "Epoch 104901/150000, Train Loss: 1409, Val Loss: 1592,  Learning Rate: 0.00043, Train Gradient: 15.8\n",
      "Epoch 105001/150000, Train Loss: 1407, Val Loss: 1590,  Learning Rate: 0.00043, Train Gradient: 15.8\n",
      "Epoch 105101/150000, Train Loss: 1404, Val Loss: 1587,  Learning Rate: 0.00043, Train Gradient: 15.7\n",
      "Epoch 105201/150000, Train Loss: 1401, Val Loss: 1584,  Learning Rate: 0.00043, Train Gradient: 15.7\n",
      "Epoch 105301/150000, Train Loss: 1399, Val Loss: 1582,  Learning Rate: 0.00043, Train Gradient: 15.7\n",
      "Epoch 105401/150000, Train Loss: 1396, Val Loss: 1579,  Learning Rate: 0.00043, Train Gradient: 15.7\n",
      "Epoch 105501/150000, Train Loss: 1393, Val Loss: 1576,  Learning Rate: 0.00043, Train Gradient: 15.7\n",
      "Epoch 105601/150000, Train Loss: 1391, Val Loss: 1574,  Learning Rate: 0.00043, Train Gradient: 15.6\n",
      "Epoch 105701/150000, Train Loss: 1388, Val Loss: 1571,  Learning Rate: 0.00043, Train Gradient: 15.6\n",
      "Epoch 105801/150000, Train Loss: 1385, Val Loss: 1569,  Learning Rate: 0.00043, Train Gradient: 15.6\n",
      "Epoch 105901/150000, Train Loss: 1383, Val Loss: 1566,  Learning Rate: 0.00043, Train Gradient: 15.6\n",
      "Epoch 106001/150000, Train Loss: 1380, Val Loss: 1564,  Learning Rate: 0.00043, Train Gradient: 15.6\n",
      "Epoch 106101/150000, Train Loss: 1378, Val Loss: 1561,  Learning Rate: 0.00043, Train Gradient: 15.5\n",
      "Epoch 106201/150000, Train Loss: 1375, Val Loss: 1559,  Learning Rate: 0.00043, Train Gradient: 15.5\n",
      "Epoch 106301/150000, Train Loss: 1372, Val Loss: 1556,  Learning Rate: 0.00043, Train Gradient: 15.5\n",
      "Epoch 106401/150000, Train Loss: 1370, Val Loss: 1554,  Learning Rate: 0.00043, Train Gradient: 15.5\n",
      "Epoch 106501/150000, Train Loss: 1367, Val Loss: 1551,  Learning Rate: 0.00043, Train Gradient: 15.5\n",
      "Epoch 106601/150000, Train Loss: 1365, Val Loss: 1549,  Learning Rate: 0.00043, Train Gradient: 15.4\n",
      "Epoch 106701/150000, Train Loss: 1362, Val Loss: 1546,  Learning Rate: 0.00043, Train Gradient: 15.4\n",
      "Epoch 106801/150000, Train Loss: 1359, Val Loss: 1544,  Learning Rate: 0.00043, Train Gradient: 15.4\n",
      "Epoch 106901/150000, Train Loss: 1357, Val Loss: 1541,  Learning Rate: 0.00043, Train Gradient: 15.4\n",
      "Epoch 107001/150000, Train Loss: 1354, Val Loss: 1539,  Learning Rate: 0.00043, Train Gradient: 15.4\n",
      "Epoch 107101/150000, Train Loss: 1352, Val Loss: 1536,  Learning Rate: 0.00043, Train Gradient: 15.3\n",
      "Epoch 107201/150000, Train Loss: 1349, Val Loss: 1534,  Learning Rate: 0.00043, Train Gradient: 15.3\n",
      "Epoch 107301/150000, Train Loss: 1346, Val Loss: 1531,  Learning Rate: 0.00043, Train Gradient: 15.3\n",
      "Epoch 107401/150000, Train Loss: 1344, Val Loss: 1529,  Learning Rate: 0.00043, Train Gradient: 15.3\n",
      "Epoch 107501/150000, Train Loss: 1341, Val Loss: 1527,  Learning Rate: 0.00043, Train Gradient: 15.3\n",
      "Epoch 107601/150000, Train Loss: 1339, Val Loss: 1524,  Learning Rate: 0.00043, Train Gradient: 15.2\n",
      "Epoch 107701/150000, Train Loss: 1336, Val Loss: 1522,  Learning Rate: 0.00043, Train Gradient: 15.2\n",
      "Epoch 107801/150000, Train Loss: 1334, Val Loss: 1519,  Learning Rate: 0.00043, Train Gradient: 15.2\n",
      "Epoch 107901/150000, Train Loss: 1331, Val Loss: 1517,  Learning Rate: 0.00043, Train Gradient: 15.2\n",
      "Epoch 108001/150000, Train Loss: 1329, Val Loss: 1514,  Learning Rate: 0.00043, Train Gradient: 15.2\n",
      "Epoch 108101/150000, Train Loss: 1326, Val Loss: 1512,  Learning Rate: 0.00043, Train Gradient: 15.1\n",
      "Epoch 108201/150000, Train Loss: 1323, Val Loss: 1510,  Learning Rate: 0.00043, Train Gradient: 15.1\n",
      "Epoch 108301/150000, Train Loss: 1321, Val Loss: 1507,  Learning Rate: 0.00043, Train Gradient: 15.1\n",
      "Epoch 108401/150000, Train Loss: 1318, Val Loss: 1505,  Learning Rate: 0.00043, Train Gradient: 15.1\n",
      "Epoch 108501/150000, Train Loss: 1316, Val Loss: 1502,  Learning Rate: 0.00043, Train Gradient: 15.1\n",
      "Epoch 108601/150000, Train Loss: 1313, Val Loss: 1500,  Learning Rate: 0.00043, Train Gradient: 15.0\n",
      "Epoch 108701/150000, Train Loss: 1311, Val Loss: 1497,  Learning Rate: 0.00043, Train Gradient: 15.0\n",
      "Epoch 108801/150000, Train Loss: 1308, Val Loss: 1495,  Learning Rate: 0.00043, Train Gradient: 15.0\n",
      "Epoch 108901/150000, Train Loss: 1306, Val Loss: 1493,  Learning Rate: 0.00043, Train Gradient: 15.0\n",
      "Epoch 109001/150000, Train Loss: 1303, Val Loss: 1490,  Learning Rate: 0.00043, Train Gradient: 15.0\n",
      "Epoch 109101/150000, Train Loss: 1301, Val Loss: 1488,  Learning Rate: 0.00043, Train Gradient: 14.9\n",
      "Epoch 109201/150000, Train Loss: 1298, Val Loss: 1485,  Learning Rate: 0.00043, Train Gradient: 14.9\n",
      "Epoch 109301/150000, Train Loss: 1296, Val Loss: 1483,  Learning Rate: 0.00043, Train Gradient: 14.9\n",
      "Epoch 109401/150000, Train Loss: 1293, Val Loss: 1481,  Learning Rate: 0.00043, Train Gradient: 14.9\n",
      "Epoch 109501/150000, Train Loss: 1291, Val Loss: 1478,  Learning Rate: 0.00043, Train Gradient: 14.9\n",
      "Epoch 109601/150000, Train Loss: 1288, Val Loss: 1476,  Learning Rate: 0.00043, Train Gradient: 14.8\n",
      "Epoch 109701/150000, Train Loss: 1286, Val Loss: 1473,  Learning Rate: 0.00043, Train Gradient: 14.8\n",
      "Epoch 109801/150000, Train Loss: 1283, Val Loss: 1471,  Learning Rate: 0.00043, Train Gradient: 14.8\n",
      "Epoch 109901/150000, Train Loss: 1281, Val Loss: 1469,  Learning Rate: 0.00043, Train Gradient: 14.8\n",
      "Epoch 110001/150000, Train Loss: 1278, Val Loss: 1466,  Learning Rate: 0.00043, Train Gradient: 14.7\n",
      "Epoch 110101/150000, Train Loss: 1276, Val Loss: 1464,  Learning Rate: 0.00043, Train Gradient: 14.7\n",
      "Epoch 110201/150000, Train Loss: 1273, Val Loss: 1462,  Learning Rate: 0.00043, Train Gradient: 14.7\n",
      "Epoch 110301/150000, Train Loss: 1271, Val Loss: 1459,  Learning Rate: 0.00042, Train Gradient: 14.7\n",
      "Epoch 110401/150000, Train Loss: 1268, Val Loss: 1457,  Learning Rate: 0.00042, Train Gradient: 14.7\n",
      "Epoch 110501/150000, Train Loss: 1266, Val Loss: 1455,  Learning Rate: 0.00042, Train Gradient: 14.6\n",
      "Epoch 110601/150000, Train Loss: 1263, Val Loss: 1452,  Learning Rate: 0.00042, Train Gradient: 14.6\n",
      "Epoch 110701/150000, Train Loss: 1261, Val Loss: 1450,  Learning Rate: 0.00042, Train Gradient: 14.6\n",
      "Epoch 110801/150000, Train Loss: 1259, Val Loss: 1447,  Learning Rate: 0.00042, Train Gradient: 14.6\n",
      "Epoch 110901/150000, Train Loss: 1256, Val Loss: 1445,  Learning Rate: 0.00042, Train Gradient: 14.6\n",
      "Epoch 111001/150000, Train Loss: 1254, Val Loss: 1443,  Learning Rate: 0.00042, Train Gradient: 14.5\n",
      "Epoch 111101/150000, Train Loss: 1251, Val Loss: 1440,  Learning Rate: 0.00042, Train Gradient: 14.5\n",
      "Epoch 111201/150000, Train Loss: 1249, Val Loss: 1438,  Learning Rate: 0.00042, Train Gradient: 14.5\n",
      "Epoch 111301/150000, Train Loss: 1246, Val Loss: 1436,  Learning Rate: 0.00042, Train Gradient: 14.5\n",
      "Epoch 111401/150000, Train Loss: 1244, Val Loss: 1433,  Learning Rate: 0.00042, Train Gradient: 14.5\n",
      "Epoch 111501/150000, Train Loss: 1241, Val Loss: 1431,  Learning Rate: 0.00042, Train Gradient: 14.4\n",
      "Epoch 111601/150000, Train Loss: 1239, Val Loss: 1429,  Learning Rate: 0.00042, Train Gradient: 14.4\n",
      "Epoch 111701/150000, Train Loss: 1237, Val Loss: 1427,  Learning Rate: 0.00042, Train Gradient: 14.4\n",
      "Epoch 111801/150000, Train Loss: 1234, Val Loss: 1424,  Learning Rate: 0.00042, Train Gradient: 14.4\n",
      "Epoch 111901/150000, Train Loss: 1232, Val Loss: 1422,  Learning Rate: 0.00042, Train Gradient: 14.4\n",
      "Epoch 112001/150000, Train Loss: 1229, Val Loss: 1420,  Learning Rate: 0.00042, Train Gradient: 14.3\n",
      "Epoch 112101/150000, Train Loss: 1227, Val Loss: 1417,  Learning Rate: 0.00042, Train Gradient: 14.3\n",
      "Epoch 112201/150000, Train Loss: 1225, Val Loss: 1415,  Learning Rate: 0.00042, Train Gradient: 14.3\n",
      "Epoch 112301/150000, Train Loss: 1222, Val Loss: 1413,  Learning Rate: 0.00042, Train Gradient: 14.3\n",
      "Epoch 112401/150000, Train Loss: 1220, Val Loss: 1410,  Learning Rate: 0.00042, Train Gradient: 14.3\n",
      "Epoch 112501/150000, Train Loss: 1217, Val Loss: 1408,  Learning Rate: 0.00042, Train Gradient: 14.2\n",
      "Epoch 112601/150000, Train Loss: 1215, Val Loss: 1406,  Learning Rate: 0.00042, Train Gradient: 14.2\n",
      "Epoch 112701/150000, Train Loss: 1213, Val Loss: 1403,  Learning Rate: 0.00042, Train Gradient: 14.2\n",
      "Epoch 112801/150000, Train Loss: 1210, Val Loss: 1401,  Learning Rate: 0.00042, Train Gradient: 14.2\n",
      "Epoch 112901/150000, Train Loss: 1208, Val Loss: 1399,  Learning Rate: 0.00042, Train Gradient: 14.2\n",
      "Epoch 113001/150000, Train Loss: 1205, Val Loss: 1396,  Learning Rate: 0.00042, Train Gradient: 14.1\n",
      "Epoch 113101/150000, Train Loss: 1203, Val Loss: 1394,  Learning Rate: 0.00042, Train Gradient: 14.1\n",
      "Epoch 113201/150000, Train Loss: 1201, Val Loss: 1392,  Learning Rate: 0.00042, Train Gradient: 14.1\n",
      "Epoch 113301/150000, Train Loss: 1198, Val Loss: 1390,  Learning Rate: 0.00042, Train Gradient: 14.1\n",
      "Epoch 113401/150000, Train Loss: 1196, Val Loss: 1387,  Learning Rate: 0.00042, Train Gradient: 14.1\n",
      "Epoch 113501/150000, Train Loss: 1194, Val Loss: 1385,  Learning Rate: 0.00042, Train Gradient: 14.0\n",
      "Epoch 113601/150000, Train Loss: 1191, Val Loss: 1383,  Learning Rate: 0.00042, Train Gradient: 14.0\n",
      "Epoch 113701/150000, Train Loss: 1189, Val Loss: 1381,  Learning Rate: 0.00042, Train Gradient: 14.0\n",
      "Epoch 113801/150000, Train Loss: 1187, Val Loss: 1378,  Learning Rate: 0.00042, Train Gradient: 14.0\n",
      "Epoch 113901/150000, Train Loss: 1184, Val Loss: 1376,  Learning Rate: 0.00042, Train Gradient: 14.0\n",
      "Epoch 114001/150000, Train Loss: 1182, Val Loss: 1374,  Learning Rate: 0.00042, Train Gradient: 13.9\n",
      "Epoch 114101/150000, Train Loss: 1180, Val Loss: 1371,  Learning Rate: 0.00042, Train Gradient: 13.9\n",
      "Epoch 114201/150000, Train Loss: 1177, Val Loss: 1369,  Learning Rate: 0.00042, Train Gradient: 13.9\n",
      "Epoch 114301/150000, Train Loss: 1175, Val Loss: 1367,  Learning Rate: 0.00042, Train Gradient: 13.9\n",
      "Epoch 114401/150000, Train Loss: 1173, Val Loss: 1365,  Learning Rate: 0.00042, Train Gradient: 13.9\n",
      "Epoch 114501/150000, Train Loss: 1170, Val Loss: 1362,  Learning Rate: 0.00042, Train Gradient: 13.8\n",
      "Epoch 114601/150000, Train Loss: 1168, Val Loss: 1360,  Learning Rate: 0.00042, Train Gradient: 13.8\n",
      "Epoch 114701/150000, Train Loss: 1166, Val Loss: 1358,  Learning Rate: 0.00042, Train Gradient: 13.8\n",
      "Epoch 114801/150000, Train Loss: 1163, Val Loss: 1356,  Learning Rate: 0.00042, Train Gradient: 13.8\n",
      "Epoch 114901/150000, Train Loss: 1161, Val Loss: 1354,  Learning Rate: 0.00042, Train Gradient: 13.8\n",
      "Epoch 115001/150000, Train Loss: 1159, Val Loss: 1351,  Learning Rate: 0.00042, Train Gradient: 13.7\n",
      "Epoch 115101/150000, Train Loss: 1157, Val Loss: 1349,  Learning Rate: 0.00042, Train Gradient: 13.7\n",
      "Epoch 115201/150000, Train Loss: 1154, Val Loss: 1347,  Learning Rate: 0.00042, Train Gradient: 13.7\n",
      "Epoch 115301/150000, Train Loss: 1152, Val Loss: 1345,  Learning Rate: 0.00042, Train Gradient: 13.7\n",
      "Epoch 115401/150000, Train Loss: 1150, Val Loss: 1343,  Learning Rate: 0.00042, Train Gradient: 13.7\n",
      "Epoch 115501/150000, Train Loss: 1148, Val Loss: 1341,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 115601/150000, Train Loss: 1145, Val Loss: 1338,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 115701/150000, Train Loss: 1143, Val Loss: 1336,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 115801/150000, Train Loss: 1141, Val Loss: 1334,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 115901/150000, Train Loss: 1139, Val Loss: 1332,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 116001/150000, Train Loss: 1136, Val Loss: 1330,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 116101/150000, Train Loss: 1134, Val Loss: 1328,  Learning Rate: 0.00042, Train Gradient: 13.5\n",
      "Epoch 116201/150000, Train Loss: 1132, Val Loss: 1325,  Learning Rate: 0.00042, Train Gradient: 13.5\n",
      "Epoch 116301/150000, Train Loss: 1130, Val Loss: 1323,  Learning Rate: 0.00042, Train Gradient: 13.5\n",
      "Epoch 116401/150000, Train Loss: 1127, Val Loss: 1321,  Learning Rate: 0.00042, Train Gradient: 13.5\n",
      "Epoch 116501/150000, Train Loss: 1125, Val Loss: 1319,  Learning Rate: 0.00042, Train Gradient: 13.5\n",
      "Epoch 116601/150000, Train Loss: 1123, Val Loss: 1317,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 116701/150000, Train Loss: 1121, Val Loss: 1315,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 116801/150000, Train Loss: 1119, Val Loss: 1312,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 116901/150000, Train Loss: 1116, Val Loss: 1310,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 117001/150000, Train Loss: 1114, Val Loss: 1308,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 117101/150000, Train Loss: 1112, Val Loss: 1306,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 117201/150000, Train Loss: 1110, Val Loss: 1304,  Learning Rate: 0.00042, Train Gradient: 13.3\n",
      "Epoch 117301/150000, Train Loss: 1107, Val Loss: 1302,  Learning Rate: 0.00042, Train Gradient: 13.3\n",
      "Epoch 117401/150000, Train Loss: 1105, Val Loss: 1300,  Learning Rate: 0.00042, Train Gradient: 13.3\n",
      "Epoch 117501/150000, Train Loss: 1103, Val Loss: 1298,  Learning Rate: 0.00042, Train Gradient: 13.3\n",
      "Epoch 117601/150000, Train Loss: 1101, Val Loss: 1296,  Learning Rate: 0.00042, Train Gradient: 13.3\n",
      "Epoch 117701/150000, Train Loss: 1099, Val Loss: 1293,  Learning Rate: 0.00042, Train Gradient: 13.2\n",
      "Epoch 117801/150000, Train Loss: 1097, Val Loss: 1291,  Learning Rate: 0.00042, Train Gradient: 13.2\n",
      "Epoch 117901/150000, Train Loss: 1094, Val Loss: 1289,  Learning Rate: 0.00042, Train Gradient: 13.2\n",
      "Epoch 118001/150000, Train Loss: 1092, Val Loss: 1287,  Learning Rate: 0.00042, Train Gradient: 13.2\n",
      "Epoch 118101/150000, Train Loss: 1090, Val Loss: 1285,  Learning Rate: 0.00042, Train Gradient: 13.2\n",
      "Epoch 118201/150000, Train Loss: 1088, Val Loss: 1283,  Learning Rate: 0.00042, Train Gradient: 13.1\n",
      "Epoch 118301/150000, Train Loss: 1086, Val Loss: 1281,  Learning Rate: 0.00042, Train Gradient: 13.1\n",
      "Epoch 118401/150000, Train Loss: 1084, Val Loss: 1279,  Learning Rate: 0.00042, Train Gradient: 13.1\n",
      "Epoch 118501/150000, Train Loss: 1081, Val Loss: 1277,  Learning Rate: 0.00042, Train Gradient: 13.1\n",
      "Epoch 118601/150000, Train Loss: 1079, Val Loss: 1274,  Learning Rate: 0.00042, Train Gradient: 13.1\n",
      "Epoch 118701/150000, Train Loss: 1077, Val Loss: 1272,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 118801/150000, Train Loss: 1075, Val Loss: 1270,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 118901/150000, Train Loss: 1073, Val Loss: 1268,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 119001/150000, Train Loss: 1071, Val Loss: 1266,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 119101/150000, Train Loss: 1068, Val Loss: 1264,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 119201/150000, Train Loss: 1066, Val Loss: 1262,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 119301/150000, Train Loss: 1064, Val Loss: 1260,  Learning Rate: 0.00042, Train Gradient: 12.9\n",
      "Epoch 119401/150000, Train Loss: 1062, Val Loss: 1258,  Learning Rate: 0.00042, Train Gradient: 12.9\n",
      "Epoch 119501/150000, Train Loss: 1060, Val Loss: 1256,  Learning Rate: 0.00042, Train Gradient: 12.9\n",
      "Epoch 119601/150000, Train Loss: 1058, Val Loss: 1254,  Learning Rate: 0.00042, Train Gradient: 12.9\n",
      "Epoch 119701/150000, Train Loss: 1056, Val Loss: 1252,  Learning Rate: 0.00042, Train Gradient: 12.9\n",
      "Epoch 119801/150000, Train Loss: 1054, Val Loss: 1250,  Learning Rate: 0.00042, Train Gradient: 12.8\n",
      "Epoch 119901/150000, Train Loss: 1051, Val Loss: 1248,  Learning Rate: 0.00042, Train Gradient: 12.8\n",
      "Epoch 120001/150000, Train Loss: 1049, Val Loss: 1245,  Learning Rate: 0.00042, Train Gradient: 12.8\n",
      "Epoch 120101/150000, Train Loss: 1047, Val Loss: 1243,  Learning Rate: 0.00042, Train Gradient: 12.8\n",
      "Epoch 120201/150000, Train Loss: 1045, Val Loss: 1241,  Learning Rate: 0.00042, Train Gradient: 12.8\n",
      "Epoch 120301/150000, Train Loss: 1043, Val Loss: 1239,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120401/150000, Train Loss: 1041, Val Loss: 1237,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120501/150000, Train Loss: 1039, Val Loss: 1235,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120601/150000, Train Loss: 1037, Val Loss: 1233,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120701/150000, Train Loss: 1035, Val Loss: 1231,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120801/150000, Train Loss: 1033, Val Loss: 1229,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120901/150000, Train Loss: 1030, Val Loss: 1227,  Learning Rate: 0.00042, Train Gradient: 12.6\n",
      "Epoch 121001/150000, Train Loss: 1028, Val Loss: 1225,  Learning Rate: 0.00042, Train Gradient: 12.6\n",
      "Epoch 121101/150000, Train Loss: 1026, Val Loss: 1223,  Learning Rate: 0.00042, Train Gradient: 12.6\n",
      "Epoch 121201/150000, Train Loss: 1024, Val Loss: 1221,  Learning Rate: 0.00042, Train Gradient: 12.6\n",
      "Epoch 121301/150000, Train Loss: 1022, Val Loss: 1219,  Learning Rate: 0.00042, Train Gradient: 12.6\n",
      "Epoch 121401/150000, Train Loss: 1020, Val Loss: 1217,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 121501/150000, Train Loss: 1018, Val Loss: 1215,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 121601/150000, Train Loss: 1016, Val Loss: 1213,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 121701/150000, Train Loss: 1014, Val Loss: 1212,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 121801/150000, Train Loss: 1012, Val Loss: 1210,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 121901/150000, Train Loss: 1010, Val Loss: 1208,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 122001/150000, Train Loss: 1008, Val Loss: 1206,  Learning Rate: 0.00042, Train Gradient: 12.4\n",
      "Epoch 122101/150000, Train Loss: 1006, Val Loss: 1204,  Learning Rate: 0.00042, Train Gradient: 12.4\n",
      "Epoch 122201/150000, Train Loss: 1004, Val Loss: 1202,  Learning Rate: 0.00042, Train Gradient: 12.4\n",
      "Epoch 122301/150000, Train Loss: 1002, Val Loss: 1200,  Learning Rate: 0.00042, Train Gradient: 12.4\n",
      "Epoch 122401/150000, Train Loss: 999, Val Loss: 1198,  Learning Rate: 0.00042, Train Gradient: 12.4\n",
      "Epoch 122501/150000, Train Loss: 997, Val Loss: 1196,  Learning Rate: 0.00042, Train Gradient: 12.3\n",
      "Epoch 122601/150000, Train Loss: 995, Val Loss: 1194,  Learning Rate: 0.00042, Train Gradient: 12.3\n",
      "Epoch 122701/150000, Train Loss: 993, Val Loss: 1192,  Learning Rate: 0.00042, Train Gradient: 12.3\n",
      "Epoch 122801/150000, Train Loss: 991, Val Loss: 1190,  Learning Rate: 0.00042, Train Gradient: 12.3\n",
      "Epoch 122901/150000, Train Loss: 989, Val Loss: 1188,  Learning Rate: 0.00042, Train Gradient: 12.3\n",
      "Epoch 123001/150000, Train Loss: 987, Val Loss: 1186,  Learning Rate: 0.00042, Train Gradient: 12.2\n",
      "Epoch 123101/150000, Train Loss: 985, Val Loss: 1185,  Learning Rate: 0.00042, Train Gradient: 12.2\n",
      "Epoch 123201/150000, Train Loss: 983, Val Loss: 1183,  Learning Rate: 0.00042, Train Gradient: 12.2\n",
      "Epoch 123301/150000, Train Loss: 981, Val Loss: 1181,  Learning Rate: 0.00042, Train Gradient: 12.2\n",
      "Epoch 123401/150000, Train Loss: 979, Val Loss: 1179,  Learning Rate: 0.00042, Train Gradient: 12.2\n",
      "Epoch 123501/150000, Train Loss: 977, Val Loss: 1177,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 123601/150000, Train Loss: 975, Val Loss: 1175,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 123701/150000, Train Loss: 973, Val Loss: 1173,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 123801/150000, Train Loss: 971, Val Loss: 1171,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 123901/150000, Train Loss: 969, Val Loss: 1169,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 124001/150000, Train Loss: 967, Val Loss: 1168,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 124101/150000, Train Loss: 965, Val Loss: 1166,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124201/150000, Train Loss: 963, Val Loss: 1164,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124301/150000, Train Loss: 961, Val Loss: 1162,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124401/150000, Train Loss: 959, Val Loss: 1160,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124501/150000, Train Loss: 957, Val Loss: 1158,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124601/150000, Train Loss: 955, Val Loss: 1156,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124701/150000, Train Loss: 953, Val Loss: 1155,  Learning Rate: 0.00042, Train Gradient: 11.9\n",
      "Epoch 124801/150000, Train Loss: 951, Val Loss: 1153,  Learning Rate: 0.00042, Train Gradient: 11.9\n",
      "Epoch 124901/150000, Train Loss: 949, Val Loss: 1151,  Learning Rate: 0.00042, Train Gradient: 11.9\n",
      "Epoch 125001/150000, Train Loss: 948, Val Loss: 1149,  Learning Rate: 0.00042, Train Gradient: 11.9\n",
      "Epoch 125101/150000, Train Loss: 946, Val Loss: 1147,  Learning Rate: 0.00042, Train Gradient: 11.9\n",
      "Epoch 125201/150000, Train Loss: 944, Val Loss: 1145,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125301/150000, Train Loss: 942, Val Loss: 1143,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125401/150000, Train Loss: 940, Val Loss: 1142,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125501/150000, Train Loss: 938, Val Loss: 1140,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125601/150000, Train Loss: 936, Val Loss: 1138,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125701/150000, Train Loss: 934, Val Loss: 1136,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125801/150000, Train Loss: 932, Val Loss: 1134,  Learning Rate: 0.00042, Train Gradient: 11.7\n",
      "Epoch 125901/150000, Train Loss: 930, Val Loss: 1133,  Learning Rate: 0.00042, Train Gradient: 11.7\n",
      "Epoch 126001/150000, Train Loss: 928, Val Loss: 1131,  Learning Rate: 0.00041, Train Gradient: 11.7\n",
      "Epoch 126101/150000, Train Loss: 926, Val Loss: 1129,  Learning Rate: 0.00041, Train Gradient: 11.7\n",
      "Epoch 126201/150000, Train Loss: 924, Val Loss: 1127,  Learning Rate: 0.00041, Train Gradient: 11.7\n",
      "Epoch 126301/150000, Train Loss: 922, Val Loss: 1125,  Learning Rate: 0.00041, Train Gradient: 11.6\n",
      "Epoch 126401/150000, Train Loss: 920, Val Loss: 1124,  Learning Rate: 0.00041, Train Gradient: 11.6\n",
      "Epoch 126501/150000, Train Loss: 918, Val Loss: 1122,  Learning Rate: 0.00041, Train Gradient: 11.6\n",
      "Epoch 126601/150000, Train Loss: 917, Val Loss: 1120,  Learning Rate: 0.00041, Train Gradient: 11.6\n",
      "Epoch 126701/150000, Train Loss: 915, Val Loss: 1118,  Learning Rate: 0.00041, Train Gradient: 11.6\n",
      "Epoch 126801/150000, Train Loss: 913, Val Loss: 1116,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 126901/150000, Train Loss: 911, Val Loss: 1115,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 127001/150000, Train Loss: 909, Val Loss: 1113,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 127101/150000, Train Loss: 907, Val Loss: 1111,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 127201/150000, Train Loss: 905, Val Loss: 1109,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 127301/150000, Train Loss: 903, Val Loss: 1108,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 127401/150000, Train Loss: 902, Val Loss: 1106,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 127501/150000, Train Loss: 900, Val Loss: 1104,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 127601/150000, Train Loss: 898, Val Loss: 1103,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 127701/150000, Train Loss: 896, Val Loss: 1101,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 127801/150000, Train Loss: 894, Val Loss: 1099,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 127901/150000, Train Loss: 892, Val Loss: 1097,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 128001/150000, Train Loss: 891, Val Loss: 1096,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128101/150000, Train Loss: 889, Val Loss: 1094,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128201/150000, Train Loss: 887, Val Loss: 1092,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128301/150000, Train Loss: 885, Val Loss: 1091,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128401/150000, Train Loss: 883, Val Loss: 1089,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128501/150000, Train Loss: 881, Val Loss: 1087,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128601/150000, Train Loss: 880, Val Loss: 1086,  Learning Rate: 0.00041, Train Gradient: 11.2\n",
      "Epoch 128701/150000, Train Loss: 878, Val Loss: 1084,  Learning Rate: 0.00041, Train Gradient: 11.2\n",
      "Epoch 128801/150000, Train Loss: 876, Val Loss: 1082,  Learning Rate: 0.00041, Train Gradient: 11.2\n",
      "Epoch 128901/150000, Train Loss: 874, Val Loss: 1081,  Learning Rate: 0.00041, Train Gradient: 11.2\n",
      "Epoch 129001/150000, Train Loss: 872, Val Loss: 1079,  Learning Rate: 0.00041, Train Gradient: 11.2\n",
      "Epoch 129101/150000, Train Loss: 871, Val Loss: 1077,  Learning Rate: 0.00041, Train Gradient: 11.1\n",
      "Epoch 129201/150000, Train Loss: 869, Val Loss: 1076,  Learning Rate: 0.00041, Train Gradient: 11.1\n",
      "Epoch 129301/150000, Train Loss: 867, Val Loss: 1074,  Learning Rate: 0.00041, Train Gradient: 11.1\n",
      "Epoch 129401/150000, Train Loss: 865, Val Loss: 1072,  Learning Rate: 0.00041, Train Gradient: 11.1\n",
      "Epoch 129501/150000, Train Loss: 863, Val Loss: 1071,  Learning Rate: 0.00041, Train Gradient: 11.1\n",
      "Epoch 129601/150000, Train Loss: 862, Val Loss: 1069,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 129701/150000, Train Loss: 860, Val Loss: 1067,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 129801/150000, Train Loss: 858, Val Loss: 1066,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 129901/150000, Train Loss: 856, Val Loss: 1064,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 130001/150000, Train Loss: 854, Val Loss: 1062,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 130101/150000, Train Loss: 853, Val Loss: 1061,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 130201/150000, Train Loss: 851, Val Loss: 1059,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 130301/150000, Train Loss: 849, Val Loss: 1057,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130401/150000, Train Loss: 847, Val Loss: 1056,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130501/150000, Train Loss: 846, Val Loss: 1054,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130601/150000, Train Loss: 844, Val Loss: 1053,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130701/150000, Train Loss: 842, Val Loss: 1051,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130801/150000, Train Loss: 840, Val Loss: 1049,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130901/150000, Train Loss: 838, Val Loss: 1048,  Learning Rate: 0.00041, Train Gradient: 10.8\n",
      "Epoch 131001/150000, Train Loss: 837, Val Loss: 1046,  Learning Rate: 0.00041, Train Gradient: 10.8\n",
      "Epoch 131101/150000, Train Loss: 835, Val Loss: 1045,  Learning Rate: 0.00041, Train Gradient: 10.8\n",
      "Epoch 131201/150000, Train Loss: 833, Val Loss: 1043,  Learning Rate: 0.00041, Train Gradient: 10.8\n",
      "Epoch 131301/150000, Train Loss: 831, Val Loss: 1041,  Learning Rate: 0.00041, Train Gradient: 10.8\n",
      "Epoch 131401/150000, Train Loss: 830, Val Loss: 1040,  Learning Rate: 0.00041, Train Gradient: 10.7\n",
      "Epoch 131501/150000, Train Loss: 828, Val Loss: 1038,  Learning Rate: 0.00041, Train Gradient: 10.7\n",
      "Epoch 131601/150000, Train Loss: 826, Val Loss: 1037,  Learning Rate: 0.00041, Train Gradient: 10.7\n",
      "Epoch 131701/150000, Train Loss: 825, Val Loss: 1035,  Learning Rate: 0.00041, Train Gradient: 10.7\n",
      "Epoch 131801/150000, Train Loss: 823, Val Loss: 1033,  Learning Rate: 0.00041, Train Gradient: 10.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3]\n",
    "num_layers_list = [1]\n",
    "\n",
    "learning_rates = [0.01]\n",
    "factors = [0.9]     # learning_rates factor (Learning Rate Scheduling)\n",
    "patience_lr = [10]      # learning_rates patience\n",
    "\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, total_steps=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Learning Rate: {current_lr:.5f}, Train Gradient: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.01, window_size=20, dropout_prob=0, weight_decay=0, factor=0.9, patience=10\n",
      "Epoch 1/150000, Loss: 29890, Val Loss: 30388,  Learning Rate: 0.00040, Train Gradient: 223.4\n",
      "Epoch 101/150000, Loss: 29176, Val Loss: 29655,  Learning Rate: 0.00750, Train Gradient: 217.0\n",
      "Epoch 201/150000, Loss: 28391, Val Loss: 28865,  Learning Rate: 0.00727, Train Gradient: 209.6\n",
      "Epoch 301/150000, Loss: 27758, Val Loss: 28225,  Learning Rate: 0.00707, Train Gradient: 203.4\n",
      "Epoch 401/150000, Loss: 27188, Val Loss: 27649,  Learning Rate: 0.00689, Train Gradient: 197.7\n",
      "Epoch 501/150000, Loss: 26663, Val Loss: 27117,  Learning Rate: 0.00672, Train Gradient: 192.4\n",
      "Epoch 601/150000, Loss: 26175, Val Loss: 26623,  Learning Rate: 0.00656, Train Gradient: 187.2\n",
      "Epoch 701/150000, Loss: 25719, Val Loss: 26162,  Learning Rate: 0.00642, Train Gradient: 182.3\n",
      "Epoch 801/150000, Loss: 25292, Val Loss: 25729,  Learning Rate: 0.00627, Train Gradient: 177.5\n",
      "Epoch 901/150000, Loss: 24891, Val Loss: 25322,  Learning Rate: 0.00614, Train Gradient: 172.9\n",
      "Epoch 1001/150000, Loss: 24512, Val Loss: 24938,  Learning Rate: 0.00601, Train Gradient: 168.5\n",
      "Epoch 1101/150000, Loss: 24155, Val Loss: 24576,  Learning Rate: 0.00589, Train Gradient: 164.2\n",
      "Epoch 1201/150000, Loss: 23817, Val Loss: 24233,  Learning Rate: 0.00578, Train Gradient: 160.1\n",
      "Epoch 1301/150000, Loss: 23498, Val Loss: 23909,  Learning Rate: 0.00567, Train Gradient: 156.0\n",
      "Epoch 1401/150000, Loss: 23195, Val Loss: 23601,  Learning Rate: 0.00557, Train Gradient: 152.1\n",
      "Epoch 1501/150000, Loss: 22908, Val Loss: 23309,  Learning Rate: 0.00547, Train Gradient: 148.3\n",
      "Epoch 1601/150000, Loss: 22635, Val Loss: 23032,  Learning Rate: 0.00538, Train Gradient: 144.5\n",
      "Epoch 1701/150000, Loss: 22376, Val Loss: 22768,  Learning Rate: 0.00529, Train Gradient: 140.9\n",
      "Epoch 1801/150000, Loss: 22003, Val Loss: 22417,  Learning Rate: 0.00517, Train Gradient: 148.0\n",
      "Epoch 1901/150000, Loss: 21712, Val Loss: 22121,  Learning Rate: 0.00507, Train Gradient: 148.4\n",
      "Epoch 2001/150000, Loss: 21437, Val Loss: 21838,  Learning Rate: 0.00498, Train Gradient: 146.7\n",
      "Epoch 2101/150000, Loss: 21174, Val Loss: 21570,  Learning Rate: 0.00489, Train Gradient: 144.8\n",
      "Epoch 2201/150000, Loss: 20920, Val Loss: 21312,  Learning Rate: 0.00480, Train Gradient: 143.0\n",
      "Epoch 2301/150000, Loss: 20674, Val Loss: 21062,  Learning Rate: 0.00472, Train Gradient: 141.3\n",
      "Epoch 2401/150000, Loss: 20435, Val Loss: 20820,  Learning Rate: 0.00464, Train Gradient: 139.7\n",
      "Epoch 2501/150000, Loss: 20204, Val Loss: 20585,  Learning Rate: 0.00456, Train Gradient: 138.1\n",
      "Epoch 2601/150000, Loss: 19979, Val Loss: 20356,  Learning Rate: 0.00449, Train Gradient: 136.5\n",
      "Epoch 2701/150000, Loss: 19761, Val Loss: 20134,  Learning Rate: 0.00441, Train Gradient: 135.0\n",
      "Epoch 2801/150000, Loss: 19549, Val Loss: 19917,  Learning Rate: 0.00434, Train Gradient: 133.6\n",
      "Epoch 2901/150000, Loss: 19343, Val Loss: 19707,  Learning Rate: 0.00427, Train Gradient: 132.2\n",
      "Epoch 3001/150000, Loss: 19142, Val Loss: 19503,  Learning Rate: 0.00420, Train Gradient: 130.8\n",
      "Epoch 3101/150000, Loss: 18946, Val Loss: 19304,  Learning Rate: 0.00414, Train Gradient: 129.5\n",
      "Epoch 3201/150000, Loss: 18756, Val Loss: 19110,  Learning Rate: 0.00408, Train Gradient: 128.2\n",
      "Epoch 3301/150000, Loss: 18570, Val Loss: 18921,  Learning Rate: 0.00401, Train Gradient: 127.0\n",
      "Epoch 3401/150000, Loss: 18389, Val Loss: 18737,  Learning Rate: 0.00395, Train Gradient: 125.7\n",
      "Epoch 3501/150000, Loss: 18212, Val Loss: 18557,  Learning Rate: 0.00390, Train Gradient: 124.5\n",
      "Epoch 3601/150000, Loss: 18039, Val Loss: 18382,  Learning Rate: 0.00384, Train Gradient: 123.4\n",
      "Epoch 3701/150000, Loss: 17870, Val Loss: 18210,  Learning Rate: 0.00378, Train Gradient: 122.2\n",
      "Epoch 3801/150000, Loss: 17705, Val Loss: 18043,  Learning Rate: 0.00373, Train Gradient: 121.1\n",
      "Epoch 3901/150000, Loss: 17544, Val Loss: 17879,  Learning Rate: 0.00368, Train Gradient: 120.0\n",
      "Epoch 4001/150000, Loss: 17386, Val Loss: 17720,  Learning Rate: 0.00363, Train Gradient: 118.9\n",
      "Epoch 4101/150000, Loss: 17232, Val Loss: 17563,  Learning Rate: 0.00358, Train Gradient: 117.8\n",
      "Epoch 4201/150000, Loss: 17082, Val Loss: 17410,  Learning Rate: 0.00353, Train Gradient: 116.8\n",
      "Epoch 4301/150000, Loss: 16934, Val Loss: 17260,  Learning Rate: 0.00348, Train Gradient: 115.7\n",
      "Epoch 4401/150000, Loss: 16790, Val Loss: 17114,  Learning Rate: 0.00344, Train Gradient: 114.7\n",
      "Epoch 4501/150000, Loss: 16649, Val Loss: 16971,  Learning Rate: 0.00339, Train Gradient: 113.7\n",
      "Epoch 4601/150000, Loss: 16510, Val Loss: 16830,  Learning Rate: 0.00335, Train Gradient: 112.7\n",
      "Epoch 4701/150000, Loss: 16375, Val Loss: 16693,  Learning Rate: 0.00331, Train Gradient: 111.8\n",
      "Epoch 4801/150000, Loss: 16243, Val Loss: 16558,  Learning Rate: 0.00327, Train Gradient: 110.8\n",
      "Epoch 4901/150000, Loss: 16113, Val Loss: 16426,  Learning Rate: 0.00323, Train Gradient: 109.9\n",
      "Epoch 5001/150000, Loss: 15985, Val Loss: 16297,  Learning Rate: 0.00319, Train Gradient: 108.9\n",
      "Epoch 5101/150000, Loss: 15861, Val Loss: 16170,  Learning Rate: 0.00315, Train Gradient: 108.0\n",
      "Epoch 5201/150000, Loss: 15738, Val Loss: 16046,  Learning Rate: 0.00311, Train Gradient: 107.2\n",
      "Epoch 5301/150000, Loss: 15618, Val Loss: 15924,  Learning Rate: 0.00307, Train Gradient: 106.3\n",
      "Epoch 5401/150000, Loss: 15501, Val Loss: 15805,  Learning Rate: 0.00304, Train Gradient: 105.4\n",
      "Epoch 5501/150000, Loss: 15385, Val Loss: 15687,  Learning Rate: 0.00300, Train Gradient: 104.6\n",
      "Epoch 5601/150000, Loss: 15272, Val Loss: 15572,  Learning Rate: 0.00297, Train Gradient: 103.7\n",
      "Epoch 5701/150000, Loss: 15161, Val Loss: 15459,  Learning Rate: 0.00294, Train Gradient: 102.9\n",
      "Epoch 5801/150000, Loss: 15052, Val Loss: 15348,  Learning Rate: 0.00290, Train Gradient: 102.1\n",
      "Epoch 5901/150000, Loss: 14944, Val Loss: 15239,  Learning Rate: 0.00287, Train Gradient: 101.4\n",
      "Epoch 6001/150000, Loss: 14839, Val Loss: 15132,  Learning Rate: 0.00284, Train Gradient: 100.6\n",
      "Epoch 6101/150000, Loss: 14736, Val Loss: 15027,  Learning Rate: 0.00281, Train Gradient: 99.9\n",
      "Epoch 6201/150000, Loss: 14634, Val Loss: 14924,  Learning Rate: 0.00278, Train Gradient: 99.1\n",
      "Epoch 6301/150000, Loss: 14534, Val Loss: 14822,  Learning Rate: 0.00275, Train Gradient: 98.4\n",
      "Epoch 6401/150000, Loss: 14435, Val Loss: 14723,  Learning Rate: 0.00272, Train Gradient: 97.7\n",
      "Epoch 6501/150000, Loss: 14339, Val Loss: 14625,  Learning Rate: 0.00269, Train Gradient: 97.0\n",
      "Epoch 6601/150000, Loss: 14244, Val Loss: 14528,  Learning Rate: 0.00267, Train Gradient: 96.3\n",
      "Epoch 6701/150000, Loss: 14150, Val Loss: 14433,  Learning Rate: 0.00264, Train Gradient: 95.6\n",
      "Epoch 6801/150000, Loss: 14058, Val Loss: 14340,  Learning Rate: 0.00261, Train Gradient: 95.0\n",
      "Epoch 6901/150000, Loss: 13968, Val Loss: 14248,  Learning Rate: 0.00259, Train Gradient: 94.3\n",
      "Epoch 7001/150000, Loss: 13878, Val Loss: 14158,  Learning Rate: 0.00256, Train Gradient: 93.6\n",
      "Epoch 7101/150000, Loss: 13791, Val Loss: 14069,  Learning Rate: 0.00254, Train Gradient: 93.0\n",
      "Epoch 7201/150000, Loss: 13704, Val Loss: 13982,  Learning Rate: 0.00251, Train Gradient: 92.4\n",
      "Epoch 7301/150000, Loss: 13620, Val Loss: 13895,  Learning Rate: 0.00249, Train Gradient: 91.8\n",
      "Epoch 7401/150000, Loss: 13536, Val Loss: 13810,  Learning Rate: 0.00246, Train Gradient: 91.2\n",
      "Epoch 7501/150000, Loss: 13453, Val Loss: 13727,  Learning Rate: 0.00244, Train Gradient: 90.6\n",
      "Epoch 7601/150000, Loss: 13372, Val Loss: 13645,  Learning Rate: 0.00242, Train Gradient: 90.0\n",
      "Epoch 7701/150000, Loss: 13292, Val Loss: 13563,  Learning Rate: 0.00240, Train Gradient: 89.4\n",
      "Epoch 7801/150000, Loss: 13213, Val Loss: 13483,  Learning Rate: 0.00237, Train Gradient: 88.9\n",
      "Epoch 7901/150000, Loss: 13136, Val Loss: 13405,  Learning Rate: 0.00235, Train Gradient: 88.3\n",
      "Epoch 8001/150000, Loss: 13059, Val Loss: 13327,  Learning Rate: 0.00233, Train Gradient: 87.7\n",
      "Epoch 8101/150000, Loss: 12984, Val Loss: 13251,  Learning Rate: 0.00231, Train Gradient: 87.2\n",
      "Epoch 8201/150000, Loss: 12909, Val Loss: 13175,  Learning Rate: 0.00229, Train Gradient: 86.7\n",
      "Epoch 8301/150000, Loss: 12836, Val Loss: 13101,  Learning Rate: 0.00227, Train Gradient: 86.2\n",
      "Epoch 8401/150000, Loss: 12763, Val Loss: 13027,  Learning Rate: 0.00225, Train Gradient: 85.6\n",
      "Epoch 8501/150000, Loss: 12692, Val Loss: 12955,  Learning Rate: 0.00223, Train Gradient: 85.2\n",
      "Epoch 8601/150000, Loss: 12622, Val Loss: 12883,  Learning Rate: 0.00221, Train Gradient: 84.6\n",
      "Epoch 8701/150000, Loss: 12552, Val Loss: 12813,  Learning Rate: 0.00220, Train Gradient: 84.1\n",
      "Epoch 8801/150000, Loss: 12484, Val Loss: 12743,  Learning Rate: 0.00218, Train Gradient: 83.7\n",
      "Epoch 8901/150000, Loss: 12416, Val Loss: 12675,  Learning Rate: 0.00216, Train Gradient: 83.2\n",
      "Epoch 9001/150000, Loss: 12349, Val Loss: 12607,  Learning Rate: 0.00214, Train Gradient: 82.7\n",
      "Epoch 9101/150000, Loss: 12283, Val Loss: 12540,  Learning Rate: 0.00213, Train Gradient: 82.3\n",
      "Epoch 9201/150000, Loss: 12218, Val Loss: 12474,  Learning Rate: 0.00211, Train Gradient: 81.8\n",
      "Epoch 9301/150000, Loss: 12154, Val Loss: 12409,  Learning Rate: 0.00209, Train Gradient: 81.4\n",
      "Epoch 9401/150000, Loss: 12090, Val Loss: 12345,  Learning Rate: 0.00208, Train Gradient: 80.9\n",
      "Epoch 9501/150000, Loss: 12028, Val Loss: 12281,  Learning Rate: 0.00206, Train Gradient: 80.5\n",
      "Epoch 9601/150000, Loss: 11966, Val Loss: 12219,  Learning Rate: 0.00204, Train Gradient: 80.1\n",
      "Epoch 9701/150000, Loss: 11905, Val Loss: 12157,  Learning Rate: 0.00203, Train Gradient: 79.7\n",
      "Epoch 9801/150000, Loss: 11844, Val Loss: 12096,  Learning Rate: 0.00201, Train Gradient: 79.3\n",
      "Epoch 9901/150000, Loss: 11785, Val Loss: 12035,  Learning Rate: 0.00200, Train Gradient: 78.9\n",
      "Epoch 10001/150000, Loss: 11726, Val Loss: 11976,  Learning Rate: 0.00198, Train Gradient: 78.4\n",
      "Epoch 10101/150000, Loss: 11667, Val Loss: 11917,  Learning Rate: 0.00197, Train Gradient: 78.1\n",
      "Epoch 10201/150000, Loss: 11610, Val Loss: 11858,  Learning Rate: 0.00195, Train Gradient: 77.7\n",
      "Epoch 10301/150000, Loss: 11553, Val Loss: 11801,  Learning Rate: 0.00194, Train Gradient: 77.3\n",
      "Epoch 10401/150000, Loss: 11497, Val Loss: 11744,  Learning Rate: 0.00193, Train Gradient: 76.9\n",
      "Epoch 10501/150000, Loss: 11441, Val Loss: 11688,  Learning Rate: 0.00191, Train Gradient: 76.5\n",
      "Epoch 10601/150000, Loss: 11386, Val Loss: 11632,  Learning Rate: 0.00190, Train Gradient: 76.1\n",
      "Epoch 10701/150000, Loss: 11332, Val Loss: 11576,  Learning Rate: 0.00188, Train Gradient: 75.8\n",
      "Epoch 10801/150000, Loss: 11278, Val Loss: 11522,  Learning Rate: 0.00187, Train Gradient: 75.4\n",
      "Epoch 10901/150000, Loss: 11225, Val Loss: 11467,  Learning Rate: 0.00186, Train Gradient: 75.0\n",
      "Epoch 11001/150000, Loss: 11172, Val Loss: 11414,  Learning Rate: 0.00185, Train Gradient: 74.7\n",
      "Epoch 11101/150000, Loss: 11120, Val Loss: 11362,  Learning Rate: 0.00183, Train Gradient: 74.3\n",
      "Epoch 11201/150000, Loss: 11069, Val Loss: 11310,  Learning Rate: 0.00182, Train Gradient: 74.0\n",
      "Epoch 11301/150000, Loss: 11018, Val Loss: 11258,  Learning Rate: 0.00181, Train Gradient: 73.7\n",
      "Epoch 11401/150000, Loss: 10968, Val Loss: 11208,  Learning Rate: 0.00180, Train Gradient: 73.3\n",
      "Epoch 11501/150000, Loss: 10918, Val Loss: 11158,  Learning Rate: 0.00178, Train Gradient: 73.0\n",
      "Epoch 11601/150000, Loss: 10869, Val Loss: 11108,  Learning Rate: 0.00177, Train Gradient: 72.6\n",
      "Epoch 11701/150000, Loss: 10820, Val Loss: 11059,  Learning Rate: 0.00176, Train Gradient: 72.3\n",
      "Epoch 11801/150000, Loss: 10772, Val Loss: 11010,  Learning Rate: 0.00175, Train Gradient: 72.0\n",
      "Epoch 11901/150000, Loss: 10725, Val Loss: 10962,  Learning Rate: 0.00174, Train Gradient: 71.7\n",
      "Epoch 12001/150000, Loss: 10678, Val Loss: 10914,  Learning Rate: 0.00173, Train Gradient: 71.4\n",
      "Epoch 12101/150000, Loss: 10631, Val Loss: 10867,  Learning Rate: 0.00172, Train Gradient: 71.1\n",
      "Epoch 12201/150000, Loss: 10585, Val Loss: 10820,  Learning Rate: 0.00171, Train Gradient: 70.8\n",
      "Epoch 12301/150000, Loss: 10539, Val Loss: 10774,  Learning Rate: 0.00170, Train Gradient: 70.5\n",
      "Epoch 12401/150000, Loss: 10494, Val Loss: 10728,  Learning Rate: 0.00168, Train Gradient: 70.2\n",
      "Epoch 12501/150000, Loss: 10449, Val Loss: 10683,  Learning Rate: 0.00167, Train Gradient: 69.9\n",
      "Epoch 12601/150000, Loss: 10404, Val Loss: 10638,  Learning Rate: 0.00166, Train Gradient: 69.6\n",
      "Epoch 12701/150000, Loss: 10360, Val Loss: 10593,  Learning Rate: 0.00165, Train Gradient: 69.3\n",
      "Epoch 12801/150000, Loss: 10317, Val Loss: 10549,  Learning Rate: 0.00164, Train Gradient: 69.0\n",
      "Epoch 12901/150000, Loss: 10274, Val Loss: 10506,  Learning Rate: 0.00163, Train Gradient: 68.7\n",
      "Epoch 13001/150000, Loss: 10231, Val Loss: 10462,  Learning Rate: 0.00162, Train Gradient: 68.5\n",
      "Epoch 13101/150000, Loss: 10189, Val Loss: 10419,  Learning Rate: 0.00162, Train Gradient: 68.2\n",
      "Epoch 13201/150000, Loss: 10147, Val Loss: 10377,  Learning Rate: 0.00161, Train Gradient: 67.9\n",
      "Epoch 13301/150000, Loss: 10105, Val Loss: 10335,  Learning Rate: 0.00160, Train Gradient: 67.6\n",
      "Epoch 13401/150000, Loss: 10064, Val Loss: 10293,  Learning Rate: 0.00159, Train Gradient: 67.4\n",
      "Epoch 13501/150000, Loss: 10023, Val Loss: 10252,  Learning Rate: 0.00158, Train Gradient: 67.1\n",
      "Epoch 13601/150000, Loss: 9983, Val Loss: 10211,  Learning Rate: 0.00157, Train Gradient: 66.9\n",
      "Epoch 13701/150000, Loss: 9943, Val Loss: 10170,  Learning Rate: 0.00156, Train Gradient: 66.6\n",
      "Epoch 13801/150000, Loss: 9903, Val Loss: 10130,  Learning Rate: 0.00155, Train Gradient: 66.4\n",
      "Epoch 13901/150000, Loss: 9864, Val Loss: 10090,  Learning Rate: 0.00154, Train Gradient: 66.1\n",
      "Epoch 14001/150000, Loss: 9825, Val Loss: 10050,  Learning Rate: 0.00153, Train Gradient: 65.9\n",
      "Epoch 14101/150000, Loss: 9786, Val Loss: 10011,  Learning Rate: 0.00153, Train Gradient: 65.6\n",
      "Epoch 14201/150000, Loss: 9748, Val Loss: 9972,  Learning Rate: 0.00152, Train Gradient: 65.4\n",
      "Epoch 14301/150000, Loss: 9710, Val Loss: 9934,  Learning Rate: 0.00151, Train Gradient: 65.1\n",
      "Epoch 14401/150000, Loss: 9673, Val Loss: 9896,  Learning Rate: 0.00150, Train Gradient: 64.9\n",
      "Epoch 14501/150000, Loss: 9635, Val Loss: 9858,  Learning Rate: 0.00149, Train Gradient: 64.7\n",
      "Epoch 14601/150000, Loss: 9598, Val Loss: 9820,  Learning Rate: 0.00148, Train Gradient: 64.4\n",
      "Epoch 14701/150000, Loss: 9562, Val Loss: 9783,  Learning Rate: 0.00148, Train Gradient: 64.2\n",
      "Epoch 14801/150000, Loss: 9526, Val Loss: 9747,  Learning Rate: 0.00147, Train Gradient: 64.0\n",
      "Epoch 14901/150000, Loss: 9490, Val Loss: 9710,  Learning Rate: 0.00146, Train Gradient: 63.8\n",
      "Epoch 15001/150000, Loss: 9454, Val Loss: 9674,  Learning Rate: 0.00145, Train Gradient: 63.5\n",
      "Epoch 15101/150000, Loss: 9418, Val Loss: 9638,  Learning Rate: 0.00145, Train Gradient: 63.3\n",
      "Epoch 15201/150000, Loss: 9383, Val Loss: 9603,  Learning Rate: 0.00144, Train Gradient: 63.1\n",
      "Epoch 15301/150000, Loss: 9348, Val Loss: 9567,  Learning Rate: 0.00143, Train Gradient: 62.9\n",
      "Epoch 15401/150000, Loss: 9314, Val Loss: 9532,  Learning Rate: 0.00142, Train Gradient: 62.7\n",
      "Epoch 15501/150000, Loss: 9280, Val Loss: 9498,  Learning Rate: 0.00142, Train Gradient: 62.5\n",
      "Epoch 15601/150000, Loss: 9246, Val Loss: 9464,  Learning Rate: 0.00141, Train Gradient: 62.3\n",
      "Epoch 15701/150000, Loss: 9212, Val Loss: 9429,  Learning Rate: 0.00140, Train Gradient: 62.1\n",
      "Epoch 15801/150000, Loss: 9178, Val Loss: 9396,  Learning Rate: 0.00140, Train Gradient: 61.9\n",
      "Epoch 15901/150000, Loss: 9145, Val Loss: 9362,  Learning Rate: 0.00139, Train Gradient: 61.7\n",
      "Epoch 16001/150000, Loss: 9112, Val Loss: 9329,  Learning Rate: 0.00138, Train Gradient: 61.5\n",
      "Epoch 16101/150000, Loss: 9080, Val Loss: 9296,  Learning Rate: 0.00138, Train Gradient: 61.3\n",
      "Epoch 16201/150000, Loss: 9047, Val Loss: 9263,  Learning Rate: 0.00137, Train Gradient: 61.1\n",
      "Epoch 16301/150000, Loss: 9015, Val Loss: 9231,  Learning Rate: 0.00136, Train Gradient: 60.9\n",
      "Epoch 16401/150000, Loss: 8983, Val Loss: 9198,  Learning Rate: 0.00136, Train Gradient: 60.7\n",
      "Epoch 16501/150000, Loss: 8951, Val Loss: 9166,  Learning Rate: 0.00135, Train Gradient: 60.6\n",
      "Epoch 16601/150000, Loss: 8920, Val Loss: 9134,  Learning Rate: 0.00134, Train Gradient: 60.4\n",
      "Epoch 16701/150000, Loss: 8888, Val Loss: 9102,  Learning Rate: 0.00134, Train Gradient: 60.2\n",
      "Epoch 16801/150000, Loss: 8857, Val Loss: 9070,  Learning Rate: 0.00133, Train Gradient: 60.1\n",
      "Epoch 16901/150000, Loss: 8826, Val Loss: 9039,  Learning Rate: 0.00132, Train Gradient: 59.9\n",
      "Epoch 17001/150000, Loss: 8796, Val Loss: 9008,  Learning Rate: 0.00132, Train Gradient: 59.7\n",
      "Epoch 17101/150000, Loss: 8765, Val Loss: 8978,  Learning Rate: 0.00131, Train Gradient: 59.5\n",
      "Epoch 17201/150000, Loss: 8735, Val Loss: 8947,  Learning Rate: 0.00131, Train Gradient: 59.3\n",
      "Epoch 17301/150000, Loss: 8705, Val Loss: 8917,  Learning Rate: 0.00130, Train Gradient: 59.2\n",
      "Epoch 17401/150000, Loss: 8676, Val Loss: 8887,  Learning Rate: 0.00129, Train Gradient: 59.0\n",
      "Epoch 17501/150000, Loss: 8646, Val Loss: 8857,  Learning Rate: 0.00129, Train Gradient: 58.8\n",
      "Epoch 17601/150000, Loss: 8617, Val Loss: 8827,  Learning Rate: 0.00128, Train Gradient: 58.7\n",
      "Epoch 17701/150000, Loss: 8588, Val Loss: 8798,  Learning Rate: 0.00128, Train Gradient: 58.5\n",
      "Epoch 17801/150000, Loss: 8559, Val Loss: 8769,  Learning Rate: 0.00127, Train Gradient: 58.4\n",
      "Epoch 17901/150000, Loss: 8530, Val Loss: 8740,  Learning Rate: 0.00127, Train Gradient: 58.2\n",
      "Epoch 18001/150000, Loss: 8501, Val Loss: 8711,  Learning Rate: 0.00126, Train Gradient: 58.0\n",
      "Epoch 18101/150000, Loss: 8473, Val Loss: 8683,  Learning Rate: 0.00126, Train Gradient: 57.9\n",
      "Epoch 18201/150000, Loss: 8445, Val Loss: 8654,  Learning Rate: 0.00125, Train Gradient: 57.7\n",
      "Epoch 18301/150000, Loss: 8417, Val Loss: 8626,  Learning Rate: 0.00124, Train Gradient: 57.6\n",
      "Epoch 18401/150000, Loss: 8389, Val Loss: 8598,  Learning Rate: 0.00124, Train Gradient: 57.4\n",
      "Epoch 18501/150000, Loss: 8362, Val Loss: 8570,  Learning Rate: 0.00123, Train Gradient: 57.2\n",
      "Epoch 18601/150000, Loss: 8334, Val Loss: 8542,  Learning Rate: 0.00123, Train Gradient: 57.1\n",
      "Epoch 18701/150000, Loss: 8307, Val Loss: 8515,  Learning Rate: 0.00122, Train Gradient: 57.0\n",
      "Epoch 18801/150000, Loss: 8280, Val Loss: 8488,  Learning Rate: 0.00122, Train Gradient: 56.8\n",
      "Epoch 18901/150000, Loss: 8253, Val Loss: 8461,  Learning Rate: 0.00121, Train Gradient: 56.7\n",
      "Epoch 19001/150000, Loss: 8227, Val Loss: 8434,  Learning Rate: 0.00121, Train Gradient: 56.5\n",
      "Epoch 19101/150000, Loss: 8200, Val Loss: 8407,  Learning Rate: 0.00120, Train Gradient: 56.4\n",
      "Epoch 19201/150000, Loss: 8174, Val Loss: 8380,  Learning Rate: 0.00120, Train Gradient: 56.2\n",
      "Epoch 19301/150000, Loss: 8148, Val Loss: 8354,  Learning Rate: 0.00119, Train Gradient: 56.1\n",
      "Epoch 19401/150000, Loss: 8122, Val Loss: 8327,  Learning Rate: 0.00119, Train Gradient: 56.0\n",
      "Epoch 19501/150000, Loss: 8096, Val Loss: 8301,  Learning Rate: 0.00118, Train Gradient: 55.8\n",
      "Epoch 19601/150000, Loss: 8070, Val Loss: 8275,  Learning Rate: 0.00118, Train Gradient: 55.7\n",
      "Epoch 19701/150000, Loss: 8045, Val Loss: 8250,  Learning Rate: 0.00117, Train Gradient: 55.6\n",
      "Epoch 19801/150000, Loss: 8019, Val Loss: 8224,  Learning Rate: 0.00117, Train Gradient: 55.4\n",
      "Epoch 19901/150000, Loss: 7994, Val Loss: 8199,  Learning Rate: 0.00117, Train Gradient: 55.3\n",
      "Epoch 20001/150000, Loss: 7969, Val Loss: 8173,  Learning Rate: 0.00116, Train Gradient: 55.1\n",
      "Epoch 20101/150000, Loss: 7944, Val Loss: 8148,  Learning Rate: 0.00116, Train Gradient: 55.0\n",
      "Epoch 20201/150000, Loss: 7920, Val Loss: 8123,  Learning Rate: 0.00115, Train Gradient: 54.9\n",
      "Epoch 20301/150000, Loss: 7895, Val Loss: 8098,  Learning Rate: 0.00115, Train Gradient: 54.7\n",
      "Epoch 20401/150000, Loss: 7871, Val Loss: 8074,  Learning Rate: 0.00114, Train Gradient: 54.6\n",
      "Epoch 20501/150000, Loss: 7846, Val Loss: 8049,  Learning Rate: 0.00114, Train Gradient: 54.5\n",
      "Epoch 20601/150000, Loss: 7822, Val Loss: 8025,  Learning Rate: 0.00113, Train Gradient: 54.4\n",
      "Epoch 20701/150000, Loss: 7798, Val Loss: 8001,  Learning Rate: 0.00113, Train Gradient: 54.2\n",
      "Epoch 20801/150000, Loss: 7774, Val Loss: 7977,  Learning Rate: 0.00113, Train Gradient: 54.1\n",
      "Epoch 20901/150000, Loss: 7751, Val Loss: 7953,  Learning Rate: 0.00112, Train Gradient: 54.0\n",
      "Epoch 21001/150000, Loss: 7727, Val Loss: 7929,  Learning Rate: 0.00112, Train Gradient: 53.8\n",
      "Epoch 21101/150000, Loss: 7704, Val Loss: 7905,  Learning Rate: 0.00111, Train Gradient: 53.7\n",
      "Epoch 21201/150000, Loss: 7681, Val Loss: 7882,  Learning Rate: 0.00111, Train Gradient: 53.6\n",
      "Epoch 21301/150000, Loss: 7657, Val Loss: 7858,  Learning Rate: 0.00110, Train Gradient: 53.5\n",
      "Epoch 21401/150000, Loss: 7634, Val Loss: 7835,  Learning Rate: 0.00110, Train Gradient: 53.4\n",
      "Epoch 21501/150000, Loss: 7612, Val Loss: 7812,  Learning Rate: 0.00110, Train Gradient: 53.2\n",
      "Epoch 21601/150000, Loss: 7589, Val Loss: 7789,  Learning Rate: 0.00109, Train Gradient: 53.1\n",
      "Epoch 21701/150000, Loss: 7566, Val Loss: 7766,  Learning Rate: 0.00109, Train Gradient: 53.0\n",
      "Epoch 21801/150000, Loss: 7544, Val Loss: 7743,  Learning Rate: 0.00108, Train Gradient: 52.9\n",
      "Epoch 21901/150000, Loss: 7521, Val Loss: 7721,  Learning Rate: 0.00108, Train Gradient: 52.8\n",
      "Epoch 22001/150000, Loss: 7499, Val Loss: 7699,  Learning Rate: 0.00108, Train Gradient: 52.6\n",
      "Epoch 22101/150000, Loss: 7477, Val Loss: 7676,  Learning Rate: 0.00107, Train Gradient: 52.5\n",
      "Epoch 22201/150000, Loss: 7455, Val Loss: 7654,  Learning Rate: 0.00107, Train Gradient: 52.4\n",
      "Epoch 22301/150000, Loss: 7433, Val Loss: 7632,  Learning Rate: 0.00107, Train Gradient: 52.3\n",
      "Epoch 22401/150000, Loss: 7412, Val Loss: 7610,  Learning Rate: 0.00106, Train Gradient: 52.2\n",
      "Epoch 22501/150000, Loss: 7390, Val Loss: 7588,  Learning Rate: 0.00106, Train Gradient: 52.1\n",
      "Epoch 22601/150000, Loss: 7369, Val Loss: 7567,  Learning Rate: 0.00105, Train Gradient: 52.0\n",
      "Epoch 22701/150000, Loss: 7347, Val Loss: 7545,  Learning Rate: 0.00105, Train Gradient: 51.8\n",
      "Epoch 22801/150000, Loss: 7326, Val Loss: 7524,  Learning Rate: 0.00105, Train Gradient: 51.7\n",
      "Epoch 22901/150000, Loss: 7305, Val Loss: 7502,  Learning Rate: 0.00104, Train Gradient: 51.6\n",
      "Epoch 23001/150000, Loss: 7284, Val Loss: 7481,  Learning Rate: 0.00104, Train Gradient: 51.5\n",
      "Epoch 23101/150000, Loss: 7263, Val Loss: 7460,  Learning Rate: 0.00104, Train Gradient: 51.4\n",
      "Epoch 23201/150000, Loss: 7242, Val Loss: 7439,  Learning Rate: 0.00103, Train Gradient: 51.3\n",
      "Epoch 23301/150000, Loss: 7221, Val Loss: 7418,  Learning Rate: 0.00103, Train Gradient: 51.2\n",
      "Epoch 23401/150000, Loss: 7201, Val Loss: 7398,  Learning Rate: 0.00103, Train Gradient: 51.1\n",
      "Epoch 23501/150000, Loss: 7180, Val Loss: 7377,  Learning Rate: 0.00102, Train Gradient: 51.0\n",
      "Epoch 23601/150000, Loss: 7160, Val Loss: 7356,  Learning Rate: 0.00102, Train Gradient: 50.9\n",
      "Epoch 23701/150000, Loss: 7140, Val Loss: 7336,  Learning Rate: 0.00102, Train Gradient: 50.8\n",
      "Epoch 23801/150000, Loss: 7120, Val Loss: 7316,  Learning Rate: 0.00101, Train Gradient: 50.6\n",
      "Epoch 23901/150000, Loss: 7100, Val Loss: 7295,  Learning Rate: 0.00101, Train Gradient: 50.6\n",
      "Epoch 24001/150000, Loss: 7080, Val Loss: 7275,  Learning Rate: 0.00101, Train Gradient: 50.5\n",
      "Epoch 24101/150000, Loss: 7060, Val Loss: 7255,  Learning Rate: 0.00100, Train Gradient: 50.4\n",
      "Epoch 24201/150000, Loss: 7040, Val Loss: 7235,  Learning Rate: 0.00100, Train Gradient: 50.3\n",
      "Epoch 24301/150000, Loss: 7021, Val Loss: 7216,  Learning Rate: 0.00100, Train Gradient: 50.2\n",
      "Epoch 24401/150000, Loss: 7001, Val Loss: 7196,  Learning Rate: 0.00099, Train Gradient: 50.1\n",
      "Epoch 24501/150000, Loss: 6982, Val Loss: 7176,  Learning Rate: 0.00099, Train Gradient: 49.9\n",
      "Epoch 24601/150000, Loss: 6962, Val Loss: 7157,  Learning Rate: 0.00099, Train Gradient: 49.9\n",
      "Epoch 24701/150000, Loss: 6943, Val Loss: 7137,  Learning Rate: 0.00098, Train Gradient: 49.8\n",
      "Epoch 24801/150000, Loss: 6924, Val Loss: 7118,  Learning Rate: 0.00098, Train Gradient: 49.7\n",
      "Epoch 24901/150000, Loss: 6905, Val Loss: 7099,  Learning Rate: 0.00098, Train Gradient: 49.6\n",
      "Epoch 25001/150000, Loss: 6886, Val Loss: 7080,  Learning Rate: 0.00097, Train Gradient: 49.4\n",
      "Epoch 25101/150000, Loss: 6867, Val Loss: 7061,  Learning Rate: 0.00097, Train Gradient: 49.4\n",
      "Epoch 25201/150000, Loss: 6848, Val Loss: 7042,  Learning Rate: 0.00097, Train Gradient: 49.3\n",
      "Epoch 25301/150000, Loss: 6830, Val Loss: 7023,  Learning Rate: 0.00097, Train Gradient: 49.2\n",
      "Epoch 25401/150000, Loss: 6811, Val Loss: 7004,  Learning Rate: 0.00096, Train Gradient: 49.1\n",
      "Epoch 25501/150000, Loss: 6793, Val Loss: 6985,  Learning Rate: 0.00096, Train Gradient: 49.0\n",
      "Epoch 25601/150000, Loss: 6774, Val Loss: 6967,  Learning Rate: 0.00096, Train Gradient: 48.9\n",
      "Epoch 25701/150000, Loss: 6756, Val Loss: 6948,  Learning Rate: 0.00095, Train Gradient: 48.8\n",
      "Epoch 25801/150000, Loss: 6738, Val Loss: 6930,  Learning Rate: 0.00095, Train Gradient: 48.7\n",
      "Epoch 25901/150000, Loss: 6720, Val Loss: 6912,  Learning Rate: 0.00095, Train Gradient: 48.6\n",
      "Epoch 26001/150000, Loss: 6702, Val Loss: 6894,  Learning Rate: 0.00095, Train Gradient: 48.5\n",
      "Epoch 26101/150000, Loss: 6684, Val Loss: 6875,  Learning Rate: 0.00094, Train Gradient: 48.4\n",
      "Epoch 26201/150000, Loss: 6666, Val Loss: 6857,  Learning Rate: 0.00094, Train Gradient: 48.3\n",
      "Epoch 26301/150000, Loss: 6648, Val Loss: 6839,  Learning Rate: 0.00094, Train Gradient: 48.2\n",
      "Epoch 26401/150000, Loss: 6631, Val Loss: 6822,  Learning Rate: 0.00093, Train Gradient: 48.1\n",
      "Epoch 26501/150000, Loss: 6613, Val Loss: 6804,  Learning Rate: 0.00093, Train Gradient: 48.0\n",
      "Epoch 26601/150000, Loss: 6595, Val Loss: 6786,  Learning Rate: 0.00093, Train Gradient: 47.9\n",
      "Epoch 26701/150000, Loss: 6578, Val Loss: 6768,  Learning Rate: 0.00093, Train Gradient: 47.8\n",
      "Epoch 26801/150000, Loss: 6561, Val Loss: 6751,  Learning Rate: 0.00092, Train Gradient: 47.8\n",
      "Epoch 26901/150000, Loss: 6543, Val Loss: 6733,  Learning Rate: 0.00092, Train Gradient: 47.7\n",
      "Epoch 27001/150000, Loss: 6526, Val Loss: 6716,  Learning Rate: 0.00092, Train Gradient: 47.6\n",
      "Epoch 27101/150000, Loss: 6509, Val Loss: 6699,  Learning Rate: 0.00092, Train Gradient: 47.5\n",
      "Epoch 27201/150000, Loss: 6492, Val Loss: 6681,  Learning Rate: 0.00091, Train Gradient: 47.4\n",
      "Epoch 27301/150000, Loss: 6475, Val Loss: 6664,  Learning Rate: 0.00091, Train Gradient: 47.3\n",
      "Epoch 27401/150000, Loss: 6458, Val Loss: 6647,  Learning Rate: 0.00091, Train Gradient: 47.2\n",
      "Epoch 27501/150000, Loss: 6442, Val Loss: 6630,  Learning Rate: 0.00091, Train Gradient: 47.1\n",
      "Epoch 27601/150000, Loss: 6425, Val Loss: 6613,  Learning Rate: 0.00090, Train Gradient: 47.0\n",
      "Epoch 27701/150000, Loss: 6408, Val Loss: 6596,  Learning Rate: 0.00090, Train Gradient: 47.0\n",
      "Epoch 27801/150000, Loss: 6392, Val Loss: 6580,  Learning Rate: 0.00090, Train Gradient: 46.9\n",
      "Epoch 27901/150000, Loss: 6375, Val Loss: 6563,  Learning Rate: 0.00090, Train Gradient: 46.8\n",
      "Epoch 28001/150000, Loss: 6359, Val Loss: 6546,  Learning Rate: 0.00089, Train Gradient: 46.7\n",
      "Epoch 28101/150000, Loss: 6342, Val Loss: 6530,  Learning Rate: 0.00089, Train Gradient: 46.6\n",
      "Epoch 28201/150000, Loss: 6326, Val Loss: 6513,  Learning Rate: 0.00089, Train Gradient: 46.5\n",
      "Epoch 28301/150000, Loss: 6310, Val Loss: 6497,  Learning Rate: 0.00089, Train Gradient: 46.5\n",
      "Epoch 28401/150000, Loss: 6294, Val Loss: 6481,  Learning Rate: 0.00088, Train Gradient: 46.4\n",
      "Epoch 28501/150000, Loss: 6278, Val Loss: 6464,  Learning Rate: 0.00088, Train Gradient: 46.3\n",
      "Epoch 28601/150000, Loss: 6262, Val Loss: 6448,  Learning Rate: 0.00088, Train Gradient: 46.2\n",
      "Epoch 28701/150000, Loss: 6246, Val Loss: 6432,  Learning Rate: 0.00088, Train Gradient: 46.1\n",
      "Epoch 28801/150000, Loss: 6230, Val Loss: 6416,  Learning Rate: 0.00087, Train Gradient: 46.0\n",
      "Epoch 28901/150000, Loss: 6214, Val Loss: 6400,  Learning Rate: 0.00087, Train Gradient: 45.9\n",
      "Epoch 29001/150000, Loss: 6199, Val Loss: 6384,  Learning Rate: 0.00087, Train Gradient: 45.9\n",
      "Epoch 29101/150000, Loss: 6183, Val Loss: 6368,  Learning Rate: 0.00087, Train Gradient: 45.8\n",
      "Epoch 29201/150000, Loss: 6168, Val Loss: 6352,  Learning Rate: 0.00086, Train Gradient: 45.7\n",
      "Epoch 29301/150000, Loss: 6152, Val Loss: 6337,  Learning Rate: 0.00086, Train Gradient: 45.6\n",
      "Epoch 29401/150000, Loss: 6137, Val Loss: 6321,  Learning Rate: 0.00086, Train Gradient: 45.5\n",
      "Epoch 29501/150000, Loss: 6121, Val Loss: 6306,  Learning Rate: 0.00086, Train Gradient: 45.5\n",
      "Epoch 29601/150000, Loss: 6106, Val Loss: 6290,  Learning Rate: 0.00086, Train Gradient: 45.4\n",
      "Epoch 29701/150000, Loss: 6091, Val Loss: 6275,  Learning Rate: 0.00085, Train Gradient: 45.3\n",
      "Epoch 29801/150000, Loss: 6076, Val Loss: 6259,  Learning Rate: 0.00085, Train Gradient: 45.2\n",
      "Epoch 29901/150000, Loss: 6061, Val Loss: 6244,  Learning Rate: 0.00085, Train Gradient: 45.1\n",
      "Epoch 30001/150000, Loss: 6046, Val Loss: 6229,  Learning Rate: 0.00085, Train Gradient: 45.1\n",
      "Epoch 30101/150000, Loss: 6031, Val Loss: 6214,  Learning Rate: 0.00084, Train Gradient: 45.0\n",
      "Epoch 30201/150000, Loss: 6016, Val Loss: 6199,  Learning Rate: 0.00084, Train Gradient: 44.9\n",
      "Epoch 30301/150000, Loss: 6001, Val Loss: 6184,  Learning Rate: 0.00084, Train Gradient: 44.8\n",
      "Epoch 30401/150000, Loss: 5986, Val Loss: 6169,  Learning Rate: 0.00084, Train Gradient: 44.7\n",
      "Epoch 30501/150000, Loss: 5971, Val Loss: 6154,  Learning Rate: 0.00084, Train Gradient: 44.7\n",
      "Epoch 30601/150000, Loss: 5957, Val Loss: 6139,  Learning Rate: 0.00083, Train Gradient: 44.6\n",
      "Epoch 30701/150000, Loss: 5942, Val Loss: 6124,  Learning Rate: 0.00083, Train Gradient: 44.5\n",
      "Epoch 30801/150000, Loss: 5928, Val Loss: 6110,  Learning Rate: 0.00083, Train Gradient: 44.4\n",
      "Epoch 30901/150000, Loss: 5913, Val Loss: 6095,  Learning Rate: 0.00083, Train Gradient: 44.4\n",
      "Epoch 31001/150000, Loss: 5899, Val Loss: 6080,  Learning Rate: 0.00083, Train Gradient: 44.3\n",
      "Epoch 31101/150000, Loss: 5884, Val Loss: 6066,  Learning Rate: 0.00082, Train Gradient: 44.2\n",
      "Epoch 31201/150000, Loss: 5870, Val Loss: 6052,  Learning Rate: 0.00082, Train Gradient: 44.1\n",
      "Epoch 31301/150000, Loss: 5856, Val Loss: 6037,  Learning Rate: 0.00082, Train Gradient: 44.0\n",
      "Epoch 31401/150000, Loss: 5842, Val Loss: 6023,  Learning Rate: 0.00082, Train Gradient: 44.0\n",
      "Epoch 31501/150000, Loss: 5828, Val Loss: 6009,  Learning Rate: 0.00082, Train Gradient: 43.9\n",
      "Epoch 31601/150000, Loss: 5814, Val Loss: 5994,  Learning Rate: 0.00081, Train Gradient: 43.8\n",
      "Epoch 31701/150000, Loss: 5800, Val Loss: 5980,  Learning Rate: 0.00081, Train Gradient: 43.7\n",
      "Epoch 31801/150000, Loss: 5786, Val Loss: 5966,  Learning Rate: 0.00081, Train Gradient: 43.7\n",
      "Epoch 31901/150000, Loss: 5772, Val Loss: 5952,  Learning Rate: 0.00081, Train Gradient: 43.6\n",
      "Epoch 32001/150000, Loss: 5758, Val Loss: 5938,  Learning Rate: 0.00081, Train Gradient: 43.5\n",
      "Epoch 32101/150000, Loss: 5744, Val Loss: 5924,  Learning Rate: 0.00080, Train Gradient: 43.4\n",
      "Epoch 32201/150000, Loss: 5730, Val Loss: 5911,  Learning Rate: 0.00080, Train Gradient: 43.4\n",
      "Epoch 32301/150000, Loss: 5717, Val Loss: 5897,  Learning Rate: 0.00080, Train Gradient: 43.3\n",
      "Epoch 32401/150000, Loss: 5703, Val Loss: 5883,  Learning Rate: 0.00080, Train Gradient: 43.2\n",
      "Epoch 32501/150000, Loss: 5690, Val Loss: 5869,  Learning Rate: 0.00080, Train Gradient: 43.1\n",
      "Epoch 32601/150000, Loss: 5676, Val Loss: 5856,  Learning Rate: 0.00080, Train Gradient: 43.1\n",
      "Epoch 32701/150000, Loss: 5663, Val Loss: 5842,  Learning Rate: 0.00079, Train Gradient: 43.0\n",
      "Epoch 32801/150000, Loss: 5649, Val Loss: 5829,  Learning Rate: 0.00079, Train Gradient: 42.9\n",
      "Epoch 32901/150000, Loss: 5636, Val Loss: 5815,  Learning Rate: 0.00079, Train Gradient: 42.8\n",
      "Epoch 33001/150000, Loss: 5623, Val Loss: 5802,  Learning Rate: 0.00079, Train Gradient: 42.8\n",
      "Epoch 33101/150000, Loss: 5609, Val Loss: 5788,  Learning Rate: 0.00079, Train Gradient: 42.7\n",
      "Epoch 33201/150000, Loss: 5596, Val Loss: 5775,  Learning Rate: 0.00079, Train Gradient: 42.6\n",
      "Epoch 33301/150000, Loss: 5583, Val Loss: 5762,  Learning Rate: 0.00078, Train Gradient: 42.5\n",
      "Epoch 33401/150000, Loss: 5570, Val Loss: 5749,  Learning Rate: 0.00078, Train Gradient: 42.4\n",
      "Epoch 33501/150000, Loss: 5557, Val Loss: 5736,  Learning Rate: 0.00078, Train Gradient: 42.4\n",
      "Epoch 33601/150000, Loss: 5544, Val Loss: 5723,  Learning Rate: 0.00078, Train Gradient: 42.3\n",
      "Epoch 33701/150000, Loss: 5531, Val Loss: 5710,  Learning Rate: 0.00078, Train Gradient: 42.3\n",
      "Epoch 33801/150000, Loss: 5518, Val Loss: 5697,  Learning Rate: 0.00077, Train Gradient: 42.2\n",
      "Epoch 33901/150000, Loss: 5505, Val Loss: 5684,  Learning Rate: 0.00077, Train Gradient: 42.1\n",
      "Epoch 34001/150000, Loss: 5493, Val Loss: 5671,  Learning Rate: 0.00077, Train Gradient: 42.0\n",
      "Epoch 34101/150000, Loss: 5480, Val Loss: 5658,  Learning Rate: 0.00077, Train Gradient: 42.0\n",
      "Epoch 34201/150000, Loss: 5467, Val Loss: 5645,  Learning Rate: 0.00077, Train Gradient: 41.9\n",
      "Epoch 34301/150000, Loss: 5455, Val Loss: 5632,  Learning Rate: 0.00077, Train Gradient: 41.8\n",
      "Epoch 34401/150000, Loss: 5442, Val Loss: 5620,  Learning Rate: 0.00076, Train Gradient: 41.8\n",
      "Epoch 34501/150000, Loss: 5430, Val Loss: 5607,  Learning Rate: 0.00076, Train Gradient: 41.7\n",
      "Epoch 34601/150000, Loss: 5417, Val Loss: 5595,  Learning Rate: 0.00076, Train Gradient: 41.6\n",
      "Epoch 34701/150000, Loss: 5405, Val Loss: 5582,  Learning Rate: 0.00076, Train Gradient: 41.5\n",
      "Epoch 34801/150000, Loss: 5392, Val Loss: 5570,  Learning Rate: 0.00076, Train Gradient: 41.5\n",
      "Epoch 34901/150000, Loss: 5380, Val Loss: 5557,  Learning Rate: 0.00076, Train Gradient: 41.4\n",
      "Epoch 35001/150000, Loss: 5368, Val Loss: 5545,  Learning Rate: 0.00076, Train Gradient: 41.3\n",
      "Epoch 35101/150000, Loss: 5355, Val Loss: 5532,  Learning Rate: 0.00075, Train Gradient: 41.3\n",
      "Epoch 35201/150000, Loss: 5343, Val Loss: 5520,  Learning Rate: 0.00075, Train Gradient: 41.2\n",
      "Epoch 35301/150000, Loss: 5331, Val Loss: 5508,  Learning Rate: 0.00075, Train Gradient: 41.1\n",
      "Epoch 35401/150000, Loss: 5319, Val Loss: 5496,  Learning Rate: 0.00075, Train Gradient: 41.1\n",
      "Epoch 35501/150000, Loss: 5307, Val Loss: 5483,  Learning Rate: 0.00075, Train Gradient: 41.0\n",
      "Epoch 35601/150000, Loss: 5295, Val Loss: 5471,  Learning Rate: 0.00075, Train Gradient: 40.9\n",
      "Epoch 35701/150000, Loss: 5283, Val Loss: 5459,  Learning Rate: 0.00074, Train Gradient: 40.9\n",
      "Epoch 35801/150000, Loss: 5271, Val Loss: 5447,  Learning Rate: 0.00074, Train Gradient: 40.8\n",
      "Epoch 35901/150000, Loss: 5259, Val Loss: 5435,  Learning Rate: 0.00074, Train Gradient: 40.7\n",
      "Epoch 36001/150000, Loss: 5247, Val Loss: 5423,  Learning Rate: 0.00074, Train Gradient: 40.6\n",
      "Epoch 36101/150000, Loss: 5235, Val Loss: 5411,  Learning Rate: 0.00074, Train Gradient: 40.6\n",
      "Epoch 36201/150000, Loss: 5224, Val Loss: 5399,  Learning Rate: 0.00074, Train Gradient: 40.5\n",
      "Epoch 36301/150000, Loss: 5212, Val Loss: 5387,  Learning Rate: 0.00074, Train Gradient: 40.4\n",
      "Epoch 36401/150000, Loss: 5200, Val Loss: 5376,  Learning Rate: 0.00073, Train Gradient: 40.4\n",
      "Epoch 36501/150000, Loss: 5188, Val Loss: 5364,  Learning Rate: 0.00073, Train Gradient: 40.3\n",
      "Epoch 36601/150000, Loss: 5177, Val Loss: 5352,  Learning Rate: 0.00073, Train Gradient: 40.2\n",
      "Epoch 36701/150000, Loss: 5165, Val Loss: 5340,  Learning Rate: 0.00073, Train Gradient: 40.2\n",
      "Epoch 36801/150000, Loss: 5154, Val Loss: 5329,  Learning Rate: 0.00073, Train Gradient: 40.1\n",
      "Epoch 36901/150000, Loss: 5142, Val Loss: 5317,  Learning Rate: 0.00073, Train Gradient: 40.0\n",
      "Epoch 37001/150000, Loss: 5131, Val Loss: 5305,  Learning Rate: 0.00073, Train Gradient: 40.0\n",
      "Epoch 37101/150000, Loss: 5120, Val Loss: 5294,  Learning Rate: 0.00072, Train Gradient: 39.9\n",
      "Epoch 37201/150000, Loss: 5108, Val Loss: 5282,  Learning Rate: 0.00072, Train Gradient: 39.8\n",
      "Epoch 37301/150000, Loss: 5097, Val Loss: 5271,  Learning Rate: 0.00072, Train Gradient: 39.8\n",
      "Epoch 37401/150000, Loss: 5086, Val Loss: 5259,  Learning Rate: 0.00072, Train Gradient: 39.7\n",
      "Epoch 37501/150000, Loss: 5074, Val Loss: 5248,  Learning Rate: 0.00072, Train Gradient: 39.7\n",
      "Epoch 37601/150000, Loss: 5063, Val Loss: 5236,  Learning Rate: 0.00072, Train Gradient: 39.6\n",
      "Epoch 37701/150000, Loss: 5052, Val Loss: 5225,  Learning Rate: 0.00072, Train Gradient: 39.5\n",
      "Epoch 37801/150000, Loss: 5041, Val Loss: 5214,  Learning Rate: 0.00071, Train Gradient: 39.5\n",
      "Epoch 37901/150000, Loss: 5030, Val Loss: 5203,  Learning Rate: 0.00071, Train Gradient: 39.4\n",
      "Epoch 38001/150000, Loss: 5019, Val Loss: 5192,  Learning Rate: 0.00071, Train Gradient: 39.4\n",
      "Epoch 38101/150000, Loss: 5008, Val Loss: 5181,  Learning Rate: 0.00071, Train Gradient: 39.3\n",
      "Epoch 38201/150000, Loss: 4997, Val Loss: 5169,  Learning Rate: 0.00071, Train Gradient: 39.2\n",
      "Epoch 38301/150000, Loss: 4986, Val Loss: 5158,  Learning Rate: 0.00071, Train Gradient: 39.2\n",
      "Epoch 38401/150000, Loss: 4975, Val Loss: 5148,  Learning Rate: 0.00071, Train Gradient: 39.1\n",
      "Epoch 38501/150000, Loss: 4964, Val Loss: 5137,  Learning Rate: 0.00071, Train Gradient: 39.0\n",
      "Epoch 38601/150000, Loss: 4953, Val Loss: 5126,  Learning Rate: 0.00070, Train Gradient: 39.0\n",
      "Epoch 38701/150000, Loss: 4942, Val Loss: 5115,  Learning Rate: 0.00070, Train Gradient: 38.9\n",
      "Epoch 38801/150000, Loss: 4932, Val Loss: 5104,  Learning Rate: 0.00070, Train Gradient: 38.9\n",
      "Epoch 38901/150000, Loss: 4921, Val Loss: 5093,  Learning Rate: 0.00070, Train Gradient: 38.8\n",
      "Epoch 39001/150000, Loss: 4910, Val Loss: 5082,  Learning Rate: 0.00070, Train Gradient: 38.7\n",
      "Epoch 39101/150000, Loss: 4900, Val Loss: 5071,  Learning Rate: 0.00070, Train Gradient: 38.7\n",
      "Epoch 39201/150000, Loss: 4889, Val Loss: 5060,  Learning Rate: 0.00070, Train Gradient: 38.6\n",
      "Epoch 39301/150000, Loss: 4878, Val Loss: 5049,  Learning Rate: 0.00070, Train Gradient: 38.6\n",
      "Epoch 39401/150000, Loss: 4868, Val Loss: 5038,  Learning Rate: 0.00069, Train Gradient: 38.5\n",
      "Epoch 39501/150000, Loss: 4857, Val Loss: 5027,  Learning Rate: 0.00069, Train Gradient: 38.5\n",
      "Epoch 39601/150000, Loss: 4847, Val Loss: 5017,  Learning Rate: 0.00069, Train Gradient: 38.4\n",
      "Epoch 39701/150000, Loss: 4836, Val Loss: 5006,  Learning Rate: 0.00069, Train Gradient: 38.3\n",
      "Epoch 39801/150000, Loss: 4826, Val Loss: 4996,  Learning Rate: 0.00069, Train Gradient: 38.3\n",
      "Epoch 39901/150000, Loss: 4816, Val Loss: 4985,  Learning Rate: 0.00069, Train Gradient: 38.2\n",
      "Epoch 40001/150000, Loss: 4805, Val Loss: 4975,  Learning Rate: 0.00069, Train Gradient: 38.2\n",
      "Epoch 40101/150000, Loss: 4795, Val Loss: 4964,  Learning Rate: 0.00069, Train Gradient: 38.1\n",
      "Epoch 40201/150000, Loss: 4785, Val Loss: 4954,  Learning Rate: 0.00068, Train Gradient: 38.0\n",
      "Epoch 40301/150000, Loss: 4774, Val Loss: 4944,  Learning Rate: 0.00068, Train Gradient: 38.0\n",
      "Epoch 40401/150000, Loss: 4764, Val Loss: 4934,  Learning Rate: 0.00068, Train Gradient: 37.9\n",
      "Epoch 40501/150000, Loss: 4754, Val Loss: 4923,  Learning Rate: 0.00068, Train Gradient: 37.9\n",
      "Epoch 40601/150000, Loss: 4744, Val Loss: 4913,  Learning Rate: 0.00068, Train Gradient: 37.8\n",
      "Epoch 40701/150000, Loss: 4734, Val Loss: 4903,  Learning Rate: 0.00068, Train Gradient: 37.8\n",
      "Epoch 40801/150000, Loss: 4724, Val Loss: 4893,  Learning Rate: 0.00068, Train Gradient: 37.7\n",
      "Epoch 40901/150000, Loss: 4714, Val Loss: 4883,  Learning Rate: 0.00068, Train Gradient: 37.7\n",
      "Epoch 41001/150000, Loss: 4704, Val Loss: 4873,  Learning Rate: 0.00068, Train Gradient: 37.6\n",
      "Epoch 41101/150000, Loss: 4694, Val Loss: 4863,  Learning Rate: 0.00067, Train Gradient: 37.5\n",
      "Epoch 41201/150000, Loss: 4684, Val Loss: 4853,  Learning Rate: 0.00067, Train Gradient: 37.5\n",
      "Epoch 41301/150000, Loss: 4674, Val Loss: 4843,  Learning Rate: 0.00067, Train Gradient: 37.4\n",
      "Epoch 41401/150000, Loss: 4664, Val Loss: 4833,  Learning Rate: 0.00067, Train Gradient: 37.4\n",
      "Epoch 41501/150000, Loss: 4654, Val Loss: 4823,  Learning Rate: 0.00067, Train Gradient: 37.3\n",
      "Epoch 41601/150000, Loss: 4644, Val Loss: 4813,  Learning Rate: 0.00067, Train Gradient: 37.3\n",
      "Epoch 41701/150000, Loss: 4634, Val Loss: 4804,  Learning Rate: 0.00067, Train Gradient: 37.2\n",
      "Epoch 41801/150000, Loss: 4625, Val Loss: 4794,  Learning Rate: 0.00067, Train Gradient: 37.2\n",
      "Epoch 41901/150000, Loss: 4615, Val Loss: 4784,  Learning Rate: 0.00067, Train Gradient: 37.1\n",
      "Epoch 42001/150000, Loss: 4605, Val Loss: 4774,  Learning Rate: 0.00066, Train Gradient: 37.0\n",
      "Epoch 42101/150000, Loss: 4595, Val Loss: 4765,  Learning Rate: 0.00066, Train Gradient: 37.0\n",
      "Epoch 42201/150000, Loss: 4586, Val Loss: 4755,  Learning Rate: 0.00066, Train Gradient: 37.0\n",
      "Epoch 42301/150000, Loss: 4576, Val Loss: 4745,  Learning Rate: 0.00066, Train Gradient: 36.9\n",
      "Epoch 42401/150000, Loss: 4567, Val Loss: 4736,  Learning Rate: 0.00066, Train Gradient: 36.8\n",
      "Epoch 42501/150000, Loss: 4557, Val Loss: 4726,  Learning Rate: 0.00066, Train Gradient: 36.8\n",
      "Epoch 42601/150000, Loss: 4547, Val Loss: 4717,  Learning Rate: 0.00066, Train Gradient: 36.7\n",
      "Epoch 42701/150000, Loss: 4538, Val Loss: 4707,  Learning Rate: 0.00066, Train Gradient: 36.7\n",
      "Epoch 42801/150000, Loss: 4528, Val Loss: 4697,  Learning Rate: 0.00066, Train Gradient: 36.6\n",
      "Epoch 42901/150000, Loss: 4519, Val Loss: 4688,  Learning Rate: 0.00065, Train Gradient: 36.6\n",
      "Epoch 43001/150000, Loss: 4509, Val Loss: 4677,  Learning Rate: 0.00065, Train Gradient: 36.5\n",
      "Epoch 43101/150000, Loss: 4500, Val Loss: 4668,  Learning Rate: 0.00065, Train Gradient: 36.5\n",
      "Epoch 43201/150000, Loss: 4491, Val Loss: 4658,  Learning Rate: 0.00065, Train Gradient: 36.4\n",
      "Epoch 43301/150000, Loss: 4481, Val Loss: 4649,  Learning Rate: 0.00065, Train Gradient: 36.4\n",
      "Epoch 43401/150000, Loss: 4472, Val Loss: 4640,  Learning Rate: 0.00065, Train Gradient: 36.3\n",
      "Epoch 43501/150000, Loss: 4463, Val Loss: 4630,  Learning Rate: 0.00065, Train Gradient: 36.3\n",
      "Epoch 43601/150000, Loss: 4453, Val Loss: 4621,  Learning Rate: 0.00065, Train Gradient: 36.2\n",
      "Epoch 43701/150000, Loss: 4444, Val Loss: 4612,  Learning Rate: 0.00065, Train Gradient: 36.2\n",
      "Epoch 43801/150000, Loss: 4435, Val Loss: 4602,  Learning Rate: 0.00065, Train Gradient: 36.1\n",
      "Epoch 43901/150000, Loss: 4426, Val Loss: 4593,  Learning Rate: 0.00064, Train Gradient: 36.1\n",
      "Epoch 44001/150000, Loss: 4417, Val Loss: 4584,  Learning Rate: 0.00064, Train Gradient: 36.0\n",
      "Epoch 44101/150000, Loss: 4407, Val Loss: 4575,  Learning Rate: 0.00064, Train Gradient: 36.0\n",
      "Epoch 44201/150000, Loss: 4398, Val Loss: 4566,  Learning Rate: 0.00064, Train Gradient: 35.9\n",
      "Epoch 44301/150000, Loss: 4389, Val Loss: 4557,  Learning Rate: 0.00064, Train Gradient: 35.9\n",
      "Epoch 44401/150000, Loss: 4380, Val Loss: 4548,  Learning Rate: 0.00064, Train Gradient: 35.8\n",
      "Epoch 44501/150000, Loss: 4371, Val Loss: 4538,  Learning Rate: 0.00064, Train Gradient: 35.8\n",
      "Epoch 44601/150000, Loss: 4362, Val Loss: 4529,  Learning Rate: 0.00064, Train Gradient: 35.7\n",
      "Epoch 44701/150000, Loss: 4353, Val Loss: 4520,  Learning Rate: 0.00064, Train Gradient: 35.6\n",
      "Epoch 44801/150000, Loss: 4344, Val Loss: 4512,  Learning Rate: 0.00064, Train Gradient: 35.6\n",
      "Epoch 44901/150000, Loss: 4335, Val Loss: 4503,  Learning Rate: 0.00064, Train Gradient: 35.5\n",
      "Epoch 45001/150000, Loss: 4326, Val Loss: 4494,  Learning Rate: 0.00063, Train Gradient: 35.5\n",
      "Epoch 45101/150000, Loss: 4318, Val Loss: 4485,  Learning Rate: 0.00063, Train Gradient: 35.4\n",
      "Epoch 45201/150000, Loss: 4309, Val Loss: 4476,  Learning Rate: 0.00063, Train Gradient: 35.4\n",
      "Epoch 45301/150000, Loss: 4300, Val Loss: 4467,  Learning Rate: 0.00063, Train Gradient: 35.4\n",
      "Epoch 45401/150000, Loss: 4291, Val Loss: 4458,  Learning Rate: 0.00063, Train Gradient: 35.3\n",
      "Epoch 45501/150000, Loss: 4282, Val Loss: 4449,  Learning Rate: 0.00063, Train Gradient: 35.3\n",
      "Epoch 45601/150000, Loss: 4274, Val Loss: 4441,  Learning Rate: 0.00063, Train Gradient: 35.2\n",
      "Epoch 45701/150000, Loss: 4265, Val Loss: 4432,  Learning Rate: 0.00063, Train Gradient: 35.1\n",
      "Epoch 45801/150000, Loss: 4256, Val Loss: 4423,  Learning Rate: 0.00063, Train Gradient: 35.1\n",
      "Epoch 45901/150000, Loss: 4247, Val Loss: 4414,  Learning Rate: 0.00063, Train Gradient: 35.0\n",
      "Epoch 46001/150000, Loss: 4239, Val Loss: 4406,  Learning Rate: 0.00063, Train Gradient: 35.0\n",
      "Epoch 46101/150000, Loss: 4230, Val Loss: 4397,  Learning Rate: 0.00062, Train Gradient: 34.9\n",
      "Epoch 46201/150000, Loss: 4222, Val Loss: 4388,  Learning Rate: 0.00062, Train Gradient: 34.9\n",
      "Epoch 46301/150000, Loss: 4213, Val Loss: 4380,  Learning Rate: 0.00062, Train Gradient: 34.9\n",
      "Epoch 46401/150000, Loss: 4204, Val Loss: 4371,  Learning Rate: 0.00062, Train Gradient: 34.8\n",
      "Epoch 46501/150000, Loss: 4196, Val Loss: 4363,  Learning Rate: 0.00062, Train Gradient: 34.7\n",
      "Epoch 46601/150000, Loss: 4187, Val Loss: 4354,  Learning Rate: 0.00062, Train Gradient: 34.7\n",
      "Epoch 46701/150000, Loss: 4179, Val Loss: 4346,  Learning Rate: 0.00062, Train Gradient: 34.6\n",
      "Epoch 46801/150000, Loss: 4171, Val Loss: 4337,  Learning Rate: 0.00062, Train Gradient: 34.6\n",
      "Epoch 46901/150000, Loss: 4162, Val Loss: 4329,  Learning Rate: 0.00062, Train Gradient: 34.5\n",
      "Epoch 47001/150000, Loss: 4154, Val Loss: 4320,  Learning Rate: 0.00062, Train Gradient: 34.5\n",
      "Epoch 47101/150000, Loss: 4145, Val Loss: 4312,  Learning Rate: 0.00062, Train Gradient: 34.4\n",
      "Epoch 47201/150000, Loss: 4137, Val Loss: 4304,  Learning Rate: 0.00062, Train Gradient: 34.4\n",
      "Epoch 47301/150000, Loss: 4129, Val Loss: 4295,  Learning Rate: 0.00061, Train Gradient: 34.4\n",
      "Epoch 47401/150000, Loss: 4120, Val Loss: 4287,  Learning Rate: 0.00061, Train Gradient: 34.3\n",
      "Epoch 47501/150000, Loss: 4112, Val Loss: 4279,  Learning Rate: 0.00061, Train Gradient: 34.2\n",
      "Epoch 47601/150000, Loss: 4104, Val Loss: 4270,  Learning Rate: 0.00061, Train Gradient: 34.2\n",
      "Epoch 47701/150000, Loss: 4095, Val Loss: 4262,  Learning Rate: 0.00061, Train Gradient: 34.2\n",
      "Epoch 47801/150000, Loss: 4087, Val Loss: 4254,  Learning Rate: 0.00061, Train Gradient: 34.1\n",
      "Epoch 47901/150000, Loss: 4079, Val Loss: 4246,  Learning Rate: 0.00061, Train Gradient: 34.1\n",
      "Epoch 48001/150000, Loss: 4071, Val Loss: 4238,  Learning Rate: 0.00061, Train Gradient: 34.0\n",
      "Epoch 48101/150000, Loss: 4063, Val Loss: 4229,  Learning Rate: 0.00061, Train Gradient: 34.0\n",
      "Epoch 48201/150000, Loss: 4054, Val Loss: 4221,  Learning Rate: 0.00061, Train Gradient: 33.9\n",
      "Epoch 48301/150000, Loss: 4046, Val Loss: 4213,  Learning Rate: 0.00061, Train Gradient: 33.9\n",
      "Epoch 48401/150000, Loss: 4038, Val Loss: 4205,  Learning Rate: 0.00061, Train Gradient: 33.8\n",
      "Epoch 48501/150000, Loss: 4030, Val Loss: 4197,  Learning Rate: 0.00060, Train Gradient: 33.8\n",
      "Epoch 48601/150000, Loss: 4022, Val Loss: 4189,  Learning Rate: 0.00060, Train Gradient: 33.7\n",
      "Epoch 48701/150000, Loss: 4014, Val Loss: 4181,  Learning Rate: 0.00060, Train Gradient: 33.7\n",
      "Epoch 48801/150000, Loss: 4006, Val Loss: 4173,  Learning Rate: 0.00060, Train Gradient: 33.6\n",
      "Epoch 48901/150000, Loss: 3998, Val Loss: 4165,  Learning Rate: 0.00060, Train Gradient: 33.6\n",
      "Epoch 49001/150000, Loss: 3990, Val Loss: 4157,  Learning Rate: 0.00060, Train Gradient: 33.5\n",
      "Epoch 49101/150000, Loss: 3982, Val Loss: 4149,  Learning Rate: 0.00060, Train Gradient: 33.5\n",
      "Epoch 49201/150000, Loss: 3974, Val Loss: 4141,  Learning Rate: 0.00060, Train Gradient: 33.4\n",
      "Epoch 49301/150000, Loss: 3967, Val Loss: 4133,  Learning Rate: 0.00060, Train Gradient: 33.4\n",
      "Epoch 49401/150000, Loss: 3959, Val Loss: 4125,  Learning Rate: 0.00060, Train Gradient: 33.3\n",
      "Epoch 49501/150000, Loss: 3951, Val Loss: 4117,  Learning Rate: 0.00060, Train Gradient: 33.3\n",
      "Epoch 49601/150000, Loss: 3943, Val Loss: 4110,  Learning Rate: 0.00060, Train Gradient: 33.2\n",
      "Epoch 49701/150000, Loss: 3935, Val Loss: 4102,  Learning Rate: 0.00060, Train Gradient: 33.2\n",
      "Epoch 49801/150000, Loss: 3927, Val Loss: 4094,  Learning Rate: 0.00059, Train Gradient: 33.2\n",
      "Epoch 49901/150000, Loss: 3920, Val Loss: 4086,  Learning Rate: 0.00059, Train Gradient: 33.1\n",
      "Epoch 50001/150000, Loss: 3912, Val Loss: 4078,  Learning Rate: 0.00059, Train Gradient: 33.1\n",
      "Epoch 50101/150000, Loss: 3904, Val Loss: 4071,  Learning Rate: 0.00059, Train Gradient: 33.0\n",
      "Epoch 50201/150000, Loss: 3896, Val Loss: 4063,  Learning Rate: 0.00059, Train Gradient: 33.0\n",
      "Epoch 50301/150000, Loss: 3889, Val Loss: 4055,  Learning Rate: 0.00059, Train Gradient: 32.9\n",
      "Epoch 50401/150000, Loss: 3881, Val Loss: 4048,  Learning Rate: 0.00059, Train Gradient: 32.9\n",
      "Epoch 50501/150000, Loss: 3873, Val Loss: 4040,  Learning Rate: 0.00059, Train Gradient: 32.8\n",
      "Epoch 50601/150000, Loss: 3866, Val Loss: 4032,  Learning Rate: 0.00059, Train Gradient: 32.8\n",
      "Epoch 50701/150000, Loss: 3858, Val Loss: 4025,  Learning Rate: 0.00059, Train Gradient: 32.7\n",
      "Epoch 50801/150000, Loss: 3851, Val Loss: 4017,  Learning Rate: 0.00059, Train Gradient: 32.7\n",
      "Epoch 50901/150000, Loss: 3843, Val Loss: 4010,  Learning Rate: 0.00059, Train Gradient: 32.7\n",
      "Epoch 51001/150000, Loss: 3836, Val Loss: 4002,  Learning Rate: 0.00059, Train Gradient: 32.6\n",
      "Epoch 51101/150000, Loss: 3828, Val Loss: 3995,  Learning Rate: 0.00059, Train Gradient: 32.5\n",
      "Epoch 51201/150000, Loss: 3820, Val Loss: 3987,  Learning Rate: 0.00058, Train Gradient: 32.5\n",
      "Epoch 51301/150000, Loss: 3813, Val Loss: 3980,  Learning Rate: 0.00058, Train Gradient: 32.5\n",
      "Epoch 51401/150000, Loss: 3805, Val Loss: 3972,  Learning Rate: 0.00058, Train Gradient: 32.4\n",
      "Epoch 51501/150000, Loss: 3798, Val Loss: 3965,  Learning Rate: 0.00058, Train Gradient: 32.4\n",
      "Epoch 51601/150000, Loss: 3791, Val Loss: 3957,  Learning Rate: 0.00058, Train Gradient: 32.3\n",
      "Epoch 51701/150000, Loss: 3783, Val Loss: 3950,  Learning Rate: 0.00058, Train Gradient: 32.3\n",
      "Epoch 51801/150000, Loss: 3776, Val Loss: 3943,  Learning Rate: 0.00058, Train Gradient: 32.2\n",
      "Epoch 51901/150000, Loss: 3768, Val Loss: 3935,  Learning Rate: 0.00058, Train Gradient: 32.2\n",
      "Epoch 52001/150000, Loss: 3761, Val Loss: 3928,  Learning Rate: 0.00058, Train Gradient: 32.1\n",
      "Epoch 52101/150000, Loss: 3754, Val Loss: 3920,  Learning Rate: 0.00058, Train Gradient: 32.1\n",
      "Epoch 52201/150000, Loss: 3746, Val Loss: 3913,  Learning Rate: 0.00058, Train Gradient: 32.0\n",
      "Epoch 52301/150000, Loss: 3739, Val Loss: 3906,  Learning Rate: 0.00058, Train Gradient: 32.0\n",
      "Epoch 52401/150000, Loss: 3732, Val Loss: 3899,  Learning Rate: 0.00058, Train Gradient: 32.0\n",
      "Epoch 52501/150000, Loss: 3725, Val Loss: 3891,  Learning Rate: 0.00058, Train Gradient: 31.9\n",
      "Epoch 52601/150000, Loss: 3717, Val Loss: 3884,  Learning Rate: 0.00058, Train Gradient: 31.9\n",
      "Epoch 52701/150000, Loss: 3710, Val Loss: 3877,  Learning Rate: 0.00057, Train Gradient: 31.8\n",
      "Epoch 52801/150000, Loss: 3703, Val Loss: 3870,  Learning Rate: 0.00057, Train Gradient: 31.8\n",
      "Epoch 52901/150000, Loss: 3696, Val Loss: 3863,  Learning Rate: 0.00057, Train Gradient: 31.7\n",
      "Epoch 53001/150000, Loss: 3689, Val Loss: 3855,  Learning Rate: 0.00057, Train Gradient: 31.7\n",
      "Epoch 53101/150000, Loss: 3681, Val Loss: 3848,  Learning Rate: 0.00057, Train Gradient: 31.6\n",
      "Epoch 53201/150000, Loss: 3674, Val Loss: 3841,  Learning Rate: 0.00057, Train Gradient: 31.6\n",
      "Epoch 53301/150000, Loss: 3667, Val Loss: 3834,  Learning Rate: 0.00057, Train Gradient: 31.6\n",
      "Epoch 53401/150000, Loss: 3660, Val Loss: 3827,  Learning Rate: 0.00057, Train Gradient: 31.5\n",
      "Epoch 53501/150000, Loss: 3653, Val Loss: 3820,  Learning Rate: 0.00057, Train Gradient: 31.5\n",
      "Epoch 53601/150000, Loss: 3646, Val Loss: 3813,  Learning Rate: 0.00057, Train Gradient: 31.4\n",
      "Epoch 53701/150000, Loss: 3639, Val Loss: 3806,  Learning Rate: 0.00057, Train Gradient: 31.4\n",
      "Epoch 53801/150000, Loss: 3632, Val Loss: 3799,  Learning Rate: 0.00057, Train Gradient: 31.3\n",
      "Epoch 53901/150000, Loss: 3625, Val Loss: 3792,  Learning Rate: 0.00057, Train Gradient: 31.3\n",
      "Epoch 54001/150000, Loss: 3618, Val Loss: 3785,  Learning Rate: 0.00057, Train Gradient: 31.3\n",
      "Epoch 54101/150000, Loss: 3611, Val Loss: 3778,  Learning Rate: 0.00057, Train Gradient: 31.2\n",
      "Epoch 54201/150000, Loss: 3604, Val Loss: 3771,  Learning Rate: 0.00057, Train Gradient: 31.2\n",
      "Epoch 54301/150000, Loss: 3597, Val Loss: 3764,  Learning Rate: 0.00056, Train Gradient: 31.1\n",
      "Epoch 54401/150000, Loss: 3590, Val Loss: 3757,  Learning Rate: 0.00056, Train Gradient: 31.1\n",
      "Epoch 54501/150000, Loss: 3583, Val Loss: 3750,  Learning Rate: 0.00056, Train Gradient: 31.0\n",
      "Epoch 54601/150000, Loss: 3576, Val Loss: 3743,  Learning Rate: 0.00056, Train Gradient: 31.0\n",
      "Epoch 54701/150000, Loss: 3569, Val Loss: 3737,  Learning Rate: 0.00056, Train Gradient: 31.0\n",
      "Epoch 54801/150000, Loss: 3562, Val Loss: 3730,  Learning Rate: 0.00056, Train Gradient: 30.9\n",
      "Epoch 54901/150000, Loss: 3556, Val Loss: 3723,  Learning Rate: 0.00056, Train Gradient: 30.9\n",
      "Epoch 55001/150000, Loss: 3549, Val Loss: 3716,  Learning Rate: 0.00056, Train Gradient: 30.8\n",
      "Epoch 55101/150000, Loss: 3542, Val Loss: 3709,  Learning Rate: 0.00056, Train Gradient: 30.8\n",
      "Epoch 55201/150000, Loss: 3535, Val Loss: 3703,  Learning Rate: 0.00056, Train Gradient: 30.7\n",
      "Epoch 55301/150000, Loss: 3528, Val Loss: 3696,  Learning Rate: 0.00056, Train Gradient: 30.7\n",
      "Epoch 55401/150000, Loss: 3522, Val Loss: 3689,  Learning Rate: 0.00056, Train Gradient: 30.7\n",
      "Epoch 55501/150000, Loss: 3515, Val Loss: 3682,  Learning Rate: 0.00056, Train Gradient: 30.6\n",
      "Epoch 55601/150000, Loss: 3508, Val Loss: 3676,  Learning Rate: 0.00056, Train Gradient: 30.6\n",
      "Epoch 55701/150000, Loss: 3501, Val Loss: 3669,  Learning Rate: 0.00056, Train Gradient: 30.5\n",
      "Epoch 55801/150000, Loss: 3495, Val Loss: 3662,  Learning Rate: 0.00056, Train Gradient: 30.5\n",
      "Epoch 55901/150000, Loss: 3488, Val Loss: 3656,  Learning Rate: 0.00056, Train Gradient: 30.4\n",
      "Epoch 56001/150000, Loss: 3481, Val Loss: 3649,  Learning Rate: 0.00056, Train Gradient: 30.4\n",
      "Epoch 56101/150000, Loss: 3475, Val Loss: 3642,  Learning Rate: 0.00055, Train Gradient: 30.4\n",
      "Epoch 56201/150000, Loss: 3468, Val Loss: 3636,  Learning Rate: 0.00055, Train Gradient: 30.3\n",
      "Epoch 56301/150000, Loss: 3461, Val Loss: 3629,  Learning Rate: 0.00055, Train Gradient: 30.3\n",
      "Epoch 56401/150000, Loss: 3455, Val Loss: 3622,  Learning Rate: 0.00055, Train Gradient: 30.2\n",
      "Epoch 56501/150000, Loss: 3448, Val Loss: 3616,  Learning Rate: 0.00055, Train Gradient: 30.2\n",
      "Epoch 56601/150000, Loss: 3442, Val Loss: 3609,  Learning Rate: 0.00055, Train Gradient: 30.1\n",
      "Epoch 56701/150000, Loss: 3435, Val Loss: 3603,  Learning Rate: 0.00055, Train Gradient: 30.1\n",
      "Epoch 56801/150000, Loss: 3429, Val Loss: 3596,  Learning Rate: 0.00055, Train Gradient: 30.1\n",
      "Epoch 56901/150000, Loss: 3422, Val Loss: 3590,  Learning Rate: 0.00055, Train Gradient: 30.0\n",
      "Epoch 57001/150000, Loss: 3416, Val Loss: 3583,  Learning Rate: 0.00055, Train Gradient: 30.0\n",
      "Epoch 57101/150000, Loss: 3409, Val Loss: 3577,  Learning Rate: 0.00055, Train Gradient: 29.9\n",
      "Epoch 57201/150000, Loss: 3403, Val Loss: 3570,  Learning Rate: 0.00055, Train Gradient: 29.9\n",
      "Epoch 57301/150000, Loss: 3396, Val Loss: 3564,  Learning Rate: 0.00055, Train Gradient: 29.8\n",
      "Epoch 57401/150000, Loss: 3390, Val Loss: 3557,  Learning Rate: 0.00055, Train Gradient: 29.8\n",
      "Epoch 57501/150000, Loss: 3383, Val Loss: 3551,  Learning Rate: 0.00055, Train Gradient: 29.8\n",
      "Epoch 57601/150000, Loss: 3377, Val Loss: 3545,  Learning Rate: 0.00055, Train Gradient: 29.7\n",
      "Epoch 57701/150000, Loss: 3370, Val Loss: 3538,  Learning Rate: 0.00055, Train Gradient: 29.7\n",
      "Epoch 57801/150000, Loss: 3364, Val Loss: 3532,  Learning Rate: 0.00055, Train Gradient: 29.6\n",
      "Epoch 57901/150000, Loss: 3358, Val Loss: 3525,  Learning Rate: 0.00054, Train Gradient: 29.6\n",
      "Epoch 58001/150000, Loss: 3351, Val Loss: 3519,  Learning Rate: 0.00054, Train Gradient: 29.6\n",
      "Epoch 58101/150000, Loss: 3345, Val Loss: 3513,  Learning Rate: 0.00054, Train Gradient: 29.5\n",
      "Epoch 58201/150000, Loss: 3339, Val Loss: 3507,  Learning Rate: 0.00054, Train Gradient: 29.5\n",
      "Epoch 58301/150000, Loss: 3332, Val Loss: 3500,  Learning Rate: 0.00054, Train Gradient: 29.4\n",
      "Epoch 58401/150000, Loss: 3326, Val Loss: 3494,  Learning Rate: 0.00054, Train Gradient: 29.4\n",
      "Epoch 58501/150000, Loss: 3320, Val Loss: 3488,  Learning Rate: 0.00054, Train Gradient: 29.4\n",
      "Epoch 58601/150000, Loss: 3314, Val Loss: 3481,  Learning Rate: 0.00054, Train Gradient: 29.3\n",
      "Epoch 58701/150000, Loss: 3307, Val Loss: 3475,  Learning Rate: 0.00054, Train Gradient: 29.3\n",
      "Epoch 58801/150000, Loss: 3301, Val Loss: 3469,  Learning Rate: 0.00054, Train Gradient: 29.2\n",
      "Epoch 58901/150000, Loss: 3295, Val Loss: 3463,  Learning Rate: 0.00054, Train Gradient: 29.2\n",
      "Epoch 59001/150000, Loss: 3289, Val Loss: 3456,  Learning Rate: 0.00054, Train Gradient: 29.2\n",
      "Epoch 59101/150000, Loss: 3282, Val Loss: 3450,  Learning Rate: 0.00054, Train Gradient: 29.1\n",
      "Epoch 59201/150000, Loss: 3276, Val Loss: 3444,  Learning Rate: 0.00054, Train Gradient: 29.1\n",
      "Epoch 59301/150000, Loss: 3270, Val Loss: 3438,  Learning Rate: 0.00054, Train Gradient: 29.0\n",
      "Epoch 59401/150000, Loss: 3264, Val Loss: 3432,  Learning Rate: 0.00054, Train Gradient: 29.0\n",
      "Epoch 59501/150000, Loss: 3258, Val Loss: 3425,  Learning Rate: 0.00054, Train Gradient: 29.0\n",
      "Epoch 59601/150000, Loss: 3252, Val Loss: 3419,  Learning Rate: 0.00054, Train Gradient: 28.9\n",
      "Epoch 59701/150000, Loss: 3246, Val Loss: 3413,  Learning Rate: 0.00054, Train Gradient: 28.9\n",
      "Epoch 59801/150000, Loss: 3240, Val Loss: 3407,  Learning Rate: 0.00054, Train Gradient: 28.8\n",
      "Epoch 59901/150000, Loss: 3234, Val Loss: 3401,  Learning Rate: 0.00053, Train Gradient: 28.8\n",
      "Epoch 60001/150000, Loss: 3227, Val Loss: 3395,  Learning Rate: 0.00053, Train Gradient: 28.8\n",
      "Epoch 60101/150000, Loss: 3221, Val Loss: 3389,  Learning Rate: 0.00053, Train Gradient: 28.7\n",
      "Epoch 60201/150000, Loss: 3215, Val Loss: 3383,  Learning Rate: 0.00053, Train Gradient: 28.7\n",
      "Epoch 60301/150000, Loss: 3209, Val Loss: 3377,  Learning Rate: 0.00053, Train Gradient: 28.6\n",
      "Epoch 60401/150000, Loss: 3203, Val Loss: 3371,  Learning Rate: 0.00053, Train Gradient: 28.6\n",
      "Epoch 60501/150000, Loss: 3197, Val Loss: 3365,  Learning Rate: 0.00053, Train Gradient: 28.5\n",
      "Epoch 60601/150000, Loss: 3191, Val Loss: 3359,  Learning Rate: 0.00053, Train Gradient: 28.5\n",
      "Epoch 60701/150000, Loss: 3185, Val Loss: 3353,  Learning Rate: 0.00053, Train Gradient: 28.5\n",
      "Epoch 60801/150000, Loss: 3179, Val Loss: 3347,  Learning Rate: 0.00053, Train Gradient: 28.4\n",
      "Epoch 60901/150000, Loss: 3173, Val Loss: 3341,  Learning Rate: 0.00053, Train Gradient: 28.4\n",
      "Epoch 61001/150000, Loss: 3167, Val Loss: 3335,  Learning Rate: 0.00053, Train Gradient: 28.3\n",
      "Epoch 61101/150000, Loss: 3162, Val Loss: 3329,  Learning Rate: 0.00053, Train Gradient: 28.3\n",
      "Epoch 61201/150000, Loss: 3156, Val Loss: 3323,  Learning Rate: 0.00053, Train Gradient: 28.3\n",
      "Epoch 61301/150000, Loss: 3150, Val Loss: 3317,  Learning Rate: 0.00053, Train Gradient: 28.2\n",
      "Epoch 61401/150000, Loss: 3144, Val Loss: 3311,  Learning Rate: 0.00053, Train Gradient: 28.2\n",
      "Epoch 61501/150000, Loss: 3138, Val Loss: 3306,  Learning Rate: 0.00053, Train Gradient: 28.1\n",
      "Epoch 61601/150000, Loss: 3132, Val Loss: 3300,  Learning Rate: 0.00053, Train Gradient: 28.1\n",
      "Epoch 61701/150000, Loss: 3126, Val Loss: 3294,  Learning Rate: 0.00053, Train Gradient: 28.1\n",
      "Epoch 61801/150000, Loss: 3121, Val Loss: 3288,  Learning Rate: 0.00053, Train Gradient: 28.0\n",
      "Epoch 61901/150000, Loss: 3115, Val Loss: 3282,  Learning Rate: 0.00053, Train Gradient: 28.0\n",
      "Epoch 62001/150000, Loss: 3109, Val Loss: 3277,  Learning Rate: 0.00053, Train Gradient: 28.0\n",
      "Epoch 62101/150000, Loss: 3103, Val Loss: 3271,  Learning Rate: 0.00052, Train Gradient: 27.9\n",
      "Epoch 62201/150000, Loss: 3097, Val Loss: 3265,  Learning Rate: 0.00052, Train Gradient: 27.9\n",
      "Epoch 62301/150000, Loss: 3092, Val Loss: 3259,  Learning Rate: 0.00052, Train Gradient: 27.8\n",
      "Epoch 62401/150000, Loss: 3086, Val Loss: 3253,  Learning Rate: 0.00052, Train Gradient: 27.8\n",
      "Epoch 62501/150000, Loss: 3080, Val Loss: 3248,  Learning Rate: 0.00052, Train Gradient: 27.8\n",
      "Epoch 62601/150000, Loss: 3074, Val Loss: 3242,  Learning Rate: 0.00052, Train Gradient: 27.7\n",
      "Epoch 62701/150000, Loss: 3069, Val Loss: 3236,  Learning Rate: 0.00052, Train Gradient: 27.7\n",
      "Epoch 62801/150000, Loss: 3063, Val Loss: 3231,  Learning Rate: 0.00052, Train Gradient: 27.7\n",
      "Epoch 62901/150000, Loss: 3057, Val Loss: 3225,  Learning Rate: 0.00052, Train Gradient: 27.6\n",
      "Epoch 63001/150000, Loss: 3052, Val Loss: 3219,  Learning Rate: 0.00052, Train Gradient: 27.6\n",
      "Epoch 63101/150000, Loss: 3046, Val Loss: 3214,  Learning Rate: 0.00052, Train Gradient: 27.5\n",
      "Epoch 63201/150000, Loss: 3040, Val Loss: 3208,  Learning Rate: 0.00052, Train Gradient: 27.5\n",
      "Epoch 63301/150000, Loss: 3035, Val Loss: 3203,  Learning Rate: 0.00052, Train Gradient: 27.5\n",
      "Epoch 63401/150000, Loss: 3029, Val Loss: 3197,  Learning Rate: 0.00052, Train Gradient: 27.4\n",
      "Epoch 63501/150000, Loss: 3023, Val Loss: 3192,  Learning Rate: 0.00052, Train Gradient: 27.4\n",
      "Epoch 63601/150000, Loss: 3018, Val Loss: 3186,  Learning Rate: 0.00052, Train Gradient: 27.4\n",
      "Epoch 63701/150000, Loss: 3012, Val Loss: 3181,  Learning Rate: 0.00052, Train Gradient: 27.3\n",
      "Epoch 63801/150000, Loss: 3007, Val Loss: 3175,  Learning Rate: 0.00052, Train Gradient: 27.3\n",
      "Epoch 63901/150000, Loss: 3001, Val Loss: 3170,  Learning Rate: 0.00052, Train Gradient: 27.3\n",
      "Epoch 64001/150000, Loss: 2995, Val Loss: 3164,  Learning Rate: 0.00052, Train Gradient: 27.2\n",
      "Epoch 64101/150000, Loss: 2990, Val Loss: 3159,  Learning Rate: 0.00052, Train Gradient: 27.2\n",
      "Epoch 64201/150000, Loss: 2984, Val Loss: 3153,  Learning Rate: 0.00052, Train Gradient: 27.1\n",
      "Epoch 64301/150000, Loss: 2979, Val Loss: 3148,  Learning Rate: 0.00052, Train Gradient: 27.1\n",
      "Epoch 64401/150000, Loss: 2973, Val Loss: 3142,  Learning Rate: 0.00052, Train Gradient: 27.1\n",
      "Epoch 64501/150000, Loss: 2968, Val Loss: 3137,  Learning Rate: 0.00051, Train Gradient: 27.0\n",
      "Epoch 64601/150000, Loss: 2962, Val Loss: 3131,  Learning Rate: 0.00051, Train Gradient: 27.0\n",
      "Epoch 64701/150000, Loss: 2957, Val Loss: 3126,  Learning Rate: 0.00051, Train Gradient: 27.0\n",
      "Epoch 64801/150000, Loss: 2951, Val Loss: 3121,  Learning Rate: 0.00051, Train Gradient: 26.9\n",
      "Epoch 64901/150000, Loss: 2946, Val Loss: 3115,  Learning Rate: 0.00051, Train Gradient: 26.9\n",
      "Epoch 65001/150000, Loss: 2940, Val Loss: 3110,  Learning Rate: 0.00051, Train Gradient: 26.9\n",
      "Epoch 65101/150000, Loss: 2935, Val Loss: 3104,  Learning Rate: 0.00051, Train Gradient: 26.8\n",
      "Epoch 65201/150000, Loss: 2930, Val Loss: 3099,  Learning Rate: 0.00051, Train Gradient: 26.8\n",
      "Epoch 65301/150000, Loss: 2924, Val Loss: 3094,  Learning Rate: 0.00051, Train Gradient: 26.8\n",
      "Epoch 65401/150000, Loss: 2919, Val Loss: 3088,  Learning Rate: 0.00051, Train Gradient: 26.7\n",
      "Epoch 65501/150000, Loss: 2914, Val Loss: 3083,  Learning Rate: 0.00051, Train Gradient: 26.7\n",
      "Epoch 65601/150000, Loss: 2908, Val Loss: 3078,  Learning Rate: 0.00051, Train Gradient: 26.6\n",
      "Epoch 65701/150000, Loss: 2903, Val Loss: 3072,  Learning Rate: 0.00051, Train Gradient: 26.6\n",
      "Epoch 65801/150000, Loss: 2897, Val Loss: 3067,  Learning Rate: 0.00051, Train Gradient: 26.6\n",
      "Epoch 65901/150000, Loss: 2892, Val Loss: 3062,  Learning Rate: 0.00051, Train Gradient: 26.5\n",
      "Epoch 66001/150000, Loss: 2887, Val Loss: 3056,  Learning Rate: 0.00051, Train Gradient: 26.5\n",
      "Epoch 66101/150000, Loss: 2881, Val Loss: 3051,  Learning Rate: 0.00051, Train Gradient: 26.5\n",
      "Epoch 66201/150000, Loss: 2876, Val Loss: 3046,  Learning Rate: 0.00051, Train Gradient: 26.4\n",
      "Epoch 66301/150000, Loss: 2871, Val Loss: 3040,  Learning Rate: 0.00051, Train Gradient: 26.4\n",
      "Epoch 66401/150000, Loss: 2865, Val Loss: 3035,  Learning Rate: 0.00051, Train Gradient: 26.3\n",
      "Epoch 66501/150000, Loss: 2860, Val Loss: 3030,  Learning Rate: 0.00051, Train Gradient: 26.3\n",
      "Epoch 66601/150000, Loss: 2855, Val Loss: 3024,  Learning Rate: 0.00051, Train Gradient: 26.3\n",
      "Epoch 66701/150000, Loss: 2850, Val Loss: 3019,  Learning Rate: 0.00051, Train Gradient: 26.3\n",
      "Epoch 66801/150000, Loss: 2844, Val Loss: 3014,  Learning Rate: 0.00051, Train Gradient: 26.2\n",
      "Epoch 66901/150000, Loss: 2839, Val Loss: 3009,  Learning Rate: 0.00051, Train Gradient: 26.2\n",
      "Epoch 67001/150000, Loss: 2834, Val Loss: 3004,  Learning Rate: 0.00051, Train Gradient: 26.1\n",
      "Epoch 67101/150000, Loss: 2829, Val Loss: 2998,  Learning Rate: 0.00050, Train Gradient: 26.1\n",
      "Epoch 67201/150000, Loss: 2824, Val Loss: 2993,  Learning Rate: 0.00050, Train Gradient: 26.1\n",
      "Epoch 67301/150000, Loss: 2818, Val Loss: 2988,  Learning Rate: 0.00050, Train Gradient: 26.0\n",
      "Epoch 67401/150000, Loss: 2813, Val Loss: 2983,  Learning Rate: 0.00050, Train Gradient: 26.0\n",
      "Epoch 67501/150000, Loss: 2808, Val Loss: 2978,  Learning Rate: 0.00050, Train Gradient: 26.0\n",
      "Epoch 67601/150000, Loss: 2803, Val Loss: 2973,  Learning Rate: 0.00050, Train Gradient: 25.9\n",
      "Epoch 67701/150000, Loss: 2798, Val Loss: 2967,  Learning Rate: 0.00050, Train Gradient: 25.9\n",
      "Epoch 67801/150000, Loss: 2793, Val Loss: 2962,  Learning Rate: 0.00050, Train Gradient: 25.9\n",
      "Epoch 67901/150000, Loss: 2788, Val Loss: 2957,  Learning Rate: 0.00050, Train Gradient: 25.8\n",
      "Epoch 68001/150000, Loss: 2782, Val Loss: 2952,  Learning Rate: 0.00050, Train Gradient: 25.8\n",
      "Epoch 68101/150000, Loss: 2777, Val Loss: 2947,  Learning Rate: 0.00050, Train Gradient: 25.8\n",
      "Epoch 68201/150000, Loss: 2772, Val Loss: 2942,  Learning Rate: 0.00050, Train Gradient: 25.7\n",
      "Epoch 68301/150000, Loss: 2767, Val Loss: 2937,  Learning Rate: 0.00050, Train Gradient: 25.7\n",
      "Epoch 68401/150000, Loss: 2762, Val Loss: 2932,  Learning Rate: 0.00050, Train Gradient: 25.7\n",
      "Epoch 68501/150000, Loss: 2757, Val Loss: 2927,  Learning Rate: 0.00050, Train Gradient: 25.6\n",
      "Epoch 68601/150000, Loss: 2752, Val Loss: 2921,  Learning Rate: 0.00050, Train Gradient: 25.6\n",
      "Epoch 68701/150000, Loss: 2747, Val Loss: 2916,  Learning Rate: 0.00050, Train Gradient: 25.5\n",
      "Epoch 68801/150000, Loss: 2742, Val Loss: 2911,  Learning Rate: 0.00050, Train Gradient: 25.5\n",
      "Epoch 68901/150000, Loss: 2737, Val Loss: 2906,  Learning Rate: 0.00050, Train Gradient: 25.5\n",
      "Epoch 69001/150000, Loss: 2732, Val Loss: 2901,  Learning Rate: 0.00050, Train Gradient: 25.4\n",
      "Epoch 69101/150000, Loss: 2727, Val Loss: 2896,  Learning Rate: 0.00050, Train Gradient: 25.4\n",
      "Epoch 69201/150000, Loss: 2722, Val Loss: 2891,  Learning Rate: 0.00050, Train Gradient: 25.4\n",
      "Epoch 69301/150000, Loss: 2717, Val Loss: 2886,  Learning Rate: 0.00050, Train Gradient: 25.3\n",
      "Epoch 69401/150000, Loss: 2712, Val Loss: 2881,  Learning Rate: 0.00050, Train Gradient: 25.3\n",
      "Epoch 69501/150000, Loss: 2707, Val Loss: 2877,  Learning Rate: 0.00050, Train Gradient: 25.3\n",
      "Epoch 69601/150000, Loss: 2702, Val Loss: 2872,  Learning Rate: 0.00050, Train Gradient: 25.2\n",
      "Epoch 69701/150000, Loss: 2697, Val Loss: 2867,  Learning Rate: 0.00050, Train Gradient: 25.2\n",
      "Epoch 69801/150000, Loss: 2692, Val Loss: 2862,  Learning Rate: 0.00050, Train Gradient: 25.2\n",
      "Epoch 69901/150000, Loss: 2687, Val Loss: 2857,  Learning Rate: 0.00050, Train Gradient: 25.1\n",
      "Epoch 70001/150000, Loss: 2682, Val Loss: 2852,  Learning Rate: 0.00049, Train Gradient: 25.1\n",
      "Epoch 70101/150000, Loss: 2677, Val Loss: 2847,  Learning Rate: 0.00049, Train Gradient: 25.1\n",
      "Epoch 70201/150000, Loss: 2673, Val Loss: 2843,  Learning Rate: 0.00049, Train Gradient: 25.0\n",
      "Epoch 70301/150000, Loss: 2668, Val Loss: 2838,  Learning Rate: 0.00049, Train Gradient: 25.0\n",
      "Epoch 70401/150000, Loss: 2663, Val Loss: 2833,  Learning Rate: 0.00049, Train Gradient: 25.0\n",
      "Epoch 70501/150000, Loss: 2658, Val Loss: 2828,  Learning Rate: 0.00049, Train Gradient: 25.0\n",
      "Epoch 70601/150000, Loss: 2653, Val Loss: 2823,  Learning Rate: 0.00049, Train Gradient: 24.9\n",
      "Epoch 70701/150000, Loss: 2648, Val Loss: 2818,  Learning Rate: 0.00049, Train Gradient: 24.9\n",
      "Epoch 70801/150000, Loss: 2643, Val Loss: 2813,  Learning Rate: 0.00049, Train Gradient: 24.8\n",
      "Epoch 70901/150000, Loss: 2638, Val Loss: 2808,  Learning Rate: 0.00049, Train Gradient: 24.8\n",
      "Epoch 71001/150000, Loss: 2634, Val Loss: 2804,  Learning Rate: 0.00049, Train Gradient: 24.8\n",
      "Epoch 71101/150000, Loss: 2629, Val Loss: 2799,  Learning Rate: 0.00049, Train Gradient: 24.7\n",
      "Epoch 71201/150000, Loss: 2624, Val Loss: 2794,  Learning Rate: 0.00049, Train Gradient: 24.7\n",
      "Epoch 71301/150000, Loss: 2619, Val Loss: 2789,  Learning Rate: 0.00049, Train Gradient: 24.7\n",
      "Epoch 71401/150000, Loss: 2614, Val Loss: 2784,  Learning Rate: 0.00049, Train Gradient: 24.6\n",
      "Epoch 71501/150000, Loss: 2610, Val Loss: 2780,  Learning Rate: 0.00049, Train Gradient: 24.6\n",
      "Epoch 71601/150000, Loss: 2605, Val Loss: 2775,  Learning Rate: 0.00049, Train Gradient: 24.6\n",
      "Epoch 71701/150000, Loss: 2600, Val Loss: 2770,  Learning Rate: 0.00049, Train Gradient: 24.5\n",
      "Epoch 71801/150000, Loss: 2595, Val Loss: 2766,  Learning Rate: 0.00049, Train Gradient: 24.5\n",
      "Epoch 71901/150000, Loss: 2591, Val Loss: 2761,  Learning Rate: 0.00049, Train Gradient: 24.5\n",
      "Epoch 72001/150000, Loss: 2586, Val Loss: 2756,  Learning Rate: 0.00049, Train Gradient: 24.4\n",
      "Epoch 72101/150000, Loss: 2581, Val Loss: 2752,  Learning Rate: 0.00049, Train Gradient: 24.4\n",
      "Epoch 72201/150000, Loss: 2577, Val Loss: 2747,  Learning Rate: 0.00049, Train Gradient: 24.4\n",
      "Epoch 72301/150000, Loss: 2572, Val Loss: 2742,  Learning Rate: 0.00049, Train Gradient: 24.3\n",
      "Epoch 72401/150000, Loss: 2567, Val Loss: 2738,  Learning Rate: 0.00049, Train Gradient: 24.3\n",
      "Epoch 72501/150000, Loss: 2563, Val Loss: 2733,  Learning Rate: 0.00049, Train Gradient: 24.3\n",
      "Epoch 72601/150000, Loss: 2558, Val Loss: 2728,  Learning Rate: 0.00049, Train Gradient: 24.2\n",
      "Epoch 72701/150000, Loss: 2553, Val Loss: 2724,  Learning Rate: 0.00049, Train Gradient: 24.2\n",
      "Epoch 72801/150000, Loss: 2549, Val Loss: 2719,  Learning Rate: 0.00049, Train Gradient: 24.2\n",
      "Epoch 72901/150000, Loss: 2544, Val Loss: 2715,  Learning Rate: 0.00049, Train Gradient: 24.2\n",
      "Epoch 73001/150000, Loss: 2539, Val Loss: 2710,  Learning Rate: 0.00049, Train Gradient: 24.1\n",
      "Epoch 73101/150000, Loss: 2535, Val Loss: 2705,  Learning Rate: 0.00049, Train Gradient: 24.1\n",
      "Epoch 73201/150000, Loss: 2530, Val Loss: 2701,  Learning Rate: 0.00049, Train Gradient: 24.1\n",
      "Epoch 73301/150000, Loss: 2525, Val Loss: 2696,  Learning Rate: 0.00048, Train Gradient: 24.0\n",
      "Epoch 73401/150000, Loss: 2521, Val Loss: 2692,  Learning Rate: 0.00048, Train Gradient: 24.0\n",
      "Epoch 73501/150000, Loss: 2516, Val Loss: 2687,  Learning Rate: 0.00048, Train Gradient: 24.0\n",
      "Epoch 73601/150000, Loss: 2512, Val Loss: 2683,  Learning Rate: 0.00048, Train Gradient: 23.9\n",
      "Epoch 73701/150000, Loss: 2507, Val Loss: 2678,  Learning Rate: 0.00048, Train Gradient: 23.9\n",
      "Epoch 73801/150000, Loss: 2502, Val Loss: 2673,  Learning Rate: 0.00048, Train Gradient: 23.9\n",
      "Epoch 73901/150000, Loss: 2498, Val Loss: 2669,  Learning Rate: 0.00048, Train Gradient: 23.8\n",
      "Epoch 74001/150000, Loss: 2493, Val Loss: 2665,  Learning Rate: 0.00048, Train Gradient: 23.8\n",
      "Epoch 74101/150000, Loss: 2489, Val Loss: 2660,  Learning Rate: 0.00048, Train Gradient: 23.8\n",
      "Epoch 74201/150000, Loss: 2484, Val Loss: 2656,  Learning Rate: 0.00048, Train Gradient: 23.7\n",
      "Epoch 74301/150000, Loss: 2480, Val Loss: 2651,  Learning Rate: 0.00048, Train Gradient: 23.7\n",
      "Epoch 74401/150000, Loss: 2475, Val Loss: 2647,  Learning Rate: 0.00048, Train Gradient: 23.7\n",
      "Epoch 74501/150000, Loss: 2471, Val Loss: 2642,  Learning Rate: 0.00048, Train Gradient: 23.6\n",
      "Epoch 74601/150000, Loss: 2466, Val Loss: 2638,  Learning Rate: 0.00048, Train Gradient: 23.6\n",
      "Epoch 74701/150000, Loss: 2462, Val Loss: 2634,  Learning Rate: 0.00048, Train Gradient: 23.6\n",
      "Epoch 74801/150000, Loss: 2457, Val Loss: 2629,  Learning Rate: 0.00048, Train Gradient: 23.6\n",
      "Epoch 74901/150000, Loss: 2453, Val Loss: 2625,  Learning Rate: 0.00048, Train Gradient: 23.5\n",
      "Epoch 75001/150000, Loss: 2449, Val Loss: 2620,  Learning Rate: 0.00048, Train Gradient: 23.5\n",
      "Epoch 75101/150000, Loss: 2444, Val Loss: 2616,  Learning Rate: 0.00048, Train Gradient: 23.5\n",
      "Epoch 75201/150000, Loss: 2440, Val Loss: 2612,  Learning Rate: 0.00048, Train Gradient: 23.4\n",
      "Epoch 75301/150000, Loss: 2435, Val Loss: 2607,  Learning Rate: 0.00048, Train Gradient: 23.4\n",
      "Epoch 75401/150000, Loss: 2431, Val Loss: 2603,  Learning Rate: 0.00048, Train Gradient: 23.4\n",
      "Epoch 75501/150000, Loss: 2426, Val Loss: 2598,  Learning Rate: 0.00048, Train Gradient: 23.3\n",
      "Epoch 75601/150000, Loss: 2422, Val Loss: 2594,  Learning Rate: 0.00048, Train Gradient: 23.3\n",
      "Epoch 75701/150000, Loss: 2418, Val Loss: 2590,  Learning Rate: 0.00048, Train Gradient: 23.3\n",
      "Epoch 75801/150000, Loss: 2413, Val Loss: 2585,  Learning Rate: 0.00048, Train Gradient: 23.2\n",
      "Epoch 75901/150000, Loss: 2409, Val Loss: 2581,  Learning Rate: 0.00048, Train Gradient: 23.2\n",
      "Epoch 76001/150000, Loss: 2404, Val Loss: 2577,  Learning Rate: 0.00048, Train Gradient: 23.2\n",
      "Epoch 76101/150000, Loss: 2400, Val Loss: 2572,  Learning Rate: 0.00048, Train Gradient: 23.2\n",
      "Epoch 76201/150000, Loss: 2396, Val Loss: 2568,  Learning Rate: 0.00048, Train Gradient: 23.1\n",
      "Epoch 76301/150000, Loss: 2391, Val Loss: 2564,  Learning Rate: 0.00048, Train Gradient: 23.1\n",
      "Epoch 76401/150000, Loss: 2387, Val Loss: 2559,  Learning Rate: 0.00048, Train Gradient: 23.1\n",
      "Epoch 76501/150000, Loss: 2383, Val Loss: 2555,  Learning Rate: 0.00048, Train Gradient: 23.0\n",
      "Epoch 76601/150000, Loss: 2378, Val Loss: 2551,  Learning Rate: 0.00048, Train Gradient: 23.0\n",
      "Epoch 76701/150000, Loss: 2374, Val Loss: 2546,  Learning Rate: 0.00048, Train Gradient: 23.0\n",
      "Epoch 76801/150000, Loss: 2369, Val Loss: 2542,  Learning Rate: 0.00048, Train Gradient: 22.9\n",
      "Epoch 76901/150000, Loss: 2365, Val Loss: 2538,  Learning Rate: 0.00048, Train Gradient: 22.9\n",
      "Epoch 77001/150000, Loss: 2361, Val Loss: 2534,  Learning Rate: 0.00047, Train Gradient: 22.9\n",
      "Epoch 77101/150000, Loss: 2357, Val Loss: 2530,  Learning Rate: 0.00047, Train Gradient: 22.9\n",
      "Epoch 77201/150000, Loss: 2352, Val Loss: 2525,  Learning Rate: 0.00047, Train Gradient: 22.8\n",
      "Epoch 77301/150000, Loss: 2348, Val Loss: 2521,  Learning Rate: 0.00047, Train Gradient: 22.8\n",
      "Epoch 77401/150000, Loss: 2344, Val Loss: 2517,  Learning Rate: 0.00047, Train Gradient: 22.8\n",
      "Epoch 77501/150000, Loss: 2340, Val Loss: 2513,  Learning Rate: 0.00047, Train Gradient: 22.7\n",
      "Epoch 77601/150000, Loss: 2335, Val Loss: 2508,  Learning Rate: 0.00047, Train Gradient: 22.7\n",
      "Epoch 77701/150000, Loss: 2331, Val Loss: 2504,  Learning Rate: 0.00047, Train Gradient: 22.7\n",
      "Epoch 77801/150000, Loss: 2327, Val Loss: 2500,  Learning Rate: 0.00047, Train Gradient: 22.7\n",
      "Epoch 77901/150000, Loss: 2323, Val Loss: 2496,  Learning Rate: 0.00047, Train Gradient: 22.6\n",
      "Epoch 78001/150000, Loss: 2319, Val Loss: 2492,  Learning Rate: 0.00047, Train Gradient: 22.6\n",
      "Epoch 78101/150000, Loss: 2314, Val Loss: 2487,  Learning Rate: 0.00047, Train Gradient: 22.6\n",
      "Epoch 78201/150000, Loss: 2310, Val Loss: 2483,  Learning Rate: 0.00047, Train Gradient: 22.5\n",
      "Epoch 78301/150000, Loss: 2306, Val Loss: 2479,  Learning Rate: 0.00047, Train Gradient: 22.5\n",
      "Epoch 78401/150000, Loss: 2302, Val Loss: 2475,  Learning Rate: 0.00047, Train Gradient: 22.5\n",
      "Epoch 78501/150000, Loss: 2298, Val Loss: 2471,  Learning Rate: 0.00047, Train Gradient: 22.4\n",
      "Epoch 78601/150000, Loss: 2293, Val Loss: 2466,  Learning Rate: 0.00047, Train Gradient: 22.4\n",
      "Epoch 78701/150000, Loss: 2289, Val Loss: 2462,  Learning Rate: 0.00047, Train Gradient: 22.4\n",
      "Epoch 78801/150000, Loss: 2285, Val Loss: 2458,  Learning Rate: 0.00047, Train Gradient: 22.4\n",
      "Epoch 78901/150000, Loss: 2281, Val Loss: 2454,  Learning Rate: 0.00047, Train Gradient: 22.3\n",
      "Epoch 79001/150000, Loss: 2277, Val Loss: 2450,  Learning Rate: 0.00047, Train Gradient: 22.3\n",
      "Epoch 79101/150000, Loss: 2273, Val Loss: 2446,  Learning Rate: 0.00047, Train Gradient: 22.3\n",
      "Epoch 79201/150000, Loss: 2268, Val Loss: 2441,  Learning Rate: 0.00047, Train Gradient: 22.2\n",
      "Epoch 79301/150000, Loss: 2264, Val Loss: 2437,  Learning Rate: 0.00047, Train Gradient: 22.2\n",
      "Epoch 79401/150000, Loss: 2260, Val Loss: 2433,  Learning Rate: 0.00047, Train Gradient: 22.2\n",
      "Epoch 79501/150000, Loss: 2256, Val Loss: 2429,  Learning Rate: 0.00047, Train Gradient: 22.2\n",
      "Epoch 79601/150000, Loss: 2252, Val Loss: 2425,  Learning Rate: 0.00047, Train Gradient: 22.1\n",
      "Epoch 79701/150000, Loss: 2248, Val Loss: 2421,  Learning Rate: 0.00047, Train Gradient: 22.1\n",
      "Epoch 79801/150000, Loss: 2244, Val Loss: 2416,  Learning Rate: 0.00047, Train Gradient: 22.1\n",
      "Epoch 79901/150000, Loss: 2240, Val Loss: 2412,  Learning Rate: 0.00047, Train Gradient: 22.0\n",
      "Epoch 80001/150000, Loss: 2235, Val Loss: 2408,  Learning Rate: 0.00047, Train Gradient: 22.0\n",
      "Epoch 80101/150000, Loss: 2231, Val Loss: 2404,  Learning Rate: 0.00047, Train Gradient: 22.0\n",
      "Epoch 80201/150000, Loss: 2227, Val Loss: 2400,  Learning Rate: 0.00047, Train Gradient: 22.0\n",
      "Epoch 80301/150000, Loss: 2223, Val Loss: 2396,  Learning Rate: 0.00047, Train Gradient: 22.0\n",
      "Epoch 80401/150000, Loss: 2219, Val Loss: 2392,  Learning Rate: 0.00047, Train Gradient: 21.9\n",
      "Epoch 80501/150000, Loss: 2215, Val Loss: 2388,  Learning Rate: 0.00047, Train Gradient: 21.9\n",
      "Epoch 80601/150000, Loss: 2211, Val Loss: 2384,  Learning Rate: 0.00047, Train Gradient: 21.9\n",
      "Epoch 80701/150000, Loss: 2207, Val Loss: 2380,  Learning Rate: 0.00047, Train Gradient: 21.8\n",
      "Epoch 80801/150000, Loss: 2203, Val Loss: 2376,  Learning Rate: 0.00047, Train Gradient: 21.8\n",
      "Epoch 80901/150000, Loss: 2199, Val Loss: 2372,  Learning Rate: 0.00047, Train Gradient: 21.8\n",
      "Epoch 81001/150000, Loss: 2195, Val Loss: 2368,  Learning Rate: 0.00047, Train Gradient: 21.7\n",
      "Epoch 81101/150000, Loss: 2191, Val Loss: 2364,  Learning Rate: 0.00047, Train Gradient: 21.7\n",
      "Epoch 81201/150000, Loss: 2187, Val Loss: 2360,  Learning Rate: 0.00047, Train Gradient: 21.7\n",
      "Epoch 81301/150000, Loss: 2183, Val Loss: 2356,  Learning Rate: 0.00046, Train Gradient: 21.7\n",
      "Epoch 81401/150000, Loss: 2179, Val Loss: 2352,  Learning Rate: 0.00046, Train Gradient: 21.6\n",
      "Epoch 81501/150000, Loss: 2175, Val Loss: 2348,  Learning Rate: 0.00046, Train Gradient: 21.6\n",
      "Epoch 81601/150000, Loss: 2171, Val Loss: 2344,  Learning Rate: 0.00046, Train Gradient: 21.6\n",
      "Epoch 81701/150000, Loss: 2168, Val Loss: 2341,  Learning Rate: 0.00046, Train Gradient: 21.6\n",
      "Epoch 81801/150000, Loss: 2164, Val Loss: 2337,  Learning Rate: 0.00046, Train Gradient: 21.5\n",
      "Epoch 81901/150000, Loss: 2160, Val Loss: 2333,  Learning Rate: 0.00046, Train Gradient: 21.5\n",
      "Epoch 82001/150000, Loss: 2156, Val Loss: 2329,  Learning Rate: 0.00046, Train Gradient: 21.5\n",
      "Epoch 82101/150000, Loss: 2152, Val Loss: 2325,  Learning Rate: 0.00046, Train Gradient: 21.4\n",
      "Epoch 82201/150000, Loss: 2148, Val Loss: 2321,  Learning Rate: 0.00046, Train Gradient: 21.4\n",
      "Epoch 82301/150000, Loss: 2144, Val Loss: 2317,  Learning Rate: 0.00046, Train Gradient: 21.4\n",
      "Epoch 82401/150000, Loss: 2140, Val Loss: 2313,  Learning Rate: 0.00046, Train Gradient: 21.4\n",
      "Epoch 82501/150000, Loss: 2136, Val Loss: 2309,  Learning Rate: 0.00046, Train Gradient: 21.3\n",
      "Epoch 82601/150000, Loss: 2132, Val Loss: 2306,  Learning Rate: 0.00046, Train Gradient: 21.3\n",
      "Epoch 82701/150000, Loss: 2128, Val Loss: 2302,  Learning Rate: 0.00046, Train Gradient: 21.3\n",
      "Epoch 82801/150000, Loss: 2124, Val Loss: 2298,  Learning Rate: 0.00046, Train Gradient: 21.2\n",
      "Epoch 82901/150000, Loss: 2120, Val Loss: 2294,  Learning Rate: 0.00046, Train Gradient: 21.2\n",
      "Epoch 83001/150000, Loss: 2117, Val Loss: 2290,  Learning Rate: 0.00046, Train Gradient: 21.2\n",
      "Epoch 83101/150000, Loss: 2113, Val Loss: 2286,  Learning Rate: 0.00046, Train Gradient: 21.2\n",
      "Epoch 83201/150000, Loss: 2109, Val Loss: 2282,  Learning Rate: 0.00046, Train Gradient: 21.1\n",
      "Epoch 83301/150000, Loss: 2105, Val Loss: 2279,  Learning Rate: 0.00046, Train Gradient: 21.1\n",
      "Epoch 83401/150000, Loss: 2101, Val Loss: 2275,  Learning Rate: 0.00046, Train Gradient: 21.1\n",
      "Epoch 83501/150000, Loss: 2097, Val Loss: 2271,  Learning Rate: 0.00046, Train Gradient: 21.1\n",
      "Epoch 83601/150000, Loss: 2093, Val Loss: 2267,  Learning Rate: 0.00046, Train Gradient: 21.0\n",
      "Epoch 83701/150000, Loss: 2090, Val Loss: 2263,  Learning Rate: 0.00046, Train Gradient: 21.0\n",
      "Epoch 83801/150000, Loss: 2086, Val Loss: 2260,  Learning Rate: 0.00046, Train Gradient: 21.0\n",
      "Epoch 83901/150000, Loss: 2082, Val Loss: 2256,  Learning Rate: 0.00046, Train Gradient: 20.9\n",
      "Epoch 84001/150000, Loss: 2078, Val Loss: 2252,  Learning Rate: 0.00046, Train Gradient: 20.9\n",
      "Epoch 84101/150000, Loss: 2074, Val Loss: 2249,  Learning Rate: 0.00046, Train Gradient: 20.9\n",
      "Epoch 84201/150000, Loss: 2071, Val Loss: 2245,  Learning Rate: 0.00046, Train Gradient: 20.9\n",
      "Epoch 84301/150000, Loss: 2067, Val Loss: 2241,  Learning Rate: 0.00046, Train Gradient: 20.8\n",
      "Epoch 84401/150000, Loss: 2063, Val Loss: 2238,  Learning Rate: 0.00046, Train Gradient: 20.8\n",
      "Epoch 84501/150000, Loss: 2059, Val Loss: 2234,  Learning Rate: 0.00046, Train Gradient: 20.8\n",
      "Epoch 84601/150000, Loss: 2056, Val Loss: 2230,  Learning Rate: 0.00046, Train Gradient: 20.8\n",
      "Epoch 84701/150000, Loss: 2052, Val Loss: 2226,  Learning Rate: 0.00046, Train Gradient: 20.7\n",
      "Epoch 84801/150000, Loss: 2048, Val Loss: 2223,  Learning Rate: 0.00046, Train Gradient: 20.7\n",
      "Epoch 84901/150000, Loss: 2044, Val Loss: 2219,  Learning Rate: 0.00046, Train Gradient: 20.7\n",
      "Epoch 85001/150000, Loss: 2041, Val Loss: 2216,  Learning Rate: 0.00046, Train Gradient: 20.7\n",
      "Epoch 85101/150000, Loss: 2037, Val Loss: 2212,  Learning Rate: 0.00046, Train Gradient: 20.6\n",
      "Epoch 85201/150000, Loss: 2033, Val Loss: 2208,  Learning Rate: 0.00046, Train Gradient: 20.6\n",
      "Epoch 85301/150000, Loss: 2030, Val Loss: 2205,  Learning Rate: 0.00046, Train Gradient: 20.6\n",
      "Epoch 85401/150000, Loss: 2026, Val Loss: 2201,  Learning Rate: 0.00046, Train Gradient: 20.6\n",
      "Epoch 85501/150000, Loss: 2022, Val Loss: 2197,  Learning Rate: 0.00046, Train Gradient: 20.5\n",
      "Epoch 85601/150000, Loss: 2018, Val Loss: 2194,  Learning Rate: 0.00046, Train Gradient: 20.5\n",
      "Epoch 85701/150000, Loss: 2015, Val Loss: 2190,  Learning Rate: 0.00046, Train Gradient: 20.5\n",
      "Epoch 85801/150000, Loss: 2011, Val Loss: 2186,  Learning Rate: 0.00046, Train Gradient: 20.5\n",
      "Epoch 85901/150000, Loss: 2007, Val Loss: 2183,  Learning Rate: 0.00046, Train Gradient: 20.4\n",
      "Epoch 86001/150000, Loss: 2004, Val Loss: 2179,  Learning Rate: 0.00046, Train Gradient: 20.4\n",
      "Epoch 86101/150000, Loss: 2000, Val Loss: 2176,  Learning Rate: 0.00046, Train Gradient: 20.4\n",
      "Epoch 86201/150000, Loss: 1996, Val Loss: 2172,  Learning Rate: 0.00046, Train Gradient: 20.3\n",
      "Epoch 86301/150000, Loss: 1993, Val Loss: 2168,  Learning Rate: 0.00045, Train Gradient: 20.3\n",
      "Epoch 86401/150000, Loss: 1989, Val Loss: 2165,  Learning Rate: 0.00045, Train Gradient: 20.3\n",
      "Epoch 86501/150000, Loss: 1985, Val Loss: 2161,  Learning Rate: 0.00045, Train Gradient: 20.3\n",
      "Epoch 86601/150000, Loss: 1982, Val Loss: 2158,  Learning Rate: 0.00045, Train Gradient: 20.2\n",
      "Epoch 86701/150000, Loss: 1978, Val Loss: 2154,  Learning Rate: 0.00045, Train Gradient: 20.2\n",
      "Epoch 86801/150000, Loss: 1974, Val Loss: 2150,  Learning Rate: 0.00045, Train Gradient: 20.2\n",
      "Epoch 86901/150000, Loss: 1971, Val Loss: 2147,  Learning Rate: 0.00045, Train Gradient: 20.2\n",
      "Epoch 87001/150000, Loss: 1967, Val Loss: 2143,  Learning Rate: 0.00045, Train Gradient: 20.1\n",
      "Epoch 87101/150000, Loss: 1964, Val Loss: 2140,  Learning Rate: 0.00045, Train Gradient: 20.1\n",
      "Epoch 87201/150000, Loss: 1960, Val Loss: 2136,  Learning Rate: 0.00045, Train Gradient: 20.1\n",
      "Epoch 87301/150000, Loss: 1956, Val Loss: 2133,  Learning Rate: 0.00045, Train Gradient: 20.1\n",
      "Epoch 87401/150000, Loss: 1953, Val Loss: 2129,  Learning Rate: 0.00045, Train Gradient: 20.0\n",
      "Epoch 87501/150000, Loss: 1949, Val Loss: 2126,  Learning Rate: 0.00045, Train Gradient: 20.0\n",
      "Epoch 87601/150000, Loss: 1946, Val Loss: 2122,  Learning Rate: 0.00045, Train Gradient: 20.0\n",
      "Epoch 87701/150000, Loss: 1942, Val Loss: 2119,  Learning Rate: 0.00045, Train Gradient: 19.9\n",
      "Epoch 87801/150000, Loss: 1938, Val Loss: 2115,  Learning Rate: 0.00045, Train Gradient: 19.9\n",
      "Epoch 87901/150000, Loss: 1935, Val Loss: 2112,  Learning Rate: 0.00045, Train Gradient: 19.9\n",
      "Epoch 88001/150000, Loss: 1931, Val Loss: 2108,  Learning Rate: 0.00045, Train Gradient: 19.9\n",
      "Epoch 88101/150000, Loss: 1928, Val Loss: 2105,  Learning Rate: 0.00045, Train Gradient: 19.8\n",
      "Epoch 88201/150000, Loss: 1924, Val Loss: 2101,  Learning Rate: 0.00045, Train Gradient: 19.8\n",
      "Epoch 88301/150000, Loss: 1921, Val Loss: 2098,  Learning Rate: 0.00045, Train Gradient: 19.8\n",
      "Epoch 88401/150000, Loss: 1917, Val Loss: 2094,  Learning Rate: 0.00045, Train Gradient: 19.8\n",
      "Epoch 88501/150000, Loss: 1914, Val Loss: 2091,  Learning Rate: 0.00045, Train Gradient: 19.8\n",
      "Epoch 88601/150000, Loss: 1910, Val Loss: 2087,  Learning Rate: 0.00045, Train Gradient: 19.7\n",
      "Epoch 88701/150000, Loss: 1907, Val Loss: 2084,  Learning Rate: 0.00045, Train Gradient: 19.7\n",
      "Epoch 88801/150000, Loss: 1903, Val Loss: 2081,  Learning Rate: 0.00045, Train Gradient: 19.7\n",
      "Epoch 88901/150000, Loss: 1900, Val Loss: 2077,  Learning Rate: 0.00045, Train Gradient: 19.6\n",
      "Epoch 89001/150000, Loss: 1896, Val Loss: 2074,  Learning Rate: 0.00045, Train Gradient: 19.6\n",
      "Epoch 89101/150000, Loss: 1893, Val Loss: 2070,  Learning Rate: 0.00045, Train Gradient: 19.6\n",
      "Epoch 89201/150000, Loss: 1889, Val Loss: 2067,  Learning Rate: 0.00045, Train Gradient: 19.5\n",
      "Epoch 89301/150000, Loss: 1886, Val Loss: 2064,  Learning Rate: 0.00045, Train Gradient: 19.5\n",
      "Epoch 89401/150000, Loss: 1882, Val Loss: 2060,  Learning Rate: 0.00045, Train Gradient: 19.5\n",
      "Epoch 89501/150000, Loss: 1879, Val Loss: 2057,  Learning Rate: 0.00045, Train Gradient: 19.5\n",
      "Epoch 89601/150000, Loss: 1876, Val Loss: 2054,  Learning Rate: 0.00045, Train Gradient: 19.5\n",
      "Epoch 89701/150000, Loss: 1872, Val Loss: 2050,  Learning Rate: 0.00045, Train Gradient: 19.4\n",
      "Epoch 89801/150000, Loss: 1869, Val Loss: 2047,  Learning Rate: 0.00045, Train Gradient: 19.4\n",
      "Epoch 89901/150000, Loss: 1865, Val Loss: 2043,  Learning Rate: 0.00045, Train Gradient: 19.4\n",
      "Epoch 90001/150000, Loss: 1862, Val Loss: 2040,  Learning Rate: 0.00045, Train Gradient: 19.4\n",
      "Epoch 90101/150000, Loss: 1858, Val Loss: 2037,  Learning Rate: 0.00045, Train Gradient: 19.3\n",
      "Epoch 90201/150000, Loss: 1855, Val Loss: 2033,  Learning Rate: 0.00045, Train Gradient: 19.3\n",
      "Epoch 90301/150000, Loss: 1852, Val Loss: 2030,  Learning Rate: 0.00045, Train Gradient: 19.3\n",
      "Epoch 90401/150000, Loss: 1848, Val Loss: 2027,  Learning Rate: 0.00045, Train Gradient: 19.2\n",
      "Epoch 90501/150000, Loss: 1845, Val Loss: 2023,  Learning Rate: 0.00045, Train Gradient: 19.2\n",
      "Epoch 90601/150000, Loss: 1841, Val Loss: 2020,  Learning Rate: 0.00045, Train Gradient: 19.2\n",
      "Epoch 90701/150000, Loss: 1838, Val Loss: 2017,  Learning Rate: 0.00045, Train Gradient: 19.2\n",
      "Epoch 90801/150000, Loss: 1835, Val Loss: 2013,  Learning Rate: 0.00045, Train Gradient: 19.1\n",
      "Epoch 90901/150000, Loss: 1831, Val Loss: 2010,  Learning Rate: 0.00045, Train Gradient: 19.1\n",
      "Epoch 91001/150000, Loss: 1828, Val Loss: 2007,  Learning Rate: 0.00045, Train Gradient: 19.1\n",
      "Epoch 91101/150000, Loss: 1824, Val Loss: 2003,  Learning Rate: 0.00045, Train Gradient: 19.1\n",
      "Epoch 91201/150000, Loss: 1821, Val Loss: 2000,  Learning Rate: 0.00045, Train Gradient: 19.0\n",
      "Epoch 91301/150000, Loss: 1818, Val Loss: 1997,  Learning Rate: 0.00045, Train Gradient: 19.0\n",
      "Epoch 91401/150000, Loss: 1814, Val Loss: 1993,  Learning Rate: 0.00045, Train Gradient: 19.0\n",
      "Epoch 91501/150000, Loss: 1811, Val Loss: 1990,  Learning Rate: 0.00045, Train Gradient: 19.0\n",
      "Epoch 91601/150000, Loss: 1808, Val Loss: 1987,  Learning Rate: 0.00045, Train Gradient: 18.9\n",
      "Epoch 91701/150000, Loss: 1804, Val Loss: 1983,  Learning Rate: 0.00045, Train Gradient: 18.9\n",
      "Epoch 91801/150000, Loss: 1801, Val Loss: 1980,  Learning Rate: 0.00045, Train Gradient: 18.9\n",
      "Epoch 91901/150000, Loss: 1797, Val Loss: 1977,  Learning Rate: 0.00045, Train Gradient: 18.9\n",
      "Epoch 92001/150000, Loss: 1794, Val Loss: 1973,  Learning Rate: 0.00045, Train Gradient: 18.9\n",
      "Epoch 92101/150000, Loss: 1791, Val Loss: 1970,  Learning Rate: 0.00045, Train Gradient: 18.8\n",
      "Epoch 92201/150000, Loss: 1787, Val Loss: 1967,  Learning Rate: 0.00045, Train Gradient: 18.8\n",
      "Epoch 92301/150000, Loss: 1784, Val Loss: 1963,  Learning Rate: 0.00045, Train Gradient: 18.8\n",
      "Epoch 92401/150000, Loss: 1781, Val Loss: 1960,  Learning Rate: 0.00044, Train Gradient: 18.7\n",
      "Epoch 92501/150000, Loss: 1778, Val Loss: 1957,  Learning Rate: 0.00044, Train Gradient: 18.7\n",
      "Epoch 92601/150000, Loss: 1774, Val Loss: 1954,  Learning Rate: 0.00044, Train Gradient: 18.7\n",
      "Epoch 92701/150000, Loss: 1771, Val Loss: 1950,  Learning Rate: 0.00044, Train Gradient: 18.7\n",
      "Epoch 92801/150000, Loss: 1768, Val Loss: 1947,  Learning Rate: 0.00044, Train Gradient: 18.6\n",
      "Epoch 92901/150000, Loss: 1764, Val Loss: 1944,  Learning Rate: 0.00044, Train Gradient: 18.6\n",
      "Epoch 93001/150000, Loss: 1761, Val Loss: 1941,  Learning Rate: 0.00044, Train Gradient: 18.6\n",
      "Epoch 93101/150000, Loss: 1758, Val Loss: 1938,  Learning Rate: 0.00044, Train Gradient: 18.6\n",
      "Epoch 93201/150000, Loss: 1755, Val Loss: 1934,  Learning Rate: 0.00044, Train Gradient: 18.5\n",
      "Epoch 93301/150000, Loss: 1752, Val Loss: 1931,  Learning Rate: 0.00044, Train Gradient: 18.5\n",
      "Epoch 93401/150000, Loss: 1748, Val Loss: 1928,  Learning Rate: 0.00044, Train Gradient: 18.5\n",
      "Epoch 93501/150000, Loss: 1745, Val Loss: 1925,  Learning Rate: 0.00044, Train Gradient: 18.5\n",
      "Epoch 93601/150000, Loss: 1742, Val Loss: 1921,  Learning Rate: 0.00044, Train Gradient: 18.4\n",
      "Epoch 93701/150000, Loss: 1739, Val Loss: 1918,  Learning Rate: 0.00044, Train Gradient: 18.4\n",
      "Epoch 93801/150000, Loss: 1735, Val Loss: 1915,  Learning Rate: 0.00044, Train Gradient: 18.4\n",
      "Epoch 93901/150000, Loss: 1732, Val Loss: 1912,  Learning Rate: 0.00044, Train Gradient: 18.4\n",
      "Epoch 94001/150000, Loss: 1729, Val Loss: 1909,  Learning Rate: 0.00044, Train Gradient: 18.3\n",
      "Epoch 94101/150000, Loss: 1726, Val Loss: 1906,  Learning Rate: 0.00044, Train Gradient: 18.3\n",
      "Epoch 94201/150000, Loss: 1723, Val Loss: 1902,  Learning Rate: 0.00044, Train Gradient: 18.3\n",
      "Epoch 94301/150000, Loss: 1720, Val Loss: 1899,  Learning Rate: 0.00044, Train Gradient: 18.3\n",
      "Epoch 94401/150000, Loss: 1716, Val Loss: 1896,  Learning Rate: 0.00044, Train Gradient: 18.2\n",
      "Epoch 94501/150000, Loss: 1713, Val Loss: 1893,  Learning Rate: 0.00044, Train Gradient: 18.2\n",
      "Epoch 94601/150000, Loss: 1710, Val Loss: 1890,  Learning Rate: 0.00044, Train Gradient: 18.2\n",
      "Epoch 94701/150000, Loss: 1707, Val Loss: 1886,  Learning Rate: 0.00044, Train Gradient: 18.2\n",
      "Epoch 94801/150000, Loss: 1704, Val Loss: 1883,  Learning Rate: 0.00044, Train Gradient: 18.1\n",
      "Epoch 94901/150000, Loss: 1701, Val Loss: 1880,  Learning Rate: 0.00044, Train Gradient: 18.1\n",
      "Epoch 95001/150000, Loss: 1697, Val Loss: 1877,  Learning Rate: 0.00044, Train Gradient: 18.1\n",
      "Epoch 95101/150000, Loss: 1694, Val Loss: 1874,  Learning Rate: 0.00044, Train Gradient: 18.1\n",
      "Epoch 95201/150000, Loss: 1691, Val Loss: 1871,  Learning Rate: 0.00044, Train Gradient: 18.0\n",
      "Epoch 95301/150000, Loss: 1688, Val Loss: 1868,  Learning Rate: 0.00044, Train Gradient: 18.0\n",
      "Epoch 95401/150000, Loss: 1685, Val Loss: 1864,  Learning Rate: 0.00044, Train Gradient: 18.0\n",
      "Epoch 95501/150000, Loss: 1682, Val Loss: 1861,  Learning Rate: 0.00044, Train Gradient: 18.0\n",
      "Epoch 95601/150000, Loss: 1679, Val Loss: 1858,  Learning Rate: 0.00044, Train Gradient: 17.9\n",
      "Epoch 95701/150000, Loss: 1675, Val Loss: 1855,  Learning Rate: 0.00044, Train Gradient: 17.9\n",
      "Epoch 95801/150000, Loss: 1672, Val Loss: 1852,  Learning Rate: 0.00044, Train Gradient: 17.9\n",
      "Epoch 95901/150000, Loss: 1669, Val Loss: 1849,  Learning Rate: 0.00044, Train Gradient: 17.9\n",
      "Epoch 96001/150000, Loss: 1666, Val Loss: 1846,  Learning Rate: 0.00044, Train Gradient: 17.8\n",
      "Epoch 96101/150000, Loss: 1663, Val Loss: 1843,  Learning Rate: 0.00044, Train Gradient: 17.8\n",
      "Epoch 96201/150000, Loss: 1660, Val Loss: 1840,  Learning Rate: 0.00044, Train Gradient: 17.8\n",
      "Epoch 96301/150000, Loss: 1657, Val Loss: 1837,  Learning Rate: 0.00044, Train Gradient: 17.8\n",
      "Epoch 96401/150000, Loss: 1654, Val Loss: 1833,  Learning Rate: 0.00044, Train Gradient: 17.7\n",
      "Epoch 96501/150000, Loss: 1651, Val Loss: 1830,  Learning Rate: 0.00044, Train Gradient: 17.7\n",
      "Epoch 96601/150000, Loss: 1648, Val Loss: 1827,  Learning Rate: 0.00044, Train Gradient: 17.7\n",
      "Epoch 96701/150000, Loss: 1644, Val Loss: 1824,  Learning Rate: 0.00044, Train Gradient: 17.7\n",
      "Epoch 96801/150000, Loss: 1641, Val Loss: 1821,  Learning Rate: 0.00044, Train Gradient: 17.6\n",
      "Epoch 96901/150000, Loss: 1638, Val Loss: 1818,  Learning Rate: 0.00044, Train Gradient: 17.6\n",
      "Epoch 97001/150000, Loss: 1635, Val Loss: 1815,  Learning Rate: 0.00044, Train Gradient: 17.6\n",
      "Epoch 97101/150000, Loss: 1632, Val Loss: 1812,  Learning Rate: 0.00044, Train Gradient: 17.6\n",
      "Epoch 97201/150000, Loss: 1629, Val Loss: 1809,  Learning Rate: 0.00044, Train Gradient: 17.5\n",
      "Epoch 97301/150000, Loss: 1626, Val Loss: 1806,  Learning Rate: 0.00044, Train Gradient: 17.5\n",
      "Epoch 97401/150000, Loss: 1623, Val Loss: 1803,  Learning Rate: 0.00044, Train Gradient: 17.5\n",
      "Epoch 97501/150000, Loss: 1620, Val Loss: 1800,  Learning Rate: 0.00044, Train Gradient: 17.5\n",
      "Epoch 97601/150000, Loss: 1617, Val Loss: 1797,  Learning Rate: 0.00044, Train Gradient: 17.5\n",
      "Epoch 97701/150000, Loss: 1614, Val Loss: 1794,  Learning Rate: 0.00044, Train Gradient: 17.4\n",
      "Epoch 97801/150000, Loss: 1611, Val Loss: 1791,  Learning Rate: 0.00044, Train Gradient: 17.4\n",
      "Epoch 97901/150000, Loss: 1608, Val Loss: 1788,  Learning Rate: 0.00044, Train Gradient: 17.4\n",
      "Epoch 98001/150000, Loss: 1605, Val Loss: 1785,  Learning Rate: 0.00044, Train Gradient: 17.4\n",
      "Epoch 98101/150000, Loss: 1602, Val Loss: 1782,  Learning Rate: 0.00044, Train Gradient: 17.3\n",
      "Epoch 98201/150000, Loss: 1599, Val Loss: 1779,  Learning Rate: 0.00044, Train Gradient: 17.3\n",
      "Epoch 98301/150000, Loss: 1596, Val Loss: 1776,  Learning Rate: 0.00044, Train Gradient: 17.3\n",
      "Epoch 98401/150000, Loss: 1593, Val Loss: 1773,  Learning Rate: 0.00044, Train Gradient: 17.3\n",
      "Epoch 98501/150000, Loss: 1590, Val Loss: 1770,  Learning Rate: 0.00044, Train Gradient: 17.2\n",
      "Epoch 98601/150000, Loss: 1587, Val Loss: 1767,  Learning Rate: 0.00044, Train Gradient: 17.2\n",
      "Epoch 98701/150000, Loss: 1584, Val Loss: 1764,  Learning Rate: 0.00044, Train Gradient: 17.2\n",
      "Epoch 98801/150000, Loss: 1581, Val Loss: 1761,  Learning Rate: 0.00044, Train Gradient: 17.2\n",
      "Epoch 98901/150000, Loss: 1578, Val Loss: 1758,  Learning Rate: 0.00044, Train Gradient: 17.1\n",
      "Epoch 99001/150000, Loss: 1575, Val Loss: 1755,  Learning Rate: 0.00044, Train Gradient: 17.1\n",
      "Epoch 99101/150000, Loss: 1572, Val Loss: 1753,  Learning Rate: 0.00044, Train Gradient: 17.1\n",
      "Epoch 99201/150000, Loss: 1569, Val Loss: 1750,  Learning Rate: 0.00044, Train Gradient: 17.1\n",
      "Epoch 99301/150000, Loss: 1567, Val Loss: 1747,  Learning Rate: 0.00044, Train Gradient: 17.1\n",
      "Epoch 99401/150000, Loss: 1564, Val Loss: 1744,  Learning Rate: 0.00044, Train Gradient: 17.0\n",
      "Epoch 99501/150000, Loss: 1561, Val Loss: 1741,  Learning Rate: 0.00044, Train Gradient: 17.0\n",
      "Epoch 99601/150000, Loss: 1558, Val Loss: 1738,  Learning Rate: 0.00044, Train Gradient: 17.0\n",
      "Epoch 99701/150000, Loss: 1555, Val Loss: 1735,  Learning Rate: 0.00044, Train Gradient: 17.0\n",
      "Epoch 99801/150000, Loss: 1552, Val Loss: 1732,  Learning Rate: 0.00044, Train Gradient: 16.9\n",
      "Epoch 99901/150000, Loss: 1549, Val Loss: 1729,  Learning Rate: 0.00043, Train Gradient: 16.9\n",
      "Epoch 100001/150000, Loss: 1546, Val Loss: 1727,  Learning Rate: 0.00043, Train Gradient: 16.9\n",
      "Epoch 100101/150000, Loss: 1543, Val Loss: 1724,  Learning Rate: 0.00043, Train Gradient: 16.9\n",
      "Epoch 100201/150000, Loss: 1540, Val Loss: 1721,  Learning Rate: 0.00043, Train Gradient: 16.8\n",
      "Epoch 100301/150000, Loss: 1538, Val Loss: 1718,  Learning Rate: 0.00043, Train Gradient: 16.8\n",
      "Epoch 100401/150000, Loss: 1535, Val Loss: 1715,  Learning Rate: 0.00043, Train Gradient: 16.8\n",
      "Epoch 100501/150000, Loss: 1532, Val Loss: 1712,  Learning Rate: 0.00043, Train Gradient: 16.8\n",
      "Epoch 100601/150000, Loss: 1529, Val Loss: 1710,  Learning Rate: 0.00043, Train Gradient: 16.7\n",
      "Epoch 100701/150000, Loss: 1526, Val Loss: 1707,  Learning Rate: 0.00043, Train Gradient: 16.7\n",
      "Epoch 100801/150000, Loss: 1523, Val Loss: 1704,  Learning Rate: 0.00043, Train Gradient: 16.7\n",
      "Epoch 100901/150000, Loss: 1520, Val Loss: 1701,  Learning Rate: 0.00043, Train Gradient: 16.7\n",
      "Epoch 101001/150000, Loss: 1518, Val Loss: 1698,  Learning Rate: 0.00043, Train Gradient: 16.7\n",
      "Epoch 101101/150000, Loss: 1515, Val Loss: 1695,  Learning Rate: 0.00043, Train Gradient: 16.6\n",
      "Epoch 101201/150000, Loss: 1512, Val Loss: 1693,  Learning Rate: 0.00043, Train Gradient: 16.6\n",
      "Epoch 101301/150000, Loss: 1509, Val Loss: 1690,  Learning Rate: 0.00043, Train Gradient: 16.6\n",
      "Epoch 101401/150000, Loss: 1506, Val Loss: 1687,  Learning Rate: 0.00043, Train Gradient: 16.6\n",
      "Epoch 101501/150000, Loss: 1503, Val Loss: 1684,  Learning Rate: 0.00043, Train Gradient: 16.5\n",
      "Epoch 101601/150000, Loss: 1501, Val Loss: 1681,  Learning Rate: 0.00043, Train Gradient: 16.5\n",
      "Epoch 101701/150000, Loss: 1498, Val Loss: 1679,  Learning Rate: 0.00043, Train Gradient: 16.5\n",
      "Epoch 101801/150000, Loss: 1495, Val Loss: 1676,  Learning Rate: 0.00043, Train Gradient: 16.5\n",
      "Epoch 101901/150000, Loss: 1492, Val Loss: 1673,  Learning Rate: 0.00043, Train Gradient: 16.4\n",
      "Epoch 102001/150000, Loss: 1489, Val Loss: 1670,  Learning Rate: 0.00043, Train Gradient: 16.4\n",
      "Epoch 102101/150000, Loss: 1486, Val Loss: 1668,  Learning Rate: 0.00043, Train Gradient: 16.4\n",
      "Epoch 102201/150000, Loss: 1484, Val Loss: 1665,  Learning Rate: 0.00043, Train Gradient: 16.4\n",
      "Epoch 102301/150000, Loss: 1481, Val Loss: 1662,  Learning Rate: 0.00043, Train Gradient: 16.4\n",
      "Epoch 102401/150000, Loss: 1478, Val Loss: 1659,  Learning Rate: 0.00043, Train Gradient: 16.3\n",
      "Epoch 102501/150000, Loss: 1475, Val Loss: 1657,  Learning Rate: 0.00043, Train Gradient: 16.3\n",
      "Epoch 102601/150000, Loss: 1472, Val Loss: 1654,  Learning Rate: 0.00043, Train Gradient: 16.3\n",
      "Epoch 102701/150000, Loss: 1470, Val Loss: 1651,  Learning Rate: 0.00043, Train Gradient: 16.3\n",
      "Epoch 102801/150000, Loss: 1467, Val Loss: 1648,  Learning Rate: 0.00043, Train Gradient: 16.2\n",
      "Epoch 102901/150000, Loss: 1464, Val Loss: 1646,  Learning Rate: 0.00043, Train Gradient: 16.2\n",
      "Epoch 103001/150000, Loss: 1461, Val Loss: 1643,  Learning Rate: 0.00043, Train Gradient: 16.2\n",
      "Epoch 103101/150000, Loss: 1459, Val Loss: 1640,  Learning Rate: 0.00043, Train Gradient: 16.2\n",
      "Epoch 103201/150000, Loss: 1456, Val Loss: 1638,  Learning Rate: 0.00043, Train Gradient: 16.2\n",
      "Epoch 103301/150000, Loss: 1453, Val Loss: 1635,  Learning Rate: 0.00043, Train Gradient: 16.1\n",
      "Epoch 103401/150000, Loss: 1450, Val Loss: 1632,  Learning Rate: 0.00043, Train Gradient: 16.1\n",
      "Epoch 103501/150000, Loss: 1448, Val Loss: 1629,  Learning Rate: 0.00043, Train Gradient: 16.1\n",
      "Epoch 103601/150000, Loss: 1445, Val Loss: 1627,  Learning Rate: 0.00043, Train Gradient: 16.1\n",
      "Epoch 103701/150000, Loss: 1442, Val Loss: 1624,  Learning Rate: 0.00043, Train Gradient: 16.0\n",
      "Epoch 103801/150000, Loss: 1439, Val Loss: 1621,  Learning Rate: 0.00043, Train Gradient: 16.0\n",
      "Epoch 103901/150000, Loss: 1437, Val Loss: 1619,  Learning Rate: 0.00043, Train Gradient: 16.0\n",
      "Epoch 104001/150000, Loss: 1434, Val Loss: 1616,  Learning Rate: 0.00043, Train Gradient: 16.0\n",
      "Epoch 104101/150000, Loss: 1431, Val Loss: 1613,  Learning Rate: 0.00043, Train Gradient: 16.0\n",
      "Epoch 104201/150000, Loss: 1428, Val Loss: 1611,  Learning Rate: 0.00043, Train Gradient: 15.9\n",
      "Epoch 104301/150000, Loss: 1426, Val Loss: 1608,  Learning Rate: 0.00043, Train Gradient: 15.9\n",
      "Epoch 104401/150000, Loss: 1423, Val Loss: 1605,  Learning Rate: 0.00043, Train Gradient: 15.9\n",
      "Epoch 104501/150000, Loss: 1420, Val Loss: 1603,  Learning Rate: 0.00043, Train Gradient: 15.9\n",
      "Epoch 104601/150000, Loss: 1417, Val Loss: 1600,  Learning Rate: 0.00043, Train Gradient: 15.8\n",
      "Epoch 104701/150000, Loss: 1415, Val Loss: 1597,  Learning Rate: 0.00043, Train Gradient: 15.8\n",
      "Epoch 104801/150000, Loss: 1412, Val Loss: 1595,  Learning Rate: 0.00043, Train Gradient: 15.8\n",
      "Epoch 104901/150000, Loss: 1409, Val Loss: 1592,  Learning Rate: 0.00043, Train Gradient: 15.8\n",
      "Epoch 105001/150000, Loss: 1407, Val Loss: 1590,  Learning Rate: 0.00043, Train Gradient: 15.8\n",
      "Epoch 105101/150000, Loss: 1404, Val Loss: 1587,  Learning Rate: 0.00043, Train Gradient: 15.7\n",
      "Epoch 105201/150000, Loss: 1401, Val Loss: 1584,  Learning Rate: 0.00043, Train Gradient: 15.7\n",
      "Epoch 105301/150000, Loss: 1399, Val Loss: 1582,  Learning Rate: 0.00043, Train Gradient: 15.7\n",
      "Epoch 105401/150000, Loss: 1396, Val Loss: 1579,  Learning Rate: 0.00043, Train Gradient: 15.7\n",
      "Epoch 105501/150000, Loss: 1393, Val Loss: 1576,  Learning Rate: 0.00043, Train Gradient: 15.7\n",
      "Epoch 105601/150000, Loss: 1391, Val Loss: 1574,  Learning Rate: 0.00043, Train Gradient: 15.6\n",
      "Epoch 105701/150000, Loss: 1388, Val Loss: 1571,  Learning Rate: 0.00043, Train Gradient: 15.6\n",
      "Epoch 105801/150000, Loss: 1385, Val Loss: 1569,  Learning Rate: 0.00043, Train Gradient: 15.6\n",
      "Epoch 105901/150000, Loss: 1383, Val Loss: 1566,  Learning Rate: 0.00043, Train Gradient: 15.6\n",
      "Epoch 106001/150000, Loss: 1380, Val Loss: 1564,  Learning Rate: 0.00043, Train Gradient: 15.6\n",
      "Epoch 106101/150000, Loss: 1378, Val Loss: 1561,  Learning Rate: 0.00043, Train Gradient: 15.5\n",
      "Epoch 106201/150000, Loss: 1375, Val Loss: 1559,  Learning Rate: 0.00043, Train Gradient: 15.5\n",
      "Epoch 106301/150000, Loss: 1372, Val Loss: 1556,  Learning Rate: 0.00043, Train Gradient: 15.5\n",
      "Epoch 106401/150000, Loss: 1370, Val Loss: 1554,  Learning Rate: 0.00043, Train Gradient: 15.5\n",
      "Epoch 106501/150000, Loss: 1367, Val Loss: 1551,  Learning Rate: 0.00043, Train Gradient: 15.5\n",
      "Epoch 106601/150000, Loss: 1365, Val Loss: 1549,  Learning Rate: 0.00043, Train Gradient: 15.4\n",
      "Epoch 106701/150000, Loss: 1362, Val Loss: 1546,  Learning Rate: 0.00043, Train Gradient: 15.4\n",
      "Epoch 106801/150000, Loss: 1359, Val Loss: 1544,  Learning Rate: 0.00043, Train Gradient: 15.4\n",
      "Epoch 106901/150000, Loss: 1357, Val Loss: 1541,  Learning Rate: 0.00043, Train Gradient: 15.4\n",
      "Epoch 107001/150000, Loss: 1354, Val Loss: 1539,  Learning Rate: 0.00043, Train Gradient: 15.4\n",
      "Epoch 107101/150000, Loss: 1352, Val Loss: 1536,  Learning Rate: 0.00043, Train Gradient: 15.3\n",
      "Epoch 107201/150000, Loss: 1349, Val Loss: 1534,  Learning Rate: 0.00043, Train Gradient: 15.3\n",
      "Epoch 107301/150000, Loss: 1346, Val Loss: 1531,  Learning Rate: 0.00043, Train Gradient: 15.3\n",
      "Epoch 107401/150000, Loss: 1344, Val Loss: 1529,  Learning Rate: 0.00043, Train Gradient: 15.3\n",
      "Epoch 107501/150000, Loss: 1341, Val Loss: 1527,  Learning Rate: 0.00043, Train Gradient: 15.3\n",
      "Epoch 107601/150000, Loss: 1339, Val Loss: 1524,  Learning Rate: 0.00043, Train Gradient: 15.2\n",
      "Epoch 107701/150000, Loss: 1336, Val Loss: 1522,  Learning Rate: 0.00043, Train Gradient: 15.2\n",
      "Epoch 107801/150000, Loss: 1334, Val Loss: 1519,  Learning Rate: 0.00043, Train Gradient: 15.2\n",
      "Epoch 107901/150000, Loss: 1331, Val Loss: 1517,  Learning Rate: 0.00043, Train Gradient: 15.2\n",
      "Epoch 108001/150000, Loss: 1329, Val Loss: 1514,  Learning Rate: 0.00043, Train Gradient: 15.2\n",
      "Epoch 108101/150000, Loss: 1326, Val Loss: 1512,  Learning Rate: 0.00043, Train Gradient: 15.1\n",
      "Epoch 108201/150000, Loss: 1323, Val Loss: 1510,  Learning Rate: 0.00043, Train Gradient: 15.1\n",
      "Epoch 108301/150000, Loss: 1321, Val Loss: 1507,  Learning Rate: 0.00043, Train Gradient: 15.1\n",
      "Epoch 108401/150000, Loss: 1318, Val Loss: 1505,  Learning Rate: 0.00043, Train Gradient: 15.1\n",
      "Epoch 108501/150000, Loss: 1316, Val Loss: 1502,  Learning Rate: 0.00043, Train Gradient: 15.1\n",
      "Epoch 108601/150000, Loss: 1313, Val Loss: 1500,  Learning Rate: 0.00043, Train Gradient: 15.0\n",
      "Epoch 108701/150000, Loss: 1311, Val Loss: 1497,  Learning Rate: 0.00043, Train Gradient: 15.0\n",
      "Epoch 108801/150000, Loss: 1308, Val Loss: 1495,  Learning Rate: 0.00043, Train Gradient: 15.0\n",
      "Epoch 108901/150000, Loss: 1306, Val Loss: 1493,  Learning Rate: 0.00043, Train Gradient: 15.0\n",
      "Epoch 109001/150000, Loss: 1303, Val Loss: 1490,  Learning Rate: 0.00043, Train Gradient: 15.0\n",
      "Epoch 109101/150000, Loss: 1301, Val Loss: 1488,  Learning Rate: 0.00043, Train Gradient: 14.9\n",
      "Epoch 109201/150000, Loss: 1298, Val Loss: 1485,  Learning Rate: 0.00043, Train Gradient: 14.9\n",
      "Epoch 109301/150000, Loss: 1296, Val Loss: 1483,  Learning Rate: 0.00043, Train Gradient: 14.9\n",
      "Epoch 109401/150000, Loss: 1293, Val Loss: 1481,  Learning Rate: 0.00043, Train Gradient: 14.9\n",
      "Epoch 109501/150000, Loss: 1291, Val Loss: 1478,  Learning Rate: 0.00043, Train Gradient: 14.9\n",
      "Epoch 109601/150000, Loss: 1288, Val Loss: 1476,  Learning Rate: 0.00043, Train Gradient: 14.8\n",
      "Epoch 109701/150000, Loss: 1286, Val Loss: 1473,  Learning Rate: 0.00043, Train Gradient: 14.8\n",
      "Epoch 109801/150000, Loss: 1283, Val Loss: 1471,  Learning Rate: 0.00043, Train Gradient: 14.8\n",
      "Epoch 109901/150000, Loss: 1281, Val Loss: 1469,  Learning Rate: 0.00043, Train Gradient: 14.8\n",
      "Epoch 110001/150000, Loss: 1278, Val Loss: 1466,  Learning Rate: 0.00043, Train Gradient: 14.7\n",
      "Epoch 110101/150000, Loss: 1276, Val Loss: 1464,  Learning Rate: 0.00043, Train Gradient: 14.7\n",
      "Epoch 110201/150000, Loss: 1273, Val Loss: 1462,  Learning Rate: 0.00043, Train Gradient: 14.7\n",
      "Epoch 110301/150000, Loss: 1271, Val Loss: 1459,  Learning Rate: 0.00042, Train Gradient: 14.7\n",
      "Epoch 110401/150000, Loss: 1268, Val Loss: 1457,  Learning Rate: 0.00042, Train Gradient: 14.7\n",
      "Epoch 110501/150000, Loss: 1266, Val Loss: 1455,  Learning Rate: 0.00042, Train Gradient: 14.6\n",
      "Epoch 110601/150000, Loss: 1263, Val Loss: 1452,  Learning Rate: 0.00042, Train Gradient: 14.6\n",
      "Epoch 110701/150000, Loss: 1261, Val Loss: 1450,  Learning Rate: 0.00042, Train Gradient: 14.6\n",
      "Epoch 110801/150000, Loss: 1259, Val Loss: 1447,  Learning Rate: 0.00042, Train Gradient: 14.6\n",
      "Epoch 110901/150000, Loss: 1256, Val Loss: 1445,  Learning Rate: 0.00042, Train Gradient: 14.6\n",
      "Epoch 111001/150000, Loss: 1254, Val Loss: 1443,  Learning Rate: 0.00042, Train Gradient: 14.5\n",
      "Epoch 111101/150000, Loss: 1251, Val Loss: 1440,  Learning Rate: 0.00042, Train Gradient: 14.5\n",
      "Epoch 111201/150000, Loss: 1249, Val Loss: 1438,  Learning Rate: 0.00042, Train Gradient: 14.5\n",
      "Epoch 111301/150000, Loss: 1246, Val Loss: 1436,  Learning Rate: 0.00042, Train Gradient: 14.5\n",
      "Epoch 111401/150000, Loss: 1244, Val Loss: 1433,  Learning Rate: 0.00042, Train Gradient: 14.5\n",
      "Epoch 111501/150000, Loss: 1241, Val Loss: 1431,  Learning Rate: 0.00042, Train Gradient: 14.4\n",
      "Epoch 111601/150000, Loss: 1239, Val Loss: 1429,  Learning Rate: 0.00042, Train Gradient: 14.4\n",
      "Epoch 111701/150000, Loss: 1237, Val Loss: 1427,  Learning Rate: 0.00042, Train Gradient: 14.4\n",
      "Epoch 111801/150000, Loss: 1234, Val Loss: 1424,  Learning Rate: 0.00042, Train Gradient: 14.4\n",
      "Epoch 111901/150000, Loss: 1232, Val Loss: 1422,  Learning Rate: 0.00042, Train Gradient: 14.4\n",
      "Epoch 112001/150000, Loss: 1229, Val Loss: 1420,  Learning Rate: 0.00042, Train Gradient: 14.3\n",
      "Epoch 112101/150000, Loss: 1227, Val Loss: 1417,  Learning Rate: 0.00042, Train Gradient: 14.3\n",
      "Epoch 112201/150000, Loss: 1225, Val Loss: 1415,  Learning Rate: 0.00042, Train Gradient: 14.3\n",
      "Epoch 112301/150000, Loss: 1222, Val Loss: 1413,  Learning Rate: 0.00042, Train Gradient: 14.3\n",
      "Epoch 112401/150000, Loss: 1220, Val Loss: 1410,  Learning Rate: 0.00042, Train Gradient: 14.3\n",
      "Epoch 112501/150000, Loss: 1217, Val Loss: 1408,  Learning Rate: 0.00042, Train Gradient: 14.2\n",
      "Epoch 112601/150000, Loss: 1215, Val Loss: 1406,  Learning Rate: 0.00042, Train Gradient: 14.2\n",
      "Epoch 112701/150000, Loss: 1213, Val Loss: 1403,  Learning Rate: 0.00042, Train Gradient: 14.2\n",
      "Epoch 112801/150000, Loss: 1210, Val Loss: 1401,  Learning Rate: 0.00042, Train Gradient: 14.2\n",
      "Epoch 112901/150000, Loss: 1208, Val Loss: 1399,  Learning Rate: 0.00042, Train Gradient: 14.2\n",
      "Epoch 113001/150000, Loss: 1205, Val Loss: 1396,  Learning Rate: 0.00042, Train Gradient: 14.1\n",
      "Epoch 113101/150000, Loss: 1203, Val Loss: 1394,  Learning Rate: 0.00042, Train Gradient: 14.1\n",
      "Epoch 113201/150000, Loss: 1201, Val Loss: 1392,  Learning Rate: 0.00042, Train Gradient: 14.1\n",
      "Epoch 113301/150000, Loss: 1198, Val Loss: 1390,  Learning Rate: 0.00042, Train Gradient: 14.1\n",
      "Epoch 113401/150000, Loss: 1196, Val Loss: 1387,  Learning Rate: 0.00042, Train Gradient: 14.1\n",
      "Epoch 113501/150000, Loss: 1194, Val Loss: 1385,  Learning Rate: 0.00042, Train Gradient: 14.0\n",
      "Epoch 113601/150000, Loss: 1191, Val Loss: 1383,  Learning Rate: 0.00042, Train Gradient: 14.0\n",
      "Epoch 113701/150000, Loss: 1189, Val Loss: 1381,  Learning Rate: 0.00042, Train Gradient: 14.0\n",
      "Epoch 113801/150000, Loss: 1187, Val Loss: 1378,  Learning Rate: 0.00042, Train Gradient: 14.0\n",
      "Epoch 113901/150000, Loss: 1184, Val Loss: 1376,  Learning Rate: 0.00042, Train Gradient: 14.0\n",
      "Epoch 114001/150000, Loss: 1182, Val Loss: 1374,  Learning Rate: 0.00042, Train Gradient: 13.9\n",
      "Epoch 114101/150000, Loss: 1180, Val Loss: 1371,  Learning Rate: 0.00042, Train Gradient: 13.9\n",
      "Epoch 114201/150000, Loss: 1177, Val Loss: 1369,  Learning Rate: 0.00042, Train Gradient: 13.9\n",
      "Epoch 114301/150000, Loss: 1175, Val Loss: 1367,  Learning Rate: 0.00042, Train Gradient: 13.9\n",
      "Epoch 114401/150000, Loss: 1173, Val Loss: 1365,  Learning Rate: 0.00042, Train Gradient: 13.9\n",
      "Epoch 114501/150000, Loss: 1170, Val Loss: 1362,  Learning Rate: 0.00042, Train Gradient: 13.8\n",
      "Epoch 114601/150000, Loss: 1168, Val Loss: 1360,  Learning Rate: 0.00042, Train Gradient: 13.8\n",
      "Epoch 114701/150000, Loss: 1166, Val Loss: 1358,  Learning Rate: 0.00042, Train Gradient: 13.8\n",
      "Epoch 114801/150000, Loss: 1163, Val Loss: 1356,  Learning Rate: 0.00042, Train Gradient: 13.8\n",
      "Epoch 114901/150000, Loss: 1161, Val Loss: 1354,  Learning Rate: 0.00042, Train Gradient: 13.8\n",
      "Epoch 115001/150000, Loss: 1159, Val Loss: 1351,  Learning Rate: 0.00042, Train Gradient: 13.7\n",
      "Epoch 115101/150000, Loss: 1157, Val Loss: 1349,  Learning Rate: 0.00042, Train Gradient: 13.7\n",
      "Epoch 115201/150000, Loss: 1154, Val Loss: 1347,  Learning Rate: 0.00042, Train Gradient: 13.7\n",
      "Epoch 115301/150000, Loss: 1152, Val Loss: 1345,  Learning Rate: 0.00042, Train Gradient: 13.7\n",
      "Epoch 115401/150000, Loss: 1150, Val Loss: 1343,  Learning Rate: 0.00042, Train Gradient: 13.7\n",
      "Epoch 115501/150000, Loss: 1148, Val Loss: 1341,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 115601/150000, Loss: 1145, Val Loss: 1338,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 115701/150000, Loss: 1143, Val Loss: 1336,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 115801/150000, Loss: 1141, Val Loss: 1334,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 115901/150000, Loss: 1139, Val Loss: 1332,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 116001/150000, Loss: 1136, Val Loss: 1330,  Learning Rate: 0.00042, Train Gradient: 13.6\n",
      "Epoch 116101/150000, Loss: 1134, Val Loss: 1328,  Learning Rate: 0.00042, Train Gradient: 13.5\n",
      "Epoch 116201/150000, Loss: 1132, Val Loss: 1325,  Learning Rate: 0.00042, Train Gradient: 13.5\n",
      "Epoch 116301/150000, Loss: 1130, Val Loss: 1323,  Learning Rate: 0.00042, Train Gradient: 13.5\n",
      "Epoch 116401/150000, Loss: 1127, Val Loss: 1321,  Learning Rate: 0.00042, Train Gradient: 13.5\n",
      "Epoch 116501/150000, Loss: 1125, Val Loss: 1319,  Learning Rate: 0.00042, Train Gradient: 13.5\n",
      "Epoch 116601/150000, Loss: 1123, Val Loss: 1317,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 116701/150000, Loss: 1121, Val Loss: 1315,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 116801/150000, Loss: 1119, Val Loss: 1312,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 116901/150000, Loss: 1116, Val Loss: 1310,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 117001/150000, Loss: 1114, Val Loss: 1308,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 117101/150000, Loss: 1112, Val Loss: 1306,  Learning Rate: 0.00042, Train Gradient: 13.4\n",
      "Epoch 117201/150000, Loss: 1110, Val Loss: 1304,  Learning Rate: 0.00042, Train Gradient: 13.3\n",
      "Epoch 117301/150000, Loss: 1107, Val Loss: 1302,  Learning Rate: 0.00042, Train Gradient: 13.3\n",
      "Epoch 117401/150000, Loss: 1105, Val Loss: 1300,  Learning Rate: 0.00042, Train Gradient: 13.3\n",
      "Epoch 117501/150000, Loss: 1103, Val Loss: 1298,  Learning Rate: 0.00042, Train Gradient: 13.3\n",
      "Epoch 117601/150000, Loss: 1101, Val Loss: 1296,  Learning Rate: 0.00042, Train Gradient: 13.3\n",
      "Epoch 117701/150000, Loss: 1099, Val Loss: 1293,  Learning Rate: 0.00042, Train Gradient: 13.2\n",
      "Epoch 117801/150000, Loss: 1097, Val Loss: 1291,  Learning Rate: 0.00042, Train Gradient: 13.2\n",
      "Epoch 117901/150000, Loss: 1094, Val Loss: 1289,  Learning Rate: 0.00042, Train Gradient: 13.2\n",
      "Epoch 118001/150000, Loss: 1092, Val Loss: 1287,  Learning Rate: 0.00042, Train Gradient: 13.2\n",
      "Epoch 118101/150000, Loss: 1090, Val Loss: 1285,  Learning Rate: 0.00042, Train Gradient: 13.2\n",
      "Epoch 118201/150000, Loss: 1088, Val Loss: 1283,  Learning Rate: 0.00042, Train Gradient: 13.1\n",
      "Epoch 118301/150000, Loss: 1086, Val Loss: 1281,  Learning Rate: 0.00042, Train Gradient: 13.1\n",
      "Epoch 118401/150000, Loss: 1084, Val Loss: 1279,  Learning Rate: 0.00042, Train Gradient: 13.1\n",
      "Epoch 118501/150000, Loss: 1081, Val Loss: 1277,  Learning Rate: 0.00042, Train Gradient: 13.1\n",
      "Epoch 118601/150000, Loss: 1079, Val Loss: 1274,  Learning Rate: 0.00042, Train Gradient: 13.1\n",
      "Epoch 118701/150000, Loss: 1077, Val Loss: 1272,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 118801/150000, Loss: 1075, Val Loss: 1270,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 118901/150000, Loss: 1073, Val Loss: 1268,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 119001/150000, Loss: 1071, Val Loss: 1266,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 119101/150000, Loss: 1068, Val Loss: 1264,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 119201/150000, Loss: 1066, Val Loss: 1262,  Learning Rate: 0.00042, Train Gradient: 13.0\n",
      "Epoch 119301/150000, Loss: 1064, Val Loss: 1260,  Learning Rate: 0.00042, Train Gradient: 12.9\n",
      "Epoch 119401/150000, Loss: 1062, Val Loss: 1258,  Learning Rate: 0.00042, Train Gradient: 12.9\n",
      "Epoch 119501/150000, Loss: 1060, Val Loss: 1256,  Learning Rate: 0.00042, Train Gradient: 12.9\n",
      "Epoch 119601/150000, Loss: 1058, Val Loss: 1254,  Learning Rate: 0.00042, Train Gradient: 12.9\n",
      "Epoch 119701/150000, Loss: 1056, Val Loss: 1252,  Learning Rate: 0.00042, Train Gradient: 12.9\n",
      "Epoch 119801/150000, Loss: 1054, Val Loss: 1250,  Learning Rate: 0.00042, Train Gradient: 12.8\n",
      "Epoch 119901/150000, Loss: 1051, Val Loss: 1248,  Learning Rate: 0.00042, Train Gradient: 12.8\n",
      "Epoch 120001/150000, Loss: 1049, Val Loss: 1245,  Learning Rate: 0.00042, Train Gradient: 12.8\n",
      "Epoch 120101/150000, Loss: 1047, Val Loss: 1243,  Learning Rate: 0.00042, Train Gradient: 12.8\n",
      "Epoch 120201/150000, Loss: 1045, Val Loss: 1241,  Learning Rate: 0.00042, Train Gradient: 12.8\n",
      "Epoch 120301/150000, Loss: 1043, Val Loss: 1239,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120401/150000, Loss: 1041, Val Loss: 1237,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120501/150000, Loss: 1039, Val Loss: 1235,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120601/150000, Loss: 1037, Val Loss: 1233,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120701/150000, Loss: 1035, Val Loss: 1231,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120801/150000, Loss: 1033, Val Loss: 1229,  Learning Rate: 0.00042, Train Gradient: 12.7\n",
      "Epoch 120901/150000, Loss: 1030, Val Loss: 1227,  Learning Rate: 0.00042, Train Gradient: 12.6\n",
      "Epoch 121001/150000, Loss: 1028, Val Loss: 1225,  Learning Rate: 0.00042, Train Gradient: 12.6\n",
      "Epoch 121101/150000, Loss: 1026, Val Loss: 1223,  Learning Rate: 0.00042, Train Gradient: 12.6\n",
      "Epoch 121201/150000, Loss: 1024, Val Loss: 1221,  Learning Rate: 0.00042, Train Gradient: 12.6\n",
      "Epoch 121301/150000, Loss: 1022, Val Loss: 1219,  Learning Rate: 0.00042, Train Gradient: 12.6\n",
      "Epoch 121401/150000, Loss: 1020, Val Loss: 1217,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 121501/150000, Loss: 1018, Val Loss: 1215,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 121601/150000, Loss: 1016, Val Loss: 1213,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 121701/150000, Loss: 1014, Val Loss: 1212,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 121801/150000, Loss: 1012, Val Loss: 1210,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 121901/150000, Loss: 1010, Val Loss: 1208,  Learning Rate: 0.00042, Train Gradient: 12.5\n",
      "Epoch 122001/150000, Loss: 1008, Val Loss: 1206,  Learning Rate: 0.00042, Train Gradient: 12.4\n",
      "Epoch 122101/150000, Loss: 1006, Val Loss: 1204,  Learning Rate: 0.00042, Train Gradient: 12.4\n",
      "Epoch 122201/150000, Loss: 1004, Val Loss: 1202,  Learning Rate: 0.00042, Train Gradient: 12.4\n",
      "Epoch 122301/150000, Loss: 1002, Val Loss: 1200,  Learning Rate: 0.00042, Train Gradient: 12.4\n",
      "Epoch 122401/150000, Loss: 999, Val Loss: 1198,  Learning Rate: 0.00042, Train Gradient: 12.4\n",
      "Epoch 122501/150000, Loss: 997, Val Loss: 1196,  Learning Rate: 0.00042, Train Gradient: 12.3\n",
      "Epoch 122601/150000, Loss: 995, Val Loss: 1194,  Learning Rate: 0.00042, Train Gradient: 12.3\n",
      "Epoch 122701/150000, Loss: 993, Val Loss: 1192,  Learning Rate: 0.00042, Train Gradient: 12.3\n",
      "Epoch 122801/150000, Loss: 991, Val Loss: 1190,  Learning Rate: 0.00042, Train Gradient: 12.3\n",
      "Epoch 122901/150000, Loss: 989, Val Loss: 1188,  Learning Rate: 0.00042, Train Gradient: 12.3\n",
      "Epoch 123001/150000, Loss: 987, Val Loss: 1186,  Learning Rate: 0.00042, Train Gradient: 12.2\n",
      "Epoch 123101/150000, Loss: 985, Val Loss: 1185,  Learning Rate: 0.00042, Train Gradient: 12.2\n",
      "Epoch 123201/150000, Loss: 983, Val Loss: 1183,  Learning Rate: 0.00042, Train Gradient: 12.2\n",
      "Epoch 123301/150000, Loss: 981, Val Loss: 1181,  Learning Rate: 0.00042, Train Gradient: 12.2\n",
      "Epoch 123401/150000, Loss: 979, Val Loss: 1179,  Learning Rate: 0.00042, Train Gradient: 12.2\n",
      "Epoch 123501/150000, Loss: 977, Val Loss: 1177,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 123601/150000, Loss: 975, Val Loss: 1175,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 123701/150000, Loss: 973, Val Loss: 1173,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 123801/150000, Loss: 971, Val Loss: 1171,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 123901/150000, Loss: 969, Val Loss: 1169,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 124001/150000, Loss: 967, Val Loss: 1168,  Learning Rate: 0.00042, Train Gradient: 12.1\n",
      "Epoch 124101/150000, Loss: 965, Val Loss: 1166,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124201/150000, Loss: 963, Val Loss: 1164,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124301/150000, Loss: 961, Val Loss: 1162,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124401/150000, Loss: 959, Val Loss: 1160,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124501/150000, Loss: 957, Val Loss: 1158,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124601/150000, Loss: 955, Val Loss: 1156,  Learning Rate: 0.00042, Train Gradient: 12.0\n",
      "Epoch 124701/150000, Loss: 953, Val Loss: 1155,  Learning Rate: 0.00042, Train Gradient: 11.9\n",
      "Epoch 124801/150000, Loss: 951, Val Loss: 1153,  Learning Rate: 0.00042, Train Gradient: 11.9\n",
      "Epoch 124901/150000, Loss: 949, Val Loss: 1151,  Learning Rate: 0.00042, Train Gradient: 11.9\n",
      "Epoch 125001/150000, Loss: 948, Val Loss: 1149,  Learning Rate: 0.00042, Train Gradient: 11.9\n",
      "Epoch 125101/150000, Loss: 946, Val Loss: 1147,  Learning Rate: 0.00042, Train Gradient: 11.9\n",
      "Epoch 125201/150000, Loss: 944, Val Loss: 1145,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125301/150000, Loss: 942, Val Loss: 1143,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125401/150000, Loss: 940, Val Loss: 1142,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125501/150000, Loss: 938, Val Loss: 1140,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125601/150000, Loss: 936, Val Loss: 1138,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125701/150000, Loss: 934, Val Loss: 1136,  Learning Rate: 0.00042, Train Gradient: 11.8\n",
      "Epoch 125801/150000, Loss: 932, Val Loss: 1134,  Learning Rate: 0.00042, Train Gradient: 11.7\n",
      "Epoch 125901/150000, Loss: 930, Val Loss: 1133,  Learning Rate: 0.00042, Train Gradient: 11.7\n",
      "Epoch 126001/150000, Loss: 928, Val Loss: 1131,  Learning Rate: 0.00041, Train Gradient: 11.7\n",
      "Epoch 126101/150000, Loss: 926, Val Loss: 1129,  Learning Rate: 0.00041, Train Gradient: 11.7\n",
      "Epoch 126201/150000, Loss: 924, Val Loss: 1127,  Learning Rate: 0.00041, Train Gradient: 11.7\n",
      "Epoch 126301/150000, Loss: 922, Val Loss: 1125,  Learning Rate: 0.00041, Train Gradient: 11.6\n",
      "Epoch 126401/150000, Loss: 920, Val Loss: 1124,  Learning Rate: 0.00041, Train Gradient: 11.6\n",
      "Epoch 126501/150000, Loss: 918, Val Loss: 1122,  Learning Rate: 0.00041, Train Gradient: 11.6\n",
      "Epoch 126601/150000, Loss: 917, Val Loss: 1120,  Learning Rate: 0.00041, Train Gradient: 11.6\n",
      "Epoch 126701/150000, Loss: 915, Val Loss: 1118,  Learning Rate: 0.00041, Train Gradient: 11.6\n",
      "Epoch 126801/150000, Loss: 913, Val Loss: 1116,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 126901/150000, Loss: 911, Val Loss: 1115,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 127001/150000, Loss: 909, Val Loss: 1113,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 127101/150000, Loss: 907, Val Loss: 1111,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 127201/150000, Loss: 905, Val Loss: 1109,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 127301/150000, Loss: 903, Val Loss: 1108,  Learning Rate: 0.00041, Train Gradient: 11.5\n",
      "Epoch 127401/150000, Loss: 902, Val Loss: 1106,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 127501/150000, Loss: 900, Val Loss: 1104,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 127601/150000, Loss: 898, Val Loss: 1103,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 127701/150000, Loss: 896, Val Loss: 1101,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 127801/150000, Loss: 894, Val Loss: 1099,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 127901/150000, Loss: 892, Val Loss: 1097,  Learning Rate: 0.00041, Train Gradient: 11.4\n",
      "Epoch 128001/150000, Loss: 891, Val Loss: 1096,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128101/150000, Loss: 889, Val Loss: 1094,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128201/150000, Loss: 887, Val Loss: 1092,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128301/150000, Loss: 885, Val Loss: 1091,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128401/150000, Loss: 883, Val Loss: 1089,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128501/150000, Loss: 881, Val Loss: 1087,  Learning Rate: 0.00041, Train Gradient: 11.3\n",
      "Epoch 128601/150000, Loss: 880, Val Loss: 1086,  Learning Rate: 0.00041, Train Gradient: 11.2\n",
      "Epoch 128701/150000, Loss: 878, Val Loss: 1084,  Learning Rate: 0.00041, Train Gradient: 11.2\n",
      "Epoch 128801/150000, Loss: 876, Val Loss: 1082,  Learning Rate: 0.00041, Train Gradient: 11.2\n",
      "Epoch 128901/150000, Loss: 874, Val Loss: 1081,  Learning Rate: 0.00041, Train Gradient: 11.2\n",
      "Epoch 129001/150000, Loss: 872, Val Loss: 1079,  Learning Rate: 0.00041, Train Gradient: 11.2\n",
      "Epoch 129101/150000, Loss: 871, Val Loss: 1077,  Learning Rate: 0.00041, Train Gradient: 11.1\n",
      "Epoch 129201/150000, Loss: 869, Val Loss: 1076,  Learning Rate: 0.00041, Train Gradient: 11.1\n",
      "Epoch 129301/150000, Loss: 867, Val Loss: 1074,  Learning Rate: 0.00041, Train Gradient: 11.1\n",
      "Epoch 129401/150000, Loss: 865, Val Loss: 1072,  Learning Rate: 0.00041, Train Gradient: 11.1\n",
      "Epoch 129501/150000, Loss: 863, Val Loss: 1071,  Learning Rate: 0.00041, Train Gradient: 11.1\n",
      "Epoch 129601/150000, Loss: 862, Val Loss: 1069,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 129701/150000, Loss: 860, Val Loss: 1067,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 129801/150000, Loss: 858, Val Loss: 1066,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 129901/150000, Loss: 856, Val Loss: 1064,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 130001/150000, Loss: 854, Val Loss: 1062,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 130101/150000, Loss: 853, Val Loss: 1061,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 130201/150000, Loss: 851, Val Loss: 1059,  Learning Rate: 0.00041, Train Gradient: 11.0\n",
      "Epoch 130301/150000, Loss: 849, Val Loss: 1057,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130401/150000, Loss: 847, Val Loss: 1056,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130501/150000, Loss: 846, Val Loss: 1054,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130601/150000, Loss: 844, Val Loss: 1053,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130701/150000, Loss: 842, Val Loss: 1051,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130801/150000, Loss: 840, Val Loss: 1049,  Learning Rate: 0.00041, Train Gradient: 10.9\n",
      "Epoch 130901/150000, Loss: 838, Val Loss: 1048,  Learning Rate: 0.00041, Train Gradient: 10.8\n",
      "Epoch 131001/150000, Loss: 837, Val Loss: 1046,  Learning Rate: 0.00041, Train Gradient: 10.8\n",
      "Epoch 131101/150000, Loss: 835, Val Loss: 1045,  Learning Rate: 0.00041, Train Gradient: 10.8\n",
      "Epoch 131201/150000, Loss: 833, Val Loss: 1043,  Learning Rate: 0.00041, Train Gradient: 10.8\n",
      "Epoch 131301/150000, Loss: 831, Val Loss: 1041,  Learning Rate: 0.00041, Train Gradient: 10.8\n",
      "Epoch 131401/150000, Loss: 830, Val Loss: 1040,  Learning Rate: 0.00041, Train Gradient: 10.7\n",
      "Epoch 131501/150000, Loss: 828, Val Loss: 1038,  Learning Rate: 0.00041, Train Gradient: 10.7\n",
      "Epoch 131601/150000, Loss: 826, Val Loss: 1037,  Learning Rate: 0.00041, Train Gradient: 10.7\n",
      "Epoch 131701/150000, Loss: 825, Val Loss: 1035,  Learning Rate: 0.00041, Train Gradient: 10.7\n",
      "Epoch 131801/150000, Loss: 823, Val Loss: 1033,  Learning Rate: 0.00041, Train Gradient: 10.7\n",
      "Epoch 131901/150000, Loss: 821, Val Loss: 1032,  Learning Rate: 0.00041, Train Gradient: 10.7\n",
      "Epoch 132001/150000, Loss: 819, Val Loss: 1030,  Learning Rate: 0.00041, Train Gradient: 10.6\n",
      "Epoch 132101/150000, Loss: 818, Val Loss: 1029,  Learning Rate: 0.00041, Train Gradient: 10.6\n",
      "Epoch 132201/150000, Loss: 816, Val Loss: 1027,  Learning Rate: 0.00041, Train Gradient: 10.6\n",
      "Epoch 132301/150000, Loss: 814, Val Loss: 1025,  Learning Rate: 0.00041, Train Gradient: 10.6\n",
      "Epoch 132401/150000, Loss: 812, Val Loss: 1024,  Learning Rate: 0.00041, Train Gradient: 10.6\n",
      "Epoch 132501/150000, Loss: 811, Val Loss: 1022,  Learning Rate: 0.00041, Train Gradient: 10.6\n",
      "Epoch 132601/150000, Loss: 809, Val Loss: 1021,  Learning Rate: 0.00041, Train Gradient: 10.6\n",
      "Epoch 132701/150000, Loss: 807, Val Loss: 1019,  Learning Rate: 0.00041, Train Gradient: 10.5\n",
      "Epoch 132801/150000, Loss: 806, Val Loss: 1018,  Learning Rate: 0.00041, Train Gradient: 10.5\n",
      "Epoch 132901/150000, Loss: 804, Val Loss: 1016,  Learning Rate: 0.00041, Train Gradient: 10.5\n",
      "Epoch 133001/150000, Loss: 802, Val Loss: 1014,  Learning Rate: 0.00041, Train Gradient: 10.5\n",
      "Epoch 133101/150000, Loss: 801, Val Loss: 1013,  Learning Rate: 0.00041, Train Gradient: 10.5\n",
      "Epoch 133201/150000, Loss: 799, Val Loss: 1011,  Learning Rate: 0.00041, Train Gradient: 10.5\n",
      "Epoch 133301/150000, Loss: 797, Val Loss: 1010,  Learning Rate: 0.00041, Train Gradient: 10.4\n",
      "Epoch 133401/150000, Loss: 795, Val Loss: 1008,  Learning Rate: 0.00041, Train Gradient: 10.4\n",
      "Epoch 133501/150000, Loss: 794, Val Loss: 1006,  Learning Rate: 0.00041, Train Gradient: 10.4\n",
      "Epoch 133601/150000, Loss: 792, Val Loss: 1005,  Learning Rate: 0.00041, Train Gradient: 10.4\n",
      "Epoch 133701/150000, Loss: 790, Val Loss: 1003,  Learning Rate: 0.00041, Train Gradient: 10.4\n",
      "Epoch 133801/150000, Loss: 789, Val Loss: 1002,  Learning Rate: 0.00041, Train Gradient: 10.4\n",
      "Epoch 133901/150000, Loss: 787, Val Loss: 1000,  Learning Rate: 0.00041, Train Gradient: 10.3\n",
      "Epoch 134001/150000, Loss: 785, Val Loss: 999,  Learning Rate: 0.00041, Train Gradient: 10.3\n",
      "Epoch 134101/150000, Loss: 784, Val Loss: 997,  Learning Rate: 0.00041, Train Gradient: 10.3\n",
      "Epoch 134201/150000, Loss: 782, Val Loss: 995,  Learning Rate: 0.00041, Train Gradient: 10.3\n",
      "Epoch 134301/150000, Loss: 780, Val Loss: 994,  Learning Rate: 0.00041, Train Gradient: 10.3\n",
      "Epoch 134401/150000, Loss: 779, Val Loss: 992,  Learning Rate: 0.00041, Train Gradient: 10.3\n",
      "Epoch 134501/150000, Loss: 777, Val Loss: 991,  Learning Rate: 0.00041, Train Gradient: 10.2\n",
      "Epoch 134601/150000, Loss: 775, Val Loss: 989,  Learning Rate: 0.00041, Train Gradient: 10.2\n",
      "Epoch 134701/150000, Loss: 774, Val Loss: 988,  Learning Rate: 0.00041, Train Gradient: 10.2\n",
      "Epoch 134801/150000, Loss: 772, Val Loss: 986,  Learning Rate: 0.00041, Train Gradient: 10.2\n",
      "Epoch 134901/150000, Loss: 770, Val Loss: 985,  Learning Rate: 0.00041, Train Gradient: 10.2\n",
      "Epoch 135001/150000, Loss: 769, Val Loss: 983,  Learning Rate: 0.00041, Train Gradient: 10.2\n",
      "Epoch 135101/150000, Loss: 767, Val Loss: 982,  Learning Rate: 0.00041, Train Gradient: 10.1\n",
      "Epoch 135201/150000, Loss: 766, Val Loss: 980,  Learning Rate: 0.00041, Train Gradient: 10.1\n",
      "Epoch 135301/150000, Loss: 764, Val Loss: 979,  Learning Rate: 0.00041, Train Gradient: 10.1\n",
      "Epoch 135401/150000, Loss: 762, Val Loss: 977,  Learning Rate: 0.00041, Train Gradient: 10.1\n",
      "Epoch 135501/150000, Loss: 761, Val Loss: 976,  Learning Rate: 0.00041, Train Gradient: 10.1\n",
      "Epoch 135601/150000, Loss: 759, Val Loss: 974,  Learning Rate: 0.00041, Train Gradient: 10.1\n",
      "Epoch 135701/150000, Loss: 757, Val Loss: 973,  Learning Rate: 0.00041, Train Gradient: 10.1\n",
      "Epoch 135801/150000, Loss: 756, Val Loss: 971,  Learning Rate: 0.00041, Train Gradient: 10.0\n",
      "Epoch 135901/150000, Loss: 754, Val Loss: 970,  Learning Rate: 0.00041, Train Gradient: 10.0\n",
      "Epoch 136001/150000, Loss: 752, Val Loss: 968,  Learning Rate: 0.00041, Train Gradient: 10.0\n",
      "Epoch 136101/150000, Loss: 751, Val Loss: 967,  Learning Rate: 0.00041, Train Gradient: 10.0\n",
      "Epoch 136201/150000, Loss: 749, Val Loss: 965,  Learning Rate: 0.00041, Train Gradient: 10.0\n",
      "Epoch 136301/150000, Loss: 748, Val Loss: 964,  Learning Rate: 0.00041, Train Gradient: 9.9\n",
      "Epoch 136401/150000, Loss: 746, Val Loss: 962,  Learning Rate: 0.00041, Train Gradient: 9.9\n",
      "Epoch 136501/150000, Loss: 744, Val Loss: 961,  Learning Rate: 0.00041, Train Gradient: 9.9\n",
      "Epoch 136601/150000, Loss: 743, Val Loss: 959,  Learning Rate: 0.00041, Train Gradient: 9.9\n",
      "Epoch 136701/150000, Loss: 741, Val Loss: 958,  Learning Rate: 0.00041, Train Gradient: 9.9\n",
      "Epoch 136801/150000, Loss: 740, Val Loss: 956,  Learning Rate: 0.00041, Train Gradient: 9.9\n",
      "Epoch 136901/150000, Loss: 738, Val Loss: 955,  Learning Rate: 0.00041, Train Gradient: 9.9\n",
      "Epoch 137001/150000, Loss: 736, Val Loss: 953,  Learning Rate: 0.00041, Train Gradient: 9.8\n",
      "Epoch 137101/150000, Loss: 735, Val Loss: 952,  Learning Rate: 0.00041, Train Gradient: 9.8\n",
      "Epoch 137201/150000, Loss: 733, Val Loss: 950,  Learning Rate: 0.00041, Train Gradient: 9.8\n",
      "Epoch 137301/150000, Loss: 732, Val Loss: 949,  Learning Rate: 0.00041, Train Gradient: 9.8\n",
      "Epoch 137401/150000, Loss: 730, Val Loss: 947,  Learning Rate: 0.00041, Train Gradient: 9.8\n",
      "Epoch 137501/150000, Loss: 728, Val Loss: 946,  Learning Rate: 0.00041, Train Gradient: 9.8\n",
      "Epoch 137601/150000, Loss: 727, Val Loss: 944,  Learning Rate: 0.00041, Train Gradient: 9.8\n",
      "Epoch 137701/150000, Loss: 725, Val Loss: 943,  Learning Rate: 0.00041, Train Gradient: 9.7\n",
      "Epoch 137801/150000, Loss: 724, Val Loss: 941,  Learning Rate: 0.00041, Train Gradient: 9.7\n",
      "Epoch 137901/150000, Loss: 722, Val Loss: 940,  Learning Rate: 0.00041, Train Gradient: 9.7\n",
      "Epoch 138001/150000, Loss: 721, Val Loss: 938,  Learning Rate: 0.00041, Train Gradient: 9.7\n",
      "Epoch 138101/150000, Loss: 719, Val Loss: 937,  Learning Rate: 0.00041, Train Gradient: 9.7\n",
      "Epoch 138201/150000, Loss: 717, Val Loss: 936,  Learning Rate: 0.00041, Train Gradient: 9.7\n",
      "Epoch 138301/150000, Loss: 716, Val Loss: 934,  Learning Rate: 0.00041, Train Gradient: 9.6\n",
      "Epoch 138401/150000, Loss: 714, Val Loss: 933,  Learning Rate: 0.00041, Train Gradient: 9.6\n",
      "Epoch 138501/150000, Loss: 713, Val Loss: 931,  Learning Rate: 0.00041, Train Gradient: 9.6\n",
      "Epoch 138601/150000, Loss: 711, Val Loss: 930,  Learning Rate: 0.00041, Train Gradient: 9.6\n",
      "Epoch 138701/150000, Loss: 710, Val Loss: 928,  Learning Rate: 0.00041, Train Gradient: 9.6\n",
      "Epoch 138801/150000, Loss: 708, Val Loss: 927,  Learning Rate: 0.00041, Train Gradient: 9.6\n",
      "Epoch 138901/150000, Loss: 707, Val Loss: 926,  Learning Rate: 0.00041, Train Gradient: 9.5\n",
      "Epoch 139001/150000, Loss: 705, Val Loss: 924,  Learning Rate: 0.00041, Train Gradient: 9.5\n",
      "Epoch 139101/150000, Loss: 703, Val Loss: 923,  Learning Rate: 0.00041, Train Gradient: 9.5\n",
      "Epoch 139201/150000, Loss: 702, Val Loss: 921,  Learning Rate: 0.00041, Train Gradient: 9.5\n",
      "Epoch 139301/150000, Loss: 700, Val Loss: 920,  Learning Rate: 0.00041, Train Gradient: 9.5\n",
      "Epoch 139401/150000, Loss: 699, Val Loss: 918,  Learning Rate: 0.00041, Train Gradient: 9.5\n",
      "Epoch 139501/150000, Loss: 697, Val Loss: 917,  Learning Rate: 0.00041, Train Gradient: 9.4\n",
      "Epoch 139601/150000, Loss: 696, Val Loss: 916,  Learning Rate: 0.00041, Train Gradient: 9.4\n",
      "Epoch 139701/150000, Loss: 694, Val Loss: 914,  Learning Rate: 0.00041, Train Gradient: 9.4\n",
      "Epoch 139801/150000, Loss: 693, Val Loss: 913,  Learning Rate: 0.00041, Train Gradient: 9.4\n",
      "Epoch 139901/150000, Loss: 691, Val Loss: 911,  Learning Rate: 0.00041, Train Gradient: 9.4\n",
      "Epoch 140001/150000, Loss: 690, Val Loss: 910,  Learning Rate: 0.00041, Train Gradient: 9.4\n",
      "Epoch 140101/150000, Loss: 688, Val Loss: 909,  Learning Rate: 0.00041, Train Gradient: 9.4\n",
      "Epoch 140201/150000, Loss: 687, Val Loss: 907,  Learning Rate: 0.00041, Train Gradient: 9.3\n",
      "Epoch 140301/150000, Loss: 685, Val Loss: 906,  Learning Rate: 0.00041, Train Gradient: 9.3\n",
      "Epoch 140401/150000, Loss: 684, Val Loss: 904,  Learning Rate: 0.00041, Train Gradient: 9.3\n",
      "Epoch 140501/150000, Loss: 682, Val Loss: 903,  Learning Rate: 0.00041, Train Gradient: 9.3\n",
      "Epoch 140601/150000, Loss: 681, Val Loss: 902,  Learning Rate: 0.00041, Train Gradient: 9.3\n",
      "Epoch 140701/150000, Loss: 679, Val Loss: 900,  Learning Rate: 0.00041, Train Gradient: 9.3\n",
      "Epoch 140801/150000, Loss: 678, Val Loss: 899,  Learning Rate: 0.00041, Train Gradient: 9.2\n",
      "Epoch 140901/150000, Loss: 676, Val Loss: 897,  Learning Rate: 0.00041, Train Gradient: 9.2\n",
      "Epoch 141001/150000, Loss: 675, Val Loss: 896,  Learning Rate: 0.00041, Train Gradient: 9.2\n",
      "Epoch 141101/150000, Loss: 673, Val Loss: 895,  Learning Rate: 0.00041, Train Gradient: 9.2\n",
      "Epoch 141201/150000, Loss: 672, Val Loss: 893,  Learning Rate: 0.00041, Train Gradient: 9.2\n",
      "Epoch 141301/150000, Loss: 670, Val Loss: 892,  Learning Rate: 0.00041, Train Gradient: 9.2\n",
      "Epoch 141401/150000, Loss: 669, Val Loss: 891,  Learning Rate: 0.00041, Train Gradient: 9.1\n",
      "Epoch 141501/150000, Loss: 667, Val Loss: 889,  Learning Rate: 0.00041, Train Gradient: 9.1\n",
      "Epoch 141601/150000, Loss: 666, Val Loss: 888,  Learning Rate: 0.00041, Train Gradient: 9.1\n",
      "Epoch 141701/150000, Loss: 664, Val Loss: 887,  Learning Rate: 0.00041, Train Gradient: 9.1\n",
      "Epoch 141801/150000, Loss: 663, Val Loss: 885,  Learning Rate: 0.00041, Train Gradient: 9.1\n",
      "Epoch 141901/150000, Loss: 661, Val Loss: 884,  Learning Rate: 0.00041, Train Gradient: 9.1\n",
      "Epoch 142001/150000, Loss: 660, Val Loss: 882,  Learning Rate: 0.00041, Train Gradient: 9.1\n",
      "Epoch 142101/150000, Loss: 658, Val Loss: 881,  Learning Rate: 0.00041, Train Gradient: 9.0\n",
      "Epoch 142201/150000, Loss: 657, Val Loss: 880,  Learning Rate: 0.00041, Train Gradient: 9.0\n",
      "Epoch 142301/150000, Loss: 655, Val Loss: 878,  Learning Rate: 0.00041, Train Gradient: 9.0\n",
      "Epoch 142401/150000, Loss: 654, Val Loss: 877,  Learning Rate: 0.00041, Train Gradient: 9.0\n",
      "Epoch 142501/150000, Loss: 653, Val Loss: 876,  Learning Rate: 0.00041, Train Gradient: 9.0\n",
      "Epoch 142601/150000, Loss: 651, Val Loss: 874,  Learning Rate: 0.00041, Train Gradient: 9.0\n",
      "Epoch 142701/150000, Loss: 650, Val Loss: 873,  Learning Rate: 0.00041, Train Gradient: 8.9\n",
      "Epoch 142801/150000, Loss: 648, Val Loss: 872,  Learning Rate: 0.00041, Train Gradient: 8.9\n",
      "Epoch 142901/150000, Loss: 647, Val Loss: 870,  Learning Rate: 0.00041, Train Gradient: 8.9\n",
      "Epoch 143001/150000, Loss: 645, Val Loss: 869,  Learning Rate: 0.00041, Train Gradient: 8.9\n",
      "Epoch 143101/150000, Loss: 644, Val Loss: 868,  Learning Rate: 0.00041, Train Gradient: 8.9\n",
      "Epoch 143201/150000, Loss: 642, Val Loss: 866,  Learning Rate: 0.00041, Train Gradient: 8.9\n",
      "Epoch 143301/150000, Loss: 641, Val Loss: 865,  Learning Rate: 0.00041, Train Gradient: 8.9\n",
      "Epoch 143401/150000, Loss: 640, Val Loss: 864,  Learning Rate: 0.00041, Train Gradient: 8.8\n",
      "Epoch 143501/150000, Loss: 638, Val Loss: 862,  Learning Rate: 0.00041, Train Gradient: 8.8\n",
      "Epoch 143601/150000, Loss: 637, Val Loss: 861,  Learning Rate: 0.00041, Train Gradient: 8.8\n",
      "Epoch 143701/150000, Loss: 635, Val Loss: 860,  Learning Rate: 0.00041, Train Gradient: 8.8\n",
      "Epoch 143801/150000, Loss: 634, Val Loss: 859,  Learning Rate: 0.00041, Train Gradient: 8.8\n",
      "Epoch 143901/150000, Loss: 632, Val Loss: 857,  Learning Rate: 0.00041, Train Gradient: 8.8\n",
      "Epoch 144001/150000, Loss: 631, Val Loss: 856,  Learning Rate: 0.00041, Train Gradient: 8.8\n",
      "Epoch 144101/150000, Loss: 630, Val Loss: 855,  Learning Rate: 0.00041, Train Gradient: 8.7\n",
      "Epoch 144201/150000, Loss: 628, Val Loss: 853,  Learning Rate: 0.00041, Train Gradient: 8.7\n",
      "Epoch 144301/150000, Loss: 627, Val Loss: 852,  Learning Rate: 0.00041, Train Gradient: 8.7\n",
      "Epoch 144401/150000, Loss: 625, Val Loss: 851,  Learning Rate: 0.00041, Train Gradient: 8.7\n",
      "Epoch 144501/150000, Loss: 624, Val Loss: 849,  Learning Rate: 0.00041, Train Gradient: 8.7\n",
      "Epoch 144601/150000, Loss: 623, Val Loss: 848,  Learning Rate: 0.00041, Train Gradient: 8.7\n",
      "Epoch 144701/150000, Loss: 621, Val Loss: 847,  Learning Rate: 0.00041, Train Gradient: 8.6\n",
      "Epoch 144801/150000, Loss: 620, Val Loss: 846,  Learning Rate: 0.00041, Train Gradient: 8.6\n",
      "Epoch 144901/150000, Loss: 618, Val Loss: 844,  Learning Rate: 0.00041, Train Gradient: 8.6\n",
      "Epoch 145001/150000, Loss: 617, Val Loss: 843,  Learning Rate: 0.00041, Train Gradient: 8.6\n",
      "Epoch 145101/150000, Loss: 616, Val Loss: 842,  Learning Rate: 0.00041, Train Gradient: 8.6\n",
      "Epoch 145201/150000, Loss: 614, Val Loss: 840,  Learning Rate: 0.00041, Train Gradient: 8.6\n",
      "Epoch 145301/150000, Loss: 613, Val Loss: 839,  Learning Rate: 0.00041, Train Gradient: 8.6\n",
      "Epoch 145401/150000, Loss: 611, Val Loss: 838,  Learning Rate: 0.00041, Train Gradient: 8.6\n",
      "Epoch 145501/150000, Loss: 610, Val Loss: 837,  Learning Rate: 0.00041, Train Gradient: 8.5\n",
      "Epoch 145601/150000, Loss: 609, Val Loss: 835,  Learning Rate: 0.00041, Train Gradient: 8.5\n",
      "Epoch 145701/150000, Loss: 607, Val Loss: 834,  Learning Rate: 0.00041, Train Gradient: 8.5\n",
      "Epoch 145801/150000, Loss: 606, Val Loss: 833,  Learning Rate: 0.00041, Train Gradient: 8.5\n",
      "Epoch 145901/150000, Loss: 604, Val Loss: 832,  Learning Rate: 0.00041, Train Gradient: 8.5\n",
      "Epoch 146001/150000, Loss: 603, Val Loss: 830,  Learning Rate: 0.00041, Train Gradient: 8.5\n",
      "Epoch 146101/150000, Loss: 602, Val Loss: 829,  Learning Rate: 0.00041, Train Gradient: 8.4\n",
      "Epoch 146201/150000, Loss: 600, Val Loss: 828,  Learning Rate: 0.00041, Train Gradient: 8.4\n",
      "Epoch 146301/150000, Loss: 599, Val Loss: 827,  Learning Rate: 0.00041, Train Gradient: 8.4\n",
      "Epoch 146401/150000, Loss: 598, Val Loss: 825,  Learning Rate: 0.00041, Train Gradient: 8.4\n",
      "Epoch 146501/150000, Loss: 596, Val Loss: 824,  Learning Rate: 0.00041, Train Gradient: 8.4\n",
      "Epoch 146601/150000, Loss: 595, Val Loss: 823,  Learning Rate: 0.00041, Train Gradient: 8.4\n",
      "Epoch 146701/150000, Loss: 594, Val Loss: 822,  Learning Rate: 0.00041, Train Gradient: 8.4\n",
      "Epoch 146801/150000, Loss: 592, Val Loss: 820,  Learning Rate: 0.00041, Train Gradient: 8.3\n",
      "Epoch 146901/150000, Loss: 591, Val Loss: 819,  Learning Rate: 0.00041, Train Gradient: 8.3\n",
      "Epoch 147001/150000, Loss: 589, Val Loss: 818,  Learning Rate: 0.00041, Train Gradient: 8.3\n",
      "Epoch 147101/150000, Loss: 588, Val Loss: 817,  Learning Rate: 0.00041, Train Gradient: 8.3\n",
      "Epoch 147201/150000, Loss: 587, Val Loss: 815,  Learning Rate: 0.00041, Train Gradient: 8.3\n",
      "Epoch 147301/150000, Loss: 585, Val Loss: 814,  Learning Rate: 0.00041, Train Gradient: 8.3\n",
      "Epoch 147401/150000, Loss: 584, Val Loss: 813,  Learning Rate: 0.00041, Train Gradient: 8.3\n",
      "Epoch 147501/150000, Loss: 583, Val Loss: 812,  Learning Rate: 0.00041, Train Gradient: 8.3\n",
      "Epoch 147601/150000, Loss: 581, Val Loss: 811,  Learning Rate: 0.00041, Train Gradient: 8.2\n",
      "Epoch 147701/150000, Loss: 580, Val Loss: 809,  Learning Rate: 0.00041, Train Gradient: 8.2\n",
      "Epoch 147801/150000, Loss: 579, Val Loss: 808,  Learning Rate: 0.00041, Train Gradient: 8.2\n",
      "Epoch 147901/150000, Loss: 577, Val Loss: 807,  Learning Rate: 0.00041, Train Gradient: 8.2\n",
      "Epoch 148001/150000, Loss: 576, Val Loss: 806,  Learning Rate: 0.00041, Train Gradient: 8.2\n",
      "Epoch 148101/150000, Loss: 575, Val Loss: 805,  Learning Rate: 0.00041, Train Gradient: 8.2\n",
      "Epoch 148201/150000, Loss: 573, Val Loss: 803,  Learning Rate: 0.00041, Train Gradient: 8.1\n",
      "Epoch 148301/150000, Loss: 572, Val Loss: 802,  Learning Rate: 0.00041, Train Gradient: 8.1\n",
      "Epoch 148401/150000, Loss: 571, Val Loss: 801,  Learning Rate: 0.00041, Train Gradient: 8.1\n",
      "Epoch 148501/150000, Loss: 570, Val Loss: 800,  Learning Rate: 0.00041, Train Gradient: 8.1\n",
      "Epoch 148601/150000, Loss: 568, Val Loss: 799,  Learning Rate: 0.00041, Train Gradient: 8.1\n",
      "Epoch 148701/150000, Loss: 567, Val Loss: 797,  Learning Rate: 0.00041, Train Gradient: 8.1\n",
      "Epoch 148801/150000, Loss: 566, Val Loss: 796,  Learning Rate: 0.00041, Train Gradient: 8.1\n",
      "Epoch 148901/150000, Loss: 564, Val Loss: 795,  Learning Rate: 0.00041, Train Gradient: 8.1\n",
      "Epoch 149001/150000, Loss: 563, Val Loss: 794,  Learning Rate: 0.00041, Train Gradient: 8.0\n",
      "Epoch 149101/150000, Loss: 562, Val Loss: 793,  Learning Rate: 0.00041, Train Gradient: 8.0\n",
      "Epoch 149201/150000, Loss: 561, Val Loss: 792,  Learning Rate: 0.00041, Train Gradient: 8.0\n",
      "Epoch 149301/150000, Loss: 559, Val Loss: 791,  Learning Rate: 0.00041, Train Gradient: 8.0\n",
      "Epoch 149401/150000, Loss: 558, Val Loss: 789,  Learning Rate: 0.00041, Train Gradient: 8.0\n",
      "Epoch 149501/150000, Loss: 557, Val Loss: 788,  Learning Rate: 0.00041, Train Gradient: 8.0\n",
      "Epoch 149601/150000, Loss: 556, Val Loss: 787,  Learning Rate: 0.00041, Train Gradient: 8.0\n",
      "Epoch 149701/150000, Loss: 554, Val Loss: 786,  Learning Rate: 0.00041, Train Gradient: 7.9\n",
      "Epoch 149801/150000, Loss: 553, Val Loss: 785,  Learning Rate: 0.00041, Train Gradient: 7.9\n",
      "Epoch 149901/150000, Loss: 552, Val Loss: 784,  Learning Rate: 0.00041, Train Gradient: 7.9\n",
      "Test Loss: 766.4598999023438\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3]\n",
    "num_layers_list = [1]\n",
    "\n",
    "learning_rates = [0.01]\n",
    "factors = [0.9]     # learning_rates factor (Learning Rate Scheduling)\n",
    "patience_lr = [10]      # learning_rates patience\n",
    "\n",
    "window_sizes = [20]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, total_steps=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Learning Rate: {current_lr:.5f}, Train Gradient: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
