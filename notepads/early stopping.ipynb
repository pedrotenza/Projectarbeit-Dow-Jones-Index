{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7561,  0.1378, -0.1828],\n",
      "        [-1.6406,  0.1378, -0.1828],\n",
      "        [-0.2283,  0.1378, -0.1828],\n",
      "        ...,\n",
      "        [ 0.1148,  1.1836,  1.3851],\n",
      "        [ 0.3431,  1.1836,  1.3851],\n",
      "        [-0.7561,  1.1836,  1.3851]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "#data.tail(30)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "#data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "\n",
    "# Specify the columns you want to standardize\n",
    "columns_to_standardize = [\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Loop through the columns and standardize each one\n",
    "for column in columns_to_standardize:\n",
    "    data[column] = scaler.fit_transform(data[[column]])\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with a 'date' column containing dates from 1980-12-12 to 2023-07-31\n",
    "data2 = pd.DataFrame({'date': pd.date_range(start='1980-12-12', end='2023-07-31')})\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "data2['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Extract day, month, and year from the date column\n",
    "data['day'] = data2['date'].dt.day\n",
    "data['month'] = data2['date'].dt.month\n",
    "data['year'] = data2['date'].dt.year\n",
    "\n",
    "# Define the embedding dimensions\n",
    "embedding_dim = 1  # You can adjust this dimension as needed\n",
    "\n",
    "# Create embedding layers for day, month, and year\n",
    "day_embedding = nn.Embedding(32, embedding_dim)  # 0-31 days\n",
    "month_embedding = nn.Embedding(13, embedding_dim)  # 1-12 months\n",
    "year_embedding = nn.Embedding(44, embedding_dim)  # Embedding for years from 1980 to 2023\n",
    "\n",
    "# Convert day, month, and year to tensors with Long data type\n",
    "day_tensor = torch.LongTensor(data['day'].values)\n",
    "month_tensor = torch.LongTensor(data['month'].values)\n",
    "year_tensor = torch.LongTensor(data['year'].values - 1980)  # Convert years to an index from 0 to 43\n",
    "\n",
    "# Pass tensors through embedding layers to get embeddings\n",
    "day_embeddings = day_embedding(day_tensor)\n",
    "month_embeddings = month_embedding(month_tensor)\n",
    "year_embeddings = year_embedding(year_tensor)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "date_embeddings = torch.cat((day_embeddings, month_embeddings, year_embeddings), dim=1)\n",
    "\n",
    "# Print the resulting embeddings\n",
    "print(date_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_5416\\1537129541.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: Learning Rate=0.00015, Sequence Length=75, Batch Size=64, Input Size=7, Date Embedding Dim=3, Hidden Dim=128,Layers=4\n",
      "Epoch [1/3000], Loss: 33744.4141, Val Loss: 29866.3281\n",
      "Epoch [2/3000], Loss: 16527.3262, Val Loss: 28240.8496\n",
      "Epoch [3/3000], Loss: 9588.0049, Val Loss: 27642.2070\n",
      "Epoch [4/3000], Loss: 43813.3945, Val Loss: 27213.4062\n",
      "Epoch [5/3000], Loss: 32710.0078, Val Loss: 26833.2812\n",
      "Epoch [6/3000], Loss: 23562.0195, Val Loss: 26482.9648\n",
      "Epoch [7/3000], Loss: 16060.9521, Val Loss: 26152.6660\n",
      "Epoch [8/3000], Loss: 39110.1211, Val Loss: 25839.2559\n",
      "Epoch [9/3000], Loss: 23867.5410, Val Loss: 25537.1074\n",
      "Epoch [10/3000], Loss: 20783.2617, Val Loss: 25243.8281\n",
      "Epoch [11/3000], Loss: 29687.6895, Val Loss: 24964.5117\n",
      "Epoch [12/3000], Loss: 18693.6797, Val Loss: 24690.8301\n",
      "Epoch [13/3000], Loss: 18826.2207, Val Loss: 24429.1680\n",
      "Epoch [14/3000], Loss: 38780.3789, Val Loss: 24174.0391\n",
      "Epoch [15/3000], Loss: 21279.2637, Val Loss: 23924.3359\n",
      "Epoch [16/3000], Loss: 14589.7979, Val Loss: 23681.1680\n",
      "Epoch [17/3000], Loss: 17302.9043, Val Loss: 23446.0117\n",
      "Epoch [18/3000], Loss: 26098.6348, Val Loss: 23219.5762\n",
      "Epoch [19/3000], Loss: 29525.1289, Val Loss: 22997.5840\n",
      "Epoch [20/3000], Loss: 31061.8574, Val Loss: 22781.2344\n",
      "Epoch [21/3000], Loss: 28268.1270, Val Loss: 22571.0469\n",
      "Epoch [22/3000], Loss: 8471.8105, Val Loss: 22365.8984\n",
      "Epoch [23/3000], Loss: 5424.9502, Val Loss: 22167.0586\n",
      "Epoch [24/3000], Loss: 25382.6855, Val Loss: 21973.5039\n",
      "Epoch [25/3000], Loss: 8649.5488, Val Loss: 21785.3984\n",
      "Epoch [26/3000], Loss: 22734.8320, Val Loss: 21602.9785\n",
      "Epoch [27/3000], Loss: 14258.9756, Val Loss: 21424.9980\n",
      "Epoch [28/3000], Loss: 21825.6074, Val Loss: 21252.7402\n",
      "Epoch [29/3000], Loss: 33345.4805, Val Loss: 21084.8105\n",
      "Epoch [30/3000], Loss: 28665.6953, Val Loss: 20922.0176\n",
      "Epoch [31/3000], Loss: 21549.6445, Val Loss: 20762.7910\n",
      "Epoch [32/3000], Loss: 23448.2910, Val Loss: 20610.9941\n",
      "Epoch [33/3000], Loss: 18090.4121, Val Loss: 20461.7715\n",
      "Epoch [34/3000], Loss: 18319.2559, Val Loss: 20318.0664\n",
      "Epoch [35/3000], Loss: 37191.1680, Val Loss: 20177.7676\n",
      "Epoch [36/3000], Loss: 14338.2744, Val Loss: 20041.6465\n",
      "Epoch [37/3000], Loss: 33692.9844, Val Loss: 19911.7480\n",
      "Epoch [38/3000], Loss: 16876.8867, Val Loss: 19783.5137\n",
      "Epoch [39/3000], Loss: 23495.3555, Val Loss: 19663.4043\n",
      "Epoch [40/3000], Loss: 8025.6509, Val Loss: 19544.2793\n",
      "Epoch [41/3000], Loss: 11518.4590, Val Loss: 19429.1680\n",
      "Epoch [42/3000], Loss: 27418.4727, Val Loss: 19321.7246\n",
      "Epoch [43/3000], Loss: 28392.1660, Val Loss: 19215.6445\n",
      "Epoch [44/3000], Loss: 13529.7959, Val Loss: 19112.4043\n",
      "Epoch [45/3000], Loss: 26071.3789, Val Loss: 19015.1484\n",
      "Epoch [46/3000], Loss: 19036.8125, Val Loss: 18921.6172\n",
      "Epoch [47/3000], Loss: 17162.2930, Val Loss: 18831.2930\n",
      "Epoch [48/3000], Loss: 27928.0234, Val Loss: 18744.2383\n",
      "Epoch [49/3000], Loss: 22078.4082, Val Loss: 18660.1094\n",
      "Epoch [50/3000], Loss: 16938.6152, Val Loss: 18581.7383\n",
      "Epoch [51/3000], Loss: 17643.2168, Val Loss: 18505.9297\n",
      "Epoch [52/3000], Loss: 15484.3760, Val Loss: 17741.7969\n",
      "Epoch [53/3000], Loss: 6668.3721, Val Loss: 16873.4121\n",
      "Epoch [54/3000], Loss: 16222.1836, Val Loss: 16612.4766\n",
      "Epoch [55/3000], Loss: 13207.3555, Val Loss: 16387.0781\n",
      "Epoch [56/3000], Loss: 11895.0049, Val Loss: 16178.8447\n",
      "Epoch [57/3000], Loss: 31353.1348, Val Loss: 15984.9639\n",
      "Epoch [58/3000], Loss: 12072.4277, Val Loss: 15794.7979\n",
      "Epoch [59/3000], Loss: 26126.0000, Val Loss: 15613.3721\n",
      "Epoch [60/3000], Loss: 21797.9961, Val Loss: 15436.3945\n",
      "Epoch [61/3000], Loss: 17655.0449, Val Loss: 15263.0488\n",
      "Epoch [62/3000], Loss: 13135.7900, Val Loss: 15095.7246\n",
      "Epoch [63/3000], Loss: 12726.6562, Val Loss: 14930.7754\n",
      "Epoch [64/3000], Loss: 22602.3965, Val Loss: 14770.8271\n",
      "Epoch [65/3000], Loss: 15632.3506, Val Loss: 14612.8965\n",
      "Epoch [66/3000], Loss: 16920.5957, Val Loss: 14458.3965\n",
      "Epoch [67/3000], Loss: 27544.6055, Val Loss: 14307.2158\n",
      "Epoch [68/3000], Loss: 19227.5762, Val Loss: 14158.9541\n",
      "Epoch [69/3000], Loss: 27538.4277, Val Loss: 14013.5537\n",
      "Epoch [70/3000], Loss: 9104.9277, Val Loss: 13868.9531\n",
      "Epoch [71/3000], Loss: 6209.6631, Val Loss: 13729.3037\n",
      "Epoch [72/3000], Loss: 4781.7939, Val Loss: 13589.3164\n",
      "Epoch [73/3000], Loss: 10059.6055, Val Loss: 13453.2754\n",
      "Epoch [74/3000], Loss: 9420.2744, Val Loss: 13318.8691\n",
      "Epoch [75/3000], Loss: 2754.5847, Val Loss: 13186.0898\n",
      "Epoch [76/3000], Loss: 13737.5762, Val Loss: 13056.2549\n",
      "Epoch [77/3000], Loss: 12193.7041, Val Loss: 12927.2568\n",
      "Epoch [78/3000], Loss: 7599.8027, Val Loss: 12797.6455\n",
      "Epoch [79/3000], Loss: 15861.0195, Val Loss: 12673.7158\n",
      "Epoch [80/3000], Loss: 17376.1426, Val Loss: 12547.7314\n",
      "Epoch [81/3000], Loss: 27643.0234, Val Loss: 12425.5859\n",
      "Epoch [82/3000], Loss: 8154.2334, Val Loss: 12302.2070\n",
      "Epoch [83/3000], Loss: 8455.1582, Val Loss: 12183.9082\n",
      "Epoch [84/3000], Loss: 7286.8584, Val Loss: 12064.6016\n",
      "Epoch [85/3000], Loss: 13835.1924, Val Loss: 11947.4004\n",
      "Epoch [86/3000], Loss: 18891.5273, Val Loss: 11832.3623\n",
      "Epoch [87/3000], Loss: 9990.6377, Val Loss: 11716.9365\n",
      "Epoch [88/3000], Loss: 5625.7432, Val Loss: 11603.3291\n",
      "Epoch [89/3000], Loss: 4384.3467, Val Loss: 11492.0508\n",
      "Epoch [90/3000], Loss: 9118.0625, Val Loss: 11379.7617\n",
      "Epoch [91/3000], Loss: 21472.4590, Val Loss: 11270.7402\n",
      "Epoch [92/3000], Loss: 6717.2993, Val Loss: 11161.5107\n",
      "Epoch [93/3000], Loss: 17510.7695, Val Loss: 11054.6953\n",
      "Epoch [94/3000], Loss: 32906.1367, Val Loss: 10949.1602\n",
      "Epoch [95/3000], Loss: 14800.6826, Val Loss: 10842.0195\n",
      "Epoch [96/3000], Loss: 11272.4463, Val Loss: 10737.5986\n",
      "Epoch [97/3000], Loss: 9809.9912, Val Loss: 10633.5205\n",
      "Epoch [98/3000], Loss: 11416.7041, Val Loss: 10532.6445\n",
      "Epoch [99/3000], Loss: 5228.8257, Val Loss: 10432.1357\n",
      "Epoch [100/3000], Loss: 11517.6709, Val Loss: 10330.4941\n",
      "Epoch [101/3000], Loss: 19078.6602, Val Loss: 10232.3262\n",
      "Epoch [102/3000], Loss: 3412.8633, Val Loss: 10135.0371\n",
      "Epoch [103/3000], Loss: 11638.1152, Val Loss: 10035.4297\n",
      "Epoch [104/3000], Loss: 1245.5540, Val Loss: 9938.9854\n",
      "Epoch [105/3000], Loss: 17022.5898, Val Loss: 9843.8867\n",
      "Epoch [106/3000], Loss: 4569.2915, Val Loss: 9747.9375\n",
      "Epoch [107/3000], Loss: 9608.6865, Val Loss: 9653.9990\n",
      "Epoch [108/3000], Loss: 7317.3252, Val Loss: 9560.7783\n",
      "Epoch [109/3000], Loss: 3214.5034, Val Loss: 9467.6758\n",
      "Epoch [110/3000], Loss: 14156.7988, Val Loss: 9376.0684\n",
      "Epoch [111/3000], Loss: 12698.6816, Val Loss: 9284.7285\n",
      "Epoch [112/3000], Loss: 6728.3389, Val Loss: 9193.3887\n",
      "Epoch [113/3000], Loss: 17806.8789, Val Loss: 9103.4482\n",
      "Epoch [114/3000], Loss: 1740.1635, Val Loss: 9013.1328\n",
      "Epoch [115/3000], Loss: 14369.7168, Val Loss: 8924.7080\n",
      "Epoch [116/3000], Loss: 11573.5557, Val Loss: 8836.2480\n",
      "Epoch [117/3000], Loss: 11172.1680, Val Loss: 8749.0850\n",
      "Epoch [118/3000], Loss: 8744.0586, Val Loss: 8661.0479\n",
      "Epoch [119/3000], Loss: 9745.7646, Val Loss: 8574.4756\n",
      "Epoch [120/3000], Loss: 11015.1670, Val Loss: 8490.1045\n",
      "Epoch [121/3000], Loss: 3595.0588, Val Loss: 8404.4785\n",
      "Epoch [122/3000], Loss: 14339.5723, Val Loss: 8321.7236\n",
      "Epoch [123/3000], Loss: 8014.8413, Val Loss: 8238.4072\n",
      "Epoch [124/3000], Loss: 8508.6250, Val Loss: 8156.0542\n",
      "Epoch [125/3000], Loss: 8262.4199, Val Loss: 8075.3057\n",
      "Epoch [126/3000], Loss: 8758.3818, Val Loss: 7994.2007\n",
      "Epoch [127/3000], Loss: 7320.1543, Val Loss: 7914.4570\n",
      "Epoch [128/3000], Loss: 5459.6377, Val Loss: 7835.0806\n",
      "Epoch [129/3000], Loss: 16918.9766, Val Loss: 7756.9141\n",
      "Epoch [130/3000], Loss: 4946.6201, Val Loss: 7676.7788\n",
      "Epoch [131/3000], Loss: 7013.4375, Val Loss: 7599.0288\n",
      "Epoch [132/3000], Loss: 1621.6884, Val Loss: 7522.1406\n",
      "Epoch [133/3000], Loss: 8797.8398, Val Loss: 7444.9946\n",
      "Epoch [134/3000], Loss: 10866.6318, Val Loss: 7370.1025\n",
      "Epoch [135/3000], Loss: 8433.4443, Val Loss: 7294.4487\n",
      "Epoch [136/3000], Loss: 10093.9531, Val Loss: 7219.2983\n",
      "Epoch [137/3000], Loss: 9481.9180, Val Loss: 7145.5469\n",
      "Epoch [138/3000], Loss: 6821.3818, Val Loss: 7071.3442\n",
      "Epoch [139/3000], Loss: 2390.7249, Val Loss: 6997.0708\n",
      "Epoch [140/3000], Loss: 8169.1431, Val Loss: 6925.6016\n",
      "Epoch [141/3000], Loss: 5115.9326, Val Loss: 6853.5552\n",
      "Epoch [142/3000], Loss: 7722.3569, Val Loss: 6781.6851\n",
      "Epoch [143/3000], Loss: 2512.9478, Val Loss: 6710.3130\n",
      "Epoch [144/3000], Loss: 7376.1064, Val Loss: 6640.7598\n",
      "Epoch [145/3000], Loss: 5700.0278, Val Loss: 6569.9678\n",
      "Epoch [146/3000], Loss: 5934.3926, Val Loss: 6500.4204\n",
      "Epoch [147/3000], Loss: 1780.1732, Val Loss: 6430.7139\n",
      "Epoch [148/3000], Loss: 4662.3696, Val Loss: 6362.3975\n",
      "Epoch [149/3000], Loss: 15504.3994, Val Loss: 6295.0269\n",
      "Epoch [150/3000], Loss: 7547.8916, Val Loss: 6226.5796\n",
      "Epoch [151/3000], Loss: 11519.4668, Val Loss: 6160.2549\n",
      "Epoch [152/3000], Loss: 1704.0111, Val Loss: 6093.1177\n",
      "Epoch [153/3000], Loss: 3120.1155, Val Loss: 6026.9873\n",
      "Epoch [154/3000], Loss: 9038.7236, Val Loss: 5961.7329\n",
      "Epoch [155/3000], Loss: 8093.2109, Val Loss: 5896.7759\n",
      "Epoch [156/3000], Loss: 3505.0940, Val Loss: 5830.5840\n",
      "Epoch [157/3000], Loss: 7034.2915, Val Loss: 5767.3311\n",
      "Epoch [158/3000], Loss: 7010.5659, Val Loss: 5704.5132\n",
      "Epoch [159/3000], Loss: 2840.7112, Val Loss: 5641.0376\n",
      "Epoch [160/3000], Loss: 3708.4426, Val Loss: 5578.4600\n",
      "Epoch [161/3000], Loss: 7586.8647, Val Loss: 5517.0908\n",
      "Epoch [162/3000], Loss: 6204.5522, Val Loss: 5455.8584\n",
      "Epoch [163/3000], Loss: 2170.1689, Val Loss: 5394.5723\n",
      "Epoch [164/3000], Loss: 2936.5522, Val Loss: 5334.8672\n",
      "Epoch [165/3000], Loss: 3438.5481, Val Loss: 5274.9292\n",
      "Epoch [166/3000], Loss: 4045.0574, Val Loss: 5216.6196\n",
      "Epoch [167/3000], Loss: 2571.2834, Val Loss: 5156.7168\n",
      "Epoch [168/3000], Loss: 325.9258, Val Loss: 5098.6426\n",
      "Epoch [169/3000], Loss: 6072.5337, Val Loss: 5041.1318\n",
      "Epoch [170/3000], Loss: 4935.0942, Val Loss: 4983.0625\n",
      "Epoch [171/3000], Loss: 1263.8749, Val Loss: 4925.8877\n",
      "Epoch [172/3000], Loss: 99.3265, Val Loss: 4869.7388\n",
      "Epoch [173/3000], Loss: 3419.7319, Val Loss: 4814.5664\n",
      "Epoch [174/3000], Loss: 6723.7764, Val Loss: 4758.9824\n",
      "Epoch [175/3000], Loss: 7705.1172, Val Loss: 4704.2446\n",
      "Epoch [176/3000], Loss: 2339.5527, Val Loss: 4648.7588\n",
      "Epoch [177/3000], Loss: 3548.1802, Val Loss: 4594.3369\n",
      "Epoch [178/3000], Loss: 9826.8613, Val Loss: 4541.1675\n",
      "Epoch [179/3000], Loss: 1272.3646, Val Loss: 4487.0347\n",
      "Epoch [180/3000], Loss: 4349.0562, Val Loss: 4434.3477\n",
      "Epoch [181/3000], Loss: 4879.2656, Val Loss: 4381.8901\n",
      "Epoch [182/3000], Loss: 3384.3140, Val Loss: 4330.0049\n",
      "Epoch [183/3000], Loss: 4776.4756, Val Loss: 4278.3809\n",
      "Epoch [184/3000], Loss: 2742.4492, Val Loss: 4226.3726\n",
      "Epoch [185/3000], Loss: 11.7891, Val Loss: 4175.9390\n",
      "Epoch [186/3000], Loss: 0.9117, Val Loss: 4125.3027\n",
      "Epoch [187/3000], Loss: 3408.9705, Val Loss: 4075.6497\n",
      "Epoch [188/3000], Loss: 4498.1348, Val Loss: 4026.1287\n",
      "Epoch [189/3000], Loss: 712.6979, Val Loss: 3976.9888\n",
      "Epoch [190/3000], Loss: 3528.9390, Val Loss: 3928.2659\n",
      "Epoch [191/3000], Loss: 6236.4375, Val Loss: 3880.5051\n",
      "Epoch [192/3000], Loss: 2720.5845, Val Loss: 3833.2397\n",
      "Epoch [193/3000], Loss: 8372.8115, Val Loss: 3786.1165\n",
      "Epoch [194/3000], Loss: 3137.5757, Val Loss: 3738.4739\n",
      "Epoch [195/3000], Loss: 6910.1504, Val Loss: 3692.3962\n",
      "Epoch [196/3000], Loss: 1525.6376, Val Loss: 3646.1914\n",
      "Epoch [197/3000], Loss: 4045.3086, Val Loss: 3600.8345\n",
      "Epoch [198/3000], Loss: 6295.4854, Val Loss: 3555.2864\n",
      "Epoch [199/3000], Loss: 3371.6736, Val Loss: 3510.0078\n",
      "Epoch [200/3000], Loss: 1014.6490, Val Loss: 3466.1045\n",
      "Epoch [201/3000], Loss: 4881.6719, Val Loss: 3422.2649\n",
      "Epoch [202/3000], Loss: 4038.9141, Val Loss: 3378.8760\n",
      "Epoch [203/3000], Loss: 1755.4705, Val Loss: 3335.4197\n",
      "Epoch [204/3000], Loss: 3891.5154, Val Loss: 3292.7148\n",
      "Epoch [205/3000], Loss: 1144.7917, Val Loss: 3250.2466\n",
      "Epoch [206/3000], Loss: 7304.5591, Val Loss: 3209.0044\n",
      "Epoch [207/3000], Loss: 5620.4795, Val Loss: 3167.3101\n",
      "Epoch [208/3000], Loss: 4370.6323, Val Loss: 3125.5474\n",
      "Epoch [209/3000], Loss: 2364.0820, Val Loss: 3085.2285\n",
      "Epoch [210/3000], Loss: 1003.0934, Val Loss: 3045.2537\n",
      "Epoch [211/3000], Loss: 2096.4163, Val Loss: 3005.2168\n",
      "Epoch [212/3000], Loss: 4548.7231, Val Loss: 2966.1152\n",
      "Epoch [213/3000], Loss: 2092.6260, Val Loss: 2926.7136\n",
      "Epoch [214/3000], Loss: 0.7144, Val Loss: 2888.9734\n",
      "Epoch [215/3000], Loss: 131.5792, Val Loss: 2850.9089\n",
      "Epoch [216/3000], Loss: 2313.4192, Val Loss: 2812.8994\n",
      "Epoch [217/3000], Loss: 7904.0264, Val Loss: 2776.0229\n",
      "Epoch [218/3000], Loss: 703.5789, Val Loss: 2738.4700\n",
      "Epoch [219/3000], Loss: 2866.3628, Val Loss: 2701.5188\n",
      "Epoch [220/3000], Loss: 3613.1724, Val Loss: 2666.0193\n",
      "Epoch [221/3000], Loss: 6510.2837, Val Loss: 2630.2615\n",
      "Epoch [222/3000], Loss: 835.9491, Val Loss: 2594.7610\n",
      "Epoch [223/3000], Loss: 833.9946, Val Loss: 2558.7681\n",
      "Epoch [224/3000], Loss: 5853.2783, Val Loss: 2524.8530\n",
      "Epoch [225/3000], Loss: 1394.5132, Val Loss: 2490.1362\n",
      "Epoch [226/3000], Loss: 3670.7671, Val Loss: 2455.9949\n",
      "Epoch [227/3000], Loss: 2993.5300, Val Loss: 2421.8826\n",
      "Epoch [228/3000], Loss: 1795.2625, Val Loss: 2389.2957\n",
      "Epoch [229/3000], Loss: 81.0371, Val Loss: 2356.3477\n",
      "Epoch [230/3000], Loss: 2290.7778, Val Loss: 2323.4714\n",
      "Epoch [231/3000], Loss: 4672.9316, Val Loss: 2291.5691\n",
      "Epoch [232/3000], Loss: 1989.7283, Val Loss: 2259.2993\n",
      "Epoch [233/3000], Loss: 2824.8237, Val Loss: 2227.4985\n",
      "Epoch [234/3000], Loss: 2551.3103, Val Loss: 2196.2422\n",
      "Epoch [235/3000], Loss: 4752.0703, Val Loss: 2165.6758\n",
      "Epoch [236/3000], Loss: 904.2941, Val Loss: 2134.3074\n",
      "Epoch [237/3000], Loss: 3079.3921, Val Loss: 2104.3833\n",
      "Epoch [238/3000], Loss: 3907.0381, Val Loss: 2074.8203\n",
      "Epoch [239/3000], Loss: 3187.6318, Val Loss: 2044.2810\n",
      "Epoch [240/3000], Loss: 3207.5959, Val Loss: 2014.8339\n",
      "Epoch [241/3000], Loss: 2582.1763, Val Loss: 1985.3939\n",
      "Epoch [242/3000], Loss: 1167.0756, Val Loss: 1957.2548\n",
      "Epoch [243/3000], Loss: 534.0060, Val Loss: 1927.8077\n",
      "Epoch [244/3000], Loss: 2525.1797, Val Loss: 1899.7377\n",
      "Epoch [245/3000], Loss: 3290.7651, Val Loss: 1871.7405\n",
      "Epoch [246/3000], Loss: 5579.9707, Val Loss: 1844.4004\n",
      "Epoch [247/3000], Loss: 1434.5037, Val Loss: 1816.8276\n",
      "Epoch [248/3000], Loss: 912.6201, Val Loss: 1789.7734\n",
      "Epoch [249/3000], Loss: 2054.7947, Val Loss: 1762.8175\n",
      "Epoch [250/3000], Loss: 1804.2279, Val Loss: 1736.1110\n",
      "Epoch [251/3000], Loss: 324.6879, Val Loss: 1709.5710\n",
      "Epoch [252/3000], Loss: 2102.2141, Val Loss: 1684.1262\n",
      "Epoch [253/3000], Loss: 971.1348, Val Loss: 1658.3152\n",
      "Epoch [254/3000], Loss: 5987.3564, Val Loss: 1633.6931\n",
      "Epoch [255/3000], Loss: 83.2005, Val Loss: 1608.0717\n",
      "Epoch [256/3000], Loss: 52.2853, Val Loss: 1583.7178\n",
      "Epoch [257/3000], Loss: 1042.4021, Val Loss: 1559.0409\n",
      "Epoch [258/3000], Loss: 2388.3784, Val Loss: 1534.7218\n",
      "Epoch [259/3000], Loss: 919.4713, Val Loss: 1510.4127\n",
      "Epoch [260/3000], Loss: 3469.2007, Val Loss: 1487.3926\n",
      "Epoch [261/3000], Loss: 2069.0232, Val Loss: 1464.1844\n",
      "Epoch [262/3000], Loss: 1910.2676, Val Loss: 1441.0863\n",
      "Epoch [263/3000], Loss: 310.7841, Val Loss: 1417.8810\n",
      "Epoch [264/3000], Loss: 1764.8748, Val Loss: 1395.4639\n",
      "Epoch [265/3000], Loss: 442.6979, Val Loss: 1373.1864\n",
      "Epoch [266/3000], Loss: 1267.7384, Val Loss: 1351.4805\n",
      "Epoch [267/3000], Loss: 1369.0560, Val Loss: 1329.9053\n",
      "Epoch [268/3000], Loss: 2069.1707, Val Loss: 1308.5797\n",
      "Epoch [269/3000], Loss: 1307.7921, Val Loss: 1287.1617\n",
      "Epoch [270/3000], Loss: 1888.1278, Val Loss: 1266.2966\n",
      "Epoch [271/3000], Loss: 177.3205, Val Loss: 1245.1445\n",
      "Epoch [272/3000], Loss: 2361.6665, Val Loss: 1225.0333\n",
      "Epoch [273/3000], Loss: 471.9150, Val Loss: 1205.1918\n",
      "Epoch [274/3000], Loss: 0.5526, Val Loss: 1184.9456\n",
      "Epoch [275/3000], Loss: 2090.0542, Val Loss: 1165.2156\n",
      "Epoch [276/3000], Loss: 83.4820, Val Loss: 1145.1808\n",
      "Epoch [277/3000], Loss: 2852.9014, Val Loss: 1126.3097\n",
      "Epoch [278/3000], Loss: 405.5910, Val Loss: 1107.3752\n",
      "Epoch [279/3000], Loss: 215.1732, Val Loss: 1088.3722\n",
      "Epoch [280/3000], Loss: 296.0701, Val Loss: 1069.9385\n",
      "Epoch [281/3000], Loss: 0.7973, Val Loss: 1051.8130\n",
      "Epoch [282/3000], Loss: 1484.8634, Val Loss: 1034.1821\n",
      "Epoch [283/3000], Loss: 2.1583, Val Loss: 1016.0102\n",
      "Epoch [284/3000], Loss: 2096.0835, Val Loss: 998.8282\n",
      "Epoch [285/3000], Loss: 2271.4739, Val Loss: 981.4444\n",
      "Epoch [286/3000], Loss: 73.4292, Val Loss: 963.7288\n",
      "Epoch [287/3000], Loss: 2767.2952, Val Loss: 947.1716\n",
      "Epoch [288/3000], Loss: 283.5864, Val Loss: 930.1770\n",
      "Epoch [289/3000], Loss: 748.8694, Val Loss: 913.6783\n",
      "Epoch [290/3000], Loss: 1344.1615, Val Loss: 897.4684\n",
      "Epoch [291/3000], Loss: 300.3494, Val Loss: 881.5938\n",
      "Epoch [292/3000], Loss: 2358.4556, Val Loss: 865.7985\n",
      "Epoch [293/3000], Loss: 1449.3293, Val Loss: 850.5719\n",
      "Epoch [294/3000], Loss: 176.2890, Val Loss: 834.6903\n",
      "Epoch [295/3000], Loss: 2172.9814, Val Loss: 819.5743\n",
      "Epoch [296/3000], Loss: 751.9971, Val Loss: 804.7102\n",
      "Epoch [297/3000], Loss: 759.0752, Val Loss: 789.8078\n",
      "Epoch [298/3000], Loss: 1552.9740, Val Loss: 775.4931\n",
      "Epoch [299/3000], Loss: 1908.7305, Val Loss: 761.3682\n",
      "Epoch [300/3000], Loss: 1904.4583, Val Loss: 747.3138\n",
      "Epoch [301/3000], Loss: 412.2938, Val Loss: 733.3663\n",
      "Epoch [302/3000], Loss: 1130.3817, Val Loss: 719.5215\n",
      "Epoch [303/3000], Loss: 1389.2579, Val Loss: 706.0623\n",
      "Epoch [304/3000], Loss: 0.5264, Val Loss: 692.7759\n",
      "Epoch [305/3000], Loss: 10.4596, Val Loss: 679.7315\n",
      "Epoch [306/3000], Loss: 0.7374, Val Loss: 666.9681\n",
      "Epoch [307/3000], Loss: 276.0179, Val Loss: 654.1406\n",
      "Epoch [308/3000], Loss: 322.3630, Val Loss: 641.4893\n",
      "Epoch [309/3000], Loss: 235.6091, Val Loss: 629.7109\n",
      "Epoch [310/3000], Loss: 1680.1654, Val Loss: 617.9259\n",
      "Epoch [311/3000], Loss: 0.4564, Val Loss: 604.9603\n",
      "Epoch [312/3000], Loss: 323.6023, Val Loss: 593.2007\n",
      "Epoch [313/3000], Loss: 242.2534, Val Loss: 581.7592\n",
      "Epoch [314/3000], Loss: 1195.3549, Val Loss: 570.3096\n",
      "Epoch [315/3000], Loss: 155.8267, Val Loss: 558.8811\n",
      "Epoch [316/3000], Loss: 0.5184, Val Loss: 547.6356\n",
      "Epoch [317/3000], Loss: 0.6769, Val Loss: 536.7295\n",
      "Epoch [318/3000], Loss: 453.5711, Val Loss: 526.0760\n",
      "Epoch [319/3000], Loss: 1061.4030, Val Loss: 515.4225\n",
      "Epoch [320/3000], Loss: 2.0647, Val Loss: 504.8263\n",
      "Epoch [321/3000], Loss: 115.1180, Val Loss: 494.6321\n",
      "Epoch [322/3000], Loss: 938.4203, Val Loss: 484.4491\n",
      "Epoch [323/3000], Loss: 78.7377, Val Loss: 474.7552\n",
      "Epoch [324/3000], Loss: 317.9980, Val Loss: 464.6625\n",
      "Epoch [325/3000], Loss: 0.4102, Val Loss: 455.0680\n",
      "Epoch [326/3000], Loss: 194.7899, Val Loss: 446.2906\n",
      "Epoch [327/3000], Loss: 14.5605, Val Loss: 436.6774\n",
      "Epoch [328/3000], Loss: 107.8738, Val Loss: 427.0987\n",
      "Epoch [329/3000], Loss: 370.2547, Val Loss: 418.3129\n",
      "Epoch [330/3000], Loss: 353.3258, Val Loss: 409.1468\n",
      "Epoch [331/3000], Loss: 256.6901, Val Loss: 400.4982\n",
      "Epoch [332/3000], Loss: 10.7466, Val Loss: 391.6016\n",
      "Epoch [333/3000], Loss: 612.9460, Val Loss: 383.0898\n",
      "Epoch [334/3000], Loss: 236.5503, Val Loss: 374.8543\n",
      "Epoch [335/3000], Loss: 150.8062, Val Loss: 366.7275\n",
      "Epoch [336/3000], Loss: 1052.6866, Val Loss: 358.5912\n",
      "Epoch [337/3000], Loss: 936.8480, Val Loss: 350.6603\n",
      "Epoch [338/3000], Loss: 221.4762, Val Loss: 342.6295\n",
      "Epoch [339/3000], Loss: 112.1605, Val Loss: 335.1367\n",
      "Epoch [340/3000], Loss: 26.2220, Val Loss: 327.2922\n",
      "Epoch [341/3000], Loss: 528.5416, Val Loss: 320.0971\n",
      "Epoch [342/3000], Loss: 638.5682, Val Loss: 312.8604\n",
      "Epoch [343/3000], Loss: 0.4726, Val Loss: 305.7987\n",
      "Epoch [344/3000], Loss: 452.4311, Val Loss: 298.7520\n",
      "Epoch [345/3000], Loss: 1.1228, Val Loss: 291.9792\n",
      "Epoch [346/3000], Loss: 158.0442, Val Loss: 285.4384\n",
      "Epoch [347/3000], Loss: 3.7824, Val Loss: 278.7101\n",
      "Epoch [348/3000], Loss: 148.5021, Val Loss: 272.3448\n",
      "Epoch [349/3000], Loss: 435.6198, Val Loss: 266.0284\n",
      "Epoch [350/3000], Loss: 573.1867, Val Loss: 260.2586\n",
      "Epoch [351/3000], Loss: 17.5000, Val Loss: 253.9633\n",
      "Epoch [352/3000], Loss: 422.6564, Val Loss: 247.9247\n",
      "Epoch [353/3000], Loss: 378.6371, Val Loss: 242.1140\n",
      "Epoch [354/3000], Loss: 35.4456, Val Loss: 236.5660\n",
      "Epoch [355/3000], Loss: 1253.8538, Val Loss: 231.0096\n",
      "Epoch [356/3000], Loss: 0.5079, Val Loss: 225.6455\n",
      "Epoch [357/3000], Loss: 0.6165, Val Loss: 220.2216\n",
      "Epoch [358/3000], Loss: 231.3776, Val Loss: 215.0837\n",
      "Epoch [359/3000], Loss: 28.8997, Val Loss: 209.8953\n",
      "Epoch [360/3000], Loss: 1.5359, Val Loss: 205.0988\n",
      "Epoch [361/3000], Loss: 509.5450, Val Loss: 201.0897\n",
      "Epoch [362/3000], Loss: 322.6347, Val Loss: 195.8449\n",
      "Epoch [363/3000], Loss: 120.8619, Val Loss: 190.9574\n",
      "Epoch [364/3000], Loss: 99.9050, Val Loss: 187.0994\n",
      "Epoch [365/3000], Loss: 114.8750, Val Loss: 181.8917\n",
      "Epoch [366/3000], Loss: 33.3077, Val Loss: 177.6998\n",
      "Epoch [367/3000], Loss: 131.1892, Val Loss: 173.5773\n",
      "Epoch [368/3000], Loss: 167.3877, Val Loss: 170.0634\n",
      "Epoch [369/3000], Loss: 1.4283, Val Loss: 165.2306\n",
      "Epoch [370/3000], Loss: 1.8662, Val Loss: 161.1993\n",
      "Epoch [371/3000], Loss: 29.1326, Val Loss: 157.2311\n",
      "Epoch [372/3000], Loss: 2.9234, Val Loss: 153.7518\n",
      "Epoch [373/3000], Loss: 145.3103, Val Loss: 149.6793\n",
      "Epoch [374/3000], Loss: 193.5561, Val Loss: 146.2456\n",
      "Epoch [375/3000], Loss: 334.2906, Val Loss: 142.7608\n",
      "Epoch [376/3000], Loss: 173.3377, Val Loss: 139.3675\n",
      "Epoch [377/3000], Loss: 98.9477, Val Loss: 135.7171\n",
      "Epoch [378/3000], Loss: 171.2444, Val Loss: 132.6671\n",
      "Epoch [379/3000], Loss: 2.3200, Val Loss: 129.5539\n",
      "Epoch [380/3000], Loss: 192.7291, Val Loss: 126.2670\n",
      "Epoch [381/3000], Loss: 9.5981, Val Loss: 123.2749\n",
      "Epoch [382/3000], Loss: 27.2877, Val Loss: 120.2601\n",
      "Epoch [383/3000], Loss: 164.9140, Val Loss: 117.5499\n",
      "Epoch [384/3000], Loss: 273.3234, Val Loss: 114.7185\n",
      "Epoch [385/3000], Loss: 206.2516, Val Loss: 111.8358\n",
      "Epoch [386/3000], Loss: 1.1491, Val Loss: 109.3422\n",
      "Epoch [387/3000], Loss: 0.8792, Val Loss: 106.2372\n",
      "Epoch [388/3000], Loss: 642.3848, Val Loss: 103.6212\n",
      "Epoch [389/3000], Loss: 1.4217, Val Loss: 101.1487\n",
      "Epoch [390/3000], Loss: 353.3647, Val Loss: 98.4772\n",
      "Epoch [391/3000], Loss: 213.7970, Val Loss: 96.1453\n",
      "Epoch [392/3000], Loss: 801.5446, Val Loss: 94.2707\n",
      "Epoch [393/3000], Loss: 1.2583, Val Loss: 91.3070\n",
      "Epoch [394/3000], Loss: 173.0142, Val Loss: 89.0838\n",
      "Epoch [395/3000], Loss: 151.5281, Val Loss: 87.1665\n",
      "Epoch [396/3000], Loss: 1.1856, Val Loss: 84.7703\n",
      "Epoch [397/3000], Loss: 318.0731, Val Loss: 82.4984\n",
      "Epoch [398/3000], Loss: 16.4745, Val Loss: 80.4635\n",
      "Epoch [399/3000], Loss: 0.4967, Val Loss: 78.5883\n",
      "Epoch [400/3000], Loss: 38.7247, Val Loss: 76.3994\n",
      "Epoch [401/3000], Loss: 160.3543, Val Loss: 74.4494\n",
      "Epoch [402/3000], Loss: 0.7422, Val Loss: 72.6583\n",
      "Epoch [403/3000], Loss: 76.3159, Val Loss: 70.6244\n",
      "Epoch [404/3000], Loss: 74.9513, Val Loss: 68.7618\n",
      "Epoch [405/3000], Loss: 0.5438, Val Loss: 67.7176\n",
      "Epoch [406/3000], Loss: 433.2850, Val Loss: 65.3569\n",
      "Epoch [407/3000], Loss: 0.3633, Val Loss: 63.7423\n",
      "Epoch [408/3000], Loss: 0.5202, Val Loss: 61.8474\n",
      "Epoch [409/3000], Loss: 0.7235, Val Loss: 60.3635\n",
      "Epoch [410/3000], Loss: 2.6681, Val Loss: 58.7882\n",
      "Epoch [411/3000], Loss: 1.9884, Val Loss: 57.1991\n",
      "Epoch [412/3000], Loss: 102.1002, Val Loss: 56.1733\n",
      "Epoch [413/3000], Loss: 0.8585, Val Loss: 54.4197\n",
      "Epoch [414/3000], Loss: 41.1523, Val Loss: 53.0010\n",
      "Epoch [415/3000], Loss: 1.7269, Val Loss: 51.4869\n",
      "Epoch [416/3000], Loss: 193.0678, Val Loss: 50.1695\n",
      "Epoch [417/3000], Loss: 616.2087, Val Loss: 49.5904\n",
      "Epoch [418/3000], Loss: 0.5678, Val Loss: 47.7966\n",
      "Epoch [419/3000], Loss: 1.2684, Val Loss: 46.1871\n",
      "Epoch [420/3000], Loss: 40.3616, Val Loss: 44.9853\n",
      "Epoch [421/3000], Loss: 0.8041, Val Loss: 44.3076\n",
      "Epoch [422/3000], Loss: 235.0255, Val Loss: 42.5475\n",
      "Epoch [423/3000], Loss: 0.8819, Val Loss: 41.4718\n",
      "Epoch [424/3000], Loss: 47.2301, Val Loss: 43.7194\n",
      "Epoch [425/3000], Loss: 0.6775, Val Loss: 39.5027\n",
      "Epoch [426/3000], Loss: 7.9237, Val Loss: 38.5182\n",
      "Epoch [427/3000], Loss: 173.8116, Val Loss: 37.2037\n",
      "Epoch [428/3000], Loss: 35.7900, Val Loss: 36.7184\n",
      "Epoch [429/3000], Loss: 0.8010, Val Loss: 35.5279\n",
      "Epoch [430/3000], Loss: 331.1219, Val Loss: 34.3878\n",
      "Epoch [431/3000], Loss: 0.4851, Val Loss: 33.5094\n",
      "Epoch [432/3000], Loss: 3.3367, Val Loss: 32.9748\n",
      "Epoch [433/3000], Loss: 133.1474, Val Loss: 31.7705\n",
      "Epoch [434/3000], Loss: 1.0007, Val Loss: 30.7158\n",
      "Epoch [435/3000], Loss: 14.1480, Val Loss: 30.1832\n",
      "Epoch [436/3000], Loss: 3.8247, Val Loss: 29.5783\n",
      "Epoch [437/3000], Loss: 0.8477, Val Loss: 28.5439\n",
      "Epoch [438/3000], Loss: 0.6811, Val Loss: 28.3793\n",
      "Epoch [439/3000], Loss: 131.6313, Val Loss: 26.9112\n",
      "Epoch [440/3000], Loss: 1.1767, Val Loss: 26.7028\n",
      "Epoch [441/3000], Loss: 169.3477, Val Loss: 25.5539\n",
      "Epoch [442/3000], Loss: 0.4780, Val Loss: 24.7512\n",
      "Epoch [443/3000], Loss: 229.2414, Val Loss: 24.1608\n",
      "Epoch [444/3000], Loss: 33.1730, Val Loss: 23.4793\n",
      "Epoch [445/3000], Loss: 0.7217, Val Loss: 23.1656\n",
      "Epoch [446/3000], Loss: 1.8294, Val Loss: 22.3127\n",
      "Epoch [447/3000], Loss: 21.8940, Val Loss: 21.8357\n",
      "Epoch [448/3000], Loss: 0.7466, Val Loss: 21.0428\n",
      "Epoch [449/3000], Loss: 29.7390, Val Loss: 20.4425\n",
      "Epoch [450/3000], Loss: 90.7073, Val Loss: 20.6634\n",
      "Epoch [451/3000], Loss: 1.1290, Val Loss: 19.2926\n",
      "Epoch [452/3000], Loss: 1.2131, Val Loss: 19.1856\n",
      "Epoch [453/3000], Loss: 1.0065, Val Loss: 18.5355\n",
      "Epoch [454/3000], Loss: 1.2713, Val Loss: 18.1357\n",
      "Epoch [455/3000], Loss: 0.4270, Val Loss: 17.3853\n",
      "Epoch [456/3000], Loss: 131.8391, Val Loss: 17.2476\n",
      "Epoch [457/3000], Loss: 9.8347, Val Loss: 16.4250\n",
      "Epoch [458/3000], Loss: 1.1142, Val Loss: 16.0202\n",
      "Epoch [459/3000], Loss: 1.8641, Val Loss: 16.0613\n",
      "Epoch [460/3000], Loss: 0.8365, Val Loss: 15.6939\n",
      "Epoch [461/3000], Loss: 0.8878, Val Loss: 15.1385\n",
      "Epoch [462/3000], Loss: 1.0703, Val Loss: 14.4975\n",
      "Epoch [463/3000], Loss: 2.7723, Val Loss: 14.0918\n",
      "Epoch [464/3000], Loss: 1.5679, Val Loss: 14.1593\n",
      "Epoch [465/3000], Loss: 1.4610, Val Loss: 13.2954\n",
      "Epoch [466/3000], Loss: 13.6462, Val Loss: 13.2985\n",
      "Epoch [467/3000], Loss: 0.5581, Val Loss: 12.6514\n",
      "Epoch [468/3000], Loss: 159.9040, Val Loss: 12.6124\n",
      "Epoch [469/3000], Loss: 1.0360, Val Loss: 12.1888\n",
      "Epoch [470/3000], Loss: 1.2695, Val Loss: 12.5435\n",
      "Epoch [471/3000], Loss: 53.1784, Val Loss: 11.4808\n",
      "Epoch [472/3000], Loss: 0.7629, Val Loss: 11.4212\n",
      "Epoch [473/3000], Loss: 0.6048, Val Loss: 11.1630\n",
      "Epoch [474/3000], Loss: 0.9937, Val Loss: 10.9198\n",
      "Epoch [475/3000], Loss: 1.5841, Val Loss: 10.6332\n",
      "Epoch [476/3000], Loss: 6.2780, Val Loss: 10.6444\n",
      "Epoch [477/3000], Loss: 0.9630, Val Loss: 9.8856\n",
      "Epoch [478/3000], Loss: 1.3203, Val Loss: 9.5972\n",
      "Epoch [479/3000], Loss: 1.0065, Val Loss: 9.6391\n",
      "Epoch [480/3000], Loss: 3.3336, Val Loss: 9.3015\n",
      "Epoch [481/3000], Loss: 52.7141, Val Loss: 9.2332\n",
      "Epoch [482/3000], Loss: 0.7328, Val Loss: 9.0112\n",
      "Epoch [483/3000], Loss: 1.2086, Val Loss: 8.6492\n",
      "Epoch [484/3000], Loss: 0.7848, Val Loss: 8.3869\n",
      "Epoch [485/3000], Loss: 1.9121, Val Loss: 8.3903\n",
      "Epoch [486/3000], Loss: 1.7223, Val Loss: 8.6347\n",
      "Epoch [487/3000], Loss: 2.2674, Val Loss: 8.1537\n",
      "Epoch [488/3000], Loss: 1.6677, Val Loss: 8.1354\n",
      "Epoch [489/3000], Loss: 112.7628, Val Loss: 7.5227\n",
      "Epoch [490/3000], Loss: 38.8559, Val Loss: 7.3447\n",
      "Epoch [491/3000], Loss: 2.4309, Val Loss: 7.0201\n",
      "Epoch [492/3000], Loss: 0.8679, Val Loss: 7.3997\n",
      "Epoch [493/3000], Loss: 1.6818, Val Loss: 6.8232\n",
      "Epoch [494/3000], Loss: 0.2696, Val Loss: 6.9980\n",
      "Epoch [495/3000], Loss: 1.0106, Val Loss: 6.7780\n",
      "Epoch [496/3000], Loss: 0.5593, Val Loss: 6.3219\n",
      "Epoch [497/3000], Loss: 1.4093, Val Loss: 6.0503\n",
      "Epoch [498/3000], Loss: 25.4650, Val Loss: 6.1088\n",
      "Epoch [499/3000], Loss: 0.4807, Val Loss: 5.9566\n",
      "Epoch [500/3000], Loss: 1.3452, Val Loss: 5.8034\n",
      "Epoch [501/3000], Loss: 2.5156, Val Loss: 5.5935\n",
      "Epoch [502/3000], Loss: 0.6040, Val Loss: 5.5211\n",
      "Epoch [503/3000], Loss: 1.4599, Val Loss: 5.4788\n",
      "Epoch [504/3000], Loss: 0.4783, Val Loss: 5.4626\n",
      "Epoch [505/3000], Loss: 1.4892, Val Loss: 5.2610\n",
      "Epoch [506/3000], Loss: 0.9521, Val Loss: 5.0961\n",
      "Epoch [507/3000], Loss: 1.4770, Val Loss: 5.2568\n",
      "Epoch [508/3000], Loss: 0.5135, Val Loss: 5.0580\n",
      "Epoch [509/3000], Loss: 99.8465, Val Loss: 4.8980\n",
      "Epoch [510/3000], Loss: 2.0159, Val Loss: 4.9400\n",
      "Epoch [511/3000], Loss: 0.6111, Val Loss: 4.6995\n",
      "Epoch [512/3000], Loss: 1.6029, Val Loss: 4.8064\n",
      "Epoch [513/3000], Loss: 0.7582, Val Loss: 4.6499\n",
      "Epoch [514/3000], Loss: 0.7861, Val Loss: 5.5890\n",
      "Epoch [515/3000], Loss: 0.9243, Val Loss: 4.8217\n",
      "Epoch [516/3000], Loss: 2.1449, Val Loss: 4.2849\n",
      "Epoch [517/3000], Loss: 0.7306, Val Loss: 4.1414\n",
      "Epoch [518/3000], Loss: 0.4394, Val Loss: 4.1105\n",
      "Epoch [519/3000], Loss: 1.6347, Val Loss: 4.8113\n",
      "Epoch [520/3000], Loss: 31.5786, Val Loss: 4.2663\n",
      "Epoch [521/3000], Loss: 0.6039, Val Loss: 4.0643\n",
      "Epoch [522/3000], Loss: 4.5958, Val Loss: 3.8031\n",
      "Epoch [523/3000], Loss: 1.3034, Val Loss: 3.6902\n",
      "Epoch [524/3000], Loss: 24.0088, Val Loss: 3.9759\n",
      "Epoch [525/3000], Loss: 13.6370, Val Loss: 3.7195\n",
      "Epoch [526/3000], Loss: 8.6436, Val Loss: 3.8346\n",
      "Epoch [527/3000], Loss: 0.7569, Val Loss: 4.1080\n",
      "Epoch [528/3000], Loss: 1.8065, Val Loss: 4.0249\n",
      "Epoch [529/3000], Loss: 0.7316, Val Loss: 3.5741\n",
      "Epoch [530/3000], Loss: 0.8250, Val Loss: 3.1944\n",
      "Epoch [531/3000], Loss: 16.1726, Val Loss: 3.6711\n",
      "Epoch [532/3000], Loss: 0.7330, Val Loss: 3.3338\n",
      "Epoch [533/3000], Loss: 3.4987, Val Loss: 3.2133\n",
      "Epoch [534/3000], Loss: 2.5005, Val Loss: 2.9678\n",
      "Epoch [535/3000], Loss: 1.5404, Val Loss: 2.8988\n",
      "Epoch [536/3000], Loss: 2.2424, Val Loss: 3.6273\n",
      "Epoch [537/3000], Loss: 0.7373, Val Loss: 3.1811\n",
      "Epoch [538/3000], Loss: 4.2923, Val Loss: 2.9184\n",
      "Epoch [539/3000], Loss: 0.9711, Val Loss: 2.9834\n",
      "Epoch [540/3000], Loss: 0.7580, Val Loss: 2.7316\n",
      "Epoch [541/3000], Loss: 1.7671, Val Loss: 3.3469\n",
      "Epoch [542/3000], Loss: 6.7047, Val Loss: 2.8342\n",
      "Epoch [543/3000], Loss: 1.7040, Val Loss: 2.5680\n",
      "Epoch [544/3000], Loss: 0.6296, Val Loss: 2.5308\n",
      "Epoch [545/3000], Loss: 0.9927, Val Loss: 2.9446\n",
      "Epoch [546/3000], Loss: 0.9551, Val Loss: 2.4762\n",
      "Epoch [547/3000], Loss: 5.9229, Val Loss: 2.5071\n",
      "Epoch [548/3000], Loss: 1.8119, Val Loss: 2.5061\n",
      "Epoch [549/3000], Loss: 5.6969, Val Loss: 3.2144\n",
      "Epoch [550/3000], Loss: 0.8884, Val Loss: 2.4875\n",
      "Epoch [551/3000], Loss: 0.5702, Val Loss: 2.4538\n",
      "Epoch [552/3000], Loss: 0.9535, Val Loss: 2.4508\n",
      "Epoch [553/3000], Loss: 1.0523, Val Loss: 2.3454\n",
      "Epoch [554/3000], Loss: 0.3365, Val Loss: 2.4918\n",
      "Epoch [555/3000], Loss: 0.5331, Val Loss: 2.7119\n",
      "Epoch [556/3000], Loss: 37.1022, Val Loss: 2.1750\n",
      "Epoch [557/3000], Loss: 3.4879, Val Loss: 2.9594\n",
      "Epoch [558/3000], Loss: 7.4881, Val Loss: 3.0229\n",
      "Epoch [559/3000], Loss: 2.4832, Val Loss: 2.6764\n",
      "Epoch [560/3000], Loss: 4.9070, Val Loss: 2.3776\n",
      "Epoch [561/3000], Loss: 1.2626, Val Loss: 2.6138\n",
      "Epoch [562/3000], Loss: 22.0114, Val Loss: 1.9627\n",
      "Epoch [563/3000], Loss: 0.7180, Val Loss: 2.1880\n",
      "Epoch [564/3000], Loss: 8.0951, Val Loss: 2.0055\n",
      "Epoch [565/3000], Loss: 3.7692, Val Loss: 2.1267\n",
      "Epoch [566/3000], Loss: 1.1225, Val Loss: 1.9992\n",
      "Epoch [567/3000], Loss: 0.5942, Val Loss: 2.7288\n",
      "Epoch [568/3000], Loss: 30.3228, Val Loss: 1.8948\n",
      "Epoch [569/3000], Loss: 1.5969, Val Loss: 2.1268\n",
      "Epoch [570/3000], Loss: 0.8576, Val Loss: 1.9986\n",
      "Epoch [571/3000], Loss: 1.5056, Val Loss: 1.8978\n",
      "Epoch [572/3000], Loss: 1.0183, Val Loss: 1.9783\n",
      "Epoch [573/3000], Loss: 0.5806, Val Loss: 1.9338\n",
      "Epoch [574/3000], Loss: 3.5815, Val Loss: 2.4771\n",
      "Epoch [575/3000], Loss: 0.3577, Val Loss: 2.5048\n",
      "Epoch [576/3000], Loss: 0.5092, Val Loss: 1.9237\n",
      "Epoch [577/3000], Loss: 1.2496, Val Loss: 2.1188\n",
      "Epoch [578/3000], Loss: 0.3508, Val Loss: 1.8728\n",
      "Epoch [579/3000], Loss: 0.9088, Val Loss: 2.0569\n",
      "Epoch [580/3000], Loss: 1.4464, Val Loss: 2.8387\n",
      "Epoch [581/3000], Loss: 1.0163, Val Loss: 2.0240\n",
      "Epoch [582/3000], Loss: 0.7728, Val Loss: 1.8008\n",
      "Epoch [583/3000], Loss: 1.1294, Val Loss: 1.8649\n",
      "Epoch [584/3000], Loss: 1.1231, Val Loss: 2.1760\n",
      "Epoch [585/3000], Loss: 0.5951, Val Loss: 2.1641\n",
      "Epoch [586/3000], Loss: 4.7087, Val Loss: 2.0851\n",
      "Epoch [587/3000], Loss: 1.1763, Val Loss: 1.6420\n",
      "Epoch [588/3000], Loss: 1.1013, Val Loss: 2.3604\n",
      "Epoch [589/3000], Loss: 1.8641, Val Loss: 1.7022\n",
      "Epoch [590/3000], Loss: 6.3518, Val Loss: 1.6689\n",
      "Epoch [591/3000], Loss: 0.6739, Val Loss: 2.4769\n",
      "Epoch [592/3000], Loss: 1.0338, Val Loss: 1.6633\n",
      "Epoch [593/3000], Loss: 1.0869, Val Loss: 2.1151\n",
      "Epoch [594/3000], Loss: 5.6284, Val Loss: 1.8907\n",
      "Epoch [595/3000], Loss: 1.6334, Val Loss: 1.8615\n",
      "Epoch [596/3000], Loss: 2.5898, Val Loss: 1.6822\n",
      "Epoch [597/3000], Loss: 1.8567, Val Loss: 1.8797\n",
      "Early stopping after 597 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the x_train and y_train data\n",
    "x_train = data[['open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].to_numpy()\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Concatenate date embeddings with your testing data\n",
    "x_test_feature_tensors = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "x_test_date_embeddings = date_embeddings[len(x_train_scaled):]  # Use the remaining embeddings for testing data\n",
    "x_test_combined = torch.cat((x_test_feature_tensors, x_test_date_embeddings), dim=1)\n",
    "\n",
    "# Convert the combined testing data to PyTorch tensors\n",
    "x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inside your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size + date_embedding_dim  # Updated input size to include date embeddings\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Define the LSTM layer as a class attribute\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Ensure input has the shape [batch_size, sequence_length, input_size]\n",
    "        x = x.view(batch_size, 1, self.input_size)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 7  # Adjust based on the number of input features (excluding change_percent)\n",
    "date_embedding_dim = 3  # Adjust based on the dimension of your date embeddings\n",
    "output_size = 1\n",
    "hidden_dim = 128\n",
    "n_layers = 4\n",
    "sequence_length = 75  # Keep this as 1 for your input data\n",
    "batch_size = 64\n",
    "\n",
    "# Concatenate date embeddings with your feature vectors\n",
    "x_train_feature_tensors = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_train_date_embeddings = date_embeddings[:len(x_train_scaled)]  # Use the same length as your training data\n",
    "x_train_combined = torch.cat((x_train_feature_tensors, x_train_date_embeddings), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the combined data to PyTorch tensors\n",
    "#x_train_tensor = torch.tensor(x_train_combined, dtype=torch.float32)\n",
    "#y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Copy x_train_combined to create x_train_tensor\n",
    "x_train_tensor = x_train_combined.clone().detach()\n",
    "x_train_tensor = x_train_tensor.to(torch.float32)\n",
    "\n",
    "# Copy x_test_combined to create x_test_tensor\n",
    "x_test_tensor = x_test_combined.clone().detach()\n",
    "x_test_tensor = x_test_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "# Create y_train_tensor directly from y_train\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader for batch training\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.00015\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Hyperparameters: Learning Rate={learning_rate}, Sequence Length={sequence_length}, Batch Size={batch_size}, Input Size={input_size}, Date Embedding Dim={date_embedding_dim}, Hidden Dim={hidden_dim},'\n",
    "              f'Layers={n_layers}')\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3000\n",
    "\n",
    "best_val_loss = float('inf')  # Initialize with a very high value\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, batch_y.view(-1, 1))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Calculate validation loss\n",
    "        val_outputs = model(x_test_tensor)\n",
    "        val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))\n",
    "\n",
    "    # Check if validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        # Save the model checkpoint (optional)\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    # Check if training should be stopped\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f'Early stopping after {epoch+1} epochs without improvement.')\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 3557 but got size 3547 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\early stopping.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/early%20stopping.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m date_embedding_11_august_broadcasted \u001b[39m=\u001b[39m date_embedding_11_august\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand(\u001b[39m3547\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/early%20stopping.ipynb#W3sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Combine 'hidden_states' and 'date_embedding_11_august_broadcasted'\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/early%20stopping.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m combined_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((hidden_states, date_embedding_11_august_broadcasted), dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/early%20stopping.ipynb#W3sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# The resulting 'combined_states' will have a shape of [3547, 1, 32 + 76]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/early%20stopping.ipynb#W3sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/early%20stopping.ipynb#W3sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/early%20stopping.ipynb#W3sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# 4. Train a Random Forest Model:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/early%20stopping.ipynb#W3sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 3557 but got size 3547 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# 1. Feature extraction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    hidden_states, _ = model.lstm(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "    val_outputs = model(x_test_tensor)\n",
    "    y_test_predictions = val_outputs\n",
    "\n",
    "\n",
    "#2. generate date embeddings for the future date\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# Define the target date: 12th August\n",
    "target_date = \"11th August\"\n",
    "\n",
    "# Define the embedding dimensions (same as in your previous code)\n",
    "embedding_dim = 1\n",
    "\n",
    "# Define the maximum values for day, month, and year based on your previous code\n",
    "max_day = 31  # Maximum day\n",
    "max_month = 12  # Maximum month\n",
    "max_year = 43  # Maximum year (from 1980 to 2023)\n",
    "\n",
    "# Create one-hot encodings for day, month, and year\n",
    "day_encoding = torch.zeros(max_day)\n",
    "month_encoding = torch.zeros(max_month)\n",
    "year_encoding = torch.zeros(max_year + 1)  # +1 to account for the inclusive range\n",
    "\n",
    "# Map the target date to one-hot encoding\n",
    "# Extract the day and month from the target date\n",
    "day_index = int(target_date.split(\" \")[0].replace(\"th\", \"\")) - 1  # Extract the day from the target date\n",
    "month_index = 8  # August is the 9th month (0-based index)\n",
    "\n",
    "# Set the corresponding elements to 1\n",
    "day_encoding[day_index] = 1\n",
    "month_encoding[month_index] = 1\n",
    "year_encoding[43] = 1  # 43 corresponds to the year 2023 in your previous code\n",
    "\n",
    "# Concatenate the day, month, and year encodings to get the date embedding\n",
    "date_embedding_11_august = torch.cat((day_encoding, month_encoding, year_encoding), dim=0)\n",
    "\n",
    "\n",
    "\n",
    "#3. Combine LSTM Features with Date Embeddings\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# Assuming 'hidden_states' has shape [3547, 1, 32]\n",
    "# 'date_embedding_11_august' has shape [76]\n",
    "\n",
    "# Broadcast 'date_embedding_11_august' to match the shape of 'hidden_states'\n",
    "# This will repeat 'date_embedding_11_august' along the second dimension\n",
    "date_embedding_11_august_broadcasted = date_embedding_11_august.reshape(1, 1, -1).expand(3547, 1, -1)\n",
    "\n",
    "# Combine 'hidden_states' and 'date_embedding_11_august_broadcasted'\n",
    "combined_states = torch.cat((hidden_states, date_embedding_11_august_broadcasted), dim=2)\n",
    "\n",
    "# The resulting 'combined_states' will have a shape of [3547, 1, 32 + 76]\n",
    "\n",
    "\n",
    "# 4. Train a Random Forest Model:\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming 'y_test_predictions' contains the predicted 'close' values\n",
    "# You can extract them as a numpy array using .numpy() method\n",
    "close_values = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape the 'combined_states' to remove the extra dimension\n",
    "combined_states_2d = combined_states.reshape(-1, combined_states.shape[-1])\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust hyperparameters as needed\n",
    "\n",
    "# Fit the model on your data\n",
    "rf_model.fit(combined_states_2d, close_values)\n",
    "\n",
    "# Now the Random Forest model is trained and ready for predictions\n",
    "\n",
    "\n",
    "\n",
    "# 5. Evaluate the model\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'y_test_predictions' contains the predicted 'close' values\n",
    "# You can extract them as a numpy array using .numpy() method\n",
    "close_values = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape the 'combined_states' to remove the extra dimension\n",
    "combined_states_2d = combined_states.reshape(-1, combined_states.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_states_2d, close_values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust hyperparameters as needed\n",
    "\n",
    "# Fit the model on your training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print or use the evaluation metrics\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f'R-squared (R^2): {r2}')\n",
    "\n",
    "\n",
    "# 6. Make Predictions\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming 'hidden_states' contains the LSTM features\n",
    "# 'y_test_predictions' contains the predicted 'close' values\n",
    "\n",
    "# Define the target date: 11th August\n",
    "target_date = \"11th August\"\n",
    "\n",
    "# Define the embedding dimensions (same as in your previous code)\n",
    "embedding_dim = 1\n",
    "\n",
    "# Define the maximum values for day, month, and year based on your previous code\n",
    "max_day = 31  # Maximum day\n",
    "max_month = 12  # Maximum month\n",
    "max_year = 43  # Maximum year (from 1980 to 2023)\n",
    "\n",
    "# Create one-hot encodings for day, month, and year\n",
    "day_encoding = torch.zeros(max_day)\n",
    "month_encoding = torch.zeros(max_month)\n",
    "year_encoding = torch.zeros(max_year + 1)  # +1 to account for the inclusive range\n",
    "\n",
    "# Map the target date to one-hot encoding\n",
    "# Extract the day and month from the target date\n",
    "day_index = int(target_date.split(\" \")[0].replace(\"th\", \"\")) - 1  # Extract the day from the target date\n",
    "month_index = 8  # August is the 9th month (0-based index)\n",
    "\n",
    "# Set the corresponding elements to 1\n",
    "day_encoding[day_index] = 1\n",
    "month_encoding[month_index] = 1\n",
    "year_encoding[43] = 1  # 43 corresponds to the year 2023 in your previous code\n",
    "\n",
    "# Concatenate the day, month, and year encodings to get the date embedding\n",
    "date_embedding_11_august = torch.cat((day_encoding, month_encoding, year_encoding), dim=0)\n",
    "\n",
    "# Broadcast 'date_embedding_11_august' to match the shape of 'hidden_states'\n",
    "date_embedding_11_august_broadcasted = date_embedding_11_august.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "\n",
    "# Combine 'hidden_states' and 'date_embedding_11_august_broadcasted'\n",
    "combined_states = torch.cat((hidden_states, date_embedding_11_august_broadcasted), dim=2)\n",
    "\n",
    "# Extract 'y_test_predictions' as a numpy array\n",
    "close_values = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape the 'combined_states' to remove the extra dimension\n",
    "combined_states_2d = combined_states.reshape(-1, combined_states.shape[-1])\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model on your data\n",
    "rf_model.fit(combined_states_2d, close_values)\n",
    "\n",
    "# Now, the Random Forest model is trained and ready for predictions\n",
    "\n",
    "# Make a prediction for 11th August 2023\n",
    "predicted_close_11_august = rf_model.predict(combined_states_2d)\n",
    "\n",
    "# 'predicted_close_11_august' contains the predicted \"close\" value for 11th August 2023\n",
    "# Print the predicted \"close\" value for 11th August 2023\n",
    "print(\"Predicted Close Value for 11th August 2023:\", predicted_close_11_august)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
