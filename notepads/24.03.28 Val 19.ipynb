{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# x_train_tensor inverse\\n\\nx_test_original = scaler.inverse_transform(x_train_tensor.numpy())\\nprint(\"\\nFirst row of x_test_original:\")\\nprint(x_test_original[0])\\n\\nprint(\"\\nFirst row of x_train:\")\\nprint(x_train.head(1))\\n\\n\\n\\nprint(\"\\nLast row of x_test_original:\")\\nprint(x_test_original[-1])\\n\\nprint(\"\\nLast row of x_train:\")\\nprint(x_train.tail(1))\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# size of the window for data preparation\n",
    "split_window_size = 20\n",
    "\n",
    "# Initialize lists to store training and temporary sets\n",
    "x_train_list, y_train_list, x_temp_list, y_temp_list = [], [], [], []\n",
    "\n",
    "# Iterate through the data with the specified window size\n",
    "for i in range(0, len(x_data) - split_window_size, split_window_size + 1):\n",
    "    x_train_temp = x_data.iloc[i:i+split_window_size+1]\n",
    "    y_train_temp = y_data.iloc[i:i+split_window_size+1]\n",
    "\n",
    "    # Separate the last row for the temporary set\n",
    "    x_train = x_train_temp.iloc[:-1]\n",
    "    y_train = y_train_temp.iloc[:-1]\n",
    "\n",
    "    x_temp = x_train_temp.iloc[-1:]\n",
    "    y_temp = y_train_temp.iloc[-1:]\n",
    "\n",
    "    x_train_list.append(x_train)\n",
    "    y_train_list.append(y_train)\n",
    "    x_temp_list.append(x_temp)\n",
    "    y_temp_list.append(y_temp)\n",
    "\n",
    "# Concatenate the lists into pandas DataFrames\n",
    "x_train = pd.concat(x_train_list)\n",
    "y_train = pd.concat(y_train_list)\n",
    "x_temp = pd.concat(x_temp_list)\n",
    "y_temp = pd.concat(y_temp_list)\n",
    "\n",
    "# print(y_train.head(50))\n",
    "x_temp_train, x_temp_val, y_temp_train, y_temp_val = train_test_split(x_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Split x_temp and y_temp into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.001, window_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Training Loss: 20887.835165217624, Validation Loss: 21235.169921875\n",
      "Epoch [2/500], Training Loss: 16789.660814101077, Validation Loss: 18935.70703125\n",
      "Epoch [3/500], Training Loss: 13477.84038354132, Validation Loss: 14569.8212890625\n",
      "Epoch [4/500], Training Loss: 11228.910932389292, Validation Loss: 11913.177734375\n",
      "Epoch [5/500], Training Loss: 9570.47167468871, Validation Loss: 10585.619140625\n",
      "Epoch [6/500], Training Loss: 8354.538875011864, Validation Loss: 9552.6533203125\n",
      "Epoch [7/500], Training Loss: 7186.938264394703, Validation Loss: 11025.0478515625\n",
      "Epoch [8/500], Training Loss: 6201.4360563806185, Validation Loss: 8171.79443359375\n",
      "Epoch [9/500], Training Loss: 5309.7475636645, Validation Loss: 6545.21044921875\n",
      "Epoch [10/500], Training Loss: 4581.155635527036, Validation Loss: 5483.7919921875\n",
      "Epoch [11/500], Training Loss: 3955.173507548765, Validation Loss: 4715.3857421875\n",
      "Epoch [12/500], Training Loss: 3400.622610118315, Validation Loss: 4068.77978515625\n",
      "Epoch [13/500], Training Loss: 2918.2295796479616, Validation Loss: 3457.212890625\n",
      "Epoch [14/500], Training Loss: 2512.205346899428, Validation Loss: 2932.07275390625\n",
      "Epoch [15/500], Training Loss: 2185.2604391162986, Validation Loss: 2522.70361328125\n",
      "Epoch [16/500], Training Loss: 1908.0416697746277, Validation Loss: 2194.3935546875\n",
      "Epoch [17/500], Training Loss: 1669.4968669414072, Validation Loss: 1922.1754150390625\n",
      "Epoch [18/500], Training Loss: 1469.031797765014, Validation Loss: 1697.5374755859375\n",
      "Epoch [19/500], Training Loss: 1297.857904763173, Validation Loss: 1513.4569091796875\n",
      "Epoch [20/500], Training Loss: 1147.8511846853041, Validation Loss: 1369.0977783203125\n",
      "Epoch [21/500], Training Loss: 1019.2813644816387, Validation Loss: 1233.8948974609375\n",
      "Epoch [22/500], Training Loss: 908.4954779053444, Validation Loss: 1107.1895751953125\n",
      "Epoch [23/500], Training Loss: 810.7282435638108, Validation Loss: 991.67236328125\n",
      "Epoch [24/500], Training Loss: 722.2496144387896, Validation Loss: 885.7218627929688\n",
      "Epoch [25/500], Training Loss: 641.9444796269523, Validation Loss: 787.8152465820312\n",
      "Epoch [26/500], Training Loss: 568.9139850068425, Validation Loss: 699.1905517578125\n",
      "Epoch [27/500], Training Loss: 502.9730234923868, Validation Loss: 616.4789428710938\n",
      "Epoch [28/500], Training Loss: 443.7229130422448, Validation Loss: 549.1871948242188\n",
      "Epoch [29/500], Training Loss: 391.30561120994815, Validation Loss: 494.2538146972656\n",
      "Epoch [30/500], Training Loss: 344.52730196641306, Validation Loss: 440.4566345214844\n",
      "Epoch [31/500], Training Loss: 302.9765176283004, Validation Loss: 387.67828369140625\n",
      "Epoch [32/500], Training Loss: 266.40409738785223, Validation Loss: 337.3106994628906\n",
      "Epoch [33/500], Training Loss: 234.49932247482116, Validation Loss: 294.33660888671875\n",
      "Epoch [34/500], Training Loss: 206.83363324738949, Validation Loss: 260.1343688964844\n",
      "Epoch [35/500], Training Loss: 182.53239878809634, Validation Loss: 233.68812561035156\n",
      "Epoch [36/500], Training Loss: 160.8254718646309, Validation Loss: 220.51272583007812\n",
      "Epoch [37/500], Training Loss: 141.67006041188174, Validation Loss: 196.4586639404297\n",
      "Epoch [38/500], Training Loss: 125.92700267161302, Validation Loss: 171.6202392578125\n",
      "Epoch [39/500], Training Loss: 112.40876211979646, Validation Loss: 149.288818359375\n",
      "Epoch [40/500], Training Loss: 100.02700220084655, Validation Loss: 132.74752807617188\n",
      "Epoch [41/500], Training Loss: 88.45738204507539, Validation Loss: 119.71369934082031\n",
      "Epoch [42/500], Training Loss: 78.83336374167673, Validation Loss: 106.7110595703125\n",
      "Epoch [43/500], Training Loss: 70.43596397349643, Validation Loss: 95.13043975830078\n",
      "Epoch [44/500], Training Loss: 63.009477372865085, Validation Loss: 84.7498550415039\n",
      "Epoch [45/500], Training Loss: 56.4789459387461, Validation Loss: 76.26349639892578\n",
      "Epoch [46/500], Training Loss: 50.973895892813495, Validation Loss: 71.59278869628906\n",
      "Epoch [47/500], Training Loss: 46.44225237408695, Validation Loss: 69.0688247680664\n",
      "Epoch [48/500], Training Loss: 42.63073779384517, Validation Loss: 66.21761322021484\n",
      "Epoch [49/500], Training Loss: 39.42335084763303, Validation Loss: 65.05252075195312\n",
      "Epoch [50/500], Training Loss: 36.80380191700264, Validation Loss: 68.0934066772461\n",
      "Epoch [51/500], Training Loss: 34.31020967465826, Validation Loss: 69.8096694946289\n",
      "Epoch [52/500], Training Loss: 31.69549063629787, Validation Loss: 65.86323547363281\n",
      "Epoch [53/500], Training Loss: 29.124858491484652, Validation Loss: 60.05561447143555\n",
      "Epoch [54/500], Training Loss: 26.87153057999564, Validation Loss: 56.38121795654297\n",
      "Epoch [55/500], Training Loss: 24.85428021885459, Validation Loss: 55.46004867553711\n",
      "Epoch [56/500], Training Loss: 23.070093065995007, Validation Loss: 52.789955139160156\n",
      "Epoch [57/500], Training Loss: 21.411486038357946, Validation Loss: 48.34797668457031\n",
      "Epoch [58/500], Training Loss: 19.825309445708694, Validation Loss: 44.0018424987793\n",
      "Epoch [59/500], Training Loss: 18.420666980604487, Validation Loss: 39.706336975097656\n",
      "Epoch [60/500], Training Loss: 17.207300736608612, Validation Loss: 35.45756149291992\n",
      "Epoch [61/500], Training Loss: 16.15321125150636, Validation Loss: 32.298622131347656\n",
      "Epoch [62/500], Training Loss: 15.228714912867314, Validation Loss: 30.572229385375977\n",
      "Epoch [63/500], Training Loss: 14.408912290719284, Validation Loss: 30.31944465637207\n",
      "Epoch [64/500], Training Loss: 13.677450743578229, Validation Loss: 31.57818031311035\n",
      "Epoch [65/500], Training Loss: 13.020429705297653, Validation Loss: 34.079811096191406\n",
      "Epoch [66/500], Training Loss: 12.428590439674007, Validation Loss: 37.009666442871094\n",
      "Epoch [67/500], Training Loss: 11.89910152831415, Validation Loss: 39.39930725097656\n",
      "Epoch [68/500], Training Loss: 11.43224279529853, Validation Loss: 40.72686767578125\n",
      "Epoch [69/500], Training Loss: 11.023093674908072, Validation Loss: 41.01075744628906\n",
      "Epoch [70/500], Training Loss: 10.658542249028581, Validation Loss: 40.648338317871094\n",
      "Epoch [71/500], Training Loss: 10.323368762039248, Validation Loss: 40.095245361328125\n",
      "Epoch [72/500], Training Loss: 10.00212930999974, Validation Loss: 39.62478256225586\n",
      "Epoch [73/500], Training Loss: 9.682008450147205, Validation Loss: 39.39003372192383\n",
      "Epoch [74/500], Training Loss: 9.356608425448398, Validation Loss: 39.46980285644531\n",
      "Epoch [75/500], Training Loss: 9.027299718880597, Validation Loss: 39.8956298828125\n",
      "Epoch [76/500], Training Loss: 8.70086170656558, Validation Loss: 40.66826248168945\n",
      "Epoch [77/500], Training Loss: 8.387455322208837, Validation Loss: 41.72350311279297\n",
      "Epoch [78/500], Training Loss: 8.096431984527365, Validation Loss: 42.8720703125\n",
      "Epoch [79/500], Training Loss: 7.8352097990386005, Validation Loss: 43.84396743774414\n",
      "Epoch [80/500], Training Loss: 7.607127140605033, Validation Loss: 44.50040054321289\n",
      "Epoch [81/500], Training Loss: 7.409469063107805, Validation Loss: 44.99391555786133\n",
      "Epoch [82/500], Training Loss: 7.234130572572103, Validation Loss: 45.519775390625\n",
      "Epoch [83/500], Training Loss: 7.074055309903366, Validation Loss: 46.08169174194336\n",
      "Epoch [84/500], Training Loss: 6.929623710291889, Validation Loss: 46.583919525146484\n",
      "Epoch [85/500], Training Loss: 6.809208279564429, Validation Loss: 46.970314025878906\n",
      "Epoch [86/500], Training Loss: 6.719933456363431, Validation Loss: 47.27994918823242\n",
      "Epoch [87/500], Training Loss: 6.660474831780942, Validation Loss: 47.616607666015625\n",
      "Epoch [88/500], Training Loss: 6.6219983909486935, Validation Loss: 47.98362350463867\n",
      "Epoch [89/500], Training Loss: 6.591837806843075, Validation Loss: 48.2343864440918\n",
      "Epoch [90/500], Training Loss: 6.559217293351766, Validation Loss: 48.234188079833984\n",
      "Epoch [91/500], Training Loss: 6.519368322679671, Validation Loss: 48.00391387939453\n",
      "Epoch [92/500], Training Loss: 6.472857651690937, Validation Loss: 47.64934158325195\n",
      "Epoch [93/500], Training Loss: 6.422613089209821, Validation Loss: 47.25991439819336\n",
      "Epoch [94/500], Training Loss: 6.372322152481744, Validation Loss: 46.8653564453125\n",
      "Epoch [95/500], Training Loss: 6.325662846059649, Validation Loss: 46.46751022338867\n",
      "Epoch [96/500], Training Loss: 6.286062383026177, Validation Loss: 46.06413269042969\n",
      "Epoch [97/500], Training Loss: 6.256379837789818, Validation Loss: 45.65687942504883\n",
      "Epoch [98/500], Training Loss: 6.2394238417921235, Validation Loss: 45.232872009277344\n",
      "Epoch [99/500], Training Loss: 6.237309429365367, Validation Loss: 44.79051971435547\n",
      "Epoch [100/500], Training Loss: 6.252346180115514, Validation Loss: 44.33967208862305\n",
      "Epoch [101/500], Training Loss: 6.285836016928252, Validation Loss: 43.893714904785156\n",
      "Epoch [102/500], Training Loss: 6.338657122279353, Validation Loss: 43.44957733154297\n",
      "Epoch [103/500], Training Loss: 6.408096418911402, Validation Loss: 42.85364532470703\n",
      "Epoch [104/500], Training Loss: 6.484443807519279, Validation Loss: 41.69214630126953\n",
      "Epoch [105/500], Training Loss: 6.551085251330014, Validation Loss: 39.93538284301758\n",
      "Epoch [106/500], Training Loss: 6.5968304457518725, Validation Loss: 38.077735900878906\n",
      "Epoch [107/500], Training Loss: 6.62539240152067, Validation Loss: 36.40850067138672\n",
      "Epoch [108/500], Training Loss: 6.642901182669622, Validation Loss: 35.00843048095703\n",
      "Epoch [109/500], Training Loss: 6.651537687185313, Validation Loss: 33.876976013183594\n",
      "Epoch [110/500], Training Loss: 6.648897565979075, Validation Loss: 32.961944580078125\n",
      "Epoch [111/500], Training Loss: 6.627929544798379, Validation Loss: 32.197669982910156\n",
      "Epoch [112/500], Training Loss: 6.580413473875689, Validation Loss: 31.593740463256836\n",
      "Epoch [113/500], Training Loss: 6.5065721394872185, Validation Loss: 31.192121505737305\n",
      "Epoch [114/500], Training Loss: 6.421226867800417, Validation Loss: 30.980432510375977\n",
      "Epoch [115/500], Training Loss: 6.343242118921948, Validation Loss: 30.905900955200195\n",
      "Epoch [116/500], Training Loss: 6.282780597586337, Validation Loss: 30.90469741821289\n",
      "Epoch [117/500], Training Loss: 6.240566098390004, Validation Loss: 30.931062698364258\n",
      "Epoch [118/500], Training Loss: 6.21278584608776, Validation Loss: 30.9533748626709\n",
      "Epoch [119/500], Training Loss: 6.1942438233564285, Validation Loss: 30.975793838500977\n",
      "Epoch [120/500], Training Loss: 6.180556749184242, Validation Loss: 31.039997100830078\n",
      "Epoch [121/500], Training Loss: 6.168517319753922, Validation Loss: 31.18923568725586\n",
      "Epoch [122/500], Training Loss: 6.156131947079465, Validation Loss: 31.449071884155273\n",
      "Epoch [123/500], Training Loss: 6.143448554433339, Validation Loss: 31.830270767211914\n",
      "Epoch [124/500], Training Loss: 6.13262362016251, Validation Loss: 32.32078552246094\n",
      "Epoch [125/500], Training Loss: 6.127906414367104, Validation Loss: 32.895320892333984\n",
      "Epoch [126/500], Training Loss: 6.1351451142141755, Validation Loss: 33.511287689208984\n",
      "Epoch [127/500], Training Loss: 6.161465769654419, Validation Loss: 34.121726989746094\n",
      "Epoch [128/500], Training Loss: 6.216782158049732, Validation Loss: 34.66584014892578\n",
      "Epoch [129/500], Training Loss: 6.314261976212931, Validation Loss: 35.05228805541992\n",
      "Epoch [130/500], Training Loss: 6.4677179665709925, Validation Loss: 35.06291580200195\n",
      "Epoch [131/500], Training Loss: 6.675729601491928, Validation Loss: 34.43352127075195\n",
      "Epoch [132/500], Training Loss: 6.899825068660695, Validation Loss: 33.31706619262695\n",
      "Epoch [133/500], Training Loss: 7.07884151672116, Validation Loss: 32.30313491821289\n",
      "Epoch [134/500], Training Loss: 7.184363338023052, Validation Loss: 31.56612777709961\n",
      "Epoch [135/500], Training Loss: 7.235746090444619, Validation Loss: 31.283931732177734\n",
      "Epoch [136/500], Training Loss: 7.226211575766017, Validation Loss: 30.9512882232666\n",
      "Epoch [137/500], Training Loss: 7.1517734312981, Validation Loss: 30.800161361694336\n",
      "Epoch [138/500], Training Loss: 7.042469321957438, Validation Loss: 30.901845932006836\n",
      "Epoch [139/500], Training Loss: 6.923613211081943, Validation Loss: 31.119915008544922\n",
      "Epoch [140/500], Training Loss: 6.796728426029305, Validation Loss: 31.461563110351562\n",
      "Epoch [141/500], Training Loss: 6.658270426690445, Validation Loss: 31.98774528503418\n",
      "Epoch [142/500], Training Loss: 6.508330343015258, Validation Loss: 32.80330276489258\n",
      "Epoch [143/500], Training Loss: 6.348672591565435, Validation Loss: 33.99817657470703\n",
      "Epoch [144/500], Training Loss: 6.179552810044466, Validation Loss: 35.656864166259766\n",
      "Epoch [145/500], Training Loss: 6.002907366184353, Validation Loss: 37.858333587646484\n",
      "Epoch [146/500], Training Loss: 5.8229957787611335, Validation Loss: 40.59493637084961\n",
      "Epoch [147/500], Training Loss: 5.645920968997685, Validation Loss: 43.747806549072266\n",
      "Epoch [148/500], Training Loss: 5.476785588366492, Validation Loss: 47.05539321899414\n",
      "Epoch [149/500], Training Loss: 5.316741325941829, Validation Loss: 50.153255462646484\n",
      "Epoch [150/500], Training Loss: 5.164704740470139, Validation Loss: 52.48408126831055\n",
      "Epoch [151/500], Training Loss: 5.020720545640305, Validation Loss: 53.46207046508789\n",
      "Epoch [152/500], Training Loss: 4.887039898240547, Validation Loss: 52.719486236572266\n",
      "Epoch [153/500], Training Loss: 4.7679325124601055, Validation Loss: 50.52203369140625\n",
      "Epoch [154/500], Training Loss: 4.667416282972871, Validation Loss: 47.76242446899414\n",
      "Epoch [155/500], Training Loss: 4.58485759194795, Validation Loss: 45.22422409057617\n",
      "Epoch [156/500], Training Loss: 4.517101500552265, Validation Loss: 43.21723937988281\n",
      "Epoch [157/500], Training Loss: 4.460607900685504, Validation Loss: 41.762908935546875\n",
      "Epoch [158/500], Training Loss: 4.4117579301489345, Validation Loss: 40.733333587646484\n",
      "Epoch [159/500], Training Loss: 4.367188240876947, Validation Loss: 39.96492385864258\n",
      "Epoch [160/500], Training Loss: 4.3240079632809, Validation Loss: 39.3030891418457\n",
      "Epoch [161/500], Training Loss: 4.27899814076837, Validation Loss: 38.616050720214844\n",
      "Epoch [162/500], Training Loss: 4.229437905222585, Validation Loss: 37.848915100097656\n",
      "Epoch [163/500], Training Loss: 4.174740231987459, Validation Loss: 36.98333740234375\n",
      "Epoch [164/500], Training Loss: 4.116571477166405, Validation Loss: 35.99120330810547\n",
      "Epoch [165/500], Training Loss: 4.058734927234714, Validation Loss: 34.85396957397461\n",
      "Epoch [166/500], Training Loss: 4.006383858437697, Validation Loss: 33.55099868774414\n",
      "Epoch [167/500], Training Loss: 3.964888810022917, Validation Loss: 32.06848907470703\n",
      "Epoch [168/500], Training Loss: 3.9406217328658415, Validation Loss: 30.69670867919922\n",
      "Epoch [169/500], Training Loss: 3.936709608066159, Validation Loss: 30.046825408935547\n",
      "Epoch [170/500], Training Loss: 3.951322077058441, Validation Loss: 30.477100372314453\n",
      "Epoch [171/500], Training Loss: 3.9783052839760997, Validation Loss: 31.59149742126465\n",
      "Epoch [172/500], Training Loss: 4.013438160200392, Validation Loss: 33.16801452636719\n",
      "Epoch [173/500], Training Loss: 4.056084715119321, Validation Loss: 35.39533233642578\n",
      "Epoch [174/500], Training Loss: 4.107595458985032, Validation Loss: 38.30504608154297\n",
      "Epoch [175/500], Training Loss: 4.167945652619632, Validation Loss: 41.38596725463867\n",
      "Epoch [176/500], Training Loss: 4.231040740165572, Validation Loss: 43.99456787109375\n",
      "Epoch [177/500], Training Loss: 4.290115577988386, Validation Loss: 45.78760528564453\n",
      "Epoch [178/500], Training Loss: 4.340976561881397, Validation Loss: 46.72151184082031\n",
      "Epoch [179/500], Training Loss: 4.382644578341084, Validation Loss: 46.920021057128906\n",
      "Epoch [180/500], Training Loss: 4.413704604172487, Validation Loss: 46.5941276550293\n",
      "Epoch [181/500], Training Loss: 4.431634350405803, Validation Loss: 45.7844352722168\n",
      "Epoch [182/500], Training Loss: 4.435789759889996, Validation Loss: 44.3911018371582\n",
      "Epoch [183/500], Training Loss: 4.4284353637447635, Validation Loss: 42.49768829345703\n",
      "Epoch [184/500], Training Loss: 4.4140447694508795, Validation Loss: 40.331295013427734\n",
      "Epoch [185/500], Training Loss: 4.396834935025048, Validation Loss: 38.120853424072266\n",
      "Epoch [186/500], Training Loss: 4.37950558419037, Validation Loss: 36.067359924316406\n",
      "Epoch [187/500], Training Loss: 4.363062461612348, Validation Loss: 34.303287506103516\n",
      "Epoch [188/500], Training Loss: 4.347274036657516, Validation Loss: 32.86404800415039\n",
      "Epoch [189/500], Training Loss: 4.331896146669827, Validation Loss: 31.708675384521484\n",
      "Epoch [190/500], Training Loss: 4.316498314033238, Validation Loss: 30.776168823242188\n",
      "Epoch [191/500], Training Loss: 4.300643090023951, Validation Loss: 30.009681701660156\n",
      "Epoch [192/500], Training Loss: 4.28404873078633, Validation Loss: 29.362146377563477\n",
      "Epoch [193/500], Training Loss: 4.266711141772551, Validation Loss: 28.807546615600586\n",
      "Epoch [194/500], Training Loss: 4.249127216106622, Validation Loss: 28.332223892211914\n",
      "Epoch [195/500], Training Loss: 4.2315781581612875, Validation Loss: 27.9254207611084\n",
      "Epoch [196/500], Training Loss: 4.214539288968148, Validation Loss: 27.58420181274414\n",
      "Epoch [197/500], Training Loss: 4.198534588006789, Validation Loss: 27.312114715576172\n",
      "Epoch [198/500], Training Loss: 4.184111502767889, Validation Loss: 27.11385726928711\n",
      "Epoch [199/500], Training Loss: 4.171912973195325, Validation Loss: 26.992660522460938\n",
      "Epoch [200/500], Training Loss: 4.162836173605423, Validation Loss: 26.94728660583496\n",
      "Epoch [201/500], Training Loss: 4.158863678495356, Validation Loss: 26.96600914001465\n",
      "Epoch [202/500], Training Loss: 4.163556014722232, Validation Loss: 27.035978317260742\n",
      "Epoch [203/500], Training Loss: 4.18135853121348, Validation Loss: 27.16965675354004\n",
      "Epoch [204/500], Training Loss: 4.2149246373499185, Validation Loss: 27.402162551879883\n",
      "Epoch [205/500], Training Loss: 4.259153797509451, Validation Loss: 27.740413665771484\n",
      "Epoch [206/500], Training Loss: 4.299504708841553, Validation Loss: 28.10358428955078\n",
      "Epoch [207/500], Training Loss: 4.323567190055184, Validation Loss: 28.33329963684082\n",
      "Epoch [208/500], Training Loss: 4.332806888230379, Validation Loss: 28.25303840637207\n",
      "Epoch [209/500], Training Loss: 4.335272455405324, Validation Loss: 27.741424560546875\n",
      "Epoch [210/500], Training Loss: 4.33362120743646, Validation Loss: 26.832839965820312\n",
      "Epoch [211/500], Training Loss: 4.324631643549049, Validation Loss: 25.680458068847656\n",
      "Epoch [212/500], Training Loss: 4.304356173929409, Validation Loss: 24.45533561706543\n",
      "Epoch [213/500], Training Loss: 4.27237421347071, Validation Loss: 23.302587509155273\n",
      "Epoch [214/500], Training Loss: 4.231363250718417, Validation Loss: 22.310070037841797\n",
      "Epoch [215/500], Training Loss: 4.1850861399840475, Validation Loss: 21.49454689025879\n",
      "Epoch [216/500], Training Loss: 4.136186683421295, Validation Loss: 20.834463119506836\n",
      "Epoch [217/500], Training Loss: 4.085704162183856, Validation Loss: 20.311676025390625\n",
      "Epoch [218/500], Training Loss: 4.033862185357747, Validation Loss: 19.925954818725586\n",
      "Epoch [219/500], Training Loss: 3.980932515596291, Validation Loss: 19.676265716552734\n",
      "Epoch [220/500], Training Loss: 3.927417886293127, Validation Loss: 19.549659729003906\n",
      "Epoch [221/500], Training Loss: 3.873812535557601, Validation Loss: 19.516765594482422\n",
      "Epoch [222/500], Training Loss: 3.8205551742884443, Validation Loss: 19.552204132080078\n",
      "Epoch [223/500], Training Loss: 3.7678654709164254, Validation Loss: 19.6322078704834\n",
      "Epoch [224/500], Training Loss: 3.715867323079517, Validation Loss: 19.73458480834961\n",
      "Epoch [225/500], Training Loss: 3.6647038762099737, Validation Loss: 19.84131622314453\n",
      "Epoch [226/500], Training Loss: 3.614202752099821, Validation Loss: 19.91990089416504\n",
      "Epoch [227/500], Training Loss: 3.564524580954016, Validation Loss: 19.935178756713867\n",
      "Epoch [228/500], Training Loss: 3.5156613306541065, Validation Loss: 19.866222381591797\n",
      "Epoch [229/500], Training Loss: 3.467864408361653, Validation Loss: 19.716480255126953\n",
      "Epoch [230/500], Training Loss: 3.421448459791629, Validation Loss: 19.508934020996094\n",
      "Epoch [231/500], Training Loss: 3.3768231425105824, Validation Loss: 19.280698776245117\n",
      "Epoch [232/500], Training Loss: 3.3340017696893485, Validation Loss: 19.051544189453125\n",
      "Epoch [233/500], Training Loss: 3.2929758679700414, Validation Loss: 18.839656829833984\n",
      "Epoch [234/500], Training Loss: 3.253703980224685, Validation Loss: 18.652145385742188\n",
      "Epoch [235/500], Training Loss: 3.2161097555617766, Validation Loss: 18.481143951416016\n",
      "Epoch [236/500], Training Loss: 3.180209015474067, Validation Loss: 18.3304386138916\n",
      "Epoch [237/500], Training Loss: 3.1459150470864965, Validation Loss: 18.186763763427734\n",
      "Epoch [238/500], Training Loss: 3.1132223214914823, Validation Loss: 18.050107955932617\n",
      "Epoch [239/500], Training Loss: 3.082075011360282, Validation Loss: 17.91303825378418\n",
      "Epoch [240/500], Training Loss: 3.052444800119301, Validation Loss: 17.769987106323242\n",
      "Epoch [241/500], Training Loss: 3.0243756724617405, Validation Loss: 17.618467330932617\n",
      "Epoch [242/500], Training Loss: 2.9978259911268355, Validation Loss: 17.450193405151367\n",
      "Epoch [243/500], Training Loss: 2.9728241815800573, Validation Loss: 17.270648956298828\n",
      "Epoch [244/500], Training Loss: 2.9493616449197093, Validation Loss: 17.07162857055664\n",
      "Epoch [245/500], Training Loss: 2.9275166383627127, Validation Loss: 16.855831146240234\n",
      "Epoch [246/500], Training Loss: 2.907284103072691, Validation Loss: 16.624605178833008\n",
      "Epoch [247/500], Training Loss: 2.888630693459268, Validation Loss: 16.380136489868164\n",
      "Epoch [248/500], Training Loss: 2.8714566430505704, Validation Loss: 16.124387741088867\n",
      "Epoch [249/500], Training Loss: 2.8556621764568284, Validation Loss: 15.86392879486084\n",
      "Epoch [250/500], Training Loss: 2.841002884118705, Validation Loss: 15.59822940826416\n",
      "Epoch [251/500], Training Loss: 2.8272897398759693, Validation Loss: 15.32672119140625\n",
      "Epoch [252/500], Training Loss: 2.814359129073421, Validation Loss: 15.051234245300293\n",
      "Epoch [253/500], Training Loss: 2.8020724481080457, Validation Loss: 14.770147323608398\n",
      "Epoch [254/500], Training Loss: 2.7903628460031316, Validation Loss: 14.48542594909668\n",
      "Epoch [255/500], Training Loss: 2.7791521229856806, Validation Loss: 14.198850631713867\n",
      "Epoch [256/500], Training Loss: 2.7684092747971527, Validation Loss: 13.907790184020996\n",
      "Epoch [257/500], Training Loss: 2.7580949958546856, Validation Loss: 13.617607116699219\n",
      "Epoch [258/500], Training Loss: 2.7481849535017915, Validation Loss: 13.330459594726562\n",
      "Epoch [259/500], Training Loss: 2.7386396377960796, Validation Loss: 13.047357559204102\n",
      "Epoch [260/500], Training Loss: 2.72947168491033, Validation Loss: 12.770489692687988\n",
      "Epoch [261/500], Training Loss: 2.720656785375525, Validation Loss: 12.50031566619873\n",
      "Epoch [262/500], Training Loss: 2.71223533318711, Validation Loss: 12.242114067077637\n",
      "Epoch [263/500], Training Loss: 2.704184409326893, Validation Loss: 11.996865272521973\n",
      "Epoch [264/500], Training Loss: 2.696547283720613, Validation Loss: 11.768589973449707\n",
      "Epoch [265/500], Training Loss: 2.6893302030595283, Validation Loss: 11.557307243347168\n",
      "Epoch [266/500], Training Loss: 2.682610122128468, Validation Loss: 11.370491981506348\n",
      "Epoch [267/500], Training Loss: 2.6763860443221903, Validation Loss: 11.209797859191895\n",
      "Epoch [268/500], Training Loss: 2.6707148513788526, Validation Loss: 11.080873489379883\n",
      "Epoch [269/500], Training Loss: 2.6656190227666055, Validation Loss: 10.987363815307617\n",
      "Epoch [270/500], Training Loss: 2.6610568338023097, Validation Loss: 10.927358627319336\n",
      "Epoch [271/500], Training Loss: 2.6569863535326976, Validation Loss: 10.897382736206055\n",
      "Epoch [272/500], Training Loss: 2.6532294692396223, Validation Loss: 10.889010429382324\n",
      "Epoch [273/500], Training Loss: 2.6496050314837136, Validation Loss: 10.884810447692871\n",
      "Epoch [274/500], Training Loss: 2.646106841291807, Validation Loss: 10.876279830932617\n",
      "Epoch [275/500], Training Loss: 2.6428516335976755, Validation Loss: 10.860505104064941\n",
      "Epoch [276/500], Training Loss: 2.6398613029803, Validation Loss: 10.833460807800293\n",
      "Epoch [277/500], Training Loss: 2.6371608663743755, Validation Loss: 10.793947219848633\n",
      "Epoch [278/500], Training Loss: 2.6347570039920605, Validation Loss: 10.739056587219238\n",
      "Epoch [279/500], Training Loss: 2.6327516316972828, Validation Loss: 10.671343803405762\n",
      "Epoch [280/500], Training Loss: 2.631100682953836, Validation Loss: 10.589983940124512\n",
      "Epoch [281/500], Training Loss: 2.6298302431308627, Validation Loss: 10.496865272521973\n",
      "Epoch [282/500], Training Loss: 2.6289179020537823, Validation Loss: 10.39504337310791\n",
      "Epoch [283/500], Training Loss: 2.6283664599149934, Validation Loss: 10.28542709350586\n",
      "Epoch [284/500], Training Loss: 2.628208933503695, Validation Loss: 10.174718856811523\n",
      "Epoch [285/500], Training Loss: 2.6284188300737465, Validation Loss: 10.060470581054688\n",
      "Epoch [286/500], Training Loss: 2.628948293195215, Validation Loss: 9.952800750732422\n",
      "Epoch [287/500], Training Loss: 2.6298144707592437, Validation Loss: 9.851804733276367\n",
      "Epoch [288/500], Training Loss: 2.6310145742932765, Validation Loss: 9.760931968688965\n",
      "Epoch [289/500], Training Loss: 2.6325144847056507, Validation Loss: 9.683488845825195\n",
      "Epoch [290/500], Training Loss: 2.6342950995358505, Validation Loss: 9.622060775756836\n",
      "Epoch [291/500], Training Loss: 2.636375686259212, Validation Loss: 9.577507972717285\n",
      "Epoch [292/500], Training Loss: 2.6386929309964264, Validation Loss: 9.553123474121094\n",
      "Epoch [293/500], Training Loss: 2.6412644436327404, Validation Loss: 9.548407554626465\n",
      "Epoch [294/500], Training Loss: 2.644042360938355, Validation Loss: 9.56375789642334\n",
      "Epoch [295/500], Training Loss: 2.646990192318124, Validation Loss: 9.601825714111328\n",
      "Epoch [296/500], Training Loss: 2.650008370449401, Validation Loss: 9.662580490112305\n",
      "Epoch [297/500], Training Loss: 2.6530231893747835, Validation Loss: 9.745037078857422\n",
      "Epoch [298/500], Training Loss: 2.6558499592167086, Validation Loss: 9.846179008483887\n",
      "Epoch [299/500], Training Loss: 2.6583587486229683, Validation Loss: 9.964034080505371\n",
      "Epoch [300/500], Training Loss: 2.6603976623371834, Validation Loss: 10.091283798217773\n",
      "Epoch [301/500], Training Loss: 2.6617488117521204, Validation Loss: 10.219196319580078\n",
      "Epoch [302/500], Training Loss: 2.662250375226232, Validation Loss: 10.337964057922363\n",
      "Epoch [303/500], Training Loss: 2.661759845804212, Validation Loss: 10.43285083770752\n",
      "Epoch [304/500], Training Loss: 2.660260411790703, Validation Loss: 10.495802879333496\n",
      "Epoch [305/500], Training Loss: 2.657759625119113, Validation Loss: 10.521114349365234\n",
      "Epoch [306/500], Training Loss: 2.654402578045909, Validation Loss: 10.503423690795898\n",
      "Epoch [307/500], Training Loss: 2.65045873625904, Validation Loss: 10.437262535095215\n",
      "Epoch [308/500], Training Loss: 2.646092896405426, Validation Loss: 10.323829650878906\n",
      "Epoch [309/500], Training Loss: 2.641410109517303, Validation Loss: 10.160346031188965\n",
      "Epoch [310/500], Training Loss: 2.6364376707327475, Validation Loss: 9.948957443237305\n",
      "Epoch [311/500], Training Loss: 2.6311094704897697, Validation Loss: 9.694903373718262\n",
      "Epoch [312/500], Training Loss: 2.6253701381046537, Validation Loss: 9.403353691101074\n",
      "Epoch [313/500], Training Loss: 2.6192651353755427, Validation Loss: 9.085614204406738\n",
      "Epoch [314/500], Training Loss: 2.6127471895267145, Validation Loss: 8.758119583129883\n",
      "Epoch [315/500], Training Loss: 2.6056922288194126, Validation Loss: 8.440446853637695\n",
      "Epoch [316/500], Training Loss: 2.5980144279044444, Validation Loss: 8.141467094421387\n",
      "Epoch [317/500], Training Loss: 2.5896214058396168, Validation Loss: 7.870467662811279\n",
      "Epoch [318/500], Training Loss: 2.580505924970178, Validation Loss: 7.630800247192383\n",
      "Epoch [319/500], Training Loss: 2.5707234716459055, Validation Loss: 7.422977924346924\n",
      "Epoch [320/500], Training Loss: 2.560317523044053, Validation Loss: 7.246268272399902\n",
      "Epoch [321/500], Training Loss: 2.5493986810653295, Validation Loss: 7.098424434661865\n",
      "Epoch [322/500], Training Loss: 2.538050094342059, Validation Loss: 6.975701808929443\n",
      "Epoch [323/500], Training Loss: 2.5264067225332325, Validation Loss: 6.874268531799316\n",
      "Epoch [324/500], Training Loss: 2.5145814754425273, Validation Loss: 6.790056228637695\n",
      "Epoch [325/500], Training Loss: 2.502723935785299, Validation Loss: 6.7193379402160645\n",
      "Epoch [326/500], Training Loss: 2.490968735966712, Validation Loss: 6.658523082733154\n",
      "Epoch [327/500], Training Loss: 2.4794145596380095, Validation Loss: 6.604829788208008\n",
      "Epoch [328/500], Training Loss: 2.4682282465284593, Validation Loss: 6.556456565856934\n",
      "Epoch [329/500], Training Loss: 2.4574783330957612, Validation Loss: 6.513151168823242\n",
      "Epoch [330/500], Training Loss: 2.4472488899571574, Validation Loss: 6.473076820373535\n",
      "Epoch [331/500], Training Loss: 2.437597159568541, Validation Loss: 6.436390399932861\n",
      "Epoch [332/500], Training Loss: 2.4285935264750464, Validation Loss: 6.402047157287598\n",
      "Epoch [333/500], Training Loss: 2.4202784033027, Validation Loss: 6.370890140533447\n",
      "Epoch [334/500], Training Loss: 2.4126795995347923, Validation Loss: 6.342522144317627\n",
      "Epoch [335/500], Training Loss: 2.4058057131383626, Validation Loss: 6.3157267570495605\n",
      "Epoch [336/500], Training Loss: 2.3996736416051143, Validation Loss: 6.291462421417236\n",
      "Epoch [337/500], Training Loss: 2.3942641395716806, Validation Loss: 6.2691650390625\n",
      "Epoch [338/500], Training Loss: 2.3895835494962427, Validation Loss: 6.248580455780029\n",
      "Epoch [339/500], Training Loss: 2.385629390635123, Validation Loss: 6.229662895202637\n",
      "Epoch [340/500], Training Loss: 2.3823670500725695, Validation Loss: 6.21204948425293\n",
      "Epoch [341/500], Training Loss: 2.379773299878409, Validation Loss: 6.195369243621826\n",
      "Epoch [342/500], Training Loss: 2.377841549775503, Validation Loss: 6.180027484893799\n",
      "Epoch [343/500], Training Loss: 2.3765504268954873, Validation Loss: 6.164745807647705\n",
      "Epoch [344/500], Training Loss: 2.3758698206390934, Validation Loss: 6.151788234710693\n",
      "Epoch [345/500], Training Loss: 2.3757617772691995, Validation Loss: 6.141149520874023\n",
      "Epoch [346/500], Training Loss: 2.376217867097104, Validation Loss: 6.132752895355225\n",
      "Epoch [347/500], Training Loss: 2.3771760689582146, Validation Loss: 6.1259918212890625\n",
      "Epoch [348/500], Training Loss: 2.378600479405732, Validation Loss: 6.122366905212402\n",
      "Epoch [349/500], Training Loss: 2.380442407744417, Validation Loss: 6.122586727142334\n",
      "Epoch [350/500], Training Loss: 2.3826895664892205, Validation Loss: 6.125655174255371\n",
      "Epoch [351/500], Training Loss: 2.385293423838368, Validation Loss: 6.134064674377441\n",
      "Epoch [352/500], Training Loss: 2.3882124271181815, Validation Loss: 6.146292686462402\n",
      "Epoch [353/500], Training Loss: 2.3913850299666097, Validation Loss: 6.163763999938965\n",
      "Epoch [354/500], Training Loss: 2.3947936120449893, Validation Loss: 6.186395645141602\n",
      "Epoch [355/500], Training Loss: 2.3983868608672703, Validation Loss: 6.215890884399414\n",
      "Epoch [356/500], Training Loss: 2.4021089430055462, Validation Loss: 6.2517547607421875\n",
      "Epoch [357/500], Training Loss: 2.405892672790457, Validation Loss: 6.296319961547852\n",
      "Epoch [358/500], Training Loss: 2.4096806195697726, Validation Loss: 6.349874496459961\n",
      "Epoch [359/500], Training Loss: 2.413361576309648, Validation Loss: 6.416722774505615\n",
      "Epoch [360/500], Training Loss: 2.41688936250419, Validation Loss: 6.499246597290039\n",
      "Epoch [361/500], Training Loss: 2.4201638191424903, Validation Loss: 6.601077556610107\n",
      "Epoch [362/500], Training Loss: 2.423051053069511, Validation Loss: 6.727480888366699\n",
      "Epoch [363/500], Training Loss: 2.4254247791863817, Validation Loss: 6.883016109466553\n",
      "Epoch [364/500], Training Loss: 2.4272328877810203, Validation Loss: 7.069940090179443\n",
      "Epoch [365/500], Training Loss: 2.428397641383971, Validation Loss: 7.289968013763428\n",
      "Epoch [366/500], Training Loss: 2.428874780710345, Validation Loss: 7.54144811630249\n",
      "Epoch [367/500], Training Loss: 2.4287419554369833, Validation Loss: 7.8221354484558105\n",
      "Epoch [368/500], Training Loss: 2.428036581875323, Validation Loss: 8.126904487609863\n",
      "Epoch [369/500], Training Loss: 2.4268920561946254, Validation Loss: 8.451160430908203\n",
      "Epoch [370/500], Training Loss: 2.425416905199991, Validation Loss: 8.792325973510742\n",
      "Epoch [371/500], Training Loss: 2.423726554045977, Validation Loss: 9.145020484924316\n",
      "Epoch [372/500], Training Loss: 2.421872354170437, Validation Loss: 9.509233474731445\n",
      "Epoch [373/500], Training Loss: 2.419926135518521, Validation Loss: 9.88672924041748\n",
      "Epoch [374/500], Training Loss: 2.4179324413010765, Validation Loss: 10.278389930725098\n",
      "Epoch [375/500], Training Loss: 2.4159277430337043, Validation Loss: 10.6862154006958\n",
      "Epoch [376/500], Training Loss: 2.4139224990126404, Validation Loss: 11.118721961975098\n",
      "Epoch [377/500], Training Loss: 2.411937208240769, Validation Loss: 11.58158016204834\n",
      "Epoch [378/500], Training Loss: 2.4100207472080393, Validation Loss: 12.077556610107422\n",
      "Epoch [379/500], Training Loss: 2.4082012941997655, Validation Loss: 12.618059158325195\n",
      "Epoch [380/500], Training Loss: 2.406529166321905, Validation Loss: 13.208321571350098\n",
      "Epoch [381/500], Training Loss: 2.4050728421534777, Validation Loss: 13.8561429977417\n",
      "Epoch [382/500], Training Loss: 2.4039349379248205, Validation Loss: 14.563779830932617\n",
      "Epoch [383/500], Training Loss: 2.403219407103086, Validation Loss: 15.337485313415527\n",
      "Epoch [384/500], Training Loss: 2.403093337691519, Validation Loss: 16.171947479248047\n",
      "Epoch [385/500], Training Loss: 2.4037023459624365, Validation Loss: 17.06645965576172\n",
      "Epoch [386/500], Training Loss: 2.405176035249517, Validation Loss: 18.012386322021484\n",
      "Epoch [387/500], Training Loss: 2.407661780624143, Validation Loss: 19.001516342163086\n",
      "Epoch [388/500], Training Loss: 2.4112205658370636, Validation Loss: 20.019678115844727\n",
      "Epoch [389/500], Training Loss: 2.415889124734456, Validation Loss: 21.05069923400879\n",
      "Epoch [390/500], Training Loss: 2.4216127404025536, Validation Loss: 22.08257293701172\n",
      "Epoch [391/500], Training Loss: 2.4282708527954457, Validation Loss: 23.093461990356445\n",
      "Epoch [392/500], Training Loss: 2.435738392746065, Validation Loss: 24.060415267944336\n",
      "Epoch [393/500], Training Loss: 2.4438182132177664, Validation Loss: 24.966419219970703\n",
      "Epoch [394/500], Training Loss: 2.45226966257478, Validation Loss: 25.78791046142578\n",
      "Epoch [395/500], Training Loss: 2.46082067312971, Validation Loss: 26.50867462158203\n",
      "Epoch [396/500], Training Loss: 2.46922026855387, Validation Loss: 27.122020721435547\n",
      "Epoch [397/500], Training Loss: 2.477136585458731, Validation Loss: 27.61821174621582\n",
      "Epoch [398/500], Training Loss: 2.4843390734956303, Validation Loss: 28.004112243652344\n",
      "Epoch [399/500], Training Loss: 2.4905882686346956, Validation Loss: 28.287776947021484\n",
      "Epoch [400/500], Training Loss: 2.4957181247599536, Validation Loss: 28.486862182617188\n",
      "Epoch [401/500], Training Loss: 2.499591135366368, Validation Loss: 28.621185302734375\n",
      "Epoch [402/500], Training Loss: 2.5021291957711944, Validation Loss: 28.71510887145996\n",
      "Epoch [403/500], Training Loss: 2.5033321257931127, Validation Loss: 28.794723510742188\n",
      "Epoch [404/500], Training Loss: 2.5031670229293206, Validation Loss: 28.88656997680664\n",
      "Epoch [405/500], Training Loss: 2.5017220260058592, Validation Loss: 29.023303985595703\n",
      "Epoch [406/500], Training Loss: 2.4990830570403584, Validation Loss: 29.229869842529297\n",
      "Epoch [407/500], Training Loss: 2.4953145144219784, Validation Loss: 29.527698516845703\n",
      "Epoch [408/500], Training Loss: 2.490483621894509, Validation Loss: 29.924148559570312\n",
      "Epoch [409/500], Training Loss: 2.484531107111757, Validation Loss: 30.41714859008789\n",
      "Epoch [410/500], Training Loss: 2.477444364767043, Validation Loss: 30.985557556152344\n",
      "Epoch [411/500], Training Loss: 2.4692333939055406, Validation Loss: 31.59776496887207\n",
      "Epoch [412/500], Training Loss: 2.4600058575448522, Validation Loss: 32.217132568359375\n",
      "Epoch [413/500], Training Loss: 2.4500585983890435, Validation Loss: 32.81313705444336\n",
      "Epoch [414/500], Training Loss: 2.4397798374475674, Validation Loss: 33.35141372680664\n",
      "Epoch [415/500], Training Loss: 2.429548355979656, Validation Loss: 33.796485900878906\n",
      "Epoch [416/500], Training Loss: 2.4197689238245896, Validation Loss: 34.10568618774414\n",
      "Epoch [417/500], Training Loss: 2.4108086729783897, Validation Loss: 34.24408721923828\n",
      "Epoch [418/500], Training Loss: 2.402963192187414, Validation Loss: 34.20933151245117\n",
      "Epoch [419/500], Training Loss: 2.3963887547568814, Validation Loss: 34.029396057128906\n",
      "Epoch [420/500], Training Loss: 2.3911296789665406, Validation Loss: 33.75480270385742\n",
      "Epoch [421/500], Training Loss: 2.387076740275849, Validation Loss: 33.434810638427734\n",
      "Epoch [422/500], Training Loss: 2.3841406888868515, Validation Loss: 33.09584045410156\n",
      "Epoch [423/500], Training Loss: 2.38218145153915, Validation Loss: 32.76130294799805\n",
      "Epoch [424/500], Training Loss: 2.381094416379407, Validation Loss: 32.42979431152344\n",
      "Epoch [425/500], Training Loss: 2.3807643584715836, Validation Loss: 32.10893249511719\n",
      "Epoch [426/500], Training Loss: 2.381137278889095, Validation Loss: 31.80377960205078\n",
      "Epoch [427/500], Training Loss: 2.3821435969727998, Validation Loss: 31.512210845947266\n",
      "Epoch [428/500], Training Loss: 2.3837623455550405, Validation Loss: 31.235410690307617\n",
      "Epoch [429/500], Training Loss: 2.3859543207330445, Validation Loss: 30.97894859313965\n",
      "Epoch [430/500], Training Loss: 2.3887286482492645, Validation Loss: 30.742794036865234\n",
      "Epoch [431/500], Training Loss: 2.3920438226496636, Validation Loss: 30.52010726928711\n",
      "Epoch [432/500], Training Loss: 2.395930935020197, Validation Loss: 30.319377899169922\n",
      "Epoch [433/500], Training Loss: 2.4003827228410337, Validation Loss: 30.13584327697754\n",
      "Epoch [434/500], Training Loss: 2.405382117321688, Validation Loss: 29.97214126586914\n",
      "Epoch [435/500], Training Loss: 2.4109292424276036, Validation Loss: 29.822612762451172\n",
      "Epoch [436/500], Training Loss: 2.416997716927756, Validation Loss: 29.6854248046875\n",
      "Epoch [437/500], Training Loss: 2.423594105913035, Validation Loss: 29.559616088867188\n",
      "Epoch [438/500], Training Loss: 2.430650979046737, Validation Loss: 29.442283630371094\n",
      "Epoch [439/500], Training Loss: 2.4381808120169386, Validation Loss: 29.335105895996094\n",
      "Epoch [440/500], Training Loss: 2.446112845805015, Validation Loss: 29.233610153198242\n",
      "Epoch [441/500], Training Loss: 2.454419709229566, Validation Loss: 29.137083053588867\n",
      "Epoch [442/500], Training Loss: 2.463024362178149, Validation Loss: 29.042455673217773\n",
      "Epoch [443/500], Training Loss: 2.471891256443343, Validation Loss: 28.950590133666992\n",
      "Epoch [444/500], Training Loss: 2.4809123824025137, Validation Loss: 28.860139846801758\n",
      "Epoch [445/500], Training Loss: 2.4899938287792436, Validation Loss: 28.774372100830078\n",
      "Epoch [446/500], Training Loss: 2.499029698397144, Validation Loss: 28.69023323059082\n",
      "Epoch [447/500], Training Loss: 2.5078953387454934, Validation Loss: 28.605051040649414\n",
      "Epoch [448/500], Training Loss: 2.516463255551064, Validation Loss: 28.521284103393555\n",
      "Epoch [449/500], Training Loss: 2.5246174280302585, Validation Loss: 28.435949325561523\n",
      "Epoch [450/500], Training Loss: 2.5322376964686546, Validation Loss: 28.35214614868164\n",
      "Epoch [451/500], Training Loss: 2.539205547578024, Validation Loss: 28.268720626831055\n",
      "Epoch [452/500], Training Loss: 2.545435934071681, Validation Loss: 28.183963775634766\n",
      "Epoch [453/500], Training Loss: 2.5508664469815745, Validation Loss: 28.09720230102539\n",
      "Epoch [454/500], Training Loss: 2.555462764690532, Validation Loss: 28.011245727539062\n",
      "Epoch [455/500], Training Loss: 2.5592156731032993, Validation Loss: 27.92388916015625\n",
      "Epoch [456/500], Training Loss: 2.5621964563253115, Validation Loss: 27.83667755126953\n",
      "Epoch [457/500], Training Loss: 2.5644096924683732, Validation Loss: 27.7493953704834\n",
      "Epoch [458/500], Training Loss: 2.5659243678765886, Validation Loss: 27.663339614868164\n",
      "Epoch [459/500], Training Loss: 2.5668344133938517, Validation Loss: 27.582008361816406\n",
      "Epoch [460/500], Training Loss: 2.5672179690138304, Validation Loss: 27.503477096557617\n",
      "Epoch [461/500], Training Loss: 2.567188404764047, Validation Loss: 27.42936134338379\n",
      "Epoch [462/500], Training Loss: 2.5668303850742964, Validation Loss: 27.363245010375977\n",
      "Epoch [463/500], Training Loss: 2.566262529240487, Validation Loss: 27.3056640625\n",
      "Epoch [464/500], Training Loss: 2.565546741766722, Validation Loss: 27.254480361938477\n",
      "Epoch [465/500], Training Loss: 2.56479347450781, Validation Loss: 27.21268081665039\n",
      "Epoch [466/500], Training Loss: 2.564038538337995, Validation Loss: 27.181060791015625\n",
      "Epoch [467/500], Training Loss: 2.563388931031982, Validation Loss: 27.160001754760742\n",
      "Epoch [468/500], Training Loss: 2.5628834084052983, Validation Loss: 27.14960479736328\n",
      "Epoch [469/500], Training Loss: 2.5625611923651315, Validation Loss: 27.151716232299805\n",
      "Epoch [470/500], Training Loss: 2.5624399181481485, Validation Loss: 27.162290573120117\n",
      "Epoch [471/500], Training Loss: 2.5625566192419664, Validation Loss: 27.18364715576172\n",
      "Epoch [472/500], Training Loss: 2.5629154658137074, Validation Loss: 27.2156925201416\n",
      "Epoch [473/500], Training Loss: 2.5635382691293476, Validation Loss: 27.25693130493164\n",
      "Epoch [474/500], Training Loss: 2.5643890450290643, Validation Loss: 27.305530548095703\n",
      "Epoch [475/500], Training Loss: 2.5654740374971152, Validation Loss: 27.36228370666504\n",
      "Epoch [476/500], Training Loss: 2.566801349063331, Validation Loss: 27.428552627563477\n",
      "Epoch [477/500], Training Loss: 2.568300665723003, Validation Loss: 27.50199317932129\n",
      "Epoch [478/500], Training Loss: 2.56998333060826, Validation Loss: 27.580644607543945\n",
      "Epoch [479/500], Training Loss: 2.5717892950280223, Validation Loss: 27.664840698242188\n",
      "Epoch [480/500], Training Loss: 2.5737002404069593, Validation Loss: 27.755020141601562\n",
      "Epoch [481/500], Training Loss: 2.575699031128625, Validation Loss: 27.85018539428711\n",
      "Epoch [482/500], Training Loss: 2.5777380645474497, Validation Loss: 27.9449405670166\n",
      "Epoch [483/500], Training Loss: 2.5797801941381846, Validation Loss: 28.044124603271484\n",
      "Epoch [484/500], Training Loss: 2.5817734211229766, Validation Loss: 28.146316528320312\n",
      "Epoch [485/500], Training Loss: 2.5837077164356472, Validation Loss: 28.250164031982422\n",
      "Epoch [486/500], Training Loss: 2.5855643589170128, Validation Loss: 28.35468864440918\n",
      "Epoch [487/500], Training Loss: 2.587290090375761, Validation Loss: 28.45874786376953\n",
      "Epoch [488/500], Training Loss: 2.5888727656561294, Validation Loss: 28.563222885131836\n",
      "Epoch [489/500], Training Loss: 2.5902587818048493, Validation Loss: 28.66983985900879\n",
      "Epoch [490/500], Training Loss: 2.591477135870602, Validation Loss: 28.77676773071289\n",
      "Epoch [491/500], Training Loss: 2.592451633253897, Validation Loss: 28.88475799560547\n",
      "Epoch [492/500], Training Loss: 2.593167322593161, Validation Loss: 28.996572494506836\n",
      "Epoch [493/500], Training Loss: 2.593585908759958, Validation Loss: 29.10996437072754\n",
      "Epoch [494/500], Training Loss: 2.593737796358837, Validation Loss: 29.225299835205078\n",
      "Epoch [495/500], Training Loss: 2.5935701056710703, Validation Loss: 29.343490600585938\n",
      "Epoch [496/500], Training Loss: 2.5931141273791356, Validation Loss: 29.465595245361328\n",
      "Epoch [497/500], Training Loss: 2.592340318259312, Validation Loss: 29.590723037719727\n",
      "Epoch [498/500], Training Loss: 2.591309320508483, Validation Loss: 29.719223022460938\n",
      "Epoch [499/500], Training Loss: 2.590078289336699, Validation Loss: 29.851850509643555\n",
      "Epoch [500/500], Training Loss: 2.5886886796809634, Validation Loss: 29.993154525756836\n",
      "Test Loss: 32.44265365600586\n",
      "Best Hyperparameters:\n",
      "input_size=7, hidden_size=8, num_layers=2, learning_rate=0.001, window_size=1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [8]\n",
    "num_layers_list = [2]\n",
    "learning_rates = [ 0.001]\n",
    "window_sizes = [1]\n",
    "num_epochs = 500\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Function to create and train LSTM model with given hyperparameters\n",
    "def train_lstm_model(input_size, hidden_size, num_layers, learning_rate, window_size, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor):\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model on test data\n",
    "def evaluate_model(model, x_test_tensor, y_test_tensor, window_size):\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    return test_loss / len(x_test_tensor)\n",
    "\n",
    "best_hyperparameters = None\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "# Iterate over hyperparameter combinations and train models\n",
    "for input_size, hidden_size, num_layers, learning_rate, window_size in hyperparameter_combinations:\n",
    "    print(f\"Training with hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    model = train_lstm_model(input_size, hidden_size, num_layers, learning_rate, window_size, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor)\n",
    "    test_loss = evaluate_model(model, x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        best_hyperparameters = (input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"input_size={best_hyperparameters[0]}, hidden_size={best_hyperparameters[1]}, num_layers={best_hyperparameters[2]}, learning_rate={best_hyperparameters[3]}, window_size={best_hyperparameters[4]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ä' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mä\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ä' is not defined"
     ]
    }
   ],
   "source": [
    "ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Training Loss: 20850.550288373997, Validation Loss: 21213.60546875\n",
      "Epoch [2/300], Training Loss: 16831.715232942737, Validation Loss: 18986.125\n",
      "Epoch [3/300], Training Loss: 13502.97734956084, Validation Loss: 14966.30078125\n",
      "Epoch [4/300], Training Loss: 11241.155068569682, Validation Loss: 11896.580078125\n",
      "Epoch [5/300], Training Loss: 9573.374792077298, Validation Loss: 10598.3017578125\n",
      "Epoch [6/300], Training Loss: 14802.57295829931, Validation Loss: 11284.69921875\n",
      "Epoch [7/300], Training Loss: 8190.930847354726, Validation Loss: 9639.861328125\n",
      "Epoch [8/300], Training Loss: 7860.064874516047, Validation Loss: 12760.9892578125\n",
      "Epoch [9/300], Training Loss: 6255.211520377863, Validation Loss: 12711.2607421875\n",
      "Epoch [10/300], Training Loss: 4830.505320496257, Validation Loss: 7154.4970703125\n",
      "Epoch [11/300], Training Loss: 4132.376038949922, Validation Loss: 5312.2060546875\n",
      "Epoch [12/300], Training Loss: 3547.0474516417307, Validation Loss: 4350.71826171875\n",
      "Epoch [13/300], Training Loss: 3041.3418254024223, Validation Loss: 3668.84521484375\n",
      "Epoch [14/300], Training Loss: 2611.9836506837937, Validation Loss: 3090.3486328125\n",
      "Epoch [15/300], Training Loss: 2259.7440899709236, Validation Loss: 2645.66455078125\n",
      "Epoch [16/300], Training Loss: 1969.4132741309484, Validation Loss: 2312.65380859375\n",
      "Epoch [17/300], Training Loss: 1718.4363295877035, Validation Loss: 2024.345458984375\n",
      "Epoch [18/300], Training Loss: 1505.0882337803178, Validation Loss: 1776.972412109375\n",
      "Epoch [19/300], Training Loss: 1327.6721101968267, Validation Loss: 1570.7764892578125\n",
      "Epoch [20/300], Training Loss: 1174.9398531175427, Validation Loss: 1399.0758056640625\n",
      "Epoch [21/300], Training Loss: 1042.5362568574787, Validation Loss: 1252.55078125\n",
      "Epoch [22/300], Training Loss: 925.254207262062, Validation Loss: 1125.2703857421875\n",
      "Epoch [23/300], Training Loss: 821.7435860354058, Validation Loss: 1006.156005859375\n",
      "Epoch [24/300], Training Loss: 729.9821994273578, Validation Loss: 909.5042724609375\n",
      "Epoch [25/300], Training Loss: 647.4687234477518, Validation Loss: 826.537109375\n",
      "Epoch [26/300], Training Loss: 573.210018197782, Validation Loss: 740.9569702148438\n",
      "Epoch [27/300], Training Loss: 506.5989818075967, Validation Loss: 663.364990234375\n",
      "Epoch [28/300], Training Loss: 446.5296476351739, Validation Loss: 587.9728393554688\n",
      "Epoch [29/300], Training Loss: 392.59882681256227, Validation Loss: 525.0224609375\n",
      "Epoch [30/300], Training Loss: 344.4876666110677, Validation Loss: 462.021484375\n",
      "Epoch [31/300], Training Loss: 301.8131562896176, Validation Loss: 403.5023498535156\n",
      "Epoch [32/300], Training Loss: 264.0852446989169, Validation Loss: 355.6791076660156\n",
      "Epoch [33/300], Training Loss: 231.21972252501624, Validation Loss: 314.6520690917969\n",
      "Epoch [34/300], Training Loss: 203.03711978635076, Validation Loss: 276.5252990722656\n",
      "Epoch [35/300], Training Loss: 178.87481193994478, Validation Loss: 240.12306213378906\n",
      "Epoch [36/300], Training Loss: 158.07113969211207, Validation Loss: 220.43035888671875\n",
      "Epoch [37/300], Training Loss: 139.9156273711224, Validation Loss: 198.7388153076172\n",
      "Epoch [38/300], Training Loss: 123.11033577540906, Validation Loss: 169.47496032714844\n",
      "Epoch [39/300], Training Loss: 108.58654902387816, Validation Loss: 149.42750549316406\n",
      "Epoch [40/300], Training Loss: 95.8449093662597, Validation Loss: 133.5485076904297\n",
      "Epoch [41/300], Training Loss: 84.81942250552532, Validation Loss: 118.25304412841797\n",
      "Epoch [42/300], Training Loss: 75.17440954223687, Validation Loss: 107.11448669433594\n",
      "Epoch [43/300], Training Loss: 66.71193779525812, Validation Loss: 98.94839477539062\n",
      "Epoch [44/300], Training Loss: 59.40346463171229, Validation Loss: 93.32353973388672\n",
      "Epoch [45/300], Training Loss: 53.08670787244184, Validation Loss: 90.48280334472656\n",
      "Epoch [46/300], Training Loss: 47.60345416759641, Validation Loss: 92.80755615234375\n",
      "Epoch [47/300], Training Loss: 42.74998816482199, Validation Loss: 100.61988830566406\n",
      "Epoch [48/300], Training Loss: 38.32532861661342, Validation Loss: 108.45420837402344\n",
      "Epoch [49/300], Training Loss: 34.42998417146407, Validation Loss: 111.82740020751953\n",
      "Epoch [50/300], Training Loss: 31.118152989736338, Validation Loss: 111.00115966796875\n",
      "Epoch [51/300], Training Loss: 28.22871242613992, Validation Loss: 108.42174530029297\n",
      "Epoch [52/300], Training Loss: 25.70315665766309, Validation Loss: 103.95197296142578\n",
      "Epoch [53/300], Training Loss: 23.569402043266845, Validation Loss: 97.43173217773438\n",
      "Epoch [54/300], Training Loss: 21.755474897187387, Validation Loss: 90.56014251708984\n",
      "Epoch [55/300], Training Loss: 20.186943997247404, Validation Loss: 84.3023910522461\n",
      "Epoch [56/300], Training Loss: 18.835341524302994, Validation Loss: 78.53137969970703\n",
      "Epoch [57/300], Training Loss: 17.660859631402282, Validation Loss: 73.53053283691406\n",
      "Epoch [58/300], Training Loss: 16.627348245214897, Validation Loss: 69.4735107421875\n",
      "Epoch [59/300], Training Loss: 15.705718591276943, Validation Loss: 66.26570892333984\n",
      "Epoch [60/300], Training Loss: 14.873865950977862, Validation Loss: 63.675601959228516\n",
      "Epoch [61/300], Training Loss: 14.118093113460906, Validation Loss: 61.39348602294922\n",
      "Epoch [62/300], Training Loss: 13.431316488138817, Validation Loss: 59.14937210083008\n",
      "Epoch [63/300], Training Loss: 12.809644936233754, Validation Loss: 56.814414978027344\n",
      "Epoch [64/300], Training Loss: 12.248973175447889, Validation Loss: 54.41741180419922\n",
      "Epoch [65/300], Training Loss: 11.744205949203169, Validation Loss: 52.12227249145508\n",
      "Epoch [66/300], Training Loss: 11.288737970700163, Validation Loss: 50.102996826171875\n",
      "Epoch [67/300], Training Loss: 10.874088623927781, Validation Loss: 48.478485107421875\n",
      "Epoch [68/300], Training Loss: 10.491500078207062, Validation Loss: 47.261104583740234\n",
      "Epoch [69/300], Training Loss: 10.134935296988438, Validation Loss: 46.388450622558594\n",
      "Epoch [70/300], Training Loss: 9.800890180053992, Validation Loss: 45.77808380126953\n",
      "Epoch [71/300], Training Loss: 9.488180055521928, Validation Loss: 45.35139083862305\n",
      "Epoch [72/300], Training Loss: 9.19597400081456, Validation Loss: 45.06666946411133\n",
      "Epoch [73/300], Training Loss: 8.923246363355759, Validation Loss: 44.90741729736328\n",
      "Epoch [74/300], Training Loss: 8.668667210582132, Validation Loss: 44.85918426513672\n",
      "Epoch [75/300], Training Loss: 8.43079816131721, Validation Loss: 44.91413879394531\n",
      "Epoch [76/300], Training Loss: 8.208155672223123, Validation Loss: 45.05240249633789\n",
      "Epoch [77/300], Training Loss: 7.999305925321395, Validation Loss: 45.24600601196289\n",
      "Epoch [78/300], Training Loss: 7.802490202817925, Validation Loss: 45.45869827270508\n",
      "Epoch [79/300], Training Loss: 7.615881399975662, Validation Loss: 45.652278900146484\n",
      "Epoch [80/300], Training Loss: 7.437263651514374, Validation Loss: 45.80405044555664\n",
      "Epoch [81/300], Training Loss: 7.264983155649981, Validation Loss: 45.89499282836914\n",
      "Epoch [82/300], Training Loss: 7.09721075892664, Validation Loss: 45.91953659057617\n",
      "Epoch [83/300], Training Loss: 6.933056531040612, Validation Loss: 45.8826789855957\n",
      "Epoch [84/300], Training Loss: 6.771263384270273, Validation Loss: 45.79431915283203\n",
      "Epoch [85/300], Training Loss: 6.611249367786845, Validation Loss: 45.677101135253906\n",
      "Epoch [86/300], Training Loss: 6.452616286065948, Validation Loss: 45.55320358276367\n",
      "Epoch [87/300], Training Loss: 6.295016111160421, Validation Loss: 45.45914840698242\n",
      "Epoch [88/300], Training Loss: 6.138208787973318, Validation Loss: 45.425453186035156\n",
      "Epoch [89/300], Training Loss: 5.98206626601662, Validation Loss: 45.49165344238281\n",
      "Epoch [90/300], Training Loss: 5.82639368684797, Validation Loss: 45.69846725463867\n",
      "Epoch [91/300], Training Loss: 5.671479023687616, Validation Loss: 46.079105377197266\n",
      "Epoch [92/300], Training Loss: 5.517560532679251, Validation Loss: 46.66960525512695\n",
      "Epoch [93/300], Training Loss: 5.3651156431374085, Validation Loss: 47.49913787841797\n",
      "Epoch [94/300], Training Loss: 5.215074430091866, Validation Loss: 48.59031677246094\n",
      "Epoch [95/300], Training Loss: 5.0683916694633595, Validation Loss: 49.95240020751953\n",
      "Epoch [96/300], Training Loss: 4.926457649102745, Validation Loss: 51.579307556152344\n",
      "Epoch [97/300], Training Loss: 4.790608744192965, Validation Loss: 53.41226577758789\n",
      "Epoch [98/300], Training Loss: 4.662061088099666, Validation Loss: 55.322208404541016\n",
      "Epoch [99/300], Training Loss: 4.541827525900495, Validation Loss: 57.09654998779297\n",
      "Epoch [100/300], Training Loss: 4.430840767248558, Validation Loss: 58.43463134765625\n",
      "Epoch [101/300], Training Loss: 4.3296174976270265, Validation Loss: 59.086463928222656\n",
      "Epoch [102/300], Training Loss: 4.2382749033640374, Validation Loss: 58.98809814453125\n",
      "Epoch [103/300], Training Loss: 4.1564097640113244, Validation Loss: 58.31498718261719\n",
      "Epoch [104/300], Training Loss: 4.082974811932467, Validation Loss: 57.34313201904297\n",
      "Epoch [105/300], Training Loss: 4.016413751923222, Validation Loss: 56.26805877685547\n",
      "Epoch [106/300], Training Loss: 3.955206758284688, Validation Loss: 55.15917205810547\n",
      "Epoch [107/300], Training Loss: 3.897984956734202, Validation Loss: 54.01932144165039\n",
      "Epoch [108/300], Training Loss: 3.843700446585142, Validation Loss: 52.838191986083984\n",
      "Epoch [109/300], Training Loss: 3.791804883938144, Validation Loss: 51.621036529541016\n",
      "Epoch [110/300], Training Loss: 3.7418522757619233, Validation Loss: 50.374576568603516\n",
      "Epoch [111/300], Training Loss: 3.6937105245818977, Validation Loss: 49.115020751953125\n",
      "Epoch [112/300], Training Loss: 3.647141626287486, Validation Loss: 47.86426544189453\n",
      "Epoch [113/300], Training Loss: 3.602107688207079, Validation Loss: 46.63591384887695\n",
      "Epoch [114/300], Training Loss: 3.5585225979269905, Validation Loss: 45.44216537475586\n",
      "Epoch [115/300], Training Loss: 3.516368846189506, Validation Loss: 44.29167556762695\n",
      "Epoch [116/300], Training Loss: 3.475536283595379, Validation Loss: 43.18756866455078\n",
      "Epoch [117/300], Training Loss: 3.43603267247455, Validation Loss: 42.132511138916016\n",
      "Epoch [118/300], Training Loss: 3.3976862710082107, Validation Loss: 41.121212005615234\n",
      "Epoch [119/300], Training Loss: 3.3604613532417758, Validation Loss: 40.146575927734375\n",
      "Epoch [120/300], Training Loss: 3.3242009702135458, Validation Loss: 39.20290756225586\n",
      "Epoch [121/300], Training Loss: 3.288754062946482, Validation Loss: 38.27909851074219\n",
      "Epoch [122/300], Training Loss: 3.2539368273188742, Validation Loss: 37.36782455444336\n",
      "Epoch [123/300], Training Loss: 3.219442514935154, Validation Loss: 36.45845413208008\n",
      "Epoch [124/300], Training Loss: 3.1849617078099994, Validation Loss: 35.53734588623047\n",
      "Epoch [125/300], Training Loss: 3.1501765410370215, Validation Loss: 34.588905334472656\n",
      "Epoch [126/300], Training Loss: 3.114865182625354, Validation Loss: 33.595909118652344\n",
      "Epoch [127/300], Training Loss: 3.0788169986502636, Validation Loss: 32.541717529296875\n",
      "Epoch [128/300], Training Loss: 3.0421149624985824, Validation Loss: 31.41581153869629\n",
      "Epoch [129/300], Training Loss: 3.0049917828874353, Validation Loss: 30.214435577392578\n",
      "Epoch [130/300], Training Loss: 2.967850809839632, Validation Loss: 28.93735694885254\n",
      "Epoch [131/300], Training Loss: 2.931396276778226, Validation Loss: 27.601850509643555\n",
      "Epoch [132/300], Training Loss: 2.896517762959664, Validation Loss: 26.238142013549805\n",
      "Epoch [133/300], Training Loss: 2.8643824826118576, Validation Loss: 24.88722038269043\n",
      "Epoch [134/300], Training Loss: 2.8363449511560948, Validation Loss: 23.59840965270996\n",
      "Epoch [135/300], Training Loss: 2.8134943850801823, Validation Loss: 22.425926208496094\n",
      "Epoch [136/300], Training Loss: 2.7963921342809552, Validation Loss: 21.408845901489258\n",
      "Epoch [137/300], Training Loss: 2.7841223356565337, Validation Loss: 20.574277877807617\n",
      "Epoch [138/300], Training Loss: 2.7732844243068024, Validation Loss: 20.068431854248047\n",
      "Epoch [139/300], Training Loss: 2.7673502873692493, Validation Loss: 20.243276596069336\n",
      "Epoch [140/300], Training Loss: 2.7744721188150385, Validation Loss: 20.590421676635742\n",
      "Epoch [141/300], Training Loss: 2.788968738342804, Validation Loss: 20.878610610961914\n",
      "Epoch [142/300], Training Loss: 2.8048137317813913, Validation Loss: 21.361379623413086\n",
      "Epoch [143/300], Training Loss: 2.8212052591478005, Validation Loss: 22.02867889404297\n",
      "Epoch [144/300], Training Loss: 2.837901972376063, Validation Loss: 22.821319580078125\n",
      "Epoch [145/300], Training Loss: 2.854823193228141, Validation Loss: 23.673128128051758\n",
      "Epoch [146/300], Training Loss: 2.871789182902896, Validation Loss: 24.533451080322266\n",
      "Epoch [147/300], Training Loss: 2.888614136978343, Validation Loss: 25.364503860473633\n",
      "Epoch [148/300], Training Loss: 2.905034457413843, Validation Loss: 26.141536712646484\n",
      "Epoch [149/300], Training Loss: 2.920727711240453, Validation Loss: 26.851425170898438\n",
      "Epoch [150/300], Training Loss: 2.935603602425985, Validation Loss: 27.485658645629883\n",
      "Epoch [151/300], Training Loss: 2.9496042741449426, Validation Loss: 28.035247802734375\n",
      "Epoch [152/300], Training Loss: 2.9626985815800957, Validation Loss: 28.50616455078125\n",
      "Epoch [153/300], Training Loss: 2.974870072108251, Validation Loss: 28.89434242248535\n",
      "Epoch [154/300], Training Loss: 2.9861052024967742, Validation Loss: 29.204912185668945\n",
      "Epoch [155/300], Training Loss: 2.996341336901003, Validation Loss: 29.44422149658203\n",
      "Epoch [156/300], Training Loss: 3.0055287317903585, Validation Loss: 29.61846160888672\n",
      "Epoch [157/300], Training Loss: 3.0136510668364784, Validation Loss: 29.739633560180664\n",
      "Epoch [158/300], Training Loss: 3.0207707343629817, Validation Loss: 29.8149356842041\n",
      "Epoch [159/300], Training Loss: 3.027014357419511, Validation Loss: 29.857303619384766\n",
      "Epoch [160/300], Training Loss: 3.0326493036284323, Validation Loss: 29.87349510192871\n",
      "Epoch [161/300], Training Loss: 3.0379662605211877, Validation Loss: 29.869539260864258\n",
      "Epoch [162/300], Training Loss: 3.043398172437136, Validation Loss: 29.84832000732422\n",
      "Epoch [163/300], Training Loss: 3.0493074735018424, Validation Loss: 29.8033504486084\n",
      "Epoch [164/300], Training Loss: 3.0560668279641856, Validation Loss: 29.73357391357422\n",
      "Epoch [165/300], Training Loss: 3.063933777131411, Validation Loss: 29.630504608154297\n",
      "Epoch [166/300], Training Loss: 3.0730647613796003, Validation Loss: 29.48944854736328\n",
      "Epoch [167/300], Training Loss: 3.0835080485259763, Validation Loss: 29.3101863861084\n",
      "Epoch [168/300], Training Loss: 3.095253011818919, Validation Loss: 29.08871078491211\n",
      "Epoch [169/300], Training Loss: 3.108159399972572, Validation Loss: 28.829246520996094\n",
      "Epoch [170/300], Training Loss: 3.122063231878875, Validation Loss: 28.53289794921875\n",
      "Epoch [171/300], Training Loss: 3.136778204183288, Validation Loss: 28.198284149169922\n",
      "Epoch [172/300], Training Loss: 3.1520232291896257, Validation Loss: 27.827598571777344\n",
      "Epoch [173/300], Training Loss: 3.1674562837212563, Validation Loss: 27.42180824279785\n",
      "Epoch [174/300], Training Loss: 3.1828399180485243, Validation Loss: 26.983760833740234\n",
      "Epoch [175/300], Training Loss: 3.197867173747415, Validation Loss: 26.517974853515625\n",
      "Epoch [176/300], Training Loss: 3.2122633644490888, Validation Loss: 26.030851364135742\n",
      "Epoch [177/300], Training Loss: 3.2257788096980264, Validation Loss: 25.527873992919922\n",
      "Epoch [178/300], Training Loss: 3.2381931718737094, Validation Loss: 25.01546287536621\n",
      "Epoch [179/300], Training Loss: 3.249294134309357, Validation Loss: 24.506052017211914\n",
      "Epoch [180/300], Training Loss: 3.2589517841604634, Validation Loss: 24.009429931640625\n",
      "Epoch [181/300], Training Loss: 3.2670838710287944, Validation Loss: 23.534650802612305\n",
      "Epoch [182/300], Training Loss: 3.273668871289266, Validation Loss: 23.08945083618164\n",
      "Epoch [183/300], Training Loss: 3.278629172670336, Validation Loss: 22.682491302490234\n",
      "Epoch [184/300], Training Loss: 3.282028816587785, Validation Loss: 22.319929122924805\n",
      "Epoch [185/300], Training Loss: 3.2839177875478756, Validation Loss: 22.014501571655273\n",
      "Epoch [186/300], Training Loss: 3.28435182198271, Validation Loss: 21.776958465576172\n",
      "Epoch [187/300], Training Loss: 3.2834484895619642, Validation Loss: 21.608150482177734\n",
      "Epoch [188/300], Training Loss: 3.281149174403609, Validation Loss: 21.496322631835938\n",
      "Epoch [189/300], Training Loss: 3.277515901587039, Validation Loss: 21.43731117248535\n",
      "Epoch [190/300], Training Loss: 3.2724846115402237, Validation Loss: 21.423112869262695\n",
      "Epoch [191/300], Training Loss: 3.2657712850460987, Validation Loss: 21.445356369018555\n",
      "Epoch [192/300], Training Loss: 3.257050686258205, Validation Loss: 21.493793487548828\n",
      "Epoch [193/300], Training Loss: 3.245199412531303, Validation Loss: 21.557775497436523\n",
      "Epoch [194/300], Training Loss: 3.2280702100419427, Validation Loss: 21.643896102905273\n",
      "Epoch [195/300], Training Loss: 3.201130166305512, Validation Loss: 21.78335952758789\n",
      "Epoch [196/300], Training Loss: 3.1575831555264906, Validation Loss: 22.03414535522461\n",
      "Epoch [197/300], Training Loss: 3.0909164779326397, Validation Loss: 22.455331802368164\n",
      "Epoch [198/300], Training Loss: 3.002049729704144, Validation Loss: 23.121387481689453\n",
      "Epoch [199/300], Training Loss: 2.905691957686108, Validation Loss: 24.17575454711914\n",
      "Epoch [200/300], Training Loss: 2.824095389712648, Validation Loss: 25.79242515563965\n",
      "Epoch [201/300], Training Loss: 2.769070747821776, Validation Loss: 28.008947372436523\n",
      "Epoch [202/300], Training Loss: 2.7357149297813006, Validation Loss: 30.626455307006836\n",
      "Epoch [203/300], Training Loss: 2.713680001033027, Validation Loss: 33.327674865722656\n",
      "Epoch [204/300], Training Loss: 2.6959309853885505, Validation Loss: 35.77921676635742\n",
      "Epoch [205/300], Training Loss: 2.679097406891601, Validation Loss: 37.7171745300293\n",
      "Epoch [206/300], Training Loss: 2.662145219876111, Validation Loss: 38.97935104370117\n",
      "Epoch [207/300], Training Loss: 2.645104195339126, Validation Loss: 39.52434539794922\n",
      "Epoch [208/300], Training Loss: 2.628728178579124, Validation Loss: 39.423065185546875\n",
      "Epoch [209/300], Training Loss: 2.6138116588072604, Validation Loss: 38.80936050415039\n",
      "Epoch [210/300], Training Loss: 2.6008506700998213, Validation Loss: 37.86653137207031\n",
      "Epoch [211/300], Training Loss: 2.5898695823549156, Validation Loss: 36.77052688598633\n",
      "Epoch [212/300], Training Loss: 2.5803971130182397, Validation Loss: 35.64865493774414\n",
      "Epoch [213/300], Training Loss: 2.5718862816176915, Validation Loss: 34.58220672607422\n",
      "Epoch [214/300], Training Loss: 2.5638203090301626, Validation Loss: 33.6102180480957\n",
      "Epoch [215/300], Training Loss: 2.5559259621264463, Validation Loss: 32.73733139038086\n",
      "Epoch [216/300], Training Loss: 2.548024840493221, Validation Loss: 31.961471557617188\n",
      "Epoch [217/300], Training Loss: 2.540089056099821, Validation Loss: 31.275575637817383\n",
      "Epoch [218/300], Training Loss: 2.5320244439019826, Validation Loss: 30.667312622070312\n",
      "Epoch [219/300], Training Loss: 2.523813904947447, Validation Loss: 30.1328067779541\n",
      "Epoch [220/300], Training Loss: 2.5154473872711183, Validation Loss: 29.6686954498291\n",
      "Epoch [221/300], Training Loss: 2.5069484210297457, Validation Loss: 29.271102905273438\n",
      "Epoch [222/300], Training Loss: 2.498327944072657, Validation Loss: 28.941152572631836\n",
      "Epoch [223/300], Training Loss: 2.4896117697142857, Validation Loss: 28.677143096923828\n",
      "Epoch [224/300], Training Loss: 2.480827733635684, Validation Loss: 28.483200073242188\n",
      "Epoch [225/300], Training Loss: 2.4720003183083925, Validation Loss: 28.354223251342773\n",
      "Epoch [226/300], Training Loss: 2.463132063636896, Validation Loss: 28.28668212890625\n",
      "Epoch [227/300], Training Loss: 2.454168669408386, Validation Loss: 28.26951789855957\n",
      "Epoch [228/300], Training Loss: 2.4451019040613677, Validation Loss: 28.289005279541016\n",
      "Epoch [229/300], Training Loss: 2.4359014547195623, Validation Loss: 28.33116912841797\n",
      "Epoch [230/300], Training Loss: 2.426576647470993, Validation Loss: 28.380847930908203\n",
      "Epoch [231/300], Training Loss: 2.417122763224827, Validation Loss: 28.425676345825195\n",
      "Epoch [232/300], Training Loss: 2.407533658753102, Validation Loss: 28.4598331451416\n",
      "Epoch [233/300], Training Loss: 2.3978560067900094, Validation Loss: 28.473848342895508\n",
      "Epoch [234/300], Training Loss: 2.3881438156937964, Validation Loss: 28.466140747070312\n",
      "Epoch [235/300], Training Loss: 2.378384749923038, Validation Loss: 28.43551254272461\n",
      "Epoch [236/300], Training Loss: 2.3686483681943304, Validation Loss: 28.37935447692871\n",
      "Epoch [237/300], Training Loss: 2.358979293298246, Validation Loss: 28.3038272857666\n",
      "Epoch [238/300], Training Loss: 2.349407421345004, Validation Loss: 28.208803176879883\n",
      "Epoch [239/300], Training Loss: 2.3399584253351082, Validation Loss: 28.096925735473633\n",
      "Epoch [240/300], Training Loss: 2.330657868469926, Validation Loss: 27.976381301879883\n",
      "Epoch [241/300], Training Loss: 2.321533295773971, Validation Loss: 27.84979820251465\n",
      "Epoch [242/300], Training Loss: 2.312637817892158, Validation Loss: 27.72085189819336\n",
      "Epoch [243/300], Training Loss: 2.3039752880603, Validation Loss: 27.591934204101562\n",
      "Epoch [244/300], Training Loss: 2.295550006147911, Validation Loss: 27.469696044921875\n",
      "Epoch [245/300], Training Loss: 2.2873786379657073, Validation Loss: 27.354257583618164\n",
      "Epoch [246/300], Training Loss: 2.2794399127764025, Validation Loss: 27.250816345214844\n",
      "Epoch [247/300], Training Loss: 2.2717919131900706, Validation Loss: 27.160301208496094\n",
      "Epoch [248/300], Training Loss: 2.2644276845250872, Validation Loss: 27.08388900756836\n",
      "Epoch [249/300], Training Loss: 2.2573102865906027, Validation Loss: 27.024337768554688\n",
      "Epoch [250/300], Training Loss: 2.2504386762505773, Validation Loss: 26.98031234741211\n",
      "Epoch [251/300], Training Loss: 2.243784772979804, Validation Loss: 26.95140838623047\n",
      "Epoch [252/300], Training Loss: 2.237325659244667, Validation Loss: 26.93687629699707\n",
      "Epoch [253/300], Training Loss: 2.2309870865493484, Validation Loss: 26.93788719177246\n",
      "Epoch [254/300], Training Loss: 2.224776471760807, Validation Loss: 26.95130157470703\n",
      "Epoch [255/300], Training Loss: 2.218607971553305, Validation Loss: 26.975744247436523\n",
      "Epoch [256/300], Training Loss: 2.212449897265769, Validation Loss: 27.01146697998047\n",
      "Epoch [257/300], Training Loss: 2.2062441771527355, Validation Loss: 27.058504104614258\n",
      "Epoch [258/300], Training Loss: 2.1999650683266827, Validation Loss: 27.115367889404297\n",
      "Epoch [259/300], Training Loss: 2.1935394679419637, Validation Loss: 27.18585968017578\n",
      "Epoch [260/300], Training Loss: 2.1869553274775053, Validation Loss: 27.26900291442871\n",
      "Epoch [261/300], Training Loss: 2.1801555796678205, Validation Loss: 27.369062423706055\n",
      "Epoch [262/300], Training Loss: 2.1732060601329914, Validation Loss: 27.486186981201172\n",
      "Epoch [263/300], Training Loss: 2.1660930906034253, Validation Loss: 27.61986541748047\n",
      "Epoch [264/300], Training Loss: 2.1589096733631528, Validation Loss: 27.771135330200195\n",
      "Epoch [265/300], Training Loss: 2.151751347865385, Validation Loss: 27.93075180053711\n",
      "Epoch [266/300], Training Loss: 2.144694407819973, Validation Loss: 28.093793869018555\n",
      "Epoch [267/300], Training Loss: 2.1378732508827873, Validation Loss: 28.2509765625\n",
      "Epoch [268/300], Training Loss: 2.1313867662204466, Validation Loss: 28.3935546875\n",
      "Epoch [269/300], Training Loss: 2.1253388351956954, Validation Loss: 28.52016830444336\n",
      "Epoch [270/300], Training Loss: 2.1198181453486638, Validation Loss: 28.623027801513672\n",
      "Epoch [271/300], Training Loss: 2.1148713805589283, Validation Loss: 28.703510284423828\n",
      "Epoch [272/300], Training Loss: 2.110524880218323, Validation Loss: 28.758100509643555\n",
      "Epoch [273/300], Training Loss: 2.1068332850284763, Validation Loss: 28.78401756286621\n",
      "Epoch [274/300], Training Loss: 2.1037851007997665, Validation Loss: 28.778167724609375\n",
      "Epoch [275/300], Training Loss: 2.1013574336147123, Validation Loss: 28.738616943359375\n",
      "Epoch [276/300], Training Loss: 2.099502214136774, Validation Loss: 28.662662506103516\n",
      "Epoch [277/300], Training Loss: 2.098198535814236, Validation Loss: 28.545608520507812\n",
      "Epoch [278/300], Training Loss: 2.0973661116985096, Validation Loss: 28.390594482421875\n",
      "Epoch [279/300], Training Loss: 2.096936861006277, Validation Loss: 28.198596954345703\n",
      "Epoch [280/300], Training Loss: 2.0968346997479923, Validation Loss: 27.969514846801758\n",
      "Epoch [281/300], Training Loss: 2.0969991466836637, Validation Loss: 27.707258224487305\n",
      "Epoch [282/300], Training Loss: 2.097375194902266, Validation Loss: 27.41695213317871\n",
      "Epoch [283/300], Training Loss: 2.097925129099477, Validation Loss: 27.104108810424805\n",
      "Epoch [284/300], Training Loss: 2.098606655502335, Validation Loss: 26.775127410888672\n",
      "Epoch [285/300], Training Loss: 2.0994085006509517, Validation Loss: 26.432645797729492\n",
      "Epoch [286/300], Training Loss: 2.1003000750465364, Validation Loss: 26.086702346801758\n",
      "Epoch [287/300], Training Loss: 2.1012442378493046, Validation Loss: 25.73548698425293\n",
      "Epoch [288/300], Training Loss: 2.102189746516425, Validation Loss: 25.37442398071289\n",
      "Epoch [289/300], Training Loss: 2.1030714290759964, Validation Loss: 24.983152389526367\n",
      "Epoch [290/300], Training Loss: 2.103840581413553, Validation Loss: 24.544687271118164\n",
      "Epoch [291/300], Training Loss: 2.104626911545982, Validation Loss: 24.055139541625977\n",
      "Epoch [292/300], Training Loss: 2.1056081624769103, Validation Loss: 23.527690887451172\n",
      "Epoch [293/300], Training Loss: 2.1070207162283037, Validation Loss: 22.98455238342285\n",
      "Epoch [294/300], Training Loss: 2.10903003828156, Validation Loss: 22.43937873840332\n",
      "Epoch [295/300], Training Loss: 2.1117180199671135, Validation Loss: 21.8997802734375\n",
      "Epoch [296/300], Training Loss: 2.115055414244984, Validation Loss: 21.36675262451172\n",
      "Epoch [297/300], Training Loss: 2.118918233120198, Validation Loss: 20.842430114746094\n",
      "Epoch [298/300], Training Loss: 2.1232599589207717, Validation Loss: 20.329641342163086\n",
      "Epoch [299/300], Training Loss: 2.1280250506383265, Validation Loss: 19.83281135559082\n",
      "Epoch [300/300], Training Loss: 2.133234445345495, Validation Loss: 19.35676383972168\n",
      "Test Loss: 22.584089279174805\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "input_size = 7\n",
    "hidden_size = 8\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "# Calculate test loss after training\n",
    "test_loss = 0.0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(x_test_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_test_tensor))\n",
    "\n",
    "        inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "        labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, labels)\n",
    "\n",
    "print(f'Test Loss: {test_loss / len(x_test_tensor)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ä' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mä\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ä' is not defined"
     ]
    }
   ],
   "source": [
    "ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Training Loss: 20890.032764768628, Validation Loss: 21236.92578125\n",
      "Epoch [2/300], Training Loss: 17500.138662843186, Validation Loss: 19276.255859375\n",
      "Epoch [3/300], Training Loss: 13969.773321590807, Validation Loss: 16890.3203125\n",
      "Epoch [4/300], Training Loss: 11619.360474948111, Validation Loss: 14103.7861328125\n",
      "Epoch [5/300], Training Loss: 9903.484623459302, Validation Loss: 11468.2314453125\n",
      "Epoch [6/300], Training Loss: 8551.398639761945, Validation Loss: 11284.8984375\n",
      "Epoch [7/300], Training Loss: 7397.702266702298, Validation Loss: 9553.578125\n",
      "Epoch [8/300], Training Loss: 6416.287938114935, Validation Loss: 7842.08154296875\n",
      "Epoch [9/300], Training Loss: 5547.303140038055, Validation Loss: 6574.96240234375\n",
      "Epoch [10/300], Training Loss: 4813.158028178025, Validation Loss: 5700.2568359375\n",
      "Epoch [11/300], Training Loss: 4182.2834019868615, Validation Loss: 4944.38330078125\n",
      "Epoch [12/300], Training Loss: 3614.4943341520784, Validation Loss: 4247.64013671875\n",
      "Epoch [13/300], Training Loss: 3110.385239418686, Validation Loss: 3649.049560546875\n",
      "Epoch [14/300], Training Loss: 2681.9833474488037, Validation Loss: 3150.960205078125\n",
      "Epoch [15/300], Training Loss: 2323.638869771617, Validation Loss: 2784.06884765625\n",
      "Epoch [16/300], Training Loss: 2028.9164289073221, Validation Loss: 2495.212890625\n",
      "Epoch [17/300], Training Loss: 1771.9780074689713, Validation Loss: 2233.071044921875\n",
      "Epoch [18/300], Training Loss: 1552.3453289793288, Validation Loss: 1991.3472900390625\n",
      "Epoch [19/300], Training Loss: 1369.6001223948856, Validation Loss: 1773.4010009765625\n",
      "Epoch [20/300], Training Loss: 1214.3865295148719, Validation Loss: 1570.7393798828125\n",
      "Epoch [21/300], Training Loss: 1078.8200538687393, Validation Loss: 1423.5655517578125\n",
      "Epoch [22/300], Training Loss: 959.6770979553844, Validation Loss: 1256.34228515625\n",
      "Epoch [23/300], Training Loss: 853.9183675917893, Validation Loss: 1115.5941162109375\n",
      "Epoch [24/300], Training Loss: 759.9940263280205, Validation Loss: 981.6708984375\n",
      "Epoch [25/300], Training Loss: 674.6332874843587, Validation Loss: 885.1785278320312\n",
      "Epoch [26/300], Training Loss: 598.0648208074881, Validation Loss: 791.3480224609375\n",
      "Epoch [27/300], Training Loss: 529.2388835221263, Validation Loss: 705.8294067382812\n",
      "Epoch [28/300], Training Loss: 466.76217364807417, Validation Loss: 633.2195434570312\n",
      "Epoch [29/300], Training Loss: 410.96952122496197, Validation Loss: 566.0801391601562\n",
      "Epoch [30/300], Training Loss: 360.9618437969422, Validation Loss: 508.10162353515625\n",
      "Epoch [31/300], Training Loss: 316.63379373293446, Validation Loss: 450.4349365234375\n",
      "Epoch [32/300], Training Loss: 277.8287183944321, Validation Loss: 395.88262939453125\n",
      "Epoch [33/300], Training Loss: 244.1685460410543, Validation Loss: 348.3010559082031\n",
      "Epoch [34/300], Training Loss: 214.5805036314263, Validation Loss: 318.08843994140625\n",
      "Epoch [35/300], Training Loss: 188.75740841970827, Validation Loss: 298.2050476074219\n",
      "Epoch [36/300], Training Loss: 166.3495914227631, Validation Loss: 274.5347900390625\n",
      "Epoch [37/300], Training Loss: 146.7770483740833, Validation Loss: 218.70948791503906\n",
      "Epoch [38/300], Training Loss: 129.190655646095, Validation Loss: 207.7212677001953\n",
      "Epoch [39/300], Training Loss: 112.85512003485961, Validation Loss: 170.71469116210938\n",
      "Epoch [40/300], Training Loss: 99.70226458422403, Validation Loss: 153.63079833984375\n",
      "Epoch [41/300], Training Loss: 88.23976139487284, Validation Loss: 145.01751708984375\n",
      "Epoch [42/300], Training Loss: 77.91838665783145, Validation Loss: 137.64137268066406\n",
      "Epoch [43/300], Training Loss: 68.95748362354246, Validation Loss: 131.22955322265625\n",
      "Epoch [44/300], Training Loss: 61.189861125295316, Validation Loss: 123.0501480102539\n",
      "Epoch [45/300], Training Loss: 54.41835012353634, Validation Loss: 110.45491790771484\n",
      "Epoch [46/300], Training Loss: 48.54616757793511, Validation Loss: 98.75717163085938\n",
      "Epoch [47/300], Training Loss: 43.38986831861637, Validation Loss: 89.7562255859375\n",
      "Epoch [48/300], Training Loss: 38.85096957996388, Validation Loss: 83.68913269042969\n",
      "Epoch [49/300], Training Loss: 34.97599378495885, Validation Loss: 79.5602798461914\n",
      "Epoch [50/300], Training Loss: 31.633611672809106, Validation Loss: 75.90320587158203\n",
      "Epoch [51/300], Training Loss: 28.70554464974061, Validation Loss: 73.23037719726562\n",
      "Epoch [52/300], Training Loss: 26.240936273298434, Validation Loss: 70.80269622802734\n",
      "Epoch [53/300], Training Loss: 24.148616491488472, Validation Loss: 68.06449127197266\n",
      "Epoch [54/300], Training Loss: 22.359431941261388, Validation Loss: 65.11917114257812\n",
      "Epoch [55/300], Training Loss: 20.815129758556928, Validation Loss: 62.30891036987305\n",
      "Epoch [56/300], Training Loss: 19.452268339508674, Validation Loss: 59.90193557739258\n",
      "Epoch [57/300], Training Loss: 18.22390061111212, Validation Loss: 57.98723602294922\n",
      "Epoch [58/300], Training Loss: 17.108181665839872, Validation Loss: 56.53412628173828\n",
      "Epoch [59/300], Training Loss: 16.08499507062603, Validation Loss: 55.427947998046875\n",
      "Epoch [60/300], Training Loss: 15.130032049097949, Validation Loss: 54.50644302368164\n",
      "Epoch [61/300], Training Loss: 14.232105257460118, Validation Loss: 53.64103317260742\n",
      "Epoch [62/300], Training Loss: 13.390889771501184, Validation Loss: 52.77285385131836\n",
      "Epoch [63/300], Training Loss: 12.604397710569655, Validation Loss: 52.02500915527344\n",
      "Epoch [64/300], Training Loss: 11.880996417115824, Validation Loss: 51.55571746826172\n",
      "Epoch [65/300], Training Loss: 11.231541393347204, Validation Loss: 51.2921028137207\n",
      "Epoch [66/300], Training Loss: 10.665799398493695, Validation Loss: 50.92610549926758\n",
      "Epoch [67/300], Training Loss: 10.16511207605791, Validation Loss: 50.63146209716797\n",
      "Epoch [68/300], Training Loss: 9.71003683159293, Validation Loss: 50.42272186279297\n",
      "Epoch [69/300], Training Loss: 9.291549779387113, Validation Loss: 50.16889190673828\n",
      "Epoch [70/300], Training Loss: 8.905283440116824, Validation Loss: 49.83511734008789\n",
      "Epoch [71/300], Training Loss: 8.545560739838354, Validation Loss: 49.55852508544922\n",
      "Epoch [72/300], Training Loss: 8.207431396093158, Validation Loss: 49.549407958984375\n",
      "Epoch [73/300], Training Loss: 7.893888031658763, Validation Loss: 49.82670211791992\n",
      "Epoch [74/300], Training Loss: 7.603408023880451, Validation Loss: 50.02137756347656\n",
      "Epoch [75/300], Training Loss: 7.327269873212215, Validation Loss: 49.42108917236328\n",
      "Epoch [76/300], Training Loss: 7.082847420478404, Validation Loss: 47.187625885009766\n",
      "Epoch [77/300], Training Loss: 6.942240615005777, Validation Loss: 46.88750076293945\n",
      "Epoch [78/300], Training Loss: 6.686528842637587, Validation Loss: 44.568355560302734\n",
      "Epoch [79/300], Training Loss: 6.441442402833959, Validation Loss: 41.73176574707031\n",
      "Epoch [80/300], Training Loss: 6.21944853610942, Validation Loss: 39.16396713256836\n",
      "Epoch [81/300], Training Loss: 6.02431659061448, Validation Loss: 37.199039459228516\n",
      "Epoch [82/300], Training Loss: 5.857102545575685, Validation Loss: 35.848541259765625\n",
      "Epoch [83/300], Training Loss: 5.717096681501752, Validation Loss: 34.938751220703125\n",
      "Epoch [84/300], Training Loss: 5.602298788858326, Validation Loss: 34.29279708862305\n",
      "Epoch [85/300], Training Loss: 5.509730642806504, Validation Loss: 33.75763702392578\n",
      "Epoch [86/300], Training Loss: 5.436522012298206, Validation Loss: 33.30991744995117\n",
      "Epoch [87/300], Training Loss: 5.378818063181883, Validation Loss: 32.94064712524414\n",
      "Epoch [88/300], Training Loss: 5.333502266624946, Validation Loss: 32.66281509399414\n",
      "Epoch [89/300], Training Loss: 5.297422707817245, Validation Loss: 32.46401596069336\n",
      "Epoch [90/300], Training Loss: 5.268100724177531, Validation Loss: 32.318702697753906\n",
      "Epoch [91/300], Training Loss: 5.243555532806196, Validation Loss: 32.204288482666016\n",
      "Epoch [92/300], Training Loss: 5.222390302594974, Validation Loss: 32.101444244384766\n",
      "Epoch [93/300], Training Loss: 5.203619960919232, Validation Loss: 32.00030517578125\n",
      "Epoch [94/300], Training Loss: 5.186589931492709, Validation Loss: 31.902690887451172\n",
      "Epoch [95/300], Training Loss: 5.171135727794401, Validation Loss: 31.80499839782715\n",
      "Epoch [96/300], Training Loss: 5.157412520501397, Validation Loss: 31.710018157958984\n",
      "Epoch [97/300], Training Loss: 5.145549429786718, Validation Loss: 31.607675552368164\n",
      "Epoch [98/300], Training Loss: 5.135832587375393, Validation Loss: 31.487565994262695\n",
      "Epoch [99/300], Training Loss: 5.128320770956977, Validation Loss: 31.33686637878418\n",
      "Epoch [100/300], Training Loss: 5.122830502485587, Validation Loss: 31.145048141479492\n",
      "Epoch [101/300], Training Loss: 5.118847683234237, Validation Loss: 30.905508041381836\n",
      "Epoch [102/300], Training Loss: 5.115498862644432, Validation Loss: 30.61968231201172\n",
      "Epoch [103/300], Training Loss: 5.111559993117373, Validation Loss: 30.295494079589844\n",
      "Epoch [104/300], Training Loss: 5.105259742199213, Validation Loss: 29.953516006469727\n",
      "Epoch [105/300], Training Loss: 5.094454276305, Validation Loss: 29.627891540527344\n",
      "Epoch [106/300], Training Loss: 5.076362329174402, Validation Loss: 29.365087509155273\n",
      "Epoch [107/300], Training Loss: 5.048440360957242, Validation Loss: 29.224355697631836\n",
      "Epoch [108/300], Training Loss: 5.009673524864507, Validation Loss: 29.284671783447266\n",
      "Epoch [109/300], Training Loss: 4.961705132158945, Validation Loss: 29.68729591369629\n",
      "Epoch [110/300], Training Loss: 4.908995753061467, Validation Loss: 30.719255447387695\n",
      "Epoch [111/300], Training Loss: 4.85867699654487, Validation Loss: 32.85789108276367\n",
      "Epoch [112/300], Training Loss: 4.820990139540066, Validation Loss: 36.33446502685547\n",
      "Epoch [113/300], Training Loss: 4.800521223788682, Validation Loss: 40.13640594482422\n",
      "Epoch [114/300], Training Loss: 4.7920707267507545, Validation Loss: 42.973358154296875\n",
      "Epoch [115/300], Training Loss: 4.792517337470747, Validation Loss: 44.69110107421875\n",
      "Epoch [116/300], Training Loss: 4.7945202715309225, Validation Loss: 45.67784118652344\n",
      "Epoch [117/300], Training Loss: 4.7928063017931, Validation Loss: 46.45891571044922\n",
      "Epoch [118/300], Training Loss: 4.785044767436334, Validation Loss: 47.219757080078125\n",
      "Epoch [119/300], Training Loss: 4.770740781388346, Validation Loss: 47.92230987548828\n",
      "Epoch [120/300], Training Loss: 4.750973929238965, Validation Loss: 48.46247482299805\n",
      "Epoch [121/300], Training Loss: 4.727347552968096, Validation Loss: 48.795406341552734\n",
      "Epoch [122/300], Training Loss: 4.701184777438968, Validation Loss: 48.927913665771484\n",
      "Epoch [123/300], Training Loss: 4.673270509113762, Validation Loss: 48.91984558105469\n",
      "Epoch [124/300], Training Loss: 4.644011214722768, Validation Loss: 48.82443618774414\n",
      "Epoch [125/300], Training Loss: 4.613640391989016, Validation Loss: 48.71062469482422\n",
      "Epoch [126/300], Training Loss: 4.582696222321403, Validation Loss: 48.656349182128906\n",
      "Epoch [127/300], Training Loss: 4.551918374723561, Validation Loss: 48.735923767089844\n",
      "Epoch [128/300], Training Loss: 4.522298248908157, Validation Loss: 49.009525299072266\n",
      "Epoch [129/300], Training Loss: 4.494948977509669, Validation Loss: 49.502723693847656\n",
      "Epoch [130/300], Training Loss: 4.471060230725958, Validation Loss: 50.19576644897461\n",
      "Epoch [131/300], Training Loss: 4.451063360887978, Validation Loss: 51.01520538330078\n",
      "Epoch [132/300], Training Loss: 4.436006982168976, Validation Loss: 51.91799545288086\n",
      "Epoch [133/300], Training Loss: 4.426090077080588, Validation Loss: 52.85114669799805\n",
      "Epoch [134/300], Training Loss: 4.4199817716111784, Validation Loss: 53.74292755126953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     46\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "input_size = 7\n",
    "hidden_size = 8\n",
    "num_layers = 3\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "# Calculate test loss after training\n",
    "test_loss = 0.0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(x_test_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_test_tensor))\n",
    "\n",
    "        inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "        labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, labels)\n",
    "\n",
    "print(f'Test Loss: {test_loss / len(x_test_tensor)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Training Loss: 17321.558742325018, Validation Loss: 18885.462890625\n",
      "Epoch [2/300], Training Loss: 11926.567034546179, Validation Loss: 13731.6337890625\n",
      "Epoch [3/300], Training Loss: 8786.351654449358, Validation Loss: 11032.8232421875\n",
      "Epoch [4/300], Training Loss: 6675.842657987934, Validation Loss: 8370.7119140625\n",
      "Epoch [5/300], Training Loss: 5050.248162783188, Validation Loss: 5932.76708984375\n",
      "Epoch [6/300], Training Loss: 3850.393570966616, Validation Loss: 4313.80517578125\n",
      "Epoch [7/300], Training Loss: 2899.5824525393873, Validation Loss: 3221.017333984375\n",
      "Epoch [8/300], Training Loss: 2207.1526585015986, Validation Loss: 2448.67578125\n",
      "Epoch [9/300], Training Loss: 1714.35814106997, Validation Loss: 1920.7452392578125\n",
      "Epoch [10/300], Training Loss: 1346.6013982689726, Validation Loss: 1543.0526123046875\n",
      "Epoch [11/300], Training Loss: 1077.4193558688632, Validation Loss: 1279.5889892578125\n",
      "Epoch [12/300], Training Loss: 864.9449481852663, Validation Loss: 1062.318115234375\n",
      "Epoch [13/300], Training Loss: 694.8953529073651, Validation Loss: 869.75927734375\n",
      "Epoch [14/300], Training Loss: 556.692113961206, Validation Loss: 690.5906372070312\n",
      "Epoch [15/300], Training Loss: 442.9441205220258, Validation Loss: 562.5009765625\n",
      "Epoch [16/300], Training Loss: 350.5916327736585, Validation Loss: 481.9317932128906\n",
      "Epoch [17/300], Training Loss: 277.0945458072224, Validation Loss: 398.3890075683594\n",
      "Epoch [18/300], Training Loss: 218.89689105208325, Validation Loss: 314.3085632324219\n",
      "Epoch [19/300], Training Loss: 174.14045040103483, Validation Loss: 252.70181274414062\n",
      "Epoch [20/300], Training Loss: 138.7626777057204, Validation Loss: 219.3713836669922\n",
      "Epoch [21/300], Training Loss: 111.13309897289899, Validation Loss: 181.5823211669922\n",
      "Epoch [22/300], Training Loss: 89.38026283561565, Validation Loss: 147.1841278076172\n",
      "Epoch [23/300], Training Loss: 72.10691131392066, Validation Loss: 129.39501953125\n",
      "Epoch [24/300], Training Loss: 58.911726381026945, Validation Loss: 115.69300842285156\n",
      "Epoch [25/300], Training Loss: 49.22048265143061, Validation Loss: 110.35799407958984\n",
      "Epoch [26/300], Training Loss: 42.14749349452214, Validation Loss: 110.05316925048828\n",
      "Epoch [27/300], Training Loss: 36.764762188375016, Validation Loss: 110.03853607177734\n",
      "Epoch [28/300], Training Loss: 31.883944684773343, Validation Loss: 106.35568237304688\n",
      "Epoch [29/300], Training Loss: 27.66119370625986, Validation Loss: 100.24240112304688\n",
      "Epoch [30/300], Training Loss: 24.192320406834646, Validation Loss: 93.58702850341797\n",
      "Epoch [31/300], Training Loss: 21.278422985166397, Validation Loss: 86.57951354980469\n",
      "Epoch [32/300], Training Loss: 18.751075603498048, Validation Loss: 81.19782257080078\n",
      "Epoch [33/300], Training Loss: 16.78253559619083, Validation Loss: 74.6297378540039\n",
      "Epoch [34/300], Training Loss: 15.181894491547533, Validation Loss: 68.11505126953125\n",
      "Epoch [35/300], Training Loss: 13.844724596718656, Validation Loss: 62.561805725097656\n",
      "Epoch [36/300], Training Loss: 12.724276138259437, Validation Loss: 58.019744873046875\n",
      "Epoch [37/300], Training Loss: 11.782119324653246, Validation Loss: 54.182132720947266\n",
      "Epoch [38/300], Training Loss: 10.987364281559588, Validation Loss: 50.66927719116211\n",
      "Epoch [39/300], Training Loss: 10.31567479577322, Validation Loss: 47.2337760925293\n",
      "Epoch [40/300], Training Loss: 9.748181366892178, Validation Loss: 43.80844497680664\n",
      "Epoch [41/300], Training Loss: 9.269309929618943, Validation Loss: 40.46756362915039\n",
      "Epoch [42/300], Training Loss: 8.865305182739915, Validation Loss: 37.340087890625\n",
      "Epoch [43/300], Training Loss: 8.522978272644707, Validation Loss: 34.529972076416016\n",
      "Epoch [44/300], Training Loss: 8.22961770103518, Validation Loss: 32.06251525878906\n",
      "Epoch [45/300], Training Loss: 7.973677225691058, Validation Loss: 29.892953872680664\n",
      "Epoch [46/300], Training Loss: 7.7447586623537665, Validation Loss: 27.96042251586914\n",
      "Epoch [47/300], Training Loss: 7.534817476414113, Validation Loss: 26.201631546020508\n",
      "Epoch [48/300], Training Loss: 7.337766824331539, Validation Loss: 24.59790802001953\n",
      "Epoch [49/300], Training Loss: 7.149773258573456, Validation Loss: 23.19731903076172\n",
      "Epoch [50/300], Training Loss: 6.969279587491223, Validation Loss: 22.050704956054688\n",
      "Epoch [51/300], Training Loss: 6.79698741666361, Validation Loss: 21.165590286254883\n",
      "Epoch [52/300], Training Loss: 6.6360043494763925, Validation Loss: 20.514450073242188\n",
      "Epoch [53/300], Training Loss: 6.4916513200989066, Validation Loss: 20.057485580444336\n",
      "Epoch [54/300], Training Loss: 6.368336335542554, Validation Loss: 19.789949417114258\n",
      "Epoch [55/300], Training Loss: 6.266679554418458, Validation Loss: 19.73130989074707\n",
      "Epoch [56/300], Training Loss: 6.183431871685123, Validation Loss: 19.87592887878418\n",
      "Epoch [57/300], Training Loss: 6.11349820883069, Validation Loss: 20.18963623046875\n",
      "Epoch [58/300], Training Loss: 6.052047787993279, Validation Loss: 20.63921356201172\n",
      "Epoch [59/300], Training Loss: 5.995461896425927, Validation Loss: 21.208194732666016\n",
      "Epoch [60/300], Training Loss: 5.941525089743523, Validation Loss: 21.89350700378418\n",
      "Epoch [61/300], Training Loss: 5.88885314872334, Validation Loss: 22.69216537475586\n",
      "Epoch [62/300], Training Loss: 5.836488628666176, Validation Loss: 23.589332580566406\n",
      "Epoch [63/300], Training Loss: 5.78408937474608, Validation Loss: 24.557762145996094\n",
      "Epoch [64/300], Training Loss: 5.7314561893578695, Validation Loss: 25.556684494018555\n",
      "Epoch [65/300], Training Loss: 5.678694998745904, Validation Loss: 26.545330047607422\n",
      "Epoch [66/300], Training Loss: 5.626443781201588, Validation Loss: 27.488344192504883\n",
      "Epoch [67/300], Training Loss: 5.575581956377512, Validation Loss: 28.36223030090332\n",
      "Epoch [68/300], Training Loss: 5.527603310021825, Validation Loss: 29.155424118041992\n",
      "Epoch [69/300], Training Loss: 5.484502481843997, Validation Loss: 29.85657501220703\n",
      "Epoch [70/300], Training Loss: 5.448329443984225, Validation Loss: 30.453718185424805\n",
      "Epoch [71/300], Training Loss: 5.42067413916132, Validation Loss: 30.938339233398438\n",
      "Epoch [72/300], Training Loss: 5.401917286354517, Validation Loss: 31.314271926879883\n",
      "Epoch [73/300], Training Loss: 5.390153153291553, Validation Loss: 31.603668212890625\n",
      "Epoch [74/300], Training Loss: 5.38128628610951, Validation Loss: 31.847070693969727\n",
      "Epoch [75/300], Training Loss: 5.370324405043188, Validation Loss: 32.098655700683594\n",
      "Epoch [76/300], Training Loss: 5.353334739912031, Validation Loss: 32.411617279052734\n",
      "Epoch [77/300], Training Loss: 5.3294241507508975, Validation Loss: 32.831016540527344\n",
      "Epoch [78/300], Training Loss: 5.3003529721674525, Validation Loss: 33.382747650146484\n",
      "Epoch [79/300], Training Loss: 5.268954712712007, Validation Loss: 34.05580520629883\n",
      "Epoch [80/300], Training Loss: 5.237357103817599, Validation Loss: 34.821197509765625\n",
      "Epoch [81/300], Training Loss: 5.206525542382071, Validation Loss: 35.65314865112305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m x_val_tensor[i:window_end]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     58\u001b[0m         labels \u001b[38;5;241m=\u001b[39m y_val_tensor[window_end \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 60\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(x_train_tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(x_val_tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     15\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, (h0, c0))\n\u001b[1;32m---> 17\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "input_size = 7\n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "# Calculate test loss after training\n",
    "test_loss = 0.0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(x_test_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_test_tensor))\n",
    "\n",
    "        inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "        labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, labels)\n",
    "\n",
    "print(f'Test Loss: {test_loss / len(x_test_tensor)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Training Loss: 17677.66852652012, Validation Loss: 18983.8828125\n",
      "Epoch [2/300], Training Loss: 13314.560460591827, Validation Loss: 19043.494140625\n",
      "Epoch [3/300], Training Loss: 9415.50923587024, Validation Loss: 13117.5380859375\n",
      "Epoch [4/300], Training Loss: 7162.658139457555, Validation Loss: 11429.8466796875\n",
      "Epoch [5/300], Training Loss: 5448.702984772235, Validation Loss: 8781.126953125\n",
      "Epoch [6/300], Training Loss: 4167.971235568658, Validation Loss: 5319.03271484375\n",
      "Epoch [7/300], Training Loss: 3147.9685416147195, Validation Loss: 3695.0166015625\n",
      "Epoch [8/300], Training Loss: 2385.342645815755, Validation Loss: 2712.600341796875\n",
      "Epoch [9/300], Training Loss: 1852.4095450932532, Validation Loss: 2096.551025390625\n",
      "Epoch [10/300], Training Loss: 1446.0212057942897, Validation Loss: 1649.9329833984375\n",
      "Epoch [11/300], Training Loss: 1161.8481102767985, Validation Loss: 1360.052001953125\n",
      "Epoch [12/300], Training Loss: 931.3764763119127, Validation Loss: 1122.692138671875\n",
      "Epoch [13/300], Training Loss: 746.1535072301848, Validation Loss: 911.971923828125\n",
      "Epoch [14/300], Training Loss: 597.0569524832147, Validation Loss: 740.6105346679688\n",
      "Epoch [15/300], Training Loss: 472.651446473328, Validation Loss: 587.3031616210938\n",
      "Epoch [16/300], Training Loss: 372.955816417758, Validation Loss: 478.62127685546875\n",
      "Epoch [17/300], Training Loss: 291.2696025688389, Validation Loss: 380.97991943359375\n",
      "Epoch [18/300], Training Loss: 226.9800534997991, Validation Loss: 302.20086669921875\n",
      "Epoch [19/300], Training Loss: 179.13976956357305, Validation Loss: 244.10743713378906\n",
      "Epoch [20/300], Training Loss: 143.08755973430698, Validation Loss: 205.02320861816406\n",
      "Epoch [21/300], Training Loss: 112.56116595940415, Validation Loss: 155.27627563476562\n",
      "Epoch [22/300], Training Loss: 89.7373179503668, Validation Loss: 125.71489715576172\n",
      "Epoch [23/300], Training Loss: 72.26237351190498, Validation Loss: 106.81385040283203\n",
      "Epoch [24/300], Training Loss: 58.77586695695059, Validation Loss: 96.3805160522461\n",
      "Epoch [25/300], Training Loss: 48.94077683169004, Validation Loss: 86.47041320800781\n",
      "Epoch [26/300], Training Loss: 41.83712282445548, Validation Loss: 78.25621032714844\n",
      "Epoch [27/300], Training Loss: 36.30534157457097, Validation Loss: 71.00843048095703\n",
      "Epoch [28/300], Training Loss: 31.393218385439535, Validation Loss: 66.07093811035156\n",
      "Epoch [29/300], Training Loss: 27.259276796603153, Validation Loss: 62.344024658203125\n",
      "Epoch [30/300], Training Loss: 23.754628734075904, Validation Loss: 59.74834060668945\n",
      "Epoch [31/300], Training Loss: 20.787530179767348, Validation Loss: 57.33980178833008\n",
      "Epoch [32/300], Training Loss: 18.40862966548792, Validation Loss: 55.598121643066406\n",
      "Epoch [33/300], Training Loss: 16.516150329812042, Validation Loss: 54.40003204345703\n",
      "Epoch [34/300], Training Loss: 14.97294124840696, Validation Loss: 53.206695556640625\n",
      "Epoch [35/300], Training Loss: 13.698569367682405, Validation Loss: 52.18234634399414\n",
      "Epoch [36/300], Training Loss: 12.635899392317716, Validation Loss: 51.507423400878906\n",
      "Epoch [37/300], Training Loss: 11.737982376798014, Validation Loss: 51.108062744140625\n",
      "Epoch [38/300], Training Loss: 10.961536556105074, Validation Loss: 50.76993942260742\n",
      "Epoch [39/300], Training Loss: 10.276290765696585, Validation Loss: 50.423484802246094\n",
      "Epoch [40/300], Training Loss: 9.670238139456233, Validation Loss: 50.14799118041992\n",
      "Epoch [41/300], Training Loss: 9.137005234163558, Validation Loss: 50.06304931640625\n",
      "Epoch [42/300], Training Loss: 8.669890441972088, Validation Loss: 50.19036102294922\n",
      "Epoch [43/300], Training Loss: 8.261525202744602, Validation Loss: 50.42923355102539\n",
      "Epoch [44/300], Training Loss: 7.905041318371999, Validation Loss: 50.601619720458984\n",
      "Epoch [45/300], Training Loss: 7.594628154636944, Validation Loss: 50.57145690917969\n",
      "Epoch [46/300], Training Loss: 7.327837597247386, Validation Loss: 50.390621185302734\n",
      "Epoch [47/300], Training Loss: 7.1034939707257925, Validation Loss: 50.18599319458008\n",
      "Epoch [48/300], Training Loss: 6.918899660406657, Validation Loss: 50.06932830810547\n",
      "Epoch [49/300], Training Loss: 6.7699416170041475, Validation Loss: 50.154579162597656\n",
      "Epoch [50/300], Training Loss: 6.652516512532342, Validation Loss: 50.47477340698242\n",
      "Epoch [51/300], Training Loss: 6.561597333991865, Validation Loss: 50.973358154296875\n",
      "Epoch [52/300], Training Loss: 6.491683354000854, Validation Loss: 51.57007598876953\n",
      "Epoch [53/300], Training Loss: 6.437246069882784, Validation Loss: 52.20560073852539\n",
      "Epoch [54/300], Training Loss: 6.3937378040257995, Validation Loss: 52.85538101196289\n",
      "Epoch [55/300], Training Loss: 6.357728978766323, Validation Loss: 53.52532958984375\n",
      "Epoch [56/300], Training Loss: 6.3269003692239885, Validation Loss: 54.23952865600586\n",
      "Epoch [57/300], Training Loss: 6.2998777460012905, Validation Loss: 55.029144287109375\n",
      "Epoch [58/300], Training Loss: 6.2759039125970615, Validation Loss: 55.916542053222656\n",
      "Epoch [59/300], Training Loss: 6.254508406810465, Validation Loss: 56.91099548339844\n",
      "Epoch [60/300], Training Loss: 6.23563535639203, Validation Loss: 58.0067024230957\n",
      "Epoch [61/300], Training Loss: 6.219233405895303, Validation Loss: 59.18163299560547\n",
      "Epoch [62/300], Training Loss: 6.205405662709061, Validation Loss: 60.39931106567383\n",
      "Epoch [63/300], Training Loss: 6.194325875237886, Validation Loss: 61.60972595214844\n",
      "Epoch [64/300], Training Loss: 6.1862991437100465, Validation Loss: 62.75691223144531\n",
      "Epoch [65/300], Training Loss: 6.181703653543846, Validation Loss: 63.78797149658203\n",
      "Epoch [66/300], Training Loss: 6.180830275821087, Validation Loss: 64.65233612060547\n",
      "Epoch [67/300], Training Loss: 6.183774054968339, Validation Loss: 65.31401824951172\n",
      "Epoch [68/300], Training Loss: 6.190309527782473, Validation Loss: 65.74566650390625\n",
      "Epoch [69/300], Training Loss: 6.199766715205546, Validation Loss: 65.92720031738281\n",
      "Epoch [70/300], Training Loss: 6.211133877488899, Validation Loss: 65.84173583984375\n",
      "Epoch [71/300], Training Loss: 6.223022104445854, Validation Loss: 65.47880554199219\n",
      "Epoch [72/300], Training Loss: 6.233987877467161, Validation Loss: 64.83404541015625\n",
      "Epoch [73/300], Training Loss: 6.242735673777482, Validation Loss: 63.926509857177734\n",
      "Epoch [74/300], Training Loss: 6.248260306535405, Validation Loss: 62.797645568847656\n",
      "Epoch [75/300], Training Loss: 6.249957352129731, Validation Loss: 61.51730728149414\n",
      "Epoch [76/300], Training Loss: 6.247703049087026, Validation Loss: 60.17584991455078\n",
      "Epoch [77/300], Training Loss: 6.241638422070403, Validation Loss: 58.87960433959961\n",
      "Epoch [78/300], Training Loss: 6.232080854175137, Validation Loss: 57.72753143310547\n",
      "Epoch [79/300], Training Loss: 6.219381944125517, Validation Loss: 56.790489196777344\n",
      "Epoch [80/300], Training Loss: 6.203338319685164, Validation Loss: 56.0756721496582\n",
      "Epoch [81/300], Training Loss: 6.183311934635473, Validation Loss: 55.52821731567383\n",
      "Epoch [82/300], Training Loss: 6.158744641693648, Validation Loss: 55.072940826416016\n",
      "Epoch [83/300], Training Loss: 6.129861311530635, Validation Loss: 54.65024948120117\n",
      "Epoch [84/300], Training Loss: 6.097327677933715, Validation Loss: 54.24013137817383\n",
      "Epoch [85/300], Training Loss: 6.061829412124349, Validation Loss: 53.865753173828125\n",
      "Epoch [86/300], Training Loss: 6.023766613654179, Validation Loss: 53.61805725097656\n",
      "Epoch [87/300], Training Loss: 5.9835337468499485, Validation Loss: 53.6209716796875\n",
      "Epoch [88/300], Training Loss: 5.942353687778954, Validation Loss: 53.79528045654297\n",
      "Epoch [89/300], Training Loss: 5.900936381402, Validation Loss: 53.92570495605469\n",
      "Epoch [90/300], Training Loss: 5.858727812934463, Validation Loss: 53.92456817626953\n",
      "Epoch [91/300], Training Loss: 5.814728964529433, Validation Loss: 53.789642333984375\n",
      "Epoch [92/300], Training Loss: 5.7687780859403555, Validation Loss: 53.46585464477539\n",
      "Epoch [93/300], Training Loss: 5.721653955852871, Validation Loss: 52.76382064819336\n",
      "Epoch [94/300], Training Loss: 5.6747684534666725, Validation Loss: 51.641845703125\n",
      "Epoch [95/300], Training Loss: 5.629974397860558, Validation Loss: 50.391136169433594\n",
      "Epoch [96/300], Training Loss: 5.587891852044517, Validation Loss: 49.338897705078125\n",
      "Epoch [97/300], Training Loss: 5.546125559988992, Validation Loss: 48.74644088745117\n",
      "Epoch [98/300], Training Loss: 5.508617139575854, Validation Loss: 48.89365005493164\n",
      "Epoch [99/300], Training Loss: 5.477555994555614, Validation Loss: 49.960472106933594\n",
      "Epoch [100/300], Training Loss: 5.445678378154373, Validation Loss: 51.730804443359375\n",
      "Epoch [101/300], Training Loss: 5.410373543186135, Validation Loss: 53.56377410888672\n",
      "Epoch [102/300], Training Loss: 5.3693254797748455, Validation Loss: 54.582393646240234\n",
      "Epoch [103/300], Training Loss: 5.322990462531704, Validation Loss: 54.77986526489258\n",
      "Epoch [104/300], Training Loss: 5.269899511241676, Validation Loss: 55.28773880004883\n",
      "Epoch [105/300], Training Loss: 5.212452141556969, Validation Loss: 57.393619537353516\n",
      "Epoch [106/300], Training Loss: 5.162357001539182, Validation Loss: 61.1864128112793\n",
      "Epoch [107/300], Training Loss: 5.1233857821608435, Validation Loss: 66.64336395263672\n",
      "Epoch [108/300], Training Loss: 5.091084493183661, Validation Loss: 73.19660949707031\n",
      "Epoch [109/300], Training Loss: 5.057610018547798, Validation Loss: 79.18877410888672\n",
      "Epoch [110/300], Training Loss: 5.0208851815023205, Validation Loss: 84.01258087158203\n",
      "Epoch [111/300], Training Loss: 4.9839962569492595, Validation Loss: 88.71879577636719\n",
      "Epoch [112/300], Training Loss: 4.958647986244548, Validation Loss: 95.42886352539062\n",
      "Epoch [113/300], Training Loss: 4.9592386659321965, Validation Loss: 105.4332275390625\n",
      "Epoch [114/300], Training Loss: 4.991077276691496, Validation Loss: 118.21382904052734\n",
      "Epoch [115/300], Training Loss: 5.045402594790839, Validation Loss: 132.10400390625\n",
      "Epoch [116/300], Training Loss: 5.1020969220374335, Validation Loss: 144.62461853027344\n",
      "Epoch [117/300], Training Loss: 5.137105986876746, Validation Loss: 152.89877319335938\n",
      "Epoch [118/300], Training Loss: 5.131245226322853, Validation Loss: 155.21241760253906\n",
      "Epoch [119/300], Training Loss: 5.080173082181781, Validation Loss: 151.3172149658203\n",
      "Epoch [120/300], Training Loss: 4.9914483389275155, Validation Loss: 142.0260772705078\n",
      "Epoch [121/300], Training Loss: 4.8777802388428695, Validation Loss: 128.68490600585938\n",
      "Epoch [122/300], Training Loss: 4.752102735661147, Validation Loss: 112.8836669921875\n",
      "Epoch [123/300], Training Loss: 4.625178550474961, Validation Loss: 96.26488494873047\n",
      "Epoch [124/300], Training Loss: 4.505170507591059, Validation Loss: 80.4375\n",
      "Epoch [125/300], Training Loss: 4.398615837638159, Validation Loss: 66.66534423828125\n",
      "Epoch [126/300], Training Loss: 4.3098230539440765, Validation Loss: 55.5599250793457\n",
      "Epoch [127/300], Training Loss: 4.239157817051583, Validation Loss: 47.02278137207031\n",
      "Epoch [128/300], Training Loss: 4.182918867899555, Validation Loss: 40.56812286376953\n",
      "Epoch [129/300], Training Loss: 4.135901823900525, Validation Loss: 35.716434478759766\n",
      "Epoch [130/300], Training Loss: 4.093658021047277, Validation Loss: 32.16572570800781\n",
      "Epoch [131/300], Training Loss: 4.053480974123505, Validation Loss: 29.772985458374023\n",
      "Epoch [132/300], Training Loss: 4.014679751403682, Validation Loss: 28.4630069732666\n",
      "Epoch [133/300], Training Loss: 3.9782441128286283, Validation Loss: 28.143796920776367\n",
      "Epoch [134/300], Training Loss: 3.945852106466999, Validation Loss: 28.66085433959961\n",
      "Epoch [135/300], Training Loss: 3.9195103337774464, Validation Loss: 29.795766830444336\n",
      "Epoch [136/300], Training Loss: 3.9026977637614855, Validation Loss: 31.323450088500977\n",
      "Epoch [137/300], Training Loss: 3.8969377112197323, Validation Loss: 33.0052490234375\n",
      "Epoch [138/300], Training Loss: 3.9036038843191956, Validation Loss: 34.631290435791016\n",
      "Epoch [139/300], Training Loss: 3.9237096734251184, Validation Loss: 36.02816390991211\n",
      "Epoch [140/300], Training Loss: 3.956850008007013, Validation Loss: 37.08177185058594\n",
      "Epoch [141/300], Training Loss: 4.0033961932427005, Validation Loss: 37.79743957519531\n",
      "Epoch [142/300], Training Loss: 4.062753271960119, Validation Loss: 38.25126266479492\n",
      "Epoch [143/300], Training Loss: 4.13300347834114, Validation Loss: 38.5679931640625\n",
      "Epoch [144/300], Training Loss: 4.211140271239363, Validation Loss: 38.854793548583984\n",
      "Epoch [145/300], Training Loss: 4.296380802587763, Validation Loss: 39.1748161315918\n",
      "Epoch [146/300], Training Loss: 4.3882973699448105, Validation Loss: 39.542991638183594\n",
      "Epoch [147/300], Training Loss: 4.483827992762659, Validation Loss: 39.93638229370117\n",
      "Epoch [148/300], Training Loss: 4.576719676276584, Validation Loss: 40.30300521850586\n",
      "Epoch [149/300], Training Loss: 4.657434054537932, Validation Loss: 40.59605026245117\n",
      "Epoch [150/300], Training Loss: 4.717638276462403, Validation Loss: 40.759979248046875\n",
      "Epoch [151/300], Training Loss: 4.756029390177558, Validation Loss: 40.7547721862793\n",
      "Epoch [152/300], Training Loss: 4.777043559238228, Validation Loss: 40.577877044677734\n",
      "Epoch [153/300], Training Loss: 4.785165772436703, Validation Loss: 40.26362609863281\n",
      "Epoch [154/300], Training Loss: 4.782864698817391, Validation Loss: 39.844017028808594\n",
      "Epoch [155/300], Training Loss: 4.7715803854750325, Validation Loss: 39.33456802368164\n",
      "Epoch [156/300], Training Loss: 4.752640874826829, Validation Loss: 38.73260498046875\n",
      "Epoch [157/300], Training Loss: 4.727449866688636, Validation Loss: 38.023433685302734\n",
      "Epoch [158/300], Training Loss: 4.697012002053026, Validation Loss: 37.19869613647461\n",
      "Epoch [159/300], Training Loss: 4.661889261995275, Validation Loss: 36.26693344116211\n",
      "Epoch [160/300], Training Loss: 4.622447904669811, Validation Loss: 35.25535583496094\n",
      "Epoch [161/300], Training Loss: 4.57916833926693, Validation Loss: 34.22575759887695\n",
      "Epoch [162/300], Training Loss: 4.532519453732385, Validation Loss: 33.23872756958008\n",
      "Epoch [163/300], Training Loss: 4.483247026194973, Validation Loss: 32.35032272338867\n",
      "Epoch [164/300], Training Loss: 4.43214915353799, Validation Loss: 31.585973739624023\n",
      "Epoch [165/300], Training Loss: 4.380087441574967, Validation Loss: 30.953889846801758\n",
      "Epoch [166/300], Training Loss: 4.327888879309895, Validation Loss: 30.44428253173828\n",
      "Epoch [167/300], Training Loss: 4.2761799610576166, Validation Loss: 30.037433624267578\n",
      "Epoch [168/300], Training Loss: 4.2255119326833945, Validation Loss: 29.703153610229492\n",
      "Epoch [169/300], Training Loss: 4.176369230823749, Validation Loss: 29.419435501098633\n",
      "Epoch [170/300], Training Loss: 4.129190711342692, Validation Loss: 29.162952423095703\n",
      "Epoch [171/300], Training Loss: 4.08434967869463, Validation Loss: 28.918649673461914\n",
      "Epoch [172/300], Training Loss: 4.042221139801414, Validation Loss: 28.680625915527344\n",
      "Epoch [173/300], Training Loss: 4.002965883431374, Validation Loss: 28.446443557739258\n",
      "Epoch [174/300], Training Loss: 3.9666454279977614, Validation Loss: 28.223899841308594\n",
      "Epoch [175/300], Training Loss: 3.933254817373327, Validation Loss: 28.02583885192871\n",
      "Epoch [176/300], Training Loss: 3.902696058419949, Validation Loss: 27.86678123474121\n",
      "Epoch [177/300], Training Loss: 3.8747957277442957, Validation Loss: 27.766998291015625\n",
      "Epoch [178/300], Training Loss: 3.8494592836084993, Validation Loss: 27.753276824951172\n",
      "Epoch [179/300], Training Loss: 3.826707490130928, Validation Loss: 27.856687545776367\n",
      "Epoch [180/300], Training Loss: 3.806801588246432, Validation Loss: 28.11868667602539\n",
      "Epoch [181/300], Training Loss: 3.79029212753947, Validation Loss: 28.570974349975586\n",
      "Epoch [182/300], Training Loss: 3.7781724393755813, Validation Loss: 29.214458465576172\n",
      "Epoch [183/300], Training Loss: 3.772297016913201, Validation Loss: 30.026554107666016\n",
      "Epoch [184/300], Training Loss: 3.7749209939905057, Validation Loss: 30.978044509887695\n",
      "Epoch [185/300], Training Loss: 3.7890063971555956, Validation Loss: 32.045536041259766\n",
      "Epoch [186/300], Training Loss: 3.818706982955382, Validation Loss: 33.23544692993164\n",
      "Epoch [187/300], Training Loss: 3.868005357077371, Validation Loss: 34.58191680908203\n",
      "Epoch [188/300], Training Loss: 3.938027430346186, Validation Loss: 36.12004852294922\n",
      "Epoch [189/300], Training Loss: 4.023995649923026, Validation Loss: 37.804962158203125\n",
      "Epoch [190/300], Training Loss: 4.114823958405936, Validation Loss: 39.42189025878906\n",
      "Epoch [191/300], Training Loss: 4.197551207616135, Validation Loss: 40.67405319213867\n",
      "Epoch [192/300], Training Loss: 4.263345693058697, Validation Loss: 41.37641525268555\n",
      "Epoch [193/300], Training Loss: 4.309769614884262, Validation Loss: 41.53670883178711\n",
      "Epoch [194/300], Training Loss: 4.338886689178961, Validation Loss: 41.26054382324219\n",
      "Epoch [195/300], Training Loss: 4.354151114677858, Validation Loss: 40.665489196777344\n",
      "Epoch [196/300], Training Loss: 4.359021946523079, Validation Loss: 39.8416862487793\n",
      "Epoch [197/300], Training Loss: 4.356181047586894, Validation Loss: 38.84717559814453\n",
      "Epoch [198/300], Training Loss: 4.347562028057942, Validation Loss: 37.728302001953125\n",
      "Epoch [199/300], Training Loss: 4.334418637498701, Validation Loss: 36.515769958496094\n",
      "Epoch [200/300], Training Loss: 4.317364031533068, Validation Loss: 35.23935317993164\n",
      "Epoch [201/300], Training Loss: 4.2967455885996815, Validation Loss: 33.91941833496094\n",
      "Epoch [202/300], Training Loss: 4.272700227708572, Validation Loss: 32.575679779052734\n",
      "Epoch [203/300], Training Loss: 4.245293755937442, Validation Loss: 31.22663688659668\n",
      "Epoch [204/300], Training Loss: 4.2146420053017, Validation Loss: 29.892362594604492\n",
      "Epoch [205/300], Training Loss: 4.180945418238047, Validation Loss: 28.595182418823242\n",
      "Epoch [206/300], Training Loss: 4.144598315704587, Validation Loss: 27.348613739013672\n",
      "Epoch [207/300], Training Loss: 4.106035036141807, Validation Loss: 26.170808792114258\n",
      "Epoch [208/300], Training Loss: 4.065795787153698, Validation Loss: 25.076772689819336\n",
      "Epoch [209/300], Training Loss: 4.0245291170190765, Validation Loss: 24.078205108642578\n",
      "Epoch [210/300], Training Loss: 3.98304457294233, Validation Loss: 23.194541931152344\n",
      "Epoch [211/300], Training Loss: 3.942126961941822, Validation Loss: 22.451438903808594\n",
      "Epoch [212/300], Training Loss: 3.9023308227452196, Validation Loss: 21.880855560302734\n",
      "Epoch [213/300], Training Loss: 3.863612991756737, Validation Loss: 21.50163459777832\n",
      "Epoch [214/300], Training Loss: 3.8252668974609714, Validation Loss: 21.305198669433594\n",
      "Epoch [215/300], Training Loss: 3.7858792200398117, Validation Loss: 21.258174896240234\n",
      "Epoch [216/300], Training Loss: 3.7442399932423105, Validation Loss: 21.32725715637207\n",
      "Epoch [217/300], Training Loss: 3.7001991067622746, Validation Loss: 21.488815307617188\n",
      "Epoch [218/300], Training Loss: 3.6546855901581714, Validation Loss: 21.81147003173828\n",
      "Epoch [219/300], Training Loss: 3.6084040403792907, Validation Loss: 22.414255142211914\n",
      "Epoch [220/300], Training Loss: 3.561577837853967, Validation Loss: 23.137271881103516\n",
      "Epoch [221/300], Training Loss: 3.5140218547506916, Validation Loss: 23.76262664794922\n",
      "Epoch [222/300], Training Loss: 3.4645173091487256, Validation Loss: 24.13691520690918\n",
      "Epoch [223/300], Training Loss: 3.4117878761770517, Validation Loss: 24.17075538635254\n",
      "Epoch [224/300], Training Loss: 3.355377649410984, Validation Loss: 23.926406860351562\n",
      "Epoch [225/300], Training Loss: 3.2986587815287276, Validation Loss: 23.611740112304688\n",
      "Epoch [226/300], Training Loss: 3.2486958243204365, Validation Loss: 23.29444694519043\n",
      "Epoch [227/300], Training Loss: 3.210321293131239, Validation Loss: 23.296836853027344\n",
      "Epoch [228/300], Training Loss: 3.192547467634344, Validation Loss: 24.056692123413086\n",
      "Epoch [229/300], Training Loss: 3.198499941354444, Validation Loss: 26.006074905395508\n",
      "Epoch [230/300], Training Loss: 3.178855200400664, Validation Loss: 28.6295108795166\n",
      "Epoch [231/300], Training Loss: 3.1247794993644207, Validation Loss: 31.214515686035156\n",
      "Epoch [232/300], Training Loss: 3.075597735425654, Validation Loss: 33.84147262573242\n",
      "Epoch [233/300], Training Loss: 3.027232031703026, Validation Loss: 35.76970291137695\n",
      "Epoch [234/300], Training Loss: 2.9891423090743596, Validation Loss: 36.50298309326172\n",
      "Epoch [235/300], Training Loss: 2.96606375327776, Validation Loss: 36.244415283203125\n",
      "Epoch [236/300], Training Loss: 2.958052469451194, Validation Loss: 35.617000579833984\n",
      "Epoch [237/300], Training Loss: 2.9638070544065966, Validation Loss: 35.107093811035156\n",
      "Epoch [238/300], Training Loss: 2.9812853196003832, Validation Loss: 34.91355514526367\n",
      "Epoch [239/300], Training Loss: 3.007604752883978, Validation Loss: 35.08155822753906\n",
      "Epoch [240/300], Training Loss: 3.038175603388243, Validation Loss: 35.594573974609375\n",
      "Epoch [241/300], Training Loss: 3.0663831604538134, Validation Loss: 36.304229736328125\n",
      "Epoch [242/300], Training Loss: 3.084943896320644, Validation Loss: 36.89683532714844\n",
      "Epoch [243/300], Training Loss: 3.0903709921736713, Validation Loss: 37.12747573852539\n",
      "Epoch [244/300], Training Loss: 3.0859509983430162, Validation Loss: 37.14238357543945\n",
      "Epoch [245/300], Training Loss: 3.0776045375680106, Validation Loss: 37.3792839050293\n",
      "Epoch [246/300], Training Loss: 3.0691907690078275, Validation Loss: 38.23689651489258\n",
      "Epoch [247/300], Training Loss: 3.0624511068854705, Validation Loss: 39.865360260009766\n",
      "Epoch [248/300], Training Loss: 3.0584591334189444, Validation Loss: 42.18172073364258\n",
      "Epoch [249/300], Training Loss: 3.058038914145339, Validation Loss: 45.00410461425781\n",
      "Epoch [250/300], Training Loss: 3.061392519122212, Validation Loss: 48.10963439941406\n",
      "Epoch [251/300], Training Loss: 3.068030313135559, Validation Loss: 51.25519561767578\n",
      "Epoch [252/300], Training Loss: 3.0772058653635956, Validation Loss: 54.23311233520508\n",
      "Epoch [253/300], Training Loss: 3.088029718068511, Validation Loss: 56.82605743408203\n",
      "Epoch [254/300], Training Loss: 3.099891316473187, Validation Loss: 58.83506774902344\n",
      "Epoch [255/300], Training Loss: 3.1124318114669447, Validation Loss: 60.08837127685547\n",
      "Epoch [256/300], Training Loss: 3.125397032989088, Validation Loss: 60.43695068359375\n",
      "Epoch [257/300], Training Loss: 3.138394507850471, Validation Loss: 59.78607177734375\n",
      "Epoch [258/300], Training Loss: 3.150973703643668, Validation Loss: 58.14484405517578\n",
      "Epoch [259/300], Training Loss: 3.1626102980546418, Validation Loss: 55.61862564086914\n",
      "Epoch [260/300], Training Loss: 3.1728722403641103, Validation Loss: 52.40215301513672\n",
      "Epoch [261/300], Training Loss: 3.1814205987183506, Validation Loss: 48.72564697265625\n",
      "Epoch [262/300], Training Loss: 3.1880433391441545, Validation Loss: 44.835758209228516\n",
      "Epoch [263/300], Training Loss: 3.1927005774919444, Validation Loss: 40.97744369506836\n",
      "Epoch [264/300], Training Loss: 3.195318197897951, Validation Loss: 37.34968185424805\n",
      "Epoch [265/300], Training Loss: 3.1959491605456734, Validation Loss: 34.11898422241211\n",
      "Epoch [266/300], Training Loss: 3.1944926655242183, Validation Loss: 31.373493194580078\n",
      "Epoch [267/300], Training Loss: 3.1912031806739187, Validation Loss: 29.136905670166016\n",
      "Epoch [268/300], Training Loss: 3.186432113666641, Validation Loss: 27.368127822875977\n",
      "Epoch [269/300], Training Loss: 3.18059883295181, Validation Loss: 26.00177001953125\n",
      "Epoch [270/300], Training Loss: 3.174103400236419, Validation Loss: 24.949228286743164\n",
      "Epoch [271/300], Training Loss: 3.167287268783235, Validation Loss: 24.127286911010742\n",
      "Epoch [272/300], Training Loss: 3.1605053124629854, Validation Loss: 23.46202850341797\n",
      "Epoch [273/300], Training Loss: 3.154031935845626, Validation Loss: 22.903711318969727\n",
      "Epoch [274/300], Training Loss: 3.148105940597923, Validation Loss: 22.413898468017578\n",
      "Epoch [275/300], Training Loss: 3.142904137501244, Validation Loss: 21.96802520751953\n",
      "Epoch [276/300], Training Loss: 3.1386218752385333, Validation Loss: 21.550905227661133\n",
      "Epoch [277/300], Training Loss: 3.1355071083687935, Validation Loss: 21.150381088256836\n",
      "Epoch [278/300], Training Loss: 3.1340014243698207, Validation Loss: 20.759462356567383\n",
      "Epoch [279/300], Training Loss: 3.13470718637722, Validation Loss: 20.395174026489258\n",
      "Epoch [280/300], Training Loss: 3.1379938522727677, Validation Loss: 20.075828552246094\n",
      "Epoch [281/300], Training Loss: 3.1439777575087957, Validation Loss: 19.842098236083984\n",
      "Epoch [282/300], Training Loss: 3.152573968569754, Validation Loss: 19.723339080810547\n",
      "Epoch [283/300], Training Loss: 3.1634325822019975, Validation Loss: 19.725149154663086\n",
      "Epoch [284/300], Training Loss: 3.175721223242183, Validation Loss: 19.830921173095703\n",
      "Epoch [285/300], Training Loss: 3.1885094776541885, Validation Loss: 20.0211181640625\n",
      "Epoch [286/300], Training Loss: 3.2010613442741023, Validation Loss: 20.29169464111328\n",
      "Epoch [287/300], Training Loss: 3.2130170614710467, Validation Loss: 20.659202575683594\n",
      "Epoch [288/300], Training Loss: 3.224437239211438, Validation Loss: 21.142892837524414\n",
      "Epoch [289/300], Training Loss: 3.2354506274693744, Validation Loss: 21.707727432250977\n",
      "Epoch [290/300], Training Loss: 3.246360104563749, Validation Loss: 22.324148178100586\n",
      "Epoch [291/300], Training Loss: 3.2574421841634797, Validation Loss: 22.926679611206055\n",
      "Epoch [292/300], Training Loss: 3.2688004063024274, Validation Loss: 23.435033798217773\n",
      "Epoch [293/300], Training Loss: 3.280425202695027, Validation Loss: 23.767921447753906\n",
      "Epoch [294/300], Training Loss: 3.2921405140045366, Validation Loss: 23.884647369384766\n",
      "Epoch [295/300], Training Loss: 3.3036818964658745, Validation Loss: 23.79798698425293\n",
      "Epoch [296/300], Training Loss: 3.314704559323868, Validation Loss: 23.5777530670166\n",
      "Epoch [297/300], Training Loss: 3.3248674116151724, Validation Loss: 23.314834594726562\n",
      "Epoch [298/300], Training Loss: 3.3338011662649674, Validation Loss: 23.09310531616211\n",
      "Epoch [299/300], Training Loss: 3.3412355296423257, Validation Loss: 22.957847595214844\n",
      "Epoch [300/300], Training Loss: 3.3470668992606023, Validation Loss: 22.933134078979492\n",
      "Test Loss: 24.611595153808594\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "input_size = 7\n",
    "hidden_size = 16\n",
    "num_layers = 3\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "# Calculate test loss after training\n",
    "test_loss = 0.0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(x_test_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_test_tensor))\n",
    "\n",
    "        inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "        labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, labels)\n",
    "\n",
    "print(f'Test Loss: {test_loss / len(x_test_tensor)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ä' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mä\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ä' is not defined"
     ]
    }
   ],
   "source": [
    "ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/75], Training Loss: 17670.56007460006, Validation Loss: 18982.166015625\n",
      "Epoch [2/75], Training Loss: 13317.910812193204, Validation Loss: 19067.453125\n",
      "Epoch [3/75], Training Loss: 9439.795367973113, Validation Loss: 12179.8896484375\n",
      "Epoch [4/75], Training Loss: 7184.406515963105, Validation Loss: 9667.3212890625\n",
      "Epoch [5/75], Training Loss: 5462.750215013006, Validation Loss: 7110.85986328125\n",
      "Epoch [6/75], Training Loss: 4184.5808915649195, Validation Loss: 5073.791015625\n",
      "Epoch [7/75], Training Loss: 3162.2398899728682, Validation Loss: 3616.109619140625\n",
      "Epoch [8/75], Training Loss: 2396.2662249388904, Validation Loss: 2747.4453125\n",
      "Epoch [9/75], Training Loss: 1865.7307931666057, Validation Loss: 2163.51708984375\n",
      "Epoch [10/75], Training Loss: 1457.1492026851538, Validation Loss: 1701.108154296875\n",
      "Epoch [11/75], Training Loss: 1169.6494664359911, Validation Loss: 1412.09716796875\n",
      "Epoch [12/75], Training Loss: 938.6326561313589, Validation Loss: 1139.8902587890625\n",
      "Epoch [13/75], Training Loss: 752.9811966073405, Validation Loss: 914.830322265625\n",
      "Epoch [14/75], Training Loss: 602.0895792727671, Validation Loss: 738.0444946289062\n",
      "Epoch [15/75], Training Loss: 477.5395328602408, Validation Loss: 591.336181640625\n",
      "Epoch [16/75], Training Loss: 377.09176880479725, Validation Loss: 482.0337829589844\n",
      "Epoch [17/75], Training Loss: 295.8440728785427, Validation Loss: 398.35931396484375\n",
      "Epoch [18/75], Training Loss: 230.57453106044483, Validation Loss: 315.73065185546875\n",
      "Epoch [19/75], Training Loss: 181.68110114013393, Validation Loss: 256.8752746582031\n",
      "Epoch [20/75], Training Loss: 144.79540766068766, Validation Loss: 214.2524871826172\n",
      "Epoch [21/75], Training Loss: 116.61979497762981, Validation Loss: 172.1431121826172\n",
      "Epoch [22/75], Training Loss: 91.33575727798652, Validation Loss: 164.7792205810547\n",
      "Epoch [23/75], Training Loss: 73.22046950809401, Validation Loss: 152.9483184814453\n",
      "Epoch [24/75], Training Loss: 59.528371777791996, Validation Loss: 143.96063232421875\n",
      "Epoch [25/75], Training Loss: 49.35172945004537, Validation Loss: 121.76033782958984\n",
      "Epoch [26/75], Training Loss: 42.08757319460297, Validation Loss: 90.5838851928711\n",
      "Epoch [27/75], Training Loss: 36.51708552975693, Validation Loss: 74.7271728515625\n",
      "Epoch [28/75], Training Loss: 31.824490999277256, Validation Loss: 67.49867248535156\n",
      "Epoch [29/75], Training Loss: 27.52545045678411, Validation Loss: 63.16394805908203\n",
      "Epoch [30/75], Training Loss: 23.902883036251414, Validation Loss: 62.09684371948242\n",
      "Epoch [31/75], Training Loss: 20.789871461949296, Validation Loss: 62.756370544433594\n",
      "Epoch [32/75], Training Loss: 18.318369176857964, Validation Loss: 64.38162994384766\n",
      "Epoch [33/75], Training Loss: 16.35684141708655, Validation Loss: 67.2054443359375\n",
      "Epoch [34/75], Training Loss: 14.734048809487176, Validation Loss: 70.54646301269531\n",
      "Epoch [35/75], Training Loss: 13.388672592557281, Validation Loss: 74.56427764892578\n",
      "Epoch [36/75], Training Loss: 12.284642016005858, Validation Loss: 78.81560516357422\n",
      "Epoch [37/75], Training Loss: 11.392222790635843, Validation Loss: 82.85675811767578\n",
      "Epoch [38/75], Training Loss: 10.672989264408848, Validation Loss: 86.61518859863281\n",
      "Epoch [39/75], Training Loss: 10.098293177517801, Validation Loss: 90.00155639648438\n",
      "Epoch [40/75], Training Loss: 9.656389343788849, Validation Loss: 93.53109741210938\n",
      "Epoch [41/75], Training Loss: 9.32618561496859, Validation Loss: 98.89762115478516\n",
      "Epoch [42/75], Training Loss: 9.0883446093833, Validation Loss: 105.83831787109375\n",
      "Epoch [43/75], Training Loss: 8.950352477003365, Validation Loss: 112.30858612060547\n",
      "Epoch [44/75], Training Loss: 8.953340132346405, Validation Loss: 118.44996643066406\n",
      "Epoch [45/75], Training Loss: 9.079871254611726, Validation Loss: 123.16184997558594\n",
      "Epoch [46/75], Training Loss: 9.404505651046325, Validation Loss: 124.839599609375\n",
      "Epoch [47/75], Training Loss: 9.905574849820196, Validation Loss: 124.90475463867188\n",
      "Epoch [48/75], Training Loss: 9.859150197828058, Validation Loss: 124.22869110107422\n",
      "Epoch [49/75], Training Loss: 9.645682533877261, Validation Loss: 125.9144058227539\n",
      "Epoch [50/75], Training Loss: 9.512128040229989, Validation Loss: 126.52889251708984\n",
      "Epoch [51/75], Training Loss: 9.351244075841343, Validation Loss: 123.43492889404297\n",
      "Epoch [52/75], Training Loss: 9.227583589008995, Validation Loss: 117.6176528930664\n",
      "Epoch [53/75], Training Loss: 9.215566595823821, Validation Loss: 108.81999969482422\n",
      "Epoch [54/75], Training Loss: 9.186934378979007, Validation Loss: 95.5088882446289\n",
      "Epoch [55/75], Training Loss: 9.008139818152262, Validation Loss: 83.68755340576172\n",
      "Epoch [56/75], Training Loss: 8.715799600920187, Validation Loss: 75.7385025024414\n",
      "Epoch [57/75], Training Loss: 8.404518824181112, Validation Loss: 70.21121978759766\n",
      "Epoch [58/75], Training Loss: 8.107029434153375, Validation Loss: 66.26508331298828\n",
      "Epoch [59/75], Training Loss: 7.848133060955322, Validation Loss: 63.634315490722656\n",
      "Epoch [60/75], Training Loss: 7.635837789856524, Validation Loss: 61.916038513183594\n",
      "Epoch [61/75], Training Loss: 7.4701865659056885, Validation Loss: 60.69050216674805\n",
      "Epoch [62/75], Training Loss: 7.340678814225533, Validation Loss: 59.56331253051758\n",
      "Epoch [63/75], Training Loss: 7.23476894755788, Validation Loss: 58.299903869628906\n",
      "Epoch [64/75], Training Loss: 7.141470417225376, Validation Loss: 56.89608383178711\n",
      "Epoch [65/75], Training Loss: 7.052290754781063, Validation Loss: 55.47885513305664\n",
      "Epoch [66/75], Training Loss: 6.963532321992285, Validation Loss: 54.18377685546875\n",
      "Epoch [67/75], Training Loss: 6.874977406062773, Validation Loss: 53.08229446411133\n",
      "Epoch [68/75], Training Loss: 6.786715108052501, Validation Loss: 52.083824157714844\n",
      "Epoch [69/75], Training Loss: 6.697151491676927, Validation Loss: 50.98466491699219\n",
      "Epoch [70/75], Training Loss: 6.604613269686402, Validation Loss: 49.70418930053711\n",
      "Epoch [71/75], Training Loss: 6.509239574840143, Validation Loss: 48.34893035888672\n",
      "Epoch [72/75], Training Loss: 6.414099424206557, Validation Loss: 47.03377914428711\n",
      "Epoch [73/75], Training Loss: 6.322506131481145, Validation Loss: 45.91468811035156\n",
      "Epoch [74/75], Training Loss: 6.237023634746387, Validation Loss: 45.228885650634766\n",
      "Epoch [75/75], Training Loss: 6.160703176501113, Validation Loss: 45.186859130859375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "input_size = 7\n",
    "hidden_size = 16\n",
    "num_layers = 3\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 75\n",
    "\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ö' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mö\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ö' is not defined"
     ]
    }
   ],
   "source": [
    "ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 17730.7561042866, Validation Loss: 18999.392578125\n",
      "Epoch [2/100], Training Loss: 14282.744352764164, Validation Loss: 18811.552734375\n",
      "Epoch [3/100], Training Loss: 10679.519531336262, Validation Loss: 21464.904296875\n",
      "Epoch [4/100], Training Loss: 8058.161620794091, Validation Loss: 14014.69921875\n",
      "Epoch [5/100], Training Loss: 6092.340851367106, Validation Loss: 10865.939453125\n",
      "Epoch [6/100], Training Loss: 4633.005494101313, Validation Loss: 7484.09619140625\n",
      "Epoch [7/100], Training Loss: 3535.1883968077527, Validation Loss: 5420.81201171875\n",
      "Epoch [8/100], Training Loss: 2684.151329534713, Validation Loss: 4247.88330078125\n",
      "Epoch [9/100], Training Loss: 2095.565768319551, Validation Loss: 3262.3642578125\n",
      "Epoch [10/100], Training Loss: 1660.5040386335654, Validation Loss: 2421.912109375\n",
      "Epoch [11/100], Training Loss: 1348.481363661686, Validation Loss: 1877.48583984375\n",
      "Epoch [12/100], Training Loss: 1116.5901727804567, Validation Loss: 1623.978515625\n",
      "Epoch [13/100], Training Loss: 925.2368712346184, Validation Loss: 1527.53466796875\n",
      "Epoch [14/100], Training Loss: 768.1851781127239, Validation Loss: 1310.991455078125\n",
      "Epoch [15/100], Training Loss: 635.2525653420014, Validation Loss: 1036.8519287109375\n",
      "Epoch [16/100], Training Loss: 528.2211845999462, Validation Loss: 1040.7301025390625\n",
      "Epoch [17/100], Training Loss: 431.68308798952927, Validation Loss: 806.314208984375\n",
      "Epoch [18/100], Training Loss: 353.70981777067954, Validation Loss: 804.6159057617188\n",
      "Epoch [19/100], Training Loss: 288.70586683185684, Validation Loss: 741.1495361328125\n",
      "Epoch [20/100], Training Loss: 236.95744440883422, Validation Loss: 697.879150390625\n",
      "Epoch [21/100], Training Loss: 198.9385457979859, Validation Loss: 886.2335815429688\n",
      "Epoch [22/100], Training Loss: 164.0878902570925, Validation Loss: 599.1897583007812\n",
      "Epoch [23/100], Training Loss: 138.37004358915124, Validation Loss: 603.5235595703125\n",
      "Epoch [24/100], Training Loss: 113.67185867227718, Validation Loss: 436.2955627441406\n",
      "Epoch [25/100], Training Loss: 95.87218320136724, Validation Loss: 443.53155517578125\n",
      "Epoch [26/100], Training Loss: 80.80757488254268, Validation Loss: 434.5276184082031\n",
      "Epoch [27/100], Training Loss: 68.77241623532251, Validation Loss: 373.5683898925781\n",
      "Epoch [28/100], Training Loss: 59.7028272281436, Validation Loss: 340.70526123046875\n",
      "Epoch [29/100], Training Loss: 51.58377541002536, Validation Loss: 290.12725830078125\n",
      "Epoch [30/100], Training Loss: 46.354495079981504, Validation Loss: 255.9193115234375\n",
      "Epoch [31/100], Training Loss: 42.02407400748015, Validation Loss: 210.97866821289062\n",
      "Epoch [32/100], Training Loss: 38.60214746346018, Validation Loss: 169.38026428222656\n",
      "Epoch [33/100], Training Loss: 35.504385123654515, Validation Loss: 142.13076782226562\n",
      "Epoch [34/100], Training Loss: 32.39151227282247, Validation Loss: 122.9510726928711\n",
      "Epoch [35/100], Training Loss: 29.781485770827068, Validation Loss: 114.29288482666016\n",
      "Epoch [36/100], Training Loss: 27.331586595763028, Validation Loss: 108.2876205444336\n",
      "Epoch [37/100], Training Loss: 24.597190975259174, Validation Loss: 102.57994079589844\n",
      "Epoch [38/100], Training Loss: 22.279690654398667, Validation Loss: 95.44319915771484\n",
      "Epoch [39/100], Training Loss: 20.483303861843105, Validation Loss: 92.65154266357422\n",
      "Epoch [40/100], Training Loss: 19.04544489679544, Validation Loss: 90.6221923828125\n",
      "Epoch [41/100], Training Loss: 17.850553697211726, Validation Loss: 88.55804443359375\n",
      "Epoch [42/100], Training Loss: 16.839286861935296, Validation Loss: 85.53858184814453\n",
      "Epoch [43/100], Training Loss: 15.981173985777337, Validation Loss: 81.89334106445312\n",
      "Epoch [44/100], Training Loss: 15.255083847813772, Validation Loss: 78.55866241455078\n",
      "Epoch [45/100], Training Loss: 14.640999690618667, Validation Loss: 76.27098846435547\n",
      "Epoch [46/100], Training Loss: 14.119686690466981, Validation Loss: 75.25367736816406\n",
      "Epoch [47/100], Training Loss: 13.684404315911516, Validation Loss: 75.2450180053711\n",
      "Epoch [48/100], Training Loss: 13.325692007056016, Validation Loss: 74.98887634277344\n",
      "Epoch [49/100], Training Loss: 13.013140411082395, Validation Loss: 71.98756408691406\n",
      "Epoch [50/100], Training Loss: 12.718363655273587, Validation Loss: 67.44558715820312\n",
      "Epoch [51/100], Training Loss: 12.382254770423305, Validation Loss: 64.0715560913086\n",
      "Epoch [52/100], Training Loss: 12.016545732239146, Validation Loss: 60.91023635864258\n",
      "Epoch [53/100], Training Loss: 11.880231022023267, Validation Loss: 64.34060668945312\n",
      "Epoch [54/100], Training Loss: 11.804556172793324, Validation Loss: 65.61538696289062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m labels \u001b[38;5;241m=\u001b[39m y_train_tensor[window_end \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Label is the last value in the window\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     13\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 15\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    816\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "input_size = 7\n",
    "hidden_size = 16\n",
    "num_layers = 4\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "öö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 17720.693621782913, Validation Loss: 18996.841796875\n",
      "Epoch [2/50], Training Loss: 14272.882685991912, Validation Loss: 18814.1875\n",
      "Epoch [3/50], Training Loss: 10674.680861884675, Validation Loss: 21226.615234375\n",
      "Epoch [4/50], Training Loss: 8588.614925458753, Validation Loss: 15010.2470703125\n",
      "Epoch [5/50], Training Loss: 6185.099233565122, Validation Loss: 11488.2373046875\n",
      "Epoch [6/50], Training Loss: 4684.1439117069285, Validation Loss: 8399.1201171875\n",
      "Epoch [7/50], Training Loss: 3575.1960553159265, Validation Loss: 5951.4814453125\n",
      "Epoch [8/50], Training Loss: 2705.8135135599478, Validation Loss: 3569.144775390625\n",
      "Epoch [9/50], Training Loss: 2090.6786706377957, Validation Loss: 2486.810302734375\n",
      "Epoch [10/50], Training Loss: 1637.921615724722, Validation Loss: 1955.5142822265625\n",
      "Epoch [11/50], Training Loss: 1318.6214037300406, Validation Loss: 1642.831298828125\n",
      "Epoch [12/50], Training Loss: 1073.980153752395, Validation Loss: 1423.8170166015625\n",
      "Epoch [13/50], Training Loss: 873.8980563554439, Validation Loss: 1229.25830078125\n",
      "Epoch [14/50], Training Loss: 710.850214089544, Validation Loss: 1070.8802490234375\n",
      "Epoch [15/50], Training Loss: 579.0495139924725, Validation Loss: 966.5640869140625\n",
      "Epoch [16/50], Training Loss: 461.2772768735165, Validation Loss: 802.1657104492188\n",
      "Epoch [17/50], Training Loss: 366.30663433151136, Validation Loss: 635.6139526367188\n",
      "Epoch [18/50], Training Loss: 289.387465672417, Validation Loss: 514.0381469726562\n",
      "Epoch [19/50], Training Loss: 229.49455017630555, Validation Loss: 444.2955017089844\n",
      "Epoch [20/50], Training Loss: 183.96339864980285, Validation Loss: 440.4447326660156\n",
      "Epoch [21/50], Training Loss: 149.35516471001304, Validation Loss: 250.17889404296875\n",
      "Epoch [22/50], Training Loss: 122.66373100313962, Validation Loss: 224.22113037109375\n",
      "Epoch [23/50], Training Loss: 98.51966388982909, Validation Loss: 241.66680908203125\n",
      "Epoch [24/50], Training Loss: 81.17418912826922, Validation Loss: 260.76123046875\n",
      "Epoch [25/50], Training Loss: 66.11096723025591, Validation Loss: 193.70614624023438\n",
      "Epoch [26/50], Training Loss: 55.52036771013444, Validation Loss: 159.6788330078125\n",
      "Epoch [27/50], Training Loss: 47.280591259557774, Validation Loss: 141.1053466796875\n",
      "Epoch [28/50], Training Loss: 41.18092123715209, Validation Loss: 121.565185546875\n",
      "Epoch [29/50], Training Loss: 36.292520699643575, Validation Loss: 100.79888916015625\n",
      "Epoch [30/50], Training Loss: 31.746619420614962, Validation Loss: 95.67321014404297\n",
      "Epoch [31/50], Training Loss: 28.412996299275115, Validation Loss: 93.42192840576172\n",
      "Epoch [32/50], Training Loss: 24.815374112649938, Validation Loss: 79.5185317993164\n",
      "Epoch [33/50], Training Loss: 21.67415716730893, Validation Loss: 75.51873016357422\n",
      "Epoch [34/50], Training Loss: 19.5844704837192, Validation Loss: 66.45893859863281\n",
      "Epoch [35/50], Training Loss: 17.835037548144935, Validation Loss: 61.37159729003906\n",
      "Epoch [36/50], Training Loss: 16.362177726597473, Validation Loss: 56.77275848388672\n",
      "Epoch [37/50], Training Loss: 15.093350059251796, Validation Loss: 52.9870491027832\n",
      "Epoch [38/50], Training Loss: 14.092402586270515, Validation Loss: 49.59032440185547\n",
      "Epoch [39/50], Training Loss: 13.373908288118502, Validation Loss: 48.20771789550781\n",
      "Epoch [40/50], Training Loss: 12.900366234542568, Validation Loss: 49.24219512939453\n",
      "Epoch [41/50], Training Loss: 12.58971989578578, Validation Loss: 51.06938934326172\n",
      "Epoch [42/50], Training Loss: 12.336046917163511, Validation Loss: 52.64528274536133\n",
      "Epoch [43/50], Training Loss: 12.119257037729858, Validation Loss: 52.84413146972656\n",
      "Epoch [44/50], Training Loss: 11.970608677983634, Validation Loss: 52.35999298095703\n",
      "Epoch [45/50], Training Loss: 11.971730430728117, Validation Loss: 52.04774475097656\n",
      "Epoch [46/50], Training Loss: 12.181901170757277, Validation Loss: 52.4902229309082\n",
      "Epoch [47/50], Training Loss: 12.52735883417288, Validation Loss: 53.26655197143555\n",
      "Epoch [48/50], Training Loss: 12.567368555494541, Validation Loss: 50.900638580322266\n",
      "Epoch [49/50], Training Loss: 12.392607001523954, Validation Loss: 49.440330505371094\n",
      "Epoch [50/50], Training Loss: 12.018990841036125, Validation Loss: 50.97494125366211\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "input_size = 7\n",
    "hidden_size = 16\n",
    "num_layers = 4\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
