{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date    open    high     low   close      volume  adjusted_close  \\\n",
      "10746  2023-07-31  196.06  196.49  195.26  196.45  38824100.0        196.1851   \n",
      "\n",
      "       change_percent  avg_vol_20d  \n",
      "10746            0.32   49803320.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "print(data.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform: date to torch: date_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with a 'date' column containing dates from 1980-12-12 to 2023-07-31\n",
    "data2 = pd.DataFrame({'date': pd.date_range(start='1980-12-12', end='2023-07-31')})\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "data2['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Extract day, month, and year from the date column\n",
    "data['day'] = data2['date'].dt.day\n",
    "data['month'] = data2['date'].dt.month\n",
    "data['year'] = data2['date'].dt.year\n",
    "\n",
    "# Define the embedding dimensions\n",
    "embedding_dim = 1  # You can adjust this dimension as needed\n",
    "\n",
    "# Create embedding layers for day, month, and year\n",
    "day_embedding = nn.Embedding(32, embedding_dim)  # 0-31 days\n",
    "month_embedding = nn.Embedding(13, embedding_dim)  # 1-12 months\n",
    "year_embedding = nn.Embedding(44, embedding_dim)  # Embedding for years from 1980 to 2023\n",
    "\n",
    "# Convert day, month, and year to tensors with Long data type\n",
    "day_tensor = torch.LongTensor(data['day'].values)\n",
    "month_tensor = torch.LongTensor(data['month'].values)\n",
    "year_tensor = torch.LongTensor(data['year'].values - 1980)  # Convert years to an index from 0 to 43\n",
    "\n",
    "\n",
    "# Pass tensors through embedding layers to get embeddings\n",
    "day_embeddings = day_embedding(day_tensor)\n",
    "month_embeddings = month_embedding(month_tensor)\n",
    "year_embeddings = year_embedding(year_tensor)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "date_embeddings = torch.cat((day_embeddings, month_embeddings, year_embeddings), dim=1)\n",
    "\n",
    "# Print the resulting embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform feature to tensor \n",
    "Concatenate date embeddings with data\n",
    "Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "y_data = data[\"close\"].values.reshape(-1, 1)  # Reshape y_data if it's 1D\n",
    "\n",
    "# Concatenate date embeddings with data\n",
    "x_feature_tensors = torch.tensor(x_data, dtype=torch.float32)\n",
    "x_combined = torch.cat((x_feature_tensors, date_embeddings), dim=1)\n",
    "\n",
    "y_feature_tensors = torch.tensor(y_data, dtype=torch.float32)\n",
    "y_combined = torch.cat((y_feature_tensors, date_embeddings), dim=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(x_combined) * 0.67)\n",
    "x_train, x_test = x_combined[:train_size], x_combined[train_size:]\n",
    "y_train, y_test = y_combined[:train_size], y_combined[train_size:]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "\n",
    "# Fit the scaler only on the training data and transform both training and test data\n",
    "x_train_scaled = scaler.fit_transform(x_train.detach().numpy())\n",
    "y_train_scaled = scaler.fit_transform(y_train.detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "# fitting the MinMaxScaler to your training data using fit_transform and then\n",
    "# applying the same transformation (based on the statistics learned) to your test data using transform\n",
    "x_train_scaled = scaler.fit_transform(x_train.detach().numpy())\n",
    "x_test_scaled = scaler.transform(x_test.detach().numpy())  # Use transform, not fit_transform\n",
    "\n",
    "y_train_scaled = scaler.fit_transform(y_train.detach().numpy())\n",
    "y_test_scaled = scaler.transform(y_test.detach().numpy())  # Use transform, not fit_transform\n",
    "\n",
    "# Convert back to tensors if needed\n",
    "x_train_scaled = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_test_scaled = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "\n",
    "y_train_scaled = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "y_test_scaled = torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model \n",
    "Initialize the model, loss, and optimizer\n",
    "Training using Walk-Forward Validation\n",
    "Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_6112\\1959663930.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_window = torch.tensor(x_train_scaled[start_idx:end_idx], dtype=torch.float32)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_6112\\1959663930.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_window = torch.tensor(y_train_scaled[start_idx:end_idx], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([50, 4])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_6112\\1959663930.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_window = torch.tensor(x_test_scaled[:test_window_size], dtype=torch.float32)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_6112\\1959663930.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_window = torch.tensor(y_test_scaled[:test_window_size], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, Train Loss: 0.19831621646881104, Test Loss: 0.2544884979724884\n",
      "Iteration 60, Train Loss: 0.11528627574443817, Test Loss: 0.10409238934516907\n",
      "Iteration 70, Train Loss: 0.10092315822839737, Test Loss: 0.13713563978672028\n",
      "Iteration 80, Train Loss: 0.08645936101675034, Test Loss: 0.13324856758117676\n",
      "Iteration 90, Train Loss: 0.07585611939430237, Test Loss: 0.13573899865150452\n",
      "Iteration 100, Train Loss: 0.07785789668560028, Test Loss: 0.15177147090435028\n",
      "Iteration 110, Train Loss: 0.0873619094491005, Test Loss: 0.14349226653575897\n",
      "Iteration 120, Train Loss: 0.09891732037067413, Test Loss: 0.13104158639907837\n",
      "Iteration 130, Train Loss: 0.10746145248413086, Test Loss: 0.1289627104997635\n",
      "Iteration 140, Train Loss: 0.12318892776966095, Test Loss: 0.11817094683647156\n",
      "Iteration 150, Train Loss: 0.13573236763477325, Test Loss: 0.11377139389514923\n",
      "Iteration 160, Train Loss: 0.14070811867713928, Test Loss: 0.11447585374116898\n",
      "Iteration 170, Train Loss: 0.1264667958021164, Test Loss: 0.11776977777481079\n",
      "Iteration 180, Train Loss: 0.1142650693655014, Test Loss: 0.13328632712364197\n",
      "Iteration 190, Train Loss: 0.1143808588385582, Test Loss: 0.1454067826271057\n",
      "Iteration 200, Train Loss: 0.11729267984628677, Test Loss: 0.14677853882312775\n",
      "Iteration 210, Train Loss: 0.1063297837972641, Test Loss: 0.14634892344474792\n",
      "Iteration 220, Train Loss: 0.11938634514808655, Test Loss: 0.14300194382667542\n",
      "Iteration 230, Train Loss: 0.12570995092391968, Test Loss: 0.11928332597017288\n",
      "Iteration 240, Train Loss: 0.10990405082702637, Test Loss: 0.13688531517982483\n",
      "Iteration 250, Train Loss: 0.09753434360027313, Test Loss: 0.1632966250181198\n",
      "Iteration 260, Train Loss: 0.10284432768821716, Test Loss: 0.17191779613494873\n",
      "Iteration 270, Train Loss: 0.11097261309623718, Test Loss: 0.15589432418346405\n",
      "Iteration 280, Train Loss: 0.12230776995420456, Test Loss: 0.14038510620594025\n",
      "Iteration 290, Train Loss: 0.12727956473827362, Test Loss: 0.11094936728477478\n",
      "Iteration 300, Train Loss: 0.11407754570245743, Test Loss: 0.10764709115028381\n",
      "Iteration 310, Train Loss: 0.10254092514514923, Test Loss: 0.10584402084350586\n",
      "Iteration 320, Train Loss: 0.09364551305770874, Test Loss: 0.1063399538397789\n",
      "Iteration 330, Train Loss: 0.08745595812797546, Test Loss: 0.10679066926240921\n",
      "Iteration 340, Train Loss: 0.0761958658695221, Test Loss: 0.10744982957839966\n",
      "Iteration 350, Train Loss: 0.07616057246923447, Test Loss: 0.1073060929775238\n",
      "Iteration 360, Train Loss: 0.08284241706132889, Test Loss: 0.11563455313444138\n",
      "Iteration 370, Train Loss: 0.09005530178546906, Test Loss: 0.11401108652353287\n",
      "Iteration 380, Train Loss: 0.09612333029508591, Test Loss: 0.11073116958141327\n",
      "Iteration 390, Train Loss: 0.10826563090085983, Test Loss: 0.10695333778858185\n",
      "Iteration 400, Train Loss: 0.12005500495433807, Test Loss: 0.10593920946121216\n",
      "Iteration 410, Train Loss: 0.12455075234174728, Test Loss: 0.10555880516767502\n",
      "Iteration 420, Train Loss: 0.11995519697666168, Test Loss: 0.10539470613002777\n",
      "Iteration 430, Train Loss: 0.10996396839618683, Test Loss: 0.10404600203037262\n",
      "Iteration 440, Train Loss: 0.10797800123691559, Test Loss: 0.11765217781066895\n",
      "Iteration 450, Train Loss: 0.10809875279664993, Test Loss: 0.11527445167303085\n",
      "Iteration 460, Train Loss: 0.10064447671175003, Test Loss: 0.11548946052789688\n",
      "Iteration 470, Train Loss: 0.0996536985039711, Test Loss: 0.11120707541704178\n",
      "Iteration 480, Train Loss: 0.10411858558654785, Test Loss: 0.10451913625001907\n",
      "Iteration 490, Train Loss: 0.09981999546289444, Test Loss: 0.10483207553625107\n",
      "Iteration 500, Train Loss: 0.09184516966342926, Test Loss: 0.1097361147403717\n",
      "Iteration 510, Train Loss: 0.08952489495277405, Test Loss: 0.14085260033607483\n",
      "Iteration 520, Train Loss: 0.09708618372678757, Test Loss: 0.13655316829681396\n",
      "Iteration 530, Train Loss: 0.10296836495399475, Test Loss: 0.13452382385730743\n",
      "Iteration 540, Train Loss: 0.10866507887840271, Test Loss: 0.12259668111801147\n",
      "Iteration 550, Train Loss: 0.1019367203116417, Test Loss: 0.11349188536405563\n",
      "Iteration 560, Train Loss: 0.09607213735580444, Test Loss: 0.11562783271074295\n",
      "Iteration 570, Train Loss: 0.08866842091083527, Test Loss: 0.12685266137123108\n",
      "Iteration 580, Train Loss: 0.07996668666601181, Test Loss: 0.12953242659568787\n",
      "Iteration 590, Train Loss: 0.06613463163375854, Test Loss: 0.11351282149553299\n",
      "Iteration 600, Train Loss: 0.061464037746191025, Test Loss: 0.1239275336265564\n",
      "Iteration 610, Train Loss: 0.06504302471876144, Test Loss: 0.15343612432479858\n",
      "Iteration 620, Train Loss: 0.07023569941520691, Test Loss: 0.13459235429763794\n",
      "Iteration 630, Train Loss: 0.0748230442404747, Test Loss: 0.13677310943603516\n",
      "Iteration 640, Train Loss: 0.0839984118938446, Test Loss: 0.1265915483236313\n",
      "Iteration 650, Train Loss: 0.09660127758979797, Test Loss: 0.11920713633298874\n",
      "Iteration 660, Train Loss: 0.10412080585956573, Test Loss: 0.11899716407060623\n",
      "Iteration 670, Train Loss: 0.10530252754688263, Test Loss: 0.10607818514108658\n",
      "Iteration 680, Train Loss: 0.09786651283502579, Test Loss: 0.10808593779802322\n",
      "Iteration 690, Train Loss: 0.09371257573366165, Test Loss: 0.11635782569646835\n",
      "Iteration 700, Train Loss: 0.09782920777797699, Test Loss: 0.20437578856945038\n",
      "Iteration 710, Train Loss: 0.09929104894399643, Test Loss: 0.17657379806041718\n",
      "Iteration 720, Train Loss: 0.09770888090133667, Test Loss: 0.15287376940250397\n",
      "Iteration 730, Train Loss: 0.10662592202425003, Test Loss: 0.14779677987098694\n",
      "Iteration 740, Train Loss: 0.10954497009515762, Test Loss: 0.10745063424110413\n",
      "Iteration 750, Train Loss: 0.09548163414001465, Test Loss: 0.12869131565093994\n",
      "Iteration 760, Train Loss: 0.08730543404817581, Test Loss: 0.21964706480503082\n",
      "Iteration 770, Train Loss: 0.09916969388723373, Test Loss: 0.20462845265865326\n",
      "Iteration 780, Train Loss: 0.11046089231967926, Test Loss: 0.17493554949760437\n",
      "Iteration 790, Train Loss: 0.12638896703720093, Test Loss: 0.1798257976770401\n",
      "Iteration 800, Train Loss: 0.13117772340774536, Test Loss: 0.15243779122829437\n",
      "Iteration 810, Train Loss: 0.12780150771141052, Test Loss: 0.16936028003692627\n",
      "Iteration 820, Train Loss: 0.12303432822227478, Test Loss: 0.1709568351507187\n",
      "Iteration 830, Train Loss: 0.1162976548075676, Test Loss: 0.19723278284072876\n",
      "Iteration 840, Train Loss: 0.10496482998132706, Test Loss: 0.16609518229961395\n",
      "Iteration 850, Train Loss: 0.09094500541687012, Test Loss: 0.15635724365711212\n",
      "Iteration 860, Train Loss: 0.09240849316120148, Test Loss: 0.19986310601234436\n",
      "Iteration 870, Train Loss: 0.0995640829205513, Test Loss: 0.234196737408638\n",
      "Iteration 880, Train Loss: 0.10848771035671234, Test Loss: 0.2330898642539978\n",
      "Iteration 890, Train Loss: 0.11828862875699997, Test Loss: 0.2276315838098526\n",
      "Iteration 900, Train Loss: 0.1397532820701599, Test Loss: 0.22687672078609467\n",
      "Iteration 910, Train Loss: 0.1487630009651184, Test Loss: 0.21218547224998474\n",
      "Iteration 920, Train Loss: 0.15299659967422485, Test Loss: 0.20683684945106506\n",
      "Iteration 930, Train Loss: 0.13558658957481384, Test Loss: 0.12111425399780273\n",
      "Iteration 940, Train Loss: 0.12039116024971008, Test Loss: 0.1667061299085617\n",
      "Iteration 950, Train Loss: 0.11834825575351715, Test Loss: 0.14688748121261597\n",
      "Iteration 960, Train Loss: 0.12006651610136032, Test Loss: 0.14370957016944885\n",
      "Iteration 970, Train Loss: 0.1094932109117508, Test Loss: 0.14545659720897675\n",
      "Iteration 980, Train Loss: 0.12117087095975876, Test Loss: 0.13803791999816895\n",
      "Iteration 990, Train Loss: 0.12860916554927826, Test Loss: 0.15067130327224731\n",
      "Iteration 1000, Train Loss: 0.11190248280763626, Test Loss: 0.17281213402748108\n",
      "Iteration 1010, Train Loss: 0.10310131311416626, Test Loss: 0.18214082717895508\n",
      "Iteration 1020, Train Loss: 0.10850603878498077, Test Loss: 0.1705348789691925\n",
      "Iteration 1030, Train Loss: 0.1226143091917038, Test Loss: 0.16903196275234222\n",
      "Iteration 1040, Train Loss: 0.13627925515174866, Test Loss: 0.14370432496070862\n",
      "Iteration 1050, Train Loss: 0.1386285424232483, Test Loss: 0.12095306068658829\n",
      "Iteration 1060, Train Loss: 0.12425442039966583, Test Loss: 0.1109800711274147\n",
      "Iteration 1070, Train Loss: 0.11029811948537827, Test Loss: 0.10610263794660568\n",
      "Iteration 1080, Train Loss: 0.09969841688871384, Test Loss: 0.10634259134531021\n",
      "Iteration 1090, Train Loss: 0.09437315165996552, Test Loss: 0.10522893816232681\n",
      "Iteration 1100, Train Loss: 0.09091497212648392, Test Loss: 0.10419464111328125\n",
      "Iteration 1110, Train Loss: 0.0929066464304924, Test Loss: 0.10485158860683441\n",
      "Iteration 1120, Train Loss: 0.10014485567808151, Test Loss: 0.11810886114835739\n",
      "Iteration 1130, Train Loss: 0.10752314329147339, Test Loss: 0.11734895408153534\n",
      "Iteration 1140, Train Loss: 0.11299174278974533, Test Loss: 0.11562332510948181\n",
      "Iteration 1150, Train Loss: 0.11995050311088562, Test Loss: 0.11301920562982559\n",
      "Iteration 1160, Train Loss: 0.12851594388484955, Test Loss: 0.11244484037160873\n",
      "Iteration 1170, Train Loss: 0.13351844251155853, Test Loss: 0.11225540190935135\n",
      "Iteration 1180, Train Loss: 0.1324244737625122, Test Loss: 0.10454725474119186\n",
      "Iteration 1190, Train Loss: 0.12986287474632263, Test Loss: 0.10539854317903519\n",
      "Iteration 1200, Train Loss: 0.1294664889574051, Test Loss: 0.1253400295972824\n",
      "Iteration 1210, Train Loss: 0.12955142557621002, Test Loss: 0.17991597950458527\n",
      "Iteration 1220, Train Loss: 0.12335696816444397, Test Loss: 0.1718580275774002\n",
      "Iteration 1230, Train Loss: 0.12158435583114624, Test Loss: 0.1580522060394287\n",
      "Iteration 1240, Train Loss: 0.12387729436159134, Test Loss: 0.14124709367752075\n",
      "Iteration 1250, Train Loss: 0.12590497732162476, Test Loss: 0.10594360530376434\n",
      "Iteration 1260, Train Loss: 0.12247566878795624, Test Loss: 0.10665031522512436\n",
      "Iteration 1270, Train Loss: 0.12212416529655457, Test Loss: 0.14148975908756256\n",
      "Iteration 1280, Train Loss: 0.12621727585792542, Test Loss: 0.15322734415531158\n",
      "Iteration 1290, Train Loss: 0.1277451366186142, Test Loss: 0.15568549931049347\n",
      "Iteration 1300, Train Loss: 0.1258317530155182, Test Loss: 0.15566465258598328\n",
      "Iteration 1310, Train Loss: 0.11402063071727753, Test Loss: 0.15473319590091705\n",
      "Iteration 1320, Train Loss: 0.10425455868244171, Test Loss: 0.15944206714630127\n",
      "Iteration 1330, Train Loss: 0.0926327109336853, Test Loss: 0.1606435775756836\n",
      "Iteration 1340, Train Loss: 0.08201608806848526, Test Loss: 0.16268546879291534\n",
      "Iteration 1350, Train Loss: 0.0652756467461586, Test Loss: 0.1573202759027481\n",
      "Iteration 1360, Train Loss: 0.05958804488182068, Test Loss: 0.15776270627975464\n",
      "Iteration 1370, Train Loss: 0.06756850332021713, Test Loss: 0.14960412681102753\n",
      "Iteration 1380, Train Loss: 0.07681089639663696, Test Loss: 0.15388280153274536\n",
      "Iteration 1390, Train Loss: 0.08234012871980667, Test Loss: 0.1491800844669342\n",
      "Iteration 1400, Train Loss: 0.09271068871021271, Test Loss: 0.14652687311172485\n",
      "Iteration 1410, Train Loss: 0.10524838417768478, Test Loss: 0.1443876028060913\n",
      "Iteration 1420, Train Loss: 0.10982302576303482, Test Loss: 0.14525315165519714\n",
      "Iteration 1430, Train Loss: 0.10672365128993988, Test Loss: 0.1481820046901703\n",
      "Iteration 1440, Train Loss: 0.09866942465305328, Test Loss: 0.1260582059621811\n",
      "Iteration 1450, Train Loss: 0.0959131047129631, Test Loss: 0.11615976691246033\n",
      "Iteration 1460, Train Loss: 0.0992487370967865, Test Loss: 0.12364127486944199\n",
      "Iteration 1470, Train Loss: 0.09389342367649078, Test Loss: 0.11721864342689514\n",
      "Iteration 1480, Train Loss: 0.09088137745857239, Test Loss: 0.12049900740385056\n",
      "Iteration 1490, Train Loss: 0.09636300802230835, Test Loss: 0.11179281026124954\n",
      "Iteration 1500, Train Loss: 0.09278151392936707, Test Loss: 0.13932375609874725\n",
      "Iteration 1510, Train Loss: 0.07973147183656693, Test Loss: 0.14558063447475433\n",
      "Iteration 1520, Train Loss: 0.07852975279092789, Test Loss: 0.1280772089958191\n",
      "Iteration 1530, Train Loss: 0.08742222189903259, Test Loss: 0.1296839714050293\n",
      "Iteration 1540, Train Loss: 0.09986580163240433, Test Loss: 0.12493905425071716\n",
      "Iteration 1550, Train Loss: 0.11607814580202103, Test Loss: 0.11705535650253296\n",
      "Iteration 1560, Train Loss: 0.11703985929489136, Test Loss: 0.11020878702402115\n",
      "Iteration 1570, Train Loss: 0.11331426352262497, Test Loss: 0.11719752103090286\n",
      "Iteration 1580, Train Loss: 0.1078476682305336, Test Loss: 0.1184343695640564\n",
      "Iteration 1590, Train Loss: 0.09714372456073761, Test Loss: 0.12185695767402649\n",
      "Iteration 1600, Train Loss: 0.07902877777814865, Test Loss: 0.12869390845298767\n",
      "Iteration 1610, Train Loss: 0.07188282161951065, Test Loss: 0.1414066106081009\n",
      "Iteration 1620, Train Loss: 0.07930433005094528, Test Loss: 0.12853029370307922\n",
      "Iteration 1630, Train Loss: 0.0904911458492279, Test Loss: 0.12278982251882553\n",
      "Iteration 1640, Train Loss: 0.09756167232990265, Test Loss: 0.11628644168376923\n",
      "Iteration 1650, Train Loss: 0.11123990267515182, Test Loss: 0.11347158253192902\n",
      "Iteration 1660, Train Loss: 0.12954595685005188, Test Loss: 0.10964718461036682\n",
      "Iteration 1670, Train Loss: 0.13907083868980408, Test Loss: 0.1105228066444397\n",
      "Iteration 1680, Train Loss: 0.1397552639245987, Test Loss: 0.11117969453334808\n",
      "Iteration 1690, Train Loss: 0.1269180327653885, Test Loss: 0.12895150482654572\n",
      "Iteration 1700, Train Loss: 0.11541830003261566, Test Loss: 0.13899382948875427\n",
      "Iteration 1710, Train Loss: 0.11532939225435257, Test Loss: 0.13917659223079681\n",
      "Iteration 1720, Train Loss: 0.11436362564563751, Test Loss: 0.13951784372329712\n",
      "Iteration 1730, Train Loss: 0.10694434493780136, Test Loss: 0.13998255133628845\n",
      "Iteration 1740, Train Loss: 0.1193377748131752, Test Loss: 0.12933951616287231\n",
      "Iteration 1750, Train Loss: 0.12137544900178909, Test Loss: 0.120677649974823\n",
      "Iteration 1760, Train Loss: 0.10300080478191376, Test Loss: 0.13675066828727722\n",
      "Iteration 1770, Train Loss: 0.09788529574871063, Test Loss: 0.18069612979888916\n",
      "Iteration 1780, Train Loss: 0.10874638706445694, Test Loss: 0.22070321440696716\n",
      "Iteration 1790, Train Loss: 0.1157107800245285, Test Loss: 0.2052948921918869\n",
      "Iteration 1800, Train Loss: 0.13320130109786987, Test Loss: 0.2012619525194168\n",
      "Iteration 1810, Train Loss: 0.13674238324165344, Test Loss: 0.16587141156196594\n",
      "Iteration 1820, Train Loss: 0.1265702098608017, Test Loss: 0.1727268248796463\n",
      "Iteration 1830, Train Loss: 0.11777587234973907, Test Loss: 0.15820573270320892\n",
      "Iteration 1840, Train Loss: 0.1081610843539238, Test Loss: 0.17245182394981384\n",
      "Iteration 1850, Train Loss: 0.09489212185144424, Test Loss: 0.16478104889392853\n",
      "Iteration 1860, Train Loss: 0.08329146355390549, Test Loss: 0.1513340175151825\n",
      "Iteration 1870, Train Loss: 0.08696331828832626, Test Loss: 0.16477707028388977\n",
      "Iteration 1880, Train Loss: 0.09236619621515274, Test Loss: 0.2507096230983734\n",
      "Iteration 1890, Train Loss: 0.09979614615440369, Test Loss: 0.260053813457489\n",
      "Iteration 1900, Train Loss: 0.1085108295083046, Test Loss: 0.25845521688461304\n",
      "Iteration 1910, Train Loss: 0.12572096288204193, Test Loss: 0.26180511713027954\n",
      "Iteration 1920, Train Loss: 0.1357998251914978, Test Loss: 0.2531929910182953\n",
      "Iteration 1930, Train Loss: 0.1407245397567749, Test Loss: 0.2561790645122528\n",
      "Iteration 1940, Train Loss: 0.12813355028629303, Test Loss: 0.23943662643432617\n",
      "Iteration 1950, Train Loss: 0.11542746424674988, Test Loss: 0.18434013426303864\n",
      "Iteration 1960, Train Loss: 0.11507133394479752, Test Loss: 0.4408344030380249\n",
      "Iteration 1970, Train Loss: 0.11793108284473419, Test Loss: 0.36567142605781555\n",
      "Iteration 1980, Train Loss: 0.10898865014314651, Test Loss: 0.27436816692352295\n",
      "Iteration 1990, Train Loss: 0.11612153053283691, Test Loss: 0.22850923240184784\n",
      "Iteration 2000, Train Loss: 0.12444406747817993, Test Loss: 0.1825699806213379\n",
      "Iteration 2010, Train Loss: 0.11094453930854797, Test Loss: 0.1676071286201477\n",
      "Iteration 2020, Train Loss: 0.09943722933530807, Test Loss: 0.16744180023670197\n",
      "Iteration 2030, Train Loss: 0.10080371797084808, Test Loss: 0.26735201478004456\n",
      "Iteration 2040, Train Loss: 0.11953790485858917, Test Loss: 0.22615815699100494\n",
      "Iteration 2050, Train Loss: 0.1357317417860031, Test Loss: 0.23595623672008514\n",
      "Iteration 2060, Train Loss: 0.1380133479833603, Test Loss: 0.2622130513191223\n",
      "Iteration 2070, Train Loss: 0.12708668410778046, Test Loss: 0.27585354447364807\n",
      "Iteration 2080, Train Loss: 0.11456184089183807, Test Loss: 0.2838876247406006\n",
      "Iteration 2090, Train Loss: 0.10732472687959671, Test Loss: 0.2857200503349304\n",
      "Iteration 2100, Train Loss: 0.1032610833644867, Test Loss: 0.28636568784713745\n",
      "Iteration 2110, Train Loss: 0.09868749976158142, Test Loss: 0.2875901460647583\n",
      "Iteration 2120, Train Loss: 0.09826574474573135, Test Loss: 0.27244842052459717\n",
      "Iteration 2130, Train Loss: 0.1036292016506195, Test Loss: 0.22623543441295624\n",
      "Iteration 2140, Train Loss: 0.10776542872190475, Test Loss: 0.18159709870815277\n",
      "Iteration 2150, Train Loss: 0.10887867212295532, Test Loss: 0.1604568362236023\n",
      "Iteration 2160, Train Loss: 0.11048775911331177, Test Loss: 0.14793233573436737\n",
      "Iteration 2170, Train Loss: 0.11257608234882355, Test Loss: 0.14234742522239685\n",
      "Iteration 2180, Train Loss: 0.11522554606199265, Test Loss: 0.14353112876415253\n",
      "Iteration 2190, Train Loss: 0.11785469949245453, Test Loss: 0.149504616856575\n",
      "Iteration 2200, Train Loss: 0.12273011356592178, Test Loss: 0.15406069159507751\n",
      "Iteration 2210, Train Loss: 0.12535999715328217, Test Loss: 0.10823430866003036\n",
      "Iteration 2220, Train Loss: 0.12666063010692596, Test Loss: 0.11163156479597092\n",
      "Iteration 2230, Train Loss: 0.12065283954143524, Test Loss: 0.11100425571203232\n",
      "Iteration 2240, Train Loss: 0.11402582377195358, Test Loss: 0.11105909943580627\n",
      "Iteration 2250, Train Loss: 0.11162951588630676, Test Loss: 0.11165829747915268\n",
      "Iteration 2260, Train Loss: 0.11925313621759415, Test Loss: 0.11992452293634415\n",
      "Iteration 2270, Train Loss: 0.12390889972448349, Test Loss: 0.10533765703439713\n",
      "Iteration 2280, Train Loss: 0.12493830919265747, Test Loss: 0.10435650497674942\n",
      "Iteration 2290, Train Loss: 0.13205796480178833, Test Loss: 0.10471374541521072\n",
      "Iteration 2300, Train Loss: 0.13519717752933502, Test Loss: 0.10454027354717255\n",
      "Iteration 2310, Train Loss: 0.1287793666124344, Test Loss: 0.10785341262817383\n",
      "Iteration 2320, Train Loss: 0.11605960130691528, Test Loss: 0.10985057801008224\n",
      "Iteration 2330, Train Loss: 0.10918861627578735, Test Loss: 0.10911113768815994\n",
      "Iteration 2340, Train Loss: 0.10411050170660019, Test Loss: 0.10628847777843475\n",
      "Iteration 2350, Train Loss: 0.09520881623029709, Test Loss: 0.10678509622812271\n",
      "Iteration 2360, Train Loss: 0.08535482734441757, Test Loss: 0.10463366657495499\n",
      "Iteration 2370, Train Loss: 0.08452408015727997, Test Loss: 0.1041274443268776\n",
      "Iteration 2380, Train Loss: 0.08946292102336884, Test Loss: 0.10420382767915726\n",
      "Iteration 2390, Train Loss: 0.09516549110412598, Test Loss: 0.10462018102407455\n",
      "Iteration 2400, Train Loss: 0.1012817844748497, Test Loss: 0.10456173866987228\n",
      "Iteration 2410, Train Loss: 0.1068764477968216, Test Loss: 0.10667793452739716\n",
      "Iteration 2420, Train Loss: 0.10885962843894958, Test Loss: 0.11113008856773376\n",
      "Iteration 2430, Train Loss: 0.1105656698346138, Test Loss: 0.11105895787477493\n",
      "Iteration 2440, Train Loss: 0.11060580611228943, Test Loss: 0.10819599032402039\n",
      "Iteration 2450, Train Loss: 0.11160891503095627, Test Loss: 0.10475634783506393\n",
      "Iteration 2460, Train Loss: 0.11286678165197372, Test Loss: 0.10404989123344421\n",
      "Iteration 2470, Train Loss: 0.11708088219165802, Test Loss: 0.10405117273330688\n",
      "Iteration 2480, Train Loss: 0.11743523925542831, Test Loss: 0.10405271500349045\n",
      "Iteration 2490, Train Loss: 0.11653494089841843, Test Loss: 0.10404817759990692\n",
      "Iteration 2500, Train Loss: 0.11674347519874573, Test Loss: 0.1047494038939476\n",
      "Iteration 2510, Train Loss: 0.12400737404823303, Test Loss: 0.10404937714338303\n",
      "Iteration 2520, Train Loss: 0.12383726239204407, Test Loss: 0.10588431358337402\n",
      "Iteration 2530, Train Loss: 0.12073052674531937, Test Loss: 0.10544943809509277\n",
      "Iteration 2540, Train Loss: 0.12238145619630814, Test Loss: 0.10450591892004013\n",
      "Iteration 2550, Train Loss: 0.12236995995044708, Test Loss: 0.10609037429094315\n",
      "Iteration 2560, Train Loss: 0.11659219115972519, Test Loss: 0.10423868149518967\n",
      "Iteration 2570, Train Loss: 0.10437260568141937, Test Loss: 0.10405237227678299\n",
      "Iteration 2580, Train Loss: 0.0928458571434021, Test Loss: 0.10579405725002289\n",
      "Iteration 2590, Train Loss: 0.08013218641281128, Test Loss: 0.10865224152803421\n",
      "Iteration 2600, Train Loss: 0.07048953324556351, Test Loss: 0.10750067979097366\n",
      "Iteration 2610, Train Loss: 0.05563199147582054, Test Loss: 0.11186613142490387\n",
      "Iteration 2620, Train Loss: 0.047722890973091125, Test Loss: 0.1161355972290039\n",
      "Iteration 2630, Train Loss: 0.05141841992735863, Test Loss: 0.11622797697782516\n",
      "Iteration 2640, Train Loss: 0.060029223561286926, Test Loss: 0.11670554429292679\n",
      "Iteration 2650, Train Loss: 0.06717877089977264, Test Loss: 0.11524777114391327\n",
      "Iteration 2660, Train Loss: 0.07966365665197372, Test Loss: 0.11190714687108994\n",
      "Iteration 2670, Train Loss: 0.09316930174827576, Test Loss: 0.10855373740196228\n",
      "Iteration 2680, Train Loss: 0.10101737827062607, Test Loss: 0.10851944237947464\n",
      "Iteration 2690, Train Loss: 0.10367316007614136, Test Loss: 0.1075560674071312\n",
      "Iteration 2700, Train Loss: 0.09333644062280655, Test Loss: 0.11620485037565231\n",
      "Iteration 2710, Train Loss: 0.08439752459526062, Test Loss: 0.121117003262043\n",
      "Iteration 2720, Train Loss: 0.08584752678871155, Test Loss: 0.11819332838058472\n",
      "Iteration 2730, Train Loss: 0.08605363965034485, Test Loss: 0.1183646097779274\n",
      "Iteration 2740, Train Loss: 0.0758582204580307, Test Loss: 0.12188352644443512\n",
      "Iteration 2750, Train Loss: 0.08147048950195312, Test Loss: 0.11167390644550323\n",
      "Iteration 2760, Train Loss: 0.08689580112695694, Test Loss: 0.1147519201040268\n",
      "Iteration 2770, Train Loss: 0.07804924249649048, Test Loss: 0.12858189642429352\n",
      "Iteration 2780, Train Loss: 0.07060988247394562, Test Loss: 0.12972445785999298\n",
      "Iteration 2790, Train Loss: 0.07676879316568375, Test Loss: 0.130170077085495\n",
      "Iteration 2800, Train Loss: 0.08491845428943634, Test Loss: 0.12254950404167175\n",
      "Iteration 2810, Train Loss: 0.08994831889867783, Test Loss: 0.11076712608337402\n",
      "Iteration 2820, Train Loss: 0.08311530947685242, Test Loss: 0.10445628315210342\n",
      "Iteration 2830, Train Loss: 0.07015468925237656, Test Loss: 0.10405481606721878\n",
      "Iteration 2840, Train Loss: 0.05838269740343094, Test Loss: 0.10465308278799057\n",
      "Iteration 2850, Train Loss: 0.049472540616989136, Test Loss: 0.1042805165052414\n",
      "Iteration 2860, Train Loss: 0.04354579374194145, Test Loss: 0.10404592752456665\n",
      "Iteration 2870, Train Loss: 0.03835926204919815, Test Loss: 0.10532160103321075\n",
      "Iteration 2880, Train Loss: 0.040510617196559906, Test Loss: 0.10600462555885315\n",
      "Iteration 2890, Train Loss: 0.04800010845065117, Test Loss: 0.10515958815813065\n",
      "Iteration 2900, Train Loss: 0.05505930259823799, Test Loss: 0.10405779629945755\n",
      "Iteration 2910, Train Loss: 0.06177488714456558, Test Loss: 0.1041172668337822\n",
      "Iteration 2920, Train Loss: 0.07131713628768921, Test Loss: 0.10435830801725388\n",
      "Iteration 2930, Train Loss: 0.08082329481840134, Test Loss: 0.10548970848321915\n",
      "Iteration 2940, Train Loss: 0.0857892706990242, Test Loss: 0.1058976873755455\n",
      "Iteration 2950, Train Loss: 0.08715388178825378, Test Loss: 0.10407116264104843\n",
      "Iteration 2960, Train Loss: 0.0844028890132904, Test Loss: 0.10550849884748459\n",
      "Iteration 2970, Train Loss: 0.08424906432628632, Test Loss: 0.10584916919469833\n",
      "Iteration 2980, Train Loss: 0.0855567529797554, Test Loss: 0.1066334918141365\n",
      "Iteration 2990, Train Loss: 0.08004257827997208, Test Loss: 0.10746102035045624\n",
      "Iteration 3000, Train Loss: 0.07856756448745728, Test Loss: 0.10498698055744171\n",
      "Iteration 3010, Train Loss: 0.07947904616594315, Test Loss: 0.10407068580389023\n",
      "Iteration 3020, Train Loss: 0.0786178708076477, Test Loss: 0.1067158579826355\n",
      "Iteration 3030, Train Loss: 0.07190240174531937, Test Loss: 0.11097145080566406\n",
      "Iteration 3040, Train Loss: 0.07240090519189835, Test Loss: 0.11030228435993195\n",
      "Iteration 3050, Train Loss: 0.07881179451942444, Test Loss: 0.10737913846969604\n",
      "Iteration 3060, Train Loss: 0.08395987749099731, Test Loss: 0.1060681939125061\n",
      "Iteration 3070, Train Loss: 0.08002015203237534, Test Loss: 0.10459154844284058\n",
      "Iteration 3080, Train Loss: 0.0721312016248703, Test Loss: 0.10568134486675262\n",
      "Iteration 3090, Train Loss: 0.067117840051651, Test Loss: 0.10580260306596756\n",
      "Iteration 3100, Train Loss: 0.06410456448793411, Test Loss: 0.10452983528375626\n",
      "Iteration 3110, Train Loss: 0.06063881516456604, Test Loss: 0.10416106134653091\n",
      "Iteration 3120, Train Loss: 0.05466251075267792, Test Loss: 0.10492739081382751\n",
      "Iteration 3130, Train Loss: 0.05317330360412598, Test Loss: 0.1046602800488472\n",
      "Iteration 3140, Train Loss: 0.057264599949121475, Test Loss: 0.10426025092601776\n",
      "Iteration 3150, Train Loss: 0.06202779337763786, Test Loss: 0.10415980964899063\n",
      "Iteration 3160, Train Loss: 0.06602668017148972, Test Loss: 0.1040668785572052\n",
      "Iteration 3170, Train Loss: 0.07527553290128708, Test Loss: 0.10465648025274277\n",
      "Iteration 3180, Train Loss: 0.08541512489318848, Test Loss: 0.10587723553180695\n",
      "Iteration 3190, Train Loss: 0.09425176680088043, Test Loss: 0.10546953231096268\n",
      "Iteration 3200, Train Loss: 0.09827366471290588, Test Loss: 0.10437533259391785\n",
      "Iteration 3210, Train Loss: 0.09880975633859634, Test Loss: 0.1046304926276207\n",
      "Iteration 3220, Train Loss: 0.09989789873361588, Test Loss: 0.10681326687335968\n",
      "Iteration 3230, Train Loss: 0.10350161790847778, Test Loss: 0.10727185010910034\n",
      "Iteration 3240, Train Loss: 0.10222354531288147, Test Loss: 0.10678709298372269\n",
      "Iteration 3250, Train Loss: 0.10101614892482758, Test Loss: 0.10690852254629135\n",
      "Iteration 3260, Train Loss: 0.10350318253040314, Test Loss: 0.10420186817646027\n",
      "Iteration 3270, Train Loss: 0.10663101822137833, Test Loss: 0.10593961924314499\n",
      "Iteration 3280, Train Loss: 0.10239555686712265, Test Loss: 0.11174047738313675\n",
      "Iteration 3290, Train Loss: 0.09875281155109406, Test Loss: 0.11219320446252823\n",
      "Iteration 3300, Train Loss: 0.10504712164402008, Test Loss: 0.10920152068138123\n",
      "Iteration 3310, Train Loss: 0.11533427983522415, Test Loss: 0.11494748294353485\n",
      "Iteration 3320, Train Loss: 0.12454557418823242, Test Loss: 0.10845457762479782\n",
      "Iteration 3330, Train Loss: 0.12536178529262543, Test Loss: 0.10898977518081665\n",
      "Iteration 3340, Train Loss: 0.12283463776111603, Test Loss: 0.11904818564653397\n",
      "Iteration 3350, Train Loss: 0.11589506268501282, Test Loss: 0.12846189737319946\n",
      "Iteration 3360, Train Loss: 0.10733454674482346, Test Loss: 0.135313481092453\n",
      "Iteration 3370, Train Loss: 0.09495919197797775, Test Loss: 0.13702711462974548\n",
      "Iteration 3380, Train Loss: 0.08582111448049545, Test Loss: 0.15039749443531036\n",
      "Iteration 3390, Train Loss: 0.0919177234172821, Test Loss: 0.14357112348079681\n",
      "Iteration 3400, Train Loss: 0.09933315217494965, Test Loss: 0.14455491304397583\n",
      "Iteration 3410, Train Loss: 0.10792544484138489, Test Loss: 0.13944095373153687\n",
      "Iteration 3420, Train Loss: 0.1204937994480133, Test Loss: 0.13850751519203186\n",
      "Iteration 3430, Train Loss: 0.13748623430728912, Test Loss: 0.12365872412919998\n",
      "Iteration 3440, Train Loss: 0.14554373919963837, Test Loss: 0.12485023587942123\n",
      "Iteration 3450, Train Loss: 0.14590707421302795, Test Loss: 0.12055721133947372\n",
      "Iteration 3460, Train Loss: 0.12912112474441528, Test Loss: 0.1395648717880249\n",
      "Iteration 3470, Train Loss: 0.11578969657421112, Test Loss: 0.14566171169281006\n",
      "Iteration 3480, Train Loss: 0.11722805351018906, Test Loss: 0.14781251549720764\n",
      "Iteration 3490, Train Loss: 0.12081075459718704, Test Loss: 0.14208312332630157\n",
      "Iteration 3500, Train Loss: 0.11256807297468185, Test Loss: 0.14605695009231567\n",
      "Iteration 3510, Train Loss: 0.12060099840164185, Test Loss: 0.12812432646751404\n",
      "Iteration 3520, Train Loss: 0.1208648681640625, Test Loss: 0.1332932412624359\n",
      "Iteration 3530, Train Loss: 0.10389028489589691, Test Loss: 0.15391504764556885\n",
      "Iteration 3540, Train Loss: 0.09203506261110306, Test Loss: 0.1644807606935501\n",
      "Iteration 3550, Train Loss: 0.10255835205316544, Test Loss: 0.15912725031375885\n",
      "Iteration 3560, Train Loss: 0.11224117130041122, Test Loss: 0.14387279748916626\n",
      "Iteration 3570, Train Loss: 0.11827997118234634, Test Loss: 0.12216195464134216\n",
      "Iteration 3580, Train Loss: 0.11037004739046097, Test Loss: 0.10749930143356323\n",
      "Iteration 3590, Train Loss: 0.09285786747932434, Test Loss: 0.10579197108745575\n",
      "Iteration 3600, Train Loss: 0.07679591327905655, Test Loss: 0.10462173819541931\n",
      "Iteration 3610, Train Loss: 0.06878673285245895, Test Loss: 0.10516227781772614\n",
      "Iteration 3620, Train Loss: 0.062383607029914856, Test Loss: 0.10543892532587051\n",
      "Iteration 3630, Train Loss: 0.05493900924921036, Test Loss: 0.10898427665233612\n",
      "Iteration 3640, Train Loss: 0.05789705738425255, Test Loss: 0.10944519937038422\n",
      "Iteration 3650, Train Loss: 0.06465768069028854, Test Loss: 0.10859425365924835\n",
      "Iteration 3660, Train Loss: 0.07104399055242538, Test Loss: 0.10697142779827118\n",
      "Iteration 3670, Train Loss: 0.07645311951637268, Test Loss: 0.1062173917889595\n",
      "Iteration 3680, Train Loss: 0.08368799835443497, Test Loss: 0.10404720157384872\n",
      "Iteration 3690, Train Loss: 0.0876467227935791, Test Loss: 0.10424463450908661\n",
      "Iteration 3700, Train Loss: 0.08794917911291122, Test Loss: 0.10460805892944336\n",
      "Iteration 3710, Train Loss: 0.08402375131845474, Test Loss: 0.10460704565048218\n",
      "Iteration 3720, Train Loss: 0.07985231280326843, Test Loss: 0.10691709816455841\n",
      "Iteration 3730, Train Loss: 0.08185409754514694, Test Loss: 0.10934816300868988\n",
      "Iteration 3740, Train Loss: 0.085741326212883, Test Loss: 0.10919038951396942\n",
      "Iteration 3750, Train Loss: 0.07957176864147186, Test Loss: 0.10952608287334442\n",
      "Iteration 3760, Train Loss: 0.08109746873378754, Test Loss: 0.10568679869174957\n",
      "Iteration 3770, Train Loss: 0.08703481405973434, Test Loss: 0.10553884506225586\n",
      "Iteration 3780, Train Loss: 0.08526972681283951, Test Loss: 0.11154255270957947\n",
      "Iteration 3790, Train Loss: 0.0782366618514061, Test Loss: 0.11760973930358887\n",
      "Iteration 3800, Train Loss: 0.0833691656589508, Test Loss: 0.11622661352157593\n",
      "Iteration 3810, Train Loss: 0.09430456906557083, Test Loss: 0.11164214462041855\n",
      "Iteration 3820, Train Loss: 0.09952706098556519, Test Loss: 0.10983982682228088\n",
      "Iteration 3830, Train Loss: 0.09881503880023956, Test Loss: 0.1047387570142746\n",
      "Iteration 3840, Train Loss: 0.0932822898030281, Test Loss: 0.1045953556895256\n",
      "Iteration 3850, Train Loss: 0.08844485878944397, Test Loss: 0.104825459420681\n",
      "Iteration 3860, Train Loss: 0.08205058425664902, Test Loss: 0.10752027481794357\n",
      "Iteration 3870, Train Loss: 0.07584036886692047, Test Loss: 0.10647647827863693\n",
      "Iteration 3880, Train Loss: 0.06724408268928528, Test Loss: 0.11230799555778503\n",
      "Iteration 3890, Train Loss: 0.06629166752099991, Test Loss: 0.11382311582565308\n",
      "Iteration 3900, Train Loss: 0.0729409009218216, Test Loss: 0.1147829070687294\n",
      "Iteration 3910, Train Loss: 0.08096103370189667, Test Loss: 0.11178155988454819\n",
      "Iteration 3920, Train Loss: 0.08592957258224487, Test Loss: 0.11195240914821625\n",
      "Iteration 3930, Train Loss: 0.09592309594154358, Test Loss: 0.10685911029577255\n",
      "Iteration 3940, Train Loss: 0.10742970556020737, Test Loss: 0.1052364856004715\n",
      "Iteration 3950, Train Loss: 0.11374712735414505, Test Loss: 0.1054365411400795\n",
      "Iteration 3960, Train Loss: 0.10997146368026733, Test Loss: 0.10746750980615616\n",
      "Iteration 3970, Train Loss: 0.10245972871780396, Test Loss: 0.11292136460542679\n",
      "Iteration 3980, Train Loss: 0.10066549479961395, Test Loss: 0.11561544239521027\n",
      "Iteration 3990, Train Loss: 0.10003191977739334, Test Loss: 0.11454833298921585\n",
      "Iteration 4000, Train Loss: 0.09331562370061874, Test Loss: 0.11430390179157257\n",
      "Iteration 4010, Train Loss: 0.09169728308916092, Test Loss: 0.11374200880527496\n",
      "Iteration 4020, Train Loss: 0.09664586931467056, Test Loss: 0.10796751827001572\n",
      "Iteration 4030, Train Loss: 0.09606891870498657, Test Loss: 0.11443737894296646\n",
      "Iteration 4040, Train Loss: 0.08889109641313553, Test Loss: 0.12402811646461487\n",
      "Iteration 4050, Train Loss: 0.08799614757299423, Test Loss: 0.12407132238149643\n",
      "Iteration 4060, Train Loss: 0.09695649892091751, Test Loss: 0.12096069008111954\n",
      "Iteration 4070, Train Loss: 0.10858161002397537, Test Loss: 0.11682011932134628\n",
      "Iteration 4080, Train Loss: 0.11791171878576279, Test Loss: 0.10527985543012619\n",
      "Iteration 4090, Train Loss: 0.11350376904010773, Test Loss: 0.10406187176704407\n",
      "Iteration 4100, Train Loss: 0.1116073876619339, Test Loss: 0.1042083278298378\n",
      "Iteration 4110, Train Loss: 0.10841133445501328, Test Loss: 0.10405581444501877\n",
      "Iteration 4120, Train Loss: 0.10324876755475998, Test Loss: 0.10417266190052032\n",
      "Iteration 4130, Train Loss: 0.09235437214374542, Test Loss: 0.10624516755342484\n",
      "Iteration 4140, Train Loss: 0.08968597650527954, Test Loss: 0.10808068513870239\n",
      "Iteration 4150, Train Loss: 0.0955548956990242, Test Loss: 0.10757347196340561\n",
      "Iteration 4160, Train Loss: 0.10270467400550842, Test Loss: 0.10698536038398743\n",
      "Iteration 4170, Train Loss: 0.10679837316274643, Test Loss: 0.10518138855695724\n",
      "Iteration 4180, Train Loss: 0.11464478820562363, Test Loss: 0.10411956906318665\n",
      "Iteration 4190, Train Loss: 0.12381985038518906, Test Loss: 0.10433763265609741\n",
      "Iteration 4200, Train Loss: 0.12911304831504822, Test Loss: 0.10424869507551193\n",
      "Iteration 4210, Train Loss: 0.13012491166591644, Test Loss: 0.10429296642541885\n",
      "Iteration 4220, Train Loss: 0.1261248141527176, Test Loss: 0.10485918819904327\n",
      "Iteration 4230, Train Loss: 0.1236257553100586, Test Loss: 0.10627157241106033\n",
      "Iteration 4240, Train Loss: 0.12349091470241547, Test Loss: 0.10596523433923721\n",
      "Iteration 4250, Train Loss: 0.12100878357887268, Test Loss: 0.10534574836492538\n",
      "Iteration 4260, Train Loss: 0.11444129794836044, Test Loss: 0.10614357143640518\n",
      "Iteration 4270, Train Loss: 0.11517227441072464, Test Loss: 0.10404638946056366\n",
      "Iteration 4280, Train Loss: 0.11993426084518433, Test Loss: 0.1042577251791954\n",
      "Iteration 4290, Train Loss: 0.11713159829378128, Test Loss: 0.10962735861539841\n",
      "Iteration 4300, Train Loss: 0.11484003812074661, Test Loss: 0.11163091659545898\n",
      "Iteration 4310, Train Loss: 0.1250501126050949, Test Loss: 0.1101367175579071\n",
      "Iteration 4320, Train Loss: 0.13055074214935303, Test Loss: 0.113907091319561\n",
      "Iteration 4330, Train Loss: 0.13520453870296478, Test Loss: 0.10842670500278473\n",
      "Iteration 4340, Train Loss: 0.12910568714141846, Test Loss: 0.10710398852825165\n",
      "Iteration 4350, Train Loss: 0.1197998896241188, Test Loss: 0.1131095141172409\n",
      "Iteration 4360, Train Loss: 0.1052977591753006, Test Loss: 0.11846902221441269\n",
      "Iteration 4370, Train Loss: 0.09413705766201019, Test Loss: 0.12498446553945541\n",
      "Iteration 4380, Train Loss: 0.07601270079612732, Test Loss: 0.12889516353607178\n",
      "Iteration 4390, Train Loss: 0.06666000932455063, Test Loss: 0.14061366021633148\n",
      "Iteration 4400, Train Loss: 0.07129436731338501, Test Loss: 0.13441544771194458\n",
      "Iteration 4410, Train Loss: 0.08153130859136581, Test Loss: 0.13250906765460968\n",
      "Iteration 4420, Train Loss: 0.0906367376446724, Test Loss: 0.12361202389001846\n",
      "Iteration 4430, Train Loss: 0.10285023599863052, Test Loss: 0.12361716479063034\n",
      "Iteration 4440, Train Loss: 0.11678066849708557, Test Loss: 0.11304105818271637\n",
      "Iteration 4450, Train Loss: 0.12294316291809082, Test Loss: 0.11279264092445374\n",
      "Iteration 4460, Train Loss: 0.12358766794204712, Test Loss: 0.11061033606529236\n",
      "Iteration 4470, Train Loss: 0.10866598784923553, Test Loss: 0.12158994376659393\n",
      "Iteration 4480, Train Loss: 0.09610873460769653, Test Loss: 0.1267063319683075\n",
      "Iteration 4490, Train Loss: 0.09695910662412643, Test Loss: 0.13022306561470032\n",
      "Iteration 4500, Train Loss: 0.09886369109153748, Test Loss: 0.12833529710769653\n",
      "Iteration 4510, Train Loss: 0.08796510845422745, Test Loss: 0.13242608308792114\n",
      "Iteration 4520, Train Loss: 0.09797108918428421, Test Loss: 0.11937300860881805\n",
      "Iteration 4530, Train Loss: 0.10153308510780334, Test Loss: 0.11767696589231491\n",
      "Iteration 4540, Train Loss: 0.0864892452955246, Test Loss: 0.13535451889038086\n",
      "Iteration 4550, Train Loss: 0.07860938459634781, Test Loss: 0.14420442283153534\n",
      "Iteration 4560, Train Loss: 0.08792215585708618, Test Loss: 0.14125794172286987\n",
      "Iteration 4570, Train Loss: 0.10683060437440872, Test Loss: 0.13213114440441132\n",
      "Iteration 4580, Train Loss: 0.13086988031864166, Test Loss: 0.1098436713218689\n",
      "Iteration 4590, Train Loss: 0.13219282031059265, Test Loss: 0.10416723042726517\n",
      "Iteration 4600, Train Loss: 0.12871737778186798, Test Loss: 0.10592830926179886\n",
      "Iteration 4610, Train Loss: 0.12458107620477676, Test Loss: 0.10999052226543427\n",
      "Iteration 4620, Train Loss: 0.12276067584753036, Test Loss: 0.1090889424085617\n",
      "Iteration 4630, Train Loss: 0.1205538734793663, Test Loss: 0.10753291845321655\n",
      "Iteration 4640, Train Loss: 0.11931122094392776, Test Loss: 0.10547620058059692\n",
      "Iteration 4650, Train Loss: 0.11737071722745895, Test Loss: 0.10514187812805176\n",
      "Iteration 4660, Train Loss: 0.1186918243765831, Test Loss: 0.10621044039726257\n",
      "Iteration 4670, Train Loss: 0.11859161406755447, Test Loss: 0.1082836166024208\n",
      "Iteration 4680, Train Loss: 0.12034020572900772, Test Loss: 0.1102772131562233\n",
      "Iteration 4690, Train Loss: 0.12115591764450073, Test Loss: 0.1159105896949768\n",
      "Iteration 4700, Train Loss: 0.12462392449378967, Test Loss: 0.11859405785799026\n",
      "Iteration 4710, Train Loss: 0.12255644053220749, Test Loss: 0.1233929842710495\n",
      "Iteration 4720, Train Loss: 0.12470851838588715, Test Loss: 0.11566850543022156\n",
      "Iteration 4730, Train Loss: 0.12392256408929825, Test Loss: 0.10998866707086563\n",
      "Iteration 4740, Train Loss: 0.12220454961061478, Test Loss: 0.10867291688919067\n",
      "Iteration 4750, Train Loss: 0.11983424425125122, Test Loss: 0.10942701250314713\n",
      "Iteration 4760, Train Loss: 0.1139569953083992, Test Loss: 0.10832109302282333\n",
      "Iteration 4770, Train Loss: 0.10477172583341599, Test Loss: 0.11587108671665192\n",
      "Iteration 4780, Train Loss: 0.10538189113140106, Test Loss: 0.12275823950767517\n",
      "Iteration 4790, Train Loss: 0.11484739184379578, Test Loss: 0.11290931701660156\n",
      "Iteration 4800, Train Loss: 0.11372643709182739, Test Loss: 0.10707554966211319\n",
      "Iteration 4810, Train Loss: 0.1122647076845169, Test Loss: 0.1089605987071991\n",
      "Iteration 4820, Train Loss: 0.11287399381399155, Test Loss: 0.10993555188179016\n",
      "Iteration 4830, Train Loss: 0.10567682236433029, Test Loss: 0.11063946038484573\n",
      "Iteration 4840, Train Loss: 0.09243644773960114, Test Loss: 0.11301063746213913\n",
      "Iteration 4850, Train Loss: 0.08521147817373276, Test Loss: 0.1059982180595398\n",
      "Iteration 4860, Train Loss: 0.07763305306434631, Test Loss: 0.10405786335468292\n",
      "Iteration 4870, Train Loss: 0.07020365446805954, Test Loss: 0.10554971545934677\n",
      "Iteration 4880, Train Loss: 0.06638383120298386, Test Loss: 0.10453493148088455\n",
      "Iteration 4890, Train Loss: 0.05855773761868477, Test Loss: 0.10584314167499542\n",
      "Iteration 4900, Train Loss: 0.05755007639527321, Test Loss: 0.10694759339094162\n",
      "Iteration 4910, Train Loss: 0.06398612260818481, Test Loss: 0.10689806938171387\n",
      "Iteration 4920, Train Loss: 0.07141546905040741, Test Loss: 0.10672003030776978\n",
      "Iteration 4930, Train Loss: 0.07378949224948883, Test Loss: 0.10821504890918732\n",
      "Iteration 4940, Train Loss: 0.085284024477005, Test Loss: 0.10534416884183884\n",
      "Iteration 4950, Train Loss: 0.09660489857196808, Test Loss: 0.10483633726835251\n",
      "Iteration 4960, Train Loss: 0.102401502430439, Test Loss: 0.10600356757640839\n",
      "Iteration 4970, Train Loss: 0.09872414916753769, Test Loss: 0.10806700587272644\n",
      "Iteration 4980, Train Loss: 0.09242087602615356, Test Loss: 0.11797615885734558\n",
      "Iteration 4990, Train Loss: 0.08770304918289185, Test Loss: 0.12946076691150665\n",
      "Iteration 5000, Train Loss: 0.09011190384626389, Test Loss: 0.12727390229701996\n",
      "Iteration 5010, Train Loss: 0.08785677701234818, Test Loss: 0.12702445685863495\n",
      "Iteration 5020, Train Loss: 0.09220793098211288, Test Loss: 0.12632551789283752\n",
      "Iteration 5030, Train Loss: 0.10123725235462189, Test Loss: 0.1181153878569603\n",
      "Iteration 5040, Train Loss: 0.10034537315368652, Test Loss: 0.12713474035263062\n",
      "Iteration 5050, Train Loss: 0.09084947407245636, Test Loss: 0.14282581210136414\n",
      "Iteration 5060, Train Loss: 0.09341869503259659, Test Loss: 0.15405690670013428\n",
      "Iteration 5070, Train Loss: 0.10846679657697678, Test Loss: 0.14953185617923737\n",
      "Iteration 5080, Train Loss: 0.11932302266359329, Test Loss: 0.1405884325504303\n",
      "Iteration 5090, Train Loss: 0.12843818962574005, Test Loss: 0.11613202095031738\n",
      "Iteration 5100, Train Loss: 0.11753381788730621, Test Loss: 0.10996893793344498\n",
      "Iteration 5110, Train Loss: 0.10577582567930222, Test Loss: 0.10757823288440704\n",
      "Iteration 5120, Train Loss: 0.09409702569246292, Test Loss: 0.10839273780584335\n",
      "Iteration 5130, Train Loss: 0.08632192015647888, Test Loss: 0.10699300467967987\n",
      "Iteration 5140, Train Loss: 0.07237482815980911, Test Loss: 0.10960707813501358\n",
      "Iteration 5150, Train Loss: 0.06766834110021591, Test Loss: 0.11220492422580719\n",
      "Iteration 5160, Train Loss: 0.07385122030973434, Test Loss: 0.11580383032560349\n",
      "Iteration 5170, Train Loss: 0.08154663443565369, Test Loss: 0.11237113177776337\n",
      "Iteration 5180, Train Loss: 0.08729124069213867, Test Loss: 0.11290339380502701\n",
      "Iteration 5190, Train Loss: 0.09977276623249054, Test Loss: 0.10796534270048141\n",
      "Iteration 5200, Train Loss: 0.11019133776426315, Test Loss: 0.10549376159906387\n",
      "Iteration 5210, Train Loss: 0.11296629905700684, Test Loss: 0.1057424247264862\n",
      "Iteration 5220, Train Loss: 0.11097583919763565, Test Loss: 0.10463489592075348\n",
      "Iteration 5230, Train Loss: 0.10354422777891159, Test Loss: 0.10906663537025452\n",
      "Iteration 5240, Train Loss: 0.098213329911232, Test Loss: 0.11477229744195938\n",
      "Iteration 5250, Train Loss: 0.10238134115934372, Test Loss: 0.11798444390296936\n",
      "Iteration 5260, Train Loss: 0.09579797834157944, Test Loss: 0.11744597554206848\n",
      "Iteration 5270, Train Loss: 0.09440036863088608, Test Loss: 0.11711414158344269\n",
      "Iteration 5280, Train Loss: 0.09940028935670853, Test Loss: 0.10809110850095749\n",
      "Iteration 5290, Train Loss: 0.10029976069927216, Test Loss: 0.109672412276268\n",
      "Iteration 5300, Train Loss: 0.09186773002147675, Test Loss: 0.11620135605335236\n",
      "Iteration 5310, Train Loss: 0.09511453658342361, Test Loss: 0.1243312805891037\n",
      "Iteration 5320, Train Loss: 0.10154911875724792, Test Loss: 0.12179543823003769\n",
      "Iteration 5330, Train Loss: 0.10880399495363235, Test Loss: 0.11994078755378723\n",
      "Iteration 5340, Train Loss: 0.11002490669488907, Test Loss: 0.10976147651672363\n",
      "Iteration 5350, Train Loss: 0.1032332330942154, Test Loss: 0.10819974541664124\n",
      "Iteration 5360, Train Loss: 0.09820438176393509, Test Loss: 0.10885266214609146\n",
      "Iteration 5370, Train Loss: 0.09080450236797333, Test Loss: 0.11299391090869904\n",
      "Iteration 5380, Train Loss: 0.08034175634384155, Test Loss: 0.10958943516016006\n",
      "Iteration 5390, Train Loss: 0.06787005066871643, Test Loss: 0.11491996794939041\n",
      "Iteration 5400, Train Loss: 0.06690860539674759, Test Loss: 0.11645707488059998\n",
      "Iteration 5410, Train Loss: 0.07283158600330353, Test Loss: 0.1264079213142395\n",
      "Iteration 5420, Train Loss: 0.08186807483434677, Test Loss: 0.11934944242238998\n",
      "Iteration 5430, Train Loss: 0.08841680735349655, Test Loss: 0.11907326430082321\n",
      "Iteration 5440, Train Loss: 0.10161634534597397, Test Loss: 0.11272226274013519\n",
      "Iteration 5450, Train Loss: 0.11393699049949646, Test Loss: 0.11020476371049881\n",
      "Iteration 5460, Train Loss: 0.12052928656339645, Test Loss: 0.1104239672422409\n",
      "Iteration 5470, Train Loss: 0.11416585743427277, Test Loss: 0.10703963041305542\n",
      "Iteration 5480, Train Loss: 0.10657470673322678, Test Loss: 0.10900415480136871\n",
      "Iteration 5490, Train Loss: 0.10540753602981567, Test Loss: 0.12168945372104645\n",
      "Iteration 5500, Train Loss: 0.10535629093647003, Test Loss: 0.12819932401180267\n",
      "Iteration 5510, Train Loss: 0.09898838400840759, Test Loss: 0.12406915426254272\n",
      "Iteration 5520, Train Loss: 0.10178378969430923, Test Loss: 0.12170176208019257\n",
      "Iteration 5530, Train Loss: 0.10667195171117783, Test Loss: 0.11253218352794647\n",
      "Iteration 5540, Train Loss: 0.10208804160356522, Test Loss: 0.10599203407764435\n",
      "Iteration 5550, Train Loss: 0.0942210927605629, Test Loss: 0.10796603560447693\n",
      "Iteration 5560, Train Loss: 0.09261879324913025, Test Loss: 0.2343645542860031\n",
      "Iteration 5570, Train Loss: 0.10459119081497192, Test Loss: 0.24098092317581177\n",
      "Iteration 5580, Train Loss: 0.13237756490707397, Test Loss: 0.24850155413150787\n",
      "Iteration 5590, Train Loss: 0.15029960870742798, Test Loss: 0.27008727192878723\n",
      "Iteration 5600, Train Loss: 0.1507510244846344, Test Loss: 0.27428874373435974\n",
      "Iteration 5610, Train Loss: 0.15397977828979492, Test Loss: 0.26900050044059753\n",
      "Iteration 5620, Train Loss: 0.15475353598594666, Test Loss: 0.26344001293182373\n",
      "Iteration 5630, Train Loss: 0.14908187091350555, Test Loss: 0.26375478506088257\n",
      "Iteration 5640, Train Loss: 0.14425437152385712, Test Loss: 0.2651996612548828\n",
      "Iteration 5650, Train Loss: 0.14564822614192963, Test Loss: 0.24709558486938477\n",
      "Iteration 5660, Train Loss: 0.15094584226608276, Test Loss: 0.2810145914554596\n",
      "Iteration 5670, Train Loss: 0.15517014265060425, Test Loss: 0.269214391708374\n",
      "Iteration 5680, Train Loss: 0.15749235451221466, Test Loss: 0.2603510916233063\n",
      "Iteration 5690, Train Loss: 0.15987278521060944, Test Loss: 0.25214171409606934\n",
      "Iteration 5700, Train Loss: 0.1629699319601059, Test Loss: 0.2442757785320282\n",
      "Iteration 5710, Train Loss: 0.1660333275794983, Test Loss: 0.24186772108078003\n",
      "Iteration 5720, Train Loss: 0.1690482348203659, Test Loss: 0.19139185547828674\n",
      "Iteration 5730, Train Loss: 0.17032600939273834, Test Loss: 0.10556652396917343\n",
      "Iteration 5740, Train Loss: 0.18090488016605377, Test Loss: 0.10552816092967987\n",
      "Iteration 5750, Train Loss: 0.17393065989017487, Test Loss: 0.10683869570493698\n",
      "Iteration 5760, Train Loss: 0.16694962978363037, Test Loss: 0.10445745289325714\n",
      "Iteration 5770, Train Loss: 0.16129213571548462, Test Loss: 0.10406572371721268\n",
      "Iteration 5780, Train Loss: 0.1572454422712326, Test Loss: 0.10562904179096222\n",
      "Iteration 5790, Train Loss: 0.16411924362182617, Test Loss: 0.11623573303222656\n",
      "Iteration 5800, Train Loss: 0.16907794773578644, Test Loss: 0.11783231049776077\n",
      "Iteration 5810, Train Loss: 0.16643844544887543, Test Loss: 0.21395929157733917\n",
      "Iteration 5820, Train Loss: 0.17358903586864471, Test Loss: 0.2267274409532547\n",
      "Iteration 5830, Train Loss: 0.1747036725282669, Test Loss: 0.22546884417533875\n",
      "Iteration 5840, Train Loss: 0.1693650186061859, Test Loss: 0.22388923168182373\n",
      "Iteration 5850, Train Loss: 0.15407642722129822, Test Loss: 0.22278828918933868\n",
      "Iteration 5860, Train Loss: 0.13699942827224731, Test Loss: 0.2237541526556015\n",
      "Iteration 5870, Train Loss: 0.11327892541885376, Test Loss: 0.22948475182056427\n",
      "Iteration 5880, Train Loss: 0.10505329817533493, Test Loss: 0.21639244258403778\n",
      "Iteration 5890, Train Loss: 0.08500387519598007, Test Loss: 0.18115095794200897\n",
      "Iteration 5900, Train Loss: 0.07989250868558884, Test Loss: 0.1805107444524765\n",
      "Iteration 5910, Train Loss: 0.08935731649398804, Test Loss: 0.1822710633277893\n",
      "Iteration 5920, Train Loss: 0.09304210543632507, Test Loss: 0.16113656759262085\n",
      "Iteration 5930, Train Loss: 0.10057124495506287, Test Loss: 0.19129808247089386\n",
      "Iteration 5940, Train Loss: 0.11382034420967102, Test Loss: 0.19720697402954102\n",
      "Iteration 5950, Train Loss: 0.12961021065711975, Test Loss: 0.19804905354976654\n",
      "Iteration 5960, Train Loss: 0.13668407499790192, Test Loss: 0.19683483242988586\n",
      "Iteration 5970, Train Loss: 0.13483721017837524, Test Loss: 0.18924708664417267\n",
      "Iteration 5980, Train Loss: 0.11747881770133972, Test Loss: 0.1720556765794754\n",
      "Iteration 5990, Train Loss: 0.10772741585969925, Test Loss: 0.13651348650455475\n",
      "Iteration 6000, Train Loss: 0.11079999804496765, Test Loss: 0.10810020565986633\n",
      "Iteration 6010, Train Loss: 0.10724729299545288, Test Loss: 0.11599425971508026\n",
      "Iteration 6020, Train Loss: 0.10257096588611603, Test Loss: 0.1301189512014389\n",
      "Iteration 6030, Train Loss: 0.11421945691108704, Test Loss: 0.11968779563903809\n",
      "Iteration 6040, Train Loss: 0.10953270643949509, Test Loss: 0.14308381080627441\n",
      "Iteration 6050, Train Loss: 0.09381591528654099, Test Loss: 0.1656053215265274\n",
      "Iteration 6060, Train Loss: 0.08602997660636902, Test Loss: 0.14521785080432892\n",
      "Iteration 6070, Train Loss: 0.0950598418712616, Test Loss: 0.13620981574058533\n",
      "Iteration 6080, Train Loss: 0.10228895395994186, Test Loss: 0.13368350267410278\n",
      "Iteration 6090, Train Loss: 0.10402090102434158, Test Loss: 0.1126129999756813\n",
      "Iteration 6100, Train Loss: 0.08681093901395798, Test Loss: 0.10406102240085602\n",
      "Iteration 6110, Train Loss: 0.07181041687726974, Test Loss: 0.104278564453125\n",
      "Iteration 6120, Train Loss: 0.06171422079205513, Test Loss: 0.10628234595060349\n",
      "Iteration 6130, Train Loss: 0.05966576561331749, Test Loss: 0.10499770939350128\n",
      "Iteration 6140, Train Loss: 0.05839967727661133, Test Loss: 0.10454948246479034\n",
      "Iteration 6150, Train Loss: 0.06038238853216171, Test Loss: 0.104047492146492\n",
      "Iteration 6160, Train Loss: 0.06757950782775879, Test Loss: 0.1043303906917572\n",
      "Iteration 6170, Train Loss: 0.07335376739501953, Test Loss: 0.10734564810991287\n",
      "Iteration 6180, Train Loss: 0.07844460755586624, Test Loss: 0.10627701133489609\n",
      "Iteration 6190, Train Loss: 0.08525130152702332, Test Loss: 0.10606466978788376\n",
      "Iteration 6200, Train Loss: 0.09210539609193802, Test Loss: 0.10404758155345917\n",
      "Iteration 6210, Train Loss: 0.09803438931703568, Test Loss: 0.10404784232378006\n",
      "Iteration 6220, Train Loss: 0.09812180697917938, Test Loss: 0.10430724918842316\n",
      "Iteration 6230, Train Loss: 0.09620918333530426, Test Loss: 0.10688550770282745\n",
      "Iteration 6240, Train Loss: 0.09225255995988846, Test Loss: 0.10469803959131241\n",
      "Iteration 6250, Train Loss: 0.09338463842868805, Test Loss: 0.11121115833520889\n",
      "Iteration 6260, Train Loss: 0.09097784757614136, Test Loss: 0.12316183000802994\n",
      "Iteration 6270, Train Loss: 0.08277153223752975, Test Loss: 0.12355434149503708\n",
      "Iteration 6280, Train Loss: 0.0781693235039711, Test Loss: 0.11701634526252747\n",
      "Iteration 6290, Train Loss: 0.08284174650907516, Test Loss: 0.10404697060585022\n",
      "Iteration 6300, Train Loss: 0.08169438689947128, Test Loss: 0.11245141178369522\n",
      "Iteration 6310, Train Loss: 0.07805418223142624, Test Loss: 0.11521956324577332\n",
      "Iteration 6320, Train Loss: 0.07998083531856537, Test Loss: 0.10458023101091385\n",
      "Iteration 6330, Train Loss: 0.08558008819818497, Test Loss: 0.10698941349983215\n",
      "Iteration 6340, Train Loss: 0.08876123279333115, Test Loss: 0.10604877769947052\n",
      "Iteration 6350, Train Loss: 0.08638212829828262, Test Loss: 0.10670465230941772\n",
      "Iteration 6360, Train Loss: 0.08543668687343597, Test Loss: 0.1055784821510315\n",
      "Iteration 6370, Train Loss: 0.08414772897958755, Test Loss: 0.10750111937522888\n",
      "Iteration 6380, Train Loss: 0.0795629471540451, Test Loss: 0.11021124571561813\n",
      "Iteration 6390, Train Loss: 0.07099708914756775, Test Loss: 0.11128970235586166\n",
      "Iteration 6400, Train Loss: 0.059859391301870346, Test Loss: 0.12055496871471405\n",
      "Iteration 6410, Train Loss: 0.06278033554553986, Test Loss: 0.12741631269454956\n",
      "Iteration 6420, Train Loss: 0.06950673460960388, Test Loss: 0.10947292298078537\n",
      "Iteration 6430, Train Loss: 0.07818915694952011, Test Loss: 0.10615519434213638\n",
      "Iteration 6440, Train Loss: 0.08694735169410706, Test Loss: 0.10601431876420975\n",
      "Iteration 6450, Train Loss: 0.10254669934511185, Test Loss: 0.10582250356674194\n",
      "Iteration 6460, Train Loss: 0.1120944619178772, Test Loss: 0.10552791506052017\n",
      "Iteration 6470, Train Loss: 0.11402903497219086, Test Loss: 0.10602109879255295\n",
      "Iteration 6480, Train Loss: 0.1035437136888504, Test Loss: 0.1082528606057167\n",
      "Iteration 6490, Train Loss: 0.09206302464008331, Test Loss: 0.12494287639856339\n",
      "Iteration 6500, Train Loss: 0.09098979830741882, Test Loss: 0.12636040151119232\n",
      "Iteration 6510, Train Loss: 0.09286332875490189, Test Loss: 0.12167573720216751\n",
      "Iteration 6520, Train Loss: 0.08424045890569687, Test Loss: 0.11483730375766754\n",
      "Iteration 6530, Train Loss: 0.08743558079004288, Test Loss: 0.11268997192382812\n",
      "Iteration 6540, Train Loss: 0.09220951050519943, Test Loss: 0.1121777594089508\n",
      "Iteration 6550, Train Loss: 0.08531084656715393, Test Loss: 0.11263077706098557\n",
      "Iteration 6560, Train Loss: 0.07637690752744675, Test Loss: 0.11358833312988281\n",
      "Iteration 6570, Train Loss: 0.08146980404853821, Test Loss: 0.11459667980670929\n",
      "Iteration 6580, Train Loss: 0.09417623281478882, Test Loss: 0.1163112223148346\n",
      "Iteration 6590, Train Loss: 0.09840741008520126, Test Loss: 0.11441557109355927\n",
      "Iteration 6600, Train Loss: 0.08640434592962265, Test Loss: 0.10404631495475769\n",
      "Iteration 6610, Train Loss: 0.07049285620450974, Test Loss: 0.10469759255647659\n",
      "Iteration 6620, Train Loss: 0.05479692295193672, Test Loss: 0.10847508162260056\n",
      "Iteration 6630, Train Loss: 0.04264899343252182, Test Loss: 0.10734526813030243\n",
      "Iteration 6640, Train Loss: 0.03463766723871231, Test Loss: 0.10949881374835968\n",
      "Iteration 6650, Train Loss: 0.03300482779741287, Test Loss: 0.1062263771891594\n",
      "Iteration 6660, Train Loss: 0.033909495919942856, Test Loss: 0.10554313659667969\n",
      "Iteration 6670, Train Loss: 0.03979751095175743, Test Loss: 0.10437946021556854\n",
      "Iteration 6680, Train Loss: 0.04427587613463402, Test Loss: 0.10613907128572464\n",
      "Iteration 6690, Train Loss: 0.04855724796652794, Test Loss: 0.10589112341403961\n",
      "Iteration 6700, Train Loss: 0.046368230134248734, Test Loss: 0.11543068289756775\n",
      "Iteration 6710, Train Loss: 0.04706983268260956, Test Loss: 0.1185692772269249\n",
      "Iteration 6720, Train Loss: 0.04520367458462715, Test Loss: 0.12116622179746628\n",
      "Iteration 6730, Train Loss: 0.052119895815849304, Test Loss: 0.12137548625469208\n",
      "Iteration 6740, Train Loss: 0.05709482356905937, Test Loss: 0.11782775074243546\n",
      "Iteration 6750, Train Loss: 0.06017139554023743, Test Loss: 0.10611870884895325\n",
      "Iteration 6760, Train Loss: 0.06149306893348694, Test Loss: 0.10686533898115158\n",
      "Iteration 6770, Train Loss: 0.05735130235552788, Test Loss: 0.10835300385951996\n",
      "Iteration 6780, Train Loss: 0.05336374789476395, Test Loss: 0.11326864361763\n",
      "Iteration 6790, Train Loss: 0.0564337782561779, Test Loss: 0.13063082098960876\n",
      "Iteration 6800, Train Loss: 0.07384056597948074, Test Loss: 0.13047094643115997\n",
      "Iteration 6810, Train Loss: 0.08048439770936966, Test Loss: 0.11604587733745575\n",
      "Iteration 6820, Train Loss: 0.08594241738319397, Test Loss: 0.10802312940359116\n",
      "Iteration 6830, Train Loss: 0.09561844915151596, Test Loss: 0.11135302484035492\n",
      "Iteration 6840, Train Loss: 0.10498330742120743, Test Loss: 0.10797259211540222\n",
      "Iteration 6850, Train Loss: 0.0992845892906189, Test Loss: 0.10995012521743774\n",
      "Iteration 6860, Train Loss: 0.0981154665350914, Test Loss: 0.10753077268600464\n",
      "Iteration 6870, Train Loss: 0.0973554253578186, Test Loss: 0.10424811393022537\n",
      "Iteration 6880, Train Loss: 0.0870925709605217, Test Loss: 0.10602416843175888\n",
      "Iteration 6890, Train Loss: 0.07460058480501175, Test Loss: 0.10552448034286499\n",
      "Iteration 6900, Train Loss: 0.06995273381471634, Test Loss: 0.1050800308585167\n",
      "Iteration 6910, Train Loss: 0.07454128563404083, Test Loss: 0.10512027144432068\n",
      "Iteration 6920, Train Loss: 0.08984985202550888, Test Loss: 0.1112411692738533\n",
      "Iteration 6930, Train Loss: 0.10348371416330338, Test Loss: 0.10495595633983612\n",
      "Iteration 6940, Train Loss: 0.11219224333763123, Test Loss: 0.10406456887722015\n",
      "Iteration 6950, Train Loss: 0.11862676590681076, Test Loss: 0.10475888848304749\n",
      "Iteration 6960, Train Loss: 0.12510494887828827, Test Loss: 0.10753319412469864\n",
      "Iteration 6970, Train Loss: 0.12589885294437408, Test Loss: 0.1060871034860611\n",
      "Iteration 6980, Train Loss: 0.126137375831604, Test Loss: 0.10878913104534149\n",
      "Iteration 6990, Train Loss: 0.12528258562088013, Test Loss: 0.10609374940395355\n",
      "Iteration 7000, Train Loss: 0.12586764991283417, Test Loss: 0.10424698144197464\n",
      "Iteration 7010, Train Loss: 0.12171875685453415, Test Loss: 0.10701356828212738\n",
      "Iteration 7020, Train Loss: 0.11177093535661697, Test Loss: 0.10721054673194885\n",
      "Iteration 7030, Train Loss: 0.10344196110963821, Test Loss: 0.1077301874756813\n",
      "Iteration 7040, Train Loss: 0.09827236086130142, Test Loss: 0.10525171458721161\n",
      "Iteration 7050, Train Loss: 0.09213286638259888, Test Loss: 0.1040821298956871\n",
      "Iteration 7060, Train Loss: 0.08207065612077713, Test Loss: 0.10412229597568512\n",
      "Iteration 7070, Train Loss: 0.08030295372009277, Test Loss: 0.1326771378517151\n",
      "Iteration 7080, Train Loss: 0.08936069160699844, Test Loss: 0.1424165666103363\n",
      "Iteration 7090, Train Loss: 0.09388956427574158, Test Loss: 0.14720512926578522\n",
      "Iteration 7100, Train Loss: 0.09777089208364487, Test Loss: 0.14359714090824127\n",
      "Iteration 7110, Train Loss: 0.08749666064977646, Test Loss: 0.13141150772571564\n",
      "Iteration 7120, Train Loss: 0.07805898040533066, Test Loss: 0.13270728290081024\n",
      "Iteration 7130, Train Loss: 0.06850820779800415, Test Loss: 0.13049516081809998\n",
      "Iteration 7140, Train Loss: 0.06171504035592079, Test Loss: 0.1325574368238449\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Adjustable Parameters\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train_scaled) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train_scaled.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "# LSTM Model Definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "model = LSTMModel(input_size, hidden_dim, n_layers, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training using Walk-Forward Validation\n",
    "for i in range(window_size, epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Extract training window with batch size\n",
    "    start_idx = i - window_size\n",
    "    end_idx = i\n",
    "    x_window = torch.tensor(x_train_scaled[start_idx:end_idx], dtype=torch.float32)\n",
    "    y_window = torch.tensor(y_train_scaled[start_idx:end_idx], dtype=torch.float32)\n",
    "\n",
    "    # Reshape x_window to match LSTM input shape [batch_size=1, sequence_length=window_size, input_size=input_size]\n",
    "    x_window = x_window.view(1, window_size, input_size)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs, hidden = model(x_window, hidden)\n",
    "    loss = criterion(outputs, y_window)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    if i % batch_evaluation_frequency == 0:\n",
    "        with torch.no_grad():\n",
    "            x_test_window = torch.tensor(x_test_scaled[:test_window_size], dtype=torch.float32)\n",
    "            y_test_window = torch.tensor(y_test_scaled[:test_window_size], dtype=torch.float32)\n",
    "\n",
    "            # Reshape x_test_window to match LSTM input shape [batch_size=1, sequence_length=test_window_size, input_size=input_size]\n",
    "            x_test_window = x_test_window.view(1, test_window_size, input_size)\n",
    "\n",
    "            # Initialize hidden state for test\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "\n",
    "            test_outputs, _ = model(x_test_window, hidden)\n",
    "            test_loss = criterion(test_outputs, y_test_window)\n",
    "            print(f\"Iteration {i}, Train Loss: {loss.item()}, Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of future time steps you want to predict\n",
    "num_future_steps = 1\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize hidden state\n",
    "hidden = model.init_hidden(1)  # Batch size is 1\n",
    "\n",
    "# Initialize the input sequence with the last window_size data points from the training set\n",
    "input_sequence = x_train_scaled[-window_size:].tolist()\n",
    "\n",
    "future_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_future_steps):\n",
    "        # Convert the input_sequence to a tensor and reshape\n",
    "        input_tensor = torch.tensor(input_sequence, dtype=torch.float32).view(1, window_size, input_size)\n",
    "\n",
    "        # Get prediction from model\n",
    "        prediction, hidden = model(input_tensor, hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for August 1st, 2023:\n",
      "tensor([[-0.3946,  2.2370,  0.8878]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with a 'date' column containing dates from 1980-12-12 to 2023-07-31\n",
    "data2 = pd.DataFrame({'date': pd.date_range(start='1980-12-12', end='2023-07-31')})\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "data2['date'] = pd.to_datetime(data2['date'])\n",
    "\n",
    "# Extract day, month, and year from the date column for August 1st, 2023\n",
    "day_august_1 = 1  # Day is 1st\n",
    "month_august_1 = 8  # Month is August\n",
    "year_august_1 = 2023  # Year is 2023\n",
    "\n",
    "# Define the embedding dimensions\n",
    "embedding_dim = 1\n",
    "\n",
    "# Create embedding layers for day, month, and year\n",
    "day_embedding = nn.Embedding(32, embedding_dim)  # 0-31 days\n",
    "month_embedding = nn.Embedding(13, embedding_dim)  # 1-12 months\n",
    "year_embedding = nn.Embedding(44, embedding_dim)  # Embedding for years from 1980 to 2023\n",
    "\n",
    "# Convert day, month, and year to tensors with Long data type\n",
    "day_tensor = torch.LongTensor([day_august_1])\n",
    "month_tensor = torch.LongTensor([month_august_1])\n",
    "year_tensor = torch.LongTensor([year_august_1 - 1980])  # Convert years to an index from 0 to 43\n",
    "\n",
    "# Pass tensors through embedding layers to get embeddings\n",
    "day_embeddings_august_1 = day_embedding(day_tensor)\n",
    "month_embeddings_august_1 = month_embedding(month_tensor)\n",
    "year_embeddings_august_1 = year_embedding(year_tensor)\n",
    "\n",
    "# Concatenate the embeddings for August 1st, 2023\n",
    "date_embeddings_august_1 = torch.cat((day_embeddings_august_1, month_embeddings_august_1, year_embeddings_august_1), dim=1)\n",
    "\n",
    "# Print the embeddings for August 1st, 2023\n",
    "print(\"Embeddings for August 1st, 2023:\")\n",
    "print(date_embeddings_august_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with a 'date' column containing dates from 1980-12-12 to 2023-07-31\n",
    "data2 = pd.DataFrame({'date': pd.date_range(start='1980-12-12', end='2023-07-31')})\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "data2['date'] = pd.to_datetime(data2['date'])\n",
    "\n",
    "# Extract day, month, and year from the date column for August 1st\n",
    "day_august_1 = data2['date'].dt.day.iloc[0]\n",
    "month_august_1 = data2['date'].dt.month.iloc[0]\n",
    "year_august_1 = data2['date'].dt.year.iloc[0]\n",
    "\n",
    "# Define the embedding dimensions\n",
    "embedding_dim = 1\n",
    "\n",
    "# Create embedding layers for day, month, and year\n",
    "day_embedding = nn.Embedding(32, embedding_dim)  # 0-31 days\n",
    "month_embedding = nn.Embedding(13, embedding_dim)  # 1-12 months\n",
    "year_embedding = nn.Embedding(44, embedding_dim)  # Embedding for years from 1980 to 2023\n",
    "\n",
    "# Convert day, month, and year to tensors with Long data type\n",
    "day_tensor = torch.LongTensor([day_august_1])\n",
    "month_tensor = torch.LongTensor([month_august_1])\n",
    "year_tensor = torch.LongTensor([year_august_1 - 1980])  # Convert years to an index from 0 to 43\n",
    "\n",
    "# Pass tensors through embedding layers to get embeddings\n",
    "day_embeddings_august_1 = day_embedding(day_tensor)\n",
    "month_embeddings_august_1 = month_embedding(month_tensor)\n",
    "year_embeddings_august_1 = year_embedding(year_tensor)\n",
    "\n",
    "# Concatenate the embeddings for August 1st\n",
    "date_embeddings_august_1 = torch.cat((day_embeddings_august_1, month_embeddings_august_1, year_embeddings_august_1), dim=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Print or use y_predicted and date_embeddings_combined as needed\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_predicted:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43my_predicted\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# This will print the single scalar value of y_predicted\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate Embeddings Combined:\u001b[39m\u001b[38;5;124m\"\u001b[39m, date_embeddings_combined)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_predicted' is not defined"
     ]
    }
   ],
   "source": [
    "# Print or use y_predicted and date_embeddings_combined as needed\n",
    "print(\"y_predicted:\", y_predicted[0].item())  # This will print the single scalar value of y_predicted\n",
    "print(\"Date Embeddings Combined:\", date_embeddings_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4098]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7200, 4])\n"
     ]
    }
   ],
   "source": [
    "print(y_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_forecast_steps):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# Predict the next value by passing both input_sequence and hidden state\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m         forecasted_value, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert forecasted_value tensor to numpy array and reshape to (1, 1)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m forecasted_value_numpy \u001b[38;5;241m=\u001b[39m forecasted_value\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[97], line 30\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, hidden):\n\u001b[1;32m---> 30\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     31\u001b[0m     out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, hidden)\n\u001b[0;32m     32\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value by passing both input_sequence and hidden state\n",
    "        forecasted_value, _ = model(input_sequence, hidden)\n",
    "\n",
    "# Convert forecasted_value tensor to numpy array and reshape to (1, 1)\n",
    "forecasted_value_numpy = forecasted_value.cpu().numpy().reshape(1, 1)\n",
    "\n",
    "# Inverse transform the forecasted value\n",
    "forecasted_value_original = scaler.inverse_transform(forecasted_value_numpy)\n",
    "\n",
    "# Print or use the original forecasted value\n",
    "print(\"Original Forecasted Value:\", forecasted_value_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (1,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m         input_sequence \u001b[38;5;241m=\u001b[39m input_sequence[\u001b[38;5;241m-\u001b[39mwindow_size:]  \u001b[38;5;66;03m# Keep only the last window_size elements\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Inverse transform the scaled predictions to get the actual predicted \"close\" values\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m future_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_predictions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Print or use future_predictions as needed\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(future_predictions)\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:529\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    523\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    525\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    526\u001b[0m     X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy, dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m )\n\u001b[1;32m--> 529\u001b[0m X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n\u001b[0;32m    530\u001b[0m X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (1,4)"
     ]
    }
   ],
   "source": [
    "# Number of future time steps you want to predict\n",
    "num_future_steps = 1\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize hidden state\n",
    "hidden = model.init_hidden(1)  # Batch size is 1\n",
    "\n",
    "# Initialize the input sequence with the last window_size data points from the training set\n",
    "input_sequence = x_train_scaled[-window_size:].tolist()\n",
    "\n",
    "future_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_future_steps):\n",
    "        # Convert the input_sequence to a tensor and reshape\n",
    "        input_tensor = torch.tensor(input_sequence, dtype=torch.float32).view(1, window_size, input_size)\n",
    "\n",
    "        # Get prediction from model\n",
    "        prediction, hidden = model(input_tensor, hidden)\n",
    "\n",
    "        # Append the prediction to future_predictions\n",
    "        future_predictions.append(prediction.item())\n",
    "\n",
    "        # Update input_sequence with the new prediction (for the next iteration)\n",
    "        input_sequence.append(prediction.item())\n",
    "        input_sequence = input_sequence[-window_size:]  # Keep only the last window_size elements\n",
    "\n",
    "# Inverse transform the scaled predictions to get the actual predicted \"close\" values\n",
    "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
    "\n",
    "# Print or use future_predictions as needed\n",
    "print(future_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 10])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_future_steps):\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;66;03m# Convert the input_sequence to a tensor and reshape\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m         input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m         input_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, window_size, input_size)\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m# Get prediction from model\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: not a sequence"
     ]
    }
   ],
   "source": [
    "# Number of future time steps you want to predict\n",
    "num_future_steps = 10\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize hidden state\n",
    "hidden = model.init_hidden(1)  # Batch size is 1\n",
    "\n",
    "# Initialize the input sequence with the last window_size data points from the training set\n",
    "input_sequence = x_train_scaled[-window_size:].tolist()\n",
    "\n",
    "future_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_future_steps):\n",
    "        # Convert the input_sequence to a tensor and reshape\n",
    "        input_tensor = torch.tensor([input_sequence], dtype=torch.float32)\n",
    "        input_tensor = input_tensor.view(1, window_size, input_size)\n",
    "\n",
    "        # Get prediction from model\n",
    "        prediction, hidden = model(input_tensor, hidden)\n",
    "\n",
    "        # Append the prediction to future_predictions\n",
    "        future_predictions.append(prediction.item())\n",
    "\n",
    "        # Update input_sequence with the new prediction (for the next iteration)\n",
    "        # Note: For simplicity, we're updating only one feature, assuming it's the 'close' value.\n",
    "        # If you have multiple features in the input_sequence, adjust accordingly.\n",
    "        input_sequence.append(prediction.item())\n",
    "        input_sequence = input_sequence[-window_size:]  # Keep only the last window_size elements\n",
    "\n",
    "# Inverse transform the scaled predictions to get the actual predicted \"close\" values\n",
    "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
    "\n",
    "# Print or use future_predictions as needed\n",
    "print(future_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_future_steps):\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# Convert input_sequence to tensor and reshape\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m         input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, window_size, input_size)\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m# Get prediction from model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m         prediction, hidden \u001b[38;5;241m=\u001b[39m model(input_tensor, hidden)\n",
      "\u001b[1;31mTypeError\u001b[0m: not a sequence"
     ]
    }
   ],
   "source": [
    "# Number of future time steps you want to predict\n",
    "num_future_steps = 10  # For example, predict the next 10 days\n",
    "\n",
    "# Use the last window_size data points from the training set to start the prediction\n",
    "input_sequence = x_train_scaled[-window_size:].tolist()  # Convert tensor to list\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Predict future values\n",
    "future_predictions = []\n",
    "\n",
    "# Initialize hidden state\n",
    "hidden = model.init_hidden(1)  # Batch size is 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_future_steps):\n",
    "        # Convert input_sequence to tensor and reshape\n",
    "        input_tensor = torch.tensor(input_sequence[-window_size:], dtype=torch.float32).view(1, window_size, input_size)\n",
    "\n",
    "        # Get prediction from model\n",
    "        prediction, hidden = model(input_tensor, hidden)\n",
    "\n",
    "        # Append prediction to future_predictions\n",
    "        future_predictions.append(prediction.item())\n",
    "\n",
    "        # Update input_sequence with new prediction (for next iteration)\n",
    "        input_sequence.append(prediction.item())\n",
    "\n",
    "# Inverse transform the scaled predictions to get actual predictions\n",
    "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
    "\n",
    "# Print or use future_predictions as needed\n",
    "print(future_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 10])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasted value tensor: tensor([[0.4852]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForecasted value tensor:\u001b[39m\u001b[38;5;124m\"\u001b[39m, forecasted_value)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m# Update the input sequence by removing the first element along the sequence dimension \u001b[39;00m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# and adding the forecasted value at the end\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m         input_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecasted_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Convert the forecasted value back to the original scale using the scaler\u001b[39;00m\n\u001b[0;32m     27\u001b[0m forecasted_value_unscaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(forecasted_value\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the number of future steps you want to forecast\n",
    "num_forecast_steps = 1\n",
    "\n",
    "# Initialize the input sequence for forecasting\n",
    "input_sequence = x_train_scaled[-window_size:].view(1, window_size, input_size)\n",
    "\n",
    "# Initialize hidden state for forecasting\n",
    "hidden = model.init_hidden(1)  # Assuming batch size is 1\n",
    "\n",
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value by passing both input_sequence and hidden state\n",
    "        forecasted_value, _ = model(input_sequence, hidden)\n",
    "\n",
    "        # Print the forecasted value directly without reshaping\n",
    "        print(\"Forecasted value tensor:\", forecasted_value)\n",
    "\n",
    "        # Update the input sequence by removing the first element along the sequence dimension\n",
    "        # and adding the forecasted value at the end\n",
    "        input_sequence = torch.cat((input_sequence[:, 1:, :], forecasted_value), dim=1)\n",
    "\n",
    "# Convert the forecasted value back to the original scale using the scaler\n",
    "forecasted_value_unscaled = scaler.inverse_transform(forecasted_value.cpu().numpy())\n",
    "\n",
    "print(f\"Forecasted value of 'close' for the next time step: {forecasted_value_unscaled[0][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 10]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m forecasted_value_size \u001b[38;5;241m=\u001b[39m forecasted_value\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Access the last dimension\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Reshape the forecasted value to match the size of the input_sequence\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m forecasted_value \u001b[38;5;241m=\u001b[39m \u001b[43mforecasted_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Update the input sequence by removing the first element along the sequence dimension \u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# and adding the forecasted value at the end\u001b[39;00m\n\u001b[0;32m     27\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((input_sequence[:, \u001b[38;5;241m1\u001b[39m:, :], forecasted_value), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 1, 10]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the number of future steps you want to forecast\n",
    "num_forecast_steps = 1\n",
    "\n",
    "# Initialize the input sequence for forecasting\n",
    "input_sequence = x_train_scaled[-window_size:].view(1, window_size, input_size)\n",
    "\n",
    "# Initialize hidden state for forecasting\n",
    "hidden = model.init_hidden(1)  # Assuming batch size is 1\n",
    "\n",
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value by passing both input_sequence and hidden state\n",
    "        forecasted_value, _ = model(input_sequence, hidden)\n",
    "\n",
    "        # Ensure that you get the correct size for the forecasted_value\n",
    "        forecasted_value_size = forecasted_value.size(-1)  # Access the last dimension\n",
    "\n",
    "        # Reshape the forecasted value to match the size of the input_sequence\n",
    "        forecasted_value = forecasted_value.view(1, 1, input_size)\n",
    "\n",
    "        # Update the input sequence by removing the first element along the sequence dimension\n",
    "        # and adding the forecasted value at the end\n",
    "        input_sequence = torch.cat((input_sequence[:, 1:, :], forecasted_value), dim=1)\n",
    "\n",
    "# Convert the forecasted value back to the original scale using the scaler\n",
    "forecasted_value_unscaled = scaler.inverse_transform(forecasted_value.cpu().numpy())\n",
    "\n",
    "print(f\"Forecasted value of 'close' for the next time step: {forecasted_value_unscaled[0][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of forecasted_value: torch.Size([1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of forecasted_value:\u001b[39m\u001b[38;5;124m\"\u001b[39m, forecasted_value\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Update the input sequence by removing the first element along the sequence dimension \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# and adding the forecasted value at the end\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecasted_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value by passing both input_sequence and hidden state\n",
    "        forecasted_value, _ = model(input_sequence, hidden)\n",
    "\n",
    "        # Print the shape of forecasted_value\n",
    "        print(\"Shape of forecasted_value:\", forecasted_value.shape)\n",
    "\n",
    "        # Update the input sequence by removing the first element along the sequence dimension\n",
    "        # and adding the forecasted value at the end\n",
    "        input_sequence = torch.cat((input_sequence[:, 1:, :], forecasted_value), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 10 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m         forecasted_value \u001b[38;5;241m=\u001b[39m forecasted_value\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, forecasted_value_size)\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;66;03m# Update the input sequence by removing the first element along the sequence dimension \u001b[39;00m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;66;03m# and adding the forecasted value at the end\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m         input_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecasted_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Convert the forecasted value back to the original scale using the scaler\u001b[39;00m\n\u001b[0;32m     30\u001b[0m forecasted_value_unscaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(forecasted_value\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 10 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the number of future steps you want to forecast\n",
    "num_forecast_steps = 1\n",
    "\n",
    "# Initialize the input sequence for forecasting\n",
    "input_sequence = x_train_scaled[-window_size:].view(1, window_size, input_size)\n",
    "\n",
    "# Initialize hidden state for forecasting\n",
    "hidden = model.init_hidden(1)  # Assuming batch size is 1\n",
    "\n",
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value by passing both input_sequence and hidden state\n",
    "        forecasted_value, _ = model(input_sequence, hidden)\n",
    "\n",
    "        # Ensure that you get the correct size for the forecasted_value\n",
    "        forecasted_value_size = forecasted_value.size(-1)  # Access the last dimension\n",
    "\n",
    "        # Reshape the forecasted value based on its size\n",
    "        forecasted_value = forecasted_value.view(1, 1, forecasted_value_size)\n",
    "\n",
    "        # Update the input sequence by removing the first element along the sequence dimension\n",
    "        # and adding the forecasted value at the end\n",
    "        input_sequence = torch.cat((input_sequence[:, 1:, :], forecasted_value), dim=1)\n",
    "\n",
    "# Convert the forecasted value back to the original scale using the scaler\n",
    "forecasted_value_unscaled = scaler.inverse_transform(forecasted_value.cpu().numpy())\n",
    "\n",
    "print(f\"Forecasted value of 'close' for the next time step: {forecasted_value_unscaled[0][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 10 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m         forecasted_value \u001b[38;5;241m=\u001b[39m forecasted_value\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, forecasted_value_size)\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;66;03m# Update the input sequence by removing the first element and adding the forecasted value at the end\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m         input_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecasted_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Convert the forecasted value back to the original scale using the scaler\u001b[39;00m\n\u001b[0;32m     29\u001b[0m forecasted_value_unscaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(forecasted_value\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 10 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the number of future steps you want to forecast\n",
    "num_forecast_steps = 1\n",
    "\n",
    "# Initialize the input sequence for forecasting\n",
    "input_sequence = x_train_scaled[-window_size:].view(1, window_size, input_size)\n",
    "\n",
    "# Initialize hidden state for forecasting\n",
    "hidden = model.init_hidden(1)  # Assuming batch size is 1\n",
    "\n",
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value by passing both input_sequence and hidden state\n",
    "        forecasted_value, _ = model(input_sequence, hidden)\n",
    "\n",
    "        # Ensure that you get the correct size for the forecasted_value\n",
    "        forecasted_value_size = forecasted_value.size(-1)  # Access the last dimension\n",
    "\n",
    "        # Reshape the forecasted value based on its size\n",
    "        forecasted_value = forecasted_value.view(1, 1, forecasted_value_size)\n",
    "\n",
    "        # Update the input sequence by removing the first element and adding the forecasted value at the end\n",
    "        input_sequence = torch.cat((input_sequence[:, 1:, :], forecasted_value), dim=1)\n",
    "\n",
    "# Convert the forecasted value back to the original scale using the scaler\n",
    "forecasted_value_unscaled = scaler.inverse_transform(forecasted_value.cpu().numpy())\n",
    "\n",
    "print(f\"Forecasted value of 'close' for the next time step: {forecasted_value_unscaled[0][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_sequence: torch.Size([1, 50, 10])\n",
      "Shape of forecasted_value: torch.Size([1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of input_sequence:\", input_sequence.shape)\n",
    "print(\"Shape of forecasted_value:\", forecasted_value.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 10]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m forecasted_value, _ \u001b[38;5;241m=\u001b[39m model(input_sequence, hidden)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Reshape the forecasted value to have the same sequence length as input_sequence\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m forecasted_value \u001b[38;5;241m=\u001b[39m \u001b[43mforecasted_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Update the input sequence by removing the first element and adding the forecasted value at the end\u001b[39;00m\n\u001b[0;32m     23\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((input_sequence[:, \u001b[38;5;241m1\u001b[39m:, :], forecasted_value), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 1, 10]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the number of future steps you want to forecast\n",
    "num_forecast_steps = 1\n",
    "\n",
    "# Initialize the input sequence for forecasting\n",
    "input_sequence = x_train_scaled[-window_size:].view(1, window_size, input_size)\n",
    "\n",
    "# Initialize hidden state for forecasting\n",
    "hidden = model.init_hidden(batch_size)\n",
    "\n",
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value by passing both input_sequence and hidden state\n",
    "        forecasted_value, _ = model(input_sequence, hidden)\n",
    "\n",
    "        # Reshape the forecasted value to have the same sequence length as input_sequence\n",
    "        forecasted_value = forecasted_value.view(1, 1, input_size)\n",
    "\n",
    "        # Update the input sequence by removing the first element and adding the forecasted value at the end\n",
    "        input_sequence = torch.cat((input_sequence[:, 1:, :], forecasted_value), dim=1)\n",
    "\n",
    "# Convert the forecasted value back to the original scale using the scaler\n",
    "forecasted_value_unscaled = scaler.inverse_transform(forecasted_value.cpu().numpy())\n",
    "\n",
    "print(f\"Forecasted value of 'close' for the next time step: {forecasted_value_unscaled[0][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_sequence: torch.Size([1, 50, 10])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of input_sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_sequence\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of hidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mhidden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of input_sequence: {input_sequence.shape}\")\n",
    "print(f\"Shape of hidden: {hidden.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m forecasted_value, _ \u001b[38;5;241m=\u001b[39m model(input_sequence, hidden)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Ensure that you get the correct size for the forecasted_value\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m forecasted_value_size \u001b[38;5;241m=\u001b[39m \u001b[43mforecasted_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Reshape the forecasted value based on its size\u001b[39;00m\n\u001b[0;32m     23\u001b[0m forecasted_value \u001b[38;5;241m=\u001b[39m forecasted_value\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, forecasted_value_size)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the number of future steps you want to forecast\n",
    "num_forecast_steps = 1\n",
    "\n",
    "# Initialize the input sequence for forecasting\n",
    "input_sequence = x_train_scaled[-window_size:].view(1, window_size, input_size)\n",
    "\n",
    "# Initialize hidden state for forecasting\n",
    "hidden = model.init_hidden(1)  # Assuming batch size is 1\n",
    "\n",
    "# Forecast the next 'num_forecast_steps' values\n",
    "for _ in range(num_forecast_steps):\n",
    "    with torch.no_grad():\n",
    "        # Predict the next value by passing both input_sequence and hidden state\n",
    "        forecasted_value, _ = model(input_sequence, hidden)\n",
    "\n",
    "        # Ensure that you get the correct size for the forecasted_value\n",
    "        forecasted_value_size = forecasted_value.size(2)\n",
    "\n",
    "        # Reshape the forecasted value based on its size\n",
    "        forecasted_value = forecasted_value.view(1, 1, forecasted_value_size)\n",
    "\n",
    "        # Update the input sequence by removing the first element and adding the forecasted value at the end\n",
    "        input_sequence = torch.cat((input_sequence[:, 1:, :], forecasted_value), dim=1)\n",
    "\n",
    "# Convert the forecasted value back to the original scale using the scaler\n",
    "forecasted_value_unscaled = scaler.inverse_transform(forecasted_value.cpu().numpy())\n",
    "\n",
    "print(f\"Forecasted value of 'close' for the next time step: {forecasted_value_unscaled[0][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de forecasted_value: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "forecasted_value_size = forecasted_value.size(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de forecasted_value: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Forma de forecasted_value: {forecasted_value.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
