{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjusted_close</th>\n",
       "      <th>change_percent</th>\n",
       "      <th>avg_vol_20d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10755</th>\n",
       "      <td>2023-08-11</td>\n",
       "      <td>177.32</td>\n",
       "      <td>178.62</td>\n",
       "      <td>176.55</td>\n",
       "      <td>177.79</td>\n",
       "      <td>51988100</td>\n",
       "      <td>177.79</td>\n",
       "      <td>0.03</td>\n",
       "      <td>58532040.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10756</th>\n",
       "      <td>2023-08-14</td>\n",
       "      <td>177.97</td>\n",
       "      <td>179.69</td>\n",
       "      <td>177.31</td>\n",
       "      <td>179.46</td>\n",
       "      <td>43675600</td>\n",
       "      <td>179.46</td>\n",
       "      <td>0.94</td>\n",
       "      <td>58189810.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10757</th>\n",
       "      <td>2023-08-15</td>\n",
       "      <td>178.88</td>\n",
       "      <td>179.48</td>\n",
       "      <td>177.05</td>\n",
       "      <td>177.45</td>\n",
       "      <td>43622600</td>\n",
       "      <td>177.45</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>57953250.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10758</th>\n",
       "      <td>2023-08-16</td>\n",
       "      <td>177.13</td>\n",
       "      <td>178.54</td>\n",
       "      <td>176.50</td>\n",
       "      <td>176.57</td>\n",
       "      <td>46964900</td>\n",
       "      <td>176.57</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>56276130.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10759</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>177.14</td>\n",
       "      <td>177.51</td>\n",
       "      <td>173.48</td>\n",
       "      <td>174.00</td>\n",
       "      <td>66062900</td>\n",
       "      <td>174.00</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>56600215.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10760</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>172.30</td>\n",
       "      <td>175.10</td>\n",
       "      <td>171.96</td>\n",
       "      <td>174.49</td>\n",
       "      <td>61114200</td>\n",
       "      <td>174.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>56060035.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10761</th>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>175.07</td>\n",
       "      <td>176.13</td>\n",
       "      <td>173.74</td>\n",
       "      <td>175.84</td>\n",
       "      <td>46311900</td>\n",
       "      <td>175.84</td>\n",
       "      <td>0.77</td>\n",
       "      <td>56106740.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10762</th>\n",
       "      <td>2023-08-22</td>\n",
       "      <td>177.06</td>\n",
       "      <td>177.68</td>\n",
       "      <td>176.25</td>\n",
       "      <td>177.23</td>\n",
       "      <td>42084200</td>\n",
       "      <td>177.23</td>\n",
       "      <td>0.79</td>\n",
       "      <td>56346790.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10763</th>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>178.52</td>\n",
       "      <td>181.55</td>\n",
       "      <td>178.33</td>\n",
       "      <td>181.12</td>\n",
       "      <td>52722800</td>\n",
       "      <td>181.12</td>\n",
       "      <td>2.19</td>\n",
       "      <td>56609335.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10764</th>\n",
       "      <td>2023-08-24</td>\n",
       "      <td>180.67</td>\n",
       "      <td>181.10</td>\n",
       "      <td>176.01</td>\n",
       "      <td>176.38</td>\n",
       "      <td>54945800</td>\n",
       "      <td>176.38</td>\n",
       "      <td>-2.62</td>\n",
       "      <td>56983615.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10765</th>\n",
       "      <td>2023-08-25</td>\n",
       "      <td>177.38</td>\n",
       "      <td>179.15</td>\n",
       "      <td>175.82</td>\n",
       "      <td>178.61</td>\n",
       "      <td>51418700</td>\n",
       "      <td>178.61</td>\n",
       "      <td>1.26</td>\n",
       "      <td>57139980.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10766</th>\n",
       "      <td>2023-08-28</td>\n",
       "      <td>180.09</td>\n",
       "      <td>180.59</td>\n",
       "      <td>178.55</td>\n",
       "      <td>180.19</td>\n",
       "      <td>43820700</td>\n",
       "      <td>180.19</td>\n",
       "      <td>0.88</td>\n",
       "      <td>57389810.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10767</th>\n",
       "      <td>2023-08-29</td>\n",
       "      <td>179.70</td>\n",
       "      <td>184.90</td>\n",
       "      <td>179.50</td>\n",
       "      <td>184.12</td>\n",
       "      <td>53003900</td>\n",
       "      <td>184.12</td>\n",
       "      <td>2.18</td>\n",
       "      <td>58281250.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10768</th>\n",
       "      <td>2023-08-30</td>\n",
       "      <td>184.94</td>\n",
       "      <td>187.85</td>\n",
       "      <td>184.74</td>\n",
       "      <td>187.65</td>\n",
       "      <td>60813900</td>\n",
       "      <td>187.65</td>\n",
       "      <td>1.92</td>\n",
       "      <td>58802480.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10769</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>187.84</td>\n",
       "      <td>189.12</td>\n",
       "      <td>187.48</td>\n",
       "      <td>187.87</td>\n",
       "      <td>60794500</td>\n",
       "      <td>187.87</td>\n",
       "      <td>0.12</td>\n",
       "      <td>58780445.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10770</th>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>189.49</td>\n",
       "      <td>189.92</td>\n",
       "      <td>188.28</td>\n",
       "      <td>189.46</td>\n",
       "      <td>45732600</td>\n",
       "      <td>189.46</td>\n",
       "      <td>0.85</td>\n",
       "      <td>55277090.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10771</th>\n",
       "      <td>2023-09-05</td>\n",
       "      <td>188.28</td>\n",
       "      <td>189.98</td>\n",
       "      <td>187.61</td>\n",
       "      <td>189.70</td>\n",
       "      <td>45280000</td>\n",
       "      <td>189.70</td>\n",
       "      <td>0.13</td>\n",
       "      <td>52662285.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10772</th>\n",
       "      <td>2023-09-06</td>\n",
       "      <td>188.40</td>\n",
       "      <td>188.85</td>\n",
       "      <td>181.47</td>\n",
       "      <td>182.91</td>\n",
       "      <td>81755800</td>\n",
       "      <td>182.91</td>\n",
       "      <td>-3.58</td>\n",
       "      <td>53358925.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10773</th>\n",
       "      <td>2023-09-07</td>\n",
       "      <td>175.18</td>\n",
       "      <td>178.21</td>\n",
       "      <td>173.54</td>\n",
       "      <td>177.56</td>\n",
       "      <td>112488800</td>\n",
       "      <td>177.56</td>\n",
       "      <td>-2.92</td>\n",
       "      <td>55964440.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10774</th>\n",
       "      <td>2023-09-08</td>\n",
       "      <td>178.35</td>\n",
       "      <td>180.24</td>\n",
       "      <td>177.79</td>\n",
       "      <td>178.18</td>\n",
       "      <td>65551300</td>\n",
       "      <td>178.18</td>\n",
       "      <td>0.35</td>\n",
       "      <td>56507660.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10775</th>\n",
       "      <td>2023-09-11</td>\n",
       "      <td>180.07</td>\n",
       "      <td>180.30</td>\n",
       "      <td>177.34</td>\n",
       "      <td>179.36</td>\n",
       "      <td>58953100</td>\n",
       "      <td>179.36</td>\n",
       "      <td>0.66</td>\n",
       "      <td>56855910.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10776</th>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>179.49</td>\n",
       "      <td>180.12</td>\n",
       "      <td>174.82</td>\n",
       "      <td>176.30</td>\n",
       "      <td>88211615</td>\n",
       "      <td>176.30</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>59082710.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date    open    high     low   close     volume  adjusted_close  \\\n",
       "10755  2023-08-11  177.32  178.62  176.55  177.79   51988100          177.79   \n",
       "10756  2023-08-14  177.97  179.69  177.31  179.46   43675600          179.46   \n",
       "10757  2023-08-15  178.88  179.48  177.05  177.45   43622600          177.45   \n",
       "10758  2023-08-16  177.13  178.54  176.50  176.57   46964900          176.57   \n",
       "10759  2023-08-17  177.14  177.51  173.48  174.00   66062900          174.00   \n",
       "10760  2023-08-18  172.30  175.10  171.96  174.49   61114200          174.49   \n",
       "10761  2023-08-21  175.07  176.13  173.74  175.84   46311900          175.84   \n",
       "10762  2023-08-22  177.06  177.68  176.25  177.23   42084200          177.23   \n",
       "10763  2023-08-23  178.52  181.55  178.33  181.12   52722800          181.12   \n",
       "10764  2023-08-24  180.67  181.10  176.01  176.38   54945800          176.38   \n",
       "10765  2023-08-25  177.38  179.15  175.82  178.61   51418700          178.61   \n",
       "10766  2023-08-28  180.09  180.59  178.55  180.19   43820700          180.19   \n",
       "10767  2023-08-29  179.70  184.90  179.50  184.12   53003900          184.12   \n",
       "10768  2023-08-30  184.94  187.85  184.74  187.65   60813900          187.65   \n",
       "10769  2023-08-31  187.84  189.12  187.48  187.87   60794500          187.87   \n",
       "10770  2023-09-01  189.49  189.92  188.28  189.46   45732600          189.46   \n",
       "10771  2023-09-05  188.28  189.98  187.61  189.70   45280000          189.70   \n",
       "10772  2023-09-06  188.40  188.85  181.47  182.91   81755800          182.91   \n",
       "10773  2023-09-07  175.18  178.21  173.54  177.56  112488800          177.56   \n",
       "10774  2023-09-08  178.35  180.24  177.79  178.18   65551300          178.18   \n",
       "10775  2023-09-11  180.07  180.30  177.34  179.36   58953100          179.36   \n",
       "10776  2023-09-12  179.49  180.12  174.82  176.30   88211615          176.30   \n",
       "\n",
       "       change_percent  avg_vol_20d  \n",
       "10755            0.03  58532040.00  \n",
       "10756            0.94  58189810.00  \n",
       "10757           -1.12  57953250.00  \n",
       "10758           -0.50  56276130.00  \n",
       "10759           -1.46  56600215.00  \n",
       "10760            0.28  56060035.00  \n",
       "10761            0.77  56106740.00  \n",
       "10762            0.79  56346790.00  \n",
       "10763            2.19  56609335.00  \n",
       "10764           -2.62  56983615.00  \n",
       "10765            1.26  57139980.00  \n",
       "10766            0.88  57389810.00  \n",
       "10767            2.18  58281250.00  \n",
       "10768            1.92  58802480.00  \n",
       "10769            0.12  58780445.00  \n",
       "10770            0.85  55277090.00  \n",
       "10771            0.13  52662285.00  \n",
       "10772           -3.58  53358925.00  \n",
       "10773           -2.92  55964440.00  \n",
       "10774            0.35  56507660.00  \n",
       "10775            0.66  56855910.00  \n",
       "10776           -1.71  59082710.75  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjusted_close</th>\n",
       "      <th>change_percent</th>\n",
       "      <th>avg_vol_20d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10746</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>196.06</td>\n",
       "      <td>196.49</td>\n",
       "      <td>195.26</td>\n",
       "      <td>196.45</td>\n",
       "      <td>38824100</td>\n",
       "      <td>196.1851</td>\n",
       "      <td>0.32</td>\n",
       "      <td>49803320.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date    open    high     low   close    volume  adjusted_close  \\\n",
       "10746  2023-07-31  196.06  196.49  195.26  196.45  38824100        196.1851   \n",
       "\n",
       "       change_percent  avg_vol_20d  \n",
       "10746            0.32   49803320.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.iloc[:10747]\n",
    "data.tail(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "#data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns you want to standardize\n",
    "columns_to_standardize = [\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Loop through the columns and standardize each one\n",
    "for column in columns_to_standardize:\n",
    "    data[column] = scaler.fit_transform(data[[column]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0136, -1.3303,  1.3633],\n",
      "        [ 1.5315, -1.3303,  1.3633],\n",
      "        [-0.8024, -1.3303,  1.3633],\n",
      "        ...,\n",
      "        [-1.3469, -0.0567, -0.8422],\n",
      "        [ 1.6268, -0.0567, -0.8422],\n",
      "        [-0.7574, -0.0567, -0.8422]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with a 'date' column containing dates from 1980-12-12 to 2023-07-31\n",
    "data2 = pd.DataFrame({'date': pd.date_range(start='1980-12-12', end='2023-07-31')})\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "data2['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Extract day, month, and year from the date column\n",
    "data['day'] = data2['date'].dt.day\n",
    "data['month'] = data2['date'].dt.month\n",
    "data['year'] = data2['date'].dt.year\n",
    "\n",
    "# Define the embedding dimensions\n",
    "embedding_dim = 1  # You can adjust this dimension as needed\n",
    "\n",
    "# Create embedding layers for day, month, and year\n",
    "day_embedding = nn.Embedding(32, embedding_dim)  # 0-31 days\n",
    "month_embedding = nn.Embedding(13, embedding_dim)  # 1-12 months\n",
    "year_embedding = nn.Embedding(44, embedding_dim)  # Embedding for years from 1980 to 2023\n",
    "\n",
    "# Convert day, month, and year to tensors with Long data type\n",
    "day_tensor = torch.LongTensor(data['day'].values)\n",
    "month_tensor = torch.LongTensor(data['month'].values)\n",
    "year_tensor = torch.LongTensor(data['year'].values - 1980)  # Convert years to an index from 0 to 43\n",
    "\n",
    "# Pass tensors through embedding layers to get embeddings\n",
    "day_embeddings = day_embedding(day_tensor)\n",
    "month_embeddings = month_embedding(month_tensor)\n",
    "year_embeddings = year_embedding(year_tensor)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "date_embeddings = torch.cat((day_embeddings, month_embeddings, year_embeddings), dim=1)\n",
    "\n",
    "# Print the resulting embeddings\n",
    "print(date_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_7272\\3653438643.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: Learning Rate=0.00015, Sequence Length=75, Batch Size=64, Input Size=7, Date Embedding Dim=3, Hidden Dim=16,Layers=4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the x_train and y_train data\n",
    "x_train = data[['open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].to_numpy()\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Concatenate date embeddings with your testing data\n",
    "x_test_feature_tensors = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "x_test_date_embeddings = date_embeddings[len(x_train_scaled):]  # Use the remaining embeddings for testing data\n",
    "x_test_combined = torch.cat((x_test_feature_tensors, x_test_date_embeddings), dim=1)\n",
    "\n",
    "# Convert the combined testing data to PyTorch tensors\n",
    "x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inside your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size + date_embedding_dim  # Updated input size to include date embeddings\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Define the LSTM layer as a class attribute\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Ensure input has the shape [batch_size, sequence_length, input_size]\n",
    "        x = x.view(batch_size, 1, self.input_size)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 7  # Adjust based on the number of input features (excluding change_percent)\n",
    "date_embedding_dim = 3  # Adjust based on the dimension of your date embeddings\n",
    "output_size = 1\n",
    "hidden_dim = 16\n",
    "n_layers = 4\n",
    "sequence_length = 75  # Keep this as 1 for your input data\n",
    "batch_size = 64\n",
    "\n",
    "# Concatenate date embeddings with your feature vectors\n",
    "x_train_feature_tensors = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_train_date_embeddings = date_embeddings[:len(x_train_scaled)]  # Use the same length as your training data\n",
    "x_train_combined = torch.cat((x_train_feature_tensors, x_train_date_embeddings), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the combined data to PyTorch tensors\n",
    "#x_train_tensor = torch.tensor(x_train_combined, dtype=torch.float32)\n",
    "#y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Copy x_train_combined to create x_train_tensor\n",
    "x_train_tensor = x_train_combined.clone().detach()\n",
    "x_train_tensor = x_train_tensor.to(torch.float32)\n",
    "\n",
    "# Copy x_test_combined to create x_test_tensor\n",
    "x_test_tensor = x_test_combined.clone().detach()\n",
    "x_test_tensor = x_test_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "# Create y_train_tensor directly from y_train\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader for batch training\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.00015\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Hyperparameters: Learning Rate={learning_rate}, Sequence Length={sequence_length}, Batch Size={batch_size}, Input Size={input_size}, Date Embedding Dim={date_embedding_dim}, Hidden Dim={hidden_dim},'\n",
    "              f'Layers={n_layers}')\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 6000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Extract features on the testing set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Extract features from the hidden states\n",
    "            hidden_states, _ = model.lstm(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "            val_outputs = model(x_test_tensor)\n",
    "            val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "        # You can now use 'hidden_states' as the feature representations of your sequences\n",
    "        # The shape of 'hidden_states' will be (batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "# Print hidden_states after training\n",
    "print(hidden_states)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    hidden_states, _ = model.lstm(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "    val_outputs = model(x_test_tensor)\n",
    "    y_test_predictions = val_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. generate date embeddings for the future date, 11th August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the target date: 12th August\n",
    "target_date = \"11th August\"\n",
    "\n",
    "# Define the embedding dimensions (same as in your previous code)\n",
    "embedding_dim = 1\n",
    "\n",
    "# Define the maximum values for day, month, and year based on your previous code\n",
    "max_day = 31  # Maximum day\n",
    "max_month = 12  # Maximum month\n",
    "max_year = 43  # Maximum year (from 1980 to 2023)\n",
    "\n",
    "# Create one-hot encodings for day, month, and year\n",
    "day_encoding = torch.zeros(max_day)\n",
    "month_encoding = torch.zeros(max_month)\n",
    "year_encoding = torch.zeros(max_year + 1)  # +1 to account for the inclusive range\n",
    "\n",
    "# Map the target date to one-hot encoding\n",
    "# Extract the day and month from the target date\n",
    "day_index = int(target_date.split(\" \")[0].replace(\"th\", \"\")) - 1  # Extract the day from the target date\n",
    "month_index = 8  # August is the 9th month (0-based index)\n",
    "\n",
    "# Set the corresponding elements to 1\n",
    "day_encoding[day_index] = 1\n",
    "month_encoding[month_index] = 1\n",
    "year_encoding[43] = 1  # 43 corresponds to the year 2023 in your previous code\n",
    "\n",
    "# Concatenate the day, month, and year encodings to get the date embedding\n",
    "date_embedding_11_august = torch.cat((day_encoding, month_encoding, year_encoding), dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Combine LSTM Features with Date Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Assuming 'hidden_states' has shape [3547, 1, 32]\n",
    "# 'date_embedding_11_august' has shape [76]\n",
    "\n",
    "# Broadcast 'date_embedding_11_august' to match the shape of 'hidden_states'\n",
    "# This will repeat 'date_embedding_11_august' along the second dimension\n",
    "date_embedding_11_august_broadcasted = date_embedding_11_august.reshape(1, 1, -1).expand(3547, 1, -1)\n",
    "\n",
    "# Combine 'hidden_states' and 'date_embedding_11_august_broadcasted'\n",
    "combined_states = torch.cat((hidden_states, date_embedding_11_august_broadcasted), dim=2)\n",
    "\n",
    "# The resulting 'combined_states' will have a shape of [3547, 1, 32 + 76]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check the shape of 'combined_states_flat'\n",
    "##print(\"Shape of combined_states_flat:\", combined_states_flat.shape)\n",
    "\n",
    "# Check the shape of 'y_train'\n",
    "##print(\"Shape of y_train:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train a Random Forest Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3500\\696125167.py:16: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf_model.fit(combined_states_2d, close_values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming 'y_test_predictions' contains the predicted 'close' values\n",
    "# You can extract them as a numpy array using .numpy() method\n",
    "close_values = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape the 'combined_states' to remove the extra dimension\n",
    "combined_states_2d = combined_states.reshape(-1, combined_states.shape[-1])\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust hyperparameters as needed\n",
    "\n",
    "# Fit the model on your data\n",
    "rf_model.fit(combined_states_2d, close_values)\n",
    "\n",
    "# Now the Random Forest model is trained and ready for predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3500\\247992320.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf_model.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.1974559440747116\n",
      "Mean Squared Error (MSE): 0.13670674518134904\n",
      "Root Mean Squared Error (RMSE): 0.36973875260966227\n",
      "R-squared (R^2): 0.9999927374686682\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'y_test_predictions' contains the predicted 'close' values\n",
    "# You can extract them as a numpy array using .numpy() method\n",
    "close_values = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape the 'combined_states' to remove the extra dimension\n",
    "combined_states_2d = combined_states.reshape(-1, combined_states.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_states_2d, close_values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust hyperparameters as needed\n",
    "\n",
    "# Fit the model on your training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print or use the evaluation metrics\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f'R-squared (R^2): {r2}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make Predictions for 11th August:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3500\\691711526.py:53: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf_model.fit(combined_states_2d, close_values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Close Value for 11th August 2023: [ 41.53316753  43.02049961  28.32345577 ... 112.46756454  27.92956419\n",
      " 174.02247238]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming 'hidden_states' contains the LSTM features\n",
    "# 'y_test_predictions' contains the predicted 'close' values\n",
    "\n",
    "# Define the target date: 11th August\n",
    "target_date = \"11th August\"\n",
    "\n",
    "# Define the embedding dimensions (same as in your previous code)\n",
    "embedding_dim = 1\n",
    "\n",
    "# Define the maximum values for day, month, and year based on your previous code\n",
    "max_day = 31  # Maximum day\n",
    "max_month = 12  # Maximum month\n",
    "max_year = 43  # Maximum year (from 1980 to 2023)\n",
    "\n",
    "# Create one-hot encodings for day, month, and year\n",
    "day_encoding = torch.zeros(max_day)\n",
    "month_encoding = torch.zeros(max_month)\n",
    "year_encoding = torch.zeros(max_year + 1)  # +1 to account for the inclusive range\n",
    "\n",
    "# Map the target date to one-hot encoding\n",
    "# Extract the day and month from the target date\n",
    "day_index = int(target_date.split(\" \")[0].replace(\"th\", \"\")) - 1  # Extract the day from the target date\n",
    "month_index = 8  # August is the 9th month (0-based index)\n",
    "\n",
    "# Set the corresponding elements to 1\n",
    "day_encoding[day_index] = 1\n",
    "month_encoding[month_index] = 1\n",
    "year_encoding[43] = 1  # 43 corresponds to the year 2023 in your previous code\n",
    "\n",
    "# Concatenate the day, month, and year encodings to get the date embedding\n",
    "date_embedding_11_august = torch.cat((day_encoding, month_encoding, year_encoding), dim=0)\n",
    "\n",
    "# Broadcast 'date_embedding_11_august' to match the shape of 'hidden_states'\n",
    "date_embedding_11_august_broadcasted = date_embedding_11_august.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "\n",
    "# Combine 'hidden_states' and 'date_embedding_11_august_broadcasted'\n",
    "combined_states = torch.cat((hidden_states, date_embedding_11_august_broadcasted), dim=2)\n",
    "\n",
    "# Extract 'y_test_predictions' as a numpy array\n",
    "close_values = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape the 'combined_states' to remove the extra dimension\n",
    "combined_states_2d = combined_states.reshape(-1, combined_states.shape[-1])\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model on your data\n",
    "rf_model.fit(combined_states_2d, close_values)\n",
    "\n",
    "# Now, the Random Forest model is trained and ready for predictions\n",
    "\n",
    "# Make a prediction for 11th August 2023\n",
    "predicted_close_11_august = rf_model.predict(combined_states_2d)\n",
    "\n",
    "# 'predicted_close_11_august' contains the predicted \"close\" value for 11th August 2023\n",
    "# Print the predicted \"close\" value for 11th August 2023\n",
    "print(\"Predicted Close Value for 11th August 2023:\", predicted_close_11_august)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3500\\2648064599.py:53: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf_model.fit(combined_states_2d, close_values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Close Value for 12th August 2023: [ 41.53316753  43.02049961  28.32345577 ... 112.46756454  27.92956419\n",
      " 174.02247238]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming 'hidden_states' contains the LSTM features\n",
    "# 'y_test_predictions' contains the predicted 'close' values\n",
    "\n",
    "# Define the target date: 12th August\n",
    "target_date = \"12th August\"\n",
    "\n",
    "# Define the embedding dimensions (same as in your previous code)\n",
    "embedding_dim = 1\n",
    "\n",
    "# Define the maximum values for day, month, and year based on your previous code\n",
    "max_day = 31  # Maximum day\n",
    "max_month = 12  # Maximum month\n",
    "max_year = 43  # Maximum year (from 1980 to 2023)\n",
    "\n",
    "# Create one-hot encodings for day, month, and year\n",
    "day_encoding = torch.zeros(max_day)\n",
    "month_encoding = torch.zeros(max_month)\n",
    "year_encoding = torch.zeros(max_year + 1)  # +1 to account for the inclusive range\n",
    "\n",
    "# Map the target date to one-hot encoding\n",
    "# Extract the day and month from the target date\n",
    "day_index = int(target_date.split(\" \")[0].replace(\"th\", \"\")) - 1  # Extract the day from the target date\n",
    "month_index = 8  # August is the 9th month (0-based index)\n",
    "\n",
    "# Set the corresponding elements to 1\n",
    "day_encoding[day_index] = 1\n",
    "month_encoding[month_index] = 1\n",
    "year_encoding[43] = 1  # 43 corresponds to the year 2023 in your previous code\n",
    "\n",
    "# Concatenate the day, month, and year encodings to get the date embedding\n",
    "date_embedding_12_august = torch.cat((day_encoding, month_encoding, year_encoding), dim=0)\n",
    "\n",
    "# Broadcast 'date_embedding_12_august' to match the shape of 'hidden_states'\n",
    "date_embedding_12_august_broadcasted = date_embedding_12_august.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "\n",
    "# Combine 'hidden_states' and 'date_embedding_12_august_broadcasted'\n",
    "combined_states = torch.cat((hidden_states, date_embedding_12_august_broadcasted), dim=2)\n",
    "\n",
    "# Extract 'y_test_predictions' as a numpy array\n",
    "close_values = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape the 'combined_states' to remove the extra dimension\n",
    "combined_states_2d = combined_states.reshape(-1, combined_states.shape[-1])\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model on your data\n",
    "rf_model.fit(combined_states_2d, close_values)\n",
    "\n",
    "# Now, the Random Forest model is trained and ready for predictions\n",
    "\n",
    "# Make a prediction for 12th August 2023\n",
    "predicted_close_12_august = rf_model.predict(combined_states_2d)\n",
    "\n",
    "# 'predicted_close_12_august' contains the predicted \"close\" value for 12th August 2023\n",
    "# Print the predicted \"close\" value for 12th August 2023\n",
    "print(\"Predicted Close Value for 12th August 2023:\", predicted_close_12_august)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3547, 710]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\notepads\\Apple LSTM Hidden Dim 32 (Until 31.08.2023) Prediction 11 August.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20Hidden%20Dim%2032%20%28Until%2031.08.2023%29%20Prediction%2011%20August.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20Hidden%20Dim%2032%20%28Until%2031.08.2023%29%20Prediction%2011%20August.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Initialize and train the current model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20Hidden%20Dim%2032%20%28Until%2031.08.2023%29%20Prediction%2011%20August.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(reshaped_features, y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20Hidden%20Dim%2032%20%28Until%2031.08.2023%29%20Prediction%2011%20August.ipynb#X40sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projectarbeit-Dow-Jones-Index/notepads/Apple%20LSTM%20Hidden%20Dim%2032%20%28Until%2031.08.2023%29%20Prediction%2011%20August.ipynb#X40sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(reshaped_features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_forest.py:327\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[0;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 327\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    328\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[0;32m    329\u001b[0m )\n\u001b[0;32m    330\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\validation.py:981\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    965\u001b[0m     X,\n\u001b[0;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    977\u001b[0m )\n\u001b[0;32m    979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[1;32m--> 981\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m    983\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3547, 710]"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "\n",
    "\n",
    "# Reshape the hidden states\n",
    "reshaped_features = hidden_states.view(x_test_tensor.size(0), -1).numpy()\n",
    "\n",
    "# List of models to iterate over\n",
    "models = [\n",
    "    RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model_name = type(model).__name__\n",
    "    # Initialize and train the current model\n",
    "    model.fit(reshaped_features, y_test)\n",
    "\n",
    "    # Make predictions\n",
    "    model_predictions = model.predict(reshaped_features)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = mean_squared_error(y_test, model_predictions, squared=False)\n",
    "\n",
    "    # Print RMSE for the current model\n",
    "    print(f\"Model: {model_name}, RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor, BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Reshape the hidden states\n",
    "reshaped_features = hidden_states.view(x_test_tensor.size(0), -1).numpy()\n",
    "\n",
    "# List of models to iterate over\n",
    "models = [\n",
    "    RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    xgb.XGBRegressor(),\n",
    "    LinearRegression(),\n",
    "    Ridge(alpha=1.0),\n",
    "    Lasso(alpha=1.0),\n",
    "    xgb.XGBRegressor(n_estimators=100, random_state=42),\n",
    "    KNeighborsRegressor(n_neighbors=5),\n",
    "    DecisionTreeRegressor(),\n",
    "\n",
    "    HuberRegressor(),\n",
    "    SVR(),\n",
    "    GaussianProcessRegressor(),\n",
    "    SGDRegressor(),\n",
    "    TweedieRegressor(power=1, alpha=0.5),\n",
    "    AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "\n",
    "    OrthogonalMatchingPursuit(),\n",
    "    IsotonicRegression()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model_name = type(model).__name__\n",
    "    # Initialize and train the current model\n",
    "    model.fit(reshaped_features, y_test)\n",
    "\n",
    "    # Make predictions\n",
    "    model_predictions = model.predict(reshaped_features)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = mean_squared_error(y_test, model_predictions, squared=False)\n",
    "\n",
    "    # Print RMSE for the current model\n",
    "    print(f\"Model: {model_name}, RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "use the model to make predictions for future time steps.\n",
    "You input historical sequences of features, and the model generates predictions for future target values.\n",
    "\n",
    "Use the features extracted from the LSTM as input to a DecissiontreeRegressor model.\n",
    "Train these models to predict the target variable, close, using the LSTM-derived features.\n",
    "close value, on the future 12 agost 2023\n",
    "We dont have the feature values in the future\n",
    "\n",
    "\n",
    "\n",
    "# Specify the columns you want to standardize\n",
    "columns_to_standardize = [\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Loop through the columns and standardize each one\n",
    "for column in columns_to_standardize:\n",
    "    data[column] = scaler.fit_transform(data[[column]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with a 'date' column containing dates from 1980-12-12 to 2023-08-31\n",
    "data2 = pd.DataFrame({'date': pd.date_range(start='1980-12-12', end='2023-08-31')})\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "data2['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Extract day, month, and year from the date column\n",
    "data['day'] = data2['date'].dt.day\n",
    "data['month'] = data2['date'].dt.month\n",
    "data['year'] = data2['date'].dt.year\n",
    "\n",
    "# Define the embedding dimensions\n",
    "embedding_dim = 1  # You can adjust this dimension as needed\n",
    "\n",
    "# Create embedding layers for day, month, and year\n",
    "day_embedding = nn.Embedding(32, embedding_dim)  # 0-31 days\n",
    "month_embedding = nn.Embedding(13, embedding_dim)  # 1-12 months\n",
    "year_embedding = nn.Embedding(44, embedding_dim)  # Embedding for years from 1980 to 2023\n",
    "\n",
    "# Convert day, month, and year to tensors with Long data type\n",
    "day_tensor = torch.LongTensor(data['day'].values)\n",
    "month_tensor = torch.LongTensor(data['month'].values)\n",
    "year_tensor = torch.LongTensor(data['year'].values - 1980)  # Convert years to an index from 0 to 43\n",
    "\n",
    "# Pass tensors through embedding layers to get embeddings\n",
    "day_embeddings = day_embedding(day_tensor)\n",
    "month_embeddings = month_embedding(month_tensor)\n",
    "year_embeddings = year_embedding(year_tensor)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "date_embeddings = torch.cat((day_embeddings, month_embeddings, year_embeddings), dim=1)\n",
    "\n",
    "# Print the resulting embeddings\n",
    "print(date_embeddings)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the x_train and y_train data\n",
    "x_train = data[['open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].to_numpy()\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Concatenate date embeddings with your testing data\n",
    "x_test_feature_tensors = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "x_test_date_embeddings = date_embeddings[len(x_train_scaled):]  # Use the remaining embeddings for testing data\n",
    "x_test_combined = torch.cat((x_test_feature_tensors, x_test_date_embeddings), dim=1)\n",
    "\n",
    "# Convert the combined testing data to PyTorch tensors\n",
    "x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inside your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size + date_embedding_dim  # Updated input size to include date embeddings\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Define the LSTM layer as a class attribute\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Ensure input has the shape [batch_size, sequence_length, input_size]\n",
    "        x = x.view(batch_size, 1, self.input_size)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 7  # Adjust based on the number of input features (excluding change_percent)\n",
    "date_embedding_dim = 3  # Adjust based on the dimension of your date embeddings\n",
    "output_size = 1\n",
    "hidden_dim = 32\n",
    "n_layers = 4\n",
    "sequence_length = 75  # Keep this as 1 for your input data\n",
    "batch_size = 64\n",
    "\n",
    "# Concatenate date embeddings with your feature vectors\n",
    "x_train_feature_tensors = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_train_date_embeddings = date_embeddings[:len(x_train_scaled)]  # Use the same length as your training data\n",
    "x_train_combined = torch.cat((x_train_feature_tensors, x_train_date_embeddings), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the combined data to PyTorch tensors\n",
    "#x_train_tensor = torch.tensor(x_train_combined, dtype=torch.float32)\n",
    "#y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Copy x_train_combined to create x_train_tensor\n",
    "x_train_tensor = x_train_combined.clone().detach()\n",
    "x_train_tensor = x_train_tensor.to(torch.float32)\n",
    "\n",
    "# Copy x_test_combined to create x_test_tensor\n",
    "x_test_tensor = x_test_combined.clone().detach()\n",
    "x_test_tensor = x_test_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "# Create y_train_tensor directly from y_train\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader for batch training\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.00015\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Hyperparameters: Learning Rate={learning_rate}, Sequence Length={sequence_length}, Batch Size={batch_size}, Input Size={input_size}, Date Embedding Dim={date_embedding_dim}, Hidden Dim={hidden_dim},'\n",
    "              f'Layers={n_layers}')\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Extract features on the testing set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Extract features from the hidden states\n",
    "            hidden_states, _ = model.lstm(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "            val_outputs = model(x_test_tensor)\n",
    "            val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "        # You can now use 'hidden_states' as the feature representations of your sequences\n",
    "        # The shape of 'hidden_states' will be (batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "# Print hidden_states after training\n",
    "print(hidden_states)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the x_train and y_train data (as you have already done)\n",
    "x_train = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d'].to_numpy()\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets (as you have already done)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler (as you have already done)\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Create a PyTorch tensor for the testing data (as you have already done)\n",
    "x_test_feature_tensors = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Assume you have the LSTM-derived features in 'hidden_states' as you did in your code\n",
    "\n",
    "# Create a DecisionTreeRegressor model\n",
    "decision_tree_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Fit the model with the LSTM-derived features\n",
    "decision_tree_model.fit(hidden_states.numpy(), y_train)  # Assuming 'y_train' contains your training labels\n",
    "\n",
    "# Use the LSTM-derived features from the last time step for August 12, 2023\n",
    "# Replace these placeholders with the actual data from your LSTM model\n",
    "features_august_12 = ...  # Replace with the LSTM-derived features for August 12, 2023\n",
    "\n",
    "# Now, you can use the LSTM-derived features to make predictions with the DecisionTreeRegressor\n",
    "predicted_close = decision_tree_model.predict([features_august_12])\n",
    "\n",
    "# 'predicted_close' will contain the predicted close value for August 12, 2023\n",
    "print(f\"Predicted close on August 12, 2023: {predicted_close[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
