{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "#print(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "x_data = data[[ 'open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "y_data = data[\"close\"].values.reshape(-1, 1)  # Reshape y_data if it's 1D\n",
    "\n",
    "\n",
    "\n",
    "# Convert data to Tensors\n",
    "x_feature_tensors = torch.tensor(x_data, dtype=torch.float32)\n",
    "y_feature_tensors = torch.tensor(y_data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(x_feature_tensors) * 0.67)\n",
    "x_train, x_test = x_feature_tensors[:train_size], x_feature_tensors[train_size:]\n",
    "y_train, y_test = y_feature_tensors[:train_size], y_feature_tensors[train_size:]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fitting the MinMaxScaler to training data using fit_transform and then\n",
    "#  test data only transform\n",
    "x_train_scaled = scaler.fit_transform(x_train.detach().numpy())\n",
    "x_test_scaled = scaler.transform(x_test.detach().numpy())  # Use transform, not fit_transform\n",
    "\n",
    "y_train_scaled = scaler.fit_transform(y_train.detach().numpy())\n",
    "y_test_scaled = scaler.transform(y_test.detach().numpy())  # Use transform, not fit_transform\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 4\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train_scaled) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train_scaled.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd  # Import pandas for handling data rows\n",
    "\n",
    "# Assuming x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, input_size, output_size, test_window_size, scaler are available\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_scaled, y_train_scaled, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'learning_rate': [0.00001, 0.00005, 0.0001],\n",
    "    'window_size': [3, 5, 6],\n",
    "    'hidden_dim': [64, 128, 256],\n",
    "    'n_layers': [10, 11, 12],\n",
    "    'batch_evaluation_frequency': [4, 5, 6, ]\n",
    "}\n",
    "\n",
    "# Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 128,\n",
    "# 'n_layers': 11, 'batch_evaluation_frequency': 7}\n",
    "\n",
    "# Best Parameters: {'learning_rate': 0.0001, 'window_size': 10, 'hidden_dim': 256,\n",
    "# 'n_layers': 10, 'batch_evaluation_frequency': 5}\n",
    "\n",
    "best_predicted_error = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Number of random search iterations\n",
    "num_iterations = 10\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    params = {\n",
    "        'learning_rate': random.choice(search_space['learning_rate']),\n",
    "        'window_size': random.choice(search_space['window_size']),\n",
    "        'hidden_dim': random.choice(search_space['hidden_dim']),\n",
    "        'n_layers': random.choice(search_space['n_layers']),\n",
    "        'batch_evaluation_frequency': random.choice(search_space['batch_evaluation_frequency'])\n",
    "    }\n",
    "\n",
    "    model = LSTMModel(input_size, params['hidden_dim'], params['n_layers'], output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Training using Walk-Forward Validation\n",
    "    for i in range(params['window_size'], len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_idx = i - params['window_size']\n",
    "        end_idx = i\n",
    "        x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        y_window = torch.tensor(y_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        x_window = x_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        outputs, hidden = model(x_window, hidden)\n",
    "        loss = criterion(outputs, y_window)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % params['batch_evaluation_frequency'] == 0:\n",
    "            with torch.no_grad():\n",
    "                x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n",
    "                y_val_window = torch.tensor(y_val[:params['window_size']], dtype=torch.float32)\n",
    "                x_val_window = x_val_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "                hidden = model.init_hidden(1)\n",
    "                val_outputs, _ = model(x_val_window, hidden)\n",
    "                val_loss = criterion(val_outputs, y_val_window)\n",
    "\n",
    "                if i % 1000 == 0:  # Print every 1000 iterations\n",
    "                    print(f\"Iteration {i}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "            # Predict and calculate error on a specific data row\n",
    "            with torch.no_grad():\n",
    "                last_test_window = x_test_scaled[-test_window_size:]\n",
    "                last_test_window_tensor = torch.tensor(last_test_window, dtype=torch.float32).view(1, test_window_size, input_size)\n",
    "                hidden = model.init_hidden(1)\n",
    "                predicted_close_scaled, _ = model(last_test_window_tensor, hidden)\n",
    "\n",
    "                predicted_close = scaler.inverse_transform(predicted_close_scaled.detach().numpy())\n",
    "                real_value = pd.read_csv(\"../data/data/aapl_raw_data.csv\").loc[10748, 'close']\n",
    "                predicted_error = abs(real_value - predicted_close[0][0])\n",
    "\n",
    "                if predicted_error < best_predicted_error:\n",
    "                    best_predicted_error = predicted_error\n",
    "                    best_params = params\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Smallest Predicted Error: {best_predicted_error}\")\n",
    "\n",
    "# Print the best predicted close value\n",
    "with torch.no_grad():\n",
    "    last_test_window = x_test_scaled[-test_window_size:]\n",
    "    last_test_window_tensor = torch.tensor(last_test_window, dtype=torch.float32).view(1, test_window_size, input_size)\n",
    "    hidden = model.init_hidden(1)\n",
    "    predicted_close_scaled, _ = model(last_test_window_tensor, hidden)\n",
    "\n",
    "    predicted_close = scaler.inverse_transform(predicted_close_scaled.detach().numpy())\n",
    "    best_predicted_close = predicted_close[0][0]\n",
    "    print(f\"Best Predicted Close Value: {best_predicted_close}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
