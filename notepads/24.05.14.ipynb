{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=5\n",
    "Early stopping at epoch 24260 with validation loss 519.005615234375.\n",
    "Test Loss: 490.448974609375\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    # Initialize lists to store training and temporary sets\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    # Iterate through the data with the specified window size\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i+split_window_size+1]\n",
    "        y_out1_out2 = y_in.iloc[i:i+split_window_size+1]\n",
    "\n",
    "        # Separate the last row for the temporary set\n",
    "        # [ :-1]: all elements except the last one\n",
    "        # [-1:]:  selects only the last element\n",
    "        # (:) is used to indicate slicing of a sequence\n",
    "        # sequence[start : end : step]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    # Concatenate the lists into pandas DataFrames\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "# Split Data to train and temp\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 3\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split temp into val and test\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 1\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.01, window_size=5\n",
      "Epoch 1/150000, Loss: 30035.376953125, Validation Loss: 30248.16796875\n",
      "Epoch 101/150000, Loss: 28638.12109375, Validation Loss: 28839.83203125\n",
      "Epoch 201/150000, Loss: 27512.259765625, Validation Loss: 27711.30859375\n",
      "Epoch 301/150000, Loss: 26525.314453125, Validation Loss: 26721.0390625\n",
      "Epoch 401/150000, Loss: 25628.5703125, Validation Loss: 25820.9375\n",
      "Epoch 501/150000, Loss: 24806.658203125, Validation Loss: 24995.703125\n",
      "Epoch 601/150000, Loss: 24051.38671875, Validation Loss: 24237.169921875\n",
      "Epoch 701/150000, Loss: 23357.142578125, Validation Loss: 23539.734375\n",
      "Epoch 801/150000, Loss: 22719.52734375, Validation Loss: 22898.994140625\n",
      "Epoch 901/150000, Loss: 22134.796875, Validation Loss: 22311.2109375\n",
      "Epoch 1001/150000, Loss: 21599.626953125, Validation Loss: 21773.060546875\n",
      "Epoch 1101/150000, Loss: 21110.970703125, Validation Loss: 21281.494140625\n",
      "Epoch 1201/150000, Loss: 20665.9765625, Validation Loss: 20833.666015625\n",
      "Epoch 1301/150000, Loss: 20261.95703125, Validation Loss: 20426.888671875\n",
      "Epoch 1401/150000, Loss: 19896.353515625, Validation Loss: 20058.60546875\n",
      "Epoch 1501/150000, Loss: 19566.697265625, Validation Loss: 19726.349609375\n",
      "Epoch 1601/150000, Loss: 19270.59375, Validation Loss: 19427.728515625\n",
      "Epoch 1701/150000, Loss: 18188.27734375, Validation Loss: 18369.103515625\n",
      "Epoch 1801/150000, Loss: 17518.212890625, Validation Loss: 17689.83984375\n",
      "Epoch 1901/150000, Loss: 16977.533203125, Validation Loss: 17148.404296875\n",
      "Epoch 2001/150000, Loss: 16464.626953125, Validation Loss: 16634.494140625\n",
      "Epoch 2101/150000, Loss: 15977.4814453125, Validation Loss: 16145.357421875\n",
      "Epoch 2201/150000, Loss: 15513.7822265625, Validation Loss: 15679.0595703125\n",
      "Epoch 2301/150000, Loss: 15071.5126953125, Validation Loss: 15233.826171875\n",
      "Epoch 2401/150000, Loss: 14649.26953125, Validation Loss: 14808.7578125\n",
      "Epoch 2501/150000, Loss: 14245.728515625, Validation Loss: 14402.6875\n",
      "Epoch 2601/150000, Loss: 13858.998046875, Validation Loss: 14013.8857421875\n",
      "Epoch 2701/150000, Loss: 13486.0068359375, Validation Loss: 13639.8056640625\n",
      "Epoch 2801/150000, Loss: 13127.15234375, Validation Loss: 13279.8896484375\n",
      "Epoch 2901/150000, Loss: 12781.8759765625, Validation Loss: 12933.447265625\n",
      "Epoch 3001/150000, Loss: 12449.044921875, Validation Loss: 12599.306640625\n",
      "Epoch 3101/150000, Loss: 12127.4091796875, Validation Loss: 12276.49609375\n",
      "Epoch 3201/150000, Loss: 11816.2109375, Validation Loss: 11964.39453125\n",
      "Epoch 3301/150000, Loss: 11514.8681640625, Validation Loss: 11662.345703125\n",
      "Epoch 3401/150000, Loss: 11222.779296875, Validation Loss: 11369.712890625\n",
      "Epoch 3501/150000, Loss: 10939.3974609375, Validation Loss: 11085.861328125\n",
      "Epoch 3601/150000, Loss: 10664.1865234375, Validation Loss: 10809.7607421875\n",
      "Epoch 3701/150000, Loss: 10396.6337890625, Validation Loss: 10540.9755859375\n",
      "Epoch 3801/150000, Loss: 10136.3251953125, Validation Loss: 10279.92578125\n",
      "Epoch 3901/150000, Loss: 9882.80859375, Validation Loss: 10025.51953125\n",
      "Epoch 4001/150000, Loss: 9635.5791015625, Validation Loss: 9776.61328125\n",
      "Epoch 4101/150000, Loss: 9394.3818359375, Validation Loss: 9534.1826171875\n",
      "Epoch 4201/150000, Loss: 9158.86328125, Validation Loss: 9297.0751953125\n",
      "Epoch 4301/150000, Loss: 8928.6015625, Validation Loss: 9064.69140625\n",
      "Epoch 4401/150000, Loss: 8703.220703125, Validation Loss: 8836.9658203125\n",
      "Epoch 4501/150000, Loss: 8482.4697265625, Validation Loss: 8614.26953125\n",
      "Epoch 4601/150000, Loss: 8266.1103515625, Validation Loss: 8396.4306640625\n",
      "Epoch 4701/150000, Loss: 8053.96044921875, Validation Loss: 8183.1474609375\n",
      "Epoch 4801/150000, Loss: 7845.8388671875, Validation Loss: 7974.130859375\n",
      "Epoch 4901/150000, Loss: 7641.62109375, Validation Loss: 7769.24267578125\n",
      "Epoch 5001/150000, Loss: 7441.173828125, Validation Loss: 7568.3046875\n",
      "Epoch 5101/150000, Loss: 7244.4287109375, Validation Loss: 7371.18408203125\n",
      "Epoch 5201/150000, Loss: 7051.29296875, Validation Loss: 7177.79736328125\n",
      "Epoch 5301/150000, Loss: 6861.703125, Validation Loss: 6988.0224609375\n",
      "Epoch 5401/150000, Loss: 6675.60107421875, Validation Loss: 6801.8037109375\n",
      "Epoch 5501/150000, Loss: 6492.94677734375, Validation Loss: 6619.18408203125\n",
      "Epoch 5601/150000, Loss: 6313.70703125, Validation Loss: 6440.1259765625\n",
      "Epoch 5701/150000, Loss: 6137.48583984375, Validation Loss: 6261.958984375\n",
      "Epoch 5801/150000, Loss: 5964.96875, Validation Loss: 6089.576171875\n",
      "Epoch 5901/150000, Loss: 5795.90625, Validation Loss: 5920.7041015625\n",
      "Epoch 6001/150000, Loss: 5630.27685546875, Validation Loss: 5755.3388671875\n",
      "Epoch 6101/150000, Loss: 5468.1337890625, Validation Loss: 5593.5234375\n",
      "Epoch 6201/150000, Loss: 5309.455078125, Validation Loss: 5435.24365234375\n",
      "Epoch 6301/150000, Loss: 5154.23779296875, Validation Loss: 5280.51123046875\n",
      "Epoch 6401/150000, Loss: 5002.44287109375, Validation Loss: 5129.30029296875\n",
      "Epoch 6501/150000, Loss: 4853.982421875, Validation Loss: 4981.58740234375\n",
      "Epoch 6601/150000, Loss: 4708.744140625, Validation Loss: 4837.3056640625\n",
      "Epoch 6701/150000, Loss: 4566.6650390625, Validation Loss: 4696.255859375\n",
      "Epoch 6801/150000, Loss: 4427.7373046875, Validation Loss: 4558.28759765625\n",
      "Epoch 6901/150000, Loss: 4291.90283203125, Validation Loss: 4423.6416015625\n",
      "Epoch 7001/150000, Loss: 4159.2216796875, Validation Loss: 4292.009765625\n",
      "Epoch 7101/150000, Loss: 4029.473876953125, Validation Loss: 4163.75244140625\n",
      "Epoch 7201/150000, Loss: 3902.829345703125, Validation Loss: 4038.56884765625\n",
      "Epoch 7301/150000, Loss: 3779.3330078125, Validation Loss: 3916.43798828125\n",
      "Epoch 7401/150000, Loss: 3658.613525390625, Validation Loss: 3797.636962890625\n",
      "Epoch 7501/150000, Loss: 3541.027587890625, Validation Loss: 3681.874267578125\n",
      "Epoch 7601/150000, Loss: 3426.442138671875, Validation Loss: 3569.21923828125\n",
      "Epoch 7701/150000, Loss: 3314.842041015625, Validation Loss: 3459.404296875\n",
      "Epoch 7801/150000, Loss: 3206.22314453125, Validation Loss: 3352.58056640625\n",
      "Epoch 7901/150000, Loss: 3100.563232421875, Validation Loss: 3248.65869140625\n",
      "Epoch 8001/150000, Loss: 2997.82470703125, Validation Loss: 3147.6474609375\n",
      "Epoch 8101/150000, Loss: 2898.285888671875, Validation Loss: 3049.349609375\n",
      "Epoch 8201/150000, Loss: 2800.797607421875, Validation Loss: 2954.036376953125\n",
      "Epoch 8301/150000, Loss: 2706.25732421875, Validation Loss: 2861.167236328125\n",
      "Epoch 8401/150000, Loss: 2614.213134765625, Validation Loss: 2770.6513671875\n",
      "Epoch 8501/150000, Loss: 2524.61572265625, Validation Loss: 2682.821533203125\n",
      "Epoch 8601/150000, Loss: 2437.463134765625, Validation Loss: 2597.396484375\n",
      "Epoch 8701/150000, Loss: 2352.738037109375, Validation Loss: 2514.512451171875\n",
      "Epoch 8801/150000, Loss: 2270.389892578125, Validation Loss: 2434.085205078125\n",
      "Epoch 8901/150000, Loss: 2190.4677734375, Validation Loss: 2355.8837890625\n",
      "Epoch 9001/150000, Loss: 2110.367919921875, Validation Loss: 2266.135498046875\n",
      "Epoch 9101/150000, Loss: 2033.083251953125, Validation Loss: 2180.351318359375\n",
      "Epoch 9201/150000, Loss: 1959.3616943359375, Validation Loss: 2108.5\n",
      "Epoch 9301/150000, Loss: 1887.50244140625, Validation Loss: 2037.605224609375\n",
      "Epoch 9401/150000, Loss: 1817.8375244140625, Validation Loss: 1969.415771484375\n",
      "Epoch 9501/150000, Loss: 1750.2283935546875, Validation Loss: 1903.357177734375\n",
      "Epoch 9601/150000, Loss: 1684.6597900390625, Validation Loss: 1839.3079833984375\n",
      "Epoch 9701/150000, Loss: 1621.1051025390625, Validation Loss: 1777.3719482421875\n",
      "Epoch 9801/150000, Loss: 1559.5391845703125, Validation Loss: 1717.4830322265625\n",
      "Epoch 9901/150000, Loss: 1499.9208984375, Validation Loss: 1659.613037109375\n",
      "Epoch 10001/150000, Loss: 1442.235595703125, Validation Loss: 1603.573974609375\n",
      "Epoch 10101/150000, Loss: 1386.3314208984375, Validation Loss: 1549.85693359375\n",
      "Epoch 10201/150000, Loss: 1332.31298828125, Validation Loss: 1497.5047607421875\n",
      "Epoch 10301/150000, Loss: 1279.7880859375, Validation Loss: 1447.839111328125\n",
      "Epoch 10401/150000, Loss: 1228.9600830078125, Validation Loss: 1399.5743408203125\n",
      "Epoch 10501/150000, Loss: 1179.72216796875, Validation Loss: 1352.8909912109375\n",
      "Epoch 10601/150000, Loss: 1132.031982421875, Validation Loss: 1307.8756103515625\n",
      "Epoch 10701/150000, Loss: 1085.8724365234375, Validation Loss: 1264.3900146484375\n",
      "Epoch 10801/150000, Loss: 1041.249755859375, Validation Loss: 1222.3975830078125\n",
      "Epoch 10901/150000, Loss: 998.1546630859375, Validation Loss: 1182.16259765625\n",
      "Epoch 11001/150000, Loss: 956.5772705078125, Validation Loss: 1143.4571533203125\n",
      "Epoch 11101/150000, Loss: 916.500732421875, Validation Loss: 1106.2847900390625\n",
      "Epoch 11201/150000, Loss: 877.886474609375, Validation Loss: 1070.702880859375\n",
      "Epoch 11301/150000, Loss: 840.70703125, Validation Loss: 1036.5634765625\n",
      "Epoch 11401/150000, Loss: 804.8403930664062, Validation Loss: 1003.9197387695312\n",
      "Epoch 11501/150000, Loss: 770.3181762695312, Validation Loss: 972.3751831054688\n",
      "Epoch 11601/150000, Loss: 737.0419921875, Validation Loss: 942.4838256835938\n",
      "Epoch 11701/150000, Loss: 705.018798828125, Validation Loss: 913.5663452148438\n",
      "Epoch 11801/150000, Loss: 674.2144775390625, Validation Loss: 885.5260009765625\n",
      "Epoch 11901/150000, Loss: 644.5704345703125, Validation Loss: 859.121826171875\n",
      "Epoch 12001/150000, Loss: 616.070068359375, Validation Loss: 833.5165405273438\n",
      "Epoch 12101/150000, Loss: 588.673583984375, Validation Loss: 808.9326171875\n",
      "Epoch 12201/150000, Loss: 562.3380737304688, Validation Loss: 785.3468627929688\n",
      "Epoch 12301/150000, Loss: 537.051025390625, Validation Loss: 763.3350219726562\n",
      "Epoch 12401/150000, Loss: 512.7621459960938, Validation Loss: 741.1235961914062\n",
      "Epoch 12501/150000, Loss: 489.4670715332031, Validation Loss: 720.4544677734375\n",
      "Epoch 12601/150000, Loss: 467.1335144042969, Validation Loss: 700.6883544921875\n",
      "Epoch 12701/150000, Loss: 445.7366638183594, Validation Loss: 681.9490356445312\n",
      "Epoch 12801/150000, Loss: 425.5273742675781, Validation Loss: 663.6683959960938\n",
      "Epoch 12901/150000, Loss: 405.8375549316406, Validation Loss: 647.2645263671875\n",
      "Epoch 13001/150000, Loss: 387.3043212890625, Validation Loss: 631.3165893554688\n",
      "Epoch 13101/150000, Loss: 369.685791015625, Validation Loss: 616.2670288085938\n",
      "Epoch 13201/150000, Loss: 352.9363098144531, Validation Loss: 602.0189819335938\n",
      "Epoch 13301/150000, Loss: 337.0407409667969, Validation Loss: 588.6141357421875\n",
      "Epoch 13401/150000, Loss: 321.96173095703125, Validation Loss: 576.131591796875\n",
      "Epoch 13501/150000, Loss: 307.65484619140625, Validation Loss: 564.4949340820312\n",
      "Epoch 13601/150000, Loss: 294.0806579589844, Validation Loss: 553.90234375\n",
      "Epoch 13701/150000, Loss: 281.18011474609375, Validation Loss: 543.7044067382812\n",
      "Epoch 13801/150000, Loss: 268.89666748046875, Validation Loss: 533.6875610351562\n",
      "Epoch 13901/150000, Loss: 256.9282531738281, Validation Loss: 523.958251953125\n",
      "Epoch 14001/150000, Loss: 245.17161560058594, Validation Loss: 517.1885986328125\n",
      "Epoch 14101/150000, Loss: 234.51678466796875, Validation Loss: 509.52545166015625\n",
      "Epoch 14201/150000, Loss: 224.5399169921875, Validation Loss: 502.2197265625\n",
      "Epoch 14301/150000, Loss: 215.17236328125, Validation Loss: 495.4222106933594\n",
      "Epoch 14401/150000, Loss: 206.298095703125, Validation Loss: 488.6236572265625\n",
      "Epoch 14501/150000, Loss: 197.89710998535156, Validation Loss: 481.8715515136719\n",
      "Epoch 14601/150000, Loss: 189.83729553222656, Validation Loss: 476.5990905761719\n",
      "Epoch 14701/150000, Loss: 182.3341064453125, Validation Loss: 471.1934509277344\n",
      "Epoch 14801/150000, Loss: 175.131103515625, Validation Loss: 466.1731872558594\n",
      "Epoch 14901/150000, Loss: 168.25936889648438, Validation Loss: 463.5693359375\n",
      "Epoch 15001/150000, Loss: 161.65780639648438, Validation Loss: 459.996826171875\n",
      "Early stopping at epoch 15018 with validation loss 459.423095703125.\n",
      "Test Loss: 422.62841796875\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=5\n",
      "Epoch 1/150000, Loss: 29966.947265625, Validation Loss: 30182.4375\n",
      "Epoch 101/150000, Loss: 29259.5625, Validation Loss: 29465.724609375\n",
      "Epoch 201/150000, Loss: 28503.546875, Validation Loss: 28710.892578125\n",
      "Epoch 301/150000, Loss: 27914.33984375, Validation Loss: 28119.9609375\n",
      "Epoch 401/150000, Loss: 27378.091796875, Validation Loss: 27581.814453125\n",
      "Epoch 501/150000, Loss: 26875.2578125, Validation Loss: 27077.06640625\n",
      "Epoch 601/150000, Loss: 26398.337890625, Validation Loss: 26598.23046875\n",
      "Epoch 701/150000, Loss: 25943.474609375, Validation Loss: 26141.4765625\n",
      "Epoch 801/150000, Loss: 25508.33203125, Validation Loss: 25704.4609375\n",
      "Epoch 901/150000, Loss: 25091.3203125, Validation Loss: 25285.595703125\n",
      "Epoch 1001/150000, Loss: 24691.255859375, Validation Loss: 24883.693359375\n",
      "Epoch 1101/150000, Loss: 24307.21484375, Validation Loss: 24497.8359375\n",
      "Epoch 1201/150000, Loss: 23938.439453125, Validation Loss: 24127.265625\n",
      "Epoch 1301/150000, Loss: 23584.283203125, Validation Loss: 23771.330078125\n",
      "Epoch 1401/150000, Loss: 23244.1796875, Validation Loss: 23429.47265625\n",
      "Epoch 1501/150000, Loss: 22917.63671875, Validation Loss: 23101.189453125\n",
      "Epoch 1601/150000, Loss: 22604.1875, Validation Loss: 22786.017578125\n",
      "Epoch 1701/150000, Loss: 22303.42578125, Validation Loss: 22483.55078125\n",
      "Epoch 1801/150000, Loss: 22014.958984375, Validation Loss: 22193.400390625\n",
      "Epoch 1901/150000, Loss: 21738.431640625, Validation Loss: 21915.201171875\n",
      "Epoch 2001/150000, Loss: 21473.501953125, Validation Loss: 21648.623046875\n",
      "Epoch 2101/150000, Loss: 21219.8515625, Validation Loss: 21393.33984375\n",
      "Epoch 2201/150000, Loss: 20977.173828125, Validation Loss: 21149.044921875\n",
      "Epoch 2301/150000, Loss: 20745.173828125, Validation Loss: 20915.4453125\n",
      "Epoch 2401/150000, Loss: 20523.57421875, Validation Loss: 20692.265625\n",
      "Epoch 2501/150000, Loss: 20312.09375, Validation Loss: 20479.224609375\n",
      "Epoch 2601/150000, Loss: 20110.478515625, Validation Loss: 20276.060546875\n",
      "Epoch 2701/150000, Loss: 19918.453125, Validation Loss: 20082.509765625\n",
      "Epoch 2801/150000, Loss: 19225.138671875, Validation Loss: 19391.919921875\n",
      "Epoch 2901/150000, Loss: 18909.21875, Validation Loss: 19083.103515625\n",
      "Epoch 3001/150000, Loss: 18533.490234375, Validation Loss: 18714.6640625\n",
      "Epoch 3101/150000, Loss: 18219.337890625, Validation Loss: 18399.990234375\n",
      "Epoch 3201/150000, Loss: 17925.419921875, Validation Loss: 18104.775390625\n",
      "Epoch 3301/150000, Loss: 17639.83984375, Validation Loss: 17818.166015625\n",
      "Epoch 3401/150000, Loss: 17361.5546875, Validation Loss: 17538.943359375\n",
      "Epoch 3501/150000, Loss: 17090.044921875, Validation Loss: 17266.5\n",
      "Epoch 3601/150000, Loss: 16824.931640625, Validation Loss: 17000.421875\n",
      "Epoch 3701/150000, Loss: 16565.904296875, Validation Loss: 16740.369140625\n",
      "Epoch 3801/150000, Loss: 16312.685546875, Validation Loss: 16486.048828125\n",
      "Epoch 3901/150000, Loss: 16065.009765625, Validation Loss: 16237.1826171875\n",
      "Epoch 4001/150000, Loss: 15822.4501953125, Validation Loss: 15993.359375\n",
      "Epoch 4101/150000, Loss: 15584.197265625, Validation Loss: 15753.7890625\n",
      "Epoch 4201/150000, Loss: 15351.74609375, Validation Loss: 15520.107421875\n",
      "Epoch 4301/150000, Loss: 15124.2099609375, Validation Loss: 15291.1328125\n",
      "Epoch 4401/150000, Loss: 14901.3359375, Validation Loss: 15066.7744140625\n",
      "Epoch 4501/150000, Loss: 14682.9287109375, Validation Loss: 14846.8984375\n",
      "Epoch 4601/150000, Loss: 14468.8271484375, Validation Loss: 14631.357421875\n",
      "Epoch 4701/150000, Loss: 14258.8427734375, Validation Loss: 14419.994140625\n",
      "Epoch 4801/150000, Loss: 14052.681640625, Validation Loss: 14212.6171875\n",
      "Epoch 4901/150000, Loss: 13849.84375, Validation Loss: 14008.9580078125\n",
      "Epoch 5001/150000, Loss: 13650.3720703125, Validation Loss: 13808.755859375\n",
      "Epoch 5101/150000, Loss: 13454.3984375, Validation Loss: 13612.0205078125\n",
      "Epoch 5201/150000, Loss: 13261.8759765625, Validation Loss: 13418.783203125\n",
      "Epoch 5301/150000, Loss: 13072.705078125, Validation Loss: 13228.943359375\n",
      "Epoch 5401/150000, Loss: 12886.76953125, Validation Loss: 13042.3564453125\n",
      "Epoch 5501/150000, Loss: 12703.931640625, Validation Loss: 12858.87109375\n",
      "Epoch 5601/150000, Loss: 12524.0419921875, Validation Loss: 12678.3349609375\n",
      "Epoch 5701/150000, Loss: 12346.939453125, Validation Loss: 12500.5986328125\n",
      "Epoch 5801/150000, Loss: 12172.513671875, Validation Loss: 12325.5048828125\n",
      "Epoch 5901/150000, Loss: 12000.708984375, Validation Loss: 12152.9892578125\n",
      "Epoch 6001/150000, Loss: 11831.4697265625, Validation Loss: 11983.02734375\n",
      "Epoch 6101/150000, Loss: 11664.724609375, Validation Loss: 11815.5791015625\n",
      "Epoch 6201/150000, Loss: 11500.4189453125, Validation Loss: 11650.6142578125\n",
      "Epoch 6301/150000, Loss: 11338.5009765625, Validation Loss: 11488.08984375\n",
      "Epoch 6401/150000, Loss: 11178.9248046875, Validation Loss: 11327.9697265625\n",
      "Epoch 6501/150000, Loss: 11021.623046875, Validation Loss: 11170.1748046875\n",
      "Epoch 6601/150000, Loss: 10866.5576171875, Validation Loss: 11014.66015625\n",
      "Epoch 6701/150000, Loss: 10713.6630859375, Validation Loss: 10861.3447265625\n",
      "Epoch 6801/150000, Loss: 10562.8916015625, Validation Loss: 10710.1650390625\n",
      "Epoch 6901/150000, Loss: 10414.19140625, Validation Loss: 10561.0595703125\n",
      "Epoch 7001/150000, Loss: 10267.470703125, Validation Loss: 10413.9140625\n",
      "Epoch 7101/150000, Loss: 10122.681640625, Validation Loss: 10268.6650390625\n",
      "Epoch 7201/150000, Loss: 9979.7705078125, Validation Loss: 10125.216796875\n",
      "Epoch 7301/150000, Loss: 9838.6865234375, Validation Loss: 9983.5888671875\n",
      "Epoch 7401/150000, Loss: 9699.4384765625, Validation Loss: 9844.16796875\n",
      "Epoch 7501/150000, Loss: 9561.970703125, Validation Loss: 9706.763671875\n",
      "Epoch 7601/150000, Loss: 9426.232421875, Validation Loss: 9571.171875\n",
      "Epoch 7701/150000, Loss: 9292.173828125, Validation Loss: 9437.296875\n",
      "Epoch 7801/150000, Loss: 9159.740234375, Validation Loss: 9305.048828125\n",
      "Epoch 7901/150000, Loss: 9028.8701171875, Validation Loss: 9174.3486328125\n",
      "Epoch 8001/150000, Loss: 8899.5068359375, Validation Loss: 9045.1162109375\n",
      "Epoch 8101/150000, Loss: 8771.5791015625, Validation Loss: 8917.283203125\n",
      "Epoch 8201/150000, Loss: 8645.0302734375, Validation Loss: 8790.7900390625\n",
      "Epoch 8301/150000, Loss: 8519.8095703125, Validation Loss: 8665.6181640625\n",
      "Epoch 8401/150000, Loss: 8395.892578125, Validation Loss: 8541.78515625\n",
      "Epoch 8501/150000, Loss: 8273.2685546875, Validation Loss: 8419.2607421875\n",
      "Epoch 8601/150000, Loss: 8151.92919921875, Validation Loss: 8297.990234375\n",
      "Epoch 8701/150000, Loss: 8031.84765625, Validation Loss: 8177.81298828125\n",
      "Epoch 8801/150000, Loss: 7912.95458984375, Validation Loss: 8057.71337890625\n",
      "Epoch 8901/150000, Loss: 7794.7978515625, Validation Loss: 7932.5458984375\n",
      "Epoch 9001/150000, Loss: 7677.65234375, Validation Loss: 7812.44775390625\n",
      "Epoch 9101/150000, Loss: 7562.0126953125, Validation Loss: 7696.05029296875\n",
      "Epoch 9201/150000, Loss: 7447.7744140625, Validation Loss: 7581.513671875\n",
      "Epoch 9301/150000, Loss: 7334.76953125, Validation Loss: 7468.32421875\n",
      "Epoch 9401/150000, Loss: 7222.95947265625, Validation Loss: 7356.42431640625\n",
      "Epoch 9501/150000, Loss: 7112.3095703125, Validation Loss: 7245.7626953125\n",
      "Epoch 9601/150000, Loss: 7002.12158203125, Validation Loss: 7131.865234375\n",
      "Epoch 9701/150000, Loss: 6893.150390625, Validation Loss: 7018.43701171875\n",
      "Epoch 9801/150000, Loss: 6785.34619140625, Validation Loss: 6908.8203125\n",
      "Epoch 9901/150000, Loss: 6679.0419921875, Validation Loss: 6801.814453125\n",
      "Epoch 10001/150000, Loss: 6573.95166015625, Validation Loss: 6696.30908203125\n",
      "Epoch 10101/150000, Loss: 6470.02587890625, Validation Loss: 6592.06640625\n",
      "Epoch 10201/150000, Loss: 6367.25244140625, Validation Loss: 6489.06201171875\n",
      "Epoch 10301/150000, Loss: 6265.60986328125, Validation Loss: 6387.19482421875\n",
      "Epoch 10401/150000, Loss: 6165.130859375, Validation Loss: 6286.51123046875\n",
      "Epoch 10501/150000, Loss: 6065.77978515625, Validation Loss: 6186.96337890625\n",
      "Epoch 10601/150000, Loss: 5967.5458984375, Validation Loss: 6088.5498046875\n",
      "Epoch 10701/150000, Loss: 5870.43408203125, Validation Loss: 5991.28271484375\n",
      "Epoch 10801/150000, Loss: 5774.4755859375, Validation Loss: 5895.1953125\n",
      "Epoch 10901/150000, Loss: 5679.69287109375, Validation Loss: 5800.3134765625\n",
      "Epoch 11001/150000, Loss: 5586.0693359375, Validation Loss: 5706.61669921875\n",
      "Epoch 11101/150000, Loss: 5493.56982421875, Validation Loss: 5614.0615234375\n",
      "Epoch 11201/150000, Loss: 5402.216796875, Validation Loss: 5522.6923828125\n",
      "Epoch 11301/150000, Loss: 5312.02587890625, Validation Loss: 5432.48046875\n",
      "Epoch 11401/150000, Loss: 5222.943359375, Validation Loss: 5343.40185546875\n",
      "Epoch 11501/150000, Loss: 5135.013671875, Validation Loss: 5255.5439453125\n",
      "Epoch 11601/150000, Loss: 5048.16748046875, Validation Loss: 5168.826171875\n",
      "Epoch 11701/150000, Loss: 4962.43896484375, Validation Loss: 5083.2177734375\n",
      "Epoch 11801/150000, Loss: 4877.77490234375, Validation Loss: 4998.80615234375\n",
      "Epoch 11901/150000, Loss: 4794.1328125, Validation Loss: 4915.38525390625\n",
      "Epoch 12001/150000, Loss: 4711.4990234375, Validation Loss: 4833.09033203125\n",
      "Epoch 12101/150000, Loss: 4629.84375, Validation Loss: 4751.91455078125\n",
      "Epoch 12201/150000, Loss: 4549.1650390625, Validation Loss: 4671.66943359375\n",
      "Epoch 12301/150000, Loss: 4469.537109375, Validation Loss: 4592.5302734375\n",
      "Epoch 12401/150000, Loss: 4390.9033203125, Validation Loss: 4514.392578125\n",
      "Epoch 12501/150000, Loss: 4313.294921875, Validation Loss: 4437.30029296875\n",
      "Epoch 12601/150000, Loss: 4236.68798828125, Validation Loss: 4361.17236328125\n",
      "Epoch 12701/150000, Loss: 4161.10009765625, Validation Loss: 4286.09521484375\n",
      "Epoch 12801/150000, Loss: 4086.497314453125, Validation Loss: 4212.03857421875\n",
      "Epoch 12901/150000, Loss: 4012.89453125, Validation Loss: 4139.0009765625\n",
      "Epoch 13001/150000, Loss: 3940.295166015625, Validation Loss: 4066.90576171875\n",
      "Epoch 13101/150000, Loss: 3868.643310546875, Validation Loss: 3995.968994140625\n",
      "Epoch 13201/150000, Loss: 3797.990234375, Validation Loss: 3925.9619140625\n",
      "Epoch 13301/150000, Loss: 3728.318359375, Validation Loss: 3856.946044921875\n",
      "Epoch 13401/150000, Loss: 3659.62451171875, Validation Loss: 3788.927490234375\n",
      "Epoch 13501/150000, Loss: 3591.900390625, Validation Loss: 3721.890380859375\n",
      "Epoch 13601/150000, Loss: 3525.144775390625, Validation Loss: 3655.845703125\n",
      "Epoch 13701/150000, Loss: 3459.351806640625, Validation Loss: 3590.770263671875\n",
      "Epoch 13801/150000, Loss: 3394.52294921875, Validation Loss: 3526.665771484375\n",
      "Epoch 13901/150000, Loss: 3330.6484375, Validation Loss: 3463.50830078125\n",
      "Epoch 14001/150000, Loss: 3267.72900390625, Validation Loss: 3401.3017578125\n",
      "Epoch 14101/150000, Loss: 3205.75537109375, Validation Loss: 3340.02685546875\n",
      "Epoch 14201/150000, Loss: 3144.71337890625, Validation Loss: 3279.625732421875\n",
      "Epoch 14301/150000, Loss: 3084.54248046875, Validation Loss: 3219.981201171875\n",
      "Epoch 14401/150000, Loss: 3025.35595703125, Validation Loss: 3161.440185546875\n",
      "Epoch 14501/150000, Loss: 2967.06884765625, Validation Loss: 3103.62939453125\n",
      "Epoch 14601/150000, Loss: 2909.644775390625, Validation Loss: 3046.371337890625\n",
      "Epoch 14701/150000, Loss: 2853.0498046875, Validation Loss: 2989.88134765625\n",
      "Epoch 14801/150000, Loss: 2797.299560546875, Validation Loss: 2934.67919921875\n",
      "Epoch 14901/150000, Loss: 2742.41259765625, Validation Loss: 2880.507080078125\n",
      "Epoch 15001/150000, Loss: 2688.34521484375, Validation Loss: 2827.228759765625\n",
      "Epoch 15101/150000, Loss: 2635.126708984375, Validation Loss: 2774.8291015625\n",
      "Epoch 15201/150000, Loss: 2582.725830078125, Validation Loss: 2723.370849609375\n",
      "Epoch 15301/150000, Loss: 2531.11865234375, Validation Loss: 2672.5498046875\n",
      "Epoch 15401/150000, Loss: 2480.31201171875, Validation Loss: 2622.64208984375\n",
      "Epoch 15501/150000, Loss: 2430.320556640625, Validation Loss: 2573.5693359375\n",
      "Epoch 15601/150000, Loss: 2381.08837890625, Validation Loss: 2525.2822265625\n",
      "Epoch 15701/150000, Loss: 2332.646728515625, Validation Loss: 2477.806640625\n",
      "Epoch 15801/150000, Loss: 2284.931396484375, Validation Loss: 2431.070068359375\n",
      "Epoch 15901/150000, Loss: 2237.979248046875, Validation Loss: 2385.094482421875\n",
      "Epoch 16001/150000, Loss: 2191.74072265625, Validation Loss: 2339.87841796875\n",
      "Epoch 16101/150000, Loss: 2146.181884765625, Validation Loss: 2295.330322265625\n",
      "Epoch 16201/150000, Loss: 2101.33203125, Validation Loss: 2251.509765625\n",
      "Epoch 16301/150000, Loss: 2057.162353515625, Validation Loss: 2208.383056640625\n",
      "Epoch 16401/150000, Loss: 2013.6444091796875, Validation Loss: 2166.031494140625\n",
      "Epoch 16501/150000, Loss: 1970.75390625, Validation Loss: 2124.577392578125\n",
      "Epoch 16601/150000, Loss: 1928.5816650390625, Validation Loss: 2083.537841796875\n",
      "Epoch 16701/150000, Loss: 1887.0909423828125, Validation Loss: 2043.005615234375\n",
      "Epoch 16801/150000, Loss: 1846.2987060546875, Validation Loss: 2003.164306640625\n",
      "Epoch 16901/150000, Loss: 1806.184326171875, Validation Loss: 1963.9749755859375\n",
      "Epoch 17001/150000, Loss: 1766.7523193359375, Validation Loss: 1925.5477294921875\n",
      "Epoch 17101/150000, Loss: 1728.0057373046875, Validation Loss: 1887.8258056640625\n",
      "Epoch 17201/150000, Loss: 1689.9268798828125, Validation Loss: 1850.7152099609375\n",
      "Epoch 17301/150000, Loss: 1652.5146484375, Validation Loss: 1814.358154296875\n",
      "Epoch 17401/150000, Loss: 1615.7618408203125, Validation Loss: 1778.63525390625\n",
      "Epoch 17501/150000, Loss: 1579.6771240234375, Validation Loss: 1743.470458984375\n",
      "Epoch 17601/150000, Loss: 1544.2127685546875, Validation Loss: 1709.380615234375\n",
      "Epoch 17701/150000, Loss: 1509.3790283203125, Validation Loss: 1675.6856689453125\n",
      "Epoch 17801/150000, Loss: 1475.17919921875, Validation Loss: 1642.666748046875\n",
      "Epoch 17901/150000, Loss: 1441.592529296875, Validation Loss: 1610.24658203125\n",
      "Epoch 18001/150000, Loss: 1408.6031494140625, Validation Loss: 1578.5224609375\n",
      "Epoch 18101/150000, Loss: 1376.1981201171875, Validation Loss: 1547.6439208984375\n",
      "Epoch 18201/150000, Loss: 1344.365234375, Validation Loss: 1517.4375\n",
      "Epoch 18301/150000, Loss: 1313.0848388671875, Validation Loss: 1487.7447509765625\n",
      "Epoch 18401/150000, Loss: 1282.3099365234375, Validation Loss: 1458.8697509765625\n",
      "Epoch 18501/150000, Loss: 1252.0914306640625, Validation Loss: 1430.3499755859375\n",
      "Epoch 18601/150000, Loss: 1222.3907470703125, Validation Loss: 1402.399658203125\n",
      "Epoch 18701/150000, Loss: 1193.2349853515625, Validation Loss: 1374.873291015625\n",
      "Epoch 18801/150000, Loss: 1164.6063232421875, Validation Loss: 1347.9581298828125\n",
      "Epoch 18901/150000, Loss: 1136.5057373046875, Validation Loss: 1321.5498046875\n",
      "Epoch 19001/150000, Loss: 1108.9073486328125, Validation Loss: 1295.988525390625\n",
      "Epoch 19101/150000, Loss: 1081.8515625, Validation Loss: 1270.83837890625\n",
      "Epoch 19201/150000, Loss: 1055.32421875, Validation Loss: 1246.373291015625\n",
      "Epoch 19301/150000, Loss: 1029.3167724609375, Validation Loss: 1221.9615478515625\n",
      "Epoch 19401/150000, Loss: 1003.828369140625, Validation Loss: 1198.3699951171875\n",
      "Epoch 19501/150000, Loss: 978.859375, Validation Loss: 1175.1822509765625\n",
      "Epoch 19601/150000, Loss: 954.4171752929688, Validation Loss: 1152.80859375\n",
      "Epoch 19701/150000, Loss: 930.3790893554688, Validation Loss: 1129.81787109375\n",
      "Epoch 19801/150000, Loss: 905.9193115234375, Validation Loss: 1099.5250244140625\n",
      "Epoch 19901/150000, Loss: 882.5971069335938, Validation Loss: 1076.0491943359375\n",
      "Epoch 20001/150000, Loss: 859.97216796875, Validation Loss: 1054.9149169921875\n",
      "Epoch 20101/150000, Loss: 837.8510131835938, Validation Loss: 1034.505615234375\n",
      "Epoch 20201/150000, Loss: 816.2032470703125, Validation Loss: 1014.65869140625\n",
      "Epoch 20301/150000, Loss: 795.0165405273438, Validation Loss: 995.1851806640625\n",
      "Epoch 20401/150000, Loss: 774.27685546875, Validation Loss: 976.3912963867188\n",
      "Epoch 20501/150000, Loss: 753.9782104492188, Validation Loss: 957.8587036132812\n",
      "Epoch 20601/150000, Loss: 734.1122436523438, Validation Loss: 939.8056030273438\n",
      "Epoch 20701/150000, Loss: 714.6737670898438, Validation Loss: 922.14013671875\n",
      "Epoch 20801/150000, Loss: 695.6566162109375, Validation Loss: 904.8502197265625\n",
      "Epoch 20901/150000, Loss: 677.0548706054688, Validation Loss: 887.9306030273438\n",
      "Epoch 21001/150000, Loss: 658.8660278320312, Validation Loss: 871.3706665039062\n",
      "Epoch 21101/150000, Loss: 641.0819702148438, Validation Loss: 855.15185546875\n",
      "Epoch 21201/150000, Loss: 623.6942749023438, Validation Loss: 839.3400268554688\n",
      "Epoch 21301/150000, Loss: 606.6973266601562, Validation Loss: 823.9125366210938\n",
      "Epoch 21401/150000, Loss: 590.0812377929688, Validation Loss: 808.8031616210938\n",
      "Epoch 21501/150000, Loss: 573.84423828125, Validation Loss: 794.0106811523438\n",
      "Epoch 21601/150000, Loss: 557.9763793945312, Validation Loss: 779.7421875\n",
      "Epoch 21701/150000, Loss: 542.4756469726562, Validation Loss: 765.7693481445312\n",
      "Epoch 21801/150000, Loss: 527.3399047851562, Validation Loss: 752.1508178710938\n",
      "Epoch 21901/150000, Loss: 512.5614624023438, Validation Loss: 738.9139404296875\n",
      "Epoch 22001/150000, Loss: 498.1315002441406, Validation Loss: 726.0982055664062\n",
      "Epoch 22101/150000, Loss: 484.0444641113281, Validation Loss: 713.8177490234375\n",
      "Epoch 22201/150000, Loss: 470.3102111816406, Validation Loss: 701.7633666992188\n",
      "Epoch 22301/150000, Loss: 456.94097900390625, Validation Loss: 689.857666015625\n",
      "Epoch 22401/150000, Loss: 443.8617858886719, Validation Loss: 678.688720703125\n",
      "Epoch 22501/150000, Loss: 431.14892578125, Validation Loss: 667.7109985351562\n",
      "Epoch 22601/150000, Loss: 415.5406799316406, Validation Loss: 643.822998046875\n",
      "Epoch 22701/150000, Loss: 396.7792053222656, Validation Loss: 617.6644897460938\n",
      "Epoch 22801/150000, Loss: 382.00225830078125, Validation Loss: 609.5433349609375\n",
      "Epoch 22901/150000, Loss: 369.340576171875, Validation Loss: 602.8333740234375\n",
      "Epoch 23001/150000, Loss: 357.81707763671875, Validation Loss: 593.6617431640625\n",
      "Epoch 23101/150000, Loss: 346.7602233886719, Validation Loss: 586.6539306640625\n",
      "Epoch 23201/150000, Loss: 336.09344482421875, Validation Loss: 579.7050170898438\n",
      "Epoch 23301/150000, Loss: 325.7708740234375, Validation Loss: 573.4241943359375\n",
      "Epoch 23401/150000, Loss: 315.7932434082031, Validation Loss: 569.2670288085938\n",
      "Early stopping at epoch 23480 with validation loss 567.8870239257812.\n",
      "Test Loss: 557.4375\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.001, window_size=5\n",
      "Epoch 1/150000, Loss: 30026.98828125, Validation Loss: 30245.748046875\n",
      "Epoch 101/150000, Loss: 29944.78515625, Validation Loss: 30163.515625\n",
      "Epoch 201/150000, Loss: 29826.865234375, Validation Loss: 30043.779296875\n",
      "Epoch 301/150000, Loss: 29558.802734375, Validation Loss: 29774.392578125\n",
      "Epoch 401/150000, Loss: 29355.529296875, Validation Loss: 29570.98828125\n",
      "Epoch 501/150000, Loss: 29199.46875, Validation Loss: 29414.548828125\n",
      "Epoch 601/150000, Loss: 29061.76171875, Validation Loss: 29276.400390625\n",
      "Epoch 701/150000, Loss: 28933.583984375, Validation Loss: 29147.765625\n",
      "Epoch 801/150000, Loss: 28811.37890625, Validation Loss: 29025.119140625\n",
      "Epoch 901/150000, Loss: 28693.384765625, Validation Loss: 28906.67578125\n",
      "Epoch 1001/150000, Loss: 28578.56640625, Validation Loss: 28791.41015625\n",
      "Epoch 1101/150000, Loss: 28466.271484375, Validation Loss: 28678.6796875\n",
      "Epoch 1201/150000, Loss: 28356.07421875, Validation Loss: 28568.041015625\n",
      "Epoch 1301/150000, Loss: 28247.658203125, Validation Loss: 28459.19140625\n",
      "Epoch 1401/150000, Loss: 28140.796875, Validation Loss: 28351.8984375\n",
      "Epoch 1501/150000, Loss: 28035.30859375, Validation Loss: 28245.982421875\n",
      "Epoch 1601/150000, Loss: 27931.068359375, Validation Loss: 28141.314453125\n",
      "Epoch 1701/150000, Loss: 27827.962890625, Validation Loss: 28037.78125\n",
      "Epoch 1801/150000, Loss: 27725.904296875, Validation Loss: 27935.298828125\n",
      "Epoch 1901/150000, Loss: 27624.822265625, Validation Loss: 27833.796875\n",
      "Epoch 2001/150000, Loss: 27524.66015625, Validation Loss: 27733.208984375\n",
      "Epoch 2101/150000, Loss: 27425.353515625, Validation Loss: 27633.48828125\n",
      "Epoch 2201/150000, Loss: 27326.875, Validation Loss: 27534.58984375\n",
      "Epoch 2301/150000, Loss: 27229.1796875, Validation Loss: 27436.4765625\n",
      "Epoch 2401/150000, Loss: 27132.2421875, Validation Loss: 27339.1171875\n",
      "Epoch 2501/150000, Loss: 27036.01953125, Validation Loss: 27242.482421875\n",
      "Epoch 2601/150000, Loss: 26940.498046875, Validation Loss: 27146.544921875\n",
      "Epoch 2701/150000, Loss: 26845.65625, Validation Loss: 27051.287109375\n",
      "Epoch 2801/150000, Loss: 26751.470703125, Validation Loss: 26956.689453125\n",
      "Epoch 2901/150000, Loss: 26657.92578125, Validation Loss: 26862.734375\n",
      "Epoch 3001/150000, Loss: 26565.009765625, Validation Loss: 26769.400390625\n",
      "Epoch 3101/150000, Loss: 26472.69921875, Validation Loss: 26676.68359375\n",
      "Epoch 3201/150000, Loss: 26380.994140625, Validation Loss: 26584.560546875\n",
      "Epoch 3301/150000, Loss: 26289.873046875, Validation Loss: 26493.029296875\n",
      "Epoch 3401/150000, Loss: 26199.330078125, Validation Loss: 26402.07421875\n",
      "Epoch 3501/150000, Loss: 26109.35546875, Validation Loss: 26311.6953125\n",
      "Epoch 3601/150000, Loss: 26019.943359375, Validation Loss: 26221.87109375\n",
      "Epoch 3701/150000, Loss: 25931.083984375, Validation Loss: 26132.599609375\n",
      "Epoch 3801/150000, Loss: 25842.76953125, Validation Loss: 26043.875\n",
      "Epoch 3901/150000, Loss: 25754.994140625, Validation Loss: 25955.6953125\n",
      "Epoch 4001/150000, Loss: 25667.75390625, Validation Loss: 25868.044921875\n",
      "Epoch 4101/150000, Loss: 25581.041015625, Validation Loss: 25780.921875\n",
      "Epoch 4201/150000, Loss: 25494.85546875, Validation Loss: 25694.326171875\n",
      "Epoch 4301/150000, Loss: 25409.18359375, Validation Loss: 25608.248046875\n",
      "Epoch 4401/150000, Loss: 25324.03125, Validation Loss: 25522.685546875\n",
      "Epoch 4501/150000, Loss: 25239.388671875, Validation Loss: 25437.638671875\n",
      "Epoch 4601/150000, Loss: 25155.25390625, Validation Loss: 25353.095703125\n",
      "Epoch 4701/150000, Loss: 25071.623046875, Validation Loss: 25269.060546875\n",
      "Epoch 4801/150000, Loss: 24988.49609375, Validation Loss: 25185.52734375\n",
      "Epoch 4901/150000, Loss: 24905.87109375, Validation Loss: 25102.494140625\n",
      "Epoch 5001/150000, Loss: 24823.73828125, Validation Loss: 25019.958984375\n",
      "Epoch 5101/150000, Loss: 24742.103515625, Validation Loss: 24937.916015625\n",
      "Epoch 5201/150000, Loss: 24660.9609375, Validation Loss: 24856.36328125\n",
      "Epoch 5301/150000, Loss: 24580.306640625, Validation Loss: 24775.306640625\n",
      "Epoch 5401/150000, Loss: 24500.1484375, Validation Loss: 24694.740234375\n",
      "Epoch 5501/150000, Loss: 24420.470703125, Validation Loss: 24614.662109375\n",
      "Epoch 5601/150000, Loss: 24341.28125, Validation Loss: 24535.06640625\n",
      "Epoch 5701/150000, Loss: 24262.578125, Validation Loss: 24455.95703125\n",
      "Epoch 5801/150000, Loss: 24184.357421875, Validation Loss: 24377.330078125\n",
      "Epoch 5901/150000, Loss: 24106.61328125, Validation Loss: 24299.1875\n",
      "Epoch 6001/150000, Loss: 24029.357421875, Validation Loss: 24221.521484375\n",
      "Epoch 6101/150000, Loss: 23952.576171875, Validation Loss: 24144.337890625\n",
      "Epoch 6201/150000, Loss: 23876.27734375, Validation Loss: 24067.63671875\n",
      "Epoch 6301/150000, Loss: 23800.45703125, Validation Loss: 23991.41015625\n",
      "Epoch 6401/150000, Loss: 23725.109375, Validation Loss: 23915.658203125\n",
      "Epoch 6501/150000, Loss: 23650.240234375, Validation Loss: 23840.38671875\n",
      "Epoch 6601/150000, Loss: 23575.845703125, Validation Loss: 23765.58984375\n",
      "Epoch 6701/150000, Loss: 23501.92578125, Validation Loss: 23691.267578125\n",
      "Epoch 6801/150000, Loss: 23428.48046875, Validation Loss: 23617.41796875\n",
      "Epoch 6901/150000, Loss: 23355.5078125, Validation Loss: 23544.041015625\n",
      "Epoch 7001/150000, Loss: 23283.0078125, Validation Loss: 23471.14453125\n",
      "Epoch 7101/150000, Loss: 23210.984375, Validation Loss: 23398.712890625\n",
      "Epoch 7201/150000, Loss: 23139.427734375, Validation Loss: 23326.7578125\n",
      "Epoch 7301/150000, Loss: 23068.345703125, Validation Loss: 23255.26953125\n",
      "Epoch 7401/150000, Loss: 22997.732421875, Validation Loss: 23184.25390625\n",
      "Epoch 7501/150000, Loss: 22927.58984375, Validation Loss: 23113.7109375\n",
      "Epoch 7601/150000, Loss: 22857.91796875, Validation Loss: 23043.63671875\n",
      "Epoch 7701/150000, Loss: 22788.712890625, Validation Loss: 22974.033203125\n",
      "Epoch 7801/150000, Loss: 22719.982421875, Validation Loss: 22904.900390625\n",
      "Epoch 7901/150000, Loss: 22651.71875, Validation Loss: 22836.23046875\n",
      "Epoch 8001/150000, Loss: 22583.912109375, Validation Loss: 22768.025390625\n",
      "Epoch 8101/150000, Loss: 22516.591796875, Validation Loss: 22700.3046875\n",
      "Epoch 8201/150000, Loss: 22449.71484375, Validation Loss: 22633.029296875\n",
      "Epoch 8301/150000, Loss: 22383.3359375, Validation Loss: 22566.248046875\n",
      "Epoch 8401/150000, Loss: 22317.400390625, Validation Loss: 22499.912109375\n",
      "Epoch 8501/150000, Loss: 22251.943359375, Validation Loss: 22434.0546875\n",
      "Epoch 8601/150000, Loss: 22186.947265625, Validation Loss: 22368.658203125\n",
      "Epoch 8701/150000, Loss: 22122.412109375, Validation Loss: 22303.72265625\n",
      "Epoch 8801/150000, Loss: 22058.359375, Validation Loss: 22239.265625\n",
      "Epoch 8901/150000, Loss: 21994.7421875, Validation Loss: 22175.25390625\n",
      "Epoch 9001/150000, Loss: 21931.626953125, Validation Loss: 22111.734375\n",
      "Epoch 9101/150000, Loss: 21868.9453125, Validation Loss: 22048.65625\n",
      "Epoch 9201/150000, Loss: 21806.748046875, Validation Loss: 21986.0625\n",
      "Epoch 9301/150000, Loss: 21745.00390625, Validation Loss: 21923.919921875\n",
      "Epoch 9401/150000, Loss: 21683.7265625, Validation Loss: 21862.244140625\n",
      "Epoch 9501/150000, Loss: 21622.9140625, Validation Loss: 21801.033203125\n",
      "Epoch 9601/150000, Loss: 21562.5625, Validation Loss: 21740.28125\n",
      "Epoch 9701/150000, Loss: 21502.677734375, Validation Loss: 21679.99609375\n",
      "Epoch 9801/150000, Loss: 21443.244140625, Validation Loss: 21620.169921875\n",
      "Epoch 9901/150000, Loss: 21384.287109375, Validation Loss: 21560.8125\n",
      "Epoch 10001/150000, Loss: 21325.77734375, Validation Loss: 21501.90625\n",
      "Epoch 10101/150000, Loss: 21267.744140625, Validation Loss: 21443.478515625\n",
      "Epoch 10201/150000, Loss: 21210.16015625, Validation Loss: 21385.49609375\n",
      "Epoch 10301/150000, Loss: 21153.048828125, Validation Loss: 21327.984375\n",
      "Epoch 10401/150000, Loss: 21096.38671875, Validation Loss: 21270.92578125\n",
      "Epoch 10501/150000, Loss: 21040.193359375, Validation Loss: 21214.3359375\n",
      "Epoch 10601/150000, Loss: 20984.453125, Validation Loss: 21158.201171875\n",
      "Epoch 10701/150000, Loss: 20929.173828125, Validation Loss: 21102.52734375\n",
      "Epoch 10801/150000, Loss: 20874.35546875, Validation Loss: 21047.310546875\n",
      "Epoch 10901/150000, Loss: 20819.98828125, Validation Loss: 20992.55078125\n",
      "Epoch 11001/150000, Loss: 20766.09375, Validation Loss: 20938.259765625\n",
      "Epoch 11101/150000, Loss: 20712.623046875, Validation Loss: 20884.39453125\n",
      "Epoch 11201/150000, Loss: 20406.94921875, Validation Loss: 20583.71484375\n",
      "Epoch 11301/150000, Loss: 20256.611328125, Validation Loss: 20438.01171875\n",
      "Epoch 11401/150000, Loss: 20161.26171875, Validation Loss: 20345.416015625\n",
      "Epoch 11501/150000, Loss: 20085.869140625, Validation Loss: 20270.033203125\n",
      "Epoch 11601/150000, Loss: 20012.41796875, Validation Loss: 20196.4375\n",
      "Epoch 11701/150000, Loss: 19940.0546875, Validation Loss: 20123.908203125\n",
      "Epoch 11801/150000, Loss: 19868.6015625, Validation Loss: 20052.314453125\n",
      "Epoch 11901/150000, Loss: 19797.953125, Validation Loss: 19981.5546875\n",
      "Epoch 12001/150000, Loss: 19728.046875, Validation Loss: 19911.55859375\n",
      "Epoch 12101/150000, Loss: 19658.8203125, Validation Loss: 19842.26171875\n",
      "Epoch 12201/150000, Loss: 19590.232421875, Validation Loss: 19773.60546875\n",
      "Epoch 12301/150000, Loss: 19522.232421875, Validation Loss: 19705.54296875\n",
      "Epoch 12401/150000, Loss: 19454.783203125, Validation Loss: 19638.0234375\n",
      "Epoch 12501/150000, Loss: 19387.841796875, Validation Loss: 19571.005859375\n",
      "Epoch 12601/150000, Loss: 19321.38671875, Validation Loss: 19504.45703125\n",
      "Epoch 12701/150000, Loss: 19255.388671875, Validation Loss: 19438.349609375\n",
      "Epoch 12801/150000, Loss: 19189.82421875, Validation Loss: 19372.662109375\n",
      "Epoch 12901/150000, Loss: 19124.6796875, Validation Loss: 19307.3671875\n",
      "Epoch 13001/150000, Loss: 19059.93359375, Validation Loss: 19242.439453125\n",
      "Epoch 13101/150000, Loss: 18995.572265625, Validation Loss: 19177.865234375\n",
      "Epoch 13201/150000, Loss: 18931.56640625, Validation Loss: 19113.611328125\n",
      "Epoch 13301/150000, Loss: 18867.876953125, Validation Loss: 19049.560546875\n",
      "Epoch 13401/150000, Loss: 18804.455078125, Validation Loss: 18985.552734375\n",
      "Epoch 13501/150000, Loss: 18741.349609375, Validation Loss: 18921.763671875\n",
      "Epoch 13601/150000, Loss: 18678.564453125, Validation Loss: 18858.35546875\n",
      "Epoch 13701/150000, Loss: 18616.08203125, Validation Loss: 18795.31640625\n",
      "Epoch 13801/150000, Loss: 18553.89453125, Validation Loss: 18732.61328125\n",
      "Epoch 13901/150000, Loss: 18491.984375, Validation Loss: 18670.212890625\n",
      "Epoch 14001/150000, Loss: 18430.34375, Validation Loss: 18608.09375\n",
      "Epoch 14101/150000, Loss: 18368.96875, Validation Loss: 18546.234375\n",
      "Epoch 14201/150000, Loss: 18307.849609375, Validation Loss: 18484.62109375\n",
      "Epoch 14301/150000, Loss: 18246.96875, Validation Loss: 18423.240234375\n",
      "Epoch 14401/150000, Loss: 18186.330078125, Validation Loss: 18362.087890625\n",
      "Epoch 14501/150000, Loss: 18125.9375, Validation Loss: 18301.1640625\n",
      "Epoch 14601/150000, Loss: 18065.76171875, Validation Loss: 18240.4453125\n",
      "Epoch 14701/150000, Loss: 18005.82421875, Validation Loss: 18179.974609375\n",
      "Epoch 14801/150000, Loss: 17946.109375, Validation Loss: 18119.740234375\n",
      "Epoch 14901/150000, Loss: 17886.60546875, Validation Loss: 18059.728515625\n",
      "Epoch 15001/150000, Loss: 17827.34765625, Validation Loss: 17999.974609375\n",
      "Epoch 15101/150000, Loss: 17768.28515625, Validation Loss: 17940.43359375\n",
      "Epoch 15201/150000, Loss: 17709.423828125, Validation Loss: 17881.099609375\n",
      "Epoch 15301/150000, Loss: 17650.806640625, Validation Loss: 17822.02734375\n",
      "Epoch 15401/150000, Loss: 17592.39453125, Validation Loss: 17763.166015625\n",
      "Epoch 15501/150000, Loss: 17534.173828125, Validation Loss: 17704.51171875\n",
      "Epoch 15601/150000, Loss: 17476.150390625, Validation Loss: 17646.0546875\n",
      "Epoch 15701/150000, Loss: 17418.361328125, Validation Loss: 17587.837890625\n",
      "Epoch 15801/150000, Loss: 17360.78125, Validation Loss: 17529.83203125\n",
      "Epoch 15901/150000, Loss: 17303.396484375, Validation Loss: 17472.021484375\n",
      "Epoch 16001/150000, Loss: 17246.208984375, Validation Loss: 17414.408203125\n",
      "Epoch 16101/150000, Loss: 17189.23046875, Validation Loss: 17357.0\n",
      "Epoch 16201/150000, Loss: 17132.46875, Validation Loss: 17299.80859375\n",
      "Epoch 16301/150000, Loss: 17075.904296875, Validation Loss: 17242.8203125\n",
      "Epoch 16401/150000, Loss: 17019.53515625, Validation Loss: 17186.02734375\n",
      "Epoch 16501/150000, Loss: 16963.359375, Validation Loss: 17129.42578125\n",
      "Epoch 16601/150000, Loss: 16907.373046875, Validation Loss: 17073.015625\n",
      "Epoch 16701/150000, Loss: 16851.578125, Validation Loss: 17016.8046875\n",
      "Epoch 16801/150000, Loss: 16795.97265625, Validation Loss: 16960.783203125\n",
      "Epoch 16901/150000, Loss: 16740.556640625, Validation Loss: 16904.9609375\n",
      "Epoch 17001/150000, Loss: 16685.33203125, Validation Loss: 16849.33203125\n",
      "Epoch 17101/150000, Loss: 16630.296875, Validation Loss: 16793.8984375\n",
      "Epoch 17201/150000, Loss: 16575.451171875, Validation Loss: 16738.65625\n",
      "Epoch 17301/150000, Loss: 16520.791015625, Validation Loss: 16683.60546875\n",
      "Epoch 17401/150000, Loss: 16466.322265625, Validation Loss: 16628.748046875\n",
      "Epoch 17501/150000, Loss: 16412.041015625, Validation Loss: 16574.080078125\n",
      "Epoch 17601/150000, Loss: 16358.01953125, Validation Loss: 16519.671875\n",
      "Epoch 17701/150000, Loss: 16304.2177734375, Validation Loss: 16465.484375\n",
      "Epoch 17801/150000, Loss: 16250.607421875, Validation Loss: 16411.484375\n",
      "Epoch 17901/150000, Loss: 16197.1787109375, Validation Loss: 16357.6708984375\n",
      "Epoch 18001/150000, Loss: 16143.9384765625, Validation Loss: 16304.04296875\n",
      "Epoch 18101/150000, Loss: 16090.8837890625, Validation Loss: 16250.5986328125\n",
      "Epoch 18201/150000, Loss: 16038.0126953125, Validation Loss: 16197.33984375\n",
      "Epoch 18301/150000, Loss: 15985.3251953125, Validation Loss: 16144.2607421875\n",
      "Epoch 18401/150000, Loss: 15932.8212890625, Validation Loss: 16091.3642578125\n",
      "Epoch 18501/150000, Loss: 15880.4970703125, Validation Loss: 16038.6435546875\n",
      "Epoch 18601/150000, Loss: 15828.349609375, Validation Loss: 15986.0986328125\n",
      "Epoch 18701/150000, Loss: 15776.3828125, Validation Loss: 15933.748046875\n",
      "Epoch 18801/150000, Loss: 15724.6015625, Validation Loss: 15881.5986328125\n",
      "Epoch 18901/150000, Loss: 15673.00390625, Validation Loss: 15829.640625\n",
      "Epoch 19001/150000, Loss: 15621.5927734375, Validation Loss: 15777.8701171875\n",
      "Epoch 19101/150000, Loss: 15570.3662109375, Validation Loss: 15726.2841796875\n",
      "Epoch 19201/150000, Loss: 15519.3193359375, Validation Loss: 15674.8896484375\n",
      "Epoch 19301/150000, Loss: 15468.4521484375, Validation Loss: 15623.6845703125\n",
      "Epoch 19401/150000, Loss: 15417.76953125, Validation Loss: 15572.666015625\n",
      "Epoch 19501/150000, Loss: 15367.263671875, Validation Loss: 15521.83203125\n",
      "Epoch 19601/150000, Loss: 15316.9384765625, Validation Loss: 15471.1884765625\n",
      "Epoch 19701/150000, Loss: 15266.7939453125, Validation Loss: 15420.7275390625\n",
      "Epoch 19801/150000, Loss: 15216.826171875, Validation Loss: 15370.453125\n",
      "Epoch 19901/150000, Loss: 15167.0390625, Validation Loss: 15320.3603515625\n",
      "Epoch 20001/150000, Loss: 15117.4296875, Validation Loss: 15270.4521484375\n",
      "Epoch 20101/150000, Loss: 15067.998046875, Validation Loss: 15220.7197265625\n",
      "Epoch 20201/150000, Loss: 15018.7451171875, Validation Loss: 15171.1689453125\n",
      "Epoch 20301/150000, Loss: 14969.6669921875, Validation Loss: 15121.79296875\n",
      "Epoch 20401/150000, Loss: 14920.763671875, Validation Loss: 15072.5927734375\n",
      "Epoch 20501/150000, Loss: 14872.0341796875, Validation Loss: 15023.564453125\n",
      "Epoch 20601/150000, Loss: 14823.470703125, Validation Loss: 14974.703125\n",
      "Epoch 20701/150000, Loss: 14775.05078125, Validation Loss: 14926.0146484375\n",
      "Epoch 20801/150000, Loss: 14726.744140625, Validation Loss: 14877.5029296875\n",
      "Epoch 20901/150000, Loss: 14678.623046875, Validation Loss: 14829.1240234375\n",
      "Epoch 21001/150000, Loss: 14630.67578125, Validation Loss: 14780.904296875\n",
      "Epoch 21101/150000, Loss: 14582.8984375, Validation Loss: 14732.8564453125\n",
      "Epoch 21201/150000, Loss: 14535.2880859375, Validation Loss: 14684.9794921875\n",
      "Epoch 21301/150000, Loss: 14487.84375, Validation Loss: 14637.26171875\n",
      "Epoch 21401/150000, Loss: 14440.5615234375, Validation Loss: 14589.66015625\n",
      "Epoch 21501/150000, Loss: 14393.4248046875, Validation Loss: 14542.22265625\n",
      "Epoch 21601/150000, Loss: 14346.470703125, Validation Loss: 14495.0302734375\n",
      "Epoch 21701/150000, Loss: 14299.6787109375, Validation Loss: 14448.01171875\n",
      "Epoch 21801/150000, Loss: 14253.0546875, Validation Loss: 14401.1552734375\n",
      "Epoch 21901/150000, Loss: 14206.5966796875, Validation Loss: 14354.4638671875\n",
      "Epoch 22001/150000, Loss: 14160.302734375, Validation Loss: 14307.9375\n",
      "Epoch 22101/150000, Loss: 14114.1708984375, Validation Loss: 14261.5732421875\n",
      "Epoch 22201/150000, Loss: 14068.205078125, Validation Loss: 14215.3759765625\n",
      "Epoch 22301/150000, Loss: 14022.4033203125, Validation Loss: 14169.341796875\n",
      "Epoch 22401/150000, Loss: 13976.763671875, Validation Loss: 14123.466796875\n",
      "Epoch 22501/150000, Loss: 13931.287109375, Validation Loss: 14077.748046875\n",
      "Epoch 22601/150000, Loss: 13885.966796875, Validation Loss: 14032.1884765625\n",
      "Epoch 22701/150000, Loss: 13840.8095703125, Validation Loss: 13986.78125\n",
      "Epoch 22801/150000, Loss: 13795.8125, Validation Loss: 13941.5361328125\n",
      "Epoch 22901/150000, Loss: 13750.97265625, Validation Loss: 13896.447265625\n",
      "Epoch 23001/150000, Loss: 13706.2919921875, Validation Loss: 13851.51953125\n",
      "Epoch 23101/150000, Loss: 13661.767578125, Validation Loss: 13806.7578125\n",
      "Epoch 23201/150000, Loss: 13617.3994140625, Validation Loss: 13762.1484375\n",
      "Epoch 23301/150000, Loss: 13573.1865234375, Validation Loss: 13717.697265625\n",
      "Epoch 23401/150000, Loss: 13529.126953125, Validation Loss: 13673.3994140625\n",
      "Epoch 23501/150000, Loss: 13485.220703125, Validation Loss: 13629.2548828125\n",
      "Epoch 23601/150000, Loss: 13441.46875, Validation Loss: 13585.263671875\n",
      "Epoch 23701/150000, Loss: 13397.8671875, Validation Loss: 13541.4248046875\n",
      "Epoch 23801/150000, Loss: 13354.416015625, Validation Loss: 13497.7470703125\n",
      "Epoch 23901/150000, Loss: 13311.1142578125, Validation Loss: 13454.2197265625\n",
      "Epoch 24001/150000, Loss: 13267.9638671875, Validation Loss: 13410.853515625\n",
      "Epoch 24101/150000, Loss: 13224.9609375, Validation Loss: 13367.640625\n",
      "Epoch 24201/150000, Loss: 13182.109375, Validation Loss: 13324.578125\n",
      "Epoch 24301/150000, Loss: 13139.4052734375, Validation Loss: 13281.6650390625\n",
      "Epoch 24401/150000, Loss: 13096.84765625, Validation Loss: 13238.90234375\n",
      "Epoch 24501/150000, Loss: 13054.439453125, Validation Loss: 13196.2861328125\n",
      "Epoch 24601/150000, Loss: 13012.1748046875, Validation Loss: 13153.814453125\n",
      "Epoch 24701/150000, Loss: 12970.0576171875, Validation Loss: 13111.4892578125\n",
      "Epoch 24801/150000, Loss: 12928.0859375, Validation Loss: 13069.2998046875\n",
      "Epoch 24901/150000, Loss: 12886.2529296875, Validation Loss: 13027.2587890625\n",
      "Epoch 25001/150000, Loss: 12844.56640625, Validation Loss: 12985.3505859375\n",
      "Epoch 25101/150000, Loss: 12803.021484375, Validation Loss: 12943.5693359375\n",
      "Epoch 25201/150000, Loss: 12761.615234375, Validation Loss: 12901.916015625\n",
      "Epoch 25301/150000, Loss: 12720.349609375, Validation Loss: 12860.3828125\n",
      "Epoch 25401/150000, Loss: 12679.22265625, Validation Loss: 12818.9912109375\n",
      "Epoch 25501/150000, Loss: 12638.234375, Validation Loss: 12777.7578125\n",
      "Epoch 25601/150000, Loss: 12597.384765625, Validation Loss: 12736.6845703125\n",
      "Epoch 25701/150000, Loss: 12556.67578125, Validation Loss: 12695.7666015625\n",
      "Epoch 25801/150000, Loss: 12516.10546875, Validation Loss: 12654.9951171875\n",
      "Epoch 25901/150000, Loss: 12475.66796875, Validation Loss: 12614.4580078125\n",
      "Epoch 26001/150000, Loss: 12435.373046875, Validation Loss: 12573.9892578125\n",
      "Epoch 26101/150000, Loss: 12395.2138671875, Validation Loss: 12533.6494140625\n",
      "Epoch 26201/150000, Loss: 12355.19140625, Validation Loss: 12493.4423828125\n",
      "Epoch 26301/150000, Loss: 12315.3056640625, Validation Loss: 12453.3681640625\n",
      "Epoch 26401/150000, Loss: 12275.5546875, Validation Loss: 12413.4423828125\n",
      "Epoch 26501/150000, Loss: 12235.939453125, Validation Loss: 12373.650390625\n",
      "Epoch 26601/150000, Loss: 12196.4541015625, Validation Loss: 12333.9921875\n",
      "Epoch 26701/150000, Loss: 12157.1005859375, Validation Loss: 12294.47265625\n",
      "Epoch 26801/150000, Loss: 12117.8798828125, Validation Loss: 12255.0908203125\n",
      "Epoch 26901/150000, Loss: 12078.7880859375, Validation Loss: 12215.841796875\n",
      "Epoch 27001/150000, Loss: 12039.8251953125, Validation Loss: 12176.7294921875\n",
      "Epoch 27101/150000, Loss: 12000.9892578125, Validation Loss: 12137.748046875\n",
      "Epoch 27201/150000, Loss: 11962.2451171875, Validation Loss: 12098.8603515625\n",
      "Epoch 27301/150000, Loss: 11923.5927734375, Validation Loss: 12060.0693359375\n",
      "Epoch 27401/150000, Loss: 11885.064453125, Validation Loss: 12021.40625\n",
      "Epoch 27501/150000, Loss: 11846.6650390625, Validation Loss: 11982.8701171875\n",
      "Epoch 27601/150000, Loss: 11808.390625, Validation Loss: 11944.4677734375\n",
      "Epoch 27701/150000, Loss: 11770.2421875, Validation Loss: 11906.1904296875\n",
      "Epoch 27801/150000, Loss: 11732.220703125, Validation Loss: 11868.033203125\n",
      "Epoch 27901/150000, Loss: 11694.322265625, Validation Loss: 11830.0146484375\n",
      "Epoch 28001/150000, Loss: 11656.55078125, Validation Loss: 11792.1171875\n",
      "Epoch 28101/150000, Loss: 11618.9033203125, Validation Loss: 11754.341796875\n",
      "Epoch 28201/150000, Loss: 11581.37890625, Validation Loss: 11716.6962890625\n",
      "Epoch 28301/150000, Loss: 11543.9775390625, Validation Loss: 11679.162109375\n",
      "Epoch 28401/150000, Loss: 11506.69921875, Validation Loss: 11641.7568359375\n",
      "Epoch 28501/150000, Loss: 11469.5439453125, Validation Loss: 11604.466796875\n",
      "Epoch 28601/150000, Loss: 11432.5107421875, Validation Loss: 11567.2978515625\n",
      "Epoch 28701/150000, Loss: 11395.599609375, Validation Loss: 11530.2490234375\n",
      "Epoch 28801/150000, Loss: 11358.8095703125, Validation Loss: 11493.3212890625\n",
      "Epoch 28901/150000, Loss: 11322.140625, Validation Loss: 11456.5107421875\n",
      "Epoch 29001/150000, Loss: 11285.591796875, Validation Loss: 11419.826171875\n",
      "Epoch 29101/150000, Loss: 11249.1669921875, Validation Loss: 11383.259765625\n",
      "Epoch 29201/150000, Loss: 11212.8623046875, Validation Loss: 11346.80859375\n",
      "Epoch 29301/150000, Loss: 11176.67578125, Validation Loss: 11310.47265625\n",
      "Epoch 29401/150000, Loss: 11140.6083984375, Validation Loss: 11274.25\n",
      "Epoch 29501/150000, Loss: 11104.6630859375, Validation Loss: 11238.1484375\n",
      "Epoch 29601/150000, Loss: 11068.8310546875, Validation Loss: 11202.1611328125\n",
      "Epoch 29701/150000, Loss: 11033.1201171875, Validation Loss: 11166.29296875\n",
      "Epoch 29801/150000, Loss: 10997.5244140625, Validation Loss: 11130.5361328125\n",
      "Epoch 29901/150000, Loss: 10962.0478515625, Validation Loss: 11094.8984375\n",
      "Epoch 30001/150000, Loss: 10926.68359375, Validation Loss: 11059.373046875\n",
      "Epoch 30101/150000, Loss: 10891.435546875, Validation Loss: 11023.9599609375\n",
      "Epoch 30201/150000, Loss: 10856.3017578125, Validation Loss: 10988.6640625\n",
      "Epoch 30301/150000, Loss: 10821.28125, Validation Loss: 10953.478515625\n",
      "Epoch 30401/150000, Loss: 10786.373046875, Validation Loss: 10918.3955078125\n",
      "Epoch 30501/150000, Loss: 10751.5771484375, Validation Loss: 10883.4423828125\n",
      "Epoch 30601/150000, Loss: 10716.8916015625, Validation Loss: 10848.5927734375\n",
      "Epoch 30701/150000, Loss: 10682.318359375, Validation Loss: 10813.8564453125\n",
      "Epoch 30801/150000, Loss: 10647.853515625, Validation Loss: 10779.2294921875\n",
      "Epoch 30901/150000, Loss: 10613.4990234375, Validation Loss: 10744.71484375\n",
      "Epoch 31001/150000, Loss: 10579.2529296875, Validation Loss: 10710.302734375\n",
      "Epoch 31101/150000, Loss: 10545.11328125, Validation Loss: 10676.0205078125\n",
      "Epoch 31201/150000, Loss: 10511.083984375, Validation Loss: 10641.841796875\n",
      "Epoch 31301/150000, Loss: 10477.16015625, Validation Loss: 10607.7705078125\n",
      "Epoch 31401/150000, Loss: 10443.3427734375, Validation Loss: 10573.8115234375\n",
      "Epoch 31501/150000, Loss: 10409.6318359375, Validation Loss: 10539.962890625\n",
      "Epoch 31601/150000, Loss: 10376.02734375, Validation Loss: 10506.2216796875\n",
      "Epoch 31701/150000, Loss: 10342.5263671875, Validation Loss: 10472.5947265625\n",
      "Epoch 31801/150000, Loss: 10309.130859375, Validation Loss: 10439.068359375\n",
      "Epoch 31901/150000, Loss: 10275.8271484375, Validation Loss: 10405.6494140625\n",
      "Epoch 32001/150000, Loss: 10242.5927734375, Validation Loss: 10372.294921875\n",
      "Epoch 32101/150000, Loss: 10209.4443359375, Validation Loss: 10339.03125\n",
      "Epoch 32201/150000, Loss: 10176.3984375, Validation Loss: 10305.8759765625\n",
      "Epoch 32301/150000, Loss: 10143.45703125, Validation Loss: 10272.826171875\n",
      "Epoch 32401/150000, Loss: 10110.6162109375, Validation Loss: 10239.8828125\n",
      "Epoch 32501/150000, Loss: 10077.87890625, Validation Loss: 10207.044921875\n",
      "Epoch 32601/150000, Loss: 10045.2412109375, Validation Loss: 10174.3125\n",
      "Epoch 32701/150000, Loss: 10012.7060546875, Validation Loss: 10141.685546875\n",
      "Epoch 32801/150000, Loss: 9980.271484375, Validation Loss: 10109.1630859375\n",
      "Epoch 32901/150000, Loss: 9947.9345703125, Validation Loss: 10076.740234375\n",
      "Epoch 33001/150000, Loss: 9915.6982421875, Validation Loss: 10044.4228515625\n",
      "Epoch 33101/150000, Loss: 9883.560546875, Validation Loss: 10012.2041015625\n",
      "Epoch 33201/150000, Loss: 9851.5185546875, Validation Loss: 9980.0908203125\n",
      "Epoch 33301/150000, Loss: 9819.578125, Validation Loss: 9948.0771484375\n",
      "Epoch 33401/150000, Loss: 9787.7314453125, Validation Loss: 9916.1708984375\n",
      "Epoch 33501/150000, Loss: 9755.9833984375, Validation Loss: 9884.359375\n",
      "Epoch 33601/150000, Loss: 9724.3330078125, Validation Loss: 9852.646484375\n",
      "Epoch 33701/150000, Loss: 9692.7783203125, Validation Loss: 9821.033203125\n",
      "Epoch 33801/150000, Loss: 9661.3212890625, Validation Loss: 9789.517578125\n",
      "Epoch 33901/150000, Loss: 9629.958984375, Validation Loss: 9758.1044921875\n",
      "Epoch 34001/150000, Loss: 9598.69140625, Validation Loss: 9726.787109375\n",
      "Epoch 34101/150000, Loss: 9567.51953125, Validation Loss: 9695.572265625\n",
      "Epoch 34201/150000, Loss: 9536.44140625, Validation Loss: 9664.453125\n",
      "Epoch 34301/150000, Loss: 9505.4560546875, Validation Loss: 9633.435546875\n",
      "Epoch 34401/150000, Loss: 9474.56640625, Validation Loss: 9602.4990234375\n",
      "Epoch 34501/150000, Loss: 9443.767578125, Validation Loss: 9571.693359375\n",
      "Epoch 34601/150000, Loss: 9413.0625, Validation Loss: 9540.9697265625\n",
      "Epoch 34701/150000, Loss: 9382.4482421875, Validation Loss: 9510.33203125\n",
      "Epoch 34801/150000, Loss: 9351.923828125, Validation Loss: 9479.794921875\n",
      "Epoch 34901/150000, Loss: 9321.4912109375, Validation Loss: 9449.345703125\n",
      "Epoch 35001/150000, Loss: 9291.1484375, Validation Loss: 9418.9892578125\n",
      "Epoch 35101/150000, Loss: 9260.892578125, Validation Loss: 9388.7236328125\n",
      "Epoch 35201/150000, Loss: 9230.7265625, Validation Loss: 9358.546875\n",
      "Epoch 35301/150000, Loss: 9200.6484375, Validation Loss: 9328.4599609375\n",
      "Epoch 35401/150000, Loss: 9170.65625, Validation Loss: 9298.46875\n",
      "Epoch 35501/150000, Loss: 9140.7529296875, Validation Loss: 9268.5654296875\n",
      "Epoch 35601/150000, Loss: 9110.9345703125, Validation Loss: 9238.751953125\n",
      "Epoch 35701/150000, Loss: 9081.2001953125, Validation Loss: 9209.0302734375\n",
      "Epoch 35801/150000, Loss: 9051.548828125, Validation Loss: 9179.3994140625\n",
      "Epoch 35901/150000, Loss: 9021.98046875, Validation Loss: 9149.857421875\n",
      "Epoch 36001/150000, Loss: 8992.4921875, Validation Loss: 9120.37109375\n",
      "Epoch 36101/150000, Loss: 8963.0810546875, Validation Loss: 9090.9443359375\n",
      "Epoch 36201/150000, Loss: 8933.6455078125, Validation Loss: 9061.4892578125\n",
      "Epoch 36301/150000, Loss: 8904.287109375, Validation Loss: 9032.107421875\n",
      "Epoch 36401/150000, Loss: 8875.0126953125, Validation Loss: 9002.8017578125\n",
      "Epoch 36501/150000, Loss: 8845.8173828125, Validation Loss: 8973.564453125\n",
      "Epoch 36601/150000, Loss: 8816.7021484375, Validation Loss: 8944.41015625\n",
      "Epoch 36701/150000, Loss: 8787.666015625, Validation Loss: 8915.3046875\n",
      "Epoch 36801/150000, Loss: 8758.7080078125, Validation Loss: 8886.27734375\n",
      "Epoch 36901/150000, Loss: 8729.830078125, Validation Loss: 8857.3359375\n",
      "Epoch 37001/150000, Loss: 8701.0302734375, Validation Loss: 8828.4716796875\n",
      "Epoch 37101/150000, Loss: 8672.30859375, Validation Loss: 8799.6953125\n",
      "Epoch 37201/150000, Loss: 8643.6640625, Validation Loss: 8770.994140625\n",
      "Epoch 37301/150000, Loss: 8615.099609375, Validation Loss: 8742.3720703125\n",
      "Epoch 37401/150000, Loss: 8586.611328125, Validation Loss: 8713.806640625\n",
      "Epoch 37501/150000, Loss: 8558.2021484375, Validation Loss: 8685.3154296875\n",
      "Epoch 37601/150000, Loss: 8529.869140625, Validation Loss: 8656.8955078125\n",
      "Epoch 37701/150000, Loss: 8501.615234375, Validation Loss: 8628.5556640625\n",
      "Epoch 37801/150000, Loss: 8473.435546875, Validation Loss: 8600.2880859375\n",
      "Epoch 37901/150000, Loss: 8445.333984375, Validation Loss: 8572.0986328125\n",
      "Epoch 38001/150000, Loss: 8417.3076171875, Validation Loss: 8543.9853515625\n",
      "Epoch 38101/150000, Loss: 8389.3564453125, Validation Loss: 8515.9501953125\n",
      "Epoch 38201/150000, Loss: 8361.48046875, Validation Loss: 8487.9833984375\n",
      "Epoch 38301/150000, Loss: 8333.6787109375, Validation Loss: 8460.0947265625\n",
      "Epoch 38401/150000, Loss: 8305.9521484375, Validation Loss: 8432.2724609375\n",
      "Epoch 38501/150000, Loss: 8278.30078125, Validation Loss: 8404.5146484375\n",
      "Epoch 38601/150000, Loss: 8250.720703125, Validation Loss: 8376.8427734375\n",
      "Epoch 38701/150000, Loss: 8223.2158203125, Validation Loss: 8349.2314453125\n",
      "Epoch 38801/150000, Loss: 8195.7822265625, Validation Loss: 8321.701171875\n",
      "Epoch 38901/150000, Loss: 8168.42041015625, Validation Loss: 8294.2294921875\n",
      "Epoch 39001/150000, Loss: 8141.13134765625, Validation Loss: 8266.841796875\n",
      "Epoch 39101/150000, Loss: 8113.91259765625, Validation Loss: 8239.5234375\n",
      "Epoch 39201/150000, Loss: 8086.76513671875, Validation Loss: 8212.26953125\n",
      "Epoch 39301/150000, Loss: 8059.68896484375, Validation Loss: 8185.09130859375\n",
      "Epoch 39401/150000, Loss: 8032.68212890625, Validation Loss: 8157.99560546875\n",
      "Epoch 39501/150000, Loss: 8005.74658203125, Validation Loss: 8130.97607421875\n",
      "Epoch 39601/150000, Loss: 7978.88134765625, Validation Loss: 8104.03125\n",
      "Epoch 39701/150000, Loss: 7952.08447265625, Validation Loss: 8077.16748046875\n",
      "Epoch 39801/150000, Loss: 7925.35791015625, Validation Loss: 8050.380859375\n",
      "Epoch 39901/150000, Loss: 7898.70166015625, Validation Loss: 8023.673828125\n",
      "Epoch 40001/150000, Loss: 7872.1142578125, Validation Loss: 7997.0439453125\n",
      "Epoch 40101/150000, Loss: 7845.59619140625, Validation Loss: 7970.4931640625\n",
      "Epoch 40201/150000, Loss: 7819.1474609375, Validation Loss: 7944.0166015625\n",
      "Epoch 40301/150000, Loss: 7792.7666015625, Validation Loss: 7917.62255859375\n",
      "Epoch 40401/150000, Loss: 7766.45361328125, Validation Loss: 7891.30908203125\n",
      "Epoch 40501/150000, Loss: 7740.2099609375, Validation Loss: 7865.05712890625\n",
      "Epoch 40601/150000, Loss: 7714.033203125, Validation Loss: 7838.89013671875\n",
      "Epoch 40701/150000, Loss: 7687.92529296875, Validation Loss: 7812.78955078125\n",
      "Epoch 40801/150000, Loss: 7661.8837890625, Validation Loss: 7786.7626953125\n",
      "Epoch 40901/150000, Loss: 7635.91015625, Validation Loss: 7760.8076171875\n",
      "Epoch 41001/150000, Loss: 7610.001953125, Validation Loss: 7734.9169921875\n",
      "Epoch 41101/150000, Loss: 7584.16162109375, Validation Loss: 7709.08984375\n",
      "Epoch 41201/150000, Loss: 7558.38525390625, Validation Loss: 7683.31494140625\n",
      "Epoch 41301/150000, Loss: 7532.6728515625, Validation Loss: 7657.56591796875\n",
      "Epoch 41401/150000, Loss: 7507.01416015625, Validation Loss: 7631.6884765625\n",
      "Epoch 41501/150000, Loss: 7481.421875, Validation Loss: 7605.9033203125\n",
      "Epoch 41601/150000, Loss: 7455.9052734375, Validation Loss: 7580.3564453125\n",
      "Epoch 41701/150000, Loss: 7430.4580078125, Validation Loss: 7554.94677734375\n",
      "Epoch 41801/150000, Loss: 7405.07568359375, Validation Loss: 7529.564453125\n",
      "Epoch 41901/150000, Loss: 7379.76171875, Validation Loss: 7504.2509765625\n",
      "Epoch 42001/150000, Loss: 7354.51220703125, Validation Loss: 7479.025390625\n",
      "Epoch 42101/150000, Loss: 7329.3271484375, Validation Loss: 7453.86669921875\n",
      "Epoch 42201/150000, Loss: 7304.20654296875, Validation Loss: 7428.72607421875\n",
      "Epoch 42301/150000, Loss: 7279.150390625, Validation Loss: 7403.67529296875\n",
      "Epoch 42401/150000, Loss: 7254.15576171875, Validation Loss: 7378.66845703125\n",
      "Epoch 42501/150000, Loss: 7229.22509765625, Validation Loss: 7353.7333984375\n",
      "Epoch 42601/150000, Loss: 7204.35791015625, Validation Loss: 7328.85107421875\n",
      "Epoch 42701/150000, Loss: 7179.5517578125, Validation Loss: 7304.0322265625\n",
      "Epoch 42801/150000, Loss: 7154.80908203125, Validation Loss: 7279.2724609375\n",
      "Epoch 42901/150000, Loss: 7130.12744140625, Validation Loss: 7254.57470703125\n",
      "Epoch 43001/150000, Loss: 7105.50537109375, Validation Loss: 7229.9462890625\n",
      "Epoch 43101/150000, Loss: 7080.93701171875, Validation Loss: 7205.3291015625\n",
      "Epoch 43201/150000, Loss: 7056.4365234375, Validation Loss: 7180.78662109375\n",
      "Epoch 43301/150000, Loss: 7031.99853515625, Validation Loss: 7156.31298828125\n",
      "Epoch 43401/150000, Loss: 7007.61474609375, Validation Loss: 7131.8935546875\n",
      "Epoch 43501/150000, Loss: 6983.28173828125, Validation Loss: 7107.52001953125\n",
      "Epoch 43601/150000, Loss: 6959.01025390625, Validation Loss: 7083.22314453125\n",
      "Epoch 43701/150000, Loss: 6934.796875, Validation Loss: 7058.97900390625\n",
      "Epoch 43801/150000, Loss: 6910.64794921875, Validation Loss: 7034.798828125\n",
      "Epoch 43901/150000, Loss: 6886.5556640625, Validation Loss: 7010.67236328125\n",
      "Epoch 44001/150000, Loss: 6862.5283203125, Validation Loss: 6986.62255859375\n",
      "Epoch 44101/150000, Loss: 6838.55908203125, Validation Loss: 6962.62109375\n",
      "Epoch 44201/150000, Loss: 6814.65380859375, Validation Loss: 6938.6923828125\n",
      "Epoch 44301/150000, Loss: 6790.80712890625, Validation Loss: 6914.81298828125\n",
      "Epoch 44401/150000, Loss: 6767.0224609375, Validation Loss: 6891.00537109375\n",
      "Epoch 44501/150000, Loss: 6743.29736328125, Validation Loss: 6867.27099609375\n",
      "Epoch 44601/150000, Loss: 6719.63330078125, Validation Loss: 6843.5537109375\n",
      "Epoch 44701/150000, Loss: 6696.02783203125, Validation Loss: 6819.91162109375\n",
      "Epoch 44801/150000, Loss: 6672.48486328125, Validation Loss: 6796.3271484375\n",
      "Epoch 44901/150000, Loss: 6649.015625, Validation Loss: 6772.791015625\n",
      "Epoch 45001/150000, Loss: 6625.60107421875, Validation Loss: 6749.23779296875\n",
      "Epoch 45101/150000, Loss: 6602.17333984375, Validation Loss: 6724.8232421875\n",
      "Epoch 45201/150000, Loss: 6578.677734375, Validation Loss: 6700.00390625\n",
      "Epoch 45301/150000, Loss: 6555.330078125, Validation Loss: 6676.4267578125\n",
      "Epoch 45401/150000, Loss: 6532.068359375, Validation Loss: 6653.05712890625\n",
      "Epoch 45501/150000, Loss: 6508.8720703125, Validation Loss: 6629.77685546875\n",
      "Epoch 45601/150000, Loss: 6485.759765625, Validation Loss: 6606.57861328125\n",
      "Epoch 45701/150000, Loss: 6462.74853515625, Validation Loss: 6583.52978515625\n",
      "Epoch 45801/150000, Loss: 6439.802734375, Validation Loss: 6560.56005859375\n",
      "Epoch 45901/150000, Loss: 6416.919921875, Validation Loss: 6537.6669921875\n",
      "Epoch 46001/150000, Loss: 6394.0947265625, Validation Loss: 6514.79052734375\n",
      "Epoch 46101/150000, Loss: 6371.330078125, Validation Loss: 6491.96630859375\n",
      "Epoch 46201/150000, Loss: 6348.6220703125, Validation Loss: 6469.25146484375\n",
      "Epoch 46301/150000, Loss: 6325.9716796875, Validation Loss: 6446.56884765625\n",
      "Epoch 46401/150000, Loss: 6303.3818359375, Validation Loss: 6423.94775390625\n",
      "Epoch 46501/150000, Loss: 6280.8466796875, Validation Loss: 6401.38037109375\n",
      "Epoch 46601/150000, Loss: 6258.36962890625, Validation Loss: 6378.89501953125\n",
      "Epoch 46701/150000, Loss: 6235.94921875, Validation Loss: 6356.43212890625\n",
      "Epoch 46801/150000, Loss: 6213.58447265625, Validation Loss: 6334.0673828125\n",
      "Epoch 46901/150000, Loss: 6191.27392578125, Validation Loss: 6311.89892578125\n",
      "Epoch 47001/150000, Loss: 6169.01806640625, Validation Loss: 6289.875\n",
      "Epoch 47101/150000, Loss: 6146.82568359375, Validation Loss: 6267.70263671875\n",
      "Epoch 47201/150000, Loss: 6124.689453125, Validation Loss: 6245.56640625\n",
      "Epoch 47301/150000, Loss: 6102.61181640625, Validation Loss: 6223.49658203125\n",
      "Epoch 47401/150000, Loss: 6080.59033203125, Validation Loss: 6201.48046875\n",
      "Epoch 47501/150000, Loss: 6058.6259765625, Validation Loss: 6179.521484375\n",
      "Epoch 47601/150000, Loss: 6036.71923828125, Validation Loss: 6157.6181640625\n",
      "Epoch 47701/150000, Loss: 6014.86767578125, Validation Loss: 6135.77099609375\n",
      "Epoch 47801/150000, Loss: 5993.07470703125, Validation Loss: 6113.9716796875\n",
      "Epoch 47901/150000, Loss: 5971.3369140625, Validation Loss: 6092.216796875\n",
      "Epoch 48001/150000, Loss: 5949.65478515625, Validation Loss: 6070.49755859375\n",
      "Epoch 48101/150000, Loss: 5928.02978515625, Validation Loss: 6048.84521484375\n",
      "Epoch 48201/150000, Loss: 5906.46240234375, Validation Loss: 6027.25732421875\n",
      "Epoch 48301/150000, Loss: 5884.951171875, Validation Loss: 6005.7216796875\n",
      "Epoch 48401/150000, Loss: 5863.49658203125, Validation Loss: 5984.2490234375\n",
      "Epoch 48501/150000, Loss: 5842.09765625, Validation Loss: 5962.8388671875\n",
      "Epoch 48601/150000, Loss: 5820.75537109375, Validation Loss: 5941.4814453125\n",
      "Epoch 48701/150000, Loss: 5799.46923828125, Validation Loss: 5920.17236328125\n",
      "Epoch 48801/150000, Loss: 5778.240234375, Validation Loss: 5898.94482421875\n",
      "Epoch 48901/150000, Loss: 5757.06787109375, Validation Loss: 5877.8037109375\n",
      "Epoch 49001/150000, Loss: 5735.9482421875, Validation Loss: 5856.63720703125\n",
      "Epoch 49101/150000, Loss: 5714.88720703125, Validation Loss: 5835.5693359375\n",
      "Epoch 49201/150000, Loss: 5693.8818359375, Validation Loss: 5814.55908203125\n",
      "Epoch 49301/150000, Loss: 5672.93115234375, Validation Loss: 5793.6064453125\n",
      "Epoch 49401/150000, Loss: 5652.037109375, Validation Loss: 5772.7099609375\n",
      "Epoch 49501/150000, Loss: 5631.19970703125, Validation Loss: 5751.87109375\n",
      "Epoch 49601/150000, Loss: 5610.41748046875, Validation Loss: 5731.09033203125\n",
      "Epoch 49701/150000, Loss: 5589.6904296875, Validation Loss: 5710.365234375\n",
      "Epoch 49801/150000, Loss: 5569.01953125, Validation Loss: 5689.6982421875\n",
      "Epoch 49901/150000, Loss: 5548.404296875, Validation Loss: 5669.08837890625\n",
      "Epoch 50001/150000, Loss: 5527.8447265625, Validation Loss: 5648.515625\n",
      "Epoch 50101/150000, Loss: 5507.34033203125, Validation Loss: 5628.0361328125\n",
      "Epoch 50201/150000, Loss: 5486.89111328125, Validation Loss: 5607.59228515625\n",
      "Epoch 50301/150000, Loss: 5466.498046875, Validation Loss: 5587.21337890625\n",
      "Epoch 50401/150000, Loss: 5446.15966796875, Validation Loss: 5566.88427734375\n",
      "Epoch 50501/150000, Loss: 5425.87646484375, Validation Loss: 5546.6171875\n",
      "Epoch 50601/150000, Loss: 5405.6484375, Validation Loss: 5526.40234375\n",
      "Epoch 50701/150000, Loss: 5385.47607421875, Validation Loss: 5506.248046875\n",
      "Epoch 50801/150000, Loss: 5365.357421875, Validation Loss: 5486.146484375\n",
      "Epoch 50901/150000, Loss: 5345.29443359375, Validation Loss: 5466.099609375\n",
      "Epoch 51001/150000, Loss: 5325.28515625, Validation Loss: 5446.115234375\n",
      "Epoch 51101/150000, Loss: 5305.333984375, Validation Loss: 5426.12890625\n",
      "Epoch 51201/150000, Loss: 5285.43212890625, Validation Loss: 5406.30029296875\n",
      "Epoch 51301/150000, Loss: 5265.5869140625, Validation Loss: 5386.46923828125\n",
      "Epoch 51401/150000, Loss: 5245.796875, Validation Loss: 5366.7041015625\n",
      "Epoch 51501/150000, Loss: 5226.060546875, Validation Loss: 5346.984375\n",
      "Epoch 51601/150000, Loss: 5206.37841796875, Validation Loss: 5327.3271484375\n",
      "Epoch 51701/150000, Loss: 5186.7509765625, Validation Loss: 5307.70263671875\n",
      "Epoch 51801/150000, Loss: 5167.17578125, Validation Loss: 5288.1748046875\n",
      "Epoch 51901/150000, Loss: 5147.6552734375, Validation Loss: 5268.68359375\n",
      "Epoch 52001/150000, Loss: 5128.18798828125, Validation Loss: 5249.25390625\n",
      "Epoch 52101/150000, Loss: 5108.7744140625, Validation Loss: 5229.8720703125\n",
      "Epoch 52201/150000, Loss: 5089.4140625, Validation Loss: 5210.5439453125\n",
      "Epoch 52301/150000, Loss: 5070.107421875, Validation Loss: 5191.28759765625\n",
      "Epoch 52401/150000, Loss: 5050.8525390625, Validation Loss: 5172.08154296875\n",
      "Epoch 52501/150000, Loss: 5031.650390625, Validation Loss: 5152.9296875\n",
      "Epoch 52601/150000, Loss: 5012.5009765625, Validation Loss: 5133.83984375\n",
      "Epoch 52701/150000, Loss: 4993.40380859375, Validation Loss: 5114.7958984375\n",
      "Epoch 52801/150000, Loss: 4974.3583984375, Validation Loss: 5095.81298828125\n",
      "Epoch 52901/150000, Loss: 4955.36474609375, Validation Loss: 5076.88916015625\n",
      "Epoch 53001/150000, Loss: 4936.4228515625, Validation Loss: 5058.00390625\n",
      "Epoch 53101/150000, Loss: 4917.53271484375, Validation Loss: 5039.2060546875\n",
      "Epoch 53201/150000, Loss: 4898.6943359375, Validation Loss: 5020.4140625\n",
      "Epoch 53301/150000, Loss: 4879.90576171875, Validation Loss: 5001.68212890625\n",
      "Epoch 53401/150000, Loss: 4861.16943359375, Validation Loss: 4983.03515625\n",
      "Epoch 53501/150000, Loss: 4842.4833984375, Validation Loss: 4964.44482421875\n",
      "Epoch 53601/150000, Loss: 4823.84716796875, Validation Loss: 4945.86767578125\n",
      "Epoch 53701/150000, Loss: 4805.26171875, Validation Loss: 4927.345703125\n",
      "Epoch 53801/150000, Loss: 4786.72607421875, Validation Loss: 4908.91064453125\n",
      "Epoch 53901/150000, Loss: 4768.24072265625, Validation Loss: 4890.50927734375\n",
      "Epoch 54001/150000, Loss: 4749.8056640625, Validation Loss: 4872.16162109375\n",
      "Epoch 54101/150000, Loss: 4731.419921875, Validation Loss: 4853.859375\n",
      "Epoch 54201/150000, Loss: 4713.0849609375, Validation Loss: 4835.63720703125\n",
      "Epoch 54301/150000, Loss: 4694.796875, Validation Loss: 4817.40966796875\n",
      "Epoch 54401/150000, Loss: 4676.55859375, Validation Loss: 4799.23486328125\n",
      "Epoch 54501/150000, Loss: 4658.36767578125, Validation Loss: 4781.1513671875\n",
      "Epoch 54601/150000, Loss: 4640.22998046875, Validation Loss: 4763.1650390625\n",
      "Epoch 54701/150000, Loss: 4622.12646484375, Validation Loss: 4745.07666015625\n",
      "Epoch 54801/150000, Loss: 4604.07275390625, Validation Loss: 4727.11083984375\n",
      "Epoch 54901/150000, Loss: 4586.0625, Validation Loss: 4709.2060546875\n",
      "Epoch 55001/150000, Loss: 4568.10205078125, Validation Loss: 4691.37451171875\n",
      "Epoch 55101/150000, Loss: 4550.19384765625, Validation Loss: 4673.61767578125\n",
      "Epoch 55201/150000, Loss: 4532.3408203125, Validation Loss: 4655.9208984375\n",
      "Epoch 55301/150000, Loss: 4514.5390625, Validation Loss: 4638.27490234375\n",
      "Epoch 55401/150000, Loss: 4496.78857421875, Validation Loss: 4620.673828125\n",
      "Epoch 55501/150000, Loss: 4479.087890625, Validation Loss: 4603.11962890625\n",
      "Epoch 55601/150000, Loss: 4461.43603515625, Validation Loss: 4585.61181640625\n",
      "Epoch 55701/150000, Loss: 4443.83349609375, Validation Loss: 4568.1611328125\n",
      "Epoch 55801/150000, Loss: 4426.279296875, Validation Loss: 4550.7255859375\n",
      "Epoch 55901/150000, Loss: 4408.7734375, Validation Loss: 4533.35546875\n",
      "Epoch 56001/150000, Loss: 4391.31640625, Validation Loss: 4516.02490234375\n",
      "Epoch 56101/150000, Loss: 4373.90771484375, Validation Loss: 4498.74462890625\n",
      "Epoch 56201/150000, Loss: 4356.5478515625, Validation Loss: 4481.509765625\n",
      "Epoch 56301/150000, Loss: 4339.23583984375, Validation Loss: 4464.2958984375\n",
      "Epoch 56401/150000, Loss: 4321.97216796875, Validation Loss: 4447.17529296875\n",
      "Epoch 56501/150000, Loss: 4304.75732421875, Validation Loss: 4430.10888671875\n",
      "Epoch 56601/150000, Loss: 4287.58984375, Validation Loss: 4413.029296875\n",
      "Epoch 56701/150000, Loss: 4270.4697265625, Validation Loss: 4396.025390625\n",
      "Epoch 56801/150000, Loss: 4253.39990234375, Validation Loss: 4379.07275390625\n",
      "Epoch 56901/150000, Loss: 4236.37744140625, Validation Loss: 4362.15673828125\n",
      "Epoch 57001/150000, Loss: 4219.40185546875, Validation Loss: 4345.310546875\n",
      "Epoch 57101/150000, Loss: 4202.474609375, Validation Loss: 4328.474609375\n",
      "Epoch 57201/150000, Loss: 4185.5966796875, Validation Loss: 4311.71875\n",
      "Epoch 57301/150000, Loss: 4168.765625, Validation Loss: 4294.97998046875\n",
      "Epoch 57401/150000, Loss: 4151.982421875, Validation Loss: 4278.32275390625\n",
      "Epoch 57501/150000, Loss: 4135.2470703125, Validation Loss: 4261.673828125\n",
      "Epoch 57601/150000, Loss: 4118.5595703125, Validation Loss: 4245.1064453125\n",
      "Epoch 57701/150000, Loss: 4101.91943359375, Validation Loss: 4228.55712890625\n",
      "Epoch 57801/150000, Loss: 4085.327392578125, Validation Loss: 4212.06982421875\n",
      "Epoch 57901/150000, Loss: 4068.78271484375, Validation Loss: 4195.62353515625\n",
      "Epoch 58001/150000, Loss: 4052.28564453125, Validation Loss: 4179.23779296875\n",
      "Epoch 58101/150000, Loss: 4035.836181640625, Validation Loss: 4162.8818359375\n",
      "Epoch 58201/150000, Loss: 4019.433837890625, Validation Loss: 4146.59765625\n",
      "Epoch 58301/150000, Loss: 4003.081298828125, Validation Loss: 4130.37451171875\n",
      "Epoch 58401/150000, Loss: 3986.770751953125, Validation Loss: 4114.14990234375\n",
      "Epoch 58501/150000, Loss: 3970.51123046875, Validation Loss: 4097.97021484375\n",
      "Epoch 58601/150000, Loss: 3954.2978515625, Validation Loss: 4081.89453125\n",
      "Epoch 58701/150000, Loss: 3938.131103515625, Validation Loss: 4065.844970703125\n",
      "Epoch 58801/150000, Loss: 3922.01171875, Validation Loss: 4049.83056640625\n",
      "Epoch 58901/150000, Loss: 3905.94091796875, Validation Loss: 4033.822509765625\n",
      "Epoch 59001/150000, Loss: 3889.9130859375, Validation Loss: 4017.9501953125\n",
      "Epoch 59101/150000, Loss: 3873.932861328125, Validation Loss: 4002.08447265625\n",
      "Epoch 59201/150000, Loss: 3857.999267578125, Validation Loss: 3986.251953125\n",
      "Epoch 59301/150000, Loss: 3842.112060546875, Validation Loss: 3970.493896484375\n",
      "Epoch 59401/150000, Loss: 3826.27001953125, Validation Loss: 3954.7626953125\n",
      "Epoch 59501/150000, Loss: 3810.4814453125, Validation Loss: 3939.105224609375\n",
      "Epoch 59601/150000, Loss: 3794.757568359375, Validation Loss: 3923.515380859375\n",
      "Epoch 59701/150000, Loss: 3779.11181640625, Validation Loss: 3908.017333984375\n",
      "Epoch 59801/150000, Loss: 3763.517822265625, Validation Loss: 3892.545166015625\n",
      "Epoch 59901/150000, Loss: 3747.96923828125, Validation Loss: 3877.135498046875\n",
      "Epoch 60001/150000, Loss: 3732.467041015625, Validation Loss: 3861.768310546875\n",
      "Epoch 60101/150000, Loss: 3717.01123046875, Validation Loss: 3846.451416015625\n",
      "Epoch 60201/150000, Loss: 3701.601806640625, Validation Loss: 3831.1767578125\n",
      "Epoch 60301/150000, Loss: 3686.237548828125, Validation Loss: 3815.951416015625\n",
      "Epoch 60401/150000, Loss: 3670.9228515625, Validation Loss: 3800.702392578125\n",
      "Epoch 60501/150000, Loss: 3655.646728515625, Validation Loss: 3785.6376953125\n",
      "Epoch 60601/150000, Loss: 3640.420166015625, Validation Loss: 3770.55615234375\n",
      "Epoch 60701/150000, Loss: 3625.239013671875, Validation Loss: 3755.5\n",
      "Epoch 60801/150000, Loss: 3610.103271484375, Validation Loss: 3740.50146484375\n",
      "Epoch 60901/150000, Loss: 3595.01708984375, Validation Loss: 3725.484375\n",
      "Epoch 61001/150000, Loss: 3579.96875, Validation Loss: 3710.635009765625\n",
      "Epoch 61101/150000, Loss: 3564.969482421875, Validation Loss: 3695.77587890625\n",
      "Epoch 61201/150000, Loss: 3550.016357421875, Validation Loss: 3680.947509765625\n",
      "Epoch 61301/150000, Loss: 3535.1083984375, Validation Loss: 3666.16943359375\n",
      "Epoch 61401/150000, Loss: 3520.246826171875, Validation Loss: 3651.476318359375\n",
      "Epoch 61501/150000, Loss: 3505.42822265625, Validation Loss: 3636.74658203125\n",
      "Epoch 61601/150000, Loss: 3490.65625, Validation Loss: 3622.098388671875\n",
      "Epoch 61701/150000, Loss: 3475.928955078125, Validation Loss: 3607.501953125\n",
      "Epoch 61801/150000, Loss: 3461.247314453125, Validation Loss: 3592.949951171875\n",
      "Epoch 61901/150000, Loss: 3446.610595703125, Validation Loss: 3578.4306640625\n",
      "Epoch 62001/150000, Loss: 3432.019287109375, Validation Loss: 3563.966064453125\n",
      "Epoch 62101/150000, Loss: 3417.472900390625, Validation Loss: 3549.534423828125\n",
      "Epoch 62201/150000, Loss: 3402.97119140625, Validation Loss: 3535.161376953125\n",
      "Epoch 62301/150000, Loss: 3388.51513671875, Validation Loss: 3520.820068359375\n",
      "Epoch 62401/150000, Loss: 3374.10400390625, Validation Loss: 3506.537353515625\n",
      "Epoch 62501/150000, Loss: 3359.737060546875, Validation Loss: 3492.294921875\n",
      "Epoch 62601/150000, Loss: 3345.416015625, Validation Loss: 3478.093994140625\n",
      "Epoch 62701/150000, Loss: 3331.139404296875, Validation Loss: 3463.9404296875\n",
      "Epoch 62801/150000, Loss: 3316.908203125, Validation Loss: 3449.830078125\n",
      "Epoch 62901/150000, Loss: 3302.720947265625, Validation Loss: 3435.773193359375\n",
      "Epoch 63001/150000, Loss: 3288.579345703125, Validation Loss: 3421.751953125\n",
      "Epoch 63101/150000, Loss: 3274.481689453125, Validation Loss: 3407.787841796875\n",
      "Epoch 63201/150000, Loss: 3260.428955078125, Validation Loss: 3393.86328125\n",
      "Epoch 63301/150000, Loss: 3246.4208984375, Validation Loss: 3379.982421875\n",
      "Epoch 63401/150000, Loss: 3232.457763671875, Validation Loss: 3366.15087890625\n",
      "Epoch 63501/150000, Loss: 3218.53857421875, Validation Loss: 3352.35888671875\n",
      "Epoch 63601/150000, Loss: 3204.663818359375, Validation Loss: 3338.615234375\n",
      "Epoch 63701/150000, Loss: 3190.833740234375, Validation Loss: 3324.913818359375\n",
      "Epoch 63801/150000, Loss: 3177.048095703125, Validation Loss: 3311.260498046875\n",
      "Epoch 63901/150000, Loss: 3163.30615234375, Validation Loss: 3297.6474609375\n",
      "Epoch 64001/150000, Loss: 3149.609130859375, Validation Loss: 3284.08447265625\n",
      "Epoch 64101/150000, Loss: 3135.955810546875, Validation Loss: 3270.560302734375\n",
      "Epoch 64201/150000, Loss: 3122.346435546875, Validation Loss: 3257.092041015625\n",
      "Epoch 64301/150000, Loss: 3108.781494140625, Validation Loss: 3243.651123046875\n",
      "Epoch 64401/150000, Loss: 3095.260009765625, Validation Loss: 3230.263427734375\n",
      "Epoch 64501/150000, Loss: 3081.7822265625, Validation Loss: 3216.95556640625\n",
      "Epoch 64601/150000, Loss: 3068.347900390625, Validation Loss: 3203.62158203125\n",
      "Epoch 64701/150000, Loss: 3054.956787109375, Validation Loss: 3190.359375\n",
      "Epoch 64801/150000, Loss: 3041.609619140625, Validation Loss: 3177.155029296875\n",
      "Epoch 64901/150000, Loss: 3028.3056640625, Validation Loss: 3163.95849609375\n",
      "Epoch 65001/150000, Loss: 3015.044189453125, Validation Loss: 3150.862548828125\n",
      "Epoch 65101/150000, Loss: 3001.826171875, Validation Loss: 3137.752197265625\n",
      "Epoch 65201/150000, Loss: 2988.650634765625, Validation Loss: 3124.744384765625\n",
      "Epoch 65301/150000, Loss: 2975.517822265625, Validation Loss: 3111.7470703125\n",
      "Epoch 65401/150000, Loss: 2962.427490234375, Validation Loss: 3098.7978515625\n",
      "Epoch 65501/150000, Loss: 2949.3798828125, Validation Loss: 3085.89111328125\n",
      "Epoch 65601/150000, Loss: 2936.3740234375, Validation Loss: 3073.0224609375\n",
      "Epoch 65701/150000, Loss: 2923.410400390625, Validation Loss: 3060.19482421875\n",
      "Epoch 65801/150000, Loss: 2910.48876953125, Validation Loss: 3047.41162109375\n",
      "Epoch 65901/150000, Loss: 2897.6083984375, Validation Loss: 3034.670166015625\n",
      "Epoch 66001/150000, Loss: 2884.769775390625, Validation Loss: 3021.948486328125\n",
      "Epoch 66101/150000, Loss: 2871.97216796875, Validation Loss: 3009.302978515625\n",
      "Epoch 66201/150000, Loss: 2859.2158203125, Validation Loss: 2996.674072265625\n",
      "Epoch 66301/150000, Loss: 2846.500732421875, Validation Loss: 2984.0966796875\n",
      "Epoch 66401/150000, Loss: 2833.826171875, Validation Loss: 2971.550048828125\n",
      "Epoch 66501/150000, Loss: 2821.1923828125, Validation Loss: 2959.077392578125\n",
      "Epoch 66601/150000, Loss: 2808.59912109375, Validation Loss: 2946.576416015625\n",
      "Epoch 66701/150000, Loss: 2796.046142578125, Validation Loss: 2934.142333984375\n",
      "Epoch 66801/150000, Loss: 2783.533447265625, Validation Loss: 2921.7568359375\n",
      "Epoch 66901/150000, Loss: 2771.06103515625, Validation Loss: 2909.403564453125\n",
      "Epoch 67001/150000, Loss: 2758.628173828125, Validation Loss: 2897.082275390625\n",
      "Epoch 67101/150000, Loss: 2746.236083984375, Validation Loss: 2884.8193359375\n",
      "Epoch 67201/150000, Loss: 2733.883056640625, Validation Loss: 2872.593017578125\n",
      "Epoch 67301/150000, Loss: 2721.570556640625, Validation Loss: 2860.39306640625\n",
      "Epoch 67401/150000, Loss: 2709.297607421875, Validation Loss: 2848.24609375\n",
      "Epoch 67501/150000, Loss: 2697.064453125, Validation Loss: 2836.126953125\n",
      "Epoch 67601/150000, Loss: 2684.87060546875, Validation Loss: 2824.052001953125\n",
      "Epoch 67701/150000, Loss: 2672.717041015625, Validation Loss: 2812.021240234375\n",
      "Epoch 67801/150000, Loss: 2660.602783203125, Validation Loss: 2800.02685546875\n",
      "Epoch 67901/150000, Loss: 2648.5283203125, Validation Loss: 2788.068603515625\n",
      "Epoch 68001/150000, Loss: 2636.492431640625, Validation Loss: 2776.168701171875\n",
      "Epoch 68101/150000, Loss: 2624.497314453125, Validation Loss: 2764.304931640625\n",
      "Epoch 68201/150000, Loss: 2612.540771484375, Validation Loss: 2752.470458984375\n",
      "Epoch 68301/150000, Loss: 2600.623779296875, Validation Loss: 2740.687255859375\n",
      "Epoch 68401/150000, Loss: 2588.745849609375, Validation Loss: 2728.93603515625\n",
      "Epoch 68501/150000, Loss: 2576.907958984375, Validation Loss: 2717.22607421875\n",
      "Epoch 68601/150000, Loss: 2565.108154296875, Validation Loss: 2705.564208984375\n",
      "Epoch 68701/150000, Loss: 2553.34912109375, Validation Loss: 2693.931396484375\n",
      "Epoch 68801/150000, Loss: 2541.6279296875, Validation Loss: 2682.354736328125\n",
      "Epoch 68901/150000, Loss: 2529.947021484375, Validation Loss: 2670.800048828125\n",
      "Epoch 69001/150000, Loss: 2518.3037109375, Validation Loss: 2659.306884765625\n",
      "Epoch 69101/150000, Loss: 2506.7021484375, Validation Loss: 2647.806640625\n",
      "Epoch 69201/150000, Loss: 2495.13525390625, Validation Loss: 2636.420166015625\n",
      "Epoch 69301/150000, Loss: 2483.610595703125, Validation Loss: 2624.99267578125\n",
      "Epoch 69401/150000, Loss: 2472.121826171875, Validation Loss: 2613.6953125\n",
      "Epoch 69501/150000, Loss: 2460.673828125, Validation Loss: 2602.39306640625\n",
      "Epoch 69601/150000, Loss: 2449.263916015625, Validation Loss: 2591.131591796875\n",
      "Epoch 69701/150000, Loss: 2437.89306640625, Validation Loss: 2579.90673828125\n",
      "Epoch 69801/150000, Loss: 2426.560302734375, Validation Loss: 2568.72314453125\n",
      "Epoch 69901/150000, Loss: 2415.266845703125, Validation Loss: 2557.582763671875\n",
      "Epoch 70001/150000, Loss: 2404.015625, Validation Loss: 2546.540771484375\n",
      "Epoch 70101/150000, Loss: 2392.7939453125, Validation Loss: 2535.419189453125\n",
      "Epoch 70201/150000, Loss: 2381.615234375, Validation Loss: 2524.38818359375\n",
      "Epoch 70301/150000, Loss: 2370.473388671875, Validation Loss: 2513.41552734375\n",
      "Epoch 70401/150000, Loss: 2359.3701171875, Validation Loss: 2502.4638671875\n",
      "Epoch 70501/150000, Loss: 2348.304931640625, Validation Loss: 2491.57177734375\n",
      "Epoch 70601/150000, Loss: 2337.279541015625, Validation Loss: 2480.66943359375\n",
      "Epoch 70701/150000, Loss: 2326.28759765625, Validation Loss: 2469.887939453125\n",
      "Epoch 70801/150000, Loss: 2315.33544921875, Validation Loss: 2459.088623046875\n",
      "Epoch 70901/150000, Loss: 2304.419921875, Validation Loss: 2448.361083984375\n",
      "Epoch 71001/150000, Loss: 2293.542236328125, Validation Loss: 2437.67822265625\n",
      "Epoch 71101/150000, Loss: 2282.701171875, Validation Loss: 2426.989501953125\n",
      "Epoch 71201/150000, Loss: 2271.89697265625, Validation Loss: 2416.36181640625\n",
      "Epoch 71301/150000, Loss: 2261.130126953125, Validation Loss: 2405.7705078125\n",
      "Epoch 71401/150000, Loss: 2250.399169921875, Validation Loss: 2395.22265625\n",
      "Epoch 71501/150000, Loss: 2239.70556640625, Validation Loss: 2384.708984375\n",
      "Epoch 71601/150000, Loss: 2229.04736328125, Validation Loss: 2374.2392578125\n",
      "Epoch 71701/150000, Loss: 2218.42578125, Validation Loss: 2363.8076171875\n",
      "Epoch 71801/150000, Loss: 2207.840576171875, Validation Loss: 2353.40673828125\n",
      "Epoch 71901/150000, Loss: 2197.291748046875, Validation Loss: 2343.043701171875\n",
      "Epoch 72001/150000, Loss: 2186.7783203125, Validation Loss: 2332.72314453125\n",
      "Epoch 72101/150000, Loss: 2176.30078125, Validation Loss: 2322.43701171875\n",
      "Epoch 72201/150000, Loss: 2165.858642578125, Validation Loss: 2312.18701171875\n",
      "Epoch 72301/150000, Loss: 2155.453125, Validation Loss: 2301.971923828125\n",
      "Epoch 72401/150000, Loss: 2145.08203125, Validation Loss: 2291.79931640625\n",
      "Epoch 72501/150000, Loss: 2134.747314453125, Validation Loss: 2281.677001953125\n",
      "Epoch 72601/150000, Loss: 2124.4462890625, Validation Loss: 2271.556396484375\n",
      "Epoch 72701/150000, Loss: 2114.18310546875, Validation Loss: 2261.51806640625\n",
      "Epoch 72801/150000, Loss: 2103.950439453125, Validation Loss: 2251.45703125\n",
      "Epoch 72901/150000, Loss: 2093.755615234375, Validation Loss: 2241.454833984375\n",
      "Epoch 73001/150000, Loss: 2083.5947265625, Validation Loss: 2231.49951171875\n",
      "Epoch 73101/150000, Loss: 2073.46875, Validation Loss: 2221.574951171875\n",
      "Epoch 73201/150000, Loss: 2063.376953125, Validation Loss: 2211.700439453125\n",
      "Epoch 73301/150000, Loss: 2053.3203125, Validation Loss: 2201.828857421875\n",
      "Epoch 73401/150000, Loss: 2043.2969970703125, Validation Loss: 2192.03857421875\n",
      "Epoch 73501/150000, Loss: 2033.30908203125, Validation Loss: 2182.221923828125\n",
      "Epoch 73601/150000, Loss: 2023.354736328125, Validation Loss: 2172.487060546875\n",
      "Epoch 73701/150000, Loss: 2013.4349365234375, Validation Loss: 2162.753173828125\n",
      "Epoch 73801/150000, Loss: 2003.5489501953125, Validation Loss: 2153.09033203125\n",
      "Epoch 73901/150000, Loss: 1993.697265625, Validation Loss: 2143.42138671875\n",
      "Epoch 74001/150000, Loss: 1983.879150390625, Validation Loss: 2133.808837890625\n",
      "Epoch 74101/150000, Loss: 1974.095458984375, Validation Loss: 2124.22705078125\n",
      "Epoch 74201/150000, Loss: 1964.3502197265625, Validation Loss: 2114.771484375\n",
      "Epoch 74301/150000, Loss: 1954.629638671875, Validation Loss: 2105.170166015625\n",
      "Epoch 74401/150000, Loss: 1944.9483642578125, Validation Loss: 2095.6806640625\n",
      "Epoch 74501/150000, Loss: 1935.2998046875, Validation Loss: 2086.250244140625\n",
      "Epoch 74601/150000, Loss: 1925.69580078125, Validation Loss: 2076.9501953125\n",
      "Epoch 74701/150000, Loss: 1916.10986328125, Validation Loss: 2067.468505859375\n",
      "Epoch 74801/150000, Loss: 1906.587646484375, Validation Loss: 2058.15185546875\n",
      "Epoch 74901/150000, Loss: 1897.131103515625, Validation Loss: 2048.910400390625\n",
      "Epoch 75001/150000, Loss: 1887.72412109375, Validation Loss: 2039.6959228515625\n",
      "Epoch 75101/150000, Loss: 1878.351318359375, Validation Loss: 2030.5302734375\n",
      "Epoch 75201/150000, Loss: 1869.0101318359375, Validation Loss: 2021.3868408203125\n",
      "Epoch 75301/150000, Loss: 1859.7042236328125, Validation Loss: 2012.3280029296875\n",
      "Epoch 75401/150000, Loss: 1850.428466796875, Validation Loss: 2003.2137451171875\n",
      "Epoch 75501/150000, Loss: 1841.1875, Validation Loss: 1994.175537109375\n",
      "Epoch 75601/150000, Loss: 1831.9796142578125, Validation Loss: 1985.1685791015625\n",
      "Epoch 75701/150000, Loss: 1822.8043212890625, Validation Loss: 1976.197998046875\n",
      "Epoch 75801/150000, Loss: 1813.6622314453125, Validation Loss: 1967.2574462890625\n",
      "Epoch 75901/150000, Loss: 1804.5526123046875, Validation Loss: 1958.350830078125\n",
      "Epoch 76001/150000, Loss: 1795.4759521484375, Validation Loss: 1949.477783203125\n",
      "Epoch 76101/150000, Loss: 1786.4320068359375, Validation Loss: 1940.6341552734375\n",
      "Epoch 76201/150000, Loss: 1777.4210205078125, Validation Loss: 1931.8282470703125\n",
      "Epoch 76301/150000, Loss: 1768.4427490234375, Validation Loss: 1923.047119140625\n",
      "Epoch 76401/150000, Loss: 1759.4970703125, Validation Loss: 1914.2952880859375\n",
      "Epoch 76501/150000, Loss: 1750.5841064453125, Validation Loss: 1905.5894775390625\n",
      "Epoch 76601/150000, Loss: 1741.7039794921875, Validation Loss: 1896.9185791015625\n",
      "Epoch 76701/150000, Loss: 1732.8563232421875, Validation Loss: 1888.260986328125\n",
      "Epoch 76801/150000, Loss: 1724.0433349609375, Validation Loss: 1879.7093505859375\n",
      "Epoch 76901/150000, Loss: 1715.2587890625, Validation Loss: 1871.062255859375\n",
      "Epoch 77001/150000, Loss: 1706.5101318359375, Validation Loss: 1862.563232421875\n",
      "Epoch 77101/150000, Loss: 1697.7916259765625, Validation Loss: 1853.9925537109375\n",
      "Epoch 77201/150000, Loss: 1689.106689453125, Validation Loss: 1845.50634765625\n",
      "Epoch 77301/150000, Loss: 1680.454345703125, Validation Loss: 1837.0494384765625\n",
      "Epoch 77401/150000, Loss: 1671.8341064453125, Validation Loss: 1828.6259765625\n",
      "Epoch 77501/150000, Loss: 1663.24658203125, Validation Loss: 1820.225341796875\n",
      "Epoch 77601/150000, Loss: 1654.6910400390625, Validation Loss: 1811.873291015625\n",
      "Epoch 77701/150000, Loss: 1646.1678466796875, Validation Loss: 1803.5172119140625\n",
      "Epoch 77801/150000, Loss: 1637.6768798828125, Validation Loss: 1795.24609375\n",
      "Epoch 77901/150000, Loss: 1629.218994140625, Validation Loss: 1787.0189208984375\n",
      "Epoch 78001/150000, Loss: 1620.791259765625, Validation Loss: 1778.742919921875\n",
      "Epoch 78101/150000, Loss: 1612.4014892578125, Validation Loss: 1770.5965576171875\n",
      "Epoch 78201/150000, Loss: 1604.0338134765625, Validation Loss: 1762.3699951171875\n",
      "Epoch 78301/150000, Loss: 1595.7047119140625, Validation Loss: 1754.2578125\n",
      "Epoch 78401/150000, Loss: 1587.404052734375, Validation Loss: 1746.1300048828125\n",
      "Epoch 78501/150000, Loss: 1579.1376953125, Validation Loss: 1738.0333251953125\n",
      "Epoch 78601/150000, Loss: 1570.901123046875, Validation Loss: 1730.029541015625\n",
      "Epoch 78701/150000, Loss: 1562.6982421875, Validation Loss: 1722.0748291015625\n",
      "Epoch 78801/150000, Loss: 1554.5247802734375, Validation Loss: 1714.06982421875\n",
      "Epoch 78901/150000, Loss: 1546.384521484375, Validation Loss: 1706.1790771484375\n",
      "Epoch 79001/150000, Loss: 1538.2745361328125, Validation Loss: 1698.2510986328125\n",
      "Epoch 79101/150000, Loss: 1530.2003173828125, Validation Loss: 1690.3177490234375\n",
      "Epoch 79201/150000, Loss: 1522.14990234375, Validation Loss: 1682.572021484375\n",
      "Epoch 79301/150000, Loss: 1514.1351318359375, Validation Loss: 1674.8331298828125\n",
      "Epoch 79401/150000, Loss: 1506.1494140625, Validation Loss: 1667.0333251953125\n",
      "Epoch 79501/150000, Loss: 1498.197021484375, Validation Loss: 1659.27587890625\n",
      "Epoch 79601/150000, Loss: 1490.273193359375, Validation Loss: 1651.6317138671875\n",
      "Epoch 79701/150000, Loss: 1482.38134765625, Validation Loss: 1643.9921875\n",
      "Epoch 79801/150000, Loss: 1474.52001953125, Validation Loss: 1636.36865234375\n",
      "Epoch 79901/150000, Loss: 1466.689453125, Validation Loss: 1628.792236328125\n",
      "Epoch 80001/150000, Loss: 1458.88916015625, Validation Loss: 1621.2425537109375\n",
      "Epoch 80101/150000, Loss: 1451.11962890625, Validation Loss: 1613.7264404296875\n",
      "Epoch 80201/150000, Loss: 1443.380126953125, Validation Loss: 1606.251708984375\n",
      "Epoch 80301/150000, Loss: 1435.674072265625, Validation Loss: 1598.7437744140625\n",
      "Epoch 80401/150000, Loss: 1427.991455078125, Validation Loss: 1591.3953857421875\n",
      "Epoch 80501/150000, Loss: 1420.3431396484375, Validation Loss: 1584.02587890625\n",
      "Epoch 80601/150000, Loss: 1412.7222900390625, Validation Loss: 1576.67138671875\n",
      "Epoch 80701/150000, Loss: 1405.1326904296875, Validation Loss: 1569.3521728515625\n",
      "Epoch 80801/150000, Loss: 1397.572265625, Validation Loss: 1562.0799560546875\n",
      "Epoch 80901/150000, Loss: 1390.0413818359375, Validation Loss: 1554.825439453125\n",
      "Epoch 81001/150000, Loss: 1382.5401611328125, Validation Loss: 1547.6180419921875\n",
      "Epoch 81101/150000, Loss: 1375.06787109375, Validation Loss: 1540.4384765625\n",
      "Epoch 81201/150000, Loss: 1367.6248779296875, Validation Loss: 1533.2841796875\n",
      "Epoch 81301/150000, Loss: 1360.2109375, Validation Loss: 1526.159423828125\n",
      "Epoch 81401/150000, Loss: 1352.82568359375, Validation Loss: 1519.0765380859375\n",
      "Epoch 81501/150000, Loss: 1345.469482421875, Validation Loss: 1512.0281982421875\n",
      "Epoch 81601/150000, Loss: 1338.1414794921875, Validation Loss: 1504.9949951171875\n",
      "Epoch 81701/150000, Loss: 1330.84228515625, Validation Loss: 1498.0069580078125\n",
      "Epoch 81801/150000, Loss: 1323.571044921875, Validation Loss: 1491.0419921875\n",
      "Epoch 81901/150000, Loss: 1316.3284912109375, Validation Loss: 1484.11279296875\n",
      "Epoch 82001/150000, Loss: 1309.11376953125, Validation Loss: 1477.221923828125\n",
      "Epoch 82101/150000, Loss: 1301.9278564453125, Validation Loss: 1470.3671875\n",
      "Epoch 82201/150000, Loss: 1294.7703857421875, Validation Loss: 1463.5299072265625\n",
      "Epoch 82301/150000, Loss: 1287.6416015625, Validation Loss: 1456.7391357421875\n",
      "Epoch 82401/150000, Loss: 1280.540771484375, Validation Loss: 1449.96533203125\n",
      "Epoch 82501/150000, Loss: 1273.468017578125, Validation Loss: 1443.228515625\n",
      "Epoch 82601/150000, Loss: 1266.42333984375, Validation Loss: 1436.527099609375\n",
      "Epoch 82701/150000, Loss: 1259.4063720703125, Validation Loss: 1429.8548583984375\n",
      "Epoch 82801/150000, Loss: 1252.4173583984375, Validation Loss: 1423.211669921875\n",
      "Epoch 82901/150000, Loss: 1245.4556884765625, Validation Loss: 1416.5955810546875\n",
      "Epoch 83001/150000, Loss: 1238.5216064453125, Validation Loss: 1410.0166015625\n",
      "Epoch 83101/150000, Loss: 1231.6151123046875, Validation Loss: 1403.464111328125\n",
      "Epoch 83201/150000, Loss: 1224.7359619140625, Validation Loss: 1396.9449462890625\n",
      "Epoch 83301/150000, Loss: 1217.88427734375, Validation Loss: 1390.4505615234375\n",
      "Epoch 83401/150000, Loss: 1211.0596923828125, Validation Loss: 1384.0057373046875\n",
      "Epoch 83501/150000, Loss: 1204.2623291015625, Validation Loss: 1377.5592041015625\n",
      "Epoch 83601/150000, Loss: 1197.491943359375, Validation Loss: 1371.1568603515625\n",
      "Epoch 83701/150000, Loss: 1190.7489013671875, Validation Loss: 1364.7864990234375\n",
      "Epoch 83801/150000, Loss: 1184.0325927734375, Validation Loss: 1358.44091796875\n",
      "Epoch 83901/150000, Loss: 1177.3446044921875, Validation Loss: 1352.185791015625\n",
      "Epoch 84001/150000, Loss: 1170.680908203125, Validation Loss: 1345.8468017578125\n",
      "Epoch 84101/150000, Loss: 1164.045654296875, Validation Loss: 1339.5399169921875\n",
      "Epoch 84201/150000, Loss: 1157.436279296875, Validation Loss: 1333.3619384765625\n",
      "Epoch 84301/150000, Loss: 1150.858154296875, Validation Loss: 1327.1162109375\n",
      "Epoch 84401/150000, Loss: 1144.2982177734375, Validation Loss: 1320.991943359375\n",
      "Epoch 84501/150000, Loss: 1137.769287109375, Validation Loss: 1314.8360595703125\n",
      "Epoch 84601/150000, Loss: 1131.2662353515625, Validation Loss: 1308.734130859375\n",
      "Epoch 84701/150000, Loss: 1124.790283203125, Validation Loss: 1302.5997314453125\n",
      "Epoch 84801/150000, Loss: 1118.340087890625, Validation Loss: 1296.59033203125\n",
      "Epoch 84901/150000, Loss: 1111.9166259765625, Validation Loss: 1290.6124267578125\n",
      "Epoch 85001/150000, Loss: 1105.5189208984375, Validation Loss: 1284.563232421875\n",
      "Epoch 85101/150000, Loss: 1099.1544189453125, Validation Loss: 1278.6795654296875\n",
      "Epoch 85201/150000, Loss: 1092.8018798828125, Validation Loss: 1272.658935546875\n",
      "Epoch 85301/150000, Loss: 1086.4813232421875, Validation Loss: 1266.7708740234375\n",
      "Epoch 85401/150000, Loss: 1080.184326171875, Validation Loss: 1260.8916015625\n",
      "Epoch 85501/150000, Loss: 1073.906982421875, Validation Loss: 1255.03857421875\n",
      "Epoch 85601/150000, Loss: 1067.6573486328125, Validation Loss: 1249.1485595703125\n",
      "Epoch 85701/150000, Loss: 1061.43994140625, Validation Loss: 1243.25341796875\n",
      "Epoch 85801/150000, Loss: 1055.2462158203125, Validation Loss: 1237.5079345703125\n",
      "Epoch 85901/150000, Loss: 1049.08203125, Validation Loss: 1231.743408203125\n",
      "Epoch 86001/150000, Loss: 1042.944091796875, Validation Loss: 1225.9873046875\n",
      "Epoch 86101/150000, Loss: 1036.8328857421875, Validation Loss: 1220.289306640625\n",
      "Epoch 86201/150000, Loss: 1030.747314453125, Validation Loss: 1214.576171875\n",
      "Epoch 86301/150000, Loss: 1024.688232421875, Validation Loss: 1208.913818359375\n",
      "Epoch 86401/150000, Loss: 1018.6549682617188, Validation Loss: 1203.2738037109375\n",
      "Epoch 86501/150000, Loss: 1012.64990234375, Validation Loss: 1197.6649169921875\n",
      "Epoch 86601/150000, Loss: 1006.6739501953125, Validation Loss: 1192.085693359375\n",
      "Epoch 86701/150000, Loss: 1000.7302856445312, Validation Loss: 1186.541015625\n",
      "Epoch 86801/150000, Loss: 994.82421875, Validation Loss: 1181.033447265625\n",
      "Epoch 86901/150000, Loss: 988.9615478515625, Validation Loss: 1175.5740966796875\n",
      "Epoch 87001/150000, Loss: 983.1288452148438, Validation Loss: 1170.1456298828125\n",
      "Epoch 87101/150000, Loss: 977.3202514648438, Validation Loss: 1164.7369384765625\n",
      "Epoch 87201/150000, Loss: 971.5379638671875, Validation Loss: 1159.349853515625\n",
      "Epoch 87301/150000, Loss: 965.7802734375, Validation Loss: 1154.01416015625\n",
      "Epoch 87401/150000, Loss: 960.0477905273438, Validation Loss: 1148.6923828125\n",
      "Epoch 87501/150000, Loss: 954.3405151367188, Validation Loss: 1143.4036865234375\n",
      "Epoch 87601/150000, Loss: 948.6580200195312, Validation Loss: 1138.13818359375\n",
      "Epoch 87701/150000, Loss: 943.0003051757812, Validation Loss: 1132.9105224609375\n",
      "Epoch 87801/150000, Loss: 937.3671875, Validation Loss: 1127.697021484375\n",
      "Epoch 87901/150000, Loss: 931.7587890625, Validation Loss: 1122.5181884765625\n",
      "Epoch 88001/150000, Loss: 926.1751098632812, Validation Loss: 1117.3697509765625\n",
      "Epoch 88101/150000, Loss: 920.6270141601562, Validation Loss: 1112.3819580078125\n",
      "Epoch 88201/150000, Loss: 915.0809936523438, Validation Loss: 1107.160400390625\n",
      "Epoch 88301/150000, Loss: 909.5703125, Validation Loss: 1102.100830078125\n",
      "Epoch 88401/150000, Loss: 904.0841674804688, Validation Loss: 1097.0675048828125\n",
      "Epoch 88501/150000, Loss: 898.6220092773438, Validation Loss: 1092.0718994140625\n",
      "Epoch 88601/150000, Loss: 893.1836547851562, Validation Loss: 1087.0965576171875\n",
      "Epoch 88701/150000, Loss: 887.7681884765625, Validation Loss: 1082.1614990234375\n",
      "Epoch 88801/150000, Loss: 882.3782958984375, Validation Loss: 1077.130859375\n",
      "Epoch 88901/150000, Loss: 876.997314453125, Validation Loss: 1072.3375244140625\n",
      "Epoch 89001/150000, Loss: 871.6513061523438, Validation Loss: 1067.443115234375\n",
      "Epoch 89101/150000, Loss: 866.3345336914062, Validation Loss: 1062.5616455078125\n",
      "Epoch 89201/150000, Loss: 861.0440063476562, Validation Loss: 1057.7232666015625\n",
      "Epoch 89301/150000, Loss: 855.7787475585938, Validation Loss: 1052.8961181640625\n",
      "Epoch 89401/150000, Loss: 850.5380249023438, Validation Loss: 1048.1158447265625\n",
      "Epoch 89501/150000, Loss: 845.322265625, Validation Loss: 1043.374755859375\n",
      "Epoch 89601/150000, Loss: 840.1299438476562, Validation Loss: 1038.6236572265625\n",
      "Epoch 89701/150000, Loss: 834.9622192382812, Validation Loss: 1033.9332275390625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "window_sizes = [5]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=5\n",
      "Epoch 1/150000, Loss: 30035.376953125, Validation Loss: 30251.630859375\n",
      "Epoch 101/150000, Loss: 29266.44921875, Validation Loss: 29474.916015625\n",
      "Epoch 201/150000, Loss: 28588.267578125, Validation Loss: 28796.248046875\n",
      "Epoch 301/150000, Loss: 28017.205078125, Validation Loss: 28223.380859375\n",
      "Epoch 401/150000, Loss: 27489.17578125, Validation Loss: 27693.4375\n",
      "Epoch 501/150000, Loss: 26990.47265625, Validation Loss: 27192.81640625\n",
      "Epoch 601/150000, Loss: 26515.4765625, Validation Loss: 26715.916015625\n",
      "Epoch 701/150000, Loss: 26061.1796875, Validation Loss: 26259.73046875\n",
      "Epoch 801/150000, Loss: 25625.68359375, Validation Loss: 25822.365234375\n",
      "Epoch 901/150000, Loss: 25207.666015625, Validation Loss: 25402.49609375\n",
      "Epoch 1001/150000, Loss: 24806.12109375, Validation Loss: 24999.123046875\n",
      "Epoch 1101/150000, Loss: 24420.23828125, Validation Loss: 24611.427734375\n",
      "Epoch 1201/150000, Loss: 24049.345703125, Validation Loss: 24238.740234375\n",
      "Epoch 1301/150000, Loss: 23692.8515625, Validation Loss: 23880.470703125\n",
      "Epoch 1401/150000, Loss: 23350.24609375, Validation Loss: 23536.109375\n",
      "Epoch 1501/150000, Loss: 23021.056640625, Validation Loss: 23205.18359375\n",
      "Epoch 1601/150000, Loss: 22704.865234375, Validation Loss: 22887.265625\n",
      "Epoch 1701/150000, Loss: 22401.2734375, Validation Loss: 22581.970703125\n",
      "Epoch 1801/150000, Loss: 22109.9140625, Validation Loss: 22288.927734375\n",
      "Epoch 1901/150000, Loss: 21830.447265625, Validation Loss: 22007.787109375\n",
      "Epoch 2001/150000, Loss: 21562.5390625, Validation Loss: 21738.23046875\n",
      "Epoch 2101/150000, Loss: 21305.888671875, Validation Loss: 21479.94140625\n",
      "Epoch 2201/150000, Loss: 21060.1953125, Validation Loss: 21232.6328125\n",
      "Epoch 2301/150000, Loss: 20825.17578125, Validation Loss: 20996.01171875\n",
      "Epoch 2401/150000, Loss: 20600.55859375, Validation Loss: 20769.810546875\n",
      "Epoch 2501/150000, Loss: 20386.07421875, Validation Loss: 20553.7578125\n",
      "Epoch 2601/150000, Loss: 20181.466796875, Validation Loss: 20347.603515625\n",
      "Epoch 2701/150000, Loss: 19986.486328125, Validation Loss: 20151.095703125\n",
      "Epoch 2801/150000, Loss: 19800.892578125, Validation Loss: 19963.984375\n",
      "Epoch 2901/150000, Loss: 19624.4296875, Validation Loss: 19786.03125\n",
      "Epoch 3001/150000, Loss: 19456.857421875, Validation Loss: 19616.984375\n",
      "Epoch 3101/150000, Loss: 19297.90625, Validation Loss: 19456.578125\n",
      "Epoch 3201/150000, Loss: 18277.046875, Validation Loss: 18452.904296875\n",
      "Epoch 3301/150000, Loss: 17959.693359375, Validation Loss: 18135.4375\n",
      "Epoch 3401/150000, Loss: 17655.84765625, Validation Loss: 17830.130859375\n",
      "Epoch 3501/150000, Loss: 17360.9453125, Validation Loss: 17534.25\n",
      "Epoch 3601/150000, Loss: 17074.888671875, Validation Loss: 17247.6640625\n",
      "Epoch 3701/150000, Loss: 16797.607421875, Validation Loss: 16970.064453125\n",
      "Epoch 3801/150000, Loss: 16528.349609375, Validation Loss: 16700.16796875\n",
      "Epoch 3901/150000, Loss: 16266.451171875, Validation Loss: 16437.27734375\n",
      "Epoch 4001/150000, Loss: 16011.3681640625, Validation Loss: 16180.9189453125\n",
      "Epoch 4101/150000, Loss: 15762.6357421875, Validation Loss: 15930.7001953125\n",
      "Epoch 4201/150000, Loss: 15519.9560546875, Validation Loss: 15686.4345703125\n",
      "Epoch 4301/150000, Loss: 15283.09765625, Validation Loss: 15447.9775390625\n",
      "Epoch 4401/150000, Loss: 15051.7900390625, Validation Loss: 15215.0986328125\n",
      "Epoch 4501/150000, Loss: 14825.748046875, Validation Loss: 14987.5283203125\n",
      "Epoch 4601/150000, Loss: 14604.716796875, Validation Loss: 14765.0390625\n",
      "Epoch 4701/150000, Loss: 14388.4560546875, Validation Loss: 14547.41796875\n",
      "Epoch 4801/150000, Loss: 14176.6533203125, Validation Loss: 14334.353515625\n",
      "Epoch 4901/150000, Loss: 13968.7197265625, Validation Loss: 14125.3818359375\n",
      "Epoch 5001/150000, Loss: 13764.4931640625, Validation Loss: 13920.287109375\n",
      "Epoch 5101/150000, Loss: 13564.1396484375, Validation Loss: 13719.017578125\n",
      "Epoch 5201/150000, Loss: 13367.53515625, Validation Loss: 13521.4521484375\n",
      "Epoch 5301/150000, Loss: 13174.544921875, Validation Loss: 13327.46875\n",
      "Epoch 5401/150000, Loss: 12985.0380859375, Validation Loss: 13136.9150390625\n",
      "Epoch 5501/150000, Loss: 12798.857421875, Validation Loss: 12949.626953125\n",
      "Epoch 5601/150000, Loss: 12615.841796875, Validation Loss: 12765.44140625\n",
      "Epoch 5701/150000, Loss: 12435.83203125, Validation Loss: 12584.2080078125\n",
      "Epoch 5801/150000, Loss: 12258.748046875, Validation Loss: 12405.912109375\n",
      "Epoch 5901/150000, Loss: 12084.5185546875, Validation Loss: 12230.7041015625\n",
      "Epoch 6001/150000, Loss: 11913.05078125, Validation Loss: 12058.4189453125\n",
      "Epoch 6101/150000, Loss: 11744.2568359375, Validation Loss: 11888.892578125\n",
      "Epoch 6201/150000, Loss: 11578.0576171875, Validation Loss: 11722.0224609375\n",
      "Epoch 6301/150000, Loss: 11414.369140625, Validation Loss: 11557.716796875\n",
      "Epoch 6401/150000, Loss: 11253.125, Validation Loss: 11395.9111328125\n",
      "Epoch 6501/150000, Loss: 11094.263671875, Validation Loss: 11236.537109375\n",
      "Epoch 6601/150000, Loss: 10937.724609375, Validation Loss: 11079.5263671875\n",
      "Epoch 6701/150000, Loss: 10783.431640625, Validation Loss: 10924.796875\n",
      "Epoch 6801/150000, Loss: 10631.33984375, Validation Loss: 10772.287109375\n",
      "Epoch 6901/150000, Loss: 10481.369140625, Validation Loss: 10621.8896484375\n",
      "Epoch 7001/150000, Loss: 10333.4462890625, Validation Loss: 10473.4599609375\n",
      "Epoch 7101/150000, Loss: 10187.4853515625, Validation Loss: 10326.6630859375\n",
      "Epoch 7201/150000, Loss: 10043.431640625, Validation Loss: 10181.9189453125\n",
      "Epoch 7301/150000, Loss: 9901.2841796875, Validation Loss: 10039.0615234375\n",
      "Epoch 7401/150000, Loss: 9760.939453125, Validation Loss: 9897.8935546875\n",
      "Epoch 7501/150000, Loss: 9622.3916015625, Validation Loss: 9758.373046875\n",
      "Epoch 7601/150000, Loss: 9485.56640625, Validation Loss: 9620.4326171875\n",
      "Epoch 7701/150000, Loss: 9350.4150390625, Validation Loss: 9483.9560546875\n",
      "Epoch 7801/150000, Loss: 9216.8544921875, Validation Loss: 9350.0615234375\n",
      "Epoch 7901/150000, Loss: 9084.9169921875, Validation Loss: 9217.375\n",
      "Epoch 8001/150000, Loss: 8954.59375, Validation Loss: 9086.548828125\n",
      "Epoch 8101/150000, Loss: 8825.7548828125, Validation Loss: 8957.171875\n",
      "Epoch 8201/150000, Loss: 8698.3154296875, Validation Loss: 8829.1572265625\n",
      "Epoch 8301/150000, Loss: 8572.2529296875, Validation Loss: 8702.5185546875\n",
      "Epoch 8401/150000, Loss: 8447.5126953125, Validation Loss: 8577.2001953125\n",
      "Epoch 8501/150000, Loss: 8324.0966796875, Validation Loss: 8453.25\n",
      "Epoch 8601/150000, Loss: 8201.96875, Validation Loss: 8330.5888671875\n",
      "Epoch 8701/150000, Loss: 8081.12451171875, Validation Loss: 8209.2314453125\n",
      "Epoch 8801/150000, Loss: 7961.552734375, Validation Loss: 8089.19482421875\n",
      "Epoch 8901/150000, Loss: 7843.22216796875, Validation Loss: 7970.4775390625\n",
      "Epoch 9001/150000, Loss: 7726.12109375, Validation Loss: 7852.99267578125\n",
      "Epoch 9101/150000, Loss: 7610.240234375, Validation Loss: 7736.8046875\n",
      "Epoch 9201/150000, Loss: 7495.56005859375, Validation Loss: 7621.87109375\n",
      "Epoch 9301/150000, Loss: 7382.06640625, Validation Loss: 7508.1611328125\n",
      "Epoch 9401/150000, Loss: 7269.7392578125, Validation Loss: 7395.650390625\n",
      "Epoch 9501/150000, Loss: 7158.57275390625, Validation Loss: 7284.30322265625\n",
      "Epoch 9601/150000, Loss: 7048.55859375, Validation Loss: 7174.0927734375\n",
      "Epoch 9701/150000, Loss: 6939.68798828125, Validation Loss: 7065.064453125\n",
      "Epoch 9801/150000, Loss: 6831.94189453125, Validation Loss: 6957.16259765625\n",
      "Epoch 9901/150000, Loss: 6725.3125, Validation Loss: 6850.39208984375\n",
      "Epoch 10001/150000, Loss: 6619.81005859375, Validation Loss: 6744.80615234375\n",
      "Epoch 10101/150000, Loss: 6515.45166015625, Validation Loss: 6640.3740234375\n",
      "Epoch 10201/150000, Loss: 6412.21630859375, Validation Loss: 6537.06005859375\n",
      "Epoch 10301/150000, Loss: 6310.13916015625, Validation Loss: 6434.9013671875\n",
      "Epoch 10401/150000, Loss: 6208.86474609375, Validation Loss: 6330.890625\n",
      "Epoch 10501/150000, Loss: 6108.8876953125, Validation Loss: 6230.57373046875\n",
      "Epoch 10601/150000, Loss: 6010.14111328125, Validation Loss: 6131.740234375\n",
      "Epoch 10701/150000, Loss: 5912.5361328125, Validation Loss: 6034.08984375\n",
      "Epoch 10801/150000, Loss: 5816.07177734375, Validation Loss: 5937.583984375\n",
      "Epoch 10901/150000, Loss: 5720.7451171875, Validation Loss: 5842.2470703125\n",
      "Epoch 11001/150000, Loss: 5626.591796875, Validation Loss: 5748.0927734375\n",
      "Epoch 11101/150000, Loss: 5533.58984375, Validation Loss: 5655.09423828125\n",
      "Epoch 11201/150000, Loss: 5441.7177734375, Validation Loss: 5563.25341796875\n",
      "Epoch 11301/150000, Loss: 5351.01513671875, Validation Loss: 5472.5390625\n",
      "Epoch 11401/150000, Loss: 5261.4345703125, Validation Loss: 5382.99462890625\n",
      "Epoch 11501/150000, Loss: 5172.9921875, Validation Loss: 5294.5986328125\n",
      "Epoch 11601/150000, Loss: 5085.66845703125, Validation Loss: 5207.4072265625\n",
      "Epoch 11701/150000, Loss: 4999.43994140625, Validation Loss: 5121.333984375\n",
      "Epoch 11801/150000, Loss: 4914.31103515625, Validation Loss: 5036.42529296875\n",
      "Epoch 11901/150000, Loss: 4830.234375, Validation Loss: 4952.712890625\n",
      "Epoch 12001/150000, Loss: 4747.17138671875, Validation Loss: 4869.86083984375\n",
      "Epoch 12101/150000, Loss: 4665.25732421875, Validation Loss: 4788.05126953125\n",
      "Epoch 12201/150000, Loss: 4584.18310546875, Validation Loss: 4707.576171875\n",
      "Epoch 12301/150000, Loss: 4504.1796875, Validation Loss: 4627.9462890625\n",
      "Epoch 12401/150000, Loss: 4425.14990234375, Validation Loss: 4549.28125\n",
      "Epoch 12501/150000, Loss: 4347.0234375, Validation Loss: 4471.845703125\n",
      "Epoch 12601/150000, Loss: 4269.95458984375, Validation Loss: 4395.1181640625\n",
      "Epoch 12701/150000, Loss: 4193.87060546875, Validation Loss: 4319.39599609375\n",
      "Epoch 12801/150000, Loss: 4118.82275390625, Validation Loss: 4244.82080078125\n",
      "Epoch 12901/150000, Loss: 4044.74658203125, Validation Loss: 4171.11767578125\n",
      "Epoch 13001/150000, Loss: 3971.66357421875, Validation Loss: 4098.5029296875\n",
      "Epoch 13101/150000, Loss: 3899.5791015625, Validation Loss: 4026.94384765625\n",
      "Epoch 13201/150000, Loss: 3828.470947265625, Validation Loss: 3956.333740234375\n",
      "Epoch 13301/150000, Loss: 3758.34521484375, Validation Loss: 3886.77685546875\n",
      "Epoch 13401/150000, Loss: 3689.197509765625, Validation Loss: 3818.175048828125\n",
      "Epoch 13501/150000, Loss: 3621.0244140625, Validation Loss: 3750.607666015625\n",
      "Epoch 13601/150000, Loss: 3553.82275390625, Validation Loss: 3684.018310546875\n",
      "Epoch 13701/150000, Loss: 3487.592041015625, Validation Loss: 3618.455078125\n",
      "Epoch 13801/150000, Loss: 3422.323486328125, Validation Loss: 3553.74072265625\n",
      "Epoch 13901/150000, Loss: 3358.016845703125, Validation Loss: 3490.034423828125\n",
      "Epoch 14001/150000, Loss: 3294.665283203125, Validation Loss: 3427.27197265625\n",
      "Epoch 14101/150000, Loss: 3232.269287109375, Validation Loss: 3365.478759765625\n",
      "Epoch 14201/150000, Loss: 3170.867919921875, Validation Loss: 3304.46630859375\n",
      "Epoch 14301/150000, Loss: 3110.31689453125, Validation Loss: 3244.710693359375\n",
      "Epoch 14401/150000, Loss: 3050.73681640625, Validation Loss: 3185.7275390625\n",
      "Epoch 14501/150000, Loss: 2992.076171875, Validation Loss: 3127.671142578125\n",
      "Epoch 14601/150000, Loss: 2934.321533203125, Validation Loss: 3070.5419921875\n",
      "Epoch 14701/150000, Loss: 2877.433349609375, Validation Loss: 3014.279296875\n",
      "Epoch 14801/150000, Loss: 2821.41259765625, Validation Loss: 2958.91552734375\n",
      "Epoch 14901/150000, Loss: 2766.2001953125, Validation Loss: 2904.42431640625\n",
      "Epoch 15001/150000, Loss: 2711.763427734375, Validation Loss: 2850.41943359375\n",
      "Epoch 15101/150000, Loss: 2658.165283203125, Validation Loss: 2797.173583984375\n",
      "Epoch 15201/150000, Loss: 2605.35400390625, Validation Loss: 2744.72607421875\n",
      "Epoch 15301/150000, Loss: 2553.301025390625, Validation Loss: 2693.05029296875\n",
      "Epoch 15401/150000, Loss: 2502.129150390625, Validation Loss: 2642.1083984375\n",
      "Epoch 15501/150000, Loss: 2451.63037109375, Validation Loss: 2592.316650390625\n",
      "Epoch 15601/150000, Loss: 2402.013671875, Validation Loss: 2543.280029296875\n",
      "Epoch 15701/150000, Loss: 2353.157470703125, Validation Loss: 2495.192626953125\n",
      "Epoch 15801/150000, Loss: 2305.097412109375, Validation Loss: 2447.8994140625\n",
      "Epoch 15901/150000, Loss: 2257.752197265625, Validation Loss: 2401.34375\n",
      "Epoch 16001/150000, Loss: 2211.1845703125, Validation Loss: 2355.604736328125\n",
      "Epoch 16101/150000, Loss: 2165.3095703125, Validation Loss: 2310.53564453125\n",
      "Epoch 16201/150000, Loss: 2120.118896484375, Validation Loss: 2266.175048828125\n",
      "Epoch 16301/150000, Loss: 2075.657958984375, Validation Loss: 2222.53955078125\n",
      "Epoch 16401/150000, Loss: 2031.8587646484375, Validation Loss: 2179.5498046875\n",
      "Epoch 16501/150000, Loss: 1988.7080078125, Validation Loss: 2137.201171875\n",
      "Epoch 16601/150000, Loss: 1946.26513671875, Validation Loss: 2095.556396484375\n",
      "Epoch 16701/150000, Loss: 1904.4761962890625, Validation Loss: 2054.581298828125\n",
      "Epoch 16801/150000, Loss: 1863.3665771484375, Validation Loss: 2014.2939453125\n",
      "Epoch 16901/150000, Loss: 1822.928955078125, Validation Loss: 1974.694580078125\n",
      "Epoch 17001/150000, Loss: 1783.1763916015625, Validation Loss: 1935.81884765625\n",
      "Epoch 17101/150000, Loss: 1744.072265625, Validation Loss: 1897.665771484375\n",
      "Epoch 17201/150000, Loss: 1705.655517578125, Validation Loss: 1860.1158447265625\n",
      "Epoch 17301/150000, Loss: 1667.9146728515625, Validation Loss: 1823.302978515625\n",
      "Epoch 17401/150000, Loss: 1630.828857421875, Validation Loss: 1787.166015625\n",
      "Epoch 17501/150000, Loss: 1594.399169921875, Validation Loss: 1751.694091796875\n",
      "Epoch 17601/150000, Loss: 1558.631103515625, Validation Loss: 1716.9122314453125\n",
      "Epoch 17701/150000, Loss: 1523.4986572265625, Validation Loss: 1682.81103515625\n",
      "Epoch 17801/150000, Loss: 1488.9925537109375, Validation Loss: 1649.4434814453125\n",
      "Epoch 17901/150000, Loss: 1455.10302734375, Validation Loss: 1616.70751953125\n",
      "Epoch 18001/150000, Loss: 1421.826904296875, Validation Loss: 1584.5589599609375\n",
      "Epoch 18101/150000, Loss: 1389.1387939453125, Validation Loss: 1553.1019287109375\n",
      "Epoch 18201/150000, Loss: 1357.025634765625, Validation Loss: 1522.2216796875\n",
      "Epoch 18301/150000, Loss: 1325.4654541015625, Validation Loss: 1492.0513916015625\n",
      "Epoch 18401/150000, Loss: 1294.4500732421875, Validation Loss: 1462.3104248046875\n",
      "Epoch 18501/150000, Loss: 1263.92724609375, Validation Loss: 1433.3773193359375\n",
      "Epoch 18601/150000, Loss: 1233.895751953125, Validation Loss: 1405.119140625\n",
      "Epoch 18701/150000, Loss: 1204.4173583984375, Validation Loss: 1377.192138671875\n",
      "Epoch 18801/150000, Loss: 1175.480224609375, Validation Loss: 1349.70361328125\n",
      "Epoch 18901/150000, Loss: 1147.0872802734375, Validation Loss: 1322.712158203125\n",
      "Epoch 19001/150000, Loss: 1119.2479248046875, Validation Loss: 1296.259033203125\n",
      "Epoch 19101/150000, Loss: 1091.9287109375, Validation Loss: 1270.51611328125\n",
      "Epoch 19201/150000, Loss: 1065.140380859375, Validation Loss: 1245.2694091796875\n",
      "Epoch 19301/150000, Loss: 1038.8800048828125, Validation Loss: 1220.5911865234375\n",
      "Epoch 19401/150000, Loss: 1013.1401977539062, Validation Loss: 1196.4949951171875\n",
      "Epoch 19501/150000, Loss: 987.922119140625, Validation Loss: 1172.9356689453125\n",
      "Epoch 19601/150000, Loss: 963.2158203125, Validation Loss: 1149.9884033203125\n",
      "Epoch 19701/150000, Loss: 939.0321044921875, Validation Loss: 1127.4906005859375\n",
      "Epoch 19801/150000, Loss: 915.3222045898438, Validation Loss: 1105.7156982421875\n",
      "Epoch 19901/150000, Loss: 892.1188354492188, Validation Loss: 1084.4083251953125\n",
      "Epoch 20001/150000, Loss: 869.3998413085938, Validation Loss: 1063.6085205078125\n",
      "Epoch 20101/150000, Loss: 847.1502075195312, Validation Loss: 1043.3250732421875\n",
      "Epoch 20201/150000, Loss: 825.3621215820312, Validation Loss: 1023.521240234375\n",
      "Epoch 20301/150000, Loss: 804.0294189453125, Validation Loss: 1004.1224975585938\n",
      "Epoch 20401/150000, Loss: 783.1475219726562, Validation Loss: 985.1769409179688\n",
      "Epoch 20501/150000, Loss: 762.7047729492188, Validation Loss: 966.6067504882812\n",
      "Epoch 20601/150000, Loss: 742.6958618164062, Validation Loss: 948.4927978515625\n",
      "Epoch 20701/150000, Loss: 723.1177368164062, Validation Loss: 930.783935546875\n",
      "Epoch 20801/150000, Loss: 703.9605102539062, Validation Loss: 913.4878540039062\n",
      "Epoch 20901/150000, Loss: 685.2197265625, Validation Loss: 896.65234375\n",
      "Epoch 21001/150000, Loss: 666.8870849609375, Validation Loss: 880.0623168945312\n",
      "Epoch 21101/150000, Loss: 648.956298828125, Validation Loss: 863.8416748046875\n",
      "Epoch 21201/150000, Loss: 631.4165649414062, Validation Loss: 848.0979614257812\n",
      "Epoch 21301/150000, Loss: 614.2586669921875, Validation Loss: 832.6395874023438\n",
      "Epoch 21401/150000, Loss: 597.4799194335938, Validation Loss: 817.5407104492188\n",
      "Epoch 21501/150000, Loss: 581.0708618164062, Validation Loss: 802.8468627929688\n",
      "Epoch 21601/150000, Loss: 565.0287475585938, Validation Loss: 788.3746948242188\n",
      "Epoch 21701/150000, Loss: 549.3351440429688, Validation Loss: 774.494384765625\n",
      "Epoch 21801/150000, Loss: 534.0027465820312, Validation Loss: 760.7705078125\n",
      "Epoch 21901/150000, Loss: 519.0313720703125, Validation Loss: 747.3795166015625\n",
      "Epoch 22001/150000, Loss: 504.4231262207031, Validation Loss: 734.3540649414062\n",
      "Epoch 22101/150000, Loss: 490.18890380859375, Validation Loss: 721.462646484375\n",
      "Epoch 22201/150000, Loss: 476.2889709472656, Validation Loss: 709.3385009765625\n",
      "Epoch 22301/150000, Loss: 462.75799560546875, Validation Loss: 697.3607177734375\n",
      "Epoch 22401/150000, Loss: 449.5800476074219, Validation Loss: 685.744873046875\n",
      "Epoch 22501/150000, Loss: 436.75421142578125, Validation Loss: 674.4482421875\n",
      "Epoch 22601/150000, Loss: 424.2772521972656, Validation Loss: 663.5059204101562\n",
      "Epoch 22701/150000, Loss: 412.1451110839844, Validation Loss: 652.9234008789062\n",
      "Epoch 22801/150000, Loss: 400.3604736328125, Validation Loss: 642.66455078125\n",
      "Epoch 22901/150000, Loss: 388.9022521972656, Validation Loss: 632.7413330078125\n",
      "Epoch 23001/150000, Loss: 377.7823791503906, Validation Loss: 623.1343383789062\n",
      "Epoch 23101/150000, Loss: 366.9922790527344, Validation Loss: 613.8661499023438\n",
      "Epoch 23201/150000, Loss: 356.52410888671875, Validation Loss: 604.8720703125\n",
      "Epoch 23301/150000, Loss: 346.4058532714844, Validation Loss: 596.55517578125\n",
      "Epoch 23401/150000, Loss: 336.5276794433594, Validation Loss: 587.6875610351562\n",
      "Epoch 23501/150000, Loss: 326.42181396484375, Validation Loss: 579.3397216796875\n",
      "Epoch 23601/150000, Loss: 315.9821472167969, Validation Loss: 568.427978515625\n",
      "Epoch 23701/150000, Loss: 306.9000244140625, Validation Loss: 559.2445068359375\n",
      "Epoch 23801/150000, Loss: 297.9416809082031, Validation Loss: 551.7218627929688\n",
      "Epoch 23901/150000, Loss: 289.2966003417969, Validation Loss: 544.5409545898438\n",
      "Epoch 24001/150000, Loss: 280.9027099609375, Validation Loss: 537.4105834960938\n",
      "Epoch 24101/150000, Loss: 272.09393310546875, Validation Loss: 529.7610473632812\n",
      "Epoch 24201/150000, Loss: 262.6543884277344, Validation Loss: 521.3081665039062\n",
      "Early stopping at epoch 24260 with validation loss 519.005615234375.\n",
      "Test Loss: 490.448974609375\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0001, window_size=5\n",
      "Epoch 1/150000, Loss: 29966.947265625, Validation Loss: 30186.34765625\n",
      "Epoch 101/150000, Loss: 29959.26953125, Validation Loss: 30178.65234375\n",
      "Epoch 201/150000, Loss: 29952.44140625, Validation Loss: 30171.810546875\n",
      "Epoch 301/150000, Loss: 29946.283203125, Validation Loss: 30165.630859375\n",
      "Epoch 401/150000, Loss: 29940.630859375, Validation Loss: 30159.966796875\n",
      "Epoch 501/150000, Loss: 29935.365234375, Validation Loss: 30154.68359375\n",
      "Epoch 601/150000, Loss: 29930.373046875, Validation Loss: 30149.67578125\n",
      "Epoch 701/150000, Loss: 29925.521484375, Validation Loss: 30144.814453125\n",
      "Epoch 801/150000, Loss: 29920.666015625, Validation Loss: 30139.939453125\n",
      "Epoch 901/150000, Loss: 29915.59375, Validation Loss: 30134.853515625\n",
      "Epoch 1001/150000, Loss: 29910.052734375, Validation Loss: 30129.296875\n",
      "Epoch 1101/150000, Loss: 29903.802734375, Validation Loss: 30123.029296875\n",
      "Epoch 1201/150000, Loss: 29896.630859375, Validation Loss: 30115.84375\n",
      "Epoch 1301/150000, Loss: 29888.376953125, Validation Loss: 30107.568359375\n",
      "Epoch 1401/150000, Loss: 29878.861328125, Validation Loss: 30098.03515625\n",
      "Epoch 1501/150000, Loss: 29867.84375, Validation Loss: 30087.00390625\n",
      "Epoch 1601/150000, Loss: 29854.92578125, Validation Loss: 30074.05078125\n",
      "Epoch 1701/150000, Loss: 29839.63671875, Validation Loss: 30058.728515625\n",
      "Epoch 1801/150000, Loss: 29821.62890625, Validation Loss: 30040.6640625\n",
      "Epoch 1901/150000, Loss: 29800.556640625, Validation Loss: 30019.509765625\n",
      "Epoch 2001/150000, Loss: 29775.962890625, Validation Loss: 29994.81640625\n",
      "Epoch 2101/150000, Loss: 29747.697265625, Validation Loss: 29966.421875\n",
      "Epoch 2201/150000, Loss: 29716.716796875, Validation Loss: 29935.310546875\n",
      "Epoch 2301/150000, Loss: 29685.251953125, Validation Loss: 29903.724609375\n",
      "Epoch 2401/150000, Loss: 29655.517578125, Validation Loss: 29873.87109375\n",
      "Epoch 2501/150000, Loss: 29628.52734375, Validation Loss: 29846.771484375\n",
      "Epoch 2601/150000, Loss: 29604.220703125, Validation Loss: 29822.365234375\n",
      "Epoch 2701/150000, Loss: 29582.044921875, Validation Loss: 29800.0859375\n",
      "Epoch 2801/150000, Loss: 29561.294921875, Validation Loss: 29779.248046875\n",
      "Epoch 2901/150000, Loss: 29541.212890625, Validation Loss: 29759.06640625\n",
      "Epoch 3001/150000, Loss: 29520.921875, Validation Loss: 29738.6796875\n",
      "Epoch 3101/150000, Loss: 29499.5234375, Validation Loss: 29717.1796875\n",
      "Epoch 3201/150000, Loss: 29476.591796875, Validation Loss: 29694.15625\n",
      "Epoch 3301/150000, Loss: 29452.671875, Validation Loss: 29670.138671875\n",
      "Epoch 3401/150000, Loss: 29428.83984375, Validation Loss: 29646.21875\n",
      "Epoch 3501/150000, Loss: 29405.91015625, Validation Loss: 29623.203125\n",
      "Epoch 3601/150000, Loss: 29384.234375, Validation Loss: 29601.4453125\n",
      "Epoch 3701/150000, Loss: 29363.841796875, Validation Loss: 29580.9765625\n",
      "Epoch 3801/150000, Loss: 29344.625, Validation Loss: 29561.67578125\n",
      "Epoch 3901/150000, Loss: 29326.419921875, Validation Loss: 29543.3984375\n",
      "Epoch 4001/150000, Loss: 29309.09375, Validation Loss: 29525.99609375\n",
      "Epoch 4101/150000, Loss: 29292.50390625, Validation Loss: 29509.3359375\n",
      "Epoch 4201/150000, Loss: 29276.546875, Validation Loss: 29493.310546875\n",
      "Epoch 4301/150000, Loss: 29261.12890625, Validation Loss: 29477.830078125\n",
      "Epoch 4401/150000, Loss: 29246.169921875, Validation Loss: 29462.80859375\n",
      "Epoch 4501/150000, Loss: 29231.6171875, Validation Loss: 29448.189453125\n",
      "Epoch 4601/150000, Loss: 29217.408203125, Validation Loss: 29433.921875\n",
      "Epoch 4701/150000, Loss: 29203.50390625, Validation Loss: 29419.958984375\n",
      "Epoch 4801/150000, Loss: 29189.869140625, Validation Loss: 29406.265625\n",
      "Epoch 4901/150000, Loss: 29176.470703125, Validation Loss: 29392.814453125\n",
      "Epoch 5001/150000, Loss: 29163.28125, Validation Loss: 29379.568359375\n",
      "Epoch 5101/150000, Loss: 29150.28515625, Validation Loss: 29366.517578125\n",
      "Epoch 5201/150000, Loss: 29137.45703125, Validation Loss: 29353.630859375\n",
      "Epoch 5301/150000, Loss: 29124.77734375, Validation Loss: 29340.904296875\n",
      "Epoch 5401/150000, Loss: 29112.23828125, Validation Loss: 29328.310546875\n",
      "Epoch 5501/150000, Loss: 29099.82421875, Validation Loss: 29315.84765625\n",
      "Epoch 5601/150000, Loss: 29087.52734375, Validation Loss: 29303.49609375\n",
      "Epoch 5701/150000, Loss: 29075.33203125, Validation Loss: 29291.251953125\n",
      "Epoch 5801/150000, Loss: 29063.23046875, Validation Loss: 29279.1015625\n",
      "Epoch 5901/150000, Loss: 29051.224609375, Validation Loss: 29267.041015625\n",
      "Epoch 6001/150000, Loss: 29039.291015625, Validation Loss: 29255.060546875\n",
      "Epoch 6101/150000, Loss: 29027.435546875, Validation Loss: 29243.158203125\n",
      "Epoch 6201/150000, Loss: 29015.654296875, Validation Loss: 29231.326171875\n",
      "Epoch 6301/150000, Loss: 29003.93359375, Validation Loss: 29219.556640625\n",
      "Epoch 6401/150000, Loss: 28992.275390625, Validation Loss: 29207.849609375\n",
      "Epoch 6501/150000, Loss: 28980.671875, Validation Loss: 29196.197265625\n",
      "Epoch 6601/150000, Loss: 28969.12109375, Validation Loss: 29184.6015625\n",
      "Epoch 6701/150000, Loss: 28957.623046875, Validation Loss: 29173.0546875\n",
      "Epoch 6801/150000, Loss: 28946.166015625, Validation Loss: 29161.55078125\n",
      "Epoch 6901/150000, Loss: 28934.755859375, Validation Loss: 29150.08984375\n",
      "Epoch 7001/150000, Loss: 28923.3828125, Validation Loss: 29138.673828125\n",
      "Epoch 7101/150000, Loss: 28912.05078125, Validation Loss: 29127.29296875\n",
      "Epoch 7201/150000, Loss: 28900.755859375, Validation Loss: 29115.951171875\n",
      "Epoch 7301/150000, Loss: 28889.49609375, Validation Loss: 29104.646484375\n",
      "Epoch 7401/150000, Loss: 28878.26953125, Validation Loss: 29093.37109375\n",
      "Epoch 7501/150000, Loss: 28867.072265625, Validation Loss: 29082.125\n",
      "Epoch 7601/150000, Loss: 28855.904296875, Validation Loss: 29070.916015625\n",
      "Epoch 7701/150000, Loss: 28844.765625, Validation Loss: 29059.728515625\n",
      "Epoch 7801/150000, Loss: 28833.654296875, Validation Loss: 29048.572265625\n",
      "Epoch 7901/150000, Loss: 28822.56640625, Validation Loss: 29037.443359375\n",
      "Epoch 8001/150000, Loss: 28811.5078125, Validation Loss: 29026.3359375\n",
      "Epoch 8101/150000, Loss: 28800.46875, Validation Loss: 29015.251953125\n",
      "Epoch 8201/150000, Loss: 28789.453125, Validation Loss: 29004.19140625\n",
      "Epoch 8301/150000, Loss: 28778.4609375, Validation Loss: 28993.15234375\n",
      "Epoch 8401/150000, Loss: 28767.482421875, Validation Loss: 28982.134765625\n",
      "Epoch 8501/150000, Loss: 28756.533203125, Validation Loss: 28971.130859375\n",
      "Epoch 8601/150000, Loss: 28745.603515625, Validation Loss: 28960.15625\n",
      "Epoch 8701/150000, Loss: 28734.6875, Validation Loss: 28949.1953125\n",
      "Epoch 8801/150000, Loss: 28723.791015625, Validation Loss: 28938.2578125\n",
      "Epoch 8901/150000, Loss: 28712.91015625, Validation Loss: 28927.33203125\n",
      "Epoch 9001/150000, Loss: 28702.044921875, Validation Loss: 28916.419921875\n",
      "Epoch 9101/150000, Loss: 28691.19921875, Validation Loss: 28905.533203125\n",
      "Epoch 9201/150000, Loss: 28680.3671875, Validation Loss: 28894.658203125\n",
      "Epoch 9301/150000, Loss: 28669.552734375, Validation Loss: 28883.798828125\n",
      "Epoch 9401/150000, Loss: 28658.75, Validation Loss: 28872.951171875\n",
      "Epoch 9501/150000, Loss: 28647.962890625, Validation Loss: 28862.11328125\n",
      "Epoch 9601/150000, Loss: 28637.18359375, Validation Loss: 28851.298828125\n",
      "Epoch 9701/150000, Loss: 28626.42578125, Validation Loss: 28840.48828125\n",
      "Epoch 9801/150000, Loss: 28615.67578125, Validation Loss: 28829.6953125\n",
      "Epoch 9901/150000, Loss: 28604.94140625, Validation Loss: 28818.916015625\n",
      "Epoch 10001/150000, Loss: 28594.22265625, Validation Loss: 28808.15625\n",
      "Epoch 10101/150000, Loss: 28583.513671875, Validation Loss: 28797.400390625\n",
      "Epoch 10201/150000, Loss: 28572.814453125, Validation Loss: 28786.658203125\n",
      "Epoch 10301/150000, Loss: 28562.126953125, Validation Loss: 28775.927734375\n",
      "Epoch 10401/150000, Loss: 28551.44921875, Validation Loss: 28765.208984375\n",
      "Epoch 10501/150000, Loss: 28540.787109375, Validation Loss: 28754.5\n",
      "Epoch 10601/150000, Loss: 28530.134765625, Validation Loss: 28743.802734375\n",
      "Epoch 10701/150000, Loss: 28519.490234375, Validation Loss: 28733.1171875\n",
      "Epoch 10801/150000, Loss: 28508.857421875, Validation Loss: 28722.4375\n",
      "Epoch 10901/150000, Loss: 28498.234375, Validation Loss: 28711.76953125\n",
      "Epoch 11001/150000, Loss: 28487.619140625, Validation Loss: 28701.111328125\n",
      "Epoch 11101/150000, Loss: 28477.013671875, Validation Loss: 28690.46484375\n",
      "Epoch 11201/150000, Loss: 28466.41796875, Validation Loss: 28679.82421875\n",
      "Epoch 11301/150000, Loss: 28455.828125, Validation Loss: 28669.189453125\n",
      "Epoch 11401/150000, Loss: 28445.251953125, Validation Loss: 28658.568359375\n",
      "Epoch 11501/150000, Loss: 28434.68359375, Validation Loss: 28647.958984375\n",
      "Epoch 11601/150000, Loss: 28424.126953125, Validation Loss: 28637.353515625\n",
      "Epoch 11701/150000, Loss: 28413.576171875, Validation Loss: 28626.763671875\n",
      "Epoch 11801/150000, Loss: 28403.037109375, Validation Loss: 28616.1796875\n",
      "Epoch 11901/150000, Loss: 28392.5078125, Validation Loss: 28605.607421875\n",
      "Epoch 12001/150000, Loss: 28381.98828125, Validation Loss: 28595.041015625\n",
      "Epoch 12101/150000, Loss: 28371.474609375, Validation Loss: 28584.484375\n",
      "Epoch 12201/150000, Loss: 28360.97265625, Validation Loss: 28573.9375\n",
      "Epoch 12301/150000, Loss: 28350.47265625, Validation Loss: 28563.39453125\n",
      "Epoch 12401/150000, Loss: 28339.982421875, Validation Loss: 28552.861328125\n",
      "Epoch 12501/150000, Loss: 28329.498046875, Validation Loss: 28542.33203125\n",
      "Epoch 12601/150000, Loss: 28319.021484375, Validation Loss: 28531.814453125\n",
      "Epoch 12701/150000, Loss: 28308.556640625, Validation Loss: 28521.302734375\n",
      "Epoch 12801/150000, Loss: 28298.09375, Validation Loss: 28510.798828125\n",
      "Epoch 12901/150000, Loss: 28287.642578125, Validation Loss: 28500.3046875\n",
      "Epoch 13001/150000, Loss: 28277.1953125, Validation Loss: 28489.81640625\n",
      "Epoch 13101/150000, Loss: 28266.7578125, Validation Loss: 28479.3359375\n",
      "Epoch 13201/150000, Loss: 28256.326171875, Validation Loss: 28468.859375\n",
      "Epoch 13301/150000, Loss: 28245.900390625, Validation Loss: 28458.388671875\n",
      "Epoch 13401/150000, Loss: 28235.48046875, Validation Loss: 28447.927734375\n",
      "Epoch 13501/150000, Loss: 28225.0703125, Validation Loss: 28437.470703125\n",
      "Epoch 13601/150000, Loss: 28214.666015625, Validation Loss: 28427.02734375\n",
      "Epoch 13701/150000, Loss: 28204.26953125, Validation Loss: 28416.5859375\n",
      "Epoch 13801/150000, Loss: 28193.87890625, Validation Loss: 28406.15234375\n",
      "Epoch 13901/150000, Loss: 28183.49609375, Validation Loss: 28395.724609375\n",
      "Epoch 14001/150000, Loss: 28173.1171875, Validation Loss: 28385.302734375\n",
      "Epoch 14101/150000, Loss: 28162.74609375, Validation Loss: 28374.888671875\n",
      "Epoch 14201/150000, Loss: 28152.380859375, Validation Loss: 28364.482421875\n",
      "Epoch 14301/150000, Loss: 28142.0234375, Validation Loss: 28354.078125\n",
      "Epoch 14401/150000, Loss: 28131.671875, Validation Loss: 28343.68359375\n",
      "Epoch 14501/150000, Loss: 28121.32421875, Validation Loss: 28333.29296875\n",
      "Epoch 14601/150000, Loss: 28110.982421875, Validation Loss: 28322.90625\n",
      "Epoch 14701/150000, Loss: 28100.65234375, Validation Loss: 28312.533203125\n",
      "Epoch 14801/150000, Loss: 28090.32421875, Validation Loss: 28302.1640625\n",
      "Epoch 14901/150000, Loss: 28080.00390625, Validation Loss: 28291.798828125\n",
      "Epoch 15001/150000, Loss: 28069.689453125, Validation Loss: 28281.443359375\n",
      "Epoch 15101/150000, Loss: 28059.3828125, Validation Loss: 28271.09375\n",
      "Epoch 15201/150000, Loss: 28049.076171875, Validation Loss: 28260.7421875\n",
      "Epoch 15301/150000, Loss: 28038.77734375, Validation Loss: 28250.400390625\n",
      "Epoch 15401/150000, Loss: 28028.484375, Validation Loss: 28240.06640625\n",
      "Epoch 15501/150000, Loss: 28018.197265625, Validation Loss: 28229.736328125\n",
      "Epoch 15601/150000, Loss: 28007.916015625, Validation Loss: 28219.41015625\n",
      "Epoch 15701/150000, Loss: 27997.638671875, Validation Loss: 28209.09375\n",
      "Epoch 15801/150000, Loss: 27987.369140625, Validation Loss: 28198.775390625\n",
      "Epoch 15901/150000, Loss: 27977.10546875, Validation Loss: 28188.470703125\n",
      "Epoch 16001/150000, Loss: 27966.845703125, Validation Loss: 28178.16796875\n",
      "Epoch 16101/150000, Loss: 27956.58984375, Validation Loss: 28167.87109375\n",
      "Epoch 16201/150000, Loss: 27946.34375, Validation Loss: 28157.580078125\n",
      "Epoch 16301/150000, Loss: 27936.103515625, Validation Loss: 28147.296875\n",
      "Epoch 16401/150000, Loss: 27925.865234375, Validation Loss: 28137.015625\n",
      "Epoch 16501/150000, Loss: 27915.638671875, Validation Loss: 28126.7421875\n",
      "Epoch 16601/150000, Loss: 27905.412109375, Validation Loss: 28116.47265625\n",
      "Epoch 16701/150000, Loss: 27895.1953125, Validation Loss: 28106.21484375\n",
      "Epoch 16801/150000, Loss: 27884.986328125, Validation Loss: 28095.96484375\n",
      "Epoch 16901/150000, Loss: 27874.78515625, Validation Loss: 28085.71875\n",
      "Epoch 17001/150000, Loss: 27864.587890625, Validation Loss: 28075.478515625\n",
      "Epoch 17101/150000, Loss: 27854.3984375, Validation Loss: 28065.2421875\n",
      "Epoch 17201/150000, Loss: 27844.2109375, Validation Loss: 28055.015625\n",
      "Epoch 17301/150000, Loss: 27834.03125, Validation Loss: 28044.791015625\n",
      "Epoch 17401/150000, Loss: 27823.857421875, Validation Loss: 28034.57421875\n",
      "Epoch 17501/150000, Loss: 27813.689453125, Validation Loss: 28024.359375\n",
      "Epoch 17601/150000, Loss: 27803.5234375, Validation Loss: 28014.15625\n",
      "Epoch 17701/150000, Loss: 27793.36328125, Validation Loss: 28003.951171875\n",
      "Epoch 17801/150000, Loss: 27783.208984375, Validation Loss: 27993.75390625\n",
      "Epoch 17901/150000, Loss: 27773.060546875, Validation Loss: 27983.5625\n",
      "Epoch 18001/150000, Loss: 27762.919921875, Validation Loss: 27973.376953125\n",
      "Epoch 18101/150000, Loss: 27752.78125, Validation Loss: 27963.197265625\n",
      "Epoch 18201/150000, Loss: 27742.646484375, Validation Loss: 27953.021484375\n",
      "Epoch 18301/150000, Loss: 27732.5234375, Validation Loss: 27942.853515625\n",
      "Epoch 18401/150000, Loss: 27722.404296875, Validation Loss: 27932.6953125\n",
      "Epoch 18501/150000, Loss: 27712.29296875, Validation Loss: 27922.5390625\n",
      "Epoch 18601/150000, Loss: 27702.185546875, Validation Loss: 27912.38671875\n",
      "Epoch 18701/150000, Loss: 27692.087890625, Validation Loss: 27902.2421875\n",
      "Epoch 18801/150000, Loss: 27681.98828125, Validation Loss: 27892.107421875\n",
      "Epoch 18901/150000, Loss: 27671.8984375, Validation Loss: 27881.970703125\n",
      "Epoch 19001/150000, Loss: 27661.810546875, Validation Loss: 27871.84375\n",
      "Epoch 19101/150000, Loss: 27651.73046875, Validation Loss: 27861.71875\n",
      "Epoch 19201/150000, Loss: 27641.65625, Validation Loss: 27851.599609375\n",
      "Epoch 19301/150000, Loss: 27631.5859375, Validation Loss: 27841.484375\n",
      "Epoch 19401/150000, Loss: 27621.5234375, Validation Loss: 27831.380859375\n",
      "Epoch 19501/150000, Loss: 27611.46484375, Validation Loss: 27821.28125\n",
      "Epoch 19601/150000, Loss: 27601.412109375, Validation Loss: 27811.185546875\n",
      "Epoch 19701/150000, Loss: 27591.3671875, Validation Loss: 27801.095703125\n",
      "Epoch 19801/150000, Loss: 27581.32421875, Validation Loss: 27791.009765625\n",
      "Epoch 19901/150000, Loss: 27571.2890625, Validation Loss: 27780.927734375\n",
      "Epoch 20001/150000, Loss: 27561.2578125, Validation Loss: 27770.853515625\n",
      "Epoch 20101/150000, Loss: 27551.228515625, Validation Loss: 27760.78515625\n",
      "Epoch 20201/150000, Loss: 27541.20703125, Validation Loss: 27750.720703125\n",
      "Epoch 20301/150000, Loss: 27531.19140625, Validation Loss: 27740.662109375\n",
      "Epoch 20401/150000, Loss: 27521.177734375, Validation Loss: 27730.607421875\n",
      "Epoch 20501/150000, Loss: 27511.173828125, Validation Loss: 27720.556640625\n",
      "Epoch 20601/150000, Loss: 27501.171875, Validation Loss: 27710.51171875\n",
      "Epoch 20701/150000, Loss: 27491.17578125, Validation Loss: 27700.47265625\n",
      "Epoch 20801/150000, Loss: 27481.18359375, Validation Loss: 27690.439453125\n",
      "Epoch 20901/150000, Loss: 27471.197265625, Validation Loss: 27680.41015625\n",
      "Epoch 21001/150000, Loss: 27461.216796875, Validation Loss: 27670.3828125\n",
      "Epoch 21101/150000, Loss: 27451.240234375, Validation Loss: 27660.365234375\n",
      "Epoch 21201/150000, Loss: 27441.26953125, Validation Loss: 27650.349609375\n",
      "Epoch 21301/150000, Loss: 27431.3046875, Validation Loss: 27640.34375\n",
      "Epoch 21401/150000, Loss: 27421.34375, Validation Loss: 27630.341796875\n",
      "Epoch 21501/150000, Loss: 27411.390625, Validation Loss: 27620.341796875\n",
      "Epoch 21601/150000, Loss: 27401.435546875, Validation Loss: 27610.34765625\n",
      "Epoch 21701/150000, Loss: 27391.4921875, Validation Loss: 27600.359375\n",
      "Epoch 21801/150000, Loss: 27381.55078125, Validation Loss: 27590.375\n",
      "Epoch 21901/150000, Loss: 27371.6171875, Validation Loss: 27580.39453125\n",
      "Epoch 22001/150000, Loss: 27361.68359375, Validation Loss: 27570.42578125\n",
      "Epoch 22101/150000, Loss: 27351.7578125, Validation Loss: 27560.455078125\n",
      "Epoch 22201/150000, Loss: 27341.83984375, Validation Loss: 27550.490234375\n",
      "Epoch 22301/150000, Loss: 27331.919921875, Validation Loss: 27540.529296875\n",
      "Epoch 22401/150000, Loss: 27322.009765625, Validation Loss: 27530.578125\n",
      "Epoch 22501/150000, Loss: 27312.107421875, Validation Loss: 27520.62890625\n",
      "Epoch 22601/150000, Loss: 27302.205078125, Validation Loss: 27510.689453125\n",
      "Epoch 22701/150000, Loss: 27292.310546875, Validation Loss: 27500.748046875\n",
      "Epoch 22801/150000, Loss: 27282.421875, Validation Loss: 27490.814453125\n",
      "Epoch 22901/150000, Loss: 27272.53515625, Validation Loss: 27480.88671875\n",
      "Epoch 23001/150000, Loss: 27262.65234375, Validation Loss: 27470.9609375\n",
      "Epoch 23101/150000, Loss: 27252.779296875, Validation Loss: 27461.041015625\n",
      "Epoch 23201/150000, Loss: 27242.91015625, Validation Loss: 27451.130859375\n",
      "Epoch 23301/150000, Loss: 27233.04296875, Validation Loss: 27441.220703125\n",
      "Epoch 23401/150000, Loss: 27223.181640625, Validation Loss: 27431.3203125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Split the training data into sliding windows\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m x_train_seq, y_train_seq \u001b[38;5;241m=\u001b[39m \u001b[43msplit_data_with_sliding_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     98\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(x_train_seq)\n",
      "Cell \u001b[1;32mIn[6], line 79\u001b[0m, in \u001b[0;36msplit_data_with_sliding_window\u001b[1;34m(x_train_tensor, y_train_tensor, window_size)\u001b[0m\n\u001b[0;32m     76\u001b[0m     y_window \u001b[38;5;241m=\u001b[39m y_train_tensor[i\u001b[38;5;241m+\u001b[39mwindow_size]  \u001b[38;5;66;03m# Next entry as target output\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     x_seq_list\u001b[38;5;241m.\u001b[39mappend(x_window)\n\u001b[1;32m---> 79\u001b[0m     y_seq_list\u001b[38;5;241m.\u001b[39mappend(y_window)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Concatenate the lists into tensors\u001b[39;00m\n\u001b[0;32m     82\u001b[0m x_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(x_seq_list)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.005, 0.0001, 0.0005]\n",
    "window_sizes = [5]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.001, window_size=5\n",
      "Epoch 1/150000, Loss: 30035.376953125, Validation Loss: 30254.376953125\n",
      "Epoch 101/150000, Loss: 29941.64453125, Validation Loss: 30159.701171875\n",
      "Epoch 201/150000, Loss: 29762.03515625, Validation Loss: 29978.4375\n",
      "Epoch 301/150000, Loss: 29534.04296875, Validation Loss: 29750.015625\n",
      "Epoch 401/150000, Loss: 29365.16015625, Validation Loss: 29580.81640625\n",
      "Epoch 501/150000, Loss: 29221.162109375, Validation Loss: 29436.388671875\n",
      "Epoch 601/150000, Loss: 29089.0703125, Validation Loss: 29303.85546875\n",
      "Epoch 701/150000, Loss: 28964.087890625, Validation Loss: 29178.419921875\n",
      "Epoch 801/150000, Loss: 28843.923828125, Validation Loss: 29057.80859375\n",
      "Epoch 901/150000, Loss: 28727.314453125, Validation Loss: 28940.751953125\n",
      "Epoch 1001/150000, Loss: 28613.478515625, Validation Loss: 28826.4765625\n",
      "Epoch 1101/150000, Loss: 28501.904296875, Validation Loss: 28714.4609375\n",
      "Epoch 1201/150000, Loss: 28392.23046875, Validation Loss: 28604.353515625\n",
      "Epoch 1301/150000, Loss: 28284.205078125, Validation Loss: 28495.892578125\n",
      "Epoch 1401/150000, Loss: 28177.625, Validation Loss: 28388.8828125\n",
      "Epoch 1501/150000, Loss: 28072.345703125, Validation Loss: 28283.173828125\n",
      "Epoch 1601/150000, Loss: 27968.2421875, Validation Loss: 28178.646484375\n",
      "Epoch 1701/150000, Loss: 27865.2265625, Validation Loss: 28075.203125\n",
      "Epoch 1801/150000, Loss: 27763.21875, Validation Loss: 27972.771484375\n",
      "Epoch 1901/150000, Loss: 27662.150390625, Validation Loss: 27871.28125\n",
      "Epoch 2001/150000, Loss: 27561.96875, Validation Loss: 27770.6796875\n",
      "Epoch 2101/150000, Loss: 27462.62890625, Validation Loss: 27670.921875\n",
      "Epoch 2201/150000, Loss: 27364.091796875, Validation Loss: 27571.96484375\n",
      "Epoch 2301/150000, Loss: 27266.3203125, Validation Loss: 27473.775390625\n",
      "Epoch 2401/150000, Loss: 27169.28515625, Validation Loss: 27376.326171875\n",
      "Epoch 2501/150000, Loss: 27072.96484375, Validation Loss: 27279.5859375\n",
      "Epoch 2601/150000, Loss: 26977.330078125, Validation Loss: 27183.5390625\n",
      "Epoch 2701/150000, Loss: 26882.36328125, Validation Loss: 27088.158203125\n",
      "Epoch 2801/150000, Loss: 26788.048828125, Validation Loss: 26993.431640625\n",
      "Epoch 2901/150000, Loss: 26694.365234375, Validation Loss: 26899.33203125\n",
      "Epoch 3001/150000, Loss: 26601.302734375, Validation Loss: 26805.85546875\n",
      "Epoch 3101/150000, Loss: 26508.84375, Validation Loss: 26712.984375\n",
      "Epoch 3201/150000, Loss: 26416.982421875, Validation Loss: 26620.712890625\n",
      "Epoch 3301/150000, Loss: 26325.705078125, Validation Loss: 26529.0234375\n",
      "Epoch 3401/150000, Loss: 26235.001953125, Validation Loss: 26437.90625\n",
      "Epoch 3501/150000, Loss: 26144.86328125, Validation Loss: 26347.359375\n",
      "Epoch 3601/150000, Loss: 26055.28125, Validation Loss: 26257.369140625\n",
      "Epoch 3701/150000, Loss: 25966.25, Validation Loss: 26167.927734375\n",
      "Epoch 3801/150000, Loss: 25877.759765625, Validation Loss: 26079.029296875\n",
      "Epoch 3901/150000, Loss: 25789.80859375, Validation Loss: 25990.669921875\n",
      "Epoch 4001/150000, Loss: 25702.39453125, Validation Loss: 25902.84375\n",
      "Epoch 4101/150000, Loss: 25615.50390625, Validation Loss: 25815.55078125\n",
      "Epoch 4201/150000, Loss: 25529.13671875, Validation Loss: 25728.771484375\n",
      "Epoch 4301/150000, Loss: 25443.287109375, Validation Loss: 25642.515625\n",
      "Epoch 4401/150000, Loss: 25357.953125, Validation Loss: 25556.771484375\n",
      "Epoch 4501/150000, Loss: 25273.126953125, Validation Loss: 25471.5390625\n",
      "Epoch 4601/150000, Loss: 25188.80859375, Validation Loss: 25386.814453125\n",
      "Epoch 4701/150000, Loss: 25104.998046875, Validation Loss: 25302.595703125\n",
      "Epoch 4801/150000, Loss: 25021.689453125, Validation Loss: 25218.880859375\n",
      "Epoch 4901/150000, Loss: 24938.87890625, Validation Loss: 25135.662109375\n",
      "Epoch 5001/150000, Loss: 24856.5625, Validation Loss: 25052.9375\n",
      "Epoch 5101/150000, Loss: 24774.73828125, Validation Loss: 24970.712890625\n",
      "Epoch 5201/150000, Loss: 24693.408203125, Validation Loss: 24888.9765625\n",
      "Epoch 5301/150000, Loss: 24612.572265625, Validation Loss: 24807.732421875\n",
      "Epoch 5401/150000, Loss: 24532.220703125, Validation Loss: 24726.978515625\n",
      "Epoch 5501/150000, Loss: 24452.361328125, Validation Loss: 24646.708984375\n",
      "Epoch 5601/150000, Loss: 24372.984375, Validation Loss: 24566.931640625\n",
      "Epoch 5701/150000, Loss: 24294.095703125, Validation Loss: 24487.630859375\n",
      "Epoch 5801/150000, Loss: 24215.68359375, Validation Loss: 24408.8203125\n",
      "Epoch 5901/150000, Loss: 24137.755859375, Validation Loss: 24330.48828125\n",
      "Epoch 6001/150000, Loss: 24060.30859375, Validation Loss: 24252.638671875\n",
      "Epoch 6101/150000, Loss: 23983.34375, Validation Loss: 24175.263671875\n",
      "Epoch 6201/150000, Loss: 23906.8515625, Validation Loss: 24098.375\n",
      "Epoch 6301/150000, Loss: 23830.84375, Validation Loss: 24021.95703125\n",
      "Epoch 6401/150000, Loss: 23755.314453125, Validation Loss: 23946.021484375\n",
      "Epoch 6501/150000, Loss: 23680.25390625, Validation Loss: 23870.564453125\n",
      "Epoch 6601/150000, Loss: 23605.673828125, Validation Loss: 23795.578125\n",
      "Epoch 6701/150000, Loss: 23531.564453125, Validation Loss: 23721.06640625\n",
      "Epoch 6801/150000, Loss: 23457.9296875, Validation Loss: 23647.029296875\n",
      "Epoch 6901/150000, Loss: 23384.771484375, Validation Loss: 23573.46875\n",
      "Epoch 7001/150000, Loss: 23312.087890625, Validation Loss: 23500.380859375\n",
      "Epoch 7101/150000, Loss: 23239.87109375, Validation Loss: 23427.763671875\n",
      "Epoch 7201/150000, Loss: 23168.12890625, Validation Loss: 23355.6171875\n",
      "Epoch 7301/150000, Loss: 23096.857421875, Validation Loss: 23283.943359375\n",
      "Epoch 7401/150000, Loss: 23026.056640625, Validation Loss: 23212.744140625\n",
      "Epoch 7501/150000, Loss: 22955.728515625, Validation Loss: 23142.009765625\n",
      "Epoch 7601/150000, Loss: 22885.869140625, Validation Loss: 23071.748046875\n",
      "Epoch 7701/150000, Loss: 22816.478515625, Validation Loss: 23001.953125\n",
      "Epoch 7801/150000, Loss: 22747.5546875, Validation Loss: 22932.630859375\n",
      "Epoch 7901/150000, Loss: 22679.107421875, Validation Loss: 22863.78125\n",
      "Epoch 8001/150000, Loss: 22611.109375, Validation Loss: 22795.38671875\n",
      "Epoch 8101/150000, Loss: 22543.603515625, Validation Loss: 22727.478515625\n",
      "Epoch 8201/150000, Loss: 22476.548828125, Validation Loss: 22660.021484375\n",
      "Epoch 8301/150000, Loss: 22409.970703125, Validation Loss: 22593.044921875\n",
      "Epoch 8401/150000, Loss: 22343.859375, Validation Loss: 22526.529296875\n",
      "Epoch 8501/150000, Loss: 22278.203125, Validation Loss: 22460.470703125\n",
      "Epoch 8601/150000, Loss: 22213.029296875, Validation Loss: 22394.90234375\n",
      "Epoch 8701/150000, Loss: 22148.3046875, Validation Loss: 22329.771484375\n",
      "Epoch 8801/150000, Loss: 22084.0625, Validation Loss: 22265.134765625\n",
      "Epoch 8901/150000, Loss: 22020.271484375, Validation Loss: 22200.943359375\n",
      "Epoch 9001/150000, Loss: 21956.955078125, Validation Loss: 22137.224609375\n",
      "Epoch 9101/150000, Loss: 21894.099609375, Validation Loss: 22073.970703125\n",
      "Epoch 9201/150000, Loss: 21831.703125, Validation Loss: 22011.17578125\n",
      "Epoch 9301/150000, Loss: 21769.783203125, Validation Loss: 21948.85546875\n",
      "Epoch 9401/150000, Loss: 21708.30859375, Validation Loss: 21886.984375\n",
      "Epoch 9501/150000, Loss: 21647.322265625, Validation Loss: 21825.599609375\n",
      "Epoch 9601/150000, Loss: 21586.767578125, Validation Loss: 21764.646484375\n",
      "Epoch 9701/150000, Loss: 21526.708984375, Validation Loss: 21704.19140625\n",
      "Epoch 9801/150000, Loss: 21467.0859375, Validation Loss: 21644.16796875\n",
      "Epoch 9901/150000, Loss: 21407.94921875, Validation Loss: 21584.6328125\n",
      "Epoch 10001/150000, Loss: 21349.25, Validation Loss: 21525.537109375\n",
      "Epoch 10101/150000, Loss: 21291.03515625, Validation Loss: 21466.92578125\n",
      "Epoch 10201/150000, Loss: 21233.26171875, Validation Loss: 21408.751953125\n",
      "Epoch 10301/150000, Loss: 21175.966796875, Validation Loss: 21351.06640625\n",
      "Epoch 10401/150000, Loss: 21119.115234375, Validation Loss: 21293.810546875\n",
      "Epoch 10501/150000, Loss: 21062.744140625, Validation Loss: 21237.046875\n",
      "Epoch 10601/150000, Loss: 21006.810546875, Validation Loss: 21180.716796875\n",
      "Epoch 10701/150000, Loss: 20951.357421875, Validation Loss: 21124.8671875\n",
      "Epoch 10801/150000, Loss: 20896.345703125, Validation Loss: 21069.46484375\n",
      "Epoch 10901/150000, Loss: 20841.80859375, Validation Loss: 21014.529296875\n",
      "Epoch 11001/150000, Loss: 20787.71875, Validation Loss: 20960.04296875\n",
      "Epoch 11101/150000, Loss: 20734.091796875, Validation Loss: 20906.017578125\n",
      "Epoch 11201/150000, Loss: 20680.921875, Validation Loss: 20852.45703125\n",
      "Epoch 11301/150000, Loss: 20628.1953125, Validation Loss: 20799.3359375\n",
      "Epoch 11401/150000, Loss: 20538.79296875, Validation Loss: 20695.087890625\n",
      "Early stopping at epoch 11418 with validation loss 20472.1640625.\n",
      "Test Loss: 20280.560546875\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0001, window_size=5\n",
      "Epoch 1/150000, Loss: 29966.947265625, Validation Loss: 30186.34765625\n",
      "Epoch 101/150000, Loss: 29959.26953125, Validation Loss: 30178.65234375\n",
      "Epoch 201/150000, Loss: 29952.44140625, Validation Loss: 30171.810546875\n",
      "Epoch 301/150000, Loss: 29946.283203125, Validation Loss: 30165.630859375\n",
      "Epoch 401/150000, Loss: 29940.630859375, Validation Loss: 30159.966796875\n",
      "Epoch 501/150000, Loss: 29935.365234375, Validation Loss: 30154.68359375\n",
      "Epoch 601/150000, Loss: 29930.373046875, Validation Loss: 30149.67578125\n",
      "Epoch 701/150000, Loss: 29925.521484375, Validation Loss: 30144.814453125\n",
      "Epoch 801/150000, Loss: 29920.666015625, Validation Loss: 30139.939453125\n",
      "Epoch 901/150000, Loss: 29915.59375, Validation Loss: 30134.853515625\n",
      "Epoch 1001/150000, Loss: 29910.052734375, Validation Loss: 30129.296875\n",
      "Epoch 1101/150000, Loss: 29903.802734375, Validation Loss: 30123.029296875\n",
      "Epoch 1201/150000, Loss: 29896.630859375, Validation Loss: 30115.84375\n",
      "Epoch 1301/150000, Loss: 29888.376953125, Validation Loss: 30107.568359375\n",
      "Epoch 1401/150000, Loss: 29878.861328125, Validation Loss: 30098.03515625\n",
      "Epoch 1501/150000, Loss: 29867.84375, Validation Loss: 30087.00390625\n",
      "Epoch 1601/150000, Loss: 29854.92578125, Validation Loss: 30074.05078125\n",
      "Epoch 1701/150000, Loss: 29839.63671875, Validation Loss: 30058.728515625\n",
      "Epoch 1801/150000, Loss: 29821.62890625, Validation Loss: 30040.6640625\n",
      "Epoch 1901/150000, Loss: 29800.556640625, Validation Loss: 30019.509765625\n",
      "Epoch 2001/150000, Loss: 29775.962890625, Validation Loss: 29994.81640625\n",
      "Epoch 2101/150000, Loss: 29747.697265625, Validation Loss: 29966.421875\n",
      "Epoch 2201/150000, Loss: 29716.716796875, Validation Loss: 29935.310546875\n",
      "Epoch 2301/150000, Loss: 29685.251953125, Validation Loss: 29903.724609375\n",
      "Epoch 2401/150000, Loss: 29655.517578125, Validation Loss: 29873.87109375\n",
      "Epoch 2501/150000, Loss: 29628.52734375, Validation Loss: 29846.771484375\n",
      "Epoch 2601/150000, Loss: 29604.220703125, Validation Loss: 29822.365234375\n",
      "Epoch 2701/150000, Loss: 29582.044921875, Validation Loss: 29800.0859375\n",
      "Epoch 2801/150000, Loss: 29561.294921875, Validation Loss: 29779.248046875\n",
      "Epoch 2901/150000, Loss: 29541.212890625, Validation Loss: 29759.06640625\n",
      "Epoch 3001/150000, Loss: 29520.921875, Validation Loss: 29738.6796875\n",
      "Epoch 3101/150000, Loss: 29499.5234375, Validation Loss: 29717.1796875\n",
      "Epoch 3201/150000, Loss: 29476.591796875, Validation Loss: 29694.15625\n",
      "Epoch 3301/150000, Loss: 29452.671875, Validation Loss: 29670.138671875\n",
      "Epoch 3401/150000, Loss: 29428.83984375, Validation Loss: 29646.21875\n",
      "Epoch 3501/150000, Loss: 29405.91015625, Validation Loss: 29623.203125\n",
      "Epoch 3601/150000, Loss: 29384.234375, Validation Loss: 29601.4453125\n",
      "Epoch 3701/150000, Loss: 29363.841796875, Validation Loss: 29580.9765625\n",
      "Epoch 3801/150000, Loss: 29344.625, Validation Loss: 29561.67578125\n",
      "Epoch 3901/150000, Loss: 29326.419921875, Validation Loss: 29543.3984375\n",
      "Epoch 4001/150000, Loss: 29309.09375, Validation Loss: 29525.99609375\n",
      "Epoch 4101/150000, Loss: 29292.50390625, Validation Loss: 29509.3359375\n",
      "Epoch 4201/150000, Loss: 29276.546875, Validation Loss: 29493.310546875\n",
      "Epoch 4301/150000, Loss: 29261.12890625, Validation Loss: 29477.830078125\n",
      "Epoch 4401/150000, Loss: 29246.169921875, Validation Loss: 29462.80859375\n",
      "Epoch 4501/150000, Loss: 29231.6171875, Validation Loss: 29448.189453125\n",
      "Epoch 4601/150000, Loss: 29217.408203125, Validation Loss: 29433.921875\n",
      "Epoch 4701/150000, Loss: 29203.50390625, Validation Loss: 29419.958984375\n",
      "Epoch 4801/150000, Loss: 29189.869140625, Validation Loss: 29406.265625\n",
      "Epoch 4901/150000, Loss: 29176.470703125, Validation Loss: 29392.814453125\n",
      "Epoch 5001/150000, Loss: 29163.28125, Validation Loss: 29379.568359375\n",
      "Epoch 5101/150000, Loss: 29150.28515625, Validation Loss: 29366.517578125\n",
      "Epoch 5201/150000, Loss: 29137.45703125, Validation Loss: 29353.630859375\n",
      "Epoch 5301/150000, Loss: 29124.77734375, Validation Loss: 29340.904296875\n",
      "Epoch 5401/150000, Loss: 29112.23828125, Validation Loss: 29328.310546875\n",
      "Epoch 5501/150000, Loss: 29099.82421875, Validation Loss: 29315.84765625\n",
      "Epoch 5601/150000, Loss: 29087.52734375, Validation Loss: 29303.49609375\n",
      "Epoch 5701/150000, Loss: 29075.33203125, Validation Loss: 29291.251953125\n",
      "Epoch 5801/150000, Loss: 29063.23046875, Validation Loss: 29279.1015625\n",
      "Epoch 5901/150000, Loss: 29051.224609375, Validation Loss: 29267.041015625\n",
      "Epoch 6001/150000, Loss: 29039.291015625, Validation Loss: 29255.060546875\n",
      "Epoch 6101/150000, Loss: 29027.435546875, Validation Loss: 29243.158203125\n",
      "Epoch 6201/150000, Loss: 29015.654296875, Validation Loss: 29231.326171875\n",
      "Epoch 6301/150000, Loss: 29003.93359375, Validation Loss: 29219.556640625\n",
      "Epoch 6401/150000, Loss: 28992.275390625, Validation Loss: 29207.849609375\n",
      "Epoch 6501/150000, Loss: 28980.671875, Validation Loss: 29196.197265625\n",
      "Epoch 6601/150000, Loss: 28969.12109375, Validation Loss: 29184.6015625\n",
      "Epoch 6701/150000, Loss: 28957.623046875, Validation Loss: 29173.0546875\n",
      "Epoch 6801/150000, Loss: 28946.166015625, Validation Loss: 29161.55078125\n",
      "Epoch 6901/150000, Loss: 28934.755859375, Validation Loss: 29150.08984375\n",
      "Epoch 7001/150000, Loss: 28923.3828125, Validation Loss: 29138.673828125\n",
      "Epoch 7101/150000, Loss: 28912.05078125, Validation Loss: 29127.29296875\n",
      "Epoch 7201/150000, Loss: 28900.755859375, Validation Loss: 29115.951171875\n",
      "Epoch 7301/150000, Loss: 28889.49609375, Validation Loss: 29104.646484375\n",
      "Epoch 7401/150000, Loss: 28878.26953125, Validation Loss: 29093.37109375\n",
      "Epoch 7501/150000, Loss: 28867.072265625, Validation Loss: 29082.125\n",
      "Epoch 7601/150000, Loss: 28855.904296875, Validation Loss: 29070.916015625\n",
      "Epoch 7701/150000, Loss: 28844.765625, Validation Loss: 29059.728515625\n",
      "Epoch 7801/150000, Loss: 28833.654296875, Validation Loss: 29048.572265625\n",
      "Epoch 7901/150000, Loss: 28822.56640625, Validation Loss: 29037.443359375\n",
      "Epoch 8001/150000, Loss: 28811.5078125, Validation Loss: 29026.3359375\n",
      "Epoch 8101/150000, Loss: 28800.46875, Validation Loss: 29015.251953125\n",
      "Epoch 8201/150000, Loss: 28789.453125, Validation Loss: 29004.19140625\n",
      "Epoch 8301/150000, Loss: 28778.4609375, Validation Loss: 28993.15234375\n",
      "Epoch 8401/150000, Loss: 28767.482421875, Validation Loss: 28982.134765625\n",
      "Epoch 8501/150000, Loss: 28756.533203125, Validation Loss: 28971.130859375\n",
      "Epoch 8601/150000, Loss: 28745.603515625, Validation Loss: 28960.15625\n",
      "Epoch 8701/150000, Loss: 28734.6875, Validation Loss: 28949.1953125\n",
      "Epoch 8801/150000, Loss: 28723.791015625, Validation Loss: 28938.2578125\n",
      "Epoch 8901/150000, Loss: 28712.91015625, Validation Loss: 28927.33203125\n",
      "Epoch 9001/150000, Loss: 28702.044921875, Validation Loss: 28916.419921875\n",
      "Epoch 9101/150000, Loss: 28691.19921875, Validation Loss: 28905.533203125\n",
      "Epoch 9201/150000, Loss: 28680.3671875, Validation Loss: 28894.658203125\n",
      "Epoch 9301/150000, Loss: 28669.552734375, Validation Loss: 28883.798828125\n",
      "Epoch 9401/150000, Loss: 28658.75, Validation Loss: 28872.951171875\n",
      "Epoch 9501/150000, Loss: 28647.962890625, Validation Loss: 28862.11328125\n",
      "Epoch 9601/150000, Loss: 28637.18359375, Validation Loss: 28851.298828125\n",
      "Epoch 9701/150000, Loss: 28626.42578125, Validation Loss: 28840.48828125\n",
      "Epoch 9801/150000, Loss: 28615.67578125, Validation Loss: 28829.6953125\n",
      "Epoch 9901/150000, Loss: 28604.94140625, Validation Loss: 28818.916015625\n",
      "Epoch 10001/150000, Loss: 28594.22265625, Validation Loss: 28808.15625\n",
      "Epoch 10101/150000, Loss: 28583.513671875, Validation Loss: 28797.400390625\n",
      "Epoch 10201/150000, Loss: 28572.814453125, Validation Loss: 28786.658203125\n",
      "Epoch 10301/150000, Loss: 28562.126953125, Validation Loss: 28775.927734375\n",
      "Epoch 10401/150000, Loss: 28551.44921875, Validation Loss: 28765.208984375\n",
      "Epoch 10501/150000, Loss: 28540.787109375, Validation Loss: 28754.5\n",
      "Epoch 10601/150000, Loss: 28530.134765625, Validation Loss: 28743.802734375\n",
      "Epoch 10701/150000, Loss: 28519.490234375, Validation Loss: 28733.1171875\n",
      "Epoch 10801/150000, Loss: 28508.857421875, Validation Loss: 28722.4375\n",
      "Epoch 10901/150000, Loss: 28498.234375, Validation Loss: 28711.76953125\n",
      "Epoch 11001/150000, Loss: 28487.619140625, Validation Loss: 28701.111328125\n",
      "Epoch 11101/150000, Loss: 28477.013671875, Validation Loss: 28690.46484375\n",
      "Epoch 11201/150000, Loss: 28466.41796875, Validation Loss: 28679.82421875\n",
      "Epoch 11301/150000, Loss: 28455.828125, Validation Loss: 28669.189453125\n",
      "Epoch 11401/150000, Loss: 28445.251953125, Validation Loss: 28658.568359375\n",
      "Epoch 11501/150000, Loss: 28434.68359375, Validation Loss: 28647.958984375\n",
      "Epoch 11601/150000, Loss: 28424.126953125, Validation Loss: 28637.353515625\n",
      "Epoch 11701/150000, Loss: 28413.576171875, Validation Loss: 28626.763671875\n",
      "Epoch 11801/150000, Loss: 28403.037109375, Validation Loss: 28616.1796875\n",
      "Epoch 11901/150000, Loss: 28392.5078125, Validation Loss: 28605.607421875\n",
      "Epoch 12001/150000, Loss: 28381.98828125, Validation Loss: 28595.041015625\n",
      "Epoch 12101/150000, Loss: 28371.474609375, Validation Loss: 28584.484375\n",
      "Epoch 12201/150000, Loss: 28360.97265625, Validation Loss: 28573.9375\n",
      "Epoch 12301/150000, Loss: 28350.47265625, Validation Loss: 28563.39453125\n",
      "Epoch 12401/150000, Loss: 28339.982421875, Validation Loss: 28552.861328125\n",
      "Epoch 12501/150000, Loss: 28329.498046875, Validation Loss: 28542.33203125\n",
      "Epoch 12601/150000, Loss: 28319.021484375, Validation Loss: 28531.814453125\n",
      "Epoch 12701/150000, Loss: 28308.556640625, Validation Loss: 28521.302734375\n",
      "Epoch 12801/150000, Loss: 28298.09375, Validation Loss: 28510.798828125\n",
      "Epoch 12901/150000, Loss: 28287.642578125, Validation Loss: 28500.3046875\n",
      "Epoch 13001/150000, Loss: 28277.1953125, Validation Loss: 28489.81640625\n",
      "Epoch 13101/150000, Loss: 28266.7578125, Validation Loss: 28479.3359375\n",
      "Epoch 13201/150000, Loss: 28256.326171875, Validation Loss: 28468.859375\n",
      "Epoch 13301/150000, Loss: 28245.900390625, Validation Loss: 28458.388671875\n",
      "Epoch 13401/150000, Loss: 28235.48046875, Validation Loss: 28447.927734375\n",
      "Epoch 13501/150000, Loss: 28225.0703125, Validation Loss: 28437.470703125\n",
      "Epoch 13601/150000, Loss: 28214.666015625, Validation Loss: 28427.02734375\n",
      "Epoch 13701/150000, Loss: 28204.26953125, Validation Loss: 28416.5859375\n",
      "Epoch 13801/150000, Loss: 28193.87890625, Validation Loss: 28406.15234375\n",
      "Epoch 13901/150000, Loss: 28183.49609375, Validation Loss: 28395.724609375\n",
      "Epoch 14001/150000, Loss: 28173.1171875, Validation Loss: 28385.302734375\n",
      "Epoch 14101/150000, Loss: 28162.74609375, Validation Loss: 28374.888671875\n",
      "Epoch 14201/150000, Loss: 28152.380859375, Validation Loss: 28364.482421875\n",
      "Epoch 14301/150000, Loss: 28142.0234375, Validation Loss: 28354.078125\n",
      "Epoch 14401/150000, Loss: 28131.671875, Validation Loss: 28343.68359375\n",
      "Epoch 14501/150000, Loss: 28121.32421875, Validation Loss: 28333.29296875\n",
      "Epoch 14601/150000, Loss: 28110.982421875, Validation Loss: 28322.90625\n",
      "Epoch 14701/150000, Loss: 28100.65234375, Validation Loss: 28312.533203125\n",
      "Epoch 14801/150000, Loss: 28090.32421875, Validation Loss: 28302.1640625\n",
      "Epoch 14901/150000, Loss: 28080.00390625, Validation Loss: 28291.798828125\n",
      "Epoch 15001/150000, Loss: 28069.689453125, Validation Loss: 28281.443359375\n",
      "Epoch 15101/150000, Loss: 28059.3828125, Validation Loss: 28271.09375\n",
      "Epoch 15201/150000, Loss: 28049.076171875, Validation Loss: 28260.7421875\n",
      "Epoch 15301/150000, Loss: 28038.77734375, Validation Loss: 28250.400390625\n",
      "Epoch 15401/150000, Loss: 28028.484375, Validation Loss: 28240.06640625\n",
      "Epoch 15501/150000, Loss: 28018.197265625, Validation Loss: 28229.736328125\n",
      "Epoch 15601/150000, Loss: 28007.916015625, Validation Loss: 28219.41015625\n",
      "Epoch 15701/150000, Loss: 27997.638671875, Validation Loss: 28209.09375\n",
      "Epoch 15801/150000, Loss: 27987.369140625, Validation Loss: 28198.775390625\n",
      "Epoch 15901/150000, Loss: 27977.10546875, Validation Loss: 28188.470703125\n",
      "Epoch 16001/150000, Loss: 27966.845703125, Validation Loss: 28178.16796875\n",
      "Epoch 16101/150000, Loss: 27956.58984375, Validation Loss: 28167.87109375\n",
      "Epoch 16201/150000, Loss: 27946.34375, Validation Loss: 28157.580078125\n",
      "Epoch 16301/150000, Loss: 27936.103515625, Validation Loss: 28147.296875\n",
      "Epoch 16401/150000, Loss: 27925.865234375, Validation Loss: 28137.015625\n",
      "Epoch 16501/150000, Loss: 27915.638671875, Validation Loss: 28126.7421875\n",
      "Epoch 16601/150000, Loss: 27905.412109375, Validation Loss: 28116.47265625\n",
      "Epoch 16701/150000, Loss: 27895.1953125, Validation Loss: 28106.21484375\n",
      "Epoch 16801/150000, Loss: 27884.986328125, Validation Loss: 28095.96484375\n",
      "Epoch 16901/150000, Loss: 27874.78515625, Validation Loss: 28085.71875\n",
      "Epoch 17001/150000, Loss: 27864.587890625, Validation Loss: 28075.478515625\n",
      "Epoch 17101/150000, Loss: 27854.3984375, Validation Loss: 28065.2421875\n",
      "Epoch 17201/150000, Loss: 27844.2109375, Validation Loss: 28055.015625\n",
      "Epoch 17301/150000, Loss: 27834.03125, Validation Loss: 28044.791015625\n",
      "Epoch 17401/150000, Loss: 27823.857421875, Validation Loss: 28034.57421875\n",
      "Epoch 17501/150000, Loss: 27813.689453125, Validation Loss: 28024.359375\n",
      "Epoch 17601/150000, Loss: 27803.5234375, Validation Loss: 28014.15625\n",
      "Epoch 17701/150000, Loss: 27793.36328125, Validation Loss: 28003.951171875\n",
      "Epoch 17801/150000, Loss: 27783.208984375, Validation Loss: 27993.75390625\n",
      "Epoch 17901/150000, Loss: 27773.060546875, Validation Loss: 27983.5625\n",
      "Epoch 18001/150000, Loss: 27762.919921875, Validation Loss: 27973.376953125\n",
      "Epoch 18101/150000, Loss: 27752.78125, Validation Loss: 27963.197265625\n",
      "Epoch 18201/150000, Loss: 27742.646484375, Validation Loss: 27953.021484375\n",
      "Epoch 18301/150000, Loss: 27732.5234375, Validation Loss: 27942.853515625\n",
      "Epoch 18401/150000, Loss: 27722.404296875, Validation Loss: 27932.6953125\n",
      "Epoch 18501/150000, Loss: 27712.29296875, Validation Loss: 27922.5390625\n",
      "Epoch 18601/150000, Loss: 27702.185546875, Validation Loss: 27912.38671875\n",
      "Epoch 18701/150000, Loss: 27692.087890625, Validation Loss: 27902.2421875\n",
      "Epoch 18801/150000, Loss: 27681.98828125, Validation Loss: 27892.107421875\n",
      "Epoch 18901/150000, Loss: 27671.8984375, Validation Loss: 27881.970703125\n",
      "Epoch 19001/150000, Loss: 27661.810546875, Validation Loss: 27871.84375\n",
      "Epoch 19101/150000, Loss: 27651.73046875, Validation Loss: 27861.71875\n",
      "Epoch 19201/150000, Loss: 27641.65625, Validation Loss: 27851.599609375\n",
      "Epoch 19301/150000, Loss: 27631.5859375, Validation Loss: 27841.484375\n",
      "Epoch 19401/150000, Loss: 27621.5234375, Validation Loss: 27831.380859375\n",
      "Epoch 19501/150000, Loss: 27611.46484375, Validation Loss: 27821.28125\n",
      "Epoch 19601/150000, Loss: 27601.412109375, Validation Loss: 27811.185546875\n",
      "Epoch 19701/150000, Loss: 27591.3671875, Validation Loss: 27801.095703125\n",
      "Epoch 19801/150000, Loss: 27581.32421875, Validation Loss: 27791.009765625\n",
      "Epoch 19901/150000, Loss: 27571.2890625, Validation Loss: 27780.927734375\n",
      "Epoch 20001/150000, Loss: 27561.2578125, Validation Loss: 27770.853515625\n",
      "Epoch 20101/150000, Loss: 27551.228515625, Validation Loss: 27760.78515625\n",
      "Epoch 20201/150000, Loss: 27541.20703125, Validation Loss: 27750.720703125\n",
      "Epoch 20301/150000, Loss: 27531.19140625, Validation Loss: 27740.662109375\n",
      "Epoch 20401/150000, Loss: 27521.177734375, Validation Loss: 27730.607421875\n",
      "Epoch 20501/150000, Loss: 27511.173828125, Validation Loss: 27720.556640625\n",
      "Epoch 20601/150000, Loss: 27501.171875, Validation Loss: 27710.51171875\n",
      "Epoch 20701/150000, Loss: 27491.17578125, Validation Loss: 27700.47265625\n",
      "Epoch 20801/150000, Loss: 27481.18359375, Validation Loss: 27690.439453125\n",
      "Epoch 20901/150000, Loss: 27471.197265625, Validation Loss: 27680.41015625\n",
      "Epoch 21001/150000, Loss: 27461.216796875, Validation Loss: 27670.3828125\n",
      "Epoch 21101/150000, Loss: 27451.240234375, Validation Loss: 27660.365234375\n",
      "Epoch 21201/150000, Loss: 27441.26953125, Validation Loss: 27650.349609375\n",
      "Epoch 21301/150000, Loss: 27431.3046875, Validation Loss: 27640.34375\n",
      "Epoch 21401/150000, Loss: 27421.34375, Validation Loss: 27630.341796875\n",
      "Epoch 21501/150000, Loss: 27411.390625, Validation Loss: 27620.341796875\n",
      "Epoch 21601/150000, Loss: 27401.435546875, Validation Loss: 27610.34765625\n",
      "Epoch 21701/150000, Loss: 27391.4921875, Validation Loss: 27600.359375\n",
      "Epoch 21801/150000, Loss: 27381.55078125, Validation Loss: 27590.375\n",
      "Epoch 21901/150000, Loss: 27371.6171875, Validation Loss: 27580.39453125\n",
      "Epoch 22001/150000, Loss: 27361.68359375, Validation Loss: 27570.42578125\n",
      "Epoch 22101/150000, Loss: 27351.7578125, Validation Loss: 27560.455078125\n",
      "Epoch 22201/150000, Loss: 27341.83984375, Validation Loss: 27550.490234375\n",
      "Epoch 22301/150000, Loss: 27331.919921875, Validation Loss: 27540.529296875\n",
      "Epoch 22401/150000, Loss: 27322.009765625, Validation Loss: 27530.578125\n",
      "Epoch 22501/150000, Loss: 27312.107421875, Validation Loss: 27520.62890625\n",
      "Epoch 22601/150000, Loss: 27302.205078125, Validation Loss: 27510.689453125\n",
      "Epoch 22701/150000, Loss: 27292.310546875, Validation Loss: 27500.748046875\n",
      "Epoch 22801/150000, Loss: 27282.421875, Validation Loss: 27490.814453125\n",
      "Epoch 22901/150000, Loss: 27272.53515625, Validation Loss: 27480.88671875\n",
      "Epoch 23001/150000, Loss: 27262.65234375, Validation Loss: 27470.9609375\n",
      "Epoch 23101/150000, Loss: 27252.779296875, Validation Loss: 27461.041015625\n",
      "Epoch 23201/150000, Loss: 27242.91015625, Validation Loss: 27451.130859375\n",
      "Epoch 23301/150000, Loss: 27233.04296875, Validation Loss: 27441.220703125\n",
      "Epoch 23401/150000, Loss: 27223.181640625, Validation Loss: 27431.3203125\n",
      "Epoch 23501/150000, Loss: 27213.32421875, Validation Loss: 27421.419921875\n",
      "Epoch 23601/150000, Loss: 27203.474609375, Validation Loss: 27411.52734375\n",
      "Epoch 23701/150000, Loss: 27193.62890625, Validation Loss: 27401.638671875\n",
      "Epoch 23801/150000, Loss: 27183.791015625, Validation Loss: 27391.75390625\n",
      "Epoch 23901/150000, Loss: 27173.951171875, Validation Loss: 27381.876953125\n",
      "Epoch 24001/150000, Loss: 27164.12109375, Validation Loss: 27372.00390625\n",
      "Epoch 24101/150000, Loss: 27154.30078125, Validation Loss: 27362.130859375\n",
      "Epoch 24201/150000, Loss: 27144.4765625, Validation Loss: 27352.26953125\n",
      "Epoch 24301/150000, Loss: 27134.66015625, Validation Loss: 27342.4140625\n",
      "Epoch 24401/150000, Loss: 27124.8515625, Validation Loss: 27332.556640625\n",
      "Epoch 24501/150000, Loss: 27115.044921875, Validation Loss: 27322.708984375\n",
      "Epoch 24601/150000, Loss: 27105.24609375, Validation Loss: 27312.865234375\n",
      "Epoch 24701/150000, Loss: 27095.4453125, Validation Loss: 27303.02734375\n",
      "Epoch 24801/150000, Loss: 27085.66015625, Validation Loss: 27293.19140625\n",
      "Epoch 24901/150000, Loss: 27075.873046875, Validation Loss: 27283.365234375\n",
      "Epoch 25001/150000, Loss: 27066.08984375, Validation Loss: 27273.5390625\n",
      "Epoch 25101/150000, Loss: 27056.314453125, Validation Loss: 27263.724609375\n",
      "Epoch 25201/150000, Loss: 27046.544921875, Validation Loss: 27253.904296875\n",
      "Epoch 25301/150000, Loss: 27036.78125, Validation Loss: 27244.099609375\n",
      "Epoch 25401/150000, Loss: 27027.015625, Validation Loss: 27234.29296875\n",
      "Epoch 25501/150000, Loss: 27017.259765625, Validation Loss: 27224.49609375\n",
      "Epoch 25601/150000, Loss: 27007.51171875, Validation Loss: 27214.703125\n",
      "Epoch 25701/150000, Loss: 26997.76171875, Validation Loss: 27204.91015625\n",
      "Epoch 25801/150000, Loss: 26988.021484375, Validation Loss: 27195.12890625\n",
      "Epoch 25901/150000, Loss: 26978.2890625, Validation Loss: 27185.349609375\n",
      "Epoch 26001/150000, Loss: 26968.5546875, Validation Loss: 27175.57421875\n",
      "Epoch 26101/150000, Loss: 26958.830078125, Validation Loss: 27165.8046875\n",
      "Epoch 26201/150000, Loss: 26949.107421875, Validation Loss: 27156.041015625\n",
      "Epoch 26301/150000, Loss: 26939.39453125, Validation Loss: 27146.28515625\n",
      "Epoch 26401/150000, Loss: 26929.681640625, Validation Loss: 27136.52734375\n",
      "Epoch 26501/150000, Loss: 26919.974609375, Validation Loss: 27126.78125\n",
      "Epoch 26601/150000, Loss: 26910.2734375, Validation Loss: 27117.03515625\n",
      "Epoch 26701/150000, Loss: 26900.576171875, Validation Loss: 27107.296875\n",
      "Epoch 26801/150000, Loss: 26890.884765625, Validation Loss: 27097.5625\n",
      "Epoch 26901/150000, Loss: 26881.19921875, Validation Loss: 27087.830078125\n",
      "Epoch 27001/150000, Loss: 26871.515625, Validation Loss: 27078.10546875\n",
      "Epoch 27101/150000, Loss: 26861.841796875, Validation Loss: 27068.388671875\n",
      "Epoch 27201/150000, Loss: 26852.166015625, Validation Loss: 27058.673828125\n",
      "Epoch 27301/150000, Loss: 26842.501953125, Validation Loss: 27048.96484375\n",
      "Epoch 27401/150000, Loss: 26832.84375, Validation Loss: 27039.25390625\n",
      "Epoch 27501/150000, Loss: 26823.185546875, Validation Loss: 27029.556640625\n",
      "Epoch 27601/150000, Loss: 26813.53515625, Validation Loss: 27019.865234375\n",
      "Epoch 27701/150000, Loss: 26803.884765625, Validation Loss: 27010.173828125\n",
      "Epoch 27801/150000, Loss: 26794.244140625, Validation Loss: 27000.48828125\n",
      "Epoch 27901/150000, Loss: 26784.607421875, Validation Loss: 26990.80859375\n",
      "Epoch 28001/150000, Loss: 26774.974609375, Validation Loss: 26981.134765625\n",
      "Epoch 28101/150000, Loss: 26765.34765625, Validation Loss: 26971.466796875\n",
      "Epoch 28201/150000, Loss: 26755.720703125, Validation Loss: 26961.798828125\n",
      "Epoch 28301/150000, Loss: 26746.103515625, Validation Loss: 26952.140625\n",
      "Epoch 28401/150000, Loss: 26736.49609375, Validation Loss: 26942.482421875\n",
      "Epoch 28501/150000, Loss: 26726.888671875, Validation Loss: 26932.83203125\n",
      "Epoch 28601/150000, Loss: 26717.287109375, Validation Loss: 26923.189453125\n",
      "Epoch 28701/150000, Loss: 26707.685546875, Validation Loss: 26913.548828125\n",
      "Epoch 28801/150000, Loss: 26698.09375, Validation Loss: 26903.916015625\n",
      "Epoch 28901/150000, Loss: 26688.509765625, Validation Loss: 26894.28125\n",
      "Epoch 29001/150000, Loss: 26678.923828125, Validation Loss: 26884.65625\n",
      "Epoch 29101/150000, Loss: 26669.349609375, Validation Loss: 26875.0390625\n",
      "Epoch 29201/150000, Loss: 26659.775390625, Validation Loss: 26865.419921875\n",
      "Epoch 29301/150000, Loss: 26650.20703125, Validation Loss: 26855.80859375\n",
      "Epoch 29401/150000, Loss: 26640.6484375, Validation Loss: 26846.203125\n",
      "Epoch 29501/150000, Loss: 26631.087890625, Validation Loss: 26836.60546875\n",
      "Epoch 29601/150000, Loss: 26621.53515625, Validation Loss: 26827.005859375\n",
      "Epoch 29701/150000, Loss: 26611.986328125, Validation Loss: 26817.419921875\n",
      "Epoch 29801/150000, Loss: 26602.443359375, Validation Loss: 26807.830078125\n",
      "Epoch 29901/150000, Loss: 26592.908203125, Validation Loss: 26798.251953125\n",
      "Epoch 30001/150000, Loss: 26583.373046875, Validation Loss: 26788.67578125\n",
      "Epoch 30101/150000, Loss: 26573.84375, Validation Loss: 26779.10546875\n",
      "Epoch 30201/150000, Loss: 26564.322265625, Validation Loss: 26769.533203125\n",
      "Epoch 30301/150000, Loss: 26554.802734375, Validation Loss: 26759.9765625\n",
      "Epoch 30401/150000, Loss: 26545.2890625, Validation Loss: 26750.419921875\n",
      "Epoch 30501/150000, Loss: 26535.779296875, Validation Loss: 26740.869140625\n",
      "Epoch 30601/150000, Loss: 26526.27734375, Validation Loss: 26731.32421875\n",
      "Epoch 30701/150000, Loss: 26516.779296875, Validation Loss: 26721.78515625\n",
      "Epoch 30801/150000, Loss: 26507.283203125, Validation Loss: 26712.2421875\n",
      "Epoch 30901/150000, Loss: 26497.798828125, Validation Loss: 26702.712890625\n",
      "Epoch 31001/150000, Loss: 26488.3125, Validation Loss: 26693.18359375\n",
      "Epoch 31101/150000, Loss: 26478.8359375, Validation Loss: 26683.66796875\n",
      "Epoch 31201/150000, Loss: 26469.36328125, Validation Loss: 26674.146484375\n",
      "Epoch 31301/150000, Loss: 26459.892578125, Validation Loss: 26664.638671875\n",
      "Epoch 31401/150000, Loss: 26450.43359375, Validation Loss: 26655.12890625\n",
      "Epoch 31501/150000, Loss: 26440.970703125, Validation Loss: 26645.62890625\n",
      "Epoch 31601/150000, Loss: 26431.515625, Validation Loss: 26636.130859375\n",
      "Epoch 31701/150000, Loss: 26422.0703125, Validation Loss: 26626.638671875\n",
      "Epoch 31801/150000, Loss: 26412.62109375, Validation Loss: 26617.15234375\n",
      "Epoch 31901/150000, Loss: 26403.185546875, Validation Loss: 26607.669921875\n",
      "Epoch 32001/150000, Loss: 26393.75, Validation Loss: 26598.19140625\n",
      "Epoch 32101/150000, Loss: 26384.3203125, Validation Loss: 26588.720703125\n",
      "Epoch 32201/150000, Loss: 26374.900390625, Validation Loss: 26579.25390625\n",
      "Epoch 32301/150000, Loss: 26365.4765625, Validation Loss: 26569.791015625\n",
      "Epoch 32401/150000, Loss: 26356.064453125, Validation Loss: 26560.3359375\n",
      "Epoch 32501/150000, Loss: 26346.65625, Validation Loss: 26550.880859375\n",
      "Epoch 32601/150000, Loss: 26337.25, Validation Loss: 26541.43359375\n",
      "Epoch 32701/150000, Loss: 26327.8515625, Validation Loss: 26531.990234375\n",
      "Epoch 32801/150000, Loss: 26318.453125, Validation Loss: 26522.55078125\n",
      "Epoch 32901/150000, Loss: 26309.06640625, Validation Loss: 26513.123046875\n",
      "Epoch 33001/150000, Loss: 26299.677734375, Validation Loss: 26503.69140625\n",
      "Epoch 33101/150000, Loss: 26290.30078125, Validation Loss: 26494.26953125\n",
      "Epoch 33201/150000, Loss: 26280.92578125, Validation Loss: 26484.853515625\n",
      "Epoch 33301/150000, Loss: 26271.556640625, Validation Loss: 26475.4375\n",
      "Epoch 33401/150000, Loss: 26262.19140625, Validation Loss: 26466.033203125\n",
      "Epoch 33501/150000, Loss: 26252.830078125, Validation Loss: 26456.62890625\n",
      "Epoch 33601/150000, Loss: 26243.4765625, Validation Loss: 26447.23046875\n",
      "Epoch 33701/150000, Loss: 26234.12890625, Validation Loss: 26437.837890625\n",
      "Epoch 33801/150000, Loss: 26224.779296875, Validation Loss: 26428.451171875\n",
      "Epoch 33901/150000, Loss: 26215.44140625, Validation Loss: 26419.068359375\n",
      "Epoch 34001/150000, Loss: 26206.10546875, Validation Loss: 26409.689453125\n",
      "Epoch 34101/150000, Loss: 26196.775390625, Validation Loss: 26400.314453125\n",
      "Epoch 34201/150000, Loss: 26187.451171875, Validation Loss: 26390.951171875\n",
      "Epoch 34301/150000, Loss: 26178.130859375, Validation Loss: 26381.583984375\n",
      "Epoch 34401/150000, Loss: 26168.814453125, Validation Loss: 26372.228515625\n",
      "Epoch 34501/150000, Loss: 26159.505859375, Validation Loss: 26362.87109375\n",
      "Epoch 34601/150000, Loss: 26150.197265625, Validation Loss: 26353.5234375\n",
      "Epoch 34701/150000, Loss: 26140.896484375, Validation Loss: 26344.18359375\n",
      "Epoch 34801/150000, Loss: 26131.6015625, Validation Loss: 26334.841796875\n",
      "Epoch 34901/150000, Loss: 26122.3125, Validation Loss: 26325.509765625\n",
      "Epoch 35001/150000, Loss: 26113.025390625, Validation Loss: 26316.17578125\n",
      "Epoch 35101/150000, Loss: 26103.7421875, Validation Loss: 26306.853515625\n",
      "Epoch 35201/150000, Loss: 26094.46875, Validation Loss: 26297.5390625\n",
      "Epoch 35301/150000, Loss: 26085.19140625, Validation Loss: 26288.21875\n",
      "Epoch 35401/150000, Loss: 26075.921875, Validation Loss: 26278.904296875\n",
      "Epoch 35501/150000, Loss: 26066.658203125, Validation Loss: 26269.595703125\n",
      "Epoch 35601/150000, Loss: 26057.3984375, Validation Loss: 26260.291015625\n",
      "Epoch 35701/150000, Loss: 26048.140625, Validation Loss: 26250.994140625\n",
      "Epoch 35801/150000, Loss: 26038.888671875, Validation Loss: 26241.701171875\n",
      "Epoch 35901/150000, Loss: 26029.64453125, Validation Loss: 26232.41015625\n",
      "Epoch 36001/150000, Loss: 26020.404296875, Validation Loss: 26223.12890625\n",
      "Epoch 36101/150000, Loss: 26011.16796875, Validation Loss: 26213.849609375\n",
      "Epoch 36201/150000, Loss: 26001.935546875, Validation Loss: 26204.57421875\n",
      "Epoch 36301/150000, Loss: 25992.708984375, Validation Loss: 26195.3046875\n",
      "Epoch 36401/150000, Loss: 25983.490234375, Validation Loss: 26186.041015625\n",
      "Epoch 36501/150000, Loss: 25974.275390625, Validation Loss: 26176.78125\n",
      "Epoch 36601/150000, Loss: 25965.0625, Validation Loss: 26167.52734375\n",
      "Epoch 36701/150000, Loss: 25955.85546875, Validation Loss: 26158.28125\n",
      "Epoch 36801/150000, Loss: 25946.654296875, Validation Loss: 26149.03515625\n",
      "Epoch 36901/150000, Loss: 25937.455078125, Validation Loss: 26139.79296875\n",
      "Epoch 37001/150000, Loss: 25928.265625, Validation Loss: 26130.556640625\n",
      "Epoch 37101/150000, Loss: 25919.072265625, Validation Loss: 26121.32421875\n",
      "Epoch 37201/150000, Loss: 25909.88671875, Validation Loss: 26112.095703125\n",
      "Epoch 37301/150000, Loss: 25900.70703125, Validation Loss: 26102.869140625\n",
      "Epoch 37401/150000, Loss: 25891.529296875, Validation Loss: 26093.65234375\n",
      "Epoch 37501/150000, Loss: 25882.359375, Validation Loss: 26084.4375\n",
      "Epoch 37601/150000, Loss: 25873.1953125, Validation Loss: 26075.23046875\n",
      "Epoch 37701/150000, Loss: 25864.033203125, Validation Loss: 26066.0234375\n",
      "Epoch 37801/150000, Loss: 25854.875, Validation Loss: 26056.830078125\n",
      "Epoch 37901/150000, Loss: 25845.724609375, Validation Loss: 26047.630859375\n",
      "Epoch 38001/150000, Loss: 25836.580078125, Validation Loss: 26038.443359375\n",
      "Epoch 38101/150000, Loss: 25827.43359375, Validation Loss: 26029.25390625\n",
      "Epoch 38201/150000, Loss: 25818.30078125, Validation Loss: 26020.080078125\n",
      "Epoch 38301/150000, Loss: 25809.171875, Validation Loss: 26010.904296875\n",
      "Epoch 38401/150000, Loss: 25800.04296875, Validation Loss: 26001.736328125\n",
      "Epoch 38501/150000, Loss: 25790.9140625, Validation Loss: 25992.5625\n",
      "Epoch 38601/150000, Loss: 25781.7890625, Validation Loss: 25983.39453125\n",
      "Epoch 38701/150000, Loss: 25772.669921875, Validation Loss: 25974.23046875\n",
      "Epoch 38801/150000, Loss: 25763.552734375, Validation Loss: 25965.072265625\n",
      "Epoch 38901/150000, Loss: 25754.443359375, Validation Loss: 25955.919921875\n",
      "Epoch 39001/150000, Loss: 25745.33984375, Validation Loss: 25946.771484375\n",
      "Epoch 39101/150000, Loss: 25736.236328125, Validation Loss: 25937.625\n",
      "Epoch 39201/150000, Loss: 25727.140625, Validation Loss: 25928.490234375\n",
      "Epoch 39301/150000, Loss: 25718.05078125, Validation Loss: 25919.35546875\n",
      "Epoch 39401/150000, Loss: 25708.962890625, Validation Loss: 25910.224609375\n",
      "Epoch 39501/150000, Loss: 25699.8828125, Validation Loss: 25901.099609375\n",
      "Epoch 39601/150000, Loss: 25690.80078125, Validation Loss: 25891.97265625\n",
      "Epoch 39701/150000, Loss: 25681.724609375, Validation Loss: 25882.85546875\n",
      "Epoch 39801/150000, Loss: 25672.65625, Validation Loss: 25873.74609375\n",
      "Epoch 39901/150000, Loss: 25663.58984375, Validation Loss: 25864.638671875\n",
      "Epoch 40001/150000, Loss: 25654.53125, Validation Loss: 25855.533203125\n",
      "Epoch 40101/150000, Loss: 25645.474609375, Validation Loss: 25846.4375\n",
      "Epoch 40201/150000, Loss: 25636.42578125, Validation Loss: 25837.341796875\n",
      "Epoch 40301/150000, Loss: 25627.37890625, Validation Loss: 25828.25390625\n",
      "Epoch 40401/150000, Loss: 25618.33984375, Validation Loss: 25819.169921875\n",
      "Epoch 40501/150000, Loss: 25609.306640625, Validation Loss: 25810.09375\n",
      "Epoch 40601/150000, Loss: 25600.27734375, Validation Loss: 25801.017578125\n",
      "Epoch 40701/150000, Loss: 25591.248046875, Validation Loss: 25791.951171875\n",
      "Epoch 40801/150000, Loss: 25582.2265625, Validation Loss: 25782.88671875\n",
      "Epoch 40901/150000, Loss: 25573.212890625, Validation Loss: 25773.826171875\n",
      "Epoch 41001/150000, Loss: 25564.201171875, Validation Loss: 25764.775390625\n",
      "Epoch 41101/150000, Loss: 25555.1953125, Validation Loss: 25755.724609375\n",
      "Epoch 41201/150000, Loss: 25546.1953125, Validation Loss: 25746.68359375\n",
      "Epoch 41301/150000, Loss: 25537.201171875, Validation Loss: 25737.64453125\n",
      "Epoch 41401/150000, Loss: 25528.2109375, Validation Loss: 25728.611328125\n",
      "Epoch 41501/150000, Loss: 25519.22265625, Validation Loss: 25719.580078125\n",
      "Epoch 41601/150000, Loss: 25510.244140625, Validation Loss: 25710.556640625\n",
      "Epoch 41701/150000, Loss: 25501.26953125, Validation Loss: 25701.5390625\n",
      "Epoch 41801/150000, Loss: 25492.296875, Validation Loss: 25692.5234375\n",
      "Epoch 41901/150000, Loss: 25483.330078125, Validation Loss: 25683.515625\n",
      "Epoch 42001/150000, Loss: 25474.37109375, Validation Loss: 25674.51171875\n",
      "Epoch 42101/150000, Loss: 25465.4140625, Validation Loss: 25665.51171875\n",
      "Epoch 42201/150000, Loss: 25456.462890625, Validation Loss: 25656.517578125\n",
      "Epoch 42301/150000, Loss: 25447.515625, Validation Loss: 25647.529296875\n",
      "Epoch 42401/150000, Loss: 25438.576171875, Validation Loss: 25638.544921875\n",
      "Epoch 42501/150000, Loss: 25429.640625, Validation Loss: 25629.568359375\n",
      "Epoch 42601/150000, Loss: 25420.708984375, Validation Loss: 25620.59375\n",
      "Epoch 42701/150000, Loss: 25411.783203125, Validation Loss: 25611.623046875\n",
      "Epoch 42801/150000, Loss: 25402.86328125, Validation Loss: 25602.658203125\n",
      "Epoch 42901/150000, Loss: 25393.9453125, Validation Loss: 25593.701171875\n",
      "Epoch 43001/150000, Loss: 25385.033203125, Validation Loss: 25584.7421875\n",
      "Epoch 43101/150000, Loss: 25376.12890625, Validation Loss: 25575.796875\n",
      "Epoch 43201/150000, Loss: 25367.2265625, Validation Loss: 25566.849609375\n",
      "Epoch 43301/150000, Loss: 25358.330078125, Validation Loss: 25557.9140625\n",
      "Epoch 43401/150000, Loss: 25349.4375, Validation Loss: 25548.978515625\n",
      "Epoch 43501/150000, Loss: 25340.552734375, Validation Loss: 25540.048828125\n",
      "Epoch 43601/150000, Loss: 25331.671875, Validation Loss: 25531.125\n",
      "Epoch 43701/150000, Loss: 25322.794921875, Validation Loss: 25522.20703125\n",
      "Epoch 43801/150000, Loss: 25313.92578125, Validation Loss: 25513.291015625\n",
      "Epoch 43901/150000, Loss: 25305.05859375, Validation Loss: 25504.380859375\n",
      "Epoch 44001/150000, Loss: 25296.197265625, Validation Loss: 25495.478515625\n",
      "Epoch 44101/150000, Loss: 25287.337890625, Validation Loss: 25486.578125\n",
      "Epoch 44201/150000, Loss: 25278.490234375, Validation Loss: 25477.68359375\n",
      "Epoch 44301/150000, Loss: 25269.640625, Validation Loss: 25468.79296875\n",
      "Epoch 44401/150000, Loss: 25260.802734375, Validation Loss: 25459.90625\n",
      "Epoch 44501/150000, Loss: 25251.9609375, Validation Loss: 25451.02734375\n",
      "Epoch 44601/150000, Loss: 25243.1328125, Validation Loss: 25442.15625\n",
      "Epoch 44701/150000, Loss: 25234.302734375, Validation Loss: 25433.28515625\n",
      "Epoch 44801/150000, Loss: 25225.482421875, Validation Loss: 25424.419921875\n",
      "Epoch 44901/150000, Loss: 25216.666015625, Validation Loss: 25415.560546875\n",
      "Epoch 45001/150000, Loss: 25207.853515625, Validation Loss: 25406.703125\n",
      "Epoch 45101/150000, Loss: 25199.046875, Validation Loss: 25397.853515625\n",
      "Epoch 45201/150000, Loss: 25190.24609375, Validation Loss: 25389.01171875\n",
      "Epoch 45301/150000, Loss: 25181.44921875, Validation Loss: 25380.16796875\n",
      "Epoch 45401/150000, Loss: 25172.65625, Validation Loss: 25371.33203125\n",
      "Epoch 45501/150000, Loss: 25163.87109375, Validation Loss: 25362.5\n",
      "Epoch 45601/150000, Loss: 25155.0859375, Validation Loss: 25353.68359375\n",
      "Epoch 45701/150000, Loss: 25146.310546875, Validation Loss: 25344.85546875\n",
      "Epoch 45801/150000, Loss: 25137.541015625, Validation Loss: 25336.044921875\n",
      "Epoch 45901/150000, Loss: 25128.771484375, Validation Loss: 25327.234375\n",
      "Epoch 46001/150000, Loss: 25120.0078125, Validation Loss: 25318.427734375\n",
      "Epoch 46101/150000, Loss: 25111.248046875, Validation Loss: 25309.62890625\n",
      "Epoch 46201/150000, Loss: 25102.501953125, Validation Loss: 25300.83203125\n",
      "Epoch 46301/150000, Loss: 25093.75, Validation Loss: 25292.0390625\n",
      "Epoch 46401/150000, Loss: 25085.009765625, Validation Loss: 25283.2578125\n",
      "Epoch 46501/150000, Loss: 25076.271484375, Validation Loss: 25274.4765625\n",
      "Epoch 46601/150000, Loss: 25067.541015625, Validation Loss: 25265.701171875\n",
      "Epoch 46701/150000, Loss: 25058.810546875, Validation Loss: 25256.927734375\n",
      "Epoch 46801/150000, Loss: 25050.08984375, Validation Loss: 25248.1640625\n",
      "Epoch 46901/150000, Loss: 25041.37109375, Validation Loss: 25239.404296875\n",
      "Epoch 47001/150000, Loss: 25032.658203125, Validation Loss: 25230.646484375\n",
      "Epoch 47101/150000, Loss: 25023.953125, Validation Loss: 25221.8984375\n",
      "Epoch 47201/150000, Loss: 25015.24609375, Validation Loss: 25213.15234375\n",
      "Epoch 47301/150000, Loss: 25006.55078125, Validation Loss: 25204.4140625\n",
      "Epoch 47401/150000, Loss: 24997.859375, Validation Loss: 25195.673828125\n",
      "Epoch 47501/150000, Loss: 24989.169921875, Validation Loss: 25186.943359375\n",
      "Epoch 47601/150000, Loss: 24980.48828125, Validation Loss: 25178.21875\n",
      "Epoch 47701/150000, Loss: 24971.80859375, Validation Loss: 25169.49609375\n",
      "Epoch 47801/150000, Loss: 24963.138671875, Validation Loss: 25160.78125\n",
      "Epoch 47901/150000, Loss: 24954.470703125, Validation Loss: 25152.068359375\n",
      "Epoch 48001/150000, Loss: 24945.806640625, Validation Loss: 25143.365234375\n",
      "Epoch 48101/150000, Loss: 24937.1484375, Validation Loss: 25134.662109375\n",
      "Epoch 48201/150000, Loss: 24928.49609375, Validation Loss: 25125.970703125\n",
      "Epoch 48301/150000, Loss: 24919.84765625, Validation Loss: 25117.275390625\n",
      "Epoch 48401/150000, Loss: 24911.203125, Validation Loss: 25108.587890625\n",
      "Epoch 48501/150000, Loss: 24902.56640625, Validation Loss: 25099.91015625\n",
      "Epoch 48601/150000, Loss: 24893.935546875, Validation Loss: 25091.234375\n",
      "Epoch 48701/150000, Loss: 24885.302734375, Validation Loss: 25082.560546875\n",
      "Epoch 48801/150000, Loss: 24876.681640625, Validation Loss: 25073.8984375\n",
      "Epoch 48901/150000, Loss: 24868.0625, Validation Loss: 25065.234375\n",
      "Epoch 49001/150000, Loss: 24859.44921875, Validation Loss: 25056.578125\n",
      "Epoch 49101/150000, Loss: 24850.84375, Validation Loss: 25047.92578125\n",
      "Epoch 49201/150000, Loss: 24842.240234375, Validation Loss: 25039.279296875\n",
      "Epoch 49301/150000, Loss: 24833.638671875, Validation Loss: 25030.638671875\n",
      "Epoch 49401/150000, Loss: 24825.046875, Validation Loss: 25022.001953125\n",
      "Epoch 49501/150000, Loss: 24816.45703125, Validation Loss: 25013.37109375\n",
      "Epoch 49601/150000, Loss: 24807.873046875, Validation Loss: 25004.744140625\n",
      "Epoch 49701/150000, Loss: 24799.296875, Validation Loss: 24996.125\n",
      "Epoch 49801/150000, Loss: 24790.720703125, Validation Loss: 24987.505859375\n",
      "Epoch 49901/150000, Loss: 24782.15625, Validation Loss: 24978.89453125\n",
      "Epoch 50001/150000, Loss: 24773.591796875, Validation Loss: 24970.287109375\n",
      "Epoch 50101/150000, Loss: 24765.03125, Validation Loss: 24961.685546875\n",
      "Epoch 50201/150000, Loss: 24756.478515625, Validation Loss: 24953.08984375\n",
      "Epoch 50301/150000, Loss: 24747.931640625, Validation Loss: 24944.498046875\n",
      "Epoch 50401/150000, Loss: 24739.384765625, Validation Loss: 24935.91015625\n",
      "Epoch 50501/150000, Loss: 24730.845703125, Validation Loss: 24927.330078125\n",
      "Epoch 50601/150000, Loss: 24722.314453125, Validation Loss: 24918.751953125\n",
      "Epoch 50701/150000, Loss: 24713.78515625, Validation Loss: 24910.17578125\n",
      "Epoch 50801/150000, Loss: 24705.263671875, Validation Loss: 24901.615234375\n",
      "Epoch 50901/150000, Loss: 24696.740234375, Validation Loss: 24893.048828125\n",
      "Epoch 51001/150000, Loss: 24688.2265625, Validation Loss: 24884.494140625\n",
      "Epoch 51101/150000, Loss: 24679.71875, Validation Loss: 24875.943359375\n",
      "Epoch 51201/150000, Loss: 24671.21484375, Validation Loss: 24867.39453125\n",
      "Epoch 51301/150000, Loss: 24662.71484375, Validation Loss: 24858.8515625\n",
      "Epoch 51401/150000, Loss: 24654.220703125, Validation Loss: 24850.31640625\n",
      "Epoch 51501/150000, Loss: 24645.732421875, Validation Loss: 24841.78515625\n",
      "Epoch 51601/150000, Loss: 24637.248046875, Validation Loss: 24833.2578125\n",
      "Epoch 51701/150000, Loss: 24628.771484375, Validation Loss: 24824.734375\n",
      "Epoch 51801/150000, Loss: 24620.294921875, Validation Loss: 24816.21484375\n",
      "Epoch 51901/150000, Loss: 24611.83203125, Validation Loss: 24807.71484375\n",
      "Epoch 52001/150000, Loss: 24603.412109375, Validation Loss: 24799.244140625\n",
      "Epoch 52101/150000, Loss: 24594.98828125, Validation Loss: 24790.783203125\n",
      "Epoch 52201/150000, Loss: 24586.576171875, Validation Loss: 24782.328125\n",
      "Epoch 52301/150000, Loss: 24578.1640625, Validation Loss: 24773.875\n",
      "Epoch 52401/150000, Loss: 24569.763671875, Validation Loss: 24765.42578125\n",
      "Epoch 52501/150000, Loss: 24561.361328125, Validation Loss: 24756.984375\n",
      "Epoch 52601/150000, Loss: 24552.97265625, Validation Loss: 24748.548828125\n",
      "Epoch 52701/150000, Loss: 24544.580078125, Validation Loss: 24740.1171875\n",
      "Epoch 52801/150000, Loss: 24536.197265625, Validation Loss: 24731.69140625\n",
      "Epoch 52901/150000, Loss: 24527.814453125, Validation Loss: 24723.265625\n",
      "Epoch 53001/150000, Loss: 24519.44140625, Validation Loss: 24714.849609375\n",
      "Epoch 53101/150000, Loss: 24511.0703125, Validation Loss: 24706.4375\n",
      "Epoch 53201/150000, Loss: 24502.705078125, Validation Loss: 24698.029296875\n",
      "Epoch 53301/150000, Loss: 24494.345703125, Validation Loss: 24689.62890625\n",
      "Epoch 53401/150000, Loss: 24485.9921875, Validation Loss: 24681.228515625\n",
      "Epoch 53501/150000, Loss: 24477.64453125, Validation Loss: 24672.8359375\n",
      "Epoch 53601/150000, Loss: 24469.296875, Validation Loss: 24664.4453125\n",
      "Epoch 53701/150000, Loss: 24460.955078125, Validation Loss: 24656.064453125\n",
      "Epoch 53801/150000, Loss: 24452.62109375, Validation Loss: 24647.6875\n",
      "Epoch 53901/150000, Loss: 24444.291015625, Validation Loss: 24639.314453125\n",
      "Epoch 54001/150000, Loss: 24435.962890625, Validation Loss: 24630.943359375\n",
      "Epoch 54101/150000, Loss: 24427.64453125, Validation Loss: 24622.58203125\n",
      "Epoch 54201/150000, Loss: 24419.328125, Validation Loss: 24614.22265625\n",
      "Epoch 54301/150000, Loss: 24411.017578125, Validation Loss: 24605.869140625\n",
      "Epoch 54401/150000, Loss: 24402.712890625, Validation Loss: 24597.521484375\n",
      "Epoch 54501/150000, Loss: 24394.41015625, Validation Loss: 24589.177734375\n",
      "Epoch 54601/150000, Loss: 24386.115234375, Validation Loss: 24580.83984375\n",
      "Epoch 54701/150000, Loss: 24377.82421875, Validation Loss: 24572.505859375\n",
      "Epoch 54801/150000, Loss: 24369.541015625, Validation Loss: 24564.17578125\n",
      "Epoch 54901/150000, Loss: 24361.255859375, Validation Loss: 24555.8515625\n",
      "Epoch 55001/150000, Loss: 24352.98046875, Validation Loss: 24547.533203125\n",
      "Epoch 55101/150000, Loss: 24344.7109375, Validation Loss: 24539.21875\n",
      "Epoch 55201/150000, Loss: 24336.443359375, Validation Loss: 24530.90625\n",
      "Epoch 55301/150000, Loss: 24328.181640625, Validation Loss: 24522.60546875\n",
      "Epoch 55401/150000, Loss: 24319.923828125, Validation Loss: 24514.3046875\n",
      "Epoch 55501/150000, Loss: 24311.671875, Validation Loss: 24506.01171875\n",
      "Epoch 55601/150000, Loss: 24303.427734375, Validation Loss: 24497.720703125\n",
      "Epoch 55701/150000, Loss: 24295.185546875, Validation Loss: 24489.435546875\n",
      "Epoch 55801/150000, Loss: 24286.94921875, Validation Loss: 24481.15625\n",
      "Epoch 55901/150000, Loss: 24278.71484375, Validation Loss: 24472.880859375\n",
      "Epoch 56001/150000, Loss: 24270.490234375, Validation Loss: 24464.611328125\n",
      "Epoch 56101/150000, Loss: 24262.263671875, Validation Loss: 24456.34375\n",
      "Epoch 56201/150000, Loss: 24254.052734375, Validation Loss: 24448.0859375\n",
      "Epoch 56301/150000, Loss: 24245.833984375, Validation Loss: 24439.83203125\n",
      "Epoch 56401/150000, Loss: 24237.62890625, Validation Loss: 24431.580078125\n",
      "Epoch 56501/150000, Loss: 24229.427734375, Validation Loss: 24423.333984375\n",
      "Epoch 56601/150000, Loss: 24221.228515625, Validation Loss: 24415.09375\n",
      "Epoch 56701/150000, Loss: 24213.03515625, Validation Loss: 24406.859375\n",
      "Epoch 56801/150000, Loss: 24204.849609375, Validation Loss: 24398.62890625\n",
      "Epoch 56901/150000, Loss: 24196.66796875, Validation Loss: 24390.40234375\n",
      "Epoch 57001/150000, Loss: 24188.490234375, Validation Loss: 24382.18359375\n",
      "Epoch 57101/150000, Loss: 24180.31640625, Validation Loss: 24373.966796875\n",
      "Epoch 57201/150000, Loss: 24172.150390625, Validation Loss: 24365.755859375\n",
      "Epoch 57301/150000, Loss: 24163.982421875, Validation Loss: 24357.55078125\n",
      "Epoch 57401/150000, Loss: 24155.826171875, Validation Loss: 24349.34765625\n",
      "Epoch 57501/150000, Loss: 24147.671875, Validation Loss: 24341.15234375\n",
      "Epoch 57601/150000, Loss: 24139.5234375, Validation Loss: 24332.958984375\n",
      "Epoch 57701/150000, Loss: 24131.37890625, Validation Loss: 24324.7734375\n",
      "Epoch 57801/150000, Loss: 24123.2421875, Validation Loss: 24316.59375\n",
      "Epoch 57901/150000, Loss: 24115.107421875, Validation Loss: 24308.416015625\n",
      "Epoch 58001/150000, Loss: 24106.98046875, Validation Loss: 24300.24609375\n",
      "Epoch 58101/150000, Loss: 24098.85546875, Validation Loss: 24292.078125\n",
      "Epoch 58201/150000, Loss: 24090.736328125, Validation Loss: 24283.916015625\n",
      "Epoch 58301/150000, Loss: 24082.625, Validation Loss: 24275.759765625\n",
      "Epoch 58401/150000, Loss: 24074.513671875, Validation Loss: 24267.607421875\n",
      "Epoch 58501/150000, Loss: 24066.41015625, Validation Loss: 24259.458984375\n",
      "Epoch 58601/150000, Loss: 24058.30859375, Validation Loss: 24251.3203125\n",
      "Epoch 58701/150000, Loss: 24050.212890625, Validation Loss: 24243.181640625\n",
      "Epoch 58801/150000, Loss: 24042.126953125, Validation Loss: 24235.048828125\n",
      "Epoch 58901/150000, Loss: 24034.041015625, Validation Loss: 24226.923828125\n",
      "Epoch 59001/150000, Loss: 24025.958984375, Validation Loss: 24218.80078125\n",
      "Epoch 59101/150000, Loss: 24017.88671875, Validation Loss: 24210.6796875\n",
      "Epoch 59201/150000, Loss: 24009.818359375, Validation Loss: 24202.568359375\n",
      "Epoch 59301/150000, Loss: 24001.751953125, Validation Loss: 24194.4609375\n",
      "Epoch 59401/150000, Loss: 23993.69140625, Validation Loss: 24186.357421875\n",
      "Epoch 59501/150000, Loss: 23985.63671875, Validation Loss: 24178.259765625\n",
      "Epoch 59601/150000, Loss: 23977.587890625, Validation Loss: 24170.16796875\n",
      "Epoch 59701/150000, Loss: 23969.54296875, Validation Loss: 24162.080078125\n",
      "Epoch 59801/150000, Loss: 23961.50390625, Validation Loss: 24153.99609375\n",
      "Epoch 59901/150000, Loss: 23953.466796875, Validation Loss: 24145.91796875\n",
      "Epoch 60001/150000, Loss: 23945.435546875, Validation Loss: 24137.845703125\n",
      "Epoch 60101/150000, Loss: 23937.412109375, Validation Loss: 24129.779296875\n",
      "Epoch 60201/150000, Loss: 23929.390625, Validation Loss: 24121.712890625\n",
      "Epoch 60301/150000, Loss: 23921.37890625, Validation Loss: 24113.65625\n",
      "Epoch 60401/150000, Loss: 23913.36328125, Validation Loss: 24105.6015625\n",
      "Epoch 60501/150000, Loss: 23905.359375, Validation Loss: 24097.552734375\n",
      "Epoch 60601/150000, Loss: 23897.357421875, Validation Loss: 24089.5078125\n",
      "Epoch 60701/150000, Loss: 23889.36328125, Validation Loss: 24081.470703125\n",
      "Epoch 60801/150000, Loss: 23881.37109375, Validation Loss: 24073.435546875\n",
      "Epoch 60901/150000, Loss: 23873.384765625, Validation Loss: 24065.40625\n",
      "Epoch 61001/150000, Loss: 23865.40234375, Validation Loss: 24057.384765625\n",
      "Epoch 61101/150000, Loss: 23857.423828125, Validation Loss: 24049.36328125\n",
      "Epoch 61201/150000, Loss: 23849.45703125, Validation Loss: 24041.349609375\n",
      "Epoch 61301/150000, Loss: 23841.490234375, Validation Loss: 24033.33984375\n",
      "Epoch 61401/150000, Loss: 23833.525390625, Validation Loss: 24025.3359375\n",
      "Epoch 61501/150000, Loss: 23825.5703125, Validation Loss: 24017.3359375\n",
      "Epoch 61601/150000, Loss: 23817.619140625, Validation Loss: 24009.34375\n",
      "Epoch 61701/150000, Loss: 23809.671875, Validation Loss: 24001.3515625\n",
      "Epoch 61801/150000, Loss: 23801.73046875, Validation Loss: 23993.369140625\n",
      "Epoch 61901/150000, Loss: 23793.79296875, Validation Loss: 23985.38671875\n",
      "Epoch 62001/150000, Loss: 23785.859375, Validation Loss: 23977.4140625\n",
      "Epoch 62101/150000, Loss: 23777.93359375, Validation Loss: 23969.443359375\n",
      "Epoch 62201/150000, Loss: 23770.01171875, Validation Loss: 23961.478515625\n",
      "Epoch 62301/150000, Loss: 23762.095703125, Validation Loss: 23953.515625\n",
      "Epoch 62401/150000, Loss: 23754.18359375, Validation Loss: 23945.560546875\n",
      "Epoch 62501/150000, Loss: 23746.271484375, Validation Loss: 23937.611328125\n",
      "Epoch 62601/150000, Loss: 23738.373046875, Validation Loss: 23929.66796875\n",
      "Epoch 62701/150000, Loss: 23730.474609375, Validation Loss: 23921.724609375\n",
      "Epoch 62801/150000, Loss: 23722.58203125, Validation Loss: 23913.7890625\n",
      "Epoch 62901/150000, Loss: 23714.693359375, Validation Loss: 23905.859375\n",
      "Epoch 63001/150000, Loss: 23706.8125, Validation Loss: 23897.935546875\n",
      "Epoch 63101/150000, Loss: 23698.93359375, Validation Loss: 23890.009765625\n",
      "Epoch 63201/150000, Loss: 23691.060546875, Validation Loss: 23882.099609375\n",
      "Epoch 63301/150000, Loss: 23683.19140625, Validation Loss: 23874.1875\n",
      "Epoch 63401/150000, Loss: 23675.328125, Validation Loss: 23866.279296875\n",
      "Epoch 63501/150000, Loss: 23667.47265625, Validation Loss: 23858.380859375\n",
      "Epoch 63601/150000, Loss: 23659.6171875, Validation Loss: 23850.482421875\n",
      "Epoch 63701/150000, Loss: 23651.76953125, Validation Loss: 23842.59375\n",
      "Epoch 63801/150000, Loss: 23643.923828125, Validation Loss: 23834.70703125\n",
      "Epoch 63901/150000, Loss: 23636.08984375, Validation Loss: 23826.82421875\n",
      "Epoch 64001/150000, Loss: 23628.25390625, Validation Loss: 23818.94921875\n",
      "Epoch 64101/150000, Loss: 23620.42578125, Validation Loss: 23811.078125\n",
      "Epoch 64201/150000, Loss: 23612.603515625, Validation Loss: 23803.208984375\n",
      "Epoch 64301/150000, Loss: 23604.78125, Validation Loss: 23795.34765625\n",
      "Epoch 64401/150000, Loss: 23596.966796875, Validation Loss: 23787.4921875\n",
      "Epoch 64501/150000, Loss: 23589.16015625, Validation Loss: 23779.638671875\n",
      "Epoch 64601/150000, Loss: 23581.35546875, Validation Loss: 23771.791015625\n",
      "Epoch 64701/150000, Loss: 23573.5546875, Validation Loss: 23763.951171875\n",
      "Epoch 64801/150000, Loss: 23565.759765625, Validation Loss: 23756.111328125\n",
      "Epoch 64901/150000, Loss: 23557.970703125, Validation Loss: 23748.28125\n",
      "Epoch 65001/150000, Loss: 23550.185546875, Validation Loss: 23740.453125\n",
      "Epoch 65101/150000, Loss: 23542.404296875, Validation Loss: 23732.630859375\n",
      "Epoch 65201/150000, Loss: 23534.6328125, Validation Loss: 23724.8125\n",
      "Epoch 65301/150000, Loss: 23526.86328125, Validation Loss: 23716.998046875\n",
      "Epoch 65401/150000, Loss: 23519.095703125, Validation Loss: 23709.193359375\n",
      "Epoch 65501/150000, Loss: 23511.337890625, Validation Loss: 23701.392578125\n",
      "Epoch 65601/150000, Loss: 23503.58203125, Validation Loss: 23693.591796875\n",
      "Epoch 65701/150000, Loss: 23495.83203125, Validation Loss: 23685.80078125\n",
      "Epoch 65801/150000, Loss: 23488.087890625, Validation Loss: 23678.009765625\n",
      "Epoch 65901/150000, Loss: 23480.345703125, Validation Loss: 23670.228515625\n",
      "Epoch 66001/150000, Loss: 23472.611328125, Validation Loss: 23662.451171875\n",
      "Epoch 66101/150000, Loss: 23464.8828125, Validation Loss: 23654.67578125\n",
      "Epoch 66201/150000, Loss: 23457.154296875, Validation Loss: 23646.90625\n",
      "Epoch 66301/150000, Loss: 23449.435546875, Validation Loss: 23639.14453125\n",
      "Epoch 66401/150000, Loss: 23441.71875, Validation Loss: 23631.38671875\n",
      "Epoch 66501/150000, Loss: 23434.009765625, Validation Loss: 23623.634765625\n",
      "Epoch 66601/150000, Loss: 23426.3046875, Validation Loss: 23615.8828125\n",
      "Epoch 66701/150000, Loss: 23418.6015625, Validation Loss: 23608.138671875\n",
      "Epoch 66801/150000, Loss: 23410.90625, Validation Loss: 23600.40234375\n",
      "Epoch 66901/150000, Loss: 23403.216796875, Validation Loss: 23592.66796875\n",
      "Epoch 67001/150000, Loss: 23395.52734375, Validation Loss: 23584.9375\n",
      "Epoch 67101/150000, Loss: 23387.849609375, Validation Loss: 23577.21484375\n",
      "Epoch 67201/150000, Loss: 23380.171875, Validation Loss: 23569.49609375\n",
      "Epoch 67301/150000, Loss: 23372.501953125, Validation Loss: 23561.78125\n",
      "Epoch 67401/150000, Loss: 23364.833984375, Validation Loss: 23554.072265625\n",
      "Epoch 67501/150000, Loss: 23357.169921875, Validation Loss: 23546.3671875\n",
      "Epoch 67601/150000, Loss: 23349.515625, Validation Loss: 23538.66796875\n",
      "Epoch 67701/150000, Loss: 23341.86328125, Validation Loss: 23530.97265625\n",
      "Epoch 67801/150000, Loss: 23334.216796875, Validation Loss: 23523.283203125\n",
      "Epoch 67901/150000, Loss: 23326.576171875, Validation Loss: 23515.59765625\n",
      "Epoch 68001/150000, Loss: 23318.94140625, Validation Loss: 23507.919921875\n",
      "Epoch 68101/150000, Loss: 23311.3046875, Validation Loss: 23500.244140625\n",
      "Epoch 68201/150000, Loss: 23303.67578125, Validation Loss: 23492.57421875\n",
      "Epoch 68301/150000, Loss: 23296.0546875, Validation Loss: 23484.90625\n",
      "Epoch 68401/150000, Loss: 23288.439453125, Validation Loss: 23477.24609375\n",
      "Epoch 68501/150000, Loss: 23280.826171875, Validation Loss: 23469.59375\n",
      "Epoch 68601/150000, Loss: 23273.216796875, Validation Loss: 23461.94140625\n",
      "Epoch 68701/150000, Loss: 23265.6171875, Validation Loss: 23454.298828125\n",
      "Epoch 68801/150000, Loss: 23258.017578125, Validation Loss: 23446.654296875\n",
      "Epoch 68901/150000, Loss: 23250.42578125, Validation Loss: 23439.021484375\n",
      "Epoch 69001/150000, Loss: 23242.83203125, Validation Loss: 23431.388671875\n",
      "Epoch 69101/150000, Loss: 23235.25390625, Validation Loss: 23423.759765625\n",
      "Epoch 69201/150000, Loss: 23227.671875, Validation Loss: 23416.140625\n",
      "Epoch 69301/150000, Loss: 23220.1015625, Validation Loss: 23408.5234375\n",
      "Epoch 69401/150000, Loss: 23212.529296875, Validation Loss: 23400.9140625\n",
      "Epoch 69501/150000, Loss: 23204.96484375, Validation Loss: 23393.302734375\n",
      "Epoch 69601/150000, Loss: 23197.408203125, Validation Loss: 23385.701171875\n",
      "Epoch 69701/150000, Loss: 23189.8515625, Validation Loss: 23378.103515625\n",
      "Epoch 69801/150000, Loss: 23182.302734375, Validation Loss: 23370.51171875\n",
      "Epoch 69901/150000, Loss: 23174.75390625, Validation Loss: 23362.921875\n",
      "Epoch 70001/150000, Loss: 23167.216796875, Validation Loss: 23355.33984375\n",
      "Epoch 70101/150000, Loss: 23159.677734375, Validation Loss: 23347.759765625\n",
      "Epoch 70201/150000, Loss: 23152.146484375, Validation Loss: 23340.185546875\n",
      "Epoch 70301/150000, Loss: 23144.619140625, Validation Loss: 23332.619140625\n",
      "Epoch 70401/150000, Loss: 23137.09765625, Validation Loss: 23325.048828125\n",
      "Epoch 70501/150000, Loss: 23129.578125, Validation Loss: 23317.48828125\n",
      "Epoch 70601/150000, Loss: 23122.05859375, Validation Loss: 23309.931640625\n",
      "Epoch 70701/150000, Loss: 23114.55078125, Validation Loss: 23302.375\n",
      "Epoch 70801/150000, Loss: 23107.041015625, Validation Loss: 23294.82421875\n",
      "Epoch 70901/150000, Loss: 23099.5390625, Validation Loss: 23287.28125\n",
      "Epoch 71001/150000, Loss: 23092.041015625, Validation Loss: 23279.740234375\n",
      "Epoch 71101/150000, Loss: 23084.54296875, Validation Loss: 23272.197265625\n",
      "Epoch 71201/150000, Loss: 23077.05078125, Validation Loss: 23264.6640625\n",
      "Epoch 71301/150000, Loss: 23069.55859375, Validation Loss: 23257.12890625\n",
      "Epoch 71401/150000, Loss: 23062.0703125, Validation Loss: 23249.599609375\n",
      "Epoch 71501/150000, Loss: 23054.58203125, Validation Loss: 23242.068359375\n",
      "Epoch 71601/150000, Loss: 23047.09765625, Validation Loss: 23234.541015625\n",
      "Epoch 71701/150000, Loss: 23039.611328125, Validation Loss: 23227.015625\n",
      "Epoch 71801/150000, Loss: 23032.1328125, Validation Loss: 23219.490234375\n",
      "Epoch 71901/150000, Loss: 23024.646484375, Validation Loss: 23211.96484375\n",
      "Epoch 72001/150000, Loss: 23017.16015625, Validation Loss: 23204.439453125\n",
      "Epoch 72101/150000, Loss: 23009.67578125, Validation Loss: 23196.91015625\n",
      "Epoch 72201/150000, Loss: 23002.185546875, Validation Loss: 23189.376953125\n",
      "Epoch 72301/150000, Loss: 22994.69140625, Validation Loss: 23181.84375\n",
      "Epoch 72401/150000, Loss: 22987.19140625, Validation Loss: 23174.3046875\n",
      "Epoch 72501/150000, Loss: 22979.685546875, Validation Loss: 23166.759765625\n",
      "Epoch 72601/150000, Loss: 22972.171875, Validation Loss: 23159.201171875\n",
      "Epoch 72701/150000, Loss: 22964.640625, Validation Loss: 23151.6328125\n",
      "Epoch 72801/150000, Loss: 22957.095703125, Validation Loss: 23144.044921875\n",
      "Epoch 72901/150000, Loss: 22949.517578125, Validation Loss: 23136.431640625\n",
      "Epoch 73001/150000, Loss: 22941.904296875, Validation Loss: 23128.78125\n",
      "Epoch 73101/150000, Loss: 22934.259765625, Validation Loss: 23121.095703125\n",
      "Epoch 73201/150000, Loss: 22926.46484375, Validation Loss: 23113.265625\n",
      "Epoch 73301/150000, Loss: 22917.359375, Validation Loss: 23104.150390625\n",
      "Epoch 73401/150000, Loss: 22898.134765625, Validation Loss: 23085.177734375\n",
      "Epoch 73501/150000, Loss: 22869.958984375, Validation Loss: 23057.791015625\n",
      "Epoch 73601/150000, Loss: 22854.669921875, Validation Loss: 23042.9453125\n",
      "Epoch 73701/150000, Loss: 22844.984375, Validation Loss: 23033.392578125\n",
      "Epoch 73801/150000, Loss: 22836.326171875, Validation Loss: 23024.775390625\n",
      "Epoch 73901/150000, Loss: 22827.869140625, Validation Loss: 23016.32421875\n",
      "Epoch 74001/150000, Loss: 22819.4921875, Validation Loss: 23007.9453125\n",
      "Epoch 74101/150000, Loss: 22811.140625, Validation Loss: 22999.5859375\n",
      "Epoch 74201/150000, Loss: 22802.8359375, Validation Loss: 22991.267578125\n",
      "Epoch 74301/150000, Loss: 22794.544921875, Validation Loss: 22982.970703125\n",
      "Epoch 74401/150000, Loss: 22786.2734375, Validation Loss: 22974.689453125\n",
      "Epoch 74501/150000, Loss: 22778.03125, Validation Loss: 22966.439453125\n",
      "Epoch 74601/150000, Loss: 22769.802734375, Validation Loss: 22958.197265625\n",
      "Epoch 74701/150000, Loss: 22761.576171875, Validation Loss: 22949.96484375\n",
      "Epoch 74801/150000, Loss: 22753.384765625, Validation Loss: 22941.76171875\n",
      "Epoch 74901/150000, Loss: 22745.203125, Validation Loss: 22933.572265625\n",
      "Epoch 75001/150000, Loss: 22737.029296875, Validation Loss: 22925.388671875\n",
      "Epoch 75101/150000, Loss: 22728.861328125, Validation Loss: 22917.20703125\n",
      "Epoch 75201/150000, Loss: 22720.701171875, Validation Loss: 22909.0390625\n",
      "Epoch 75301/150000, Loss: 22712.55078125, Validation Loss: 22900.880859375\n",
      "Epoch 75401/150000, Loss: 22704.42578125, Validation Loss: 22892.74609375\n",
      "Epoch 75501/150000, Loss: 22696.302734375, Validation Loss: 22884.615234375\n",
      "Epoch 75601/150000, Loss: 22688.185546875, Validation Loss: 22876.48828125\n",
      "Epoch 75701/150000, Loss: 22680.064453125, Validation Loss: 22868.359375\n",
      "Epoch 75801/150000, Loss: 22671.953125, Validation Loss: 22860.240234375\n",
      "Epoch 75901/150000, Loss: 22663.845703125, Validation Loss: 22852.119140625\n",
      "Epoch 76001/150000, Loss: 22655.73828125, Validation Loss: 22844.005859375\n",
      "Epoch 76101/150000, Loss: 22647.625, Validation Loss: 22835.892578125\n",
      "Epoch 76201/150000, Loss: 22639.513671875, Validation Loss: 22827.7734375\n",
      "Epoch 76301/150000, Loss: 22631.40234375, Validation Loss: 22819.650390625\n",
      "Epoch 76401/150000, Loss: 22623.28125, Validation Loss: 22811.529296875\n",
      "Epoch 76501/150000, Loss: 22615.162109375, Validation Loss: 22803.404296875\n",
      "Epoch 76601/150000, Loss: 22607.03515625, Validation Loss: 22795.2734375\n",
      "Epoch 76701/150000, Loss: 22598.904296875, Validation Loss: 22787.138671875\n",
      "Epoch 76801/150000, Loss: 22590.76953125, Validation Loss: 22779.00390625\n",
      "Epoch 76901/150000, Loss: 22582.630859375, Validation Loss: 22770.861328125\n",
      "Epoch 77001/150000, Loss: 22574.486328125, Validation Loss: 22762.720703125\n",
      "Epoch 77101/150000, Loss: 22566.34375, Validation Loss: 22754.576171875\n",
      "Epoch 77201/150000, Loss: 22558.193359375, Validation Loss: 22746.42578125\n",
      "Epoch 77301/150000, Loss: 22550.03125, Validation Loss: 22738.26953125\n",
      "Epoch 77401/150000, Loss: 22541.861328125, Validation Loss: 22730.10546875\n",
      "Epoch 77501/150000, Loss: 22533.6875, Validation Loss: 22721.93359375\n",
      "Epoch 77601/150000, Loss: 22525.4921875, Validation Loss: 22713.75\n",
      "Epoch 77701/150000, Loss: 22517.298828125, Validation Loss: 22705.560546875\n",
      "Epoch 77801/150000, Loss: 22509.091796875, Validation Loss: 22697.361328125\n",
      "Epoch 77901/150000, Loss: 22500.880859375, Validation Loss: 22689.166015625\n",
      "Epoch 78001/150000, Loss: 22492.671875, Validation Loss: 22680.966796875\n",
      "Epoch 78101/150000, Loss: 22484.458984375, Validation Loss: 22672.765625\n",
      "Epoch 78201/150000, Loss: 22476.251953125, Validation Loss: 22664.568359375\n",
      "Epoch 78301/150000, Loss: 22468.044921875, Validation Loss: 22656.37890625\n",
      "Epoch 78401/150000, Loss: 22459.849609375, Validation Loss: 22648.1953125\n",
      "Epoch 78501/150000, Loss: 22451.6640625, Validation Loss: 22640.021484375\n",
      "Epoch 78601/150000, Loss: 22443.490234375, Validation Loss: 22631.861328125\n",
      "Epoch 78701/150000, Loss: 22435.3359375, Validation Loss: 22623.720703125\n",
      "Epoch 78801/150000, Loss: 22427.201171875, Validation Loss: 22615.59765625\n",
      "Epoch 78901/150000, Loss: 22419.07421875, Validation Loss: 22607.48828125\n",
      "Epoch 79001/150000, Loss: 22410.97265625, Validation Loss: 22599.39453125\n",
      "Epoch 79101/150000, Loss: 22402.884765625, Validation Loss: 22591.3203125\n",
      "Epoch 79201/150000, Loss: 22394.818359375, Validation Loss: 22583.263671875\n",
      "Epoch 79301/150000, Loss: 22386.765625, Validation Loss: 22575.2265625\n",
      "Epoch 79401/150000, Loss: 22378.732421875, Validation Loss: 22567.205078125\n",
      "Epoch 79501/150000, Loss: 22370.716796875, Validation Loss: 22559.201171875\n",
      "Epoch 79601/150000, Loss: 22362.716796875, Validation Loss: 22551.212890625\n",
      "Epoch 79701/150000, Loss: 22354.734375, Validation Loss: 22543.244140625\n",
      "Epoch 79801/150000, Loss: 22346.771484375, Validation Loss: 22535.291015625\n",
      "Epoch 79901/150000, Loss: 22338.8203125, Validation Loss: 22527.35546875\n",
      "Epoch 80001/150000, Loss: 22330.88671875, Validation Loss: 22519.43359375\n",
      "Epoch 80101/150000, Loss: 22322.970703125, Validation Loss: 22511.529296875\n",
      "Epoch 80201/150000, Loss: 22315.068359375, Validation Loss: 22503.638671875\n",
      "Epoch 80301/150000, Loss: 22307.181640625, Validation Loss: 22495.765625\n",
      "Epoch 80401/150000, Loss: 22299.3125, Validation Loss: 22487.904296875\n",
      "Epoch 80501/150000, Loss: 22291.455078125, Validation Loss: 22480.060546875\n",
      "Epoch 80601/150000, Loss: 22283.611328125, Validation Loss: 22472.23046875\n",
      "Epoch 80701/150000, Loss: 22275.78515625, Validation Loss: 22464.41015625\n",
      "Epoch 80801/150000, Loss: 22267.966796875, Validation Loss: 22456.60546875\n",
      "Epoch 80901/150000, Loss: 22260.166015625, Validation Loss: 22448.8125\n",
      "Epoch 81001/150000, Loss: 22252.376953125, Validation Loss: 22441.033203125\n",
      "Epoch 81101/150000, Loss: 22244.599609375, Validation Loss: 22433.26171875\n",
      "Epoch 81201/150000, Loss: 22236.83203125, Validation Loss: 22425.501953125\n",
      "Epoch 81301/150000, Loss: 22229.078125, Validation Loss: 22417.75390625\n",
      "Epoch 81401/150000, Loss: 22221.333984375, Validation Loss: 22410.013671875\n",
      "Epoch 81501/150000, Loss: 22213.599609375, Validation Loss: 22402.283203125\n",
      "Epoch 81601/150000, Loss: 22205.890625, Validation Loss: 22394.578125\n",
      "Epoch 81701/150000, Loss: 22198.208984375, Validation Loss: 22386.896484375\n",
      "Epoch 81801/150000, Loss: 22190.548828125, Validation Loss: 22379.236328125\n",
      "Epoch 81901/150000, Loss: 22182.908203125, Validation Loss: 22371.59765625\n",
      "Epoch 82001/150000, Loss: 22175.27734375, Validation Loss: 22363.9609375\n",
      "Epoch 82101/150000, Loss: 22167.650390625, Validation Loss: 22356.333984375\n",
      "Epoch 82201/150000, Loss: 22160.03125, Validation Loss: 22348.71484375\n",
      "Epoch 82301/150000, Loss: 22152.419921875, Validation Loss: 22341.09765625\n",
      "Epoch 82401/150000, Loss: 22144.81640625, Validation Loss: 22333.484375\n",
      "Epoch 82501/150000, Loss: 22137.21484375, Validation Loss: 22325.87890625\n",
      "Epoch 82601/150000, Loss: 22129.62109375, Validation Loss: 22318.279296875\n",
      "Epoch 82701/150000, Loss: 22122.033203125, Validation Loss: 22310.68359375\n",
      "Epoch 82801/150000, Loss: 22114.447265625, Validation Loss: 22303.091796875\n",
      "Epoch 82901/150000, Loss: 22106.87109375, Validation Loss: 22295.50390625\n",
      "Epoch 83001/150000, Loss: 22099.294921875, Validation Loss: 22287.921875\n",
      "Epoch 83101/150000, Loss: 22091.7265625, Validation Loss: 22280.341796875\n",
      "Epoch 83201/150000, Loss: 22084.15625, Validation Loss: 22272.765625\n",
      "Epoch 83301/150000, Loss: 22076.59765625, Validation Loss: 22265.1953125\n",
      "Epoch 83401/150000, Loss: 22069.041015625, Validation Loss: 22257.625\n",
      "Epoch 83501/150000, Loss: 22061.490234375, Validation Loss: 22250.06640625\n",
      "Epoch 83601/150000, Loss: 22053.939453125, Validation Loss: 22242.50390625\n",
      "Epoch 83701/150000, Loss: 22046.396484375, Validation Loss: 22234.951171875\n",
      "Epoch 83801/150000, Loss: 22038.859375, Validation Loss: 22227.3984375\n",
      "Epoch 83901/150000, Loss: 22031.3203125, Validation Loss: 22219.849609375\n",
      "Epoch 84001/150000, Loss: 22023.791015625, Validation Loss: 22212.3046875\n",
      "Epoch 84101/150000, Loss: 22016.2578125, Validation Loss: 22204.763671875\n",
      "Epoch 84201/150000, Loss: 22008.734375, Validation Loss: 22197.23046875\n",
      "Epoch 84301/150000, Loss: 22001.2109375, Validation Loss: 22189.6953125\n",
      "Epoch 84401/150000, Loss: 21993.701171875, Validation Loss: 22182.16796875\n",
      "Epoch 84501/150000, Loss: 21986.1875, Validation Loss: 22174.642578125\n",
      "Epoch 84601/150000, Loss: 21978.67578125, Validation Loss: 22167.119140625\n",
      "Epoch 84701/150000, Loss: 21971.173828125, Validation Loss: 22159.603515625\n",
      "Epoch 84801/150000, Loss: 21963.671875, Validation Loss: 22152.087890625\n",
      "Epoch 84901/150000, Loss: 21956.17578125, Validation Loss: 22144.580078125\n",
      "Epoch 85001/150000, Loss: 21948.681640625, Validation Loss: 22137.0703125\n",
      "Epoch 85101/150000, Loss: 21941.19140625, Validation Loss: 22129.56640625\n",
      "Epoch 85201/150000, Loss: 21933.703125, Validation Loss: 22122.06640625\n",
      "Epoch 85301/150000, Loss: 21926.22265625, Validation Loss: 22114.5703125\n",
      "Epoch 85401/150000, Loss: 21918.7421875, Validation Loss: 22107.07421875\n",
      "Epoch 85501/150000, Loss: 21911.265625, Validation Loss: 22099.583984375\n",
      "Epoch 85601/150000, Loss: 21903.79296875, Validation Loss: 22092.095703125\n",
      "Epoch 85701/150000, Loss: 21896.326171875, Validation Loss: 22084.609375\n",
      "Epoch 85801/150000, Loss: 21888.857421875, Validation Loss: 22077.130859375\n",
      "Epoch 85901/150000, Loss: 21881.396484375, Validation Loss: 22069.650390625\n",
      "Epoch 86001/150000, Loss: 21873.9375, Validation Loss: 22062.17578125\n",
      "Epoch 86101/150000, Loss: 21866.478515625, Validation Loss: 22054.703125\n",
      "Epoch 86201/150000, Loss: 21859.02734375, Validation Loss: 22047.232421875\n",
      "Epoch 86301/150000, Loss: 21851.57421875, Validation Loss: 22039.763671875\n",
      "Epoch 86401/150000, Loss: 21844.12890625, Validation Loss: 22032.298828125\n",
      "Epoch 86501/150000, Loss: 21836.685546875, Validation Loss: 22024.8359375\n",
      "Epoch 86601/150000, Loss: 21829.24609375, Validation Loss: 22017.375\n",
      "Epoch 86701/150000, Loss: 21821.806640625, Validation Loss: 22009.919921875\n",
      "Epoch 86801/150000, Loss: 21814.37109375, Validation Loss: 22002.46484375\n",
      "Epoch 86901/150000, Loss: 21806.939453125, Validation Loss: 21995.015625\n",
      "Epoch 87001/150000, Loss: 21799.5078125, Validation Loss: 21987.564453125\n",
      "Epoch 87101/150000, Loss: 21792.08203125, Validation Loss: 21980.119140625\n",
      "Epoch 87201/150000, Loss: 21784.66015625, Validation Loss: 21972.673828125\n",
      "Epoch 87301/150000, Loss: 21777.236328125, Validation Loss: 21965.232421875\n",
      "Epoch 87401/150000, Loss: 21769.818359375, Validation Loss: 21957.79296875\n",
      "Epoch 87501/150000, Loss: 21762.40234375, Validation Loss: 21950.35546875\n",
      "Epoch 87601/150000, Loss: 21754.9921875, Validation Loss: 21942.923828125\n",
      "Epoch 87701/150000, Loss: 21747.58203125, Validation Loss: 21935.4921875\n",
      "Epoch 87801/150000, Loss: 21740.17578125, Validation Loss: 21928.064453125\n",
      "Epoch 87901/150000, Loss: 21732.76953125, Validation Loss: 21920.63671875\n",
      "Epoch 88001/150000, Loss: 21725.369140625, Validation Loss: 21913.212890625\n",
      "Epoch 88101/150000, Loss: 21717.970703125, Validation Loss: 21905.794921875\n",
      "Epoch 88201/150000, Loss: 21710.57421875, Validation Loss: 21898.375\n",
      "Epoch 88301/150000, Loss: 21703.18359375, Validation Loss: 21890.958984375\n",
      "Epoch 88401/150000, Loss: 21695.791015625, Validation Loss: 21883.546875\n",
      "Epoch 88501/150000, Loss: 21688.404296875, Validation Loss: 21876.13671875\n",
      "Epoch 88601/150000, Loss: 21681.01953125, Validation Loss: 21868.728515625\n",
      "Epoch 88701/150000, Loss: 21673.638671875, Validation Loss: 21861.322265625\n",
      "Epoch 88801/150000, Loss: 21666.255859375, Validation Loss: 21853.916015625\n",
      "Epoch 88901/150000, Loss: 21658.8828125, Validation Loss: 21846.515625\n",
      "Epoch 89001/150000, Loss: 21651.505859375, Validation Loss: 21839.11328125\n",
      "Epoch 89101/150000, Loss: 21644.134765625, Validation Loss: 21831.71484375\n",
      "Epoch 89201/150000, Loss: 21636.765625, Validation Loss: 21824.318359375\n",
      "Epoch 89301/150000, Loss: 21629.3984375, Validation Loss: 21816.921875\n",
      "Epoch 89401/150000, Loss: 21622.03515625, Validation Loss: 21809.529296875\n",
      "Epoch 89501/150000, Loss: 21614.671875, Validation Loss: 21802.138671875\n",
      "Epoch 89601/150000, Loss: 21607.314453125, Validation Loss: 21794.751953125\n",
      "Epoch 89701/150000, Loss: 21599.9609375, Validation Loss: 21787.36328125\n",
      "Epoch 89801/150000, Loss: 21592.6015625, Validation Loss: 21779.978515625\n",
      "Epoch 89901/150000, Loss: 21585.251953125, Validation Loss: 21772.59765625\n",
      "Epoch 90001/150000, Loss: 21577.900390625, Validation Loss: 21765.220703125\n",
      "Epoch 90101/150000, Loss: 21570.5546875, Validation Loss: 21757.845703125\n",
      "Epoch 90201/150000, Loss: 21563.2109375, Validation Loss: 21750.4765625\n",
      "Epoch 90301/150000, Loss: 21555.87109375, Validation Loss: 21743.111328125\n",
      "Epoch 90401/150000, Loss: 21548.533203125, Validation Loss: 21735.748046875\n",
      "Epoch 90501/150000, Loss: 21541.193359375, Validation Loss: 21728.390625\n",
      "Epoch 90601/150000, Loss: 21533.86328125, Validation Loss: 21721.037109375\n",
      "Epoch 90701/150000, Loss: 21526.533203125, Validation Loss: 21713.685546875\n",
      "Epoch 90801/150000, Loss: 21519.20703125, Validation Loss: 21706.33984375\n",
      "Epoch 90901/150000, Loss: 21511.8828125, Validation Loss: 21698.99609375\n",
      "Epoch 91001/150000, Loss: 21504.5625, Validation Loss: 21691.65625\n",
      "Epoch 91101/150000, Loss: 21497.2421875, Validation Loss: 21684.31640625\n",
      "Epoch 91201/150000, Loss: 21489.9296875, Validation Loss: 21676.9765625\n",
      "Epoch 91301/150000, Loss: 21482.615234375, Validation Loss: 21669.626953125\n",
      "Epoch 91401/150000, Loss: 21475.3046875, Validation Loss: 21662.279296875\n",
      "Epoch 91501/150000, Loss: 21467.998046875, Validation Loss: 21654.93359375\n",
      "Epoch 91601/150000, Loss: 21460.6953125, Validation Loss: 21647.599609375\n",
      "Epoch 91701/150000, Loss: 21453.39453125, Validation Loss: 21640.279296875\n",
      "Epoch 91801/150000, Loss: 21446.09765625, Validation Loss: 21632.9609375\n",
      "Epoch 91901/150000, Loss: 21438.80078125, Validation Loss: 21625.650390625\n",
      "Epoch 92001/150000, Loss: 21431.5078125, Validation Loss: 21618.345703125\n",
      "Epoch 92101/150000, Loss: 21424.220703125, Validation Loss: 21611.041015625\n",
      "Epoch 92201/150000, Loss: 21416.93359375, Validation Loss: 21603.740234375\n",
      "Epoch 92301/150000, Loss: 21409.654296875, Validation Loss: 21596.443359375\n",
      "Epoch 92401/150000, Loss: 21402.373046875, Validation Loss: 21589.1484375\n",
      "Epoch 92501/150000, Loss: 21395.095703125, Validation Loss: 21581.85546875\n",
      "Epoch 92601/150000, Loss: 21387.826171875, Validation Loss: 21574.56640625\n",
      "Epoch 92701/150000, Loss: 21380.556640625, Validation Loss: 21567.27734375\n",
      "Epoch 92801/150000, Loss: 21373.28515625, Validation Loss: 21559.990234375\n",
      "Epoch 92901/150000, Loss: 21366.01953125, Validation Loss: 21552.70703125\n",
      "Epoch 93001/150000, Loss: 21358.759765625, Validation Loss: 21545.42578125\n",
      "Epoch 93101/150000, Loss: 21351.5, Validation Loss: 21538.14453125\n",
      "Epoch 93201/150000, Loss: 21344.2421875, Validation Loss: 21530.869140625\n",
      "Epoch 93301/150000, Loss: 21336.98828125, Validation Loss: 21523.59375\n",
      "Epoch 93401/150000, Loss: 21329.736328125, Validation Loss: 21516.318359375\n",
      "Epoch 93501/150000, Loss: 21322.48828125, Validation Loss: 21509.048828125\n",
      "Epoch 93601/150000, Loss: 21315.2421875, Validation Loss: 21501.779296875\n",
      "Epoch 93701/150000, Loss: 21307.99609375, Validation Loss: 21494.515625\n",
      "Epoch 93801/150000, Loss: 21300.755859375, Validation Loss: 21487.251953125\n",
      "Epoch 93901/150000, Loss: 21293.517578125, Validation Loss: 21479.990234375\n",
      "Epoch 94001/150000, Loss: 21286.283203125, Validation Loss: 21472.728515625\n",
      "Epoch 94101/150000, Loss: 21279.046875, Validation Loss: 21465.47265625\n",
      "Epoch 94201/150000, Loss: 21271.81640625, Validation Loss: 21458.21875\n",
      "Epoch 94301/150000, Loss: 21264.587890625, Validation Loss: 21450.96484375\n",
      "Epoch 94401/150000, Loss: 21257.36328125, Validation Loss: 21443.716796875\n",
      "Epoch 94501/150000, Loss: 21250.134765625, Validation Loss: 21436.46875\n",
      "Epoch 94601/150000, Loss: 21242.916015625, Validation Loss: 21429.224609375\n",
      "Epoch 94701/150000, Loss: 21235.697265625, Validation Loss: 21421.982421875\n",
      "Epoch 94801/150000, Loss: 21228.48046875, Validation Loss: 21414.7421875\n",
      "Epoch 94901/150000, Loss: 21221.26953125, Validation Loss: 21407.505859375\n",
      "Epoch 95001/150000, Loss: 21214.056640625, Validation Loss: 21400.26953125\n",
      "Epoch 95101/150000, Loss: 21206.849609375, Validation Loss: 21393.04296875\n",
      "Epoch 95201/150000, Loss: 21199.64453125, Validation Loss: 21385.8125\n",
      "Epoch 95301/150000, Loss: 21192.44140625, Validation Loss: 21378.587890625\n",
      "Epoch 95401/150000, Loss: 21185.240234375, Validation Loss: 21371.3671875\n",
      "Epoch 95501/150000, Loss: 21178.0390625, Validation Loss: 21364.1484375\n",
      "Epoch 95601/150000, Loss: 21170.84375, Validation Loss: 21356.931640625\n",
      "Epoch 95701/150000, Loss: 21163.654296875, Validation Loss: 21349.716796875\n",
      "Epoch 95801/150000, Loss: 21156.462890625, Validation Loss: 21342.5078125\n",
      "Epoch 95901/150000, Loss: 21149.275390625, Validation Loss: 21335.302734375\n",
      "Epoch 96001/150000, Loss: 21142.09375, Validation Loss: 21328.099609375\n",
      "Epoch 96101/150000, Loss: 21134.912109375, Validation Loss: 21320.89453125\n",
      "Epoch 96201/150000, Loss: 21127.732421875, Validation Loss: 21313.701171875\n",
      "Epoch 96301/150000, Loss: 21120.552734375, Validation Loss: 21306.505859375\n",
      "Epoch 96401/150000, Loss: 21113.3828125, Validation Loss: 21299.310546875\n",
      "Epoch 96501/150000, Loss: 21106.2109375, Validation Loss: 21292.12109375\n",
      "Epoch 96601/150000, Loss: 21099.04296875, Validation Loss: 21284.93359375\n",
      "Epoch 96701/150000, Loss: 21091.876953125, Validation Loss: 21277.751953125\n",
      "Epoch 96801/150000, Loss: 21084.712890625, Validation Loss: 21270.56640625\n",
      "Epoch 96901/150000, Loss: 21077.552734375, Validation Loss: 21263.388671875\n",
      "Epoch 97001/150000, Loss: 21070.39453125, Validation Loss: 21256.208984375\n",
      "Epoch 97101/150000, Loss: 21063.23828125, Validation Loss: 21249.0390625\n",
      "Epoch 97201/150000, Loss: 21056.087890625, Validation Loss: 21241.865234375\n",
      "Epoch 97301/150000, Loss: 21048.935546875, Validation Loss: 21234.6953125\n",
      "Epoch 97401/150000, Loss: 21041.7890625, Validation Loss: 21227.529296875\n",
      "Epoch 97501/150000, Loss: 21034.642578125, Validation Loss: 21220.361328125\n",
      "Epoch 97601/150000, Loss: 21027.501953125, Validation Loss: 21213.203125\n",
      "Epoch 97701/150000, Loss: 21020.361328125, Validation Loss: 21206.041015625\n",
      "Epoch 97801/150000, Loss: 21013.22265625, Validation Loss: 21198.88671875\n",
      "Epoch 97901/150000, Loss: 21006.087890625, Validation Loss: 21191.728515625\n",
      "Epoch 98001/150000, Loss: 20998.95703125, Validation Loss: 21184.578125\n",
      "Epoch 98101/150000, Loss: 20991.82421875, Validation Loss: 21177.427734375\n",
      "Epoch 98201/150000, Loss: 20984.697265625, Validation Loss: 21170.28125\n",
      "Epoch 98301/150000, Loss: 20977.57421875, Validation Loss: 21163.134765625\n",
      "Epoch 98401/150000, Loss: 20970.451171875, Validation Loss: 21155.9921875\n",
      "Epoch 98501/150000, Loss: 20963.33203125, Validation Loss: 21148.85546875\n",
      "Epoch 98601/150000, Loss: 20956.212890625, Validation Loss: 21141.71875\n",
      "Epoch 98701/150000, Loss: 20949.099609375, Validation Loss: 21134.583984375\n",
      "Epoch 98801/150000, Loss: 20941.98828125, Validation Loss: 21127.451171875\n",
      "Epoch 98901/150000, Loss: 20934.876953125, Validation Loss: 21120.32421875\n",
      "Epoch 99001/150000, Loss: 20927.771484375, Validation Loss: 21113.1953125\n",
      "Epoch 99101/150000, Loss: 20920.666015625, Validation Loss: 21106.068359375\n",
      "Epoch 99201/150000, Loss: 20913.56640625, Validation Loss: 21098.947265625\n",
      "Epoch 99301/150000, Loss: 20906.466796875, Validation Loss: 21091.826171875\n",
      "Epoch 99401/150000, Loss: 20899.369140625, Validation Loss: 21084.7109375\n",
      "Epoch 99501/150000, Loss: 20892.275390625, Validation Loss: 21077.59375\n",
      "Epoch 99601/150000, Loss: 20885.185546875, Validation Loss: 21070.4765625\n",
      "Epoch 99701/150000, Loss: 20878.09375, Validation Loss: 21063.3671875\n",
      "Epoch 99801/150000, Loss: 20871.0078125, Validation Loss: 21056.255859375\n",
      "Epoch 99901/150000, Loss: 20863.923828125, Validation Loss: 21049.14453125\n",
      "Epoch 100001/150000, Loss: 20856.84375, Validation Loss: 21042.03515625\n",
      "Epoch 100101/150000, Loss: 20849.765625, Validation Loss: 21034.92578125\n",
      "Epoch 100201/150000, Loss: 20842.6875, Validation Loss: 21027.822265625\n",
      "Epoch 100301/150000, Loss: 20835.61328125, Validation Loss: 21020.71484375\n",
      "Epoch 100401/150000, Loss: 20828.54296875, Validation Loss: 21013.61328125\n",
      "Epoch 100501/150000, Loss: 20821.474609375, Validation Loss: 21006.513671875\n",
      "Epoch 100601/150000, Loss: 20814.40625, Validation Loss: 20999.416015625\n",
      "Epoch 100701/150000, Loss: 20807.34375, Validation Loss: 20992.32421875\n",
      "Epoch 100801/150000, Loss: 20800.283203125, Validation Loss: 20985.234375\n",
      "Epoch 100901/150000, Loss: 20793.22265625, Validation Loss: 20978.146484375\n",
      "Epoch 101001/150000, Loss: 20786.169921875, Validation Loss: 20971.06640625\n",
      "Epoch 101101/150000, Loss: 20779.115234375, Validation Loss: 20963.984375\n",
      "Epoch 101201/150000, Loss: 20772.06640625, Validation Loss: 20956.90625\n",
      "Epoch 101301/150000, Loss: 20765.01953125, Validation Loss: 20949.830078125\n",
      "Epoch 101401/150000, Loss: 20757.97265625, Validation Loss: 20942.759765625\n",
      "Epoch 101501/150000, Loss: 20750.9296875, Validation Loss: 20935.689453125\n",
      "Epoch 101601/150000, Loss: 20743.888671875, Validation Loss: 20928.62109375\n",
      "Epoch 101701/150000, Loss: 20736.8515625, Validation Loss: 20921.556640625\n",
      "Epoch 101801/150000, Loss: 20729.81640625, Validation Loss: 20914.4921875\n",
      "Epoch 101901/150000, Loss: 20722.783203125, Validation Loss: 20907.431640625\n",
      "Epoch 102001/150000, Loss: 20715.751953125, Validation Loss: 20900.375\n",
      "Epoch 102101/150000, Loss: 20708.724609375, Validation Loss: 20893.3203125\n",
      "Epoch 102201/150000, Loss: 20701.69921875, Validation Loss: 20886.267578125\n",
      "Epoch 102301/150000, Loss: 20694.673828125, Validation Loss: 20879.21484375\n",
      "Epoch 102401/150000, Loss: 20687.654296875, Validation Loss: 20872.169921875\n",
      "Epoch 102501/150000, Loss: 20680.638671875, Validation Loss: 20865.12109375\n",
      "Epoch 102601/150000, Loss: 20673.619140625, Validation Loss: 20858.080078125\n",
      "Epoch 102701/150000, Loss: 20666.60546875, Validation Loss: 20851.0390625\n",
      "Epoch 102801/150000, Loss: 20659.59765625, Validation Loss: 20843.99609375\n",
      "Epoch 102901/150000, Loss: 20652.587890625, Validation Loss: 20836.958984375\n",
      "Epoch 103001/150000, Loss: 20645.58203125, Validation Loss: 20829.927734375\n",
      "Epoch 103101/150000, Loss: 20638.580078125, Validation Loss: 20822.89453125\n",
      "Epoch 103201/150000, Loss: 20631.578125, Validation Loss: 20815.8671875\n",
      "Epoch 103301/150000, Loss: 20624.578125, Validation Loss: 20808.83984375\n",
      "Epoch 103401/150000, Loss: 20617.583984375, Validation Loss: 20801.814453125\n",
      "Epoch 103501/150000, Loss: 20610.587890625, Validation Loss: 20794.79296875\n",
      "Epoch 103601/150000, Loss: 20603.59765625, Validation Loss: 20787.771484375\n",
      "Epoch 103701/150000, Loss: 20596.61328125, Validation Loss: 20780.7578125\n",
      "Epoch 103801/150000, Loss: 20589.626953125, Validation Loss: 20773.740234375\n",
      "Epoch 103901/150000, Loss: 20582.642578125, Validation Loss: 20766.732421875\n",
      "Epoch 104001/150000, Loss: 20575.66015625, Validation Loss: 20759.720703125\n",
      "Epoch 104101/150000, Loss: 20568.681640625, Validation Loss: 20752.712890625\n",
      "Epoch 104201/150000, Loss: 20561.705078125, Validation Loss: 20745.708984375\n",
      "Epoch 104301/150000, Loss: 20554.73046875, Validation Loss: 20738.703125\n",
      "Epoch 104401/150000, Loss: 20547.759765625, Validation Loss: 20731.705078125\n",
      "Epoch 104501/150000, Loss: 20540.791015625, Validation Loss: 20724.705078125\n",
      "Epoch 104601/150000, Loss: 20533.82421875, Validation Loss: 20717.712890625\n",
      "Epoch 104701/150000, Loss: 20526.859375, Validation Loss: 20710.716796875\n",
      "Epoch 104801/150000, Loss: 20519.900390625, Validation Loss: 20703.728515625\n",
      "Epoch 104901/150000, Loss: 20512.9375, Validation Loss: 20696.740234375\n",
      "Epoch 105001/150000, Loss: 20505.982421875, Validation Loss: 20689.75390625\n",
      "Epoch 105101/150000, Loss: 20499.02734375, Validation Loss: 20682.771484375\n",
      "Epoch 105201/150000, Loss: 20492.07421875, Validation Loss: 20675.791015625\n",
      "Epoch 105301/150000, Loss: 20485.125, Validation Loss: 20668.810546875\n",
      "Epoch 105401/150000, Loss: 20478.1796875, Validation Loss: 20661.8359375\n",
      "Epoch 105501/150000, Loss: 20471.232421875, Validation Loss: 20654.8671875\n",
      "Epoch 105601/150000, Loss: 20464.2890625, Validation Loss: 20647.896484375\n",
      "Epoch 105701/150000, Loss: 20457.349609375, Validation Loss: 20640.9296875\n",
      "Epoch 105801/150000, Loss: 20450.412109375, Validation Loss: 20633.962890625\n",
      "Epoch 105901/150000, Loss: 20443.474609375, Validation Loss: 20626.99609375\n",
      "Epoch 106001/150000, Loss: 20436.54296875, Validation Loss: 20620.037109375\n",
      "Epoch 106101/150000, Loss: 20429.609375, Validation Loss: 20613.07421875\n",
      "Epoch 106201/150000, Loss: 20422.68359375, Validation Loss: 20606.115234375\n",
      "Epoch 106301/150000, Loss: 20415.7578125, Validation Loss: 20599.15625\n",
      "Epoch 106401/150000, Loss: 20408.833984375, Validation Loss: 20592.201171875\n",
      "Epoch 106501/150000, Loss: 20401.9140625, Validation Loss: 20585.24609375\n",
      "Epoch 106601/150000, Loss: 20394.99609375, Validation Loss: 20578.2890625\n",
      "Epoch 106701/150000, Loss: 20388.076171875, Validation Loss: 20571.337890625\n",
      "Epoch 106801/150000, Loss: 20381.1640625, Validation Loss: 20564.38671875\n",
      "Epoch 106901/150000, Loss: 20374.25390625, Validation Loss: 20557.435546875\n",
      "Epoch 107001/150000, Loss: 20367.34375, Validation Loss: 20550.486328125\n",
      "Epoch 107101/150000, Loss: 20360.4375, Validation Loss: 20543.541015625\n",
      "Epoch 107201/150000, Loss: 20353.533203125, Validation Loss: 20536.595703125\n",
      "Epoch 107301/150000, Loss: 20346.6328125, Validation Loss: 20529.65234375\n",
      "Epoch 107401/150000, Loss: 20339.732421875, Validation Loss: 20522.708984375\n",
      "Epoch 107501/150000, Loss: 20332.833984375, Validation Loss: 20515.7734375\n",
      "Epoch 107601/150000, Loss: 20325.94140625, Validation Loss: 20508.8359375\n",
      "Epoch 107701/150000, Loss: 20319.048828125, Validation Loss: 20501.904296875\n",
      "Epoch 107801/150000, Loss: 20312.16015625, Validation Loss: 20494.96875\n",
      "Epoch 107901/150000, Loss: 20305.271484375, Validation Loss: 20488.0390625\n",
      "Epoch 108001/150000, Loss: 20298.38671875, Validation Loss: 20481.111328125\n",
      "Epoch 108101/150000, Loss: 20291.505859375, Validation Loss: 20474.185546875\n",
      "Epoch 108201/150000, Loss: 20284.625, Validation Loss: 20467.259765625\n",
      "Epoch 108301/150000, Loss: 20277.74609375, Validation Loss: 20460.337890625\n",
      "Epoch 108401/150000, Loss: 20270.869140625, Validation Loss: 20453.419921875\n",
      "Epoch 108501/150000, Loss: 20263.998046875, Validation Loss: 20446.50390625\n",
      "Epoch 108601/150000, Loss: 20257.126953125, Validation Loss: 20439.5859375\n",
      "Epoch 108701/150000, Loss: 20250.26171875, Validation Loss: 20432.673828125\n",
      "Epoch 108801/150000, Loss: 20243.39453125, Validation Loss: 20425.763671875\n",
      "Epoch 108901/150000, Loss: 20236.529296875, Validation Loss: 20418.857421875\n",
      "Epoch 109001/150000, Loss: 20229.671875, Validation Loss: 20411.951171875\n",
      "Epoch 109101/150000, Loss: 20222.80859375, Validation Loss: 20405.05078125\n",
      "Epoch 109201/150000, Loss: 20215.953125, Validation Loss: 20398.150390625\n",
      "Epoch 109301/150000, Loss: 20209.099609375, Validation Loss: 20391.25390625\n",
      "Epoch 109401/150000, Loss: 20202.25, Validation Loss: 20384.361328125\n",
      "Epoch 109501/150000, Loss: 20195.3984375, Validation Loss: 20377.466796875\n",
      "Epoch 109601/150000, Loss: 20188.552734375, Validation Loss: 20370.580078125\n",
      "Epoch 109701/150000, Loss: 20181.6875, Validation Loss: 20363.669921875\n",
      "Epoch 109801/150000, Loss: 20174.80859375, Validation Loss: 20356.751953125\n",
      "Epoch 109901/150000, Loss: 20167.931640625, Validation Loss: 20349.83203125\n",
      "Epoch 110001/150000, Loss: 20161.048828125, Validation Loss: 20342.90625\n",
      "Epoch 110101/150000, Loss: 20154.166015625, Validation Loss: 20335.982421875\n",
      "Epoch 110201/150000, Loss: 20147.28125, Validation Loss: 20329.060546875\n",
      "Epoch 110301/150000, Loss: 20140.400390625, Validation Loss: 20322.138671875\n",
      "Epoch 110401/150000, Loss: 20133.51953125, Validation Loss: 20315.220703125\n",
      "Epoch 110501/150000, Loss: 20126.63671875, Validation Loss: 20308.30078125\n",
      "Epoch 110601/150000, Loss: 20119.763671875, Validation Loss: 20301.3828125\n",
      "Epoch 110701/150000, Loss: 20112.888671875, Validation Loss: 20294.470703125\n",
      "Epoch 110801/150000, Loss: 20106.01953125, Validation Loss: 20287.560546875\n",
      "Epoch 110901/150000, Loss: 20099.150390625, Validation Loss: 20280.650390625\n",
      "Epoch 111001/150000, Loss: 20092.28125, Validation Loss: 20273.74609375\n",
      "Epoch 111101/150000, Loss: 20085.41796875, Validation Loss: 20266.83984375\n",
      "Epoch 111201/150000, Loss: 20078.552734375, Validation Loss: 20259.9375\n",
      "Epoch 111301/150000, Loss: 20071.697265625, Validation Loss: 20253.03515625\n",
      "Epoch 111401/150000, Loss: 20064.83984375, Validation Loss: 20246.138671875\n",
      "Epoch 111501/150000, Loss: 20057.984375, Validation Loss: 20239.2421875\n",
      "Epoch 111601/150000, Loss: 20051.1328125, Validation Loss: 20232.349609375\n",
      "Epoch 111701/150000, Loss: 20044.28125, Validation Loss: 20225.45703125\n",
      "Epoch 111801/150000, Loss: 20037.4296875, Validation Loss: 20218.568359375\n",
      "Epoch 111901/150000, Loss: 20030.587890625, Validation Loss: 20211.68359375\n",
      "Epoch 112001/150000, Loss: 20023.744140625, Validation Loss: 20204.798828125\n",
      "Epoch 112101/150000, Loss: 20016.904296875, Validation Loss: 20197.91796875\n",
      "Epoch 112201/150000, Loss: 20010.06640625, Validation Loss: 20191.037109375\n",
      "Epoch 112301/150000, Loss: 20003.2265625, Validation Loss: 20184.158203125\n",
      "Epoch 112401/150000, Loss: 19996.39453125, Validation Loss: 20177.28515625\n",
      "Epoch 112501/150000, Loss: 19989.56640625, Validation Loss: 20170.4140625\n",
      "Epoch 112601/150000, Loss: 19982.734375, Validation Loss: 20163.541015625\n",
      "Epoch 112701/150000, Loss: 19975.908203125, Validation Loss: 20156.673828125\n",
      "Epoch 112801/150000, Loss: 19969.08203125, Validation Loss: 20149.810546875\n",
      "Epoch 112901/150000, Loss: 19962.26171875, Validation Loss: 20142.94921875\n",
      "Epoch 113001/150000, Loss: 19955.4453125, Validation Loss: 20136.0859375\n",
      "Epoch 113101/150000, Loss: 19948.625, Validation Loss: 20129.228515625\n",
      "Epoch 113201/150000, Loss: 19941.810546875, Validation Loss: 20122.369140625\n",
      "Epoch 113301/150000, Loss: 19935.0, Validation Loss: 20115.517578125\n",
      "Epoch 113401/150000, Loss: 19928.1875, Validation Loss: 20108.666015625\n",
      "Epoch 113501/150000, Loss: 19921.380859375, Validation Loss: 20101.81640625\n",
      "Epoch 113601/150000, Loss: 19914.57421875, Validation Loss: 20094.970703125\n",
      "Epoch 113701/150000, Loss: 19907.7734375, Validation Loss: 20088.125\n",
      "Epoch 113801/150000, Loss: 19900.96875, Validation Loss: 20081.279296875\n",
      "Epoch 113901/150000, Loss: 19894.16796875, Validation Loss: 20074.439453125\n",
      "Epoch 114001/150000, Loss: 19887.365234375, Validation Loss: 20067.595703125\n",
      "Epoch 114101/150000, Loss: 19880.564453125, Validation Loss: 20060.751953125\n",
      "Epoch 114201/150000, Loss: 19873.767578125, Validation Loss: 20053.912109375\n",
      "Epoch 114301/150000, Loss: 19866.974609375, Validation Loss: 20047.0703125\n",
      "Epoch 114401/150000, Loss: 19860.17578125, Validation Loss: 20040.234375\n",
      "Epoch 114501/150000, Loss: 19853.384765625, Validation Loss: 20033.3984375\n",
      "Epoch 114601/150000, Loss: 19846.58984375, Validation Loss: 20026.5625\n",
      "Epoch 114701/150000, Loss: 19839.802734375, Validation Loss: 20019.728515625\n",
      "Epoch 114801/150000, Loss: 19833.013671875, Validation Loss: 20012.8984375\n",
      "Epoch 114901/150000, Loss: 19826.2265625, Validation Loss: 20006.06640625\n",
      "Epoch 115001/150000, Loss: 19819.44140625, Validation Loss: 19999.240234375\n",
      "Epoch 115101/150000, Loss: 19812.654296875, Validation Loss: 19992.412109375\n",
      "Epoch 115201/150000, Loss: 19805.876953125, Validation Loss: 19985.58984375\n",
      "Epoch 115301/150000, Loss: 19799.099609375, Validation Loss: 19978.765625\n",
      "Epoch 115401/150000, Loss: 19792.3203125, Validation Loss: 19971.9453125\n",
      "Epoch 115501/150000, Loss: 19785.546875, Validation Loss: 19965.12890625\n",
      "Epoch 115601/150000, Loss: 19778.7734375, Validation Loss: 19958.31640625\n",
      "Epoch 115701/150000, Loss: 19772.005859375, Validation Loss: 19951.501953125\n",
      "Epoch 115801/150000, Loss: 19765.23828125, Validation Loss: 19944.689453125\n",
      "Epoch 115901/150000, Loss: 19758.4765625, Validation Loss: 19937.8828125\n",
      "Epoch 116001/150000, Loss: 19751.712890625, Validation Loss: 19931.076171875\n",
      "Epoch 116101/150000, Loss: 19744.953125, Validation Loss: 19924.271484375\n",
      "Epoch 116201/150000, Loss: 19738.193359375, Validation Loss: 19917.470703125\n",
      "Epoch 116301/150000, Loss: 19731.4375, Validation Loss: 19910.673828125\n",
      "Epoch 116401/150000, Loss: 19724.6875, Validation Loss: 19903.87890625\n",
      "Epoch 116501/150000, Loss: 19717.935546875, Validation Loss: 19897.083984375\n",
      "Epoch 116601/150000, Loss: 19711.1875, Validation Loss: 19890.291015625\n",
      "Epoch 116701/150000, Loss: 19704.44140625, Validation Loss: 19883.501953125\n",
      "Epoch 116801/150000, Loss: 19697.69921875, Validation Loss: 19876.71484375\n",
      "Epoch 116901/150000, Loss: 19690.95703125, Validation Loss: 19869.93359375\n",
      "Epoch 117001/150000, Loss: 19684.216796875, Validation Loss: 19863.150390625\n",
      "Epoch 117101/150000, Loss: 19677.48046875, Validation Loss: 19856.369140625\n",
      "Epoch 117201/150000, Loss: 19670.75, Validation Loss: 19849.595703125\n",
      "Epoch 117301/150000, Loss: 19664.015625, Validation Loss: 19842.818359375\n",
      "Epoch 117401/150000, Loss: 19657.28515625, Validation Loss: 19836.048828125\n",
      "Epoch 117501/150000, Loss: 19650.55859375, Validation Loss: 19829.27734375\n",
      "Epoch 117601/150000, Loss: 19643.833984375, Validation Loss: 19822.51171875\n",
      "Epoch 117701/150000, Loss: 19637.109375, Validation Loss: 19815.74609375\n",
      "Epoch 117801/150000, Loss: 19630.390625, Validation Loss: 19808.984375\n",
      "Epoch 117901/150000, Loss: 19623.66796875, Validation Loss: 19802.228515625\n",
      "Epoch 118001/150000, Loss: 19616.955078125, Validation Loss: 19795.470703125\n",
      "Epoch 118101/150000, Loss: 19610.244140625, Validation Loss: 19788.71484375\n",
      "Epoch 118201/150000, Loss: 19603.529296875, Validation Loss: 19781.96484375\n",
      "Epoch 118301/150000, Loss: 19596.8203125, Validation Loss: 19775.21484375\n",
      "Epoch 118401/150000, Loss: 19590.115234375, Validation Loss: 19768.470703125\n",
      "Epoch 118501/150000, Loss: 19583.41015625, Validation Loss: 19761.724609375\n",
      "Epoch 118601/150000, Loss: 19576.708984375, Validation Loss: 19754.984375\n",
      "Epoch 118701/150000, Loss: 19570.009765625, Validation Loss: 19748.24609375\n",
      "Epoch 118801/150000, Loss: 19563.310546875, Validation Loss: 19741.51171875\n",
      "Epoch 118901/150000, Loss: 19556.6171875, Validation Loss: 19734.779296875\n",
      "Epoch 119001/150000, Loss: 19549.923828125, Validation Loss: 19728.048828125\n",
      "Epoch 119101/150000, Loss: 19543.23046875, Validation Loss: 19721.318359375\n",
      "Epoch 119201/150000, Loss: 19536.544921875, Validation Loss: 19714.59375\n",
      "Epoch 119301/150000, Loss: 19529.861328125, Validation Loss: 19707.869140625\n",
      "Epoch 119401/150000, Loss: 19523.173828125, Validation Loss: 19701.146484375\n",
      "Epoch 119501/150000, Loss: 19516.4921875, Validation Loss: 19694.4296875\n",
      "Epoch 119601/150000, Loss: 19509.81640625, Validation Loss: 19687.712890625\n",
      "Epoch 119701/150000, Loss: 19503.138671875, Validation Loss: 19680.998046875\n",
      "Epoch 119801/150000, Loss: 19496.46484375, Validation Loss: 19674.291015625\n",
      "Epoch 119901/150000, Loss: 19489.791015625, Validation Loss: 19667.580078125\n",
      "Epoch 120001/150000, Loss: 19483.125, Validation Loss: 19660.875\n",
      "Epoch 120101/150000, Loss: 19476.453125, Validation Loss: 19654.169921875\n",
      "Epoch 120201/150000, Loss: 19469.7890625, Validation Loss: 19647.470703125\n",
      "Epoch 120301/150000, Loss: 19463.125, Validation Loss: 19640.7734375\n",
      "Epoch 120401/150000, Loss: 19456.466796875, Validation Loss: 19634.078125\n",
      "Epoch 120501/150000, Loss: 19449.80859375, Validation Loss: 19627.3828125\n",
      "Epoch 120601/150000, Loss: 19443.150390625, Validation Loss: 19620.693359375\n",
      "Epoch 120701/150000, Loss: 19436.49609375, Validation Loss: 19614.005859375\n",
      "Epoch 120801/150000, Loss: 19429.845703125, Validation Loss: 19607.31640625\n",
      "Epoch 120901/150000, Loss: 19423.1953125, Validation Loss: 19600.6328125\n",
      "Epoch 121001/150000, Loss: 19416.55078125, Validation Loss: 19593.94921875\n",
      "Epoch 121101/150000, Loss: 19409.904296875, Validation Loss: 19587.26953125\n",
      "Epoch 121201/150000, Loss: 19403.26171875, Validation Loss: 19580.59375\n",
      "Epoch 121301/150000, Loss: 19396.62109375, Validation Loss: 19573.91796875\n",
      "Epoch 121401/150000, Loss: 19389.982421875, Validation Loss: 19567.244140625\n",
      "Epoch 121501/150000, Loss: 19383.345703125, Validation Loss: 19560.576171875\n",
      "Epoch 121601/150000, Loss: 19376.716796875, Validation Loss: 19553.90625\n",
      "Epoch 121701/150000, Loss: 19370.083984375, Validation Loss: 19547.2421875\n",
      "Epoch 121801/150000, Loss: 19363.455078125, Validation Loss: 19540.578125\n",
      "Epoch 121901/150000, Loss: 19356.828125, Validation Loss: 19533.916015625\n",
      "Epoch 122001/150000, Loss: 19350.205078125, Validation Loss: 19527.259765625\n",
      "Epoch 122101/150000, Loss: 19343.583984375, Validation Loss: 19520.6015625\n",
      "Epoch 122201/150000, Loss: 19336.962890625, Validation Loss: 19513.94921875\n",
      "Epoch 122301/150000, Loss: 19330.34375, Validation Loss: 19507.296875\n",
      "Epoch 122401/150000, Loss: 19323.73046875, Validation Loss: 19500.650390625\n",
      "Epoch 122501/150000, Loss: 19317.1171875, Validation Loss: 19494.0\n",
      "Epoch 122601/150000, Loss: 19310.505859375, Validation Loss: 19487.35546875\n",
      "Epoch 122701/150000, Loss: 19303.8984375, Validation Loss: 19480.7109375\n",
      "Epoch 122801/150000, Loss: 19297.29296875, Validation Loss: 19474.07421875\n",
      "Epoch 122901/150000, Loss: 19290.685546875, Validation Loss: 19467.435546875\n",
      "Epoch 123001/150000, Loss: 19284.087890625, Validation Loss: 19460.798828125\n",
      "Epoch 123101/150000, Loss: 19277.486328125, Validation Loss: 19454.1640625\n",
      "Epoch 123201/150000, Loss: 19270.890625, Validation Loss: 19447.53515625\n",
      "Epoch 123301/150000, Loss: 19264.294921875, Validation Loss: 19440.904296875\n",
      "Epoch 123401/150000, Loss: 19257.703125, Validation Loss: 19434.279296875\n",
      "Epoch 123501/150000, Loss: 19251.111328125, Validation Loss: 19427.654296875\n",
      "Epoch 123601/150000, Loss: 19244.5234375, Validation Loss: 19421.03125\n",
      "Epoch 123701/150000, Loss: 19237.9375, Validation Loss: 19414.41015625\n",
      "Epoch 123801/150000, Loss: 19231.35546875, Validation Loss: 19407.791015625\n",
      "Epoch 123901/150000, Loss: 19224.7734375, Validation Loss: 19401.1796875\n",
      "Epoch 124001/150000, Loss: 19218.193359375, Validation Loss: 19394.5625\n",
      "Epoch 124101/150000, Loss: 19211.619140625, Validation Loss: 19387.953125\n",
      "Epoch 124201/150000, Loss: 19205.04296875, Validation Loss: 19381.34375\n",
      "Epoch 124301/150000, Loss: 19198.46875, Validation Loss: 19374.73828125\n",
      "Epoch 124401/150000, Loss: 19191.90234375, Validation Loss: 19368.1328125\n",
      "Epoch 124501/150000, Loss: 19185.33203125, Validation Loss: 19361.533203125\n",
      "Epoch 124601/150000, Loss: 19178.76953125, Validation Loss: 19354.9296875\n",
      "Epoch 124701/150000, Loss: 19172.203125, Validation Loss: 19348.333984375\n",
      "Epoch 124801/150000, Loss: 19165.642578125, Validation Loss: 19341.73828125\n",
      "Epoch 124901/150000, Loss: 19159.083984375, Validation Loss: 19335.146484375\n",
      "Epoch 125001/150000, Loss: 19152.525390625, Validation Loss: 19328.5546875\n",
      "Epoch 125101/150000, Loss: 19145.97265625, Validation Loss: 19321.96484375\n",
      "Epoch 125201/150000, Loss: 19139.421875, Validation Loss: 19315.37890625\n",
      "Epoch 125301/150000, Loss: 19132.869140625, Validation Loss: 19308.794921875\n",
      "Epoch 125401/150000, Loss: 19126.32421875, Validation Loss: 19302.212890625\n",
      "Epoch 125501/150000, Loss: 19119.775390625, Validation Loss: 19295.6328125\n",
      "Epoch 125601/150000, Loss: 19113.234375, Validation Loss: 19289.0546875\n",
      "Epoch 125701/150000, Loss: 19106.6953125, Validation Loss: 19282.48046875\n",
      "Epoch 125801/150000, Loss: 19100.15625, Validation Loss: 19275.904296875\n",
      "Epoch 125901/150000, Loss: 19093.6171875, Validation Loss: 19269.3359375\n",
      "Epoch 126001/150000, Loss: 19087.0859375, Validation Loss: 19262.765625\n",
      "Epoch 126101/150000, Loss: 19080.5546875, Validation Loss: 19256.19921875\n",
      "Epoch 126201/150000, Loss: 19074.0234375, Validation Loss: 19249.634765625\n",
      "Epoch 126301/150000, Loss: 19067.498046875, Validation Loss: 19243.072265625\n",
      "Epoch 126401/150000, Loss: 19060.97265625, Validation Loss: 19236.51171875\n",
      "Epoch 126501/150000, Loss: 19054.44921875, Validation Loss: 19229.955078125\n",
      "Epoch 126601/150000, Loss: 19047.927734375, Validation Loss: 19223.3984375\n",
      "Epoch 126701/150000, Loss: 19041.408203125, Validation Loss: 19216.84765625\n",
      "Epoch 126801/150000, Loss: 19034.89453125, Validation Loss: 19210.29296875\n",
      "Epoch 126901/150000, Loss: 19028.37890625, Validation Loss: 19203.74609375\n",
      "Epoch 127001/150000, Loss: 19021.869140625, Validation Loss: 19197.19921875\n",
      "Epoch 127101/150000, Loss: 19015.359375, Validation Loss: 19190.654296875\n",
      "Epoch 127201/150000, Loss: 19008.8515625, Validation Loss: 19184.111328125\n",
      "Epoch 127301/150000, Loss: 19002.34765625, Validation Loss: 19177.568359375\n",
      "Epoch 127401/150000, Loss: 18995.83984375, Validation Loss: 19171.033203125\n",
      "Epoch 127501/150000, Loss: 18989.34375, Validation Loss: 19164.49609375\n",
      "Epoch 127601/150000, Loss: 18982.84375, Validation Loss: 19157.96484375\n",
      "Epoch 127701/150000, Loss: 18976.34765625, Validation Loss: 19151.427734375\n",
      "Epoch 127801/150000, Loss: 18969.853515625, Validation Loss: 19144.8984375\n",
      "Epoch 127901/150000, Loss: 18963.36328125, Validation Loss: 19138.37109375\n",
      "Epoch 128001/150000, Loss: 18956.873046875, Validation Loss: 19131.84375\n",
      "Epoch 128101/150000, Loss: 18950.38671875, Validation Loss: 19125.322265625\n",
      "Epoch 128201/150000, Loss: 18943.900390625, Validation Loss: 19118.80078125\n",
      "Epoch 128301/150000, Loss: 18937.41796875, Validation Loss: 19112.28125\n",
      "Epoch 128401/150000, Loss: 18930.939453125, Validation Loss: 19105.763671875\n",
      "Epoch 128501/150000, Loss: 18924.4609375, Validation Loss: 19099.24609375\n",
      "Epoch 128601/150000, Loss: 18917.98046875, Validation Loss: 19092.734375\n",
      "Epoch 128701/150000, Loss: 18911.5078125, Validation Loss: 19086.22265625\n",
      "Epoch 128801/150000, Loss: 18905.037109375, Validation Loss: 19079.712890625\n",
      "Epoch 128901/150000, Loss: 18898.568359375, Validation Loss: 19073.208984375\n",
      "Epoch 129001/150000, Loss: 18892.09765625, Validation Loss: 19066.703125\n",
      "Epoch 129101/150000, Loss: 18885.634765625, Validation Loss: 19060.201171875\n",
      "Epoch 129201/150000, Loss: 18879.171875, Validation Loss: 19053.701171875\n",
      "Epoch 129301/150000, Loss: 18872.712890625, Validation Loss: 19047.19921875\n",
      "Epoch 129401/150000, Loss: 18866.251953125, Validation Loss: 19040.705078125\n",
      "Epoch 129501/150000, Loss: 18859.796875, Validation Loss: 19034.21484375\n",
      "Epoch 129601/150000, Loss: 18853.341796875, Validation Loss: 19027.71875\n",
      "Epoch 129701/150000, Loss: 18846.892578125, Validation Loss: 19021.23046875\n",
      "Epoch 129801/150000, Loss: 18840.44140625, Validation Loss: 19014.7421875\n",
      "Epoch 129901/150000, Loss: 18833.994140625, Validation Loss: 19008.259765625\n",
      "Epoch 130001/150000, Loss: 18827.548828125, Validation Loss: 19001.775390625\n",
      "Epoch 130101/150000, Loss: 18821.103515625, Validation Loss: 18995.29296875\n",
      "Epoch 130201/150000, Loss: 18814.662109375, Validation Loss: 18988.81640625\n",
      "Epoch 130301/150000, Loss: 18808.224609375, Validation Loss: 18982.341796875\n",
      "Epoch 130401/150000, Loss: 18801.7890625, Validation Loss: 18975.8671875\n",
      "Epoch 130501/150000, Loss: 18795.353515625, Validation Loss: 18969.39453125\n",
      "Epoch 130601/150000, Loss: 18788.921875, Validation Loss: 18962.92578125\n",
      "Epoch 130701/150000, Loss: 18782.494140625, Validation Loss: 18956.458984375\n",
      "Epoch 130801/150000, Loss: 18776.064453125, Validation Loss: 18949.99609375\n",
      "Epoch 130901/150000, Loss: 18769.638671875, Validation Loss: 18943.529296875\n",
      "Epoch 131001/150000, Loss: 18763.216796875, Validation Loss: 18937.068359375\n",
      "Epoch 131101/150000, Loss: 18756.796875, Validation Loss: 18930.611328125\n",
      "Epoch 131201/150000, Loss: 18750.376953125, Validation Loss: 18924.154296875\n",
      "Epoch 131301/150000, Loss: 18743.9609375, Validation Loss: 18917.701171875\n",
      "Epoch 131401/150000, Loss: 18737.54296875, Validation Loss: 18911.248046875\n",
      "Epoch 131501/150000, Loss: 18731.1328125, Validation Loss: 18904.798828125\n",
      "Epoch 131601/150000, Loss: 18724.72265625, Validation Loss: 18898.349609375\n",
      "Epoch 131701/150000, Loss: 18718.314453125, Validation Loss: 18891.904296875\n",
      "Epoch 131801/150000, Loss: 18711.90625, Validation Loss: 18885.4609375\n",
      "Epoch 131901/150000, Loss: 18705.505859375, Validation Loss: 18879.017578125\n",
      "Epoch 132001/150000, Loss: 18699.103515625, Validation Loss: 18872.580078125\n",
      "Epoch 132101/150000, Loss: 18692.703125, Validation Loss: 18866.138671875\n",
      "Epoch 132201/150000, Loss: 18686.306640625, Validation Loss: 18859.703125\n",
      "Epoch 132301/150000, Loss: 18679.912109375, Validation Loss: 18853.26953125\n",
      "Epoch 132401/150000, Loss: 18673.517578125, Validation Loss: 18846.837890625\n",
      "Epoch 132501/150000, Loss: 18667.12890625, Validation Loss: 18840.40625\n",
      "Epoch 132601/150000, Loss: 18660.740234375, Validation Loss: 18833.978515625\n",
      "Epoch 132701/150000, Loss: 18654.353515625, Validation Loss: 18827.548828125\n",
      "Epoch 132801/150000, Loss: 18647.96875, Validation Loss: 18821.125\n",
      "Epoch 132901/150000, Loss: 18641.587890625, Validation Loss: 18814.701171875\n",
      "Epoch 133001/150000, Loss: 18635.20703125, Validation Loss: 18808.283203125\n",
      "Epoch 133101/150000, Loss: 18628.830078125, Validation Loss: 18801.861328125\n",
      "Epoch 133201/150000, Loss: 18622.453125, Validation Loss: 18795.443359375\n",
      "Epoch 133301/150000, Loss: 18616.078125, Validation Loss: 18789.029296875\n",
      "Epoch 133401/150000, Loss: 18609.708984375, Validation Loss: 18782.619140625\n",
      "Epoch 133501/150000, Loss: 18603.33984375, Validation Loss: 18776.212890625\n",
      "Epoch 133601/150000, Loss: 18596.97265625, Validation Loss: 18769.80859375\n",
      "Epoch 133701/150000, Loss: 18590.607421875, Validation Loss: 18763.404296875\n",
      "Epoch 133801/150000, Loss: 18584.244140625, Validation Loss: 18757.00390625\n",
      "Epoch 133901/150000, Loss: 18577.884765625, Validation Loss: 18750.607421875\n",
      "Epoch 134001/150000, Loss: 18571.525390625, Validation Loss: 18744.208984375\n",
      "Epoch 134101/150000, Loss: 18565.169921875, Validation Loss: 18737.81640625\n",
      "Epoch 134201/150000, Loss: 18558.818359375, Validation Loss: 18731.42578125\n",
      "Epoch 134301/150000, Loss: 18552.46484375, Validation Loss: 18725.03515625\n",
      "Epoch 134401/150000, Loss: 18546.115234375, Validation Loss: 18718.6484375\n",
      "Epoch 134501/150000, Loss: 18539.767578125, Validation Loss: 18712.263671875\n",
      "Epoch 134601/150000, Loss: 18533.421875, Validation Loss: 18705.880859375\n",
      "Epoch 134701/150000, Loss: 18527.078125, Validation Loss: 18699.5\n",
      "Epoch 134801/150000, Loss: 18520.73828125, Validation Loss: 18693.123046875\n",
      "Epoch 134901/150000, Loss: 18514.3984375, Validation Loss: 18686.744140625\n",
      "Epoch 135001/150000, Loss: 18508.0625, Validation Loss: 18680.37109375\n",
      "Epoch 135101/150000, Loss: 18501.7265625, Validation Loss: 18673.99609375\n",
      "Epoch 135201/150000, Loss: 18495.39453125, Validation Loss: 18667.626953125\n",
      "Epoch 135301/150000, Loss: 18489.064453125, Validation Loss: 18661.259765625\n",
      "Epoch 135401/150000, Loss: 18482.73828125, Validation Loss: 18654.89453125\n",
      "Epoch 135501/150000, Loss: 18476.41015625, Validation Loss: 18648.53125\n",
      "Epoch 135601/150000, Loss: 18470.0859375, Validation Loss: 18642.16796875\n",
      "Epoch 135701/150000, Loss: 18463.765625, Validation Loss: 18635.810546875\n",
      "Epoch 135801/150000, Loss: 18457.4453125, Validation Loss: 18629.451171875\n",
      "Epoch 135901/150000, Loss: 18451.130859375, Validation Loss: 18623.095703125\n",
      "Epoch 136001/150000, Loss: 18444.8125, Validation Loss: 18616.7421875\n",
      "Epoch 136101/150000, Loss: 18438.498046875, Validation Loss: 18610.390625\n",
      "Epoch 136201/150000, Loss: 18432.189453125, Validation Loss: 18604.04296875\n",
      "Epoch 136301/150000, Loss: 18425.880859375, Validation Loss: 18597.697265625\n",
      "Epoch 136401/150000, Loss: 18419.572265625, Validation Loss: 18591.353515625\n",
      "Epoch 136501/150000, Loss: 18413.265625, Validation Loss: 18585.009765625\n",
      "Epoch 136601/150000, Loss: 18406.96484375, Validation Loss: 18578.669921875\n",
      "Epoch 136701/150000, Loss: 18400.666015625, Validation Loss: 18572.330078125\n",
      "Epoch 136801/150000, Loss: 18394.369140625, Validation Loss: 18565.994140625\n",
      "Epoch 136901/150000, Loss: 18388.0703125, Validation Loss: 18559.662109375\n",
      "Epoch 137001/150000, Loss: 18381.77734375, Validation Loss: 18553.330078125\n",
      "Epoch 137101/150000, Loss: 18375.484375, Validation Loss: 18547.001953125\n",
      "Epoch 137201/150000, Loss: 18369.193359375, Validation Loss: 18540.67578125\n",
      "Epoch 137301/150000, Loss: 18362.90625, Validation Loss: 18534.349609375\n",
      "Epoch 137401/150000, Loss: 18356.623046875, Validation Loss: 18528.029296875\n",
      "Epoch 137501/150000, Loss: 18350.337890625, Validation Loss: 18521.708984375\n",
      "Epoch 137601/150000, Loss: 18344.056640625, Validation Loss: 18515.392578125\n",
      "Epoch 137701/150000, Loss: 18337.779296875, Validation Loss: 18509.07421875\n",
      "Epoch 137801/150000, Loss: 18331.50390625, Validation Loss: 18502.759765625\n",
      "Epoch 137901/150000, Loss: 18325.2265625, Validation Loss: 18496.451171875\n",
      "Epoch 138001/150000, Loss: 18318.953125, Validation Loss: 18490.140625\n",
      "Epoch 138101/150000, Loss: 18312.68359375, Validation Loss: 18483.83203125\n",
      "Epoch 138201/150000, Loss: 18306.416015625, Validation Loss: 18477.52734375\n",
      "Epoch 138301/150000, Loss: 18300.1484375, Validation Loss: 18471.224609375\n",
      "Epoch 138401/150000, Loss: 18293.884765625, Validation Loss: 18464.923828125\n",
      "Epoch 138501/150000, Loss: 18287.623046875, Validation Loss: 18458.625\n",
      "Epoch 138601/150000, Loss: 18281.361328125, Validation Loss: 18452.328125\n",
      "Epoch 138701/150000, Loss: 18275.10546875, Validation Loss: 18446.03515625\n",
      "Epoch 138801/150000, Loss: 18268.84765625, Validation Loss: 18439.740234375\n",
      "Epoch 138901/150000, Loss: 18262.59375, Validation Loss: 18433.451171875\n",
      "Epoch 139001/150000, Loss: 18256.34375, Validation Loss: 18427.1640625\n",
      "Epoch 139101/150000, Loss: 18250.095703125, Validation Loss: 18420.876953125\n",
      "Epoch 139201/150000, Loss: 18243.84765625, Validation Loss: 18414.59375\n",
      "Epoch 139301/150000, Loss: 18237.6015625, Validation Loss: 18408.3125\n",
      "Epoch 139401/150000, Loss: 18231.357421875, Validation Loss: 18402.03125\n",
      "Epoch 139501/150000, Loss: 18225.12109375, Validation Loss: 18395.75390625\n",
      "Epoch 139601/150000, Loss: 18218.880859375, Validation Loss: 18389.478515625\n",
      "Epoch 139701/150000, Loss: 18212.64453125, Validation Loss: 18383.205078125\n",
      "Epoch 139801/150000, Loss: 18206.408203125, Validation Loss: 18376.93359375\n",
      "Epoch 139901/150000, Loss: 18200.17578125, Validation Loss: 18370.6640625\n",
      "Epoch 140001/150000, Loss: 18193.9453125, Validation Loss: 18364.39453125\n",
      "Epoch 140101/150000, Loss: 18187.71875, Validation Loss: 18358.130859375\n",
      "Epoch 140201/150000, Loss: 18181.4921875, Validation Loss: 18351.869140625\n",
      "Epoch 140301/150000, Loss: 18175.267578125, Validation Loss: 18345.60546875\n",
      "Epoch 140401/150000, Loss: 18169.046875, Validation Loss: 18339.349609375\n",
      "Epoch 140501/150000, Loss: 18162.826171875, Validation Loss: 18333.08984375\n",
      "Epoch 140601/150000, Loss: 18156.609375, Validation Loss: 18326.833984375\n",
      "Epoch 140701/150000, Loss: 18150.392578125, Validation Loss: 18320.583984375\n",
      "Epoch 140801/150000, Loss: 18144.1796875, Validation Loss: 18314.333984375\n",
      "Epoch 140901/150000, Loss: 18137.966796875, Validation Loss: 18308.083984375\n",
      "Epoch 141001/150000, Loss: 18131.7578125, Validation Loss: 18301.8359375\n",
      "Epoch 141101/150000, Loss: 18125.55078125, Validation Loss: 18295.591796875\n",
      "Epoch 141201/150000, Loss: 18119.34765625, Validation Loss: 18289.349609375\n",
      "Epoch 141301/150000, Loss: 18113.142578125, Validation Loss: 18283.10546875\n",
      "Epoch 141401/150000, Loss: 18106.94140625, Validation Loss: 18276.869140625\n",
      "Epoch 141501/150000, Loss: 18100.744140625, Validation Loss: 18270.630859375\n",
      "Epoch 141601/150000, Loss: 18094.546875, Validation Loss: 18264.3984375\n",
      "Epoch 141701/150000, Loss: 18088.353515625, Validation Loss: 18258.16796875\n",
      "Epoch 141801/150000, Loss: 18082.16015625, Validation Loss: 18251.9375\n",
      "Epoch 141901/150000, Loss: 18075.970703125, Validation Loss: 18245.70703125\n",
      "Epoch 142001/150000, Loss: 18069.783203125, Validation Loss: 18239.482421875\n",
      "Epoch 142101/150000, Loss: 18063.59375, Validation Loss: 18233.2578125\n",
      "Epoch 142201/150000, Loss: 18057.412109375, Validation Loss: 18227.03515625\n",
      "Epoch 142301/150000, Loss: 18051.228515625, Validation Loss: 18220.81640625\n",
      "Epoch 142401/150000, Loss: 18045.05078125, Validation Loss: 18214.595703125\n",
      "Epoch 142501/150000, Loss: 18038.873046875, Validation Loss: 18208.380859375\n",
      "Epoch 142601/150000, Loss: 18032.69921875, Validation Loss: 18202.16796875\n",
      "Epoch 142701/150000, Loss: 18026.5234375, Validation Loss: 18195.955078125\n",
      "Epoch 142801/150000, Loss: 18020.349609375, Validation Loss: 18189.74609375\n",
      "Epoch 142901/150000, Loss: 18014.181640625, Validation Loss: 18183.537109375\n",
      "Epoch 143001/150000, Loss: 18008.017578125, Validation Loss: 18177.33203125\n",
      "Epoch 143101/150000, Loss: 18001.849609375, Validation Loss: 18171.125\n",
      "Epoch 143201/150000, Loss: 17995.6875, Validation Loss: 18164.92578125\n",
      "Epoch 143301/150000, Loss: 17989.525390625, Validation Loss: 18158.728515625\n",
      "Epoch 143401/150000, Loss: 17983.369140625, Validation Loss: 18152.529296875\n",
      "Epoch 143501/150000, Loss: 17977.2109375, Validation Loss: 18146.3359375\n",
      "Epoch 143601/150000, Loss: 17971.056640625, Validation Loss: 18140.140625\n",
      "Epoch 143701/150000, Loss: 17964.904296875, Validation Loss: 18133.951171875\n",
      "Epoch 143801/150000, Loss: 17958.75390625, Validation Loss: 18127.759765625\n",
      "Epoch 143901/150000, Loss: 17952.607421875, Validation Loss: 18121.57421875\n",
      "Epoch 144001/150000, Loss: 17946.4609375, Validation Loss: 18115.388671875\n",
      "Epoch 144101/150000, Loss: 17940.31640625, Validation Loss: 18109.20703125\n",
      "Epoch 144201/150000, Loss: 17934.17578125, Validation Loss: 18103.0234375\n",
      "Epoch 144301/150000, Loss: 17928.03515625, Validation Loss: 18096.84765625\n",
      "Epoch 144401/150000, Loss: 17921.896484375, Validation Loss: 18090.669921875\n",
      "Epoch 144501/150000, Loss: 17915.76171875, Validation Loss: 18084.494140625\n",
      "Epoch 144601/150000, Loss: 17909.62890625, Validation Loss: 18078.322265625\n",
      "Epoch 144701/150000, Loss: 17903.49609375, Validation Loss: 18072.150390625\n",
      "Epoch 144801/150000, Loss: 17897.36328125, Validation Loss: 18065.984375\n",
      "Epoch 144901/150000, Loss: 17891.240234375, Validation Loss: 18059.81640625\n",
      "Epoch 145001/150000, Loss: 17885.115234375, Validation Loss: 18053.65234375\n",
      "Epoch 145101/150000, Loss: 17878.990234375, Validation Loss: 18047.48828125\n",
      "Epoch 145201/150000, Loss: 17872.8671875, Validation Loss: 18041.330078125\n",
      "Epoch 145301/150000, Loss: 17866.75, Validation Loss: 18035.171875\n",
      "Epoch 145401/150000, Loss: 17860.6328125, Validation Loss: 18029.015625\n",
      "Epoch 145501/150000, Loss: 17854.51953125, Validation Loss: 18022.859375\n",
      "Epoch 145601/150000, Loss: 17848.404296875, Validation Loss: 18016.708984375\n",
      "Epoch 145701/150000, Loss: 17842.296875, Validation Loss: 18010.560546875\n",
      "Epoch 145801/150000, Loss: 17836.185546875, Validation Loss: 18004.4140625\n",
      "Epoch 145901/150000, Loss: 17830.080078125, Validation Loss: 17998.265625\n",
      "Epoch 146001/150000, Loss: 17823.9765625, Validation Loss: 17992.12109375\n",
      "Epoch 146101/150000, Loss: 17817.87109375, Validation Loss: 17985.982421875\n",
      "Epoch 146201/150000, Loss: 17811.775390625, Validation Loss: 17979.84375\n",
      "Epoch 146301/150000, Loss: 17805.673828125, Validation Loss: 17973.705078125\n",
      "Epoch 146401/150000, Loss: 17799.580078125, Validation Loss: 17967.568359375\n",
      "Epoch 146501/150000, Loss: 17793.486328125, Validation Loss: 17961.435546875\n",
      "Epoch 146601/150000, Loss: 17787.39453125, Validation Loss: 17955.3046875\n",
      "Epoch 146701/150000, Loss: 17781.3046875, Validation Loss: 17949.17578125\n",
      "Epoch 146801/150000, Loss: 17775.21484375, Validation Loss: 17943.048828125\n",
      "Epoch 146901/150000, Loss: 17769.130859375, Validation Loss: 17936.92578125\n",
      "Epoch 147001/150000, Loss: 17763.046875, Validation Loss: 17930.8046875\n",
      "Epoch 147101/150000, Loss: 17756.96484375, Validation Loss: 17924.68359375\n",
      "Epoch 147201/150000, Loss: 17750.884765625, Validation Loss: 17918.56640625\n",
      "Epoch 147301/150000, Loss: 17744.810546875, Validation Loss: 17912.447265625\n",
      "Epoch 147401/150000, Loss: 17738.732421875, Validation Loss: 17906.333984375\n",
      "Epoch 147501/150000, Loss: 17732.66015625, Validation Loss: 17900.220703125\n",
      "Epoch 147601/150000, Loss: 17726.587890625, Validation Loss: 17894.111328125\n",
      "Epoch 147701/150000, Loss: 17720.521484375, Validation Loss: 17888.00390625\n",
      "Epoch 147801/150000, Loss: 17714.453125, Validation Loss: 17881.900390625\n",
      "Epoch 147901/150000, Loss: 17708.388671875, Validation Loss: 17875.794921875\n",
      "Epoch 148001/150000, Loss: 17702.326171875, Validation Loss: 17869.693359375\n",
      "Epoch 148101/150000, Loss: 17696.267578125, Validation Loss: 17863.59375\n",
      "Epoch 148201/150000, Loss: 17690.205078125, Validation Loss: 17857.49609375\n",
      "Epoch 148301/150000, Loss: 17684.150390625, Validation Loss: 17851.400390625\n",
      "Epoch 148401/150000, Loss: 17678.095703125, Validation Loss: 17845.310546875\n",
      "Epoch 148501/150000, Loss: 17672.04296875, Validation Loss: 17839.216796875\n",
      "Epoch 148601/150000, Loss: 17665.994140625, Validation Loss: 17833.126953125\n",
      "Epoch 148701/150000, Loss: 17659.947265625, Validation Loss: 17827.041015625\n",
      "Epoch 148801/150000, Loss: 17653.900390625, Validation Loss: 17820.958984375\n",
      "Epoch 148901/150000, Loss: 17647.85546875, Validation Loss: 17814.873046875\n",
      "Epoch 149001/150000, Loss: 17641.810546875, Validation Loss: 17808.791015625\n",
      "Epoch 149101/150000, Loss: 17635.771484375, Validation Loss: 17802.71484375\n",
      "Epoch 149201/150000, Loss: 17629.736328125, Validation Loss: 17796.638671875\n",
      "Epoch 149301/150000, Loss: 17623.69921875, Validation Loss: 17790.564453125\n",
      "Epoch 149401/150000, Loss: 17617.6640625, Validation Loss: 17784.4921875\n",
      "Epoch 149501/150000, Loss: 17611.634765625, Validation Loss: 17778.421875\n",
      "Epoch 149601/150000, Loss: 17605.60546875, Validation Loss: 17772.35546875\n",
      "Epoch 149701/150000, Loss: 17599.576171875, Validation Loss: 17766.287109375\n",
      "Epoch 149801/150000, Loss: 17593.548828125, Validation Loss: 17760.224609375\n",
      "Epoch 149901/150000, Loss: 17587.525390625, Validation Loss: 17754.162109375\n",
      "Test Loss: 17565.03515625\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=1e-05, window_size=5\n",
      "Epoch 1/150000, Loss: 30026.98828125, Validation Loss: 30246.708984375\n",
      "Epoch 101/150000, Loss: 30026.021484375, Validation Loss: 30245.7421875\n",
      "Epoch 201/150000, Loss: 30025.0625, Validation Loss: 30244.775390625\n",
      "Epoch 301/150000, Loss: 30024.103515625, Validation Loss: 30243.81640625\n",
      "Epoch 401/150000, Loss: 30023.1484375, Validation Loss: 30242.869140625\n",
      "Epoch 501/150000, Loss: 30022.208984375, Validation Loss: 30241.916015625\n",
      "Epoch 601/150000, Loss: 30021.26171875, Validation Loss: 30240.97265625\n",
      "Epoch 701/150000, Loss: 30020.32421875, Validation Loss: 30240.033203125\n",
      "Epoch 801/150000, Loss: 30019.390625, Validation Loss: 30239.095703125\n",
      "Epoch 901/150000, Loss: 30018.458984375, Validation Loss: 30238.162109375\n",
      "Epoch 1001/150000, Loss: 30017.53125, Validation Loss: 30237.236328125\n",
      "Epoch 1101/150000, Loss: 30016.61328125, Validation Loss: 30236.314453125\n",
      "Epoch 1201/150000, Loss: 30015.69140625, Validation Loss: 30235.39453125\n",
      "Epoch 1301/150000, Loss: 30014.779296875, Validation Loss: 30234.478515625\n",
      "Epoch 1401/150000, Loss: 30013.869140625, Validation Loss: 30233.5625\n",
      "Epoch 1501/150000, Loss: 30012.9609375, Validation Loss: 30232.65234375\n",
      "Epoch 1601/150000, Loss: 30012.060546875, Validation Loss: 30231.748046875\n",
      "Epoch 1701/150000, Loss: 30011.158203125, Validation Loss: 30230.84765625\n",
      "Epoch 1801/150000, Loss: 30010.26171875, Validation Loss: 30229.94921875\n",
      "Epoch 1901/150000, Loss: 30009.365234375, Validation Loss: 30229.0546875\n",
      "Epoch 2001/150000, Loss: 30008.478515625, Validation Loss: 30228.162109375\n",
      "Epoch 2101/150000, Loss: 30007.591796875, Validation Loss: 30227.271484375\n",
      "Epoch 2201/150000, Loss: 30006.70703125, Validation Loss: 30226.388671875\n",
      "Epoch 2301/150000, Loss: 30005.828125, Validation Loss: 30225.50390625\n",
      "Epoch 2401/150000, Loss: 30004.953125, Validation Loss: 30224.625\n",
      "Epoch 2501/150000, Loss: 30004.07421875, Validation Loss: 30223.748046875\n",
      "Epoch 2601/150000, Loss: 30003.201171875, Validation Loss: 30222.875\n",
      "Epoch 2701/150000, Loss: 30002.337890625, Validation Loss: 30222.0\n",
      "Epoch 2801/150000, Loss: 30001.47265625, Validation Loss: 30221.134765625\n",
      "Epoch 2901/150000, Loss: 30000.609375, Validation Loss: 30220.275390625\n",
      "Epoch 3001/150000, Loss: 29999.751953125, Validation Loss: 30219.416015625\n",
      "Epoch 3101/150000, Loss: 29998.896484375, Validation Loss: 30218.55078125\n",
      "Epoch 3201/150000, Loss: 29998.041015625, Validation Loss: 30217.697265625\n",
      "Epoch 3301/150000, Loss: 29997.1875, Validation Loss: 30216.84375\n",
      "Epoch 3401/150000, Loss: 29996.341796875, Validation Loss: 30215.99609375\n",
      "Epoch 3501/150000, Loss: 29995.494140625, Validation Loss: 30215.146484375\n",
      "Epoch 3601/150000, Loss: 29994.654296875, Validation Loss: 30214.302734375\n",
      "Epoch 3701/150000, Loss: 29993.814453125, Validation Loss: 30213.4609375\n",
      "Epoch 3801/150000, Loss: 29992.978515625, Validation Loss: 30212.623046875\n",
      "Epoch 3901/150000, Loss: 29992.14453125, Validation Loss: 30211.78515625\n",
      "Epoch 4001/150000, Loss: 29991.314453125, Validation Loss: 30210.951171875\n",
      "Epoch 4101/150000, Loss: 29990.484375, Validation Loss: 30210.119140625\n",
      "Epoch 4201/150000, Loss: 29989.65625, Validation Loss: 30209.291015625\n",
      "Epoch 4301/150000, Loss: 29988.8359375, Validation Loss: 30208.46484375\n",
      "Epoch 4401/150000, Loss: 29988.013671875, Validation Loss: 30207.64453125\n",
      "Epoch 4501/150000, Loss: 29987.197265625, Validation Loss: 30206.82421875\n",
      "Epoch 4601/150000, Loss: 29986.37890625, Validation Loss: 30206.005859375\n",
      "Epoch 4701/150000, Loss: 29985.568359375, Validation Loss: 30205.189453125\n",
      "Epoch 4801/150000, Loss: 29984.755859375, Validation Loss: 30204.376953125\n",
      "Epoch 4901/150000, Loss: 29983.9453125, Validation Loss: 30203.56640625\n",
      "Epoch 5001/150000, Loss: 29983.140625, Validation Loss: 30202.7578125\n",
      "Epoch 5101/150000, Loss: 29982.3359375, Validation Loss: 30201.951171875\n",
      "Epoch 5201/150000, Loss: 29981.53515625, Validation Loss: 30201.146484375\n",
      "Epoch 5301/150000, Loss: 29980.736328125, Validation Loss: 30200.34375\n",
      "Epoch 5401/150000, Loss: 29979.9375, Validation Loss: 30199.541015625\n",
      "Epoch 5501/150000, Loss: 29979.14453125, Validation Loss: 30198.748046875\n",
      "Epoch 5601/150000, Loss: 29978.349609375, Validation Loss: 30197.951171875\n",
      "Epoch 5701/150000, Loss: 29977.55859375, Validation Loss: 30197.158203125\n",
      "Epoch 5801/150000, Loss: 29976.76953125, Validation Loss: 30196.369140625\n",
      "Epoch 5901/150000, Loss: 29975.984375, Validation Loss: 30195.578125\n",
      "Epoch 6001/150000, Loss: 29975.19921875, Validation Loss: 30194.791015625\n",
      "Epoch 6101/150000, Loss: 29974.4140625, Validation Loss: 30194.00390625\n",
      "Epoch 6201/150000, Loss: 29973.6328125, Validation Loss: 30193.220703125\n",
      "Epoch 6301/150000, Loss: 29972.853515625, Validation Loss: 30192.4375\n",
      "Epoch 6401/150000, Loss: 29972.0703125, Validation Loss: 30191.65625\n",
      "Epoch 6501/150000, Loss: 29971.296875, Validation Loss: 30190.87109375\n",
      "Epoch 6601/150000, Loss: 29970.517578125, Validation Loss: 30190.095703125\n",
      "Epoch 6701/150000, Loss: 29969.740234375, Validation Loss: 30189.314453125\n",
      "Epoch 6801/150000, Loss: 29968.966796875, Validation Loss: 30188.53515625\n",
      "Epoch 6901/150000, Loss: 29968.193359375, Validation Loss: 30187.759765625\n",
      "Epoch 7001/150000, Loss: 29967.416015625, Validation Loss: 30186.982421875\n",
      "Epoch 7101/150000, Loss: 29966.642578125, Validation Loss: 30186.203125\n",
      "Epoch 7201/150000, Loss: 29965.8671875, Validation Loss: 30185.421875\n",
      "Epoch 7301/150000, Loss: 29965.091796875, Validation Loss: 30184.646484375\n",
      "Epoch 7401/150000, Loss: 29964.3125, Validation Loss: 30183.869140625\n",
      "Epoch 7501/150000, Loss: 29963.533203125, Validation Loss: 30183.083984375\n",
      "Epoch 7601/150000, Loss: 29962.7578125, Validation Loss: 30182.3046875\n",
      "Epoch 7701/150000, Loss: 29961.9765625, Validation Loss: 30181.521484375\n",
      "Epoch 7801/150000, Loss: 29961.193359375, Validation Loss: 30180.736328125\n",
      "Epoch 7901/150000, Loss: 29960.408203125, Validation Loss: 30179.9453125\n",
      "Epoch 8001/150000, Loss: 29959.625, Validation Loss: 30179.162109375\n",
      "Epoch 8101/150000, Loss: 29958.8359375, Validation Loss: 30178.369140625\n",
      "Epoch 8201/150000, Loss: 29958.046875, Validation Loss: 30177.578125\n",
      "Epoch 8301/150000, Loss: 29957.255859375, Validation Loss: 30176.78125\n",
      "Epoch 8401/150000, Loss: 29956.462890625, Validation Loss: 30175.98828125\n",
      "Epoch 8501/150000, Loss: 29955.66796875, Validation Loss: 30175.189453125\n",
      "Epoch 8601/150000, Loss: 29954.8671875, Validation Loss: 30174.3828125\n",
      "Epoch 8701/150000, Loss: 29954.064453125, Validation Loss: 30173.580078125\n",
      "Epoch 8801/150000, Loss: 29953.26171875, Validation Loss: 30172.771484375\n",
      "Epoch 8901/150000, Loss: 29952.45703125, Validation Loss: 30171.96484375\n",
      "Epoch 9001/150000, Loss: 29951.646484375, Validation Loss: 30171.15234375\n",
      "Epoch 9101/150000, Loss: 29950.837890625, Validation Loss: 30170.341796875\n",
      "Epoch 9201/150000, Loss: 29950.021484375, Validation Loss: 30169.521484375\n",
      "Epoch 9301/150000, Loss: 29949.203125, Validation Loss: 30168.703125\n",
      "Epoch 9401/150000, Loss: 29948.384765625, Validation Loss: 30167.880859375\n",
      "Epoch 9501/150000, Loss: 29947.5625, Validation Loss: 30167.05078125\n",
      "Epoch 9601/150000, Loss: 29946.73828125, Validation Loss: 30166.224609375\n",
      "Epoch 9701/150000, Loss: 29945.912109375, Validation Loss: 30165.39453125\n",
      "Epoch 9801/150000, Loss: 29945.083984375, Validation Loss: 30164.5625\n",
      "Epoch 9901/150000, Loss: 29944.248046875, Validation Loss: 30163.724609375\n",
      "Epoch 10001/150000, Loss: 29943.412109375, Validation Loss: 30162.88671875\n",
      "Epoch 10101/150000, Loss: 29942.572265625, Validation Loss: 30162.041015625\n",
      "Epoch 10201/150000, Loss: 29941.73046875, Validation Loss: 30161.201171875\n",
      "Epoch 10301/150000, Loss: 29940.88671875, Validation Loss: 30160.353515625\n",
      "Epoch 10401/150000, Loss: 29940.0390625, Validation Loss: 30159.50390625\n",
      "Epoch 10501/150000, Loss: 29939.189453125, Validation Loss: 30158.650390625\n",
      "Epoch 10601/150000, Loss: 29938.337890625, Validation Loss: 30157.79296875\n",
      "Epoch 10701/150000, Loss: 29937.482421875, Validation Loss: 30156.9375\n",
      "Epoch 10801/150000, Loss: 29936.626953125, Validation Loss: 30156.07421875\n",
      "Epoch 10901/150000, Loss: 29935.76171875, Validation Loss: 30155.212890625\n",
      "Epoch 11001/150000, Loss: 29934.900390625, Validation Loss: 30154.34375\n",
      "Epoch 11101/150000, Loss: 29934.03125, Validation Loss: 30153.47265625\n",
      "Epoch 11201/150000, Loss: 29933.162109375, Validation Loss: 30152.6015625\n",
      "Epoch 11301/150000, Loss: 29932.2890625, Validation Loss: 30151.73046875\n",
      "Epoch 11401/150000, Loss: 29931.4140625, Validation Loss: 30150.84765625\n",
      "Epoch 11501/150000, Loss: 29930.537109375, Validation Loss: 30149.96484375\n",
      "Epoch 11601/150000, Loss: 29929.654296875, Validation Loss: 30149.080078125\n",
      "Epoch 11701/150000, Loss: 29928.771484375, Validation Loss: 30148.1953125\n",
      "Epoch 11801/150000, Loss: 29927.8828125, Validation Loss: 30147.302734375\n",
      "Epoch 11901/150000, Loss: 29926.9921875, Validation Loss: 30146.41015625\n",
      "Epoch 12001/150000, Loss: 29926.099609375, Validation Loss: 30145.515625\n",
      "Epoch 12101/150000, Loss: 29925.203125, Validation Loss: 30144.6171875\n",
      "Epoch 12201/150000, Loss: 29924.3046875, Validation Loss: 30143.708984375\n",
      "Epoch 12301/150000, Loss: 29923.3984375, Validation Loss: 30142.80859375\n",
      "Epoch 12401/150000, Loss: 29922.494140625, Validation Loss: 30141.8984375\n",
      "Epoch 12501/150000, Loss: 29921.583984375, Validation Loss: 30140.984375\n",
      "Epoch 12601/150000, Loss: 29920.671875, Validation Loss: 30140.068359375\n",
      "Epoch 12701/150000, Loss: 29919.759765625, Validation Loss: 30139.150390625\n",
      "Epoch 12801/150000, Loss: 29918.8359375, Validation Loss: 30138.228515625\n",
      "Epoch 12901/150000, Loss: 29917.916015625, Validation Loss: 30137.302734375\n",
      "Epoch 13001/150000, Loss: 29916.9921875, Validation Loss: 30136.375\n",
      "Epoch 13101/150000, Loss: 29916.0625, Validation Loss: 30135.439453125\n",
      "Epoch 13201/150000, Loss: 29915.12890625, Validation Loss: 30134.505859375\n",
      "Epoch 13301/150000, Loss: 29914.193359375, Validation Loss: 30133.568359375\n",
      "Epoch 13401/150000, Loss: 29913.25390625, Validation Loss: 30132.623046875\n",
      "Epoch 13501/150000, Loss: 29912.310546875, Validation Loss: 30131.68359375\n",
      "Epoch 13601/150000, Loss: 29911.365234375, Validation Loss: 30130.73046875\n",
      "Epoch 13701/150000, Loss: 29910.416015625, Validation Loss: 30129.775390625\n",
      "Epoch 13801/150000, Loss: 29909.458984375, Validation Loss: 30128.814453125\n",
      "Epoch 13901/150000, Loss: 29908.498046875, Validation Loss: 30127.85546875\n",
      "Epoch 14001/150000, Loss: 29907.53515625, Validation Loss: 30126.892578125\n",
      "Epoch 14101/150000, Loss: 29906.568359375, Validation Loss: 30125.921875\n",
      "Epoch 14201/150000, Loss: 29905.595703125, Validation Loss: 30124.9453125\n",
      "Epoch 14301/150000, Loss: 29904.623046875, Validation Loss: 30123.966796875\n",
      "Epoch 14401/150000, Loss: 29903.64453125, Validation Loss: 30122.982421875\n",
      "Epoch 14501/150000, Loss: 29902.65625, Validation Loss: 30121.994140625\n",
      "Epoch 14601/150000, Loss: 29901.6640625, Validation Loss: 30121.00390625\n",
      "Epoch 14701/150000, Loss: 29900.671875, Validation Loss: 30120.00390625\n",
      "Epoch 14801/150000, Loss: 29899.66796875, Validation Loss: 30119.0\n",
      "Epoch 14901/150000, Loss: 29898.662109375, Validation Loss: 30117.98828125\n",
      "Epoch 15001/150000, Loss: 29897.6484375, Validation Loss: 30116.97265625\n",
      "Epoch 15101/150000, Loss: 29896.626953125, Validation Loss: 30115.951171875\n",
      "Epoch 15201/150000, Loss: 29895.603515625, Validation Loss: 30114.921875\n",
      "Epoch 15301/150000, Loss: 29894.5703125, Validation Loss: 30113.8828125\n",
      "Epoch 15401/150000, Loss: 29893.53125, Validation Loss: 30112.84375\n",
      "Epoch 15501/150000, Loss: 29892.48046875, Validation Loss: 30111.79296875\n",
      "Epoch 15601/150000, Loss: 29891.427734375, Validation Loss: 30110.734375\n",
      "Epoch 15701/150000, Loss: 29890.36328125, Validation Loss: 30109.669921875\n",
      "Epoch 15801/150000, Loss: 29889.294921875, Validation Loss: 30108.599609375\n",
      "Epoch 15901/150000, Loss: 29888.216796875, Validation Loss: 30107.517578125\n",
      "Epoch 16001/150000, Loss: 29887.134765625, Validation Loss: 30106.431640625\n",
      "Epoch 16101/150000, Loss: 29886.037109375, Validation Loss: 30105.330078125\n",
      "Epoch 16201/150000, Loss: 29884.935546875, Validation Loss: 30104.228515625\n",
      "Epoch 16301/150000, Loss: 29883.826171875, Validation Loss: 30103.1171875\n",
      "Epoch 16401/150000, Loss: 29882.70703125, Validation Loss: 30101.994140625\n",
      "Epoch 16501/150000, Loss: 29881.58203125, Validation Loss: 30100.865234375\n",
      "Epoch 16601/150000, Loss: 29880.44921875, Validation Loss: 30099.728515625\n",
      "Epoch 16701/150000, Loss: 29879.302734375, Validation Loss: 30098.578125\n",
      "Epoch 16801/150000, Loss: 29878.154296875, Validation Loss: 30097.42578125\n",
      "Epoch 16901/150000, Loss: 29876.990234375, Validation Loss: 30096.259765625\n",
      "Epoch 17001/150000, Loss: 29875.822265625, Validation Loss: 30095.09375\n",
      "Epoch 17101/150000, Loss: 29874.64453125, Validation Loss: 30093.904296875\n",
      "Epoch 17201/150000, Loss: 29873.45703125, Validation Loss: 30092.71875\n",
      "Epoch 17301/150000, Loss: 29872.26171875, Validation Loss: 30091.521484375\n",
      "Epoch 17401/150000, Loss: 29871.060546875, Validation Loss: 30090.310546875\n",
      "Epoch 17501/150000, Loss: 29869.84765625, Validation Loss: 30089.099609375\n",
      "Epoch 17601/150000, Loss: 29868.626953125, Validation Loss: 30087.875\n",
      "Epoch 17701/150000, Loss: 29867.3984375, Validation Loss: 30086.640625\n",
      "Epoch 17801/150000, Loss: 29866.158203125, Validation Loss: 30085.400390625\n",
      "Epoch 17901/150000, Loss: 29864.912109375, Validation Loss: 30084.150390625\n",
      "Epoch 18001/150000, Loss: 29863.654296875, Validation Loss: 30082.892578125\n",
      "Epoch 18101/150000, Loss: 29862.38671875, Validation Loss: 30081.623046875\n",
      "Epoch 18201/150000, Loss: 29861.11328125, Validation Loss: 30080.34375\n",
      "Epoch 18301/150000, Loss: 29859.833984375, Validation Loss: 30079.056640625\n",
      "Epoch 18401/150000, Loss: 29858.54296875, Validation Loss: 30077.763671875\n",
      "Epoch 18501/150000, Loss: 29857.2421875, Validation Loss: 30076.4609375\n",
      "Epoch 18601/150000, Loss: 29855.931640625, Validation Loss: 30075.150390625\n",
      "Epoch 18701/150000, Loss: 29854.615234375, Validation Loss: 30073.830078125\n",
      "Epoch 18801/150000, Loss: 29853.291015625, Validation Loss: 30072.5\n",
      "Epoch 18901/150000, Loss: 29851.95703125, Validation Loss: 30071.158203125\n",
      "Epoch 19001/150000, Loss: 29850.609375, Validation Loss: 30069.814453125\n",
      "Epoch 19101/150000, Loss: 29849.2578125, Validation Loss: 30068.455078125\n",
      "Epoch 19201/150000, Loss: 29847.8984375, Validation Loss: 30067.08984375\n",
      "Epoch 19301/150000, Loss: 29846.525390625, Validation Loss: 30065.71875\n",
      "Epoch 19401/150000, Loss: 29845.1484375, Validation Loss: 30064.3359375\n",
      "Epoch 19501/150000, Loss: 29843.7578125, Validation Loss: 30062.939453125\n",
      "Epoch 19601/150000, Loss: 29842.361328125, Validation Loss: 30061.541015625\n",
      "Epoch 19701/150000, Loss: 29840.958984375, Validation Loss: 30060.138671875\n",
      "Epoch 19801/150000, Loss: 29839.548828125, Validation Loss: 30058.71875\n",
      "Epoch 19901/150000, Loss: 29838.12109375, Validation Loss: 30057.296875\n",
      "Epoch 20001/150000, Loss: 29836.693359375, Validation Loss: 30055.861328125\n",
      "Epoch 20101/150000, Loss: 29835.25390625, Validation Loss: 30054.419921875\n",
      "Epoch 20201/150000, Loss: 29833.806640625, Validation Loss: 30052.970703125\n",
      "Epoch 20301/150000, Loss: 29832.353515625, Validation Loss: 30051.509765625\n",
      "Epoch 20401/150000, Loss: 29830.890625, Validation Loss: 30050.0390625\n",
      "Epoch 20501/150000, Loss: 29829.419921875, Validation Loss: 30048.56640625\n",
      "Epoch 20601/150000, Loss: 29827.939453125, Validation Loss: 30047.083984375\n",
      "Epoch 20701/150000, Loss: 29826.453125, Validation Loss: 30045.59375\n",
      "Epoch 20801/150000, Loss: 29824.955078125, Validation Loss: 30044.09375\n",
      "Epoch 20901/150000, Loss: 29823.453125, Validation Loss: 30042.5859375\n",
      "Epoch 21001/150000, Loss: 29821.943359375, Validation Loss: 30041.072265625\n",
      "Epoch 21101/150000, Loss: 29820.423828125, Validation Loss: 30039.55078125\n",
      "Epoch 21201/150000, Loss: 29818.8984375, Validation Loss: 30038.015625\n",
      "Epoch 21301/150000, Loss: 29817.36328125, Validation Loss: 30036.482421875\n",
      "Epoch 21401/150000, Loss: 29815.822265625, Validation Loss: 30034.93359375\n",
      "Epoch 21501/150000, Loss: 29814.275390625, Validation Loss: 30033.3828125\n",
      "Epoch 21601/150000, Loss: 29812.72265625, Validation Loss: 30031.8203125\n",
      "Epoch 21701/150000, Loss: 29811.16015625, Validation Loss: 30030.25390625\n",
      "Epoch 21801/150000, Loss: 29809.58984375, Validation Loss: 30028.68359375\n",
      "Epoch 21901/150000, Loss: 29808.015625, Validation Loss: 30027.1015625\n",
      "Epoch 22001/150000, Loss: 29806.431640625, Validation Loss: 30025.517578125\n",
      "Epoch 22101/150000, Loss: 29804.841796875, Validation Loss: 30023.921875\n",
      "Epoch 22201/150000, Loss: 29803.24609375, Validation Loss: 30022.3203125\n",
      "Epoch 22301/150000, Loss: 29801.64453125, Validation Loss: 30020.71875\n",
      "Epoch 22401/150000, Loss: 29800.0390625, Validation Loss: 30019.10546875\n",
      "Epoch 22501/150000, Loss: 29798.423828125, Validation Loss: 30017.484375\n",
      "Epoch 22601/150000, Loss: 29796.8046875, Validation Loss: 30015.861328125\n",
      "Epoch 22701/150000, Loss: 29795.181640625, Validation Loss: 30014.23046875\n",
      "Epoch 22801/150000, Loss: 29793.552734375, Validation Loss: 30012.595703125\n",
      "Epoch 22901/150000, Loss: 29791.916015625, Validation Loss: 30010.951171875\n",
      "Epoch 23001/150000, Loss: 29790.275390625, Validation Loss: 30009.310546875\n",
      "Epoch 23101/150000, Loss: 29788.630859375, Validation Loss: 30007.658203125\n",
      "Epoch 23201/150000, Loss: 29786.982421875, Validation Loss: 30006.00390625\n",
      "Epoch 23301/150000, Loss: 29785.32421875, Validation Loss: 30004.34375\n",
      "Epoch 23401/150000, Loss: 29783.6640625, Validation Loss: 30002.6796875\n",
      "Epoch 23501/150000, Loss: 29782.001953125, Validation Loss: 30001.009765625\n",
      "Epoch 23601/150000, Loss: 29780.33203125, Validation Loss: 29999.3359375\n",
      "Epoch 23701/150000, Loss: 29778.65625, Validation Loss: 29997.65625\n",
      "Epoch 23801/150000, Loss: 29776.98046875, Validation Loss: 29995.97265625\n",
      "Epoch 23901/150000, Loss: 29775.298828125, Validation Loss: 29994.287109375\n",
      "Epoch 24001/150000, Loss: 29773.61328125, Validation Loss: 29992.595703125\n",
      "Epoch 24101/150000, Loss: 29771.931640625, Validation Loss: 29990.90625\n",
      "Epoch 24201/150000, Loss: 29770.236328125, Validation Loss: 29989.203125\n",
      "Epoch 24301/150000, Loss: 29768.541015625, Validation Loss: 29987.50390625\n",
      "Epoch 24401/150000, Loss: 29766.84375, Validation Loss: 29985.798828125\n",
      "Epoch 24501/150000, Loss: 29765.140625, Validation Loss: 29984.09375\n",
      "Epoch 24601/150000, Loss: 29763.43359375, Validation Loss: 29982.380859375\n",
      "Epoch 24701/150000, Loss: 29761.7265625, Validation Loss: 29980.66796875\n",
      "Epoch 24801/150000, Loss: 29760.01953125, Validation Loss: 29978.94921875\n",
      "Epoch 24901/150000, Loss: 29758.30859375, Validation Loss: 29977.23046875\n",
      "Epoch 25001/150000, Loss: 29756.59375, Validation Loss: 29975.51171875\n",
      "Epoch 25101/150000, Loss: 29754.87890625, Validation Loss: 29973.791015625\n",
      "Epoch 25201/150000, Loss: 29753.158203125, Validation Loss: 29972.06640625\n",
      "Epoch 25301/150000, Loss: 29751.4375, Validation Loss: 29970.337890625\n",
      "Epoch 25401/150000, Loss: 29749.716796875, Validation Loss: 29968.611328125\n",
      "Epoch 25501/150000, Loss: 29747.994140625, Validation Loss: 29966.880859375\n",
      "Epoch 25601/150000, Loss: 29746.265625, Validation Loss: 29965.150390625\n",
      "Epoch 25701/150000, Loss: 29744.5390625, Validation Loss: 29963.4140625\n",
      "Epoch 25801/150000, Loss: 29742.810546875, Validation Loss: 29961.67578125\n",
      "Epoch 25901/150000, Loss: 29741.083984375, Validation Loss: 29959.943359375\n",
      "Epoch 26001/150000, Loss: 29739.3515625, Validation Loss: 29958.20703125\n",
      "Epoch 26101/150000, Loss: 29737.62109375, Validation Loss: 29956.466796875\n",
      "Epoch 26201/150000, Loss: 29735.890625, Validation Loss: 29954.73046875\n",
      "Epoch 26301/150000, Loss: 29734.15625, Validation Loss: 29952.98828125\n",
      "Epoch 26401/150000, Loss: 29732.423828125, Validation Loss: 29951.248046875\n",
      "Epoch 26501/150000, Loss: 29730.689453125, Validation Loss: 29949.509765625\n",
      "Epoch 26601/150000, Loss: 29728.95703125, Validation Loss: 29947.76953125\n",
      "Epoch 26701/150000, Loss: 29727.220703125, Validation Loss: 29946.0234375\n",
      "Epoch 26801/150000, Loss: 29725.484375, Validation Loss: 29944.28515625\n",
      "Epoch 26901/150000, Loss: 29723.75, Validation Loss: 29942.544921875\n",
      "Epoch 27001/150000, Loss: 29722.015625, Validation Loss: 29940.796875\n",
      "Epoch 27101/150000, Loss: 29720.283203125, Validation Loss: 29939.056640625\n",
      "Epoch 27201/150000, Loss: 29718.55078125, Validation Loss: 29937.314453125\n",
      "Epoch 27301/150000, Loss: 29716.8125, Validation Loss: 29935.57421875\n",
      "Epoch 27401/150000, Loss: 29715.080078125, Validation Loss: 29933.83203125\n",
      "Epoch 27501/150000, Loss: 29713.34765625, Validation Loss: 29932.08984375\n",
      "Epoch 27601/150000, Loss: 29711.611328125, Validation Loss: 29930.353515625\n",
      "Epoch 27701/150000, Loss: 29709.8828125, Validation Loss: 29928.61328125\n",
      "Epoch 27801/150000, Loss: 29708.154296875, Validation Loss: 29926.875\n",
      "Epoch 27901/150000, Loss: 29706.419921875, Validation Loss: 29925.134765625\n",
      "Epoch 28001/150000, Loss: 29704.693359375, Validation Loss: 29923.3984375\n",
      "Epoch 28101/150000, Loss: 29702.962890625, Validation Loss: 29921.662109375\n",
      "Epoch 28201/150000, Loss: 29701.236328125, Validation Loss: 29919.927734375\n",
      "Epoch 28301/150000, Loss: 29699.509765625, Validation Loss: 29918.1953125\n",
      "Epoch 28401/150000, Loss: 29697.78515625, Validation Loss: 29916.4609375\n",
      "Epoch 28501/150000, Loss: 29696.0625, Validation Loss: 29914.73046875\n",
      "Epoch 28601/150000, Loss: 29694.3359375, Validation Loss: 29913.0\n",
      "Epoch 28701/150000, Loss: 29692.615234375, Validation Loss: 29911.26953125\n",
      "Epoch 28801/150000, Loss: 29690.89453125, Validation Loss: 29909.5390625\n",
      "Epoch 28901/150000, Loss: 29689.17578125, Validation Loss: 29907.810546875\n",
      "Epoch 29001/150000, Loss: 29687.45703125, Validation Loss: 29906.0859375\n",
      "Epoch 29101/150000, Loss: 29685.740234375, Validation Loss: 29904.361328125\n",
      "Epoch 29201/150000, Loss: 29684.025390625, Validation Loss: 29902.640625\n",
      "Epoch 29301/150000, Loss: 29682.3125, Validation Loss: 29900.916015625\n",
      "Epoch 29401/150000, Loss: 29680.599609375, Validation Loss: 29899.201171875\n",
      "Epoch 29501/150000, Loss: 29678.892578125, Validation Loss: 29897.478515625\n",
      "Epoch 29601/150000, Loss: 29677.1796875, Validation Loss: 29895.763671875\n",
      "Epoch 29701/150000, Loss: 29675.47265625, Validation Loss: 29894.048828125\n",
      "Epoch 29801/150000, Loss: 29673.767578125, Validation Loss: 29892.3359375\n",
      "Epoch 29901/150000, Loss: 29672.06640625, Validation Loss: 29890.625\n",
      "Epoch 30001/150000, Loss: 29670.361328125, Validation Loss: 29888.9140625\n",
      "Epoch 30101/150000, Loss: 29668.6640625, Validation Loss: 29887.20703125\n",
      "Epoch 30201/150000, Loss: 29666.96484375, Validation Loss: 29885.50390625\n",
      "Epoch 30301/150000, Loss: 29665.271484375, Validation Loss: 29883.796875\n",
      "Epoch 30401/150000, Loss: 29663.57421875, Validation Loss: 29882.095703125\n",
      "Epoch 30501/150000, Loss: 29661.8828125, Validation Loss: 29880.39453125\n",
      "Epoch 30601/150000, Loss: 29660.193359375, Validation Loss: 29878.697265625\n",
      "Epoch 30701/150000, Loss: 29658.505859375, Validation Loss: 29877.00390625\n",
      "Epoch 30801/150000, Loss: 29656.8203125, Validation Loss: 29875.30859375\n",
      "Epoch 30901/150000, Loss: 29655.13671875, Validation Loss: 29873.619140625\n",
      "Epoch 31001/150000, Loss: 29653.458984375, Validation Loss: 29871.927734375\n",
      "Epoch 31101/150000, Loss: 29651.77734375, Validation Loss: 29870.2421875\n",
      "Epoch 31201/150000, Loss: 29650.099609375, Validation Loss: 29868.556640625\n",
      "Epoch 31301/150000, Loss: 29648.42578125, Validation Loss: 29866.875\n",
      "Epoch 31401/150000, Loss: 29646.751953125, Validation Loss: 29865.1953125\n",
      "Epoch 31501/150000, Loss: 29645.080078125, Validation Loss: 29863.515625\n",
      "Epoch 31601/150000, Loss: 29643.4140625, Validation Loss: 29861.837890625\n",
      "Epoch 31701/150000, Loss: 29641.74609375, Validation Loss: 29860.16796875\n",
      "Epoch 31801/150000, Loss: 29640.083984375, Validation Loss: 29858.494140625\n",
      "Epoch 31901/150000, Loss: 29638.423828125, Validation Loss: 29856.82421875\n",
      "Epoch 32001/150000, Loss: 29636.765625, Validation Loss: 29855.162109375\n",
      "Epoch 32101/150000, Loss: 29635.109375, Validation Loss: 29853.5\n",
      "Epoch 32201/150000, Loss: 29633.45703125, Validation Loss: 29851.8359375\n",
      "Epoch 32301/150000, Loss: 29631.8046875, Validation Loss: 29850.17578125\n",
      "Epoch 32401/150000, Loss: 29630.15625, Validation Loss: 29848.517578125\n",
      "Epoch 32501/150000, Loss: 29628.509765625, Validation Loss: 29846.861328125\n",
      "Epoch 32601/150000, Loss: 29626.865234375, Validation Loss: 29845.212890625\n",
      "Epoch 32701/150000, Loss: 29625.22265625, Validation Loss: 29843.5625\n",
      "Epoch 32801/150000, Loss: 29623.5859375, Validation Loss: 29841.916015625\n",
      "Epoch 32901/150000, Loss: 29621.943359375, Validation Loss: 29840.271484375\n",
      "Epoch 33001/150000, Loss: 29620.3125, Validation Loss: 29838.630859375\n",
      "Epoch 33101/150000, Loss: 29618.681640625, Validation Loss: 29836.990234375\n",
      "Epoch 33201/150000, Loss: 29617.052734375, Validation Loss: 29835.35546875\n",
      "Epoch 33301/150000, Loss: 29615.423828125, Validation Loss: 29833.720703125\n",
      "Epoch 33401/150000, Loss: 29613.80078125, Validation Loss: 29832.0859375\n",
      "Epoch 33501/150000, Loss: 29612.177734375, Validation Loss: 29830.4609375\n",
      "Epoch 33601/150000, Loss: 29610.560546875, Validation Loss: 29828.83203125\n",
      "Epoch 33701/150000, Loss: 29608.943359375, Validation Loss: 29827.208984375\n",
      "Epoch 33801/150000, Loss: 29607.330078125, Validation Loss: 29825.583984375\n",
      "Epoch 33901/150000, Loss: 29605.71875, Validation Loss: 29823.966796875\n",
      "Epoch 34001/150000, Loss: 29604.107421875, Validation Loss: 29822.349609375\n",
      "Epoch 34101/150000, Loss: 29602.501953125, Validation Loss: 29820.736328125\n",
      "Epoch 34201/150000, Loss: 29600.8984375, Validation Loss: 29819.125\n",
      "Epoch 34301/150000, Loss: 29599.296875, Validation Loss: 29817.517578125\n",
      "Epoch 34401/150000, Loss: 29597.69921875, Validation Loss: 29815.91015625\n",
      "Epoch 34501/150000, Loss: 29596.103515625, Validation Loss: 29814.30859375\n",
      "Epoch 34601/150000, Loss: 29594.509765625, Validation Loss: 29812.708984375\n",
      "Epoch 34701/150000, Loss: 29592.921875, Validation Loss: 29811.11328125\n",
      "Epoch 34801/150000, Loss: 29591.3359375, Validation Loss: 29809.517578125\n",
      "Epoch 34901/150000, Loss: 29589.75, Validation Loss: 29807.92578125\n",
      "Epoch 35001/150000, Loss: 29588.16796875, Validation Loss: 29806.3359375\n",
      "Epoch 35101/150000, Loss: 29586.587890625, Validation Loss: 29804.748046875\n",
      "Epoch 35201/150000, Loss: 29585.013671875, Validation Loss: 29803.162109375\n",
      "Epoch 35301/150000, Loss: 29583.44140625, Validation Loss: 29801.583984375\n",
      "Epoch 35401/150000, Loss: 29581.8671875, Validation Loss: 29800.00390625\n",
      "Epoch 35501/150000, Loss: 29580.294921875, Validation Loss: 29798.427734375\n",
      "Epoch 35601/150000, Loss: 29578.732421875, Validation Loss: 29796.85546875\n",
      "Epoch 35701/150000, Loss: 29577.16796875, Validation Loss: 29795.28125\n",
      "Epoch 35801/150000, Loss: 29575.60546875, Validation Loss: 29793.71484375\n",
      "Epoch 35901/150000, Loss: 29574.046875, Validation Loss: 29792.150390625\n",
      "Epoch 36001/150000, Loss: 29572.490234375, Validation Loss: 29790.5859375\n",
      "Epoch 36101/150000, Loss: 29570.935546875, Validation Loss: 29789.02734375\n",
      "Epoch 36201/150000, Loss: 29569.384765625, Validation Loss: 29787.466796875\n",
      "Epoch 36301/150000, Loss: 29567.8359375, Validation Loss: 29785.91015625\n",
      "Epoch 36401/150000, Loss: 29566.291015625, Validation Loss: 29784.35546875\n",
      "Epoch 36501/150000, Loss: 29564.74609375, Validation Loss: 29782.80859375\n",
      "Epoch 36601/150000, Loss: 29563.20703125, Validation Loss: 29781.2578125\n",
      "Epoch 36701/150000, Loss: 29561.6640625, Validation Loss: 29779.708984375\n",
      "Epoch 36801/150000, Loss: 29560.12890625, Validation Loss: 29778.1640625\n",
      "Epoch 36901/150000, Loss: 29558.595703125, Validation Loss: 29776.625\n",
      "Epoch 37001/150000, Loss: 29557.06640625, Validation Loss: 29775.0859375\n",
      "Epoch 37101/150000, Loss: 29555.533203125, Validation Loss: 29773.548828125\n",
      "Epoch 37201/150000, Loss: 29554.0078125, Validation Loss: 29772.017578125\n",
      "Epoch 37301/150000, Loss: 29552.484375, Validation Loss: 29770.48828125\n",
      "Epoch 37401/150000, Loss: 29550.962890625, Validation Loss: 29768.9609375\n",
      "Epoch 37501/150000, Loss: 29549.443359375, Validation Loss: 29767.43359375\n",
      "Epoch 37601/150000, Loss: 29547.927734375, Validation Loss: 29765.90625\n",
      "Epoch 37701/150000, Loss: 29546.412109375, Validation Loss: 29764.38671875\n",
      "Epoch 37801/150000, Loss: 29544.900390625, Validation Loss: 29762.865234375\n",
      "Epoch 37901/150000, Loss: 29543.388671875, Validation Loss: 29761.349609375\n",
      "Epoch 38001/150000, Loss: 29541.8828125, Validation Loss: 29759.8359375\n",
      "Epoch 38101/150000, Loss: 29540.376953125, Validation Loss: 29758.32421875\n",
      "Epoch 38201/150000, Loss: 29538.875, Validation Loss: 29756.81640625\n",
      "Epoch 38301/150000, Loss: 29537.375, Validation Loss: 29755.3046875\n",
      "Epoch 38401/150000, Loss: 29535.875, Validation Loss: 29753.8046875\n",
      "Epoch 38501/150000, Loss: 29534.380859375, Validation Loss: 29752.298828125\n",
      "Epoch 38601/150000, Loss: 29532.88671875, Validation Loss: 29750.798828125\n",
      "Epoch 38701/150000, Loss: 29531.39453125, Validation Loss: 29749.302734375\n",
      "Epoch 38801/150000, Loss: 29529.90234375, Validation Loss: 29747.80859375\n",
      "Epoch 38901/150000, Loss: 29528.41796875, Validation Loss: 29746.314453125\n",
      "Epoch 39001/150000, Loss: 29526.93359375, Validation Loss: 29744.8203125\n",
      "Epoch 39101/150000, Loss: 29525.44921875, Validation Loss: 29743.33203125\n",
      "Epoch 39201/150000, Loss: 29523.970703125, Validation Loss: 29741.841796875\n",
      "Epoch 39301/150000, Loss: 29522.494140625, Validation Loss: 29740.359375\n",
      "Epoch 39401/150000, Loss: 29521.01953125, Validation Loss: 29738.876953125\n",
      "Epoch 39501/150000, Loss: 29519.544921875, Validation Loss: 29737.3984375\n",
      "Epoch 39601/150000, Loss: 29518.07421875, Validation Loss: 29735.919921875\n",
      "Epoch 39701/150000, Loss: 29516.603515625, Validation Loss: 29734.4453125\n",
      "Epoch 39801/150000, Loss: 29515.140625, Validation Loss: 29732.97265625\n",
      "Epoch 39901/150000, Loss: 29513.677734375, Validation Loss: 29731.5\n",
      "Epoch 40001/150000, Loss: 29512.216796875, Validation Loss: 29730.03515625\n",
      "Epoch 40101/150000, Loss: 29510.7578125, Validation Loss: 29728.568359375\n",
      "Epoch 40201/150000, Loss: 29509.296875, Validation Loss: 29727.107421875\n",
      "Epoch 40301/150000, Loss: 29507.841796875, Validation Loss: 29725.64453125\n",
      "Epoch 40401/150000, Loss: 29506.390625, Validation Loss: 29724.18359375\n",
      "Epoch 40501/150000, Loss: 29504.939453125, Validation Loss: 29722.728515625\n",
      "Epoch 40601/150000, Loss: 29503.490234375, Validation Loss: 29721.271484375\n",
      "Epoch 40701/150000, Loss: 29502.044921875, Validation Loss: 29719.81640625\n",
      "Epoch 40801/150000, Loss: 29500.59765625, Validation Loss: 29718.369140625\n",
      "Epoch 40901/150000, Loss: 29499.154296875, Validation Loss: 29716.916015625\n",
      "Epoch 41001/150000, Loss: 29497.712890625, Validation Loss: 29715.470703125\n",
      "Epoch 41101/150000, Loss: 29496.275390625, Validation Loss: 29714.0234375\n",
      "Epoch 41201/150000, Loss: 29494.83984375, Validation Loss: 29712.583984375\n",
      "Epoch 41301/150000, Loss: 29493.404296875, Validation Loss: 29711.140625\n",
      "Epoch 41401/150000, Loss: 29491.974609375, Validation Loss: 29709.701171875\n",
      "Epoch 41501/150000, Loss: 29490.541015625, Validation Loss: 29708.26953125\n",
      "Epoch 41601/150000, Loss: 29489.111328125, Validation Loss: 29706.83203125\n",
      "Epoch 41701/150000, Loss: 29487.685546875, Validation Loss: 29705.39453125\n",
      "Epoch 41801/150000, Loss: 29486.26171875, Validation Loss: 29703.96484375\n",
      "Epoch 41901/150000, Loss: 29484.8359375, Validation Loss: 29702.5390625\n",
      "Epoch 42001/150000, Loss: 29483.4140625, Validation Loss: 29701.107421875\n",
      "Epoch 42101/150000, Loss: 29481.99609375, Validation Loss: 29699.68359375\n",
      "Epoch 42201/150000, Loss: 29480.580078125, Validation Loss: 29698.2578125\n",
      "Epoch 42301/150000, Loss: 29479.166015625, Validation Loss: 29696.8359375\n",
      "Epoch 42401/150000, Loss: 29477.75390625, Validation Loss: 29695.419921875\n",
      "Epoch 42501/150000, Loss: 29476.341796875, Validation Loss: 29694.0\n",
      "Epoch 42601/150000, Loss: 29474.931640625, Validation Loss: 29692.58984375\n",
      "Epoch 42701/150000, Loss: 29473.521484375, Validation Loss: 29691.169921875\n",
      "Epoch 42801/150000, Loss: 29472.115234375, Validation Loss: 29689.7578125\n",
      "Epoch 42901/150000, Loss: 29470.712890625, Validation Loss: 29688.349609375\n",
      "Epoch 43001/150000, Loss: 29469.310546875, Validation Loss: 29686.939453125\n",
      "Epoch 43101/150000, Loss: 29467.91015625, Validation Loss: 29685.533203125\n",
      "Epoch 43201/150000, Loss: 29466.509765625, Validation Loss: 29684.12890625\n",
      "Epoch 43301/150000, Loss: 29465.11328125, Validation Loss: 29682.724609375\n",
      "Epoch 43401/150000, Loss: 29463.720703125, Validation Loss: 29681.32421875\n",
      "Epoch 43501/150000, Loss: 29462.326171875, Validation Loss: 29679.92578125\n",
      "Epoch 43601/150000, Loss: 29460.9375, Validation Loss: 29678.52734375\n",
      "Epoch 43701/150000, Loss: 29459.544921875, Validation Loss: 29677.130859375\n",
      "Epoch 43801/150000, Loss: 29458.15625, Validation Loss: 29675.736328125\n",
      "Epoch 43901/150000, Loss: 29456.76953125, Validation Loss: 29674.34765625\n",
      "Epoch 44001/150000, Loss: 29455.384765625, Validation Loss: 29672.955078125\n",
      "Epoch 44101/150000, Loss: 29454.0, Validation Loss: 29671.5625\n",
      "Epoch 44201/150000, Loss: 29452.619140625, Validation Loss: 29670.1796875\n",
      "Epoch 44301/150000, Loss: 29451.2421875, Validation Loss: 29668.79296875\n",
      "Epoch 44401/150000, Loss: 29449.865234375, Validation Loss: 29667.41015625\n",
      "Epoch 44501/150000, Loss: 29448.490234375, Validation Loss: 29666.029296875\n",
      "Epoch 44601/150000, Loss: 29447.115234375, Validation Loss: 29664.646484375\n",
      "Epoch 44701/150000, Loss: 29445.7421875, Validation Loss: 29663.271484375\n",
      "Epoch 44801/150000, Loss: 29444.373046875, Validation Loss: 29661.89453125\n",
      "Epoch 44901/150000, Loss: 29443.005859375, Validation Loss: 29660.521484375\n",
      "Epoch 45001/150000, Loss: 29441.638671875, Validation Loss: 29659.150390625\n",
      "Epoch 45101/150000, Loss: 29440.275390625, Validation Loss: 29657.779296875\n",
      "Epoch 45201/150000, Loss: 29438.91015625, Validation Loss: 29656.40625\n",
      "Epoch 45301/150000, Loss: 29437.548828125, Validation Loss: 29655.0390625\n",
      "Epoch 45401/150000, Loss: 29436.189453125, Validation Loss: 29653.673828125\n",
      "Epoch 45501/150000, Loss: 29434.830078125, Validation Loss: 29652.30859375\n",
      "Epoch 45601/150000, Loss: 29433.47265625, Validation Loss: 29650.9453125\n",
      "Epoch 45701/150000, Loss: 29432.12109375, Validation Loss: 29649.5859375\n",
      "Epoch 45801/150000, Loss: 29430.767578125, Validation Loss: 29648.228515625\n",
      "Epoch 45901/150000, Loss: 29429.41015625, Validation Loss: 29646.865234375\n",
      "Epoch 46001/150000, Loss: 29428.060546875, Validation Loss: 29645.51171875\n",
      "Epoch 46101/150000, Loss: 29426.712890625, Validation Loss: 29644.158203125\n",
      "Epoch 46201/150000, Loss: 29425.365234375, Validation Loss: 29642.802734375\n",
      "Epoch 46301/150000, Loss: 29424.021484375, Validation Loss: 29641.44921875\n",
      "Epoch 46401/150000, Loss: 29422.677734375, Validation Loss: 29640.099609375\n",
      "Epoch 46501/150000, Loss: 29421.330078125, Validation Loss: 29638.751953125\n",
      "Epoch 46601/150000, Loss: 29419.986328125, Validation Loss: 29637.400390625\n",
      "Epoch 46701/150000, Loss: 29418.650390625, Validation Loss: 29636.060546875\n",
      "Epoch 46801/150000, Loss: 29417.310546875, Validation Loss: 29634.71484375\n",
      "Epoch 46901/150000, Loss: 29415.97265625, Validation Loss: 29633.369140625\n",
      "Epoch 47001/150000, Loss: 29414.63671875, Validation Loss: 29632.02734375\n",
      "Epoch 47101/150000, Loss: 29413.30078125, Validation Loss: 29630.689453125\n",
      "Epoch 47201/150000, Loss: 29411.96875, Validation Loss: 29629.34765625\n",
      "Epoch 47301/150000, Loss: 29410.638671875, Validation Loss: 29628.01171875\n",
      "Epoch 47401/150000, Loss: 29409.306640625, Validation Loss: 29626.67578125\n",
      "Epoch 47501/150000, Loss: 29407.9765625, Validation Loss: 29625.341796875\n",
      "Epoch 47601/150000, Loss: 29406.650390625, Validation Loss: 29624.005859375\n",
      "Epoch 47701/150000, Loss: 29405.326171875, Validation Loss: 29622.673828125\n",
      "Epoch 47801/150000, Loss: 29403.99609375, Validation Loss: 29621.34375\n",
      "Epoch 47901/150000, Loss: 29402.673828125, Validation Loss: 29620.015625\n",
      "Epoch 48001/150000, Loss: 29401.353515625, Validation Loss: 29618.689453125\n",
      "Epoch 48101/150000, Loss: 29400.033203125, Validation Loss: 29617.359375\n",
      "Epoch 48201/150000, Loss: 29398.7109375, Validation Loss: 29616.03515625\n",
      "Epoch 48301/150000, Loss: 29397.39453125, Validation Loss: 29614.712890625\n",
      "Epoch 48401/150000, Loss: 29396.076171875, Validation Loss: 29613.392578125\n",
      "Epoch 48501/150000, Loss: 29394.7578125, Validation Loss: 29612.068359375\n",
      "Epoch 48601/150000, Loss: 29393.447265625, Validation Loss: 29610.748046875\n",
      "Epoch 48701/150000, Loss: 29392.1328125, Validation Loss: 29609.427734375\n",
      "Epoch 48801/150000, Loss: 29390.82421875, Validation Loss: 29608.11328125\n",
      "Epoch 48901/150000, Loss: 29389.51171875, Validation Loss: 29606.796875\n",
      "Epoch 49001/150000, Loss: 29388.205078125, Validation Loss: 29605.484375\n",
      "Epoch 49101/150000, Loss: 29386.8984375, Validation Loss: 29604.16796875\n",
      "Epoch 49201/150000, Loss: 29385.591796875, Validation Loss: 29602.85546875\n",
      "Epoch 49301/150000, Loss: 29384.28515625, Validation Loss: 29601.544921875\n",
      "Epoch 49401/150000, Loss: 29382.982421875, Validation Loss: 29600.240234375\n",
      "Epoch 49501/150000, Loss: 29381.6796875, Validation Loss: 29598.92578125\n",
      "Epoch 49601/150000, Loss: 29380.373046875, Validation Loss: 29597.619140625\n",
      "Epoch 49701/150000, Loss: 29379.076171875, Validation Loss: 29596.314453125\n",
      "Epoch 49801/150000, Loss: 29377.77734375, Validation Loss: 29595.009765625\n",
      "Epoch 49901/150000, Loss: 29376.48046875, Validation Loss: 29593.708984375\n",
      "Epoch 50001/150000, Loss: 29375.181640625, Validation Loss: 29592.404296875\n",
      "Epoch 50101/150000, Loss: 29373.88671875, Validation Loss: 29591.10546875\n",
      "Epoch 50201/150000, Loss: 29372.595703125, Validation Loss: 29589.8046875\n",
      "Epoch 50301/150000, Loss: 29371.3046875, Validation Loss: 29588.509765625\n",
      "Epoch 50401/150000, Loss: 29370.01171875, Validation Loss: 29587.212890625\n",
      "Epoch 50501/150000, Loss: 29368.72265625, Validation Loss: 29585.916015625\n",
      "Epoch 50601/150000, Loss: 29367.43359375, Validation Loss: 29584.623046875\n",
      "Epoch 50701/150000, Loss: 29366.146484375, Validation Loss: 29583.330078125\n",
      "Epoch 50801/150000, Loss: 29364.857421875, Validation Loss: 29582.041015625\n",
      "Epoch 50901/150000, Loss: 29363.578125, Validation Loss: 29580.748046875\n",
      "Epoch 51001/150000, Loss: 29362.296875, Validation Loss: 29579.4609375\n",
      "Epoch 51101/150000, Loss: 29361.013671875, Validation Loss: 29578.17578125\n",
      "Epoch 51201/150000, Loss: 29359.73046875, Validation Loss: 29576.888671875\n",
      "Epoch 51301/150000, Loss: 29358.455078125, Validation Loss: 29575.60546875\n",
      "Epoch 51401/150000, Loss: 29357.177734375, Validation Loss: 29574.32421875\n",
      "Epoch 51501/150000, Loss: 29355.900390625, Validation Loss: 29573.0390625\n",
      "Epoch 51601/150000, Loss: 29354.623046875, Validation Loss: 29571.759765625\n",
      "Epoch 51701/150000, Loss: 29353.34765625, Validation Loss: 29570.478515625\n",
      "Epoch 51801/150000, Loss: 29352.076171875, Validation Loss: 29569.203125\n",
      "Epoch 51901/150000, Loss: 29350.8046875, Validation Loss: 29567.921875\n",
      "Epoch 52001/150000, Loss: 29349.533203125, Validation Loss: 29566.646484375\n",
      "Epoch 52101/150000, Loss: 29348.26171875, Validation Loss: 29565.37109375\n",
      "Epoch 52201/150000, Loss: 29346.9921875, Validation Loss: 29564.095703125\n",
      "Epoch 52301/150000, Loss: 29345.7265625, Validation Loss: 29562.82421875\n",
      "Epoch 52401/150000, Loss: 29344.455078125, Validation Loss: 29561.55078125\n",
      "Epoch 52501/150000, Loss: 29343.193359375, Validation Loss: 29560.279296875\n",
      "Epoch 52601/150000, Loss: 29341.92578125, Validation Loss: 29559.009765625\n",
      "Epoch 52701/150000, Loss: 29340.6640625, Validation Loss: 29557.740234375\n",
      "Epoch 52801/150000, Loss: 29339.40234375, Validation Loss: 29556.470703125\n",
      "Epoch 52901/150000, Loss: 29338.140625, Validation Loss: 29555.20703125\n",
      "Epoch 53001/150000, Loss: 29336.8828125, Validation Loss: 29553.939453125\n",
      "Epoch 53101/150000, Loss: 29335.623046875, Validation Loss: 29552.67578125\n",
      "Epoch 53201/150000, Loss: 29334.36328125, Validation Loss: 29551.4140625\n",
      "Epoch 53301/150000, Loss: 29333.107421875, Validation Loss: 29550.15234375\n",
      "Epoch 53401/150000, Loss: 29331.853515625, Validation Loss: 29548.892578125\n",
      "Epoch 53501/150000, Loss: 29330.6015625, Validation Loss: 29547.634765625\n",
      "Epoch 53601/150000, Loss: 29329.34765625, Validation Loss: 29546.375\n",
      "Epoch 53701/150000, Loss: 29328.09375, Validation Loss: 29545.1171875\n",
      "Epoch 53801/150000, Loss: 29326.83984375, Validation Loss: 29543.861328125\n",
      "Epoch 53901/150000, Loss: 29325.58984375, Validation Loss: 29542.60546875\n",
      "Epoch 54001/150000, Loss: 29324.345703125, Validation Loss: 29541.34765625\n",
      "Epoch 54101/150000, Loss: 29323.095703125, Validation Loss: 29540.095703125\n",
      "Epoch 54201/150000, Loss: 29321.84765625, Validation Loss: 29538.84375\n",
      "Epoch 54301/150000, Loss: 29320.599609375, Validation Loss: 29537.59375\n",
      "Epoch 54401/150000, Loss: 29319.353515625, Validation Loss: 29536.341796875\n",
      "Epoch 54501/150000, Loss: 29318.109375, Validation Loss: 29535.08984375\n",
      "Epoch 54601/150000, Loss: 29316.86328125, Validation Loss: 29533.841796875\n",
      "Epoch 54701/150000, Loss: 29315.623046875, Validation Loss: 29532.59375\n",
      "Epoch 54801/150000, Loss: 29314.37890625, Validation Loss: 29531.34765625\n",
      "Epoch 54901/150000, Loss: 29313.138671875, Validation Loss: 29530.099609375\n",
      "Epoch 55001/150000, Loss: 29311.8984375, Validation Loss: 29528.85546875\n",
      "Epoch 55101/150000, Loss: 29310.658203125, Validation Loss: 29527.611328125\n",
      "Epoch 55201/150000, Loss: 29309.421875, Validation Loss: 29526.369140625\n",
      "Epoch 55301/150000, Loss: 29308.185546875, Validation Loss: 29525.123046875\n",
      "Epoch 55401/150000, Loss: 29306.94921875, Validation Loss: 29523.8828125\n",
      "Epoch 55501/150000, Loss: 29305.71484375, Validation Loss: 29522.640625\n",
      "Epoch 55601/150000, Loss: 29304.478515625, Validation Loss: 29521.400390625\n",
      "Epoch 55701/150000, Loss: 29303.248046875, Validation Loss: 29520.162109375\n",
      "Epoch 55801/150000, Loss: 29302.013671875, Validation Loss: 29518.92578125\n",
      "Epoch 55901/150000, Loss: 29300.78125, Validation Loss: 29517.689453125\n",
      "Epoch 56001/150000, Loss: 29299.552734375, Validation Loss: 29516.455078125\n",
      "Epoch 56101/150000, Loss: 29298.322265625, Validation Loss: 29515.21875\n",
      "Epoch 56201/150000, Loss: 29297.091796875, Validation Loss: 29513.982421875\n",
      "Epoch 56301/150000, Loss: 29295.86328125, Validation Loss: 29512.751953125\n",
      "Epoch 56401/150000, Loss: 29294.63671875, Validation Loss: 29511.521484375\n",
      "Epoch 56501/150000, Loss: 29293.408203125, Validation Loss: 29510.287109375\n",
      "Epoch 56601/150000, Loss: 29292.181640625, Validation Loss: 29509.0546875\n",
      "Epoch 56701/150000, Loss: 29290.95703125, Validation Loss: 29507.82421875\n",
      "Epoch 56801/150000, Loss: 29289.734375, Validation Loss: 29506.595703125\n",
      "Epoch 56901/150000, Loss: 29288.509765625, Validation Loss: 29505.365234375\n",
      "Epoch 57001/150000, Loss: 29287.287109375, Validation Loss: 29504.138671875\n",
      "Epoch 57101/150000, Loss: 29286.064453125, Validation Loss: 29502.91015625\n",
      "Epoch 57201/150000, Loss: 29284.845703125, Validation Loss: 29501.689453125\n",
      "Epoch 57301/150000, Loss: 29283.625, Validation Loss: 29500.458984375\n",
      "Epoch 57401/150000, Loss: 29282.404296875, Validation Loss: 29499.236328125\n",
      "Epoch 57501/150000, Loss: 29281.1875, Validation Loss: 29498.01171875\n",
      "Epoch 57601/150000, Loss: 29279.96875, Validation Loss: 29496.787109375\n",
      "Epoch 57701/150000, Loss: 29278.75, Validation Loss: 29495.568359375\n",
      "Epoch 57801/150000, Loss: 29277.537109375, Validation Loss: 29494.34765625\n",
      "Epoch 57901/150000, Loss: 29276.318359375, Validation Loss: 29493.125\n",
      "Epoch 58001/150000, Loss: 29275.10546875, Validation Loss: 29491.90625\n",
      "Epoch 58101/150000, Loss: 29273.890625, Validation Loss: 29490.685546875\n",
      "Epoch 58201/150000, Loss: 29272.677734375, Validation Loss: 29489.470703125\n",
      "Epoch 58301/150000, Loss: 29271.462890625, Validation Loss: 29488.251953125\n",
      "Epoch 58401/150000, Loss: 29270.255859375, Validation Loss: 29487.03515625\n",
      "Epoch 58501/150000, Loss: 29269.046875, Validation Loss: 29485.81640625\n",
      "Epoch 58601/150000, Loss: 29267.83203125, Validation Loss: 29484.60546875\n",
      "Epoch 58701/150000, Loss: 29266.626953125, Validation Loss: 29483.388671875\n",
      "Epoch 58801/150000, Loss: 29265.416015625, Validation Loss: 29482.17578125\n",
      "Epoch 58901/150000, Loss: 29264.208984375, Validation Loss: 29480.96484375\n",
      "Epoch 59001/150000, Loss: 29263.001953125, Validation Loss: 29479.751953125\n",
      "Epoch 59101/150000, Loss: 29261.794921875, Validation Loss: 29478.541015625\n",
      "Epoch 59201/150000, Loss: 29260.591796875, Validation Loss: 29477.330078125\n",
      "Epoch 59301/150000, Loss: 29259.384765625, Validation Loss: 29476.123046875\n",
      "Epoch 59401/150000, Loss: 29258.1796875, Validation Loss: 29474.91015625\n",
      "Epoch 59501/150000, Loss: 29256.98046875, Validation Loss: 29473.701171875\n",
      "Epoch 59601/150000, Loss: 29255.7734375, Validation Loss: 29472.490234375\n",
      "Epoch 59701/150000, Loss: 29254.57421875, Validation Loss: 29471.287109375\n",
      "Epoch 59801/150000, Loss: 29253.369140625, Validation Loss: 29470.080078125\n",
      "Epoch 59901/150000, Loss: 29252.171875, Validation Loss: 29468.875\n",
      "Epoch 60001/150000, Loss: 29250.97265625, Validation Loss: 29467.669921875\n",
      "Epoch 60101/150000, Loss: 29249.771484375, Validation Loss: 29466.466796875\n",
      "Epoch 60201/150000, Loss: 29248.572265625, Validation Loss: 29465.263671875\n",
      "Epoch 60301/150000, Loss: 29247.375, Validation Loss: 29464.060546875\n",
      "Epoch 60401/150000, Loss: 29246.177734375, Validation Loss: 29462.859375\n",
      "Epoch 60501/150000, Loss: 29244.984375, Validation Loss: 29461.65625\n",
      "Epoch 60601/150000, Loss: 29243.7890625, Validation Loss: 29460.455078125\n",
      "Epoch 60701/150000, Loss: 29242.591796875, Validation Loss: 29459.2578125\n",
      "Epoch 60801/150000, Loss: 29241.3984375, Validation Loss: 29458.060546875\n",
      "Epoch 60901/150000, Loss: 29240.20703125, Validation Loss: 29456.859375\n",
      "Epoch 61001/150000, Loss: 29239.013671875, Validation Loss: 29455.6640625\n",
      "Epoch 61101/150000, Loss: 29237.822265625, Validation Loss: 29454.470703125\n",
      "Epoch 61201/150000, Loss: 29236.634765625, Validation Loss: 29453.26953125\n",
      "Epoch 61301/150000, Loss: 29235.4453125, Validation Loss: 29452.078125\n",
      "Epoch 61401/150000, Loss: 29234.255859375, Validation Loss: 29450.8828125\n",
      "Epoch 61501/150000, Loss: 29233.064453125, Validation Loss: 29449.69140625\n",
      "Epoch 61601/150000, Loss: 29231.87890625, Validation Loss: 29448.49609375\n",
      "Epoch 61701/150000, Loss: 29230.689453125, Validation Loss: 29447.3046875\n",
      "Epoch 61801/150000, Loss: 29229.50390625, Validation Loss: 29446.11328125\n",
      "Epoch 61901/150000, Loss: 29228.31640625, Validation Loss: 29444.921875\n",
      "Epoch 62001/150000, Loss: 29227.1328125, Validation Loss: 29443.734375\n",
      "Epoch 62101/150000, Loss: 29225.94921875, Validation Loss: 29442.541015625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m x_train_seq, y_train_seq \u001b[38;5;241m=\u001b[39m split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m    101\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train_seq)\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1504\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[0;32m   1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 1507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1509\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.001, 0.0001, 0.00001]\n",
    "window_sizes = [5]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define LSTM Model:\n",
    "First, let's define your LSTM model. You've already implemented the LSTMModel class. This class represents your LSTM model architecture.\n",
    "\n",
    "2. Sliding Window Approach:\n",
    "Next, you'll implement a sliding window approach to capture patterns from one window to the next. You've partially implemented this with the split_data_with_window function. However, you'll need to slightly modify it to generate sequential windows.\n",
    "\n",
    "Here's how you can do it:\n",
    "\n",
    "Iterate over your data with a window size.\n",
    "At each step, take a window of data as input and the next entry as the target output.\n",
    "Slide the window by one step and repeat until the end of the data.\n",
    "3. Training Loop:\n",
    "Once you have your sliding windows, you'll train your LSTM model using these windows. Here's what the training loop should do:\n",
    "\n",
    "Iterate through each window of training data.\n",
    "For each window, use the LSTM model to predict the next entry.\n",
    "Compute the loss between the predicted next entry and the actual next entry.\n",
    "Update the model parameters using backpropagation.\n",
    "Repeat this process for a number of epochs.\n",
    "4. Prediction:\n",
    "After training, you can make predictions using the trained model. To predict the next entry, you'll:\n",
    "\n",
    "Take the last window of data.\n",
    "Use the model to predict the next entry.\n",
    "Append the predicted entry to the window.\n",
    "Remove the first entry of the window to maintain the window size.\n",
    "Repeat this process for each prediction you want to make.\n",
    "Implementation:\n",
    "I see you've already started on some of these steps. You've defined the LSTM model and implemented the sliding window approach partially. You'll need to complete the sliding window approach and then proceed with the training loop and prediction steps.\n",
    "\n",
    "If you have any specific questions or need assistance with a particular part of the implementation, feel free to ask!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
