{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005\n",
    "\n",
    "window_size=  5     Test Loss: 490\n",
    "window_size= 10     Test Loss: 449\n",
    "window_size= 15     Test Loss: 768\n",
    "window_size= 20     Test Loss: 431\n",
    "window_size= 25     Test Loss: 695\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    # Initialize lists to store training and temporary sets\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    # Iterate through the data with the specified window size\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i+split_window_size+1]\n",
    "        y_out1_out2 = y_in.iloc[i:i+split_window_size+1]\n",
    "\n",
    "        # Separate the last row for the temporary set\n",
    "        # [ :-1]: all elements except the last one\n",
    "        # [-1:]:  selects only the last element\n",
    "        # (:) is used to indicate slicing of a sequence\n",
    "        # sequence[start : end : step]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    # Concatenate the lists into pandas DataFrames\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "# Split Data to train and temp\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 3\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split temp into val and test\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 1\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=15\n",
      "Epoch 1/150000, Loss: 30069.9375, Validation Loss: 30472.572265625\n",
      "Epoch 101/150000, Loss: 29317.927734375, Validation Loss: 29709.943359375\n",
      "Epoch 201/150000, Loss: 28668.640625, Validation Loss: 29056.333984375\n",
      "Epoch 301/150000, Loss: 28104.4609375, Validation Loss: 28487.37890625\n",
      "Epoch 401/150000, Loss: 27579.2734375, Validation Loss: 27957.46875\n",
      "Epoch 501/150000, Loss: 27081.6953125, Validation Loss: 27455.240234375\n",
      "Epoch 601/150000, Loss: 26606.87890625, Validation Loss: 26975.8515625\n",
      "Epoch 701/150000, Loss: 26152.16796875, Validation Loss: 26516.6328125\n",
      "Epoch 801/150000, Loss: 25715.8671875, Validation Loss: 26075.87890625\n",
      "Epoch 901/150000, Loss: 25296.75390625, Validation Loss: 25652.375\n",
      "Epoch 1001/150000, Loss: 24893.90625, Validation Loss: 25245.181640625\n",
      "Epoch 1101/150000, Loss: 24506.560546875, Validation Loss: 24853.548828125\n",
      "Epoch 1201/150000, Loss: 24134.0859375, Validation Loss: 24476.830078125\n",
      "Epoch 1301/150000, Loss: 23775.91796875, Validation Loss: 24114.466796875\n",
      "Epoch 1401/150000, Loss: 23431.5625, Validation Loss: 23765.962890625\n",
      "Epoch 1501/150000, Loss: 23100.578125, Validation Loss: 23430.87109375\n",
      "Epoch 1601/150000, Loss: 22782.537109375, Validation Loss: 23108.77734375\n",
      "Epoch 1701/150000, Loss: 22477.072265625, Validation Loss: 22799.29296875\n",
      "Epoch 1801/150000, Loss: 22183.81640625, Validation Loss: 22502.06640625\n",
      "Epoch 1901/150000, Loss: 21902.431640625, Validation Loss: 22216.751953125\n",
      "Epoch 2001/150000, Loss: 21632.6015625, Validation Loss: 21943.033203125\n",
      "Epoch 2101/150000, Loss: 21374.001953125, Validation Loss: 21680.587890625\n",
      "Epoch 2201/150000, Loss: 20840.8046875, Validation Loss: 21173.6484375\n",
      "Epoch 2301/150000, Loss: 20514.33984375, Validation Loss: 20839.939453125\n",
      "Epoch 2401/150000, Loss: 20197.337890625, Validation Loss: 20518.771484375\n",
      "Epoch 2501/150000, Loss: 19887.09375, Validation Loss: 20204.482421875\n",
      "Epoch 2601/150000, Loss: 19583.298828125, Validation Loss: 19897.1328125\n",
      "Epoch 2701/150000, Loss: 19285.59765625, Validation Loss: 19595.548828125\n",
      "Epoch 2801/150000, Loss: 18993.591796875, Validation Loss: 19299.396484375\n",
      "Epoch 2901/150000, Loss: 18707.244140625, Validation Loss: 19009.181640625\n",
      "Epoch 3001/150000, Loss: 18426.224609375, Validation Loss: 18723.541015625\n",
      "Epoch 3101/150000, Loss: 18150.28515625, Validation Loss: 18443.349609375\n",
      "Epoch 3201/150000, Loss: 17879.244140625, Validation Loss: 18168.318359375\n",
      "Epoch 3301/150000, Loss: 17612.95703125, Validation Loss: 17898.2265625\n",
      "Epoch 3401/150000, Loss: 17351.35546875, Validation Loss: 17632.970703125\n",
      "Epoch 3501/150000, Loss: 17094.380859375, Validation Loss: 17372.4921875\n",
      "Epoch 3601/150000, Loss: 16841.958984375, Validation Loss: 17116.666015625\n",
      "Epoch 3701/150000, Loss: 16594.02734375, Validation Loss: 16865.521484375\n",
      "Epoch 3801/150000, Loss: 16350.4921875, Validation Loss: 16618.9765625\n",
      "Epoch 3901/150000, Loss: 16111.31640625, Validation Loss: 16376.4716796875\n",
      "Epoch 4001/150000, Loss: 15876.41796875, Validation Loss: 16138.2021484375\n",
      "Epoch 4101/150000, Loss: 15645.708984375, Validation Loss: 15904.4814453125\n",
      "Epoch 4201/150000, Loss: 15419.1025390625, Validation Loss: 15674.6787109375\n",
      "Epoch 4301/150000, Loss: 15196.5234375, Validation Loss: 15449.19140625\n",
      "Epoch 4401/150000, Loss: 14977.8701171875, Validation Loss: 15227.4375\n",
      "Epoch 4501/150000, Loss: 14762.7939453125, Validation Loss: 15010.1083984375\n",
      "Epoch 4601/150000, Loss: 14551.2919921875, Validation Loss: 14796.2529296875\n",
      "Epoch 4701/150000, Loss: 14343.322265625, Validation Loss: 14585.7236328125\n",
      "Epoch 4801/150000, Loss: 14138.830078125, Validation Loss: 14378.58984375\n",
      "Epoch 4901/150000, Loss: 13937.7392578125, Validation Loss: 14174.8056640625\n",
      "Epoch 5001/150000, Loss: 13739.9697265625, Validation Loss: 13974.5654296875\n",
      "Epoch 5101/150000, Loss: 13545.4228515625, Validation Loss: 13777.7255859375\n",
      "Epoch 5201/150000, Loss: 13353.9912109375, Validation Loss: 13584.1220703125\n",
      "Epoch 5301/150000, Loss: 13165.5693359375, Validation Loss: 13393.640625\n",
      "Epoch 5401/150000, Loss: 12980.0576171875, Validation Loss: 13206.1337890625\n",
      "Epoch 5501/150000, Loss: 12797.40234375, Validation Loss: 13021.451171875\n",
      "Epoch 5601/150000, Loss: 12617.533203125, Validation Loss: 12839.5068359375\n",
      "Epoch 5701/150000, Loss: 12440.3916015625, Validation Loss: 12660.3515625\n",
      "Epoch 5801/150000, Loss: 12265.8662109375, Validation Loss: 12483.8037109375\n",
      "Epoch 5901/150000, Loss: 12093.8720703125, Validation Loss: 12310.01171875\n",
      "Epoch 6001/150000, Loss: 11924.3564453125, Validation Loss: 12138.8408203125\n",
      "Epoch 6101/150000, Loss: 11757.212890625, Validation Loss: 11969.9560546875\n",
      "Epoch 6201/150000, Loss: 11592.4169921875, Validation Loss: 11803.4697265625\n",
      "Epoch 6301/150000, Loss: 11429.9921875, Validation Loss: 11639.3759765625\n",
      "Epoch 6401/150000, Loss: 11269.90234375, Validation Loss: 11477.7080078125\n",
      "Epoch 6501/150000, Loss: 11112.0947265625, Validation Loss: 11318.4384765625\n",
      "Epoch 6601/150000, Loss: 10956.50390625, Validation Loss: 11161.4482421875\n",
      "Epoch 6701/150000, Loss: 10803.04296875, Validation Loss: 11006.60546875\n",
      "Epoch 6801/150000, Loss: 10651.41796875, Validation Loss: 10854.0546875\n",
      "Epoch 6901/150000, Loss: 10502.015625, Validation Loss: 10703.4873046875\n",
      "Epoch 7001/150000, Loss: 10354.5869140625, Validation Loss: 10554.9921875\n",
      "Epoch 7101/150000, Loss: 10209.05078125, Validation Loss: 10408.388671875\n",
      "Epoch 7201/150000, Loss: 10065.4091796875, Validation Loss: 10263.61328125\n",
      "Epoch 7301/150000, Loss: 9923.59765625, Validation Loss: 10120.517578125\n",
      "Epoch 7401/150000, Loss: 9783.6298828125, Validation Loss: 9979.107421875\n",
      "Epoch 7501/150000, Loss: 9645.4140625, Validation Loss: 9839.501953125\n",
      "Epoch 7601/150000, Loss: 9508.95703125, Validation Loss: 9701.59765625\n",
      "Epoch 7701/150000, Loss: 9374.0849609375, Validation Loss: 9564.9658203125\n",
      "Epoch 7801/150000, Loss: 9240.8193359375, Validation Loss: 9429.826171875\n",
      "Epoch 7901/150000, Loss: 9109.029296875, Validation Loss: 9296.267578125\n",
      "Epoch 8001/150000, Loss: 8978.5986328125, Validation Loss: 9164.3701171875\n",
      "Epoch 8101/150000, Loss: 8849.67578125, Validation Loss: 9034.482421875\n",
      "Epoch 8201/150000, Loss: 8722.109375, Validation Loss: 8905.8310546875\n",
      "Epoch 8301/150000, Loss: 8595.953125, Validation Loss: 8778.7587890625\n",
      "Epoch 8401/150000, Loss: 8471.1376953125, Validation Loss: 8653.001953125\n",
      "Epoch 8501/150000, Loss: 8347.6396484375, Validation Loss: 8528.3408203125\n",
      "Epoch 8601/150000, Loss: 8225.3779296875, Validation Loss: 8404.3759765625\n",
      "Epoch 8701/150000, Loss: 8104.3818359375, Validation Loss: 8281.962890625\n",
      "Epoch 8801/150000, Loss: 7984.6318359375, Validation Loss: 8161.1298828125\n",
      "Epoch 8901/150000, Loss: 7866.1455078125, Validation Loss: 8041.60498046875\n",
      "Epoch 9001/150000, Loss: 7748.830078125, Validation Loss: 7922.7919921875\n",
      "Epoch 9101/150000, Loss: 7632.69384765625, Validation Loss: 7805.66455078125\n",
      "Epoch 9201/150000, Loss: 7517.75634765625, Validation Loss: 7689.98046875\n",
      "Epoch 9301/150000, Loss: 7404.10107421875, Validation Loss: 7575.337890625\n",
      "Epoch 9401/150000, Loss: 7291.4306640625, Validation Loss: 7461.94189453125\n",
      "Epoch 9501/150000, Loss: 7180.0068359375, Validation Loss: 7349.576171875\n",
      "Epoch 9601/150000, Loss: 7069.72265625, Validation Loss: 7238.41259765625\n",
      "Epoch 9701/150000, Loss: 6960.5791015625, Validation Loss: 7128.4365234375\n",
      "Epoch 9801/150000, Loss: 6852.60546875, Validation Loss: 7019.75341796875\n",
      "Epoch 9901/150000, Loss: 6745.728515625, Validation Loss: 6911.8447265625\n",
      "Epoch 10001/150000, Loss: 6640.029296875, Validation Loss: 6805.27978515625\n",
      "Epoch 10101/150000, Loss: 6535.4599609375, Validation Loss: 6699.8515625\n",
      "Epoch 10201/150000, Loss: 6432.0341796875, Validation Loss: 6595.6484375\n",
      "Epoch 10301/150000, Loss: 6329.740234375, Validation Loss: 6492.78271484375\n",
      "Epoch 10401/150000, Loss: 6228.4814453125, Validation Loss: 6390.72119140625\n",
      "Epoch 10501/150000, Loss: 6128.37646484375, Validation Loss: 6290.05615234375\n",
      "Epoch 10601/150000, Loss: 6029.423828125, Validation Loss: 6190.62548828125\n",
      "Epoch 10701/150000, Loss: 5931.6123046875, Validation Loss: 6092.27685546875\n",
      "Epoch 10801/150000, Loss: 5835.06005859375, Validation Loss: 5995.53076171875\n",
      "Epoch 10901/150000, Loss: 5739.419921875, Validation Loss: 5899.00244140625\n",
      "Epoch 11001/150000, Loss: 5645.03369140625, Validation Loss: 5803.9892578125\n",
      "Epoch 11101/150000, Loss: 5551.78515625, Validation Loss: 5710.03515625\n",
      "Epoch 11201/150000, Loss: 5459.72021484375, Validation Loss: 5617.24072265625\n",
      "Epoch 11301/150000, Loss: 5368.84228515625, Validation Loss: 5525.30029296875\n",
      "Epoch 11401/150000, Loss: 5279.06591796875, Validation Loss: 5435.16650390625\n",
      "Epoch 11501/150000, Loss: 5190.4580078125, Validation Loss: 5345.90625\n",
      "Epoch 11601/150000, Loss: 5103.01220703125, Validation Loss: 5257.80224609375\n",
      "Epoch 11701/150000, Loss: 5016.63427734375, Validation Loss: 5170.85888671875\n",
      "Epoch 11801/150000, Loss: 4931.28955078125, Validation Loss: 5084.85693359375\n",
      "Epoch 11901/150000, Loss: 4846.86767578125, Validation Loss: 5000.33984375\n",
      "Epoch 12001/150000, Loss: 4763.4892578125, Validation Loss: 4917.224609375\n",
      "Epoch 12101/150000, Loss: 4681.12744140625, Validation Loss: 4835.232421875\n",
      "Epoch 12201/150000, Loss: 4599.7685546875, Validation Loss: 4754.1201171875\n",
      "Epoch 12301/150000, Loss: 4519.45849609375, Validation Loss: 4673.9443359375\n",
      "Epoch 12401/150000, Loss: 4440.16357421875, Validation Loss: 4594.6357421875\n",
      "Epoch 12501/150000, Loss: 4361.90869140625, Validation Loss: 4516.39990234375\n",
      "Epoch 12601/150000, Loss: 4284.60205078125, Validation Loss: 4439.14013671875\n",
      "Epoch 12701/150000, Loss: 4208.333984375, Validation Loss: 4363.0400390625\n",
      "Epoch 12801/150000, Loss: 4133.0712890625, Validation Loss: 4288.00732421875\n",
      "Epoch 12901/150000, Loss: 4058.81298828125, Validation Loss: 4213.90966796875\n",
      "Epoch 13001/150000, Loss: 3985.54052734375, Validation Loss: 4140.79052734375\n",
      "Epoch 13101/150000, Loss: 3913.25439453125, Validation Loss: 4068.6552734375\n",
      "Epoch 13201/150000, Loss: 3841.9677734375, Validation Loss: 3997.5068359375\n",
      "Epoch 13301/150000, Loss: 3771.663330078125, Validation Loss: 3927.255615234375\n",
      "Epoch 13401/150000, Loss: 3702.345458984375, Validation Loss: 3858.038818359375\n",
      "Epoch 13501/150000, Loss: 3634.00830078125, Validation Loss: 3789.791748046875\n",
      "Epoch 13601/150000, Loss: 3566.650634765625, Validation Loss: 3722.497314453125\n",
      "Epoch 13701/150000, Loss: 3500.258056640625, Validation Loss: 3656.161376953125\n",
      "Epoch 13801/150000, Loss: 3434.834228515625, Validation Loss: 3590.739013671875\n",
      "Epoch 13901/150000, Loss: 3370.374267578125, Validation Loss: 3526.2548828125\n",
      "Epoch 14001/150000, Loss: 3306.885009765625, Validation Loss: 3462.763916015625\n",
      "Epoch 14101/150000, Loss: 3244.3486328125, Validation Loss: 3400.060302734375\n",
      "Epoch 14201/150000, Loss: 3182.7412109375, Validation Loss: 3338.5087890625\n",
      "Epoch 14301/150000, Loss: 3122.084228515625, Validation Loss: 3277.923828125\n",
      "Epoch 14401/150000, Loss: 3062.3505859375, Validation Loss: 3218.307373046875\n",
      "Epoch 14501/150000, Loss: 3003.515380859375, Validation Loss: 3159.662841796875\n",
      "Epoch 14601/150000, Loss: 2945.544921875, Validation Loss: 3101.9755859375\n",
      "Epoch 14701/150000, Loss: 2888.4140625, Validation Loss: 3045.080810546875\n",
      "Epoch 14801/150000, Loss: 2832.030029296875, Validation Loss: 2988.72509765625\n",
      "Epoch 14901/150000, Loss: 2776.4248046875, Validation Loss: 2932.820068359375\n",
      "Epoch 15001/150000, Loss: 2721.729248046875, Validation Loss: 2878.593017578125\n",
      "Epoch 15101/150000, Loss: 2667.883544921875, Validation Loss: 2825.3896484375\n",
      "Epoch 15201/150000, Loss: 2614.919189453125, Validation Loss: 2773.006103515625\n",
      "Epoch 15301/150000, Loss: 2562.815673828125, Validation Loss: 2721.27685546875\n",
      "Epoch 15401/150000, Loss: 2511.95068359375, Validation Loss: 2670.19287109375\n",
      "Epoch 15501/150000, Loss: 2461.03271484375, Validation Loss: 2620.447998046875\n",
      "Epoch 15601/150000, Loss: 2411.3232421875, Validation Loss: 2571.49169921875\n",
      "Epoch 15701/150000, Loss: 2362.4033203125, Validation Loss: 2523.570556640625\n",
      "Epoch 15801/150000, Loss: 2314.201904296875, Validation Loss: 2476.000244140625\n",
      "Epoch 15901/150000, Loss: 2266.702880859375, Validation Loss: 2428.841064453125\n",
      "Epoch 16001/150000, Loss: 2219.86279296875, Validation Loss: 2382.98095703125\n",
      "Epoch 16101/150000, Loss: 2173.729736328125, Validation Loss: 2337.515380859375\n",
      "Epoch 16201/150000, Loss: 2128.324951171875, Validation Loss: 2292.9970703125\n",
      "Epoch 16301/150000, Loss: 2083.576904296875, Validation Loss: 2248.551025390625\n",
      "Epoch 16401/150000, Loss: 2039.5118408203125, Validation Loss: 2205.159912109375\n",
      "Epoch 16501/150000, Loss: 1996.2451171875, Validation Loss: 2162.018798828125\n",
      "Epoch 16601/150000, Loss: 1953.4549560546875, Validation Loss: 2119.988037109375\n",
      "Epoch 16701/150000, Loss: 1911.439453125, Validation Loss: 2078.639404296875\n",
      "Epoch 16801/150000, Loss: 1870.120361328125, Validation Loss: 2037.967529296875\n",
      "Epoch 16901/150000, Loss: 1829.533935546875, Validation Loss: 1998.038818359375\n",
      "Epoch 17001/150000, Loss: 1789.62158203125, Validation Loss: 1958.8629150390625\n",
      "Epoch 17101/150000, Loss: 1750.3577880859375, Validation Loss: 1920.3404541015625\n",
      "Epoch 17201/150000, Loss: 1711.8419189453125, Validation Loss: 1882.61083984375\n",
      "Epoch 17301/150000, Loss: 1673.9935302734375, Validation Loss: 1845.4010009765625\n",
      "Epoch 17401/150000, Loss: 1636.8106689453125, Validation Loss: 1809.00390625\n",
      "Epoch 17501/150000, Loss: 1600.3314208984375, Validation Loss: 1773.3089599609375\n",
      "Epoch 17601/150000, Loss: 1564.5020751953125, Validation Loss: 1738.2659912109375\n",
      "Epoch 17701/150000, Loss: 1529.2969970703125, Validation Loss: 1703.7977294921875\n",
      "Epoch 17801/150000, Loss: 1494.804443359375, Validation Loss: 1670.0665283203125\n",
      "Epoch 17901/150000, Loss: 1460.8792724609375, Validation Loss: 1637.015869140625\n",
      "Epoch 18001/150000, Loss: 1427.5509033203125, Validation Loss: 1604.589599609375\n",
      "Epoch 18101/150000, Loss: 1394.8494873046875, Validation Loss: 1572.9141845703125\n",
      "Epoch 18201/150000, Loss: 1362.6553955078125, Validation Loss: 1541.92138671875\n",
      "Epoch 18301/150000, Loss: 1331.047119140625, Validation Loss: 1511.2576904296875\n",
      "Epoch 18401/150000, Loss: 1299.840087890625, Validation Loss: 1481.0107421875\n",
      "Epoch 18501/150000, Loss: 1269.2022705078125, Validation Loss: 1451.322509765625\n",
      "Epoch 18601/150000, Loss: 1239.0369873046875, Validation Loss: 1422.4351806640625\n",
      "Epoch 18701/150000, Loss: 1209.4337158203125, Validation Loss: 1394.0010986328125\n",
      "Epoch 18801/150000, Loss: 1180.424072265625, Validation Loss: 1366.152099609375\n",
      "Epoch 18901/150000, Loss: 1151.9305419921875, Validation Loss: 1338.7850341796875\n",
      "Epoch 19001/150000, Loss: 1123.9647216796875, Validation Loss: 1311.9617919921875\n",
      "Epoch 19101/150000, Loss: 1096.56298828125, Validation Loss: 1285.7034912109375\n",
      "Epoch 19201/150000, Loss: 1069.777099609375, Validation Loss: 1260.1243896484375\n",
      "Epoch 19301/150000, Loss: 1043.451904296875, Validation Loss: 1235.0489501953125\n",
      "Epoch 19401/150000, Loss: 1017.7259521484375, Validation Loss: 1210.481689453125\n",
      "Epoch 19501/150000, Loss: 992.5093383789062, Validation Loss: 1186.5279541015625\n",
      "Epoch 19601/150000, Loss: 967.7904663085938, Validation Loss: 1163.1885986328125\n",
      "Epoch 19701/150000, Loss: 943.5368041992188, Validation Loss: 1140.41845703125\n",
      "Epoch 19801/150000, Loss: 919.6245727539062, Validation Loss: 1117.905517578125\n",
      "Epoch 19901/150000, Loss: 896.2564697265625, Validation Loss: 1096.095458984375\n",
      "Epoch 20001/150000, Loss: 873.4265747070312, Validation Loss: 1074.817626953125\n",
      "Epoch 20101/150000, Loss: 851.101806640625, Validation Loss: 1054.4093017578125\n",
      "Epoch 20201/150000, Loss: 829.210693359375, Validation Loss: 1034.7919921875\n",
      "Epoch 20301/150000, Loss: 807.792236328125, Validation Loss: 1015.8904418945312\n",
      "Epoch 20401/150000, Loss: 786.8169555664062, Validation Loss: 997.1594848632812\n",
      "Epoch 20501/150000, Loss: 766.2947387695312, Validation Loss: 978.13330078125\n",
      "Epoch 20601/150000, Loss: 746.1598510742188, Validation Loss: 959.5460205078125\n",
      "Epoch 20701/150000, Loss: 726.4671630859375, Validation Loss: 941.2908935546875\n",
      "Epoch 20801/150000, Loss: 707.1964111328125, Validation Loss: 923.5619506835938\n",
      "Epoch 20901/150000, Loss: 688.3324584960938, Validation Loss: 906.19482421875\n",
      "Epoch 21001/150000, Loss: 669.8807373046875, Validation Loss: 889.2147216796875\n",
      "Epoch 21101/150000, Loss: 652.015869140625, Validation Loss: 872.2692260742188\n",
      "Epoch 21201/150000, Loss: 634.2025756835938, Validation Loss: 856.9158935546875\n",
      "Epoch 21301/150000, Loss: 616.9563598632812, Validation Loss: 841.464111328125\n",
      "Epoch 21401/150000, Loss: 600.517333984375, Validation Loss: 827.6317138671875\n",
      "Epoch 21501/150000, Loss: 583.6342163085938, Validation Loss: 812.3007202148438\n",
      "Epoch 21601/150000, Loss: 567.5332641601562, Validation Loss: 797.792236328125\n",
      "Epoch 21701/150000, Loss: 551.81298828125, Validation Loss: 783.5276489257812\n",
      "Early stopping at epoch 21749 with validation loss 776.178466796875.\n",
      "Test Loss: 768.6978149414062\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.005]\n",
    "window_sizes = [15]j\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=5\n",
      "Epoch 1/150000, Loss: 30035.376953125, Validation Loss: 30251.630859375\n",
      "Epoch 101/150000, Loss: 29266.44921875, Validation Loss: 29474.916015625\n",
      "Epoch 201/150000, Loss: 28588.267578125, Validation Loss: 28796.248046875\n",
      "Epoch 301/150000, Loss: 28017.205078125, Validation Loss: 28223.380859375\n",
      "Epoch 401/150000, Loss: 27489.17578125, Validation Loss: 27693.4375\n",
      "Epoch 501/150000, Loss: 26990.47265625, Validation Loss: 27192.81640625\n",
      "Epoch 601/150000, Loss: 26515.4765625, Validation Loss: 26715.916015625\n",
      "Epoch 701/150000, Loss: 26061.1796875, Validation Loss: 26259.73046875\n",
      "Epoch 801/150000, Loss: 25625.68359375, Validation Loss: 25822.365234375\n",
      "Epoch 901/150000, Loss: 25207.666015625, Validation Loss: 25402.49609375\n",
      "Epoch 1001/150000, Loss: 24806.12109375, Validation Loss: 24999.123046875\n",
      "Epoch 1101/150000, Loss: 24420.23828125, Validation Loss: 24611.427734375\n",
      "Epoch 1201/150000, Loss: 24049.345703125, Validation Loss: 24238.740234375\n",
      "Epoch 1301/150000, Loss: 23692.8515625, Validation Loss: 23880.470703125\n",
      "Epoch 1401/150000, Loss: 23350.24609375, Validation Loss: 23536.109375\n",
      "Epoch 1501/150000, Loss: 23021.056640625, Validation Loss: 23205.18359375\n",
      "Epoch 1601/150000, Loss: 22704.865234375, Validation Loss: 22887.265625\n",
      "Epoch 1701/150000, Loss: 22401.2734375, Validation Loss: 22581.970703125\n",
      "Epoch 1801/150000, Loss: 22109.9140625, Validation Loss: 22288.927734375\n",
      "Epoch 1901/150000, Loss: 21830.447265625, Validation Loss: 22007.787109375\n",
      "Epoch 2001/150000, Loss: 21562.5390625, Validation Loss: 21738.23046875\n",
      "Epoch 2101/150000, Loss: 21305.888671875, Validation Loss: 21479.94140625\n",
      "Epoch 2201/150000, Loss: 21060.1953125, Validation Loss: 21232.6328125\n",
      "Epoch 2301/150000, Loss: 20825.17578125, Validation Loss: 20996.01171875\n",
      "Epoch 2401/150000, Loss: 20600.55859375, Validation Loss: 20769.810546875\n",
      "Epoch 2501/150000, Loss: 20386.07421875, Validation Loss: 20553.7578125\n",
      "Epoch 2601/150000, Loss: 20181.466796875, Validation Loss: 20347.603515625\n",
      "Epoch 2701/150000, Loss: 19986.486328125, Validation Loss: 20151.095703125\n",
      "Epoch 2801/150000, Loss: 19800.892578125, Validation Loss: 19963.984375\n",
      "Epoch 2901/150000, Loss: 19624.4296875, Validation Loss: 19786.03125\n",
      "Epoch 3001/150000, Loss: 19456.857421875, Validation Loss: 19616.984375\n",
      "Epoch 3101/150000, Loss: 19297.90625, Validation Loss: 19456.578125\n",
      "Epoch 3201/150000, Loss: 18277.046875, Validation Loss: 18452.904296875\n",
      "Epoch 3301/150000, Loss: 17959.693359375, Validation Loss: 18135.4375\n",
      "Epoch 3401/150000, Loss: 17655.84765625, Validation Loss: 17830.130859375\n",
      "Epoch 3501/150000, Loss: 17360.9453125, Validation Loss: 17534.25\n",
      "Epoch 3601/150000, Loss: 17074.888671875, Validation Loss: 17247.6640625\n",
      "Epoch 3701/150000, Loss: 16797.607421875, Validation Loss: 16970.064453125\n",
      "Epoch 3801/150000, Loss: 16528.349609375, Validation Loss: 16700.16796875\n",
      "Epoch 3901/150000, Loss: 16266.451171875, Validation Loss: 16437.27734375\n",
      "Epoch 4001/150000, Loss: 16011.3681640625, Validation Loss: 16180.9189453125\n",
      "Epoch 4101/150000, Loss: 15762.6357421875, Validation Loss: 15930.7001953125\n",
      "Epoch 4201/150000, Loss: 15519.9560546875, Validation Loss: 15686.4345703125\n",
      "Epoch 4301/150000, Loss: 15283.09765625, Validation Loss: 15447.9775390625\n",
      "Epoch 4401/150000, Loss: 15051.7900390625, Validation Loss: 15215.0986328125\n",
      "Epoch 4501/150000, Loss: 14825.748046875, Validation Loss: 14987.5283203125\n",
      "Epoch 4601/150000, Loss: 14604.716796875, Validation Loss: 14765.0390625\n",
      "Epoch 4701/150000, Loss: 14388.4560546875, Validation Loss: 14547.41796875\n",
      "Epoch 4801/150000, Loss: 14176.6533203125, Validation Loss: 14334.353515625\n",
      "Epoch 4901/150000, Loss: 13968.7197265625, Validation Loss: 14125.3818359375\n",
      "Epoch 5001/150000, Loss: 13764.4931640625, Validation Loss: 13920.287109375\n",
      "Epoch 5101/150000, Loss: 13564.1396484375, Validation Loss: 13719.017578125\n",
      "Epoch 5201/150000, Loss: 13367.53515625, Validation Loss: 13521.4521484375\n",
      "Epoch 5301/150000, Loss: 13174.544921875, Validation Loss: 13327.46875\n",
      "Epoch 5401/150000, Loss: 12985.0380859375, Validation Loss: 13136.9150390625\n",
      "Epoch 5501/150000, Loss: 12798.857421875, Validation Loss: 12949.626953125\n",
      "Epoch 5601/150000, Loss: 12615.841796875, Validation Loss: 12765.44140625\n",
      "Epoch 5701/150000, Loss: 12435.83203125, Validation Loss: 12584.2080078125\n",
      "Epoch 5801/150000, Loss: 12258.748046875, Validation Loss: 12405.912109375\n",
      "Epoch 5901/150000, Loss: 12084.5185546875, Validation Loss: 12230.7041015625\n",
      "Epoch 6001/150000, Loss: 11913.05078125, Validation Loss: 12058.4189453125\n",
      "Epoch 6101/150000, Loss: 11744.2568359375, Validation Loss: 11888.892578125\n",
      "Epoch 6201/150000, Loss: 11578.0576171875, Validation Loss: 11722.0224609375\n",
      "Epoch 6301/150000, Loss: 11414.369140625, Validation Loss: 11557.716796875\n",
      "Epoch 6401/150000, Loss: 11253.125, Validation Loss: 11395.9111328125\n",
      "Epoch 6501/150000, Loss: 11094.263671875, Validation Loss: 11236.537109375\n",
      "Epoch 6601/150000, Loss: 10937.724609375, Validation Loss: 11079.5263671875\n",
      "Epoch 6701/150000, Loss: 10783.431640625, Validation Loss: 10924.796875\n",
      "Epoch 6801/150000, Loss: 10631.33984375, Validation Loss: 10772.287109375\n",
      "Epoch 6901/150000, Loss: 10481.369140625, Validation Loss: 10621.8896484375\n",
      "Epoch 7001/150000, Loss: 10333.4462890625, Validation Loss: 10473.4599609375\n",
      "Epoch 7101/150000, Loss: 10187.4853515625, Validation Loss: 10326.6630859375\n",
      "Epoch 7201/150000, Loss: 10043.431640625, Validation Loss: 10181.9189453125\n",
      "Epoch 7301/150000, Loss: 9901.2841796875, Validation Loss: 10039.0615234375\n",
      "Epoch 7401/150000, Loss: 9760.939453125, Validation Loss: 9897.8935546875\n",
      "Epoch 7501/150000, Loss: 9622.3916015625, Validation Loss: 9758.373046875\n",
      "Epoch 7601/150000, Loss: 9485.56640625, Validation Loss: 9620.4326171875\n",
      "Epoch 7701/150000, Loss: 9350.4150390625, Validation Loss: 9483.9560546875\n",
      "Epoch 7801/150000, Loss: 9216.8544921875, Validation Loss: 9350.0615234375\n",
      "Epoch 7901/150000, Loss: 9084.9169921875, Validation Loss: 9217.375\n",
      "Epoch 8001/150000, Loss: 8954.59375, Validation Loss: 9086.548828125\n",
      "Epoch 8101/150000, Loss: 8825.7548828125, Validation Loss: 8957.171875\n",
      "Epoch 8201/150000, Loss: 8698.3154296875, Validation Loss: 8829.1572265625\n",
      "Epoch 8301/150000, Loss: 8572.2529296875, Validation Loss: 8702.5185546875\n",
      "Epoch 8401/150000, Loss: 8447.5126953125, Validation Loss: 8577.2001953125\n",
      "Epoch 8501/150000, Loss: 8324.0966796875, Validation Loss: 8453.25\n",
      "Epoch 8601/150000, Loss: 8201.96875, Validation Loss: 8330.5888671875\n",
      "Epoch 8701/150000, Loss: 8081.12451171875, Validation Loss: 8209.2314453125\n",
      "Epoch 8801/150000, Loss: 7961.552734375, Validation Loss: 8089.19482421875\n",
      "Epoch 8901/150000, Loss: 7843.22216796875, Validation Loss: 7970.4775390625\n",
      "Epoch 9001/150000, Loss: 7726.12109375, Validation Loss: 7852.99267578125\n",
      "Epoch 9101/150000, Loss: 7610.240234375, Validation Loss: 7736.8046875\n",
      "Epoch 9201/150000, Loss: 7495.56005859375, Validation Loss: 7621.87109375\n",
      "Epoch 9301/150000, Loss: 7382.06640625, Validation Loss: 7508.1611328125\n",
      "Epoch 9401/150000, Loss: 7269.7392578125, Validation Loss: 7395.650390625\n",
      "Epoch 9501/150000, Loss: 7158.57275390625, Validation Loss: 7284.30322265625\n",
      "Epoch 9601/150000, Loss: 7048.55859375, Validation Loss: 7174.0927734375\n",
      "Epoch 9701/150000, Loss: 6939.68798828125, Validation Loss: 7065.064453125\n",
      "Epoch 9801/150000, Loss: 6831.94189453125, Validation Loss: 6957.16259765625\n",
      "Epoch 9901/150000, Loss: 6725.3125, Validation Loss: 6850.39208984375\n",
      "Epoch 10001/150000, Loss: 6619.81005859375, Validation Loss: 6744.80615234375\n",
      "Epoch 10101/150000, Loss: 6515.45166015625, Validation Loss: 6640.3740234375\n",
      "Epoch 10201/150000, Loss: 6412.21630859375, Validation Loss: 6537.06005859375\n",
      "Epoch 10301/150000, Loss: 6310.13916015625, Validation Loss: 6434.9013671875\n",
      "Epoch 10401/150000, Loss: 6208.86474609375, Validation Loss: 6330.890625\n",
      "Epoch 10501/150000, Loss: 6108.8876953125, Validation Loss: 6230.57373046875\n",
      "Epoch 10601/150000, Loss: 6010.14111328125, Validation Loss: 6131.740234375\n",
      "Epoch 10701/150000, Loss: 5912.5361328125, Validation Loss: 6034.08984375\n",
      "Epoch 10801/150000, Loss: 5816.07177734375, Validation Loss: 5937.583984375\n",
      "Epoch 10901/150000, Loss: 5720.7451171875, Validation Loss: 5842.2470703125\n",
      "Epoch 11001/150000, Loss: 5626.591796875, Validation Loss: 5748.0927734375\n",
      "Epoch 11101/150000, Loss: 5533.58984375, Validation Loss: 5655.09423828125\n",
      "Epoch 11201/150000, Loss: 5441.7177734375, Validation Loss: 5563.25341796875\n",
      "Epoch 11301/150000, Loss: 5351.01513671875, Validation Loss: 5472.5390625\n",
      "Epoch 11401/150000, Loss: 5261.4345703125, Validation Loss: 5382.99462890625\n",
      "Epoch 11501/150000, Loss: 5172.9921875, Validation Loss: 5294.5986328125\n",
      "Epoch 11601/150000, Loss: 5085.66845703125, Validation Loss: 5207.4072265625\n",
      "Epoch 11701/150000, Loss: 4999.43994140625, Validation Loss: 5121.333984375\n",
      "Epoch 11801/150000, Loss: 4914.31103515625, Validation Loss: 5036.42529296875\n",
      "Epoch 11901/150000, Loss: 4830.234375, Validation Loss: 4952.712890625\n",
      "Epoch 12001/150000, Loss: 4747.17138671875, Validation Loss: 4869.86083984375\n",
      "Epoch 12101/150000, Loss: 4665.25732421875, Validation Loss: 4788.05126953125\n",
      "Epoch 12201/150000, Loss: 4584.18310546875, Validation Loss: 4707.576171875\n",
      "Epoch 12301/150000, Loss: 4504.1796875, Validation Loss: 4627.9462890625\n",
      "Epoch 12401/150000, Loss: 4425.14990234375, Validation Loss: 4549.28125\n",
      "Epoch 12501/150000, Loss: 4347.0234375, Validation Loss: 4471.845703125\n",
      "Epoch 12601/150000, Loss: 4269.95458984375, Validation Loss: 4395.1181640625\n",
      "Epoch 12701/150000, Loss: 4193.87060546875, Validation Loss: 4319.39599609375\n",
      "Epoch 12801/150000, Loss: 4118.82275390625, Validation Loss: 4244.82080078125\n",
      "Epoch 12901/150000, Loss: 4044.74658203125, Validation Loss: 4171.11767578125\n",
      "Epoch 13001/150000, Loss: 3971.66357421875, Validation Loss: 4098.5029296875\n",
      "Epoch 13101/150000, Loss: 3899.5791015625, Validation Loss: 4026.94384765625\n",
      "Epoch 13201/150000, Loss: 3828.470947265625, Validation Loss: 3956.333740234375\n",
      "Epoch 13301/150000, Loss: 3758.34521484375, Validation Loss: 3886.77685546875\n",
      "Epoch 13401/150000, Loss: 3689.197509765625, Validation Loss: 3818.175048828125\n",
      "Epoch 13501/150000, Loss: 3621.0244140625, Validation Loss: 3750.607666015625\n",
      "Epoch 13601/150000, Loss: 3553.82275390625, Validation Loss: 3684.018310546875\n",
      "Epoch 13701/150000, Loss: 3487.592041015625, Validation Loss: 3618.455078125\n",
      "Epoch 13801/150000, Loss: 3422.323486328125, Validation Loss: 3553.74072265625\n",
      "Epoch 13901/150000, Loss: 3358.016845703125, Validation Loss: 3490.034423828125\n",
      "Epoch 14001/150000, Loss: 3294.665283203125, Validation Loss: 3427.27197265625\n",
      "Epoch 14101/150000, Loss: 3232.269287109375, Validation Loss: 3365.478759765625\n",
      "Epoch 14201/150000, Loss: 3170.867919921875, Validation Loss: 3304.46630859375\n",
      "Epoch 14301/150000, Loss: 3110.31689453125, Validation Loss: 3244.710693359375\n",
      "Epoch 14401/150000, Loss: 3050.73681640625, Validation Loss: 3185.7275390625\n",
      "Epoch 14501/150000, Loss: 2992.076171875, Validation Loss: 3127.671142578125\n",
      "Epoch 14601/150000, Loss: 2934.321533203125, Validation Loss: 3070.5419921875\n",
      "Epoch 14701/150000, Loss: 2877.433349609375, Validation Loss: 3014.279296875\n",
      "Epoch 14801/150000, Loss: 2821.41259765625, Validation Loss: 2958.91552734375\n",
      "Epoch 14901/150000, Loss: 2766.2001953125, Validation Loss: 2904.42431640625\n",
      "Epoch 15001/150000, Loss: 2711.763427734375, Validation Loss: 2850.41943359375\n",
      "Epoch 15101/150000, Loss: 2658.165283203125, Validation Loss: 2797.173583984375\n",
      "Epoch 15201/150000, Loss: 2605.35400390625, Validation Loss: 2744.72607421875\n",
      "Epoch 15301/150000, Loss: 2553.301025390625, Validation Loss: 2693.05029296875\n",
      "Epoch 15401/150000, Loss: 2502.129150390625, Validation Loss: 2642.1083984375\n",
      "Epoch 15501/150000, Loss: 2451.63037109375, Validation Loss: 2592.316650390625\n",
      "Epoch 15601/150000, Loss: 2402.013671875, Validation Loss: 2543.280029296875\n",
      "Epoch 15701/150000, Loss: 2353.157470703125, Validation Loss: 2495.192626953125\n",
      "Epoch 15801/150000, Loss: 2305.097412109375, Validation Loss: 2447.8994140625\n",
      "Epoch 15901/150000, Loss: 2257.752197265625, Validation Loss: 2401.34375\n",
      "Epoch 16001/150000, Loss: 2211.1845703125, Validation Loss: 2355.604736328125\n",
      "Epoch 16101/150000, Loss: 2165.3095703125, Validation Loss: 2310.53564453125\n",
      "Epoch 16201/150000, Loss: 2120.118896484375, Validation Loss: 2266.175048828125\n",
      "Epoch 16301/150000, Loss: 2075.657958984375, Validation Loss: 2222.53955078125\n",
      "Epoch 16401/150000, Loss: 2031.8587646484375, Validation Loss: 2179.5498046875\n",
      "Epoch 16501/150000, Loss: 1988.7080078125, Validation Loss: 2137.201171875\n",
      "Epoch 16601/150000, Loss: 1946.26513671875, Validation Loss: 2095.556396484375\n",
      "Epoch 16701/150000, Loss: 1904.4761962890625, Validation Loss: 2054.581298828125\n",
      "Epoch 16801/150000, Loss: 1863.3665771484375, Validation Loss: 2014.2939453125\n",
      "Epoch 16901/150000, Loss: 1822.928955078125, Validation Loss: 1974.694580078125\n",
      "Epoch 17001/150000, Loss: 1783.1763916015625, Validation Loss: 1935.81884765625\n",
      "Epoch 17101/150000, Loss: 1744.072265625, Validation Loss: 1897.665771484375\n",
      "Epoch 17201/150000, Loss: 1705.655517578125, Validation Loss: 1860.1158447265625\n",
      "Epoch 17301/150000, Loss: 1667.9146728515625, Validation Loss: 1823.302978515625\n",
      "Epoch 17401/150000, Loss: 1630.828857421875, Validation Loss: 1787.166015625\n",
      "Epoch 17501/150000, Loss: 1594.399169921875, Validation Loss: 1751.694091796875\n",
      "Epoch 17601/150000, Loss: 1558.631103515625, Validation Loss: 1716.9122314453125\n",
      "Epoch 17701/150000, Loss: 1523.4986572265625, Validation Loss: 1682.81103515625\n",
      "Epoch 17801/150000, Loss: 1488.9925537109375, Validation Loss: 1649.4434814453125\n",
      "Epoch 17901/150000, Loss: 1455.10302734375, Validation Loss: 1616.70751953125\n",
      "Epoch 18001/150000, Loss: 1421.826904296875, Validation Loss: 1584.5589599609375\n",
      "Epoch 18101/150000, Loss: 1389.1387939453125, Validation Loss: 1553.1019287109375\n",
      "Epoch 18201/150000, Loss: 1357.025634765625, Validation Loss: 1522.2216796875\n",
      "Epoch 18301/150000, Loss: 1325.4654541015625, Validation Loss: 1492.0513916015625\n",
      "Epoch 18401/150000, Loss: 1294.4500732421875, Validation Loss: 1462.3104248046875\n",
      "Epoch 18501/150000, Loss: 1263.92724609375, Validation Loss: 1433.3773193359375\n",
      "Epoch 18601/150000, Loss: 1233.895751953125, Validation Loss: 1405.119140625\n",
      "Epoch 18701/150000, Loss: 1204.4173583984375, Validation Loss: 1377.192138671875\n",
      "Epoch 18801/150000, Loss: 1175.480224609375, Validation Loss: 1349.70361328125\n",
      "Epoch 18901/150000, Loss: 1147.0872802734375, Validation Loss: 1322.712158203125\n",
      "Epoch 19001/150000, Loss: 1119.2479248046875, Validation Loss: 1296.259033203125\n",
      "Epoch 19101/150000, Loss: 1091.9287109375, Validation Loss: 1270.51611328125\n",
      "Epoch 19201/150000, Loss: 1065.140380859375, Validation Loss: 1245.2694091796875\n",
      "Epoch 19301/150000, Loss: 1038.8800048828125, Validation Loss: 1220.5911865234375\n",
      "Epoch 19401/150000, Loss: 1013.1401977539062, Validation Loss: 1196.4949951171875\n",
      "Epoch 19501/150000, Loss: 987.922119140625, Validation Loss: 1172.9356689453125\n",
      "Epoch 19601/150000, Loss: 963.2158203125, Validation Loss: 1149.9884033203125\n",
      "Epoch 19701/150000, Loss: 939.0321044921875, Validation Loss: 1127.4906005859375\n",
      "Epoch 19801/150000, Loss: 915.3222045898438, Validation Loss: 1105.7156982421875\n",
      "Epoch 19901/150000, Loss: 892.1188354492188, Validation Loss: 1084.4083251953125\n",
      "Epoch 20001/150000, Loss: 869.3998413085938, Validation Loss: 1063.6085205078125\n",
      "Epoch 20101/150000, Loss: 847.1502075195312, Validation Loss: 1043.3250732421875\n",
      "Epoch 20201/150000, Loss: 825.3621215820312, Validation Loss: 1023.521240234375\n",
      "Epoch 20301/150000, Loss: 804.0294189453125, Validation Loss: 1004.1224975585938\n",
      "Epoch 20401/150000, Loss: 783.1475219726562, Validation Loss: 985.1769409179688\n",
      "Epoch 20501/150000, Loss: 762.7047729492188, Validation Loss: 966.6067504882812\n",
      "Epoch 20601/150000, Loss: 742.6958618164062, Validation Loss: 948.4927978515625\n",
      "Epoch 20701/150000, Loss: 723.1177368164062, Validation Loss: 930.783935546875\n",
      "Epoch 20801/150000, Loss: 703.9605102539062, Validation Loss: 913.4878540039062\n",
      "Epoch 20901/150000, Loss: 685.2197265625, Validation Loss: 896.65234375\n",
      "Epoch 21001/150000, Loss: 666.8870849609375, Validation Loss: 880.0623168945312\n",
      "Epoch 21101/150000, Loss: 648.956298828125, Validation Loss: 863.8416748046875\n",
      "Epoch 21201/150000, Loss: 631.4165649414062, Validation Loss: 848.0979614257812\n",
      "Epoch 21301/150000, Loss: 614.2586669921875, Validation Loss: 832.6395874023438\n",
      "Epoch 21401/150000, Loss: 597.4799194335938, Validation Loss: 817.5407104492188\n",
      "Epoch 21501/150000, Loss: 581.0708618164062, Validation Loss: 802.8468627929688\n",
      "Epoch 21601/150000, Loss: 565.0287475585938, Validation Loss: 788.3746948242188\n",
      "Epoch 21701/150000, Loss: 549.3351440429688, Validation Loss: 774.494384765625\n",
      "Epoch 21801/150000, Loss: 534.0027465820312, Validation Loss: 760.7705078125\n",
      "Epoch 21901/150000, Loss: 519.0313720703125, Validation Loss: 747.3795166015625\n",
      "Epoch 22001/150000, Loss: 504.4231262207031, Validation Loss: 734.3540649414062\n",
      "Epoch 22101/150000, Loss: 490.18890380859375, Validation Loss: 721.462646484375\n",
      "Epoch 22201/150000, Loss: 476.2889709472656, Validation Loss: 709.3385009765625\n",
      "Epoch 22301/150000, Loss: 462.75799560546875, Validation Loss: 697.3607177734375\n",
      "Epoch 22401/150000, Loss: 449.5800476074219, Validation Loss: 685.744873046875\n",
      "Epoch 22501/150000, Loss: 436.75421142578125, Validation Loss: 674.4482421875\n",
      "Epoch 22601/150000, Loss: 424.2772521972656, Validation Loss: 663.5059204101562\n",
      "Epoch 22701/150000, Loss: 412.1451110839844, Validation Loss: 652.9234008789062\n",
      "Epoch 22801/150000, Loss: 400.3604736328125, Validation Loss: 642.66455078125\n",
      "Epoch 22901/150000, Loss: 388.9022521972656, Validation Loss: 632.7413330078125\n",
      "Epoch 23001/150000, Loss: 377.7823791503906, Validation Loss: 623.1343383789062\n",
      "Epoch 23101/150000, Loss: 366.9922790527344, Validation Loss: 613.8661499023438\n",
      "Epoch 23201/150000, Loss: 356.52410888671875, Validation Loss: 604.8720703125\n",
      "Epoch 23301/150000, Loss: 346.4058532714844, Validation Loss: 596.55517578125\n",
      "Epoch 23401/150000, Loss: 336.5276794433594, Validation Loss: 587.6875610351562\n",
      "Epoch 23501/150000, Loss: 326.42181396484375, Validation Loss: 579.3397216796875\n",
      "Epoch 23601/150000, Loss: 315.9821472167969, Validation Loss: 568.427978515625\n",
      "Epoch 23701/150000, Loss: 306.9000244140625, Validation Loss: 559.2445068359375\n",
      "Epoch 23801/150000, Loss: 297.9416809082031, Validation Loss: 551.7218627929688\n",
      "Epoch 23901/150000, Loss: 289.2966003417969, Validation Loss: 544.5409545898438\n",
      "Epoch 24001/150000, Loss: 280.9027099609375, Validation Loss: 537.4105834960938\n",
      "Epoch 24101/150000, Loss: 272.09393310546875, Validation Loss: 529.7610473632812\n",
      "Epoch 24201/150000, Loss: 262.6543884277344, Validation Loss: 521.3081665039062\n",
      "Early stopping at epoch 24260 with validation loss 519.005615234375.\n",
      "Test Loss: 490.448974609375\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=10\n",
      "Epoch 1/150000, Loss: 29985.3828125, Validation Loss: 30293.947265625\n",
      "Epoch 101/150000, Loss: 29271.564453125, Validation Loss: 29569.4609375\n",
      "Epoch 201/150000, Loss: 28526.8046875, Validation Loss: 28823.5546875\n",
      "Epoch 301/150000, Loss: 27939.38671875, Validation Loss: 28232.82421875\n",
      "Epoch 401/150000, Loss: 27403.853515625, Validation Loss: 27693.912109375\n",
      "Epoch 501/150000, Loss: 26901.296875, Validation Loss: 27188.00390625\n",
      "Epoch 601/150000, Loss: 26424.41796875, Validation Loss: 26707.826171875\n",
      "Epoch 701/150000, Loss: 25969.451171875, Validation Loss: 26249.60546875\n",
      "Epoch 801/150000, Loss: 25534.11328125, Validation Loss: 25811.052734375\n",
      "Epoch 901/150000, Loss: 25116.82421875, Validation Loss: 25390.599609375\n",
      "Epoch 1001/150000, Loss: 24716.4375, Validation Loss: 24987.078125\n",
      "Epoch 1101/150000, Loss: 24332.03125, Validation Loss: 24599.58203125\n",
      "Epoch 1201/150000, Loss: 23962.865234375, Validation Loss: 24227.353515625\n",
      "Epoch 1301/150000, Loss: 23608.29296875, Validation Loss: 23869.759765625\n",
      "Epoch 1401/150000, Loss: 23267.76171875, Validation Loss: 23526.240234375\n",
      "Epoch 1501/150000, Loss: 22940.76171875, Validation Loss: 23196.2890625\n",
      "Epoch 1601/150000, Loss: 22626.859375, Validation Loss: 22879.458984375\n",
      "Epoch 1701/150000, Loss: 22325.630859375, Validation Loss: 22575.341796875\n",
      "Epoch 1801/150000, Loss: 22036.693359375, Validation Loss: 22283.54296875\n",
      "Epoch 1901/150000, Loss: 21759.69140625, Validation Loss: 22003.7109375\n",
      "Epoch 2001/150000, Loss: 21494.279296875, Validation Loss: 21735.505859375\n",
      "Epoch 2101/150000, Loss: 20998.359375, Validation Loss: 21253.37890625\n",
      "Epoch 2201/150000, Loss: 20658.6640625, Validation Loss: 20919.234375\n",
      "Epoch 2301/150000, Loss: 20337.9609375, Validation Loss: 20597.982421875\n",
      "Epoch 2401/150000, Loss: 20024.49609375, Validation Loss: 20283.595703125\n",
      "Epoch 2501/150000, Loss: 19717.765625, Validation Loss: 19975.626953125\n",
      "Epoch 2601/150000, Loss: 19417.26953125, Validation Loss: 19673.62890625\n",
      "Epoch 2701/150000, Loss: 19122.634765625, Validation Loss: 19377.0703125\n",
      "Epoch 2801/150000, Loss: 18833.63671875, Validation Loss: 19085.775390625\n",
      "Epoch 2901/150000, Loss: 18550.05078125, Validation Loss: 18799.509765625\n",
      "Epoch 3001/150000, Loss: 18271.6640625, Validation Loss: 18518.0\n",
      "Epoch 3101/150000, Loss: 17998.296875, Validation Loss: 18241.103515625\n",
      "Epoch 3201/150000, Loss: 17729.783203125, Validation Loss: 17968.669921875\n",
      "Epoch 3301/150000, Loss: 17465.931640625, Validation Loss: 17700.279296875\n",
      "Epoch 3401/150000, Loss: 17206.65234375, Validation Loss: 17436.345703125\n",
      "Epoch 3501/150000, Loss: 16951.986328125, Validation Loss: 17178.04296875\n",
      "Epoch 3601/150000, Loss: 16701.91796875, Validation Loss: 16924.69921875\n",
      "Epoch 3701/150000, Loss: 16456.29296875, Validation Loss: 16675.97265625\n",
      "Epoch 3801/150000, Loss: 16215.0703125, Validation Loss: 16431.8671875\n",
      "Epoch 3901/150000, Loss: 15978.1494140625, Validation Loss: 16192.20703125\n",
      "Epoch 4001/150000, Loss: 15745.482421875, Validation Loss: 15956.890625\n",
      "Epoch 4101/150000, Loss: 15516.8876953125, Validation Loss: 15725.94140625\n",
      "Epoch 4201/150000, Loss: 15292.3935546875, Validation Loss: 15499.1015625\n",
      "Epoch 4301/150000, Loss: 15071.873046875, Validation Loss: 15276.45703125\n",
      "Epoch 4401/150000, Loss: 14855.251953125, Validation Loss: 15057.7880859375\n",
      "Epoch 4501/150000, Loss: 14642.08203125, Validation Loss: 14843.1513671875\n",
      "Epoch 4601/150000, Loss: 14432.38671875, Validation Loss: 14632.0634765625\n",
      "Epoch 4701/150000, Loss: 14226.228515625, Validation Loss: 14424.5380859375\n",
      "Epoch 4801/150000, Loss: 14023.5400390625, Validation Loss: 14220.443359375\n",
      "Epoch 4901/150000, Loss: 13824.228515625, Validation Loss: 14019.6337890625\n",
      "Epoch 5001/150000, Loss: 13628.2109375, Validation Loss: 13822.0498046875\n",
      "Epoch 5101/150000, Loss: 13435.3759765625, Validation Loss: 13627.5517578125\n",
      "Epoch 5201/150000, Loss: 13245.630859375, Validation Loss: 13436.12890625\n",
      "Epoch 5301/150000, Loss: 13058.84375, Validation Loss: 13247.70703125\n",
      "Epoch 5401/150000, Loss: 12874.947265625, Validation Loss: 13062.1025390625\n",
      "Epoch 5501/150000, Loss: 12693.8671875, Validation Loss: 12879.3359375\n",
      "Epoch 5601/150000, Loss: 12515.5, Validation Loss: 12699.0986328125\n",
      "Epoch 5701/150000, Loss: 12339.72265625, Validation Loss: 12521.41796875\n",
      "Epoch 5801/150000, Loss: 12166.56640625, Validation Loss: 12346.5\n",
      "Epoch 5901/150000, Loss: 11995.984375, Validation Loss: 12174.337890625\n",
      "Epoch 6001/150000, Loss: 11827.8251953125, Validation Loss: 12004.767578125\n",
      "Epoch 6101/150000, Loss: 11662.033203125, Validation Loss: 11837.7216796875\n",
      "Epoch 6201/150000, Loss: 11498.6064453125, Validation Loss: 11673.1455078125\n",
      "Epoch 6301/150000, Loss: 11337.5263671875, Validation Loss: 11510.826171875\n",
      "Epoch 6401/150000, Loss: 11178.666015625, Validation Loss: 11350.6767578125\n",
      "Epoch 6501/150000, Loss: 11022.072265625, Validation Loss: 11192.74609375\n",
      "Epoch 6601/150000, Loss: 10867.669921875, Validation Loss: 11037.212890625\n",
      "Epoch 6701/150000, Loss: 10715.384765625, Validation Loss: 10884.1591796875\n",
      "Epoch 6801/150000, Loss: 10565.140625, Validation Loss: 10733.337890625\n",
      "Epoch 6901/150000, Loss: 10416.87109375, Validation Loss: 10584.533203125\n",
      "Epoch 7001/150000, Loss: 10270.541015625, Validation Loss: 10437.6298828125\n",
      "Epoch 7101/150000, Loss: 10126.072265625, Validation Loss: 10292.7216796875\n",
      "Epoch 7201/150000, Loss: 9983.5029296875, Validation Loss: 10149.546875\n",
      "Epoch 7301/150000, Loss: 9842.716796875, Validation Loss: 10008.263671875\n",
      "Epoch 7401/150000, Loss: 9703.7451171875, Validation Loss: 9868.5634765625\n",
      "Epoch 7501/150000, Loss: 9566.5556640625, Validation Loss: 9730.3994140625\n",
      "Epoch 7601/150000, Loss: 9431.0556640625, Validation Loss: 9593.6181640625\n",
      "Epoch 7701/150000, Loss: 9297.1689453125, Validation Loss: 9458.25390625\n",
      "Epoch 7801/150000, Loss: 9164.8271484375, Validation Loss: 9324.3935546875\n",
      "Epoch 7901/150000, Loss: 9033.943359375, Validation Loss: 9192.080078125\n",
      "Epoch 8001/150000, Loss: 8904.48828125, Validation Loss: 9061.544921875\n",
      "Epoch 8101/150000, Loss: 8776.4140625, Validation Loss: 8932.70703125\n",
      "Epoch 8201/150000, Loss: 8649.71875, Validation Loss: 8805.3759765625\n",
      "Epoch 8301/150000, Loss: 8524.3427734375, Validation Loss: 8679.232421875\n",
      "Epoch 8401/150000, Loss: 8400.248046875, Validation Loss: 8554.52734375\n",
      "Epoch 8501/150000, Loss: 8277.5546875, Validation Loss: 8431.2666015625\n",
      "Epoch 8601/150000, Loss: 8156.12841796875, Validation Loss: 8309.40234375\n",
      "Epoch 8701/150000, Loss: 8035.98486328125, Validation Loss: 8188.7255859375\n",
      "Epoch 8801/150000, Loss: 7917.0498046875, Validation Loss: 8069.4599609375\n",
      "Epoch 8901/150000, Loss: 7799.3720703125, Validation Loss: 7951.42431640625\n",
      "Epoch 9001/150000, Loss: 7682.9150390625, Validation Loss: 7834.59130859375\n",
      "Epoch 9101/150000, Loss: 7567.5419921875, Validation Loss: 7718.73193359375\n",
      "Epoch 9201/150000, Loss: 7453.2548828125, Validation Loss: 7604.05322265625\n",
      "Epoch 9301/150000, Loss: 7340.224609375, Validation Loss: 7490.8037109375\n",
      "Epoch 9401/150000, Loss: 7228.32568359375, Validation Loss: 7378.55126953125\n",
      "Epoch 9501/150000, Loss: 7117.541015625, Validation Loss: 7267.298828125\n",
      "Epoch 9601/150000, Loss: 7007.8935546875, Validation Loss: 7157.31103515625\n",
      "Epoch 9701/150000, Loss: 6899.39990234375, Validation Loss: 7048.609375\n",
      "Epoch 9801/150000, Loss: 6792.05078125, Validation Loss: 6941.09228515625\n",
      "Epoch 9901/150000, Loss: 6685.8740234375, Validation Loss: 6834.86474609375\n",
      "Epoch 10001/150000, Loss: 6580.802734375, Validation Loss: 6729.58984375\n",
      "Epoch 10101/150000, Loss: 6476.90185546875, Validation Loss: 6625.4365234375\n",
      "Epoch 10201/150000, Loss: 6374.0986328125, Validation Loss: 6521.73095703125\n",
      "Epoch 10301/150000, Loss: 6272.35595703125, Validation Loss: 6419.14111328125\n",
      "Epoch 10401/150000, Loss: 6171.7041015625, Validation Loss: 6317.9345703125\n",
      "Epoch 10501/150000, Loss: 6072.22216796875, Validation Loss: 6218.02685546875\n",
      "Epoch 10601/150000, Loss: 5973.83837890625, Validation Loss: 6119.0634765625\n",
      "Epoch 10701/150000, Loss: 5876.59619140625, Validation Loss: 6021.396484375\n",
      "Epoch 10801/150000, Loss: 5780.50390625, Validation Loss: 5924.7841796875\n",
      "Epoch 10901/150000, Loss: 5685.57373046875, Validation Loss: 5829.30908203125\n",
      "Epoch 11001/150000, Loss: 5592.1650390625, Validation Loss: 5734.51171875\n",
      "Epoch 11101/150000, Loss: 5499.2705078125, Validation Loss: 5639.7099609375\n",
      "Epoch 11201/150000, Loss: 5407.8203125, Validation Loss: 5547.552734375\n",
      "Epoch 11301/150000, Loss: 5317.54150390625, Validation Loss: 5456.77685546875\n",
      "Epoch 11401/150000, Loss: 5228.4375, Validation Loss: 5367.25341796875\n",
      "Epoch 11501/150000, Loss: 5140.482421875, Validation Loss: 5278.9111328125\n",
      "Epoch 11601/150000, Loss: 5053.662109375, Validation Loss: 5191.77587890625\n",
      "Epoch 11701/150000, Loss: 4967.9150390625, Validation Loss: 5105.87890625\n",
      "Epoch 11801/150000, Loss: 4883.1787109375, Validation Loss: 5021.13916015625\n",
      "Epoch 11901/150000, Loss: 4799.45703125, Validation Loss: 4937.53759765625\n",
      "Epoch 12001/150000, Loss: 4716.70947265625, Validation Loss: 4855.0322265625\n",
      "Epoch 12101/150000, Loss: 4634.99853515625, Validation Loss: 4773.58740234375\n",
      "Epoch 12201/150000, Loss: 4554.30078125, Validation Loss: 4693.35693359375\n",
      "Epoch 12301/150000, Loss: 4474.576171875, Validation Loss: 4614.45849609375\n",
      "Epoch 12401/150000, Loss: 4395.8095703125, Validation Loss: 4536.29443359375\n",
      "Epoch 12501/150000, Loss: 4318.06005859375, Validation Loss: 4458.87890625\n",
      "Epoch 12601/150000, Loss: 4241.33447265625, Validation Loss: 4382.6162109375\n",
      "Epoch 12701/150000, Loss: 4165.623046875, Validation Loss: 4307.1669921875\n",
      "Epoch 12801/150000, Loss: 4090.928466796875, Validation Loss: 4232.81005859375\n",
      "Epoch 12901/150000, Loss: 4017.2578125, Validation Loss: 4159.5771484375\n",
      "Epoch 13001/150000, Loss: 3944.556396484375, Validation Loss: 4087.11572265625\n",
      "Epoch 13101/150000, Loss: 3872.847412109375, Validation Loss: 4015.82861328125\n",
      "Epoch 13201/150000, Loss: 3802.10546875, Validation Loss: 3945.5947265625\n",
      "Epoch 13301/150000, Loss: 3732.329833984375, Validation Loss: 3876.32373046875\n",
      "Epoch 13401/150000, Loss: 3663.52880859375, Validation Loss: 3808.089111328125\n",
      "Epoch 13501/150000, Loss: 3595.68994140625, Validation Loss: 3740.646484375\n",
      "Epoch 13601/150000, Loss: 3528.818115234375, Validation Loss: 3674.215576171875\n",
      "Epoch 13701/150000, Loss: 3462.923095703125, Validation Loss: 3608.7587890625\n",
      "Epoch 13801/150000, Loss: 3398.0009765625, Validation Loss: 3544.19384765625\n",
      "Epoch 13901/150000, Loss: 3334.05810546875, Validation Loss: 3480.638916015625\n",
      "Epoch 14001/150000, Loss: 3271.07421875, Validation Loss: 3418.093994140625\n",
      "Epoch 14101/150000, Loss: 3209.063720703125, Validation Loss: 3356.528564453125\n",
      "Epoch 14201/150000, Loss: 3148.007568359375, Validation Loss: 3295.7392578125\n",
      "Epoch 14301/150000, Loss: 3087.921142578125, Validation Loss: 3235.955322265625\n",
      "Epoch 14401/150000, Loss: 3028.751220703125, Validation Loss: 3176.83203125\n",
      "Epoch 14501/150000, Loss: 2970.499755859375, Validation Loss: 3118.719970703125\n",
      "Epoch 14601/150000, Loss: 2912.875244140625, Validation Loss: 3061.096435546875\n",
      "Epoch 14701/150000, Loss: 2856.201171875, Validation Loss: 3004.5615234375\n",
      "Epoch 14801/150000, Loss: 2800.33154296875, Validation Loss: 2948.781982421875\n",
      "Epoch 14901/150000, Loss: 2745.327392578125, Validation Loss: 2893.971435546875\n",
      "Epoch 15001/150000, Loss: 2691.142822265625, Validation Loss: 2840.106689453125\n",
      "Epoch 15101/150000, Loss: 2637.79931640625, Validation Loss: 2787.18603515625\n",
      "Epoch 15201/150000, Loss: 2585.3017578125, Validation Loss: 2735.051025390625\n",
      "Epoch 15301/150000, Loss: 2533.59423828125, Validation Loss: 2684.0126953125\n",
      "Epoch 15401/150000, Loss: 2482.70947265625, Validation Loss: 2633.72509765625\n",
      "Epoch 15501/150000, Loss: 2432.63818359375, Validation Loss: 2584.29833984375\n",
      "Epoch 15601/150000, Loss: 2383.35693359375, Validation Loss: 2535.786865234375\n",
      "Epoch 15701/150000, Loss: 2334.8623046875, Validation Loss: 2488.073974609375\n",
      "Epoch 15801/150000, Loss: 2287.09765625, Validation Loss: 2441.25537109375\n",
      "Epoch 15901/150000, Loss: 2240.058837890625, Validation Loss: 2395.14501953125\n",
      "Epoch 16001/150000, Loss: 2193.716552734375, Validation Loss: 2349.61328125\n",
      "Epoch 16101/150000, Loss: 2148.0419921875, Validation Loss: 2304.64501953125\n",
      "Epoch 16201/150000, Loss: 2103.027099609375, Validation Loss: 2260.285400390625\n",
      "Epoch 16301/150000, Loss: 2058.677001953125, Validation Loss: 2216.5615234375\n",
      "Epoch 16401/150000, Loss: 2015.015625, Validation Loss: 2173.60546875\n",
      "Epoch 16501/150000, Loss: 1972.0557861328125, Validation Loss: 2131.362060546875\n",
      "Epoch 16601/150000, Loss: 1929.8238525390625, Validation Loss: 2089.7138671875\n",
      "Epoch 16701/150000, Loss: 1888.1929931640625, Validation Loss: 2049.032958984375\n",
      "Epoch 16801/150000, Loss: 1847.3028564453125, Validation Loss: 2008.987060546875\n",
      "Epoch 16901/150000, Loss: 1807.1365966796875, Validation Loss: 1969.61669921875\n",
      "Epoch 17001/150000, Loss: 1767.5819091796875, Validation Loss: 1931.1446533203125\n",
      "Epoch 17101/150000, Loss: 1728.7540283203125, Validation Loss: 1893.2998046875\n",
      "Epoch 17201/150000, Loss: 1690.603759765625, Validation Loss: 1856.1502685546875\n",
      "Epoch 17301/150000, Loss: 1653.1351318359375, Validation Loss: 1819.58203125\n",
      "Epoch 17401/150000, Loss: 1616.3282470703125, Validation Loss: 1783.87890625\n",
      "Epoch 17501/150000, Loss: 1580.2027587890625, Validation Loss: 1748.782470703125\n",
      "Epoch 17601/150000, Loss: 1544.7431640625, Validation Loss: 1714.3853759765625\n",
      "Epoch 17701/150000, Loss: 1509.9263916015625, Validation Loss: 1680.6304931640625\n",
      "Epoch 17801/150000, Loss: 1475.7451171875, Validation Loss: 1647.595458984375\n",
      "Epoch 17901/150000, Loss: 1442.1929931640625, Validation Loss: 1615.02294921875\n",
      "Epoch 18001/150000, Loss: 1409.29638671875, Validation Loss: 1583.5013427734375\n",
      "Epoch 18101/150000, Loss: 1376.81298828125, Validation Loss: 1552.29296875\n",
      "Epoch 18201/150000, Loss: 1344.8795166015625, Validation Loss: 1521.91943359375\n",
      "Epoch 18301/150000, Loss: 1313.467041015625, Validation Loss: 1491.9051513671875\n",
      "Epoch 18401/150000, Loss: 1282.6551513671875, Validation Loss: 1462.82958984375\n",
      "Epoch 18501/150000, Loss: 1252.2535400390625, Validation Loss: 1433.422119140625\n",
      "Epoch 18601/150000, Loss: 1222.43359375, Validation Loss: 1404.982421875\n",
      "Epoch 18701/150000, Loss: 1193.1602783203125, Validation Loss: 1377.1011962890625\n",
      "Epoch 18801/150000, Loss: 1164.434814453125, Validation Loss: 1349.6065673828125\n",
      "Epoch 18901/150000, Loss: 1136.257568359375, Validation Loss: 1322.6964111328125\n",
      "Epoch 19001/150000, Loss: 1108.625244140625, Validation Loss: 1296.2802734375\n",
      "Epoch 19101/150000, Loss: 1081.5345458984375, Validation Loss: 1270.4666748046875\n",
      "Epoch 19201/150000, Loss: 1054.9581298828125, Validation Loss: 1245.2896728515625\n",
      "Epoch 19301/150000, Loss: 1028.905517578125, Validation Loss: 1220.8016357421875\n",
      "Epoch 19401/150000, Loss: 1003.373779296875, Validation Loss: 1196.81005859375\n",
      "Epoch 19501/150000, Loss: 978.3515625, Validation Loss: 1173.5762939453125\n",
      "Epoch 19601/150000, Loss: 953.8346557617188, Validation Loss: 1150.6800537109375\n",
      "Epoch 19701/150000, Loss: 929.8246459960938, Validation Loss: 1128.1171875\n",
      "Epoch 19801/150000, Loss: 906.2861938476562, Validation Loss: 1106.2547607421875\n",
      "Epoch 19901/150000, Loss: 883.2352905273438, Validation Loss: 1084.9249267578125\n",
      "Epoch 20001/150000, Loss: 860.660888671875, Validation Loss: 1064.1314697265625\n",
      "Epoch 20101/150000, Loss: 838.5552368164062, Validation Loss: 1043.8560791015625\n",
      "Epoch 20201/150000, Loss: 816.8939208984375, Validation Loss: 1024.117919921875\n",
      "Epoch 20301/150000, Loss: 795.6911010742188, Validation Loss: 1004.8047485351562\n",
      "Epoch 20401/150000, Loss: 774.9485473632812, Validation Loss: 985.8587646484375\n",
      "Epoch 20501/150000, Loss: 754.6604614257812, Validation Loss: 967.4368896484375\n",
      "Epoch 20601/150000, Loss: 734.8382568359375, Validation Loss: 949.5308227539062\n",
      "Epoch 20701/150000, Loss: 715.392578125, Validation Loss: 931.83056640625\n",
      "Epoch 20801/150000, Loss: 696.3702392578125, Validation Loss: 914.37255859375\n",
      "Epoch 20901/150000, Loss: 677.7705688476562, Validation Loss: 897.4859008789062\n",
      "Epoch 21001/150000, Loss: 659.5485229492188, Validation Loss: 881.1067504882812\n",
      "Epoch 21101/150000, Loss: 641.7337646484375, Validation Loss: 865.0613403320312\n",
      "Epoch 21201/150000, Loss: 624.313232421875, Validation Loss: 849.4638671875\n",
      "Epoch 21301/150000, Loss: 607.2907104492188, Validation Loss: 834.1320190429688\n",
      "Epoch 21401/150000, Loss: 590.7704467773438, Validation Loss: 818.7636108398438\n",
      "Epoch 21501/150000, Loss: 574.5006713867188, Validation Loss: 804.4693603515625\n",
      "Epoch 21601/150000, Loss: 558.5287475585938, Validation Loss: 790.462646484375\n",
      "Epoch 21701/150000, Loss: 543.0533447265625, Validation Loss: 776.6220703125\n",
      "Epoch 21801/150000, Loss: 527.918212890625, Validation Loss: 763.1124877929688\n",
      "Epoch 21901/150000, Loss: 513.1951293945312, Validation Loss: 749.8311157226562\n",
      "Epoch 22001/150000, Loss: 498.75885009765625, Validation Loss: 737.1424560546875\n",
      "Epoch 22101/150000, Loss: 484.6951599121094, Validation Loss: 724.7423706054688\n",
      "Epoch 22201/150000, Loss: 471.07684326171875, Validation Loss: 712.4310302734375\n",
      "Epoch 22301/150000, Loss: 457.6224060058594, Validation Loss: 700.7587890625\n",
      "Epoch 22401/150000, Loss: 444.5976257324219, Validation Loss: 689.2017211914062\n",
      "Epoch 22501/150000, Loss: 432.13458251953125, Validation Loss: 678.8174438476562\n",
      "Epoch 22601/150000, Loss: 419.54302978515625, Validation Loss: 667.1221313476562\n",
      "Epoch 22701/150000, Loss: 410.3061828613281, Validation Loss: 657.77587890625\n",
      "Epoch 22801/150000, Loss: 396.3201904296875, Validation Loss: 644.890380859375\n",
      "Epoch 22901/150000, Loss: 384.87359619140625, Validation Loss: 635.4736328125\n",
      "Epoch 23001/150000, Loss: 373.75244140625, Validation Loss: 626.1787109375\n",
      "Epoch 23101/150000, Loss: 362.9180603027344, Validation Loss: 617.0241088867188\n",
      "Epoch 23201/150000, Loss: 352.35760498046875, Validation Loss: 608.0105590820312\n",
      "Epoch 23301/150000, Loss: 342.0250549316406, Validation Loss: 599.1719970703125\n",
      "Epoch 23401/150000, Loss: 331.9087829589844, Validation Loss: 590.4959106445312\n",
      "Epoch 23501/150000, Loss: 322.0833435058594, Validation Loss: 581.9697875976562\n",
      "Epoch 23601/150000, Loss: 312.5708312988281, Validation Loss: 573.6128540039062\n",
      "Epoch 23701/150000, Loss: 303.38409423828125, Validation Loss: 565.5857543945312\n",
      "Epoch 23801/150000, Loss: 294.5153503417969, Validation Loss: 558.12548828125\n",
      "Epoch 23901/150000, Loss: 285.9053955078125, Validation Loss: 550.7481079101562\n",
      "Epoch 24001/150000, Loss: 277.5867919921875, Validation Loss: 543.6366577148438\n",
      "Epoch 24101/150000, Loss: 269.5310974121094, Validation Loss: 537.2568359375\n",
      "Epoch 24201/150000, Loss: 261.7022399902344, Validation Loss: 531.1121215820312\n",
      "Epoch 24301/150000, Loss: 254.0233154296875, Validation Loss: 525.8441162109375\n",
      "Epoch 24401/150000, Loss: 246.76368713378906, Validation Loss: 520.0761108398438\n",
      "Epoch 24501/150000, Loss: 239.5919647216797, Validation Loss: 515.24169921875\n",
      "Epoch 24601/150000, Loss: 232.45301818847656, Validation Loss: 510.5497131347656\n",
      "Epoch 24701/150000, Loss: 224.65155029296875, Validation Loss: 508.6015319824219\n",
      "Epoch 24801/150000, Loss: 223.9134521484375, Validation Loss: 506.5096130371094\n",
      "Early stopping at epoch 24834 with validation loss 503.3526916503906.\n",
      "Test Loss: 449.4964294433594\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=15\n",
      "Epoch 1/150000, Loss: 30059.91015625, Validation Loss: 30460.876953125\n",
      "Epoch 101/150000, Loss: 29370.0859375, Validation Loss: 29761.26171875\n",
      "Epoch 201/150000, Loss: 28670.58984375, Validation Loss: 29058.08984375\n",
      "Epoch 301/150000, Loss: 28093.18359375, Validation Loss: 28475.900390625\n",
      "Epoch 401/150000, Loss: 27561.388671875, Validation Loss: 27939.357421875\n",
      "Epoch 501/150000, Loss: 27059.87890625, Validation Loss: 27433.17578125\n",
      "Epoch 601/150000, Loss: 26582.556640625, Validation Loss: 26951.26171875\n",
      "Epoch 701/150000, Loss: 26126.216796875, Validation Loss: 26490.392578125\n",
      "Epoch 801/150000, Loss: 25688.86328125, Validation Loss: 26048.580078125\n",
      "Epoch 901/150000, Loss: 25269.12109375, Validation Loss: 25624.42578125\n",
      "Epoch 1001/150000, Loss: 24865.943359375, Validation Loss: 25216.8984375\n",
      "Epoch 1101/150000, Loss: 24478.501953125, Validation Loss: 24825.16015625\n",
      "Epoch 1201/150000, Loss: 24106.103515625, Validation Loss: 24448.513671875\n",
      "Epoch 1301/150000, Loss: 23748.16015625, Validation Loss: 24086.369140625\n",
      "Epoch 1401/150000, Loss: 23404.142578125, Validation Loss: 23738.19921875\n",
      "Epoch 1501/150000, Loss: 23073.578125, Validation Loss: 23403.525390625\n",
      "Epoch 1601/150000, Loss: 22756.0390625, Validation Loss: 23081.927734375\n",
      "Epoch 1701/150000, Loss: 22451.12890625, Validation Loss: 22773.001953125\n",
      "Epoch 1801/150000, Loss: 22158.48046875, Validation Loss: 22476.376953125\n",
      "Epoch 1901/150000, Loss: 21877.70703125, Validation Loss: 22191.671875\n",
      "Epoch 2001/150000, Loss: 21396.646484375, Validation Loss: 21731.97265625\n",
      "Epoch 2101/150000, Loss: 21066.4609375, Validation Loss: 21399.095703125\n",
      "Epoch 2201/150000, Loss: 20743.482421875, Validation Loss: 21073.255859375\n",
      "Epoch 2301/150000, Loss: 20427.18359375, Validation Loss: 20753.248046875\n",
      "Epoch 2401/150000, Loss: 20117.19140625, Validation Loss: 20439.193359375\n",
      "Epoch 2501/150000, Loss: 19813.119140625, Validation Loss: 20130.220703125\n",
      "Epoch 2601/150000, Loss: 19514.779296875, Validation Loss: 19827.56640625\n",
      "Epoch 2701/150000, Loss: 19221.931640625, Validation Loss: 19530.625\n",
      "Epoch 2801/150000, Loss: 18934.349609375, Validation Loss: 19239.08984375\n",
      "Epoch 2901/150000, Loss: 18651.873046875, Validation Loss: 18953.029296875\n",
      "Epoch 3001/150000, Loss: 18374.353515625, Validation Loss: 18672.171875\n",
      "Epoch 3101/150000, Loss: 18101.638671875, Validation Loss: 18396.162109375\n",
      "Epoch 3201/150000, Loss: 17833.537109375, Validation Loss: 18124.82421875\n",
      "Epoch 3301/150000, Loss: 17569.984375, Validation Loss: 17858.001953125\n",
      "Epoch 3401/150000, Loss: 17310.916015625, Validation Loss: 17595.638671875\n",
      "Epoch 3501/150000, Loss: 17056.205078125, Validation Loss: 17337.759765625\n",
      "Epoch 3601/150000, Loss: 16805.853515625, Validation Loss: 17084.228515625\n",
      "Epoch 3701/150000, Loss: 16559.79296875, Validation Loss: 16835.044921875\n",
      "Epoch 3801/150000, Loss: 16317.9619140625, Validation Loss: 16590.15625\n",
      "Epoch 3901/150000, Loss: 16080.296875, Validation Loss: 16349.490234375\n",
      "Epoch 4001/150000, Loss: 15846.759765625, Validation Loss: 16113.072265625\n",
      "Epoch 4101/150000, Loss: 15617.2451171875, Validation Loss: 15880.8203125\n",
      "Epoch 4201/150000, Loss: 15391.732421875, Validation Loss: 15652.6953125\n",
      "Epoch 4301/150000, Loss: 15170.171875, Validation Loss: 15428.6728515625\n",
      "Epoch 4401/150000, Loss: 14952.4560546875, Validation Loss: 15208.5703125\n",
      "Epoch 4501/150000, Loss: 14738.6142578125, Validation Loss: 14992.451171875\n",
      "Epoch 4601/150000, Loss: 14528.4482421875, Validation Loss: 14780.1708984375\n",
      "Epoch 4701/150000, Loss: 14320.9365234375, Validation Loss: 14571.6845703125\n",
      "Epoch 4801/150000, Loss: 14117.15234375, Validation Loss: 14366.2158203125\n",
      "Epoch 4901/150000, Loss: 13916.765625, Validation Loss: 14164.0478515625\n",
      "Epoch 5001/150000, Loss: 13719.609375, Validation Loss: 13964.96875\n",
      "Epoch 5101/150000, Loss: 13525.46875, Validation Loss: 13768.4296875\n",
      "Epoch 5201/150000, Loss: 13334.4384765625, Validation Loss: 13574.76953125\n",
      "Epoch 5301/150000, Loss: 13146.5107421875, Validation Loss: 13385.2958984375\n",
      "Epoch 5401/150000, Loss: 12962.1396484375, Validation Loss: 13201.171875\n",
      "Epoch 5501/150000, Loss: 12779.8955078125, Validation Loss: 13016.701171875\n",
      "Epoch 5601/150000, Loss: 12600.435546875, Validation Loss: 12834.9951171875\n",
      "Epoch 5701/150000, Loss: 12423.8125, Validation Loss: 12656.6650390625\n",
      "Epoch 5801/150000, Loss: 12249.830078125, Validation Loss: 12480.904296875\n",
      "Epoch 5901/150000, Loss: 12078.36328125, Validation Loss: 12307.662109375\n",
      "Epoch 6001/150000, Loss: 11909.3251953125, Validation Loss: 12136.90234375\n",
      "Epoch 6101/150000, Loss: 11742.6435546875, Validation Loss: 11968.5400390625\n",
      "Epoch 6201/150000, Loss: 11578.2880859375, Validation Loss: 11802.5302734375\n",
      "Epoch 6301/150000, Loss: 11416.205078125, Validation Loss: 11638.8408203125\n",
      "Epoch 6401/150000, Loss: 11256.41015625, Validation Loss: 11477.5546875\n",
      "Epoch 6501/150000, Loss: 11098.8408203125, Validation Loss: 11318.6142578125\n",
      "Epoch 6601/150000, Loss: 10943.47265625, Validation Loss: 11161.9521484375\n",
      "Epoch 6701/150000, Loss: 10790.2294921875, Validation Loss: 11007.5947265625\n",
      "Epoch 6801/150000, Loss: 10639.052734375, Validation Loss: 10855.3671875\n",
      "Epoch 6901/150000, Loss: 10489.8623046875, Validation Loss: 10705.19921875\n",
      "Epoch 7001/150000, Loss: 10342.6044921875, Validation Loss: 10557.064453125\n",
      "Epoch 7101/150000, Loss: 10197.2060546875, Validation Loss: 10410.6953125\n",
      "Epoch 7201/150000, Loss: 10053.5751953125, Validation Loss: 10265.9853515625\n",
      "Epoch 7301/150000, Loss: 9911.6572265625, Validation Loss: 10123.388671875\n",
      "Early stopping at epoch 7363 with validation loss 10050.7041015625.\n",
      "Test Loss: 9917.521484375\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=20\n",
      "Epoch 1/150000, Loss: 30052.681640625, Validation Loss: 30549.21484375\n",
      "Epoch 101/150000, Loss: 29224.8671875, Validation Loss: 29708.25390625\n",
      "Epoch 201/150000, Loss: 28587.333984375, Validation Loss: 29064.73828125\n",
      "Epoch 301/150000, Loss: 28029.42578125, Validation Loss: 28500.603515625\n",
      "Epoch 401/150000, Loss: 27508.7421875, Validation Loss: 27973.80078125\n",
      "Epoch 501/150000, Loss: 27014.806640625, Validation Loss: 27473.859375\n",
      "Epoch 601/150000, Loss: 26543.119140625, Validation Loss: 26996.265625\n",
      "Epoch 701/150000, Loss: 26091.19921875, Validation Loss: 26538.529296875\n",
      "Epoch 801/150000, Loss: 25657.44921875, Validation Loss: 26099.037109375\n",
      "Epoch 901/150000, Loss: 25240.71484375, Validation Loss: 25676.63671875\n",
      "Epoch 1001/150000, Loss: 24840.107421875, Validation Loss: 25270.431640625\n",
      "Epoch 1101/150000, Loss: 24454.896484375, Validation Loss: 24879.689453125\n",
      "Epoch 1201/150000, Loss: 24084.4609375, Validation Loss: 24503.78515625\n",
      "Epoch 1301/150000, Loss: 23728.263671875, Validation Loss: 24142.18359375\n",
      "Epoch 1401/150000, Loss: 23385.814453125, Validation Loss: 23794.38671875\n",
      "Epoch 1501/150000, Loss: 23056.673828125, Validation Loss: 23459.95703125\n",
      "Epoch 1601/150000, Loss: 22740.439453125, Validation Loss: 23138.494140625\n",
      "Epoch 1701/150000, Loss: 22436.728515625, Validation Loss: 22829.609375\n",
      "Epoch 1801/150000, Loss: 22145.18359375, Validation Loss: 22532.953125\n",
      "Epoch 1901/150000, Loss: 21842.08984375, Validation Loss: 22222.587890625\n",
      "Epoch 2001/150000, Loss: 21373.3359375, Validation Loss: 21782.68359375\n",
      "Epoch 2101/150000, Loss: 21044.607421875, Validation Loss: 21450.677734375\n",
      "Epoch 2201/150000, Loss: 20722.962890625, Validation Loss: 21125.513671875\n",
      "Epoch 2301/150000, Loss: 20407.84375, Validation Loss: 20806.30078125\n",
      "Epoch 2401/150000, Loss: 20098.837890625, Validation Loss: 20492.328125\n",
      "Epoch 2501/150000, Loss: 19795.5859375, Validation Loss: 20183.3671875\n",
      "Epoch 2601/150000, Loss: 19497.8515625, Validation Loss: 19879.712890625\n",
      "Epoch 2701/150000, Loss: 19205.525390625, Validation Loss: 19581.6171875\n",
      "Epoch 2801/150000, Loss: 18918.4375, Validation Loss: 19289.115234375\n",
      "Epoch 2901/150000, Loss: 18636.41015625, Validation Loss: 19001.861328125\n",
      "Epoch 3001/150000, Loss: 18359.255859375, Validation Loss: 18719.66015625\n",
      "Epoch 3101/150000, Loss: 18086.822265625, Validation Loss: 18442.458984375\n",
      "Epoch 3201/150000, Loss: 17818.978515625, Validation Loss: 18170.078125\n",
      "Epoch 3301/150000, Loss: 17555.650390625, Validation Loss: 17902.388671875\n",
      "Epoch 3401/150000, Loss: 17296.77734375, Validation Loss: 17639.21484375\n",
      "Epoch 3501/150000, Loss: 17042.306640625, Validation Loss: 17380.578125\n",
      "Epoch 3601/150000, Loss: 16792.185546875, Validation Loss: 17126.400390625\n",
      "Epoch 3701/150000, Loss: 16546.37109375, Validation Loss: 16876.62890625\n",
      "Epoch 3801/150000, Loss: 16304.82421875, Validation Loss: 16631.26171875\n",
      "Epoch 3901/150000, Loss: 16067.50390625, Validation Loss: 16390.177734375\n",
      "Epoch 4001/150000, Loss: 15834.330078125, Validation Loss: 16153.4052734375\n",
      "Epoch 4101/150000, Loss: 15605.2294921875, Validation Loss: 15920.9345703125\n",
      "Epoch 4201/150000, Loss: 15380.1240234375, Validation Loss: 15692.6357421875\n",
      "Epoch 4301/150000, Loss: 15158.94921875, Validation Loss: 15468.3818359375\n",
      "Epoch 4401/150000, Loss: 14941.6455078125, Validation Loss: 15248.0048828125\n",
      "Epoch 4501/150000, Loss: 14728.0302734375, Validation Loss: 15031.1689453125\n",
      "Epoch 4601/150000, Loss: 14517.8125, Validation Loss: 14817.9541015625\n",
      "Epoch 4701/150000, Loss: 14311.0517578125, Validation Loss: 14607.9951171875\n",
      "Epoch 4801/150000, Loss: 14107.673828125, Validation Loss: 14401.2900390625\n",
      "Epoch 4901/150000, Loss: 13907.609375, Validation Loss: 14197.798828125\n",
      "Epoch 5001/150000, Loss: 13710.763671875, Validation Loss: 13997.5927734375\n",
      "Epoch 5101/150000, Loss: 13517.0517578125, Validation Loss: 13800.6552734375\n",
      "Epoch 5201/150000, Loss: 13326.4033203125, Validation Loss: 13606.8330078125\n",
      "Epoch 5301/150000, Loss: 13138.7421875, Validation Loss: 13416.203125\n",
      "Epoch 5401/150000, Loss: 12953.9599609375, Validation Loss: 13228.6279296875\n",
      "Epoch 5501/150000, Loss: 12771.9501953125, Validation Loss: 13043.708984375\n",
      "Epoch 5601/150000, Loss: 12592.6220703125, Validation Loss: 12861.171875\n",
      "Epoch 5701/150000, Loss: 12415.888671875, Validation Loss: 12681.20703125\n",
      "Epoch 5801/150000, Loss: 12241.6787109375, Validation Loss: 12503.9501953125\n",
      "Epoch 5901/150000, Loss: 12069.947265625, Validation Loss: 12329.244140625\n",
      "Epoch 6001/150000, Loss: 11900.6767578125, Validation Loss: 12156.7861328125\n",
      "Epoch 6101/150000, Loss: 11733.8427734375, Validation Loss: 11986.9765625\n",
      "Epoch 6201/150000, Loss: 11569.404296875, Validation Loss: 11820.2197265625\n",
      "Epoch 6301/150000, Loss: 11407.33203125, Validation Loss: 11655.4931640625\n",
      "Epoch 6401/150000, Loss: 11247.515625, Validation Loss: 11493.021484375\n",
      "Epoch 6501/150000, Loss: 11089.9921875, Validation Loss: 11333.25\n",
      "Epoch 6601/150000, Loss: 10934.73828125, Validation Loss: 11176.015625\n",
      "Epoch 6701/150000, Loss: 10781.6318359375, Validation Loss: 11021.1669921875\n",
      "Epoch 6801/150000, Loss: 10630.5986328125, Validation Loss: 10868.5078125\n",
      "Epoch 6901/150000, Loss: 10481.591796875, Validation Loss: 10717.802734375\n",
      "Epoch 7001/150000, Loss: 10334.4990234375, Validation Loss: 10568.9951171875\n",
      "Epoch 7101/150000, Loss: 10189.248046875, Validation Loss: 10421.8330078125\n",
      "Epoch 7201/150000, Loss: 10045.8134765625, Validation Loss: 10276.78125\n",
      "Epoch 7301/150000, Loss: 9904.23046875, Validation Loss: 10133.65234375\n",
      "Epoch 7401/150000, Loss: 9764.48046875, Validation Loss: 9992.1494140625\n",
      "Epoch 7501/150000, Loss: 9626.5048828125, Validation Loss: 9852.3115234375\n",
      "Epoch 7601/150000, Loss: 9490.2275390625, Validation Loss: 9713.951171875\n",
      "Epoch 7701/150000, Loss: 9355.5751953125, Validation Loss: 9576.8603515625\n",
      "Epoch 7801/150000, Loss: 9222.4619140625, Validation Loss: 9440.9111328125\n",
      "Epoch 7901/150000, Loss: 9090.8330078125, Validation Loss: 9307.375\n",
      "Epoch 8001/150000, Loss: 8960.6357421875, Validation Loss: 9175.5166015625\n",
      "Epoch 8101/150000, Loss: 8831.8701171875, Validation Loss: 9045.1796875\n",
      "Epoch 8201/150000, Loss: 8704.486328125, Validation Loss: 8916.3115234375\n",
      "Epoch 8301/150000, Loss: 8578.4833984375, Validation Loss: 8788.8408203125\n",
      "Epoch 8401/150000, Loss: 8453.83203125, Validation Loss: 8662.673828125\n",
      "Epoch 8501/150000, Loss: 8330.490234375, Validation Loss: 8537.732421875\n",
      "Epoch 8601/150000, Loss: 8208.41015625, Validation Loss: 8414.0927734375\n",
      "Epoch 8701/150000, Loss: 8087.57861328125, Validation Loss: 8291.7353515625\n",
      "Epoch 8801/150000, Loss: 7967.9951171875, Validation Loss: 8170.6796875\n",
      "Epoch 8901/150000, Loss: 7849.65966796875, Validation Loss: 8050.91455078125\n",
      "Epoch 9001/150000, Loss: 7732.5302734375, Validation Loss: 7932.39599609375\n",
      "Epoch 9101/150000, Loss: 7616.61572265625, Validation Loss: 7815.103515625\n",
      "Epoch 9201/150000, Loss: 7501.88623046875, Validation Loss: 7699.05517578125\n",
      "Epoch 9301/150000, Loss: 7388.31787109375, Validation Loss: 7584.251953125\n",
      "Epoch 9401/150000, Loss: 7275.9033203125, Validation Loss: 7470.66943359375\n",
      "Epoch 9501/150000, Loss: 7164.7041015625, Validation Loss: 7358.2890625\n",
      "Epoch 9601/150000, Loss: 7054.50244140625, Validation Loss: 7247.18603515625\n",
      "Epoch 9701/150000, Loss: 6945.53271484375, Validation Loss: 7137.06591796875\n",
      "Epoch 9801/150000, Loss: 6837.6650390625, Validation Loss: 7028.18603515625\n",
      "Epoch 9901/150000, Loss: 6730.97216796875, Validation Loss: 6920.3935546875\n",
      "Epoch 10001/150000, Loss: 6625.4228515625, Validation Loss: 6813.74169921875\n",
      "Epoch 10101/150000, Loss: 6520.99560546875, Validation Loss: 6708.18603515625\n",
      "Epoch 10201/150000, Loss: 6417.708984375, Validation Loss: 6603.8349609375\n",
      "Epoch 10301/150000, Loss: 6315.53857421875, Validation Loss: 6500.64404296875\n",
      "Epoch 10401/150000, Loss: 6214.48291015625, Validation Loss: 6398.640625\n",
      "Epoch 10501/150000, Loss: 6114.5458984375, Validation Loss: 6297.775390625\n",
      "Epoch 10601/150000, Loss: 6015.75732421875, Validation Loss: 6198.154296875\n",
      "Epoch 10701/150000, Loss: 5918.11279296875, Validation Loss: 6099.70166015625\n",
      "Epoch 10801/150000, Loss: 5821.60595703125, Validation Loss: 6002.42626953125\n",
      "Epoch 10901/150000, Loss: 5726.25244140625, Validation Loss: 5906.3369140625\n",
      "Epoch 11001/150000, Loss: 5632.04052734375, Validation Loss: 5811.421875\n",
      "Epoch 11101/150000, Loss: 5538.9931640625, Validation Loss: 5717.70556640625\n",
      "Epoch 11201/150000, Loss: 5447.1181640625, Validation Loss: 5625.154296875\n",
      "Epoch 11301/150000, Loss: 5356.41064453125, Validation Loss: 5533.85546875\n",
      "Epoch 11401/150000, Loss: 5266.8740234375, Validation Loss: 5443.6953125\n",
      "Epoch 11501/150000, Loss: 5178.51220703125, Validation Loss: 5354.681640625\n",
      "Epoch 11601/150000, Loss: 5091.2998046875, Validation Loss: 5266.74267578125\n",
      "Epoch 11701/150000, Loss: 5004.99267578125, Validation Loss: 5177.97607421875\n",
      "Epoch 11801/150000, Loss: 4919.810546875, Validation Loss: 5091.7587890625\n",
      "Epoch 11901/150000, Loss: 4835.65576171875, Validation Loss: 5007.06494140625\n",
      "Epoch 12001/150000, Loss: 4752.53173828125, Validation Loss: 4923.58056640625\n",
      "Epoch 12101/150000, Loss: 4670.458984375, Validation Loss: 4841.060546875\n",
      "Epoch 12201/150000, Loss: 4589.392578125, Validation Loss: 4759.6875\n",
      "Epoch 12301/150000, Loss: 4509.32421875, Validation Loss: 4679.33056640625\n",
      "Epoch 12401/150000, Loss: 4430.26904296875, Validation Loss: 4600.09326171875\n",
      "Epoch 12501/150000, Loss: 4352.23095703125, Validation Loss: 4521.896484375\n",
      "Epoch 12601/150000, Loss: 4275.18701171875, Validation Loss: 4444.71923828125\n",
      "Epoch 12701/150000, Loss: 4198.4716796875, Validation Loss: 4368.05419921875\n",
      "Epoch 12801/150000, Loss: 4123.142578125, Validation Loss: 4291.2939453125\n",
      "Epoch 12901/150000, Loss: 4048.968017578125, Validation Loss: 4215.44873046875\n",
      "Epoch 13001/150000, Loss: 3975.712890625, Validation Loss: 4140.87109375\n",
      "Epoch 13101/150000, Loss: 3903.34423828125, Validation Loss: 4069.33154296875\n",
      "Epoch 13201/150000, Loss: 3831.95361328125, Validation Loss: 3998.44287109375\n",
      "Epoch 13301/150000, Loss: 3761.62451171875, Validation Loss: 3927.900390625\n",
      "Epoch 13401/150000, Loss: 3692.321533203125, Validation Loss: 3858.385986328125\n",
      "Epoch 13501/150000, Loss: 3624.015625, Validation Loss: 3789.86376953125\n",
      "Epoch 13601/150000, Loss: 3556.69384765625, Validation Loss: 3722.3369140625\n",
      "Epoch 13701/150000, Loss: 3490.360595703125, Validation Loss: 3655.852294921875\n",
      "Epoch 13801/150000, Loss: 3425.011962890625, Validation Loss: 3590.37255859375\n",
      "Epoch 13901/150000, Loss: 3360.63232421875, Validation Loss: 3525.824462890625\n",
      "Epoch 14001/150000, Loss: 3297.287353515625, Validation Loss: 3462.08544921875\n",
      "Epoch 14101/150000, Loss: 3234.804443359375, Validation Loss: 3399.692626953125\n",
      "Epoch 14201/150000, Loss: 3173.363525390625, Validation Loss: 3338.125732421875\n",
      "Epoch 14301/150000, Loss: 3112.84814453125, Validation Loss: 3277.5400390625\n",
      "Epoch 14401/150000, Loss: 3053.321533203125, Validation Loss: 3217.923095703125\n",
      "Epoch 14501/150000, Loss: 2994.6474609375, Validation Loss: 3159.34375\n",
      "Epoch 14601/150000, Loss: 2936.91845703125, Validation Loss: 3101.41455078125\n",
      "Epoch 14701/150000, Loss: 2879.954345703125, Validation Loss: 3044.619384765625\n",
      "Epoch 14801/150000, Loss: 2823.903564453125, Validation Loss: 2988.832275390625\n",
      "Epoch 14901/150000, Loss: 2768.128173828125, Validation Loss: 2932.33154296875\n",
      "Epoch 15001/150000, Loss: 2713.469482421875, Validation Loss: 2877.572509765625\n",
      "Epoch 15101/150000, Loss: 2659.714599609375, Validation Loss: 2823.882080078125\n",
      "Epoch 15201/150000, Loss: 2606.849609375, Validation Loss: 2771.109375\n",
      "Epoch 15301/150000, Loss: 2554.836181640625, Validation Loss: 2719.254638671875\n",
      "Epoch 15401/150000, Loss: 2503.6455078125, Validation Loss: 2668.273193359375\n",
      "Epoch 15501/150000, Loss: 2453.271728515625, Validation Loss: 2618.188720703125\n",
      "Epoch 15601/150000, Loss: 2403.653076171875, Validation Loss: 2568.930908203125\n",
      "Epoch 15701/150000, Loss: 2354.796875, Validation Loss: 2520.56689453125\n",
      "Epoch 15801/150000, Loss: 2306.928955078125, Validation Loss: 2472.72900390625\n",
      "Epoch 15901/150000, Loss: 2259.28466796875, Validation Loss: 2426.1552734375\n",
      "Epoch 16001/150000, Loss: 2212.555908203125, Validation Loss: 2379.92431640625\n",
      "Epoch 16101/150000, Loss: 2166.56005859375, Validation Loss: 2334.343505859375\n",
      "Epoch 16201/150000, Loss: 2121.265869140625, Validation Loss: 2289.498779296875\n",
      "Epoch 16301/150000, Loss: 2076.898681640625, Validation Loss: 2245.9853515625\n",
      "Epoch 16401/150000, Loss: 2032.673095703125, Validation Loss: 2201.798828125\n",
      "Epoch 16501/150000, Loss: 1989.4354248046875, Validation Loss: 2159.02001953125\n",
      "Epoch 16601/150000, Loss: 1947.2005615234375, Validation Loss: 2116.467041015625\n",
      "Epoch 16701/150000, Loss: 1905.0672607421875, Validation Loss: 2075.56787109375\n",
      "Epoch 16801/150000, Loss: 1863.861328125, Validation Loss: 2034.9742431640625\n",
      "Epoch 16901/150000, Loss: 1823.37109375, Validation Loss: 1995.00146484375\n",
      "Epoch 17001/150000, Loss: 1783.5848388671875, Validation Loss: 1955.751953125\n",
      "Epoch 17101/150000, Loss: 1744.4854736328125, Validation Loss: 1917.2354736328125\n",
      "Epoch 17201/150000, Loss: 1706.0806884765625, Validation Loss: 1879.3837890625\n",
      "Epoch 17301/150000, Loss: 1669.2010498046875, Validation Loss: 1843.91943359375\n",
      "Epoch 17401/150000, Loss: 1631.3427734375, Validation Loss: 1805.8116455078125\n",
      "Epoch 17501/150000, Loss: 1594.99951171875, Validation Loss: 1770.0804443359375\n",
      "Epoch 17601/150000, Loss: 1559.3118896484375, Validation Loss: 1735.115966796875\n",
      "Epoch 17701/150000, Loss: 1524.2962646484375, Validation Loss: 1700.785400390625\n",
      "Epoch 17801/150000, Loss: 1489.8990478515625, Validation Loss: 1667.1048583984375\n",
      "Epoch 17901/150000, Loss: 1456.0986328125, Validation Loss: 1634.0347900390625\n",
      "Epoch 18001/150000, Loss: 1422.9293212890625, Validation Loss: 1601.4617919921875\n",
      "Epoch 18101/150000, Loss: 1390.1177978515625, Validation Loss: 1569.921875\n",
      "Epoch 18201/150000, Loss: 1357.942626953125, Validation Loss: 1538.8348388671875\n",
      "Epoch 18301/150000, Loss: 1326.3026123046875, Validation Loss: 1508.3414306640625\n",
      "Epoch 18401/150000, Loss: 1295.1776123046875, Validation Loss: 1478.38525390625\n",
      "Epoch 18501/150000, Loss: 1264.6380615234375, Validation Loss: 1448.71044921875\n",
      "Epoch 18601/150000, Loss: 1234.5068359375, Validation Loss: 1420.1710205078125\n",
      "Epoch 18701/150000, Loss: 1204.9775390625, Validation Loss: 1391.8572998046875\n",
      "Epoch 18801/150000, Loss: 1176.0069580078125, Validation Loss: 1364.0244140625\n",
      "Epoch 18901/150000, Loss: 1147.591796875, Validation Loss: 1336.829345703125\n",
      "Epoch 19001/150000, Loss: 1119.7354736328125, Validation Loss: 1310.1453857421875\n",
      "Epoch 19101/150000, Loss: 1092.4383544921875, Validation Loss: 1283.972412109375\n",
      "Epoch 19201/150000, Loss: 1065.6954345703125, Validation Loss: 1258.4144287109375\n",
      "Epoch 19301/150000, Loss: 1039.50341796875, Validation Loss: 1233.1849365234375\n",
      "Epoch 19401/150000, Loss: 1013.8587646484375, Validation Loss: 1208.87841796875\n",
      "Epoch 19501/150000, Loss: 988.7526245117188, Validation Loss: 1185.0177001953125\n",
      "Epoch 19601/150000, Loss: 964.1812133789062, Validation Loss: 1161.9866943359375\n",
      "Epoch 19701/150000, Loss: 940.1461791992188, Validation Loss: 1140.2430419921875\n",
      "Epoch 19801/150000, Loss: 916.6492919921875, Validation Loss: 1120.968505859375\n",
      "Epoch 19901/150000, Loss: 893.6722412109375, Validation Loss: 1103.0279541015625\n",
      "Epoch 20001/150000, Loss: 871.110595703125, Validation Loss: 1085.3468017578125\n",
      "Epoch 20101/150000, Loss: 849.1117553710938, Validation Loss: 1052.9697265625\n",
      "Epoch 20201/150000, Loss: 827.0620727539062, Validation Loss: 1032.9722900390625\n",
      "Epoch 20301/150000, Loss: 805.5314331054688, Validation Loss: 1013.5887451171875\n",
      "Epoch 20401/150000, Loss: 784.4827880859375, Validation Loss: 994.5447387695312\n",
      "Epoch 20501/150000, Loss: 763.9072875976562, Validation Loss: 975.8836059570312\n",
      "Epoch 20601/150000, Loss: 743.7938232421875, Validation Loss: 957.6343383789062\n",
      "Epoch 20701/150000, Loss: 724.1289672851562, Validation Loss: 939.8115234375\n",
      "Epoch 20801/150000, Loss: 704.8956298828125, Validation Loss: 922.41552734375\n",
      "Epoch 20901/150000, Loss: 686.0748901367188, Validation Loss: 905.439453125\n",
      "Epoch 21001/150000, Loss: 667.6531372070312, Validation Loss: 888.8810424804688\n",
      "Epoch 21101/150000, Loss: 649.6202392578125, Validation Loss: 872.7452392578125\n",
      "Epoch 21201/150000, Loss: 631.9747314453125, Validation Loss: 857.0331420898438\n",
      "Epoch 21301/150000, Loss: 614.7142333984375, Validation Loss: 841.7290649414062\n",
      "Epoch 21401/150000, Loss: 597.8493041992188, Validation Loss: 826.8618774414062\n",
      "Epoch 21501/150000, Loss: 581.3887329101562, Validation Loss: 812.3019409179688\n",
      "Epoch 21601/150000, Loss: 565.3213500976562, Validation Loss: 798.3381958007812\n",
      "Epoch 21701/150000, Loss: 549.6751098632812, Validation Loss: 785.0348510742188\n",
      "Epoch 21801/150000, Loss: 534.385986328125, Validation Loss: 771.43212890625\n",
      "Epoch 21901/150000, Loss: 519.5057373046875, Validation Loss: 758.5997924804688\n",
      "Epoch 22001/150000, Loss: 505.0115661621094, Validation Loss: 746.15771484375\n",
      "Epoch 22101/150000, Loss: 490.8946533203125, Validation Loss: 734.1787109375\n",
      "Epoch 22201/150000, Loss: 477.15606689453125, Validation Loss: 722.5399169921875\n",
      "Epoch 22301/150000, Loss: 463.7753601074219, Validation Loss: 711.5215454101562\n",
      "Epoch 22401/150000, Loss: 451.032470703125, Validation Loss: 700.0632934570312\n",
      "Epoch 22501/150000, Loss: 438.0782775878906, Validation Loss: 690.2957763671875\n",
      "Epoch 22601/150000, Loss: 425.73040771484375, Validation Loss: 679.5780639648438\n",
      "Epoch 22701/150000, Loss: 413.6999206542969, Validation Loss: 668.5917358398438\n",
      "Epoch 22801/150000, Loss: 401.9671630859375, Validation Loss: 657.5208129882812\n",
      "Epoch 22901/150000, Loss: 390.50067138671875, Validation Loss: 646.4491577148438\n",
      "Epoch 23001/150000, Loss: 379.23681640625, Validation Loss: 635.322509765625\n",
      "Epoch 23101/150000, Loss: 368.1357116699219, Validation Loss: 624.05126953125\n",
      "Epoch 23201/150000, Loss: 357.2268371582031, Validation Loss: 613.272216796875\n",
      "Epoch 23301/150000, Loss: 346.6183166503906, Validation Loss: 603.3899536132812\n",
      "Epoch 23401/150000, Loss: 336.33087158203125, Validation Loss: 594.407958984375\n",
      "Epoch 23501/150000, Loss: 326.3304443359375, Validation Loss: 586.0751953125\n",
      "Epoch 23601/150000, Loss: 316.6187744140625, Validation Loss: 578.3643798828125\n",
      "Epoch 23701/150000, Loss: 307.1981201171875, Validation Loss: 571.0955810546875\n",
      "Epoch 23801/150000, Loss: 298.0892028808594, Validation Loss: 564.39990234375\n",
      "Epoch 23901/150000, Loss: 289.3406982421875, Validation Loss: 557.5761108398438\n",
      "Epoch 24001/150000, Loss: 280.8206481933594, Validation Loss: 551.9561157226562\n",
      "Epoch 24101/150000, Loss: 272.6568908691406, Validation Loss: 546.103759765625\n",
      "Epoch 24201/150000, Loss: 264.7965393066406, Validation Loss: 540.6669921875\n",
      "Epoch 24301/150000, Loss: 257.2381591796875, Validation Loss: 535.4237060546875\n",
      "Epoch 24401/150000, Loss: 249.9753875732422, Validation Loss: 530.35302734375\n",
      "Epoch 24501/150000, Loss: 242.99850463867188, Validation Loss: 525.5567016601562\n",
      "Epoch 24601/150000, Loss: 236.30197143554688, Validation Loss: 520.9721069335938\n",
      "Epoch 24701/150000, Loss: 229.87417602539062, Validation Loss: 516.7135009765625\n",
      "Epoch 24801/150000, Loss: 223.80569458007812, Validation Loss: 513.1173706054688\n",
      "Epoch 24901/150000, Loss: 217.78414916992188, Validation Loss: 508.9593505859375\n",
      "Epoch 25001/150000, Loss: 212.1046142578125, Validation Loss: 505.4400939941406\n",
      "Epoch 25101/150000, Loss: 206.64437866210938, Validation Loss: 502.08819580078125\n",
      "Epoch 25201/150000, Loss: 201.39767456054688, Validation Loss: 499.1485595703125\n",
      "Epoch 25301/150000, Loss: 196.3397674560547, Validation Loss: 496.33477783203125\n",
      "Epoch 25401/150000, Loss: 191.4301300048828, Validation Loss: 493.7718200683594\n",
      "Epoch 25501/150000, Loss: 186.60105895996094, Validation Loss: 490.627685546875\n",
      "Epoch 25601/150000, Loss: 181.9195098876953, Validation Loss: 486.929443359375\n",
      "Epoch 25701/150000, Loss: 177.33306884765625, Validation Loss: 482.79022216796875\n",
      "Epoch 25801/150000, Loss: 172.88638305664062, Validation Loss: 478.92742919921875\n",
      "Epoch 25901/150000, Loss: 168.57980346679688, Validation Loss: 475.9920654296875\n",
      "Epoch 26001/150000, Loss: 164.44293212890625, Validation Loss: 473.4969787597656\n",
      "Epoch 26101/150000, Loss: 160.43685913085938, Validation Loss: 471.37518310546875\n",
      "Epoch 26201/150000, Loss: 156.5984344482422, Validation Loss: 469.7734069824219\n",
      "Epoch 26301/150000, Loss: 152.90846252441406, Validation Loss: 467.9187316894531\n",
      "Epoch 26401/150000, Loss: 149.31314086914062, Validation Loss: 466.8494567871094\n",
      "Epoch 26501/150000, Loss: 145.89675903320312, Validation Loss: 465.9059753417969\n",
      "Epoch 26601/150000, Loss: 142.61753845214844, Validation Loss: 465.13104248046875\n",
      "Early stopping at epoch 26639 with validation loss 464.9963684082031.\n",
      "Test Loss: 431.3981018066406\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.005, window_size=25\n",
      "Epoch 1/150000, Loss: 30088.869140625, Validation Loss: 30680.01171875\n",
      "Epoch 101/150000, Loss: 29273.166015625, Validation Loss: 29850.3046875\n",
      "Epoch 201/150000, Loss: 28606.826171875, Validation Loss: 29176.349609375\n",
      "Epoch 301/150000, Loss: 28040.333984375, Validation Loss: 28601.955078125\n",
      "Epoch 401/150000, Loss: 27515.001953125, Validation Loss: 28068.9140625\n",
      "Epoch 501/150000, Loss: 27018.091796875, Validation Loss: 27564.46875\n",
      "Epoch 601/150000, Loss: 26544.37109375, Validation Loss: 27083.337890625\n",
      "Epoch 701/150000, Loss: 26091.00390625, Validation Loss: 26622.681640625\n",
      "Epoch 801/150000, Loss: 25656.201171875, Validation Loss: 26180.6953125\n",
      "Epoch 901/150000, Loss: 25238.70703125, Validation Loss: 25756.115234375\n",
      "Epoch 1001/150000, Loss: 24837.548828125, Validation Loss: 25347.962890625\n",
      "Epoch 1101/150000, Loss: 24451.953125, Validation Loss: 24955.458984375\n",
      "Epoch 1201/150000, Loss: 24081.26953125, Validation Loss: 24577.94140625\n",
      "Epoch 1301/150000, Loss: 23724.919921875, Validation Loss: 24214.845703125\n",
      "Epoch 1401/150000, Loss: 23382.404296875, Validation Loss: 23865.65625\n",
      "Epoch 1501/150000, Loss: 23053.26953125, Validation Loss: 23529.921875\n",
      "Epoch 1601/150000, Loss: 22737.091796875, Validation Loss: 23207.21875\n",
      "Epoch 1701/150000, Loss: 22433.484375, Validation Loss: 22897.16015625\n",
      "Epoch 1801/150000, Loss: 22142.083984375, Validation Loss: 22599.37890625\n",
      "Epoch 1901/150000, Loss: 21862.556640625, Validation Loss: 22313.54296875\n",
      "Epoch 2001/150000, Loss: 21594.578125, Validation Loss: 22039.3203125\n",
      "Epoch 2101/150000, Loss: 21337.8359375, Validation Loss: 21776.408203125\n",
      "Epoch 2201/150000, Loss: 20864.28125, Validation Loss: 21323.5\n",
      "Epoch 2301/150000, Loss: 20491.990234375, Validation Loss: 20961.5078125\n",
      "Epoch 2401/150000, Loss: 20172.48046875, Validation Loss: 20635.2109375\n",
      "Epoch 2501/150000, Loss: 19860.294921875, Validation Loss: 20317.05859375\n",
      "Epoch 2601/150000, Loss: 19554.861328125, Validation Loss: 20005.6953125\n",
      "Epoch 2701/150000, Loss: 19255.69921875, Validation Loss: 19700.2421875\n",
      "Epoch 2801/150000, Loss: 18962.47265625, Validation Loss: 19400.634765625\n",
      "Epoch 2901/150000, Loss: 18674.99609375, Validation Loss: 19106.943359375\n",
      "Epoch 3001/150000, Loss: 18393.03515625, Validation Loss: 18818.837890625\n",
      "Epoch 3101/150000, Loss: 18116.31640625, Validation Loss: 18535.93359375\n",
      "Epoch 3201/150000, Loss: 17844.623046875, Validation Loss: 18258.0\n",
      "Epoch 3301/150000, Loss: 17577.771484375, Validation Loss: 17984.919921875\n",
      "Epoch 3401/150000, Loss: 17315.603515625, Validation Loss: 17717.3046875\n",
      "Epoch 3501/150000, Loss: 17058.20703125, Validation Loss: 17454.962890625\n",
      "Epoch 3601/150000, Loss: 16805.548828125, Validation Loss: 17197.380859375\n",
      "Epoch 3701/150000, Loss: 16557.48046875, Validation Loss: 16944.447265625\n",
      "Epoch 3801/150000, Loss: 16313.9296875, Validation Loss: 16696.1015625\n",
      "Epoch 3901/150000, Loss: 16074.83984375, Validation Loss: 16452.220703125\n",
      "Epoch 4001/150000, Loss: 15840.125, Validation Loss: 16212.541015625\n",
      "Epoch 4101/150000, Loss: 15609.65234375, Validation Loss: 15977.20703125\n",
      "Epoch 4201/150000, Loss: 15383.3203125, Validation Loss: 15746.3857421875\n",
      "Epoch 4301/150000, Loss: 15161.064453125, Validation Loss: 15519.8359375\n",
      "Epoch 4401/150000, Loss: 14942.8310546875, Validation Loss: 15297.4619140625\n",
      "Epoch 4501/150000, Loss: 14728.5712890625, Validation Loss: 15079.267578125\n",
      "Epoch 4601/150000, Loss: 14518.228515625, Validation Loss: 14865.3642578125\n",
      "Epoch 4701/150000, Loss: 14311.7333984375, Validation Loss: 14655.486328125\n",
      "Epoch 4801/150000, Loss: 14108.0732421875, Validation Loss: 14447.404296875\n",
      "Epoch 4901/150000, Loss: 13907.134765625, Validation Loss: 14241.79296875\n",
      "Epoch 5001/150000, Loss: 13709.6474609375, Validation Loss: 14041.1640625\n",
      "Epoch 5101/150000, Loss: 13515.4453125, Validation Loss: 13843.48828125\n",
      "Epoch 5201/150000, Loss: 13324.380859375, Validation Loss: 13648.8935546875\n",
      "Epoch 5301/150000, Loss: 13136.2939453125, Validation Loss: 13457.314453125\n",
      "Epoch 5401/150000, Loss: 12951.0751953125, Validation Loss: 13268.6181640625\n",
      "Epoch 5501/150000, Loss: 12768.6865234375, Validation Loss: 13082.7431640625\n",
      "Epoch 5601/150000, Loss: 12589.04296875, Validation Loss: 12899.58203125\n",
      "Epoch 5701/150000, Loss: 12412.0546875, Validation Loss: 12718.8154296875\n",
      "Epoch 5801/150000, Loss: 12237.5830078125, Validation Loss: 12540.314453125\n",
      "Epoch 5901/150000, Loss: 12065.642578125, Validation Loss: 12364.359375\n",
      "Epoch 6001/150000, Loss: 11896.244140625, Validation Loss: 12191.291015625\n",
      "Epoch 6101/150000, Loss: 11729.298828125, Validation Loss: 12021.13671875\n",
      "Epoch 6201/150000, Loss: 11564.7919921875, Validation Loss: 11853.7099609375\n",
      "Epoch 6301/150000, Loss: 11402.611328125, Validation Loss: 11688.8740234375\n",
      "Epoch 6401/150000, Loss: 11242.7607421875, Validation Loss: 11526.1884765625\n",
      "Epoch 6501/150000, Loss: 11085.134765625, Validation Loss: 11365.7919921875\n",
      "Epoch 6601/150000, Loss: 10929.7470703125, Validation Loss: 11207.9677734375\n",
      "Epoch 6701/150000, Loss: 10776.55078125, Validation Loss: 11052.48046875\n",
      "Epoch 6801/150000, Loss: 10625.43359375, Validation Loss: 10899.318359375\n",
      "Epoch 6901/150000, Loss: 10476.4619140625, Validation Loss: 10748.3525390625\n",
      "Epoch 7001/150000, Loss: 10329.228515625, Validation Loss: 10599.3818359375\n",
      "Epoch 7101/150000, Loss: 10184.0185546875, Validation Loss: 10452.240234375\n",
      "Epoch 7201/150000, Loss: 10040.708984375, Validation Loss: 10306.8837890625\n",
      "Epoch 7301/150000, Loss: 9899.201171875, Validation Loss: 10162.9423828125\n",
      "Epoch 7401/150000, Loss: 9759.419921875, Validation Loss: 10020.6181640625\n",
      "Epoch 7501/150000, Loss: 9621.5146484375, Validation Loss: 9880.5703125\n",
      "Epoch 7601/150000, Loss: 9485.1630859375, Validation Loss: 9742.0517578125\n",
      "Epoch 7701/150000, Loss: 9350.5205078125, Validation Loss: 9604.6533203125\n",
      "Epoch 7801/150000, Loss: 9217.38671875, Validation Loss: 9468.7119140625\n",
      "Epoch 7901/150000, Loss: 9085.7353515625, Validation Loss: 9334.13671875\n",
      "Epoch 8001/150000, Loss: 8955.5224609375, Validation Loss: 9201.2880859375\n",
      "Epoch 8101/150000, Loss: 8826.6669921875, Validation Loss: 9069.88671875\n",
      "Epoch 8201/150000, Loss: 8699.248046875, Validation Loss: 8940.451171875\n",
      "Epoch 8301/150000, Loss: 8573.220703125, Validation Loss: 8812.4306640625\n",
      "Epoch 8401/150000, Loss: 8448.525390625, Validation Loss: 8685.388671875\n",
      "Epoch 8501/150000, Loss: 8325.212890625, Validation Loss: 8560.0703125\n",
      "Epoch 8601/150000, Loss: 8203.04296875, Validation Loss: 8435.5224609375\n",
      "Epoch 8701/150000, Loss: 8082.3359375, Validation Loss: 8313.1796875\n",
      "Epoch 8801/150000, Loss: 7962.63720703125, Validation Loss: 8191.26904296875\n",
      "Epoch 8901/150000, Loss: 7844.30615234375, Validation Loss: 8071.07080078125\n",
      "Epoch 9001/150000, Loss: 7727.21240234375, Validation Loss: 7952.1259765625\n",
      "Epoch 9101/150000, Loss: 7611.32861328125, Validation Loss: 7834.40576171875\n",
      "Epoch 9201/150000, Loss: 7496.607421875, Validation Loss: 7717.7939453125\n",
      "Epoch 9301/150000, Loss: 7383.04736328125, Validation Loss: 7602.2734375\n",
      "Epoch 9401/150000, Loss: 7270.646484375, Validation Loss: 7488.05224609375\n",
      "Epoch 9501/150000, Loss: 7159.3935546875, Validation Loss: 7375.150390625\n",
      "Epoch 9601/150000, Loss: 7049.29150390625, Validation Loss: 7263.59326171875\n",
      "Epoch 9701/150000, Loss: 6940.337890625, Validation Loss: 7153.2080078125\n",
      "Epoch 9801/150000, Loss: 6832.53857421875, Validation Loss: 7044.00927734375\n",
      "Epoch 9901/150000, Loss: 6725.890625, Validation Loss: 6935.99169921875\n",
      "Epoch 10001/150000, Loss: 6620.4033203125, Validation Loss: 6829.3154296875\n",
      "Epoch 10101/150000, Loss: 6516.01953125, Validation Loss: 6723.4697265625\n",
      "Epoch 10201/150000, Loss: 6412.77587890625, Validation Loss: 6618.91162109375\n",
      "Epoch 10301/150000, Loss: 6310.650390625, Validation Loss: 6515.60205078125\n",
      "Epoch 10401/150000, Loss: 6209.6220703125, Validation Loss: 6413.24169921875\n",
      "Epoch 10501/150000, Loss: 6109.7607421875, Validation Loss: 6312.46533203125\n",
      "Epoch 10601/150000, Loss: 6010.97021484375, Validation Loss: 6212.24755859375\n",
      "Epoch 10701/150000, Loss: 5913.3525390625, Validation Loss: 6113.49755859375\n",
      "Epoch 10801/150000, Loss: 5816.87451171875, Validation Loss: 6015.90185546875\n",
      "Epoch 10901/150000, Loss: 5721.54736328125, Validation Loss: 5919.28857421875\n",
      "Epoch 11001/150000, Loss: 5627.29833984375, Validation Loss: 5822.60595703125\n",
      "Epoch 11101/150000, Loss: 5533.9580078125, Validation Loss: 5726.51708984375\n",
      "Epoch 11201/150000, Loss: 5441.91943359375, Validation Loss: 5632.9462890625\n",
      "Epoch 11301/150000, Loss: 5351.1162109375, Validation Loss: 5540.8876953125\n",
      "Epoch 11401/150000, Loss: 5261.50146484375, Validation Loss: 5450.091796875\n",
      "Epoch 11501/150000, Loss: 5173.06884765625, Validation Loss: 5360.48046875\n",
      "Epoch 11601/150000, Loss: 5085.8115234375, Validation Loss: 5272.19970703125\n",
      "Epoch 11701/150000, Loss: 4999.63818359375, Validation Loss: 5185.181640625\n",
      "Epoch 11801/150000, Loss: 4914.56298828125, Validation Loss: 5099.375\n",
      "Epoch 11901/150000, Loss: 4830.51611328125, Validation Loss: 5014.55712890625\n",
      "Epoch 12001/150000, Loss: 4747.4189453125, Validation Loss: 4930.65380859375\n",
      "Epoch 12101/150000, Loss: 4665.29248046875, Validation Loss: 4847.93115234375\n",
      "Epoch 12201/150000, Loss: 4584.1171875, Validation Loss: 4766.1201171875\n",
      "Epoch 12301/150000, Loss: 4503.98486328125, Validation Loss: 4685.478515625\n",
      "Epoch 12401/150000, Loss: 4424.61083984375, Validation Loss: 4606.00830078125\n",
      "Epoch 12501/150000, Loss: 4346.45458984375, Validation Loss: 4527.28955078125\n",
      "Epoch 12601/150000, Loss: 4269.32861328125, Validation Loss: 4449.52978515625\n",
      "Epoch 12701/150000, Loss: 4193.212890625, Validation Loss: 4373.00830078125\n",
      "Epoch 12801/150000, Loss: 4118.1953125, Validation Loss: 4297.0419921875\n",
      "Epoch 12901/150000, Loss: 4044.0400390625, Validation Loss: 4222.9765625\n",
      "Epoch 13001/150000, Loss: 3970.937255859375, Validation Loss: 4149.55126953125\n",
      "Epoch 13101/150000, Loss: 3898.783203125, Validation Loss: 4077.161376953125\n",
      "Epoch 13201/150000, Loss: 3827.634521484375, Validation Loss: 4005.801025390625\n",
      "Epoch 13301/150000, Loss: 3757.50244140625, Validation Loss: 3935.1591796875\n",
      "Epoch 13401/150000, Loss: 3688.297607421875, Validation Loss: 3866.134521484375\n",
      "Epoch 13501/150000, Loss: 3620.100830078125, Validation Loss: 3797.845703125\n",
      "Epoch 13601/150000, Loss: 3552.928955078125, Validation Loss: 3730.74365234375\n",
      "Epoch 13701/150000, Loss: 3486.67041015625, Validation Loss: 3664.57666015625\n",
      "Epoch 13801/150000, Loss: 3421.387939453125, Validation Loss: 3599.2158203125\n",
      "Epoch 13901/150000, Loss: 3357.076904296875, Validation Loss: 3534.8154296875\n",
      "Epoch 14001/150000, Loss: 3293.732177734375, Validation Loss: 3471.170166015625\n",
      "Epoch 14101/150000, Loss: 3231.350341796875, Validation Loss: 3408.5390625\n",
      "Epoch 14201/150000, Loss: 3169.919921875, Validation Loss: 3346.85107421875\n",
      "Epoch 14301/150000, Loss: 3109.44189453125, Validation Loss: 3286.11865234375\n",
      "Epoch 14401/150000, Loss: 3049.88720703125, Validation Loss: 3226.4052734375\n",
      "Epoch 14501/150000, Loss: 2990.943603515625, Validation Loss: 3167.224853515625\n",
      "Epoch 14601/150000, Loss: 2932.996826171875, Validation Loss: 3108.873779296875\n",
      "Epoch 14701/150000, Loss: 2875.95703125, Validation Loss: 3051.563720703125\n",
      "Epoch 14801/150000, Loss: 2819.781982421875, Validation Loss: 2995.238525390625\n",
      "Epoch 14901/150000, Loss: 2764.508056640625, Validation Loss: 2940.23974609375\n",
      "Epoch 15001/150000, Loss: 2710.021240234375, Validation Loss: 2885.234375\n",
      "Epoch 15101/150000, Loss: 2656.440673828125, Validation Loss: 2831.56884765625\n",
      "Epoch 15201/150000, Loss: 2603.66650390625, Validation Loss: 2778.73486328125\n",
      "Epoch 15301/150000, Loss: 2551.735595703125, Validation Loss: 2726.791015625\n",
      "Epoch 15401/150000, Loss: 2500.5888671875, Validation Loss: 2675.711181640625\n",
      "Epoch 15501/150000, Loss: 2450.28173828125, Validation Loss: 2625.823486328125\n",
      "Epoch 15601/150000, Loss: 2400.674072265625, Validation Loss: 2576.236572265625\n",
      "Epoch 15701/150000, Loss: 2351.887451171875, Validation Loss: 2527.7734375\n",
      "Epoch 15801/150000, Loss: 2303.8046875, Validation Loss: 2480.03564453125\n",
      "Epoch 15901/150000, Loss: 2256.410400390625, Validation Loss: 2433.029541015625\n",
      "Epoch 16001/150000, Loss: 2209.714111328125, Validation Loss: 2386.652099609375\n",
      "Epoch 16101/150000, Loss: 2163.74169921875, Validation Loss: 2341.0166015625\n",
      "Epoch 16201/150000, Loss: 2118.46826171875, Validation Loss: 2296.04345703125\n",
      "Epoch 16301/150000, Loss: 2073.86474609375, Validation Loss: 2251.753662109375\n",
      "Epoch 16401/150000, Loss: 2029.9666748046875, Validation Loss: 2208.15234375\n",
      "Epoch 16501/150000, Loss: 1986.7857666015625, Validation Loss: 2165.5869140625\n",
      "Epoch 16601/150000, Loss: 1944.2376708984375, Validation Loss: 2123.09033203125\n",
      "Epoch 16701/150000, Loss: 1902.413330078125, Validation Loss: 2081.60986328125\n",
      "Epoch 16801/150000, Loss: 1861.2730712890625, Validation Loss: 2040.78955078125\n",
      "Epoch 16901/150000, Loss: 1820.8150634765625, Validation Loss: 2000.652587890625\n",
      "Epoch 17001/150000, Loss: 1781.063232421875, Validation Loss: 1961.247314453125\n",
      "Epoch 17101/150000, Loss: 1741.992919921875, Validation Loss: 1922.466552734375\n",
      "Epoch 17201/150000, Loss: 1703.6104736328125, Validation Loss: 1884.2769775390625\n",
      "Epoch 17301/150000, Loss: 1665.9014892578125, Validation Loss: 1847.0811767578125\n",
      "Epoch 17401/150000, Loss: 1628.8875732421875, Validation Loss: 1810.4654541015625\n",
      "Epoch 17501/150000, Loss: 1592.55224609375, Validation Loss: 1774.57421875\n",
      "Epoch 17601/150000, Loss: 1556.8819580078125, Validation Loss: 1739.3985595703125\n",
      "Epoch 17701/150000, Loss: 1521.8321533203125, Validation Loss: 1704.948486328125\n",
      "Epoch 17801/150000, Loss: 1487.3712158203125, Validation Loss: 1671.362060546875\n",
      "Epoch 17901/150000, Loss: 1453.462158203125, Validation Loss: 1638.197998046875\n",
      "Epoch 18001/150000, Loss: 1420.1470947265625, Validation Loss: 1605.7445068359375\n",
      "Epoch 18101/150000, Loss: 1387.374267578125, Validation Loss: 1573.94970703125\n",
      "Epoch 18201/150000, Loss: 1355.1370849609375, Validation Loss: 1542.7122802734375\n",
      "Epoch 18301/150000, Loss: 1323.4793701171875, Validation Loss: 1512.0352783203125\n",
      "Epoch 18401/150000, Loss: 1292.3916015625, Validation Loss: 1481.969482421875\n",
      "Epoch 18501/150000, Loss: 1261.8736572265625, Validation Loss: 1452.3709716796875\n",
      "Epoch 18601/150000, Loss: 1231.9310302734375, Validation Loss: 1423.4031982421875\n",
      "Epoch 18701/150000, Loss: 1202.5367431640625, Validation Loss: 1394.9901123046875\n",
      "Epoch 18801/150000, Loss: 1173.6925048828125, Validation Loss: 1367.1534423828125\n",
      "Epoch 18901/150000, Loss: 1145.410400390625, Validation Loss: 1339.7738037109375\n",
      "Epoch 19001/150000, Loss: 1117.6480712890625, Validation Loss: 1313.236328125\n",
      "Epoch 19101/150000, Loss: 1090.5399169921875, Validation Loss: 1286.853515625\n",
      "Epoch 19201/150000, Loss: 1063.7685546875, Validation Loss: 1261.692626953125\n",
      "Epoch 19301/150000, Loss: 1037.6181640625, Validation Loss: 1237.0272216796875\n",
      "Epoch 19401/150000, Loss: 1011.8325805664062, Validation Loss: 1213.5989990234375\n",
      "Epoch 19501/150000, Loss: 986.5927124023438, Validation Loss: 1189.6551513671875\n",
      "Epoch 19601/150000, Loss: 961.8856811523438, Validation Loss: 1166.2115478515625\n",
      "Epoch 19701/150000, Loss: 937.6996459960938, Validation Loss: 1143.332275390625\n",
      "Epoch 19801/150000, Loss: 914.0224609375, Validation Loss: 1121.0421142578125\n",
      "Epoch 19901/150000, Loss: 890.84326171875, Validation Loss: 1099.288330078125\n",
      "Epoch 20001/150000, Loss: 868.1543579101562, Validation Loss: 1078.14208984375\n",
      "Epoch 20101/150000, Loss: 845.9437255859375, Validation Loss: 1057.5478515625\n",
      "Epoch 20201/150000, Loss: 824.1976318359375, Validation Loss: 1037.399658203125\n",
      "Epoch 20301/150000, Loss: 802.9080200195312, Validation Loss: 1018.148193359375\n",
      "Epoch 20401/150000, Loss: 782.0615234375, Validation Loss: 998.9236450195312\n",
      "Epoch 20501/150000, Loss: 761.6390991210938, Validation Loss: 980.8856201171875\n",
      "Epoch 20601/150000, Loss: 741.6397705078125, Validation Loss: 963.2896118164062\n",
      "Epoch 20701/150000, Loss: 722.0574951171875, Validation Loss: 945.703857421875\n",
      "Epoch 20801/150000, Loss: 702.9012451171875, Validation Loss: 927.6962280273438\n",
      "Epoch 20901/150000, Loss: 684.1240844726562, Validation Loss: 910.2073364257812\n",
      "Epoch 21001/150000, Loss: 665.8133544921875, Validation Loss: 893.2002563476562\n",
      "Epoch 21101/150000, Loss: 647.844482421875, Validation Loss: 876.6958618164062\n",
      "Epoch 21201/150000, Loss: 630.3153076171875, Validation Loss: 860.7178955078125\n",
      "Epoch 21301/150000, Loss: 613.189208984375, Validation Loss: 845.1990966796875\n",
      "Epoch 21401/150000, Loss: 596.4591674804688, Validation Loss: 830.1976928710938\n",
      "Epoch 21501/150000, Loss: 580.128173828125, Validation Loss: 815.5040893554688\n",
      "Epoch 21601/150000, Loss: 564.171142578125, Validation Loss: 801.1079711914062\n",
      "Epoch 21701/150000, Loss: 548.5813598632812, Validation Loss: 786.712646484375\n",
      "Epoch 21801/150000, Loss: 533.2318115234375, Validation Loss: 771.164306640625\n",
      "Epoch 21901/150000, Loss: 517.9244384765625, Validation Loss: 756.3070678710938\n",
      "Epoch 22001/150000, Loss: 502.9682312011719, Validation Loss: 744.4907836914062\n",
      "Epoch 22101/150000, Loss: 488.3497009277344, Validation Loss: 733.8087158203125\n",
      "Epoch 22201/150000, Loss: 473.8196105957031, Validation Loss: 722.6287841796875\n",
      "Early stopping at epoch 22281 with validation loss 714.7830200195312.\n",
      "Test Loss: 695.7548217773438\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.005]\n",
    "window_sizes = [5, 10, 15, 20, 25]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach, \n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
