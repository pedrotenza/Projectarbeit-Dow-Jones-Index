{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    # Initialize lists to store training and temporary sets\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    # Iterate through the data with the specified window size\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i+split_window_size+1]\n",
    "        y_out1_out2 = y_in.iloc[i:i+split_window_size+1]\n",
    "\n",
    "        # Separate the last row for the temporary set\n",
    "        # [ :-1]: all elements except the last one\n",
    "        # [-1:]:  selects only the last element\n",
    "        # (:) is used to indicate slicing of a sequence\n",
    "        # sequence[start : end : step]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    # Concatenate the lists into pandas DataFrames\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "# Split Data to train and temp\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 3\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split temp into val and test\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 1\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob=0.5):  # Added dropout_prob\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)  # Included dropout in LSTM\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Added dropout layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=2, num_layers=1, learning_rate=0.005, window_size=15, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150000, Train Loss: 29946, Val Loss: 30347,  Lear. Rate: 0.00500, Train Grad.: 224.0\n",
      "Epoch 101/150000, Train Loss: 29482, Val Loss: 29879,  Lear. Rate: 0.00453, Train Grad.: 219.8\n",
      "Epoch 201/150000, Train Loss: 29135, Val Loss: 29530,  Lear. Rate: 0.00454, Train Grad.: 216.6\n",
      "Epoch 301/150000, Train Loss: 28822, Val Loss: 29214,  Lear. Rate: 0.00455, Train Grad.: 213.7\n",
      "Epoch 401/150000, Train Loss: 28523, Val Loss: 28912,  Lear. Rate: 0.00456, Train Grad.: 210.9\n",
      "Epoch 501/150000, Train Loss: 28233, Val Loss: 28620,  Lear. Rate: 0.00456, Train Grad.: 208.1\n",
      "Epoch 601/150000, Train Loss: 27950, Val Loss: 28334,  Lear. Rate: 0.00457, Train Grad.: 205.4\n",
      "Epoch 701/150000, Train Loss: 27674, Val Loss: 28056,  Lear. Rate: 0.00458, Train Grad.: 202.7\n",
      "Epoch 801/150000, Train Loss: 27404, Val Loss: 27783,  Lear. Rate: 0.00459, Train Grad.: 200.0\n",
      "Epoch 901/150000, Train Loss: 27139, Val Loss: 27515,  Lear. Rate: 0.00460, Train Grad.: 197.3\n",
      "Epoch 1001/150000, Train Loss: 26879, Val Loss: 27252,  Lear. Rate: 0.00460, Train Grad.: 194.7\n",
      "Epoch 1101/150000, Train Loss: 26623, Val Loss: 26995,  Lear. Rate: 0.00461, Train Grad.: 192.0\n",
      "Epoch 1201/150000, Train Loss: 26373, Val Loss: 26742,  Lear. Rate: 0.00462, Train Grad.: 189.4\n",
      "Epoch 1301/150000, Train Loss: 26127, Val Loss: 26493,  Lear. Rate: 0.00462, Train Grad.: 186.8\n",
      "Epoch 1401/150000, Train Loss: 25886, Val Loss: 26249,  Lear. Rate: 0.00463, Train Grad.: 184.2\n",
      "Epoch 1501/150000, Train Loss: 25649, Val Loss: 26010,  Lear. Rate: 0.00464, Train Grad.: 181.6\n",
      "Epoch 1601/150000, Train Loss: 25416, Val Loss: 25775,  Lear. Rate: 0.00464, Train Grad.: 179.0\n",
      "Epoch 1701/150000, Train Loss: 25187, Val Loss: 25544,  Lear. Rate: 0.00465, Train Grad.: 176.4\n",
      "Epoch 1801/150000, Train Loss: 24963, Val Loss: 25317,  Lear. Rate: 0.00466, Train Grad.: 173.9\n",
      "Epoch 1901/150000, Train Loss: 24743, Val Loss: 25094,  Lear. Rate: 0.00466, Train Grad.: 171.3\n",
      "Epoch 2001/150000, Train Loss: 24526, Val Loss: 24875,  Lear. Rate: 0.00467, Train Grad.: 168.8\n",
      "Epoch 2101/150000, Train Loss: 24314, Val Loss: 24660,  Lear. Rate: 0.00467, Train Grad.: 166.2\n",
      "Epoch 2201/150000, Train Loss: 24105, Val Loss: 24449,  Lear. Rate: 0.00468, Train Grad.: 163.7\n",
      "Epoch 2301/150000, Train Loss: 23901, Val Loss: 24242,  Lear. Rate: 0.00468, Train Grad.: 161.2\n",
      "Epoch 2401/150000, Train Loss: 23700, Val Loss: 24039,  Lear. Rate: 0.00469, Train Grad.: 158.7\n",
      "Epoch 2501/150000, Train Loss: 23503, Val Loss: 23840,  Lear. Rate: 0.00469, Train Grad.: 156.2\n",
      "Epoch 2601/150000, Train Loss: 23310, Val Loss: 23644,  Lear. Rate: 0.00470, Train Grad.: 153.7\n",
      "Epoch 2701/150000, Train Loss: 23120, Val Loss: 23452,  Lear. Rate: 0.00470, Train Grad.: 151.2\n",
      "Epoch 2801/150000, Train Loss: 22935, Val Loss: 23264,  Lear. Rate: 0.00471, Train Grad.: 148.7\n",
      "Epoch 2901/150000, Train Loss: 22752, Val Loss: 23079,  Lear. Rate: 0.00471, Train Grad.: 146.2\n",
      "Epoch 3001/150000, Train Loss: 22574, Val Loss: 22898,  Lear. Rate: 0.00472, Train Grad.: 143.8\n",
      "Epoch 3101/150000, Train Loss: 22282, Val Loss: 22620,  Lear. Rate: 0.00472, Train Grad.: 152.8\n",
      "Epoch 3201/150000, Train Loss: 22080, Val Loss: 22416,  Lear. Rate: 0.00473, Train Grad.: 151.4\n",
      "Epoch 3301/150000, Train Loss: 21881, Val Loss: 22215,  Lear. Rate: 0.00473, Train Grad.: 150.0\n",
      "Epoch 3401/150000, Train Loss: 21685, Val Loss: 22017,  Lear. Rate: 0.00474, Train Grad.: 148.5\n",
      "Epoch 3501/150000, Train Loss: 21490, Val Loss: 21821,  Lear. Rate: 0.00474, Train Grad.: 147.1\n",
      "Epoch 3601/150000, Train Loss: 21297, Val Loss: 21626,  Lear. Rate: 0.00475, Train Grad.: 145.7\n",
      "Epoch 3701/150000, Train Loss: 21107, Val Loss: 21434,  Lear. Rate: 0.00475, Train Grad.: 144.4\n",
      "Epoch 3801/150000, Train Loss: 20918, Val Loss: 21243,  Lear. Rate: 0.00476, Train Grad.: 143.0\n",
      "Epoch 3901/150000, Train Loss: 20731, Val Loss: 21054,  Lear. Rate: 0.00476, Train Grad.: 141.7\n",
      "Epoch 4001/150000, Train Loss: 20546, Val Loss: 20866,  Lear. Rate: 0.00476, Train Grad.: 140.4\n",
      "Epoch 4101/150000, Train Loss: 20363, Val Loss: 20681,  Lear. Rate: 0.00477, Train Grad.: 139.2\n",
      "Epoch 4201/150000, Train Loss: 20181, Val Loss: 20497,  Lear. Rate: 0.00477, Train Grad.: 137.9\n",
      "Epoch 4301/150000, Train Loss: 20001, Val Loss: 20314,  Lear. Rate: 0.00478, Train Grad.: 136.7\n",
      "Epoch 4401/150000, Train Loss: 19822, Val Loss: 20134,  Lear. Rate: 0.00478, Train Grad.: 135.4\n",
      "Epoch 4501/150000, Train Loss: 19645, Val Loss: 19955,  Lear. Rate: 0.00478, Train Grad.: 134.2\n",
      "Epoch 4601/150000, Train Loss: 19470, Val Loss: 19777,  Lear. Rate: 0.00479, Train Grad.: 133.0\n",
      "Epoch 4701/150000, Train Loss: 19296, Val Loss: 19601,  Lear. Rate: 0.00479, Train Grad.: 131.8\n",
      "Epoch 4801/150000, Train Loss: 19123, Val Loss: 19426,  Lear. Rate: 0.00480, Train Grad.: 130.7\n",
      "Epoch 4901/150000, Train Loss: 18952, Val Loss: 19253,  Lear. Rate: 0.00480, Train Grad.: 129.5\n",
      "Epoch 5001/150000, Train Loss: 18783, Val Loss: 19081,  Lear. Rate: 0.00480, Train Grad.: 128.4\n",
      "Epoch 5101/150000, Train Loss: 18615, Val Loss: 18910,  Lear. Rate: 0.00481, Train Grad.: 127.2\n",
      "Epoch 5201/150000, Train Loss: 18448, Val Loss: 18741,  Lear. Rate: 0.00481, Train Grad.: 126.1\n",
      "Epoch 5301/150000, Train Loss: 18283, Val Loss: 18574,  Lear. Rate: 0.00481, Train Grad.: 125.0\n",
      "Epoch 5401/150000, Train Loss: 18119, Val Loss: 18408,  Lear. Rate: 0.00482, Train Grad.: 123.9\n",
      "Epoch 5501/150000, Train Loss: 17957, Val Loss: 18243,  Lear. Rate: 0.00482, Train Grad.: 122.8\n",
      "Epoch 5601/150000, Train Loss: 17796, Val Loss: 18080,  Lear. Rate: 0.00482, Train Grad.: 121.7\n",
      "Epoch 5701/150000, Train Loss: 17636, Val Loss: 17918,  Lear. Rate: 0.00483, Train Grad.: 120.6\n",
      "Epoch 5801/150000, Train Loss: 17478, Val Loss: 17758,  Lear. Rate: 0.00483, Train Grad.: 119.5\n",
      "Epoch 5901/150000, Train Loss: 17321, Val Loss: 17599,  Lear. Rate: 0.00483, Train Grad.: 118.4\n",
      "Epoch 6001/150000, Train Loss: 17165, Val Loss: 17441,  Lear. Rate: 0.00484, Train Grad.: 117.3\n",
      "Epoch 6101/150000, Train Loss: 17011, Val Loss: 17285,  Lear. Rate: 0.00484, Train Grad.: 116.3\n",
      "Epoch 6201/150000, Train Loss: 16858, Val Loss: 17131,  Lear. Rate: 0.00484, Train Grad.: 115.2\n",
      "Epoch 6301/150000, Train Loss: 16707, Val Loss: 16977,  Lear. Rate: 0.00484, Train Grad.: 114.1\n",
      "Epoch 6401/150000, Train Loss: 16557, Val Loss: 16825,  Lear. Rate: 0.00485, Train Grad.: 113.1\n",
      "Epoch 6501/150000, Train Loss: 16409, Val Loss: 16675,  Lear. Rate: 0.00485, Train Grad.: 112.0\n",
      "Epoch 6601/150000, Train Loss: 16261, Val Loss: 16526,  Lear. Rate: 0.00485, Train Grad.: 110.9\n",
      "Epoch 6701/150000, Train Loss: 16116, Val Loss: 16378,  Lear. Rate: 0.00485, Train Grad.: 109.9\n",
      "Epoch 6801/150000, Train Loss: 15971, Val Loss: 16232,  Lear. Rate: 0.00486, Train Grad.: 108.9\n",
      "Epoch 6901/150000, Train Loss: 15828, Val Loss: 16087,  Lear. Rate: 0.00486, Train Grad.: 107.8\n",
      "Epoch 7001/150000, Train Loss: 15687, Val Loss: 15944,  Lear. Rate: 0.00486, Train Grad.: 106.8\n",
      "Epoch 7101/150000, Train Loss: 15547, Val Loss: 15802,  Lear. Rate: 0.00486, Train Grad.: 105.8\n",
      "Epoch 7201/150000, Train Loss: 15408, Val Loss: 15661,  Lear. Rate: 0.00487, Train Grad.: 104.7\n",
      "Epoch 7301/150000, Train Loss: 15270, Val Loss: 15522,  Lear. Rate: 0.00487, Train Grad.: 103.7\n",
      "Epoch 7401/150000, Train Loss: 15134, Val Loss: 15384,  Lear. Rate: 0.00487, Train Grad.: 102.7\n",
      "Epoch 7501/150000, Train Loss: 14999, Val Loss: 15248,  Lear. Rate: 0.00487, Train Grad.: 101.7\n",
      "Epoch 7601/150000, Train Loss: 14865, Val Loss: 15113,  Lear. Rate: 0.00488, Train Grad.: 100.7\n",
      "Epoch 7701/150000, Train Loss: 14733, Val Loss: 14978,  Lear. Rate: 0.00488, Train Grad.: 99.8\n",
      "Epoch 7801/150000, Train Loss: 14601, Val Loss: 14846,  Lear. Rate: 0.00488, Train Grad.: 98.9\n",
      "Epoch 7901/150000, Train Loss: 14471, Val Loss: 14715,  Lear. Rate: 0.00488, Train Grad.: 98.0\n",
      "Epoch 8001/150000, Train Loss: 14342, Val Loss: 14585,  Lear. Rate: 0.00488, Train Grad.: 97.0\n",
      "Epoch 8101/150000, Train Loss: 14215, Val Loss: 14456,  Lear. Rate: 0.00489, Train Grad.: 96.1\n",
      "Epoch 8201/150000, Train Loss: 14088, Val Loss: 14328,  Lear. Rate: 0.00489, Train Grad.: 95.2\n",
      "Epoch 8301/150000, Train Loss: 13963, Val Loss: 14201,  Lear. Rate: 0.00489, Train Grad.: 94.3\n",
      "Epoch 8401/150000, Train Loss: 13839, Val Loss: 14075,  Lear. Rate: 0.00489, Train Grad.: 93.4\n",
      "Epoch 8501/150000, Train Loss: 13716, Val Loss: 13951,  Lear. Rate: 0.00489, Train Grad.: 92.5\n",
      "Epoch 8601/150000, Train Loss: 13594, Val Loss: 13828,  Lear. Rate: 0.00490, Train Grad.: 91.6\n",
      "Epoch 8701/150000, Train Loss: 13473, Val Loss: 13706,  Lear. Rate: 0.00490, Train Grad.: 90.7\n",
      "Epoch 8801/150000, Train Loss: 13354, Val Loss: 13585,  Lear. Rate: 0.00490, Train Grad.: 89.8\n",
      "Epoch 8901/150000, Train Loss: 13235, Val Loss: 13465,  Lear. Rate: 0.00490, Train Grad.: 89.0\n",
      "Epoch 9001/150000, Train Loss: 13118, Val Loss: 13346,  Lear. Rate: 0.00490, Train Grad.: 88.2\n",
      "Epoch 9101/150000, Train Loss: 13002, Val Loss: 13229,  Lear. Rate: 0.00490, Train Grad.: 87.3\n",
      "Epoch 9201/150000, Train Loss: 12886, Val Loss: 13112,  Lear. Rate: 0.00491, Train Grad.: 86.5\n",
      "Epoch 9301/150000, Train Loss: 12772, Val Loss: 12996,  Lear. Rate: 0.00491, Train Grad.: 85.7\n",
      "Epoch 9401/150000, Train Loss: 12659, Val Loss: 12882,  Lear. Rate: 0.00491, Train Grad.: 84.9\n",
      "Epoch 9501/150000, Train Loss: 12547, Val Loss: 12768,  Lear. Rate: 0.00491, Train Grad.: 84.1\n",
      "Epoch 9601/150000, Train Loss: 12435, Val Loss: 12656,  Lear. Rate: 0.00491, Train Grad.: 83.3\n",
      "Epoch 9701/150000, Train Loss: 12325, Val Loss: 12544,  Lear. Rate: 0.00491, Train Grad.: 82.5\n",
      "Epoch 9801/150000, Train Loss: 12216, Val Loss: 12434,  Lear. Rate: 0.00492, Train Grad.: 81.8\n",
      "Epoch 9901/150000, Train Loss: 12108, Val Loss: 12325,  Lear. Rate: 0.00492, Train Grad.: 80.8\n",
      "Epoch 10001/150000, Train Loss: 12000, Val Loss: 12216,  Lear. Rate: 0.00492, Train Grad.: 80.3\n",
      "Epoch 10101/150000, Train Loss: 11894, Val Loss: 12109,  Lear. Rate: 0.00492, Train Grad.: 79.6\n",
      "Epoch 10201/150000, Train Loss: 11788, Val Loss: 12003,  Lear. Rate: 0.00492, Train Grad.: 78.9\n",
      "Epoch 10301/150000, Train Loss: 11683, Val Loss: 11897,  Lear. Rate: 0.00492, Train Grad.: 78.2\n",
      "Epoch 10401/150000, Train Loss: 11580, Val Loss: 11792,  Lear. Rate: 0.00492, Train Grad.: 77.4\n",
      "Epoch 10501/150000, Train Loss: 11477, Val Loss: 11688,  Lear. Rate: 0.00493, Train Grad.: 76.7\n",
      "Epoch 10601/150000, Train Loss: 11375, Val Loss: 11585,  Lear. Rate: 0.00493, Train Grad.: 75.8\n",
      "Epoch 10701/150000, Train Loss: 11274, Val Loss: 11483,  Lear. Rate: 0.00493, Train Grad.: 75.3\n",
      "Epoch 10801/150000, Train Loss: 11173, Val Loss: 11383,  Lear. Rate: 0.00493, Train Grad.: 74.6\n",
      "Epoch 10901/150000, Train Loss: 11074, Val Loss: 11282,  Lear. Rate: 0.00493, Train Grad.: 74.0\n",
      "Epoch 11001/150000, Train Loss: 10976, Val Loss: 11183,  Lear. Rate: 0.00493, Train Grad.: 73.2\n",
      "Epoch 11101/150000, Train Loss: 10878, Val Loss: 11084,  Lear. Rate: 0.00493, Train Grad.: 72.7\n",
      "Epoch 11201/150000, Train Loss: 10781, Val Loss: 10987,  Lear. Rate: 0.00493, Train Grad.: 72.0\n",
      "Epoch 11301/150000, Train Loss: 10685, Val Loss: 10889,  Lear. Rate: 0.00494, Train Grad.: 71.4\n",
      "Epoch 11401/150000, Train Loss: 10590, Val Loss: 10793,  Lear. Rate: 0.00494, Train Grad.: 70.7\n",
      "Epoch 11501/150000, Train Loss: 10495, Val Loss: 10698,  Lear. Rate: 0.00494, Train Grad.: 70.2\n",
      "Epoch 11601/150000, Train Loss: 10401, Val Loss: 10604,  Lear. Rate: 0.00494, Train Grad.: 69.6\n",
      "Epoch 11701/150000, Train Loss: 10308, Val Loss: 10510,  Lear. Rate: 0.00494, Train Grad.: 68.9\n",
      "Epoch 11801/150000, Train Loss: 10216, Val Loss: 10416,  Lear. Rate: 0.00494, Train Grad.: 68.4\n",
      "Epoch 11901/150000, Train Loss: 10125, Val Loss: 10324,  Lear. Rate: 0.00494, Train Grad.: 67.8\n",
      "Epoch 12001/150000, Train Loss: 10034, Val Loss: 10232,  Lear. Rate: 0.00494, Train Grad.: 67.1\n",
      "Epoch 12101/150000, Train Loss: 9944, Val Loss: 10141,  Lear. Rate: 0.00494, Train Grad.: 66.7\n",
      "Epoch 12201/150000, Train Loss: 9854, Val Loss: 10051,  Lear. Rate: 0.00494, Train Grad.: 66.1\n",
      "Epoch 12301/150000, Train Loss: 9766, Val Loss: 9962,  Lear. Rate: 0.00495, Train Grad.: 65.5\n",
      "Epoch 12401/150000, Train Loss: 9678, Val Loss: 9873,  Lear. Rate: 0.00495, Train Grad.: 65.0\n",
      "Epoch 12501/150000, Train Loss: 9591, Val Loss: 9785,  Lear. Rate: 0.00495, Train Grad.: 64.4\n",
      "Epoch 12601/150000, Train Loss: 9504, Val Loss: 9698,  Lear. Rate: 0.00495, Train Grad.: 63.7\n",
      "Epoch 12701/150000, Train Loss: 9419, Val Loss: 9611,  Lear. Rate: 0.00495, Train Grad.: 63.1\n",
      "Epoch 12801/150000, Train Loss: 9333, Val Loss: 9524,  Lear. Rate: 0.00495, Train Grad.: 62.8\n",
      "Epoch 12901/150000, Train Loss: 9249, Val Loss: 9439,  Lear. Rate: 0.00495, Train Grad.: 62.3\n",
      "Epoch 13001/150000, Train Loss: 9165, Val Loss: 9352,  Lear. Rate: 0.00495, Train Grad.: 61.8\n",
      "Epoch 13101/150000, Train Loss: 9082, Val Loss: 9268,  Lear. Rate: 0.00495, Train Grad.: 61.3\n",
      "Epoch 13201/150000, Train Loss: 8999, Val Loss: 9185,  Lear. Rate: 0.00495, Train Grad.: 60.9\n",
      "Epoch 13301/150000, Train Loss: 8917, Val Loss: 9102,  Lear. Rate: 0.00495, Train Grad.: 60.5\n",
      "Epoch 13401/150000, Train Loss: 8835, Val Loss: 9019,  Lear. Rate: 0.00496, Train Grad.: 59.9\n",
      "Epoch 13501/150000, Train Loss: 8754, Val Loss: 8938,  Lear. Rate: 0.00496, Train Grad.: 59.5\n",
      "Epoch 13601/150000, Train Loss: 8673, Val Loss: 8857,  Lear. Rate: 0.00496, Train Grad.: 59.0\n",
      "Epoch 13701/150000, Train Loss: 8593, Val Loss: 8776,  Lear. Rate: 0.00496, Train Grad.: 58.5\n",
      "Epoch 13801/150000, Train Loss: 8514, Val Loss: 8696,  Lear. Rate: 0.00496, Train Grad.: 58.1\n",
      "Epoch 13901/150000, Train Loss: 8435, Val Loss: 8616,  Lear. Rate: 0.00496, Train Grad.: 57.7\n",
      "Epoch 14001/150000, Train Loss: 8357, Val Loss: 8537,  Lear. Rate: 0.00496, Train Grad.: 57.2\n",
      "Epoch 14101/150000, Train Loss: 8279, Val Loss: 8459,  Lear. Rate: 0.00496, Train Grad.: 56.8\n",
      "Epoch 14201/150000, Train Loss: 8202, Val Loss: 8381,  Lear. Rate: 0.00496, Train Grad.: 56.3\n",
      "Epoch 14301/150000, Train Loss: 8125, Val Loss: 8304,  Lear. Rate: 0.00496, Train Grad.: 56.0\n",
      "Epoch 14401/150000, Train Loss: 8049, Val Loss: 8227,  Lear. Rate: 0.00496, Train Grad.: 55.5\n",
      "Epoch 14501/150000, Train Loss: 7973, Val Loss: 8150,  Lear. Rate: 0.00496, Train Grad.: 55.1\n",
      "Epoch 14601/150000, Train Loss: 7898, Val Loss: 8074,  Lear. Rate: 0.00496, Train Grad.: 54.6\n",
      "Epoch 14701/150000, Train Loss: 7823, Val Loss: 7999,  Lear. Rate: 0.00496, Train Grad.: 54.3\n",
      "Epoch 14801/150000, Train Loss: 7749, Val Loss: 7924,  Lear. Rate: 0.00497, Train Grad.: 53.9\n",
      "Epoch 14901/150000, Train Loss: 7675, Val Loss: 7850,  Lear. Rate: 0.00497, Train Grad.: 53.7\n",
      "Epoch 15001/150000, Train Loss: 7602, Val Loss: 7776,  Lear. Rate: 0.00497, Train Grad.: 53.3\n",
      "Epoch 15101/150000, Train Loss: 7529, Val Loss: 7703,  Lear. Rate: 0.00497, Train Grad.: 52.9\n",
      "Epoch 15201/150000, Train Loss: 7457, Val Loss: 7630,  Lear. Rate: 0.00497, Train Grad.: 52.4\n",
      "Epoch 15301/150000, Train Loss: 7385, Val Loss: 7558,  Lear. Rate: 0.00497, Train Grad.: 52.1\n",
      "Epoch 15401/150000, Train Loss: 7314, Val Loss: 7488,  Lear. Rate: 0.00497, Train Grad.: 51.7\n",
      "Epoch 15501/150000, Train Loss: 7243, Val Loss: 7418,  Lear. Rate: 0.00497, Train Grad.: 51.3\n",
      "Epoch 15601/150000, Train Loss: 7173, Val Loss: 7347,  Lear. Rate: 0.00497, Train Grad.: 50.9\n",
      "Epoch 15701/150000, Train Loss: 7103, Val Loss: 7276,  Lear. Rate: 0.00497, Train Grad.: 50.6\n",
      "Epoch 15801/150000, Train Loss: 7034, Val Loss: 7206,  Lear. Rate: 0.00497, Train Grad.: 50.4\n",
      "Epoch 15901/150000, Train Loss: 6965, Val Loss: 7137,  Lear. Rate: 0.00497, Train Grad.: 49.9\n",
      "Epoch 16001/150000, Train Loss: 6896, Val Loss: 7068,  Lear. Rate: 0.00497, Train Grad.: 49.4\n",
      "Epoch 16101/150000, Train Loss: 6828, Val Loss: 7000,  Lear. Rate: 0.00497, Train Grad.: 49.1\n",
      "Epoch 16201/150000, Train Loss: 6761, Val Loss: 6931,  Lear. Rate: 0.00497, Train Grad.: 48.8\n",
      "Epoch 16301/150000, Train Loss: 6693, Val Loss: 6864,  Lear. Rate: 0.00497, Train Grad.: 48.4\n",
      "Epoch 16401/150000, Train Loss: 6627, Val Loss: 6797,  Lear. Rate: 0.00497, Train Grad.: 48.0\n",
      "Epoch 16501/150000, Train Loss: 6561, Val Loss: 6730,  Lear. Rate: 0.00498, Train Grad.: 47.7\n",
      "Epoch 16601/150000, Train Loss: 6495, Val Loss: 6664,  Lear. Rate: 0.00498, Train Grad.: 47.3\n",
      "Epoch 16701/150000, Train Loss: 6430, Val Loss: 6598,  Lear. Rate: 0.00498, Train Grad.: 47.1\n",
      "Epoch 16801/150000, Train Loss: 6365, Val Loss: 6533,  Lear. Rate: 0.00498, Train Grad.: 46.8\n",
      "Epoch 16901/150000, Train Loss: 6301, Val Loss: 6467,  Lear. Rate: 0.00498, Train Grad.: 46.7\n",
      "Epoch 17001/150000, Train Loss: 6237, Val Loss: 6403,  Lear. Rate: 0.00498, Train Grad.: 45.9\n",
      "Epoch 17101/150000, Train Loss: 6173, Val Loss: 6339,  Lear. Rate: 0.00498, Train Grad.: 45.7\n",
      "Epoch 17201/150000, Train Loss: 6110, Val Loss: 6275,  Lear. Rate: 0.00498, Train Grad.: 45.3\n",
      "Epoch 17301/150000, Train Loss: 6048, Val Loss: 6212,  Lear. Rate: 0.00498, Train Grad.: 45.1\n",
      "Epoch 17401/150000, Train Loss: 5986, Val Loss: 6149,  Lear. Rate: 0.00498, Train Grad.: 44.7\n",
      "Epoch 17501/150000, Train Loss: 5924, Val Loss: 6087,  Lear. Rate: 0.00498, Train Grad.: 44.4\n",
      "Epoch 17601/150000, Train Loss: 5863, Val Loss: 6025,  Lear. Rate: 0.00498, Train Grad.: 44.1\n",
      "Epoch 17701/150000, Train Loss: 5802, Val Loss: 5964,  Lear. Rate: 0.00498, Train Grad.: 43.8\n",
      "Epoch 17801/150000, Train Loss: 5742, Val Loss: 5903,  Lear. Rate: 0.00498, Train Grad.: 43.5\n",
      "Epoch 17901/150000, Train Loss: 5682, Val Loss: 5843,  Lear. Rate: 0.00498, Train Grad.: 43.1\n",
      "Epoch 18001/150000, Train Loss: 5623, Val Loss: 5784,  Lear. Rate: 0.00498, Train Grad.: 42.9\n",
      "Epoch 18101/150000, Train Loss: 5564, Val Loss: 5725,  Lear. Rate: 0.00498, Train Grad.: 42.4\n",
      "Epoch 18201/150000, Train Loss: 5506, Val Loss: 5667,  Lear. Rate: 0.00498, Train Grad.: 41.9\n",
      "Epoch 18301/150000, Train Loss: 5448, Val Loss: 5609,  Lear. Rate: 0.00498, Train Grad.: 41.7\n",
      "Epoch 18401/150000, Train Loss: 5391, Val Loss: 5551,  Lear. Rate: 0.00498, Train Grad.: 41.5\n",
      "Epoch 18501/150000, Train Loss: 5334, Val Loss: 5494,  Lear. Rate: 0.00498, Train Grad.: 41.3\n",
      "Epoch 18601/150000, Train Loss: 5277, Val Loss: 5437,  Lear. Rate: 0.00498, Train Grad.: 40.8\n",
      "Epoch 18701/150000, Train Loss: 5221, Val Loss: 5381,  Lear. Rate: 0.00498, Train Grad.: 40.4\n",
      "Epoch 18801/150000, Train Loss: 5165, Val Loss: 5325,  Lear. Rate: 0.00498, Train Grad.: 40.1\n",
      "Epoch 18901/150000, Train Loss: 5110, Val Loss: 5270,  Lear. Rate: 0.00498, Train Grad.: 39.8\n",
      "Epoch 19001/150000, Train Loss: 5056, Val Loss: 5215,  Lear. Rate: 0.00499, Train Grad.: 39.4\n",
      "Epoch 19101/150000, Train Loss: 5001, Val Loss: 5161,  Lear. Rate: 0.00499, Train Grad.: 39.4\n",
      "Epoch 19201/150000, Train Loss: 4947, Val Loss: 5107,  Lear. Rate: 0.00499, Train Grad.: 39.0\n",
      "Epoch 19301/150000, Train Loss: 4894, Val Loss: 5054,  Lear. Rate: 0.00499, Train Grad.: 38.5\n",
      "Epoch 19401/150000, Train Loss: 4841, Val Loss: 5001,  Lear. Rate: 0.00499, Train Grad.: 38.3\n",
      "Epoch 19501/150000, Train Loss: 4788, Val Loss: 4948,  Lear. Rate: 0.00499, Train Grad.: 37.9\n",
      "Epoch 19601/150000, Train Loss: 4736, Val Loss: 4896,  Lear. Rate: 0.00499, Train Grad.: 37.9\n",
      "Epoch 19701/150000, Train Loss: 4684, Val Loss: 4845,  Lear. Rate: 0.00499, Train Grad.: 37.5\n",
      "Epoch 19801/150000, Train Loss: 4633, Val Loss: 4794,  Lear. Rate: 0.00499, Train Grad.: 37.2\n",
      "Epoch 19901/150000, Train Loss: 4581, Val Loss: 4743,  Lear. Rate: 0.00499, Train Grad.: 37.0\n",
      "Epoch 20001/150000, Train Loss: 4531, Val Loss: 4694,  Lear. Rate: 0.00499, Train Grad.: 36.8\n",
      "Epoch 20101/150000, Train Loss: 4480, Val Loss: 4644,  Lear. Rate: 0.00499, Train Grad.: 36.3\n",
      "Epoch 20201/150000, Train Loss: 4430, Val Loss: 4594,  Lear. Rate: 0.00499, Train Grad.: 36.2\n",
      "Epoch 20301/150000, Train Loss: 4381, Val Loss: 4545,  Lear. Rate: 0.00499, Train Grad.: 35.8\n",
      "Epoch 20401/150000, Train Loss: 4332, Val Loss: 4495,  Lear. Rate: 0.00499, Train Grad.: 35.4\n",
      "Epoch 20501/150000, Train Loss: 4283, Val Loss: 4447,  Lear. Rate: 0.00499, Train Grad.: 35.2\n",
      "Epoch 20601/150000, Train Loss: 4235, Val Loss: 4398,  Lear. Rate: 0.00499, Train Grad.: 35.0\n",
      "Epoch 20701/150000, Train Loss: 4187, Val Loss: 4350,  Lear. Rate: 0.00499, Train Grad.: 34.7\n",
      "Epoch 20801/150000, Train Loss: 4139, Val Loss: 4302,  Lear. Rate: 0.00499, Train Grad.: 34.2\n",
      "Epoch 20901/150000, Train Loss: 4092, Val Loss: 4255,  Lear. Rate: 0.00499, Train Grad.: 34.1\n",
      "Epoch 21001/150000, Train Loss: 4045, Val Loss: 4208,  Lear. Rate: 0.00499, Train Grad.: 33.5\n",
      "Epoch 21101/150000, Train Loss: 3999, Val Loss: 4157,  Lear. Rate: 0.00499, Train Grad.: 33.4\n",
      "Epoch 21201/150000, Train Loss: 3953, Val Loss: 4111,  Lear. Rate: 0.00499, Train Grad.: 33.3\n",
      "Epoch 21301/150000, Train Loss: 3908, Val Loss: 4066,  Lear. Rate: 0.00499, Train Grad.: 33.0\n",
      "Epoch 21401/150000, Train Loss: 3862, Val Loss: 4020,  Lear. Rate: 0.00499, Train Grad.: 32.7\n",
      "Epoch 21501/150000, Train Loss: 3817, Val Loss: 3975,  Lear. Rate: 0.00499, Train Grad.: 32.4\n",
      "Epoch 21601/150000, Train Loss: 3773, Val Loss: 3930,  Lear. Rate: 0.00499, Train Grad.: 32.2\n",
      "Epoch 21701/150000, Train Loss: 3729, Val Loss: 3886,  Lear. Rate: 0.00499, Train Grad.: 32.0\n",
      "Epoch 21801/150000, Train Loss: 3685, Val Loss: 3842,  Lear. Rate: 0.00499, Train Grad.: 31.6\n",
      "Epoch 21901/150000, Train Loss: 3642, Val Loss: 3798,  Lear. Rate: 0.00499, Train Grad.: 31.2\n",
      "Epoch 22001/150000, Train Loss: 3599, Val Loss: 3755,  Lear. Rate: 0.00499, Train Grad.: 31.1\n",
      "Epoch 22101/150000, Train Loss: 3557, Val Loss: 3713,  Lear. Rate: 0.00499, Train Grad.: 30.8\n",
      "Epoch 22201/150000, Train Loss: 3515, Val Loss: 3671,  Lear. Rate: 0.00499, Train Grad.: 30.6\n",
      "Epoch 22301/150000, Train Loss: 3473, Val Loss: 3629,  Lear. Rate: 0.00499, Train Grad.: 30.3\n",
      "Epoch 22401/150000, Train Loss: 3432, Val Loss: 3588,  Lear. Rate: 0.00499, Train Grad.: 30.0\n",
      "Epoch 22501/150000, Train Loss: 3391, Val Loss: 3548,  Lear. Rate: 0.00499, Train Grad.: 29.8\n",
      "Epoch 22601/150000, Train Loss: 3350, Val Loss: 3508,  Lear. Rate: 0.00499, Train Grad.: 29.5\n",
      "Epoch 22701/150000, Train Loss: 3310, Val Loss: 3468,  Lear. Rate: 0.00499, Train Grad.: 29.2\n",
      "Epoch 22801/150000, Train Loss: 3270, Val Loss: 3430,  Lear. Rate: 0.00499, Train Grad.: 29.0\n",
      "Epoch 22901/150000, Train Loss: 3231, Val Loss: 3392,  Lear. Rate: 0.00499, Train Grad.: 28.6\n",
      "Epoch 23001/150000, Train Loss: 3191, Val Loss: 3354,  Lear. Rate: 0.00499, Train Grad.: 28.5\n",
      "Epoch 23101/150000, Train Loss: 3153, Val Loss: 3316,  Lear. Rate: 0.00499, Train Grad.: 28.1\n",
      "Epoch 23201/150000, Train Loss: 3114, Val Loss: 3279,  Lear. Rate: 0.00499, Train Grad.: 28.0\n",
      "Epoch 23301/150000, Train Loss: 3077, Val Loss: 3236,  Lear. Rate: 0.00499, Train Grad.: 27.7\n",
      "Epoch 23401/150000, Train Loss: 3039, Val Loss: 3199,  Lear. Rate: 0.00499, Train Grad.: 27.5\n",
      "Epoch 23501/150000, Train Loss: 3001, Val Loss: 3162,  Lear. Rate: 0.00499, Train Grad.: 27.3\n",
      "Epoch 23601/150000, Train Loss: 2964, Val Loss: 3127,  Lear. Rate: 0.00499, Train Grad.: 27.0\n",
      "Epoch 23701/150000, Train Loss: 2928, Val Loss: 3091,  Lear. Rate: 0.00499, Train Grad.: 26.7\n",
      "Epoch 23801/150000, Train Loss: 2891, Val Loss: 3054,  Lear. Rate: 0.00499, Train Grad.: 26.5\n",
      "Epoch 23901/150000, Train Loss: 2856, Val Loss: 3018,  Lear. Rate: 0.00500, Train Grad.: 26.3\n",
      "Epoch 24001/150000, Train Loss: 2820, Val Loss: 2983,  Lear. Rate: 0.00500, Train Grad.: 26.7\n",
      "Epoch 24101/150000, Train Loss: 2785, Val Loss: 2948,  Lear. Rate: 0.00500, Train Grad.: 25.8\n",
      "Epoch 24201/150000, Train Loss: 2750, Val Loss: 2913,  Lear. Rate: 0.00500, Train Grad.: 25.5\n",
      "Epoch 24301/150000, Train Loss: 2715, Val Loss: 2879,  Lear. Rate: 0.00500, Train Grad.: 25.3\n",
      "Epoch 24401/150000, Train Loss: 2681, Val Loss: 2845,  Lear. Rate: 0.00500, Train Grad.: 25.1\n",
      "Epoch 24501/150000, Train Loss: 2647, Val Loss: 2812,  Lear. Rate: 0.00500, Train Grad.: 25.3\n",
      "Epoch 24601/150000, Train Loss: 2613, Val Loss: 2778,  Lear. Rate: 0.00500, Train Grad.: 24.6\n",
      "Epoch 24701/150000, Train Loss: 2580, Val Loss: 2746,  Lear. Rate: 0.00500, Train Grad.: 24.4\n",
      "Epoch 24801/150000, Train Loss: 2547, Val Loss: 2713,  Lear. Rate: 0.00500, Train Grad.: 24.2\n",
      "Epoch 24901/150000, Train Loss: 2514, Val Loss: 2681,  Lear. Rate: 0.00500, Train Grad.: 23.9\n",
      "Epoch 25001/150000, Train Loss: 2482, Val Loss: 2649,  Lear. Rate: 0.00500, Train Grad.: 24.0\n",
      "Epoch 25101/150000, Train Loss: 2450, Val Loss: 2613,  Lear. Rate: 0.00500, Train Grad.: 23.4\n",
      "Epoch 25201/150000, Train Loss: 2418, Val Loss: 2582,  Lear. Rate: 0.00500, Train Grad.: 23.2\n",
      "Epoch 25301/150000, Train Loss: 2387, Val Loss: 2551,  Lear. Rate: 0.00500, Train Grad.: 23.0\n",
      "Epoch 25401/150000, Train Loss: 2356, Val Loss: 2519,  Lear. Rate: 0.00500, Train Grad.: 22.8\n",
      "Epoch 25501/150000, Train Loss: 2325, Val Loss: 2488,  Lear. Rate: 0.00500, Train Grad.: 22.6\n",
      "Epoch 25601/150000, Train Loss: 2294, Val Loss: 2458,  Lear. Rate: 0.00500, Train Grad.: 22.4\n",
      "Epoch 25701/150000, Train Loss: 2264, Val Loss: 2428,  Lear. Rate: 0.00500, Train Grad.: 22.2\n",
      "Epoch 25801/150000, Train Loss: 2234, Val Loss: 2399,  Lear. Rate: 0.00500, Train Grad.: 22.0\n",
      "Epoch 25901/150000, Train Loss: 2205, Val Loss: 2370,  Lear. Rate: 0.00500, Train Grad.: 21.8\n",
      "Epoch 26001/150000, Train Loss: 2175, Val Loss: 2341,  Lear. Rate: 0.00500, Train Grad.: 21.6\n",
      "Epoch 26101/150000, Train Loss: 2146, Val Loss: 2312,  Lear. Rate: 0.00500, Train Grad.: 21.2\n",
      "Epoch 26201/150000, Train Loss: 2117, Val Loss: 2284,  Lear. Rate: 0.00500, Train Grad.: 21.2\n",
      "Epoch 26301/150000, Train Loss: 2089, Val Loss: 2256,  Lear. Rate: 0.00500, Train Grad.: 21.1\n",
      "Epoch 26401/150000, Train Loss: 2060, Val Loss: 2229,  Lear. Rate: 0.00500, Train Grad.: 20.8\n",
      "Epoch 26501/150000, Train Loss: 2032, Val Loss: 2201,  Lear. Rate: 0.00500, Train Grad.: 20.7\n",
      "Epoch 26601/150000, Train Loss: 2005, Val Loss: 2177,  Lear. Rate: 0.00500, Train Grad.: 20.4\n",
      "Epoch 26701/150000, Train Loss: 1977, Val Loss: 2153,  Lear. Rate: 0.00500, Train Grad.: 20.2\n",
      "Epoch 26801/150000, Train Loss: 1950, Val Loss: 2132,  Lear. Rate: 0.00500, Train Grad.: 20.0\n",
      "Epoch 26901/150000, Train Loss: 1923, Val Loss: 2122,  Lear. Rate: 0.00500, Train Grad.: 19.7\n",
      "Epoch 27001/150000, Train Loss: 1896, Val Loss: 2122,  Lear. Rate: 0.00500, Train Grad.: 19.6\n",
      "Early stopping at epoch 27100 with validation loss 2056.615478515625.\n",
      "Test Loss: 1995.321044921875\n",
      "Hyperparameters: input_size=7, hidden_size=2, num_layers=1, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 29964, Val Loss: 30459,  Lear. Rate: 0.00500, Train Grad.: 224.1\n",
      "Epoch 101/150000, Train Loss: 29500, Val Loss: 29990,  Lear. Rate: 0.00452, Train Grad.: 219.9\n",
      "Epoch 201/150000, Train Loss: 29153, Val Loss: 29640,  Lear. Rate: 0.00453, Train Grad.: 216.7\n",
      "Epoch 301/150000, Train Loss: 28840, Val Loss: 29323,  Lear. Rate: 0.00454, Train Grad.: 213.8\n",
      "Epoch 401/150000, Train Loss: 28541, Val Loss: 29021,  Lear. Rate: 0.00455, Train Grad.: 211.0\n",
      "Epoch 501/150000, Train Loss: 28251, Val Loss: 28728,  Lear. Rate: 0.00456, Train Grad.: 208.2\n",
      "Epoch 601/150000, Train Loss: 27969, Val Loss: 28442,  Lear. Rate: 0.00457, Train Grad.: 205.5\n",
      "Epoch 701/150000, Train Loss: 27693, Val Loss: 28162,  Lear. Rate: 0.00458, Train Grad.: 202.8\n",
      "Epoch 801/150000, Train Loss: 27422, Val Loss: 27888,  Lear. Rate: 0.00459, Train Grad.: 200.1\n",
      "Epoch 901/150000, Train Loss: 27157, Val Loss: 27620,  Lear. Rate: 0.00459, Train Grad.: 197.4\n",
      "Epoch 1001/150000, Train Loss: 26897, Val Loss: 27357,  Lear. Rate: 0.00460, Train Grad.: 194.8\n",
      "Epoch 1101/150000, Train Loss: 26642, Val Loss: 27098,  Lear. Rate: 0.00461, Train Grad.: 192.1\n",
      "Epoch 1201/150000, Train Loss: 26391, Val Loss: 26845,  Lear. Rate: 0.00462, Train Grad.: 189.5\n",
      "Epoch 1301/150000, Train Loss: 26145, Val Loss: 26596,  Lear. Rate: 0.00462, Train Grad.: 186.9\n",
      "Epoch 1401/150000, Train Loss: 25904, Val Loss: 26351,  Lear. Rate: 0.00463, Train Grad.: 184.3\n",
      "Epoch 1501/150000, Train Loss: 25667, Val Loss: 26111,  Lear. Rate: 0.00464, Train Grad.: 181.7\n",
      "Epoch 1601/150000, Train Loss: 25434, Val Loss: 25875,  Lear. Rate: 0.00464, Train Grad.: 179.1\n",
      "Epoch 1701/150000, Train Loss: 25206, Val Loss: 25643,  Lear. Rate: 0.00465, Train Grad.: 176.5\n",
      "Epoch 1801/150000, Train Loss: 24981, Val Loss: 25415,  Lear. Rate: 0.00465, Train Grad.: 174.0\n",
      "Epoch 1901/150000, Train Loss: 24761, Val Loss: 25192,  Lear. Rate: 0.00466, Train Grad.: 171.4\n",
      "Epoch 2001/150000, Train Loss: 24544, Val Loss: 24972,  Lear. Rate: 0.00467, Train Grad.: 168.9\n",
      "Epoch 2101/150000, Train Loss: 24332, Val Loss: 24756,  Lear. Rate: 0.00467, Train Grad.: 166.4\n",
      "Epoch 2201/150000, Train Loss: 24123, Val Loss: 24545,  Lear. Rate: 0.00468, Train Grad.: 163.8\n",
      "Epoch 2301/150000, Train Loss: 23919, Val Loss: 24337,  Lear. Rate: 0.00468, Train Grad.: 161.3\n",
      "Epoch 2401/150000, Train Loss: 23718, Val Loss: 24133,  Lear. Rate: 0.00469, Train Grad.: 158.8\n",
      "Epoch 2501/150000, Train Loss: 23521, Val Loss: 23933,  Lear. Rate: 0.00469, Train Grad.: 156.3\n",
      "Epoch 2601/150000, Train Loss: 23327, Val Loss: 23736,  Lear. Rate: 0.00470, Train Grad.: 153.8\n",
      "Epoch 2701/150000, Train Loss: 23138, Val Loss: 23544,  Lear. Rate: 0.00470, Train Grad.: 151.3\n",
      "Epoch 2801/150000, Train Loss: 22952, Val Loss: 23355,  Lear. Rate: 0.00471, Train Grad.: 148.8\n",
      "Epoch 2901/150000, Train Loss: 22769, Val Loss: 23169,  Lear. Rate: 0.00471, Train Grad.: 146.4\n",
      "Epoch 3001/150000, Train Loss: 22486, Val Loss: 22899,  Lear. Rate: 0.00472, Train Grad.: 154.3\n",
      "Epoch 3101/150000, Train Loss: 22284, Val Loss: 22695,  Lear. Rate: 0.00472, Train Grad.: 152.9\n",
      "Epoch 3201/150000, Train Loss: 22084, Val Loss: 22492,  Lear. Rate: 0.00473, Train Grad.: 151.4\n",
      "Epoch 3301/150000, Train Loss: 21886, Val Loss: 22292,  Lear. Rate: 0.00473, Train Grad.: 150.0\n",
      "Epoch 3401/150000, Train Loss: 21691, Val Loss: 22095,  Lear. Rate: 0.00474, Train Grad.: 148.5\n",
      "Epoch 3501/150000, Train Loss: 21497, Val Loss: 21898,  Lear. Rate: 0.00474, Train Grad.: 147.1\n",
      "Epoch 3601/150000, Train Loss: 21305, Val Loss: 21704,  Lear. Rate: 0.00475, Train Grad.: 145.8\n",
      "Epoch 3701/150000, Train Loss: 21116, Val Loss: 21511,  Lear. Rate: 0.00475, Train Grad.: 144.4\n",
      "Epoch 3801/150000, Train Loss: 20928, Val Loss: 21320,  Lear. Rate: 0.00475, Train Grad.: 143.1\n",
      "Epoch 3901/150000, Train Loss: 20741, Val Loss: 21131,  Lear. Rate: 0.00476, Train Grad.: 141.8\n",
      "Epoch 4001/150000, Train Loss: 20557, Val Loss: 20944,  Lear. Rate: 0.00476, Train Grad.: 140.5\n",
      "Epoch 4101/150000, Train Loss: 20374, Val Loss: 20758,  Lear. Rate: 0.00477, Train Grad.: 139.2\n",
      "Epoch 4201/150000, Train Loss: 20192, Val Loss: 20574,  Lear. Rate: 0.00477, Train Grad.: 138.0\n",
      "Epoch 4301/150000, Train Loss: 20013, Val Loss: 20391,  Lear. Rate: 0.00478, Train Grad.: 136.7\n",
      "Epoch 4401/150000, Train Loss: 19834, Val Loss: 20210,  Lear. Rate: 0.00478, Train Grad.: 135.5\n",
      "Epoch 4501/150000, Train Loss: 19658, Val Loss: 20030,  Lear. Rate: 0.00478, Train Grad.: 134.3\n",
      "Epoch 4601/150000, Train Loss: 19482, Val Loss: 19851,  Lear. Rate: 0.00479, Train Grad.: 133.1\n",
      "Epoch 4701/150000, Train Loss: 19309, Val Loss: 19675,  Lear. Rate: 0.00479, Train Grad.: 131.9\n",
      "Epoch 4801/150000, Train Loss: 19136, Val Loss: 19499,  Lear. Rate: 0.00479, Train Grad.: 130.8\n",
      "Epoch 4901/150000, Train Loss: 18965, Val Loss: 19326,  Lear. Rate: 0.00480, Train Grad.: 129.6\n",
      "Epoch 5001/150000, Train Loss: 18796, Val Loss: 19153,  Lear. Rate: 0.00480, Train Grad.: 128.5\n",
      "Epoch 5101/150000, Train Loss: 18628, Val Loss: 18982,  Lear. Rate: 0.00480, Train Grad.: 127.3\n",
      "Epoch 5201/150000, Train Loss: 18461, Val Loss: 18813,  Lear. Rate: 0.00481, Train Grad.: 126.2\n",
      "Epoch 5301/150000, Train Loss: 18296, Val Loss: 18645,  Lear. Rate: 0.00481, Train Grad.: 125.1\n",
      "Epoch 5401/150000, Train Loss: 18133, Val Loss: 18479,  Lear. Rate: 0.00482, Train Grad.: 124.0\n",
      "Epoch 5501/150000, Train Loss: 17970, Val Loss: 18314,  Lear. Rate: 0.00482, Train Grad.: 122.9\n",
      "Epoch 5601/150000, Train Loss: 17809, Val Loss: 18150,  Lear. Rate: 0.00482, Train Grad.: 121.8\n",
      "Epoch 5701/150000, Train Loss: 17650, Val Loss: 17989,  Lear. Rate: 0.00482, Train Grad.: 120.6\n",
      "Epoch 5801/150000, Train Loss: 17491, Val Loss: 17828,  Lear. Rate: 0.00483, Train Grad.: 119.6\n",
      "Epoch 5901/150000, Train Loss: 17335, Val Loss: 17669,  Lear. Rate: 0.00483, Train Grad.: 118.5\n",
      "Epoch 6001/150000, Train Loss: 17179, Val Loss: 17511,  Lear. Rate: 0.00483, Train Grad.: 117.4\n",
      "Epoch 6101/150000, Train Loss: 17025, Val Loss: 17355,  Lear. Rate: 0.00484, Train Grad.: 116.3\n",
      "Epoch 6201/150000, Train Loss: 16872, Val Loss: 17200,  Lear. Rate: 0.00484, Train Grad.: 115.3\n",
      "Epoch 6301/150000, Train Loss: 16721, Val Loss: 17046,  Lear. Rate: 0.00484, Train Grad.: 114.2\n",
      "Epoch 6401/150000, Train Loss: 16571, Val Loss: 16894,  Lear. Rate: 0.00485, Train Grad.: 113.1\n",
      "Epoch 6501/150000, Train Loss: 16423, Val Loss: 16743,  Lear. Rate: 0.00485, Train Grad.: 112.1\n",
      "Epoch 6601/150000, Train Loss: 16275, Val Loss: 16594,  Lear. Rate: 0.00485, Train Grad.: 111.0\n",
      "Epoch 6701/150000, Train Loss: 16130, Val Loss: 16446,  Lear. Rate: 0.00485, Train Grad.: 110.0\n",
      "Epoch 6801/150000, Train Loss: 15985, Val Loss: 16299,  Lear. Rate: 0.00486, Train Grad.: 108.9\n",
      "Epoch 6901/150000, Train Loss: 15842, Val Loss: 16154,  Lear. Rate: 0.00486, Train Grad.: 107.9\n",
      "Epoch 7001/150000, Train Loss: 15700, Val Loss: 16010,  Lear. Rate: 0.00486, Train Grad.: 106.9\n",
      "Epoch 7101/150000, Train Loss: 15560, Val Loss: 15867,  Lear. Rate: 0.00486, Train Grad.: 105.8\n",
      "Epoch 7201/150000, Train Loss: 15421, Val Loss: 15726,  Lear. Rate: 0.00487, Train Grad.: 104.8\n",
      "Epoch 7301/150000, Train Loss: 15283, Val Loss: 15586,  Lear. Rate: 0.00487, Train Grad.: 103.8\n",
      "Epoch 7401/150000, Train Loss: 15147, Val Loss: 15448,  Lear. Rate: 0.00487, Train Grad.: 102.8\n",
      "Epoch 7501/150000, Train Loss: 15012, Val Loss: 15311,  Lear. Rate: 0.00487, Train Grad.: 101.8\n",
      "Epoch 7601/150000, Train Loss: 14878, Val Loss: 15175,  Lear. Rate: 0.00487, Train Grad.: 100.8\n",
      "Epoch 7701/150000, Train Loss: 14746, Val Loss: 15040,  Lear. Rate: 0.00488, Train Grad.: 99.9\n",
      "Epoch 7801/150000, Train Loss: 14614, Val Loss: 14908,  Lear. Rate: 0.00488, Train Grad.: 99.0\n",
      "Epoch 7901/150000, Train Loss: 14484, Val Loss: 14776,  Lear. Rate: 0.00488, Train Grad.: 98.0\n",
      "Epoch 8001/150000, Train Loss: 14355, Val Loss: 14645,  Lear. Rate: 0.00488, Train Grad.: 97.1\n",
      "Epoch 8101/150000, Train Loss: 14227, Val Loss: 14515,  Lear. Rate: 0.00489, Train Grad.: 96.2\n",
      "Epoch 8201/150000, Train Loss: 14101, Val Loss: 14387,  Lear. Rate: 0.00489, Train Grad.: 95.3\n",
      "Epoch 8301/150000, Train Loss: 13975, Val Loss: 14260,  Lear. Rate: 0.00489, Train Grad.: 94.3\n",
      "Epoch 8401/150000, Train Loss: 13851, Val Loss: 14134,  Lear. Rate: 0.00489, Train Grad.: 93.4\n",
      "Epoch 8501/150000, Train Loss: 13728, Val Loss: 14009,  Lear. Rate: 0.00489, Train Grad.: 92.5\n",
      "Epoch 8601/150000, Train Loss: 13606, Val Loss: 13885,  Lear. Rate: 0.00489, Train Grad.: 91.7\n",
      "Epoch 8701/150000, Train Loss: 13485, Val Loss: 13763,  Lear. Rate: 0.00490, Train Grad.: 90.8\n",
      "Epoch 8801/150000, Train Loss: 13366, Val Loss: 13641,  Lear. Rate: 0.00490, Train Grad.: 89.9\n",
      "Epoch 8901/150000, Train Loss: 13247, Val Loss: 13521,  Lear. Rate: 0.00490, Train Grad.: 89.1\n",
      "Epoch 9001/150000, Train Loss: 13130, Val Loss: 13402,  Lear. Rate: 0.00490, Train Grad.: 88.2\n",
      "Epoch 9101/150000, Train Loss: 13013, Val Loss: 13284,  Lear. Rate: 0.00490, Train Grad.: 87.4\n",
      "Epoch 9201/150000, Train Loss: 12898, Val Loss: 13167,  Lear. Rate: 0.00491, Train Grad.: 86.6\n",
      "Early stopping at epoch 9211 with validation loss 13420.31640625.\n",
      "Test Loss: 13268.7529296875\n",
      "Hyperparameters: input_size=7, hidden_size=2, num_layers=1, learning_rate=0.005, window_size=25, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 29982, Val Loss: 30573,  Lear. Rate: 0.00500, Train Grad.: 224.2\n",
      "Epoch 101/150000, Train Loss: 29518, Val Loss: 30103,  Lear. Rate: 0.00452, Train Grad.: 220.0\n",
      "Epoch 201/150000, Train Loss: 29171, Val Loss: 29752,  Lear. Rate: 0.00453, Train Grad.: 216.8\n",
      "Epoch 301/150000, Train Loss: 28858, Val Loss: 29434,  Lear. Rate: 0.00454, Train Grad.: 213.9\n",
      "Epoch 401/150000, Train Loss: 28559, Val Loss: 29131,  Lear. Rate: 0.00455, Train Grad.: 211.1\n",
      "Epoch 501/150000, Train Loss: 28270, Val Loss: 28837,  Lear. Rate: 0.00456, Train Grad.: 208.3\n",
      "Epoch 601/150000, Train Loss: 27987, Val Loss: 28551,  Lear. Rate: 0.00457, Train Grad.: 205.6\n",
      "Epoch 701/150000, Train Loss: 27711, Val Loss: 28270,  Lear. Rate: 0.00457, Train Grad.: 202.9\n",
      "Epoch 801/150000, Train Loss: 27440, Val Loss: 27996,  Lear. Rate: 0.00458, Train Grad.: 200.2\n",
      "Epoch 901/150000, Train Loss: 27175, Val Loss: 27727,  Lear. Rate: 0.00459, Train Grad.: 197.5\n",
      "Epoch 1001/150000, Train Loss: 26915, Val Loss: 27462,  Lear. Rate: 0.00460, Train Grad.: 194.9\n",
      "Epoch 1101/150000, Train Loss: 26660, Val Loss: 27203,  Lear. Rate: 0.00460, Train Grad.: 192.3\n",
      "Epoch 1201/150000, Train Loss: 26410, Val Loss: 26949,  Lear. Rate: 0.00461, Train Grad.: 189.6\n",
      "Epoch 1301/150000, Train Loss: 26164, Val Loss: 26699,  Lear. Rate: 0.00462, Train Grad.: 187.0\n",
      "Epoch 1401/150000, Train Loss: 25922, Val Loss: 26453,  Lear. Rate: 0.00463, Train Grad.: 184.4\n",
      "Epoch 1501/150000, Train Loss: 25685, Val Loss: 26212,  Lear. Rate: 0.00463, Train Grad.: 181.8\n",
      "Epoch 1601/150000, Train Loss: 25452, Val Loss: 25975,  Lear. Rate: 0.00464, Train Grad.: 179.2\n",
      "Epoch 1701/150000, Train Loss: 25224, Val Loss: 25743,  Lear. Rate: 0.00465, Train Grad.: 176.7\n",
      "Epoch 1801/150000, Train Loss: 24999, Val Loss: 25514,  Lear. Rate: 0.00465, Train Grad.: 174.1\n",
      "Epoch 1901/150000, Train Loss: 24779, Val Loss: 25290,  Lear. Rate: 0.00466, Train Grad.: 171.6\n",
      "Epoch 2001/150000, Train Loss: 24562, Val Loss: 25069,  Lear. Rate: 0.00466, Train Grad.: 169.0\n",
      "Epoch 2101/150000, Train Loss: 24350, Val Loss: 24853,  Lear. Rate: 0.00467, Train Grad.: 166.5\n",
      "Epoch 2201/150000, Train Loss: 24141, Val Loss: 24641,  Lear. Rate: 0.00467, Train Grad.: 164.0\n",
      "Epoch 2301/150000, Train Loss: 23936, Val Loss: 24432,  Lear. Rate: 0.00468, Train Grad.: 161.4\n",
      "Epoch 2401/150000, Train Loss: 23735, Val Loss: 24227,  Lear. Rate: 0.00468, Train Grad.: 158.9\n",
      "Epoch 2501/150000, Train Loss: 23538, Val Loss: 24026,  Lear. Rate: 0.00469, Train Grad.: 156.4\n",
      "Epoch 2601/150000, Train Loss: 23345, Val Loss: 23829,  Lear. Rate: 0.00470, Train Grad.: 153.9\n",
      "Epoch 2701/150000, Train Loss: 23155, Val Loss: 23635,  Lear. Rate: 0.00470, Train Grad.: 151.5\n",
      "Epoch 2801/150000, Train Loss: 22969, Val Loss: 23445,  Lear. Rate: 0.00470, Train Grad.: 149.0\n",
      "Epoch 2901/150000, Train Loss: 22701, Val Loss: 23191,  Lear. Rate: 0.00471, Train Grad.: 156.3\n",
      "Epoch 3001/150000, Train Loss: 22497, Val Loss: 22984,  Lear. Rate: 0.00472, Train Grad.: 154.4\n",
      "Epoch 3101/150000, Train Loss: 22295, Val Loss: 22780,  Lear. Rate: 0.00472, Train Grad.: 152.8\n",
      "Epoch 3201/150000, Train Loss: 22096, Val Loss: 22578,  Lear. Rate: 0.00473, Train Grad.: 151.4\n",
      "Epoch 3301/150000, Train Loss: 21899, Val Loss: 22378,  Lear. Rate: 0.00473, Train Grad.: 149.9\n",
      "Epoch 3401/150000, Train Loss: 21704, Val Loss: 22180,  Lear. Rate: 0.00474, Train Grad.: 148.6\n",
      "Epoch 3501/150000, Train Loss: 21511, Val Loss: 21984,  Lear. Rate: 0.00474, Train Grad.: 147.2\n",
      "Epoch 3601/150000, Train Loss: 21319, Val Loss: 21789,  Lear. Rate: 0.00474, Train Grad.: 145.8\n",
      "Epoch 3701/150000, Train Loss: 21130, Val Loss: 21597,  Lear. Rate: 0.00475, Train Grad.: 144.5\n",
      "Epoch 3801/150000, Train Loss: 20942, Val Loss: 21406,  Lear. Rate: 0.00475, Train Grad.: 143.1\n",
      "Epoch 3901/150000, Train Loss: 20756, Val Loss: 21216,  Lear. Rate: 0.00476, Train Grad.: 141.8\n",
      "Epoch 4001/150000, Train Loss: 20571, Val Loss: 21027,  Lear. Rate: 0.00476, Train Grad.: 140.6\n",
      "Epoch 4101/150000, Train Loss: 20388, Val Loss: 20840,  Lear. Rate: 0.00477, Train Grad.: 139.3\n",
      "Epoch 4201/150000, Train Loss: 20207, Val Loss: 20655,  Lear. Rate: 0.00477, Train Grad.: 138.1\n",
      "Epoch 4301/150000, Train Loss: 20027, Val Loss: 20471,  Lear. Rate: 0.00477, Train Grad.: 136.8\n",
      "Epoch 4401/150000, Train Loss: 19849, Val Loss: 20290,  Lear. Rate: 0.00478, Train Grad.: 135.6\n",
      "Epoch 4501/150000, Train Loss: 19673, Val Loss: 20109,  Lear. Rate: 0.00478, Train Grad.: 134.4\n",
      "Epoch 4601/150000, Train Loss: 19497, Val Loss: 19930,  Lear. Rate: 0.00479, Train Grad.: 133.2\n",
      "Epoch 4701/150000, Train Loss: 19324, Val Loss: 19753,  Lear. Rate: 0.00479, Train Grad.: 132.0\n",
      "Epoch 4801/150000, Train Loss: 19151, Val Loss: 19577,  Lear. Rate: 0.00479, Train Grad.: 130.9\n",
      "Epoch 4901/150000, Train Loss: 18980, Val Loss: 19402,  Lear. Rate: 0.00480, Train Grad.: 129.7\n",
      "Epoch 5001/150000, Train Loss: 18811, Val Loss: 19229,  Lear. Rate: 0.00480, Train Grad.: 128.6\n",
      "Epoch 5101/150000, Train Loss: 18643, Val Loss: 19058,  Lear. Rate: 0.00480, Train Grad.: 127.4\n",
      "Epoch 5201/150000, Train Loss: 18476, Val Loss: 18888,  Lear. Rate: 0.00481, Train Grad.: 126.3\n",
      "Epoch 5301/150000, Train Loss: 18311, Val Loss: 18719,  Lear. Rate: 0.00481, Train Grad.: 125.2\n",
      "Epoch 5401/150000, Train Loss: 18147, Val Loss: 18551,  Lear. Rate: 0.00481, Train Grad.: 124.1\n",
      "Epoch 5501/150000, Train Loss: 17985, Val Loss: 18385,  Lear. Rate: 0.00482, Train Grad.: 123.0\n",
      "Epoch 5601/150000, Train Loss: 17824, Val Loss: 18221,  Lear. Rate: 0.00482, Train Grad.: 121.9\n",
      "Epoch 5701/150000, Train Loss: 17664, Val Loss: 18059,  Lear. Rate: 0.00482, Train Grad.: 120.8\n",
      "Epoch 5801/150000, Train Loss: 17506, Val Loss: 17897,  Lear. Rate: 0.00483, Train Grad.: 119.7\n",
      "Epoch 5901/150000, Train Loss: 17349, Val Loss: 17737,  Lear. Rate: 0.00483, Train Grad.: 118.6\n",
      "Epoch 6001/150000, Train Loss: 17193, Val Loss: 17579,  Lear. Rate: 0.00483, Train Grad.: 117.5\n",
      "Epoch 6101/150000, Train Loss: 17039, Val Loss: 17422,  Lear. Rate: 0.00484, Train Grad.: 116.5\n",
      "Epoch 6201/150000, Train Loss: 16886, Val Loss: 17266,  Lear. Rate: 0.00484, Train Grad.: 115.4\n",
      "Epoch 6301/150000, Train Loss: 16735, Val Loss: 17112,  Lear. Rate: 0.00484, Train Grad.: 114.3\n",
      "Epoch 6401/150000, Train Loss: 16585, Val Loss: 16960,  Lear. Rate: 0.00484, Train Grad.: 113.3\n",
      "Epoch 6501/150000, Train Loss: 16436, Val Loss: 16808,  Lear. Rate: 0.00485, Train Grad.: 112.2\n",
      "Epoch 6601/150000, Train Loss: 16289, Val Loss: 16658,  Lear. Rate: 0.00485, Train Grad.: 111.1\n",
      "Epoch 6701/150000, Train Loss: 16143, Val Loss: 16510,  Lear. Rate: 0.00485, Train Grad.: 110.1\n",
      "Epoch 6801/150000, Train Loss: 15999, Val Loss: 16363,  Lear. Rate: 0.00485, Train Grad.: 109.0\n",
      "Epoch 6901/150000, Train Loss: 15856, Val Loss: 16217,  Lear. Rate: 0.00486, Train Grad.: 108.0\n",
      "Epoch 7001/150000, Train Loss: 15714, Val Loss: 16073,  Lear. Rate: 0.00486, Train Grad.: 107.0\n",
      "Epoch 7101/150000, Train Loss: 15573, Val Loss: 15930,  Lear. Rate: 0.00486, Train Grad.: 105.9\n",
      "Epoch 7201/150000, Train Loss: 15434, Val Loss: 15789,  Lear. Rate: 0.00486, Train Grad.: 104.9\n",
      "Epoch 7301/150000, Train Loss: 15297, Val Loss: 15648,  Lear. Rate: 0.00487, Train Grad.: 103.9\n",
      "Epoch 7401/150000, Train Loss: 15160, Val Loss: 15510,  Lear. Rate: 0.00487, Train Grad.: 102.9\n",
      "Epoch 7501/150000, Train Loss: 15025, Val Loss: 15372,  Lear. Rate: 0.00487, Train Grad.: 101.9\n",
      "Epoch 7601/150000, Train Loss: 14891, Val Loss: 15235,  Lear. Rate: 0.00487, Train Grad.: 100.9\n",
      "Epoch 7701/150000, Train Loss: 14759, Val Loss: 15099,  Lear. Rate: 0.00488, Train Grad.: 100.0\n",
      "Epoch 7801/150000, Train Loss: 14627, Val Loss: 14965,  Lear. Rate: 0.00488, Train Grad.: 99.1\n",
      "Epoch 7901/150000, Train Loss: 14497, Val Loss: 14833,  Lear. Rate: 0.00488, Train Grad.: 98.2\n",
      "Epoch 8001/150000, Train Loss: 14368, Val Loss: 14701,  Lear. Rate: 0.00488, Train Grad.: 97.2\n",
      "Epoch 8101/150000, Train Loss: 14240, Val Loss: 14571,  Lear. Rate: 0.00488, Train Grad.: 96.3\n",
      "Epoch 8201/150000, Train Loss: 14113, Val Loss: 14442,  Lear. Rate: 0.00489, Train Grad.: 95.3\n",
      "Epoch 8301/150000, Train Loss: 13988, Val Loss: 14315,  Lear. Rate: 0.00489, Train Grad.: 94.4\n",
      "Epoch 8401/150000, Train Loss: 13864, Val Loss: 14188,  Lear. Rate: 0.00489, Train Grad.: 93.5\n",
      "Epoch 8501/150000, Train Loss: 13741, Val Loss: 14063,  Lear. Rate: 0.00489, Train Grad.: 92.6\n",
      "Epoch 8601/150000, Train Loss: 13619, Val Loss: 13939,  Lear. Rate: 0.00489, Train Grad.: 91.7\n",
      "Epoch 8701/150000, Train Loss: 13498, Val Loss: 13816,  Lear. Rate: 0.00490, Train Grad.: 90.9\n",
      "Epoch 8801/150000, Train Loss: 13378, Val Loss: 13694,  Lear. Rate: 0.00490, Train Grad.: 90.1\n",
      "Epoch 8901/150000, Train Loss: 13259, Val Loss: 13573,  Lear. Rate: 0.00490, Train Grad.: 89.2\n",
      "Epoch 9001/150000, Train Loss: 13142, Val Loss: 13454,  Lear. Rate: 0.00490, Train Grad.: 88.3\n",
      "Epoch 9101/150000, Train Loss: 13025, Val Loss: 13335,  Lear. Rate: 0.00490, Train Grad.: 87.5\n",
      "Epoch 9201/150000, Train Loss: 12910, Val Loss: 13218,  Lear. Rate: 0.00490, Train Grad.: 86.7\n",
      "Epoch 9301/150000, Train Loss: 12795, Val Loss: 13101,  Lear. Rate: 0.00491, Train Grad.: 85.9\n",
      "Epoch 9401/150000, Train Loss: 12682, Val Loss: 12986,  Lear. Rate: 0.00491, Train Grad.: 85.0\n",
      "Epoch 9501/150000, Train Loss: 12569, Val Loss: 12872,  Lear. Rate: 0.00491, Train Grad.: 84.2\n",
      "Epoch 9601/150000, Train Loss: 12458, Val Loss: 12758,  Lear. Rate: 0.00491, Train Grad.: 83.5\n",
      "Epoch 9701/150000, Train Loss: 12348, Val Loss: 12646,  Lear. Rate: 0.00491, Train Grad.: 82.7\n",
      "Epoch 9801/150000, Train Loss: 12238, Val Loss: 12535,  Lear. Rate: 0.00491, Train Grad.: 81.9\n",
      "Epoch 9901/150000, Train Loss: 12130, Val Loss: 12425,  Lear. Rate: 0.00492, Train Grad.: 81.3\n",
      "Epoch 10001/150000, Train Loss: 12022, Val Loss: 12316,  Lear. Rate: 0.00492, Train Grad.: 80.5\n",
      "Epoch 10101/150000, Train Loss: 11915, Val Loss: 12207,  Lear. Rate: 0.00492, Train Grad.: 79.7\n",
      "Epoch 10201/150000, Train Loss: 11809, Val Loss: 12099,  Lear. Rate: 0.00492, Train Grad.: 79.0\n",
      "Epoch 10301/150000, Train Loss: 11705, Val Loss: 11993,  Lear. Rate: 0.00492, Train Grad.: 78.3\n",
      "Epoch 10401/150000, Train Loss: 11601, Val Loss: 11887,  Lear. Rate: 0.00492, Train Grad.: 77.6\n",
      "Epoch 10501/150000, Train Loss: 11498, Val Loss: 11783,  Lear. Rate: 0.00492, Train Grad.: 76.8\n",
      "Epoch 10601/150000, Train Loss: 11395, Val Loss: 11679,  Lear. Rate: 0.00493, Train Grad.: 76.2\n",
      "Epoch 10701/150000, Train Loss: 11294, Val Loss: 11576,  Lear. Rate: 0.00493, Train Grad.: 75.4\n",
      "Epoch 10801/150000, Train Loss: 11194, Val Loss: 11475,  Lear. Rate: 0.00493, Train Grad.: 75.0\n",
      "Epoch 10901/150000, Train Loss: 11095, Val Loss: 11374,  Lear. Rate: 0.00493, Train Grad.: 74.1\n",
      "Epoch 11001/150000, Train Loss: 10996, Val Loss: 11274,  Lear. Rate: 0.00493, Train Grad.: 73.4\n",
      "Epoch 11101/150000, Train Loss: 10898, Val Loss: 11175,  Lear. Rate: 0.00493, Train Grad.: 72.8\n",
      "Epoch 11201/150000, Train Loss: 10801, Val Loss: 11077,  Lear. Rate: 0.00493, Train Grad.: 72.1\n",
      "Epoch 11301/150000, Train Loss: 10705, Val Loss: 10979,  Lear. Rate: 0.00493, Train Grad.: 71.5\n",
      "Epoch 11401/150000, Train Loss: 10610, Val Loss: 10882,  Lear. Rate: 0.00494, Train Grad.: 70.9\n",
      "Epoch 11501/150000, Train Loss: 10515, Val Loss: 10786,  Lear. Rate: 0.00494, Train Grad.: 70.3\n",
      "Epoch 11601/150000, Train Loss: 10421, Val Loss: 10691,  Lear. Rate: 0.00494, Train Grad.: 69.7\n",
      "Epoch 11701/150000, Train Loss: 10328, Val Loss: 10596,  Lear. Rate: 0.00494, Train Grad.: 69.1\n",
      "Epoch 11801/150000, Train Loss: 10235, Val Loss: 10501,  Lear. Rate: 0.00494, Train Grad.: 68.5\n",
      "Epoch 11901/150000, Train Loss: 10144, Val Loss: 10408,  Lear. Rate: 0.00494, Train Grad.: 67.9\n",
      "Epoch 12001/150000, Train Loss: 10053, Val Loss: 10316,  Lear. Rate: 0.00494, Train Grad.: 67.3\n",
      "Epoch 12101/150000, Train Loss: 9963, Val Loss: 10224,  Lear. Rate: 0.00494, Train Grad.: 66.7\n",
      "Epoch 12201/150000, Train Loss: 9873, Val Loss: 10132,  Lear. Rate: 0.00494, Train Grad.: 66.1\n",
      "Epoch 12301/150000, Train Loss: 9784, Val Loss: 10041,  Lear. Rate: 0.00494, Train Grad.: 65.6\n",
      "Epoch 12401/150000, Train Loss: 9696, Val Loss: 9951,  Lear. Rate: 0.00495, Train Grad.: 65.0\n",
      "Epoch 12501/150000, Train Loss: 9609, Val Loss: 9862,  Lear. Rate: 0.00495, Train Grad.: 64.5\n",
      "Epoch 12601/150000, Train Loss: 9522, Val Loss: 9774,  Lear. Rate: 0.00495, Train Grad.: 63.9\n",
      "Epoch 12701/150000, Train Loss: 9436, Val Loss: 9686,  Lear. Rate: 0.00495, Train Grad.: 63.4\n",
      "Epoch 12801/150000, Train Loss: 9351, Val Loss: 9599,  Lear. Rate: 0.00495, Train Grad.: 62.9\n",
      "Epoch 12901/150000, Train Loss: 9266, Val Loss: 9513,  Lear. Rate: 0.00495, Train Grad.: 62.4\n",
      "Epoch 13001/150000, Train Loss: 9182, Val Loss: 9428,  Lear. Rate: 0.00495, Train Grad.: 61.9\n",
      "Epoch 13101/150000, Train Loss: 9099, Val Loss: 9344,  Lear. Rate: 0.00495, Train Grad.: 61.4\n",
      "Epoch 13201/150000, Train Loss: 9016, Val Loss: 9260,  Lear. Rate: 0.00495, Train Grad.: 60.9\n",
      "Epoch 13301/150000, Train Loss: 8933, Val Loss: 9176,  Lear. Rate: 0.00495, Train Grad.: 60.5\n",
      "Epoch 13401/150000, Train Loss: 8852, Val Loss: 9094,  Lear. Rate: 0.00495, Train Grad.: 60.0\n",
      "Epoch 13501/150000, Train Loss: 8770, Val Loss: 9012,  Lear. Rate: 0.00496, Train Grad.: 59.5\n",
      "Epoch 13601/150000, Train Loss: 8690, Val Loss: 8930,  Lear. Rate: 0.00496, Train Grad.: 59.1\n",
      "Epoch 13701/150000, Train Loss: 8610, Val Loss: 8849,  Lear. Rate: 0.00496, Train Grad.: 58.6\n",
      "Epoch 13801/150000, Train Loss: 8530, Val Loss: 8769,  Lear. Rate: 0.00496, Train Grad.: 58.2\n",
      "Epoch 13901/150000, Train Loss: 8451, Val Loss: 8689,  Lear. Rate: 0.00496, Train Grad.: 57.8\n",
      "Epoch 14001/150000, Train Loss: 8373, Val Loss: 8609,  Lear. Rate: 0.00496, Train Grad.: 57.6\n",
      "Epoch 14101/150000, Train Loss: 8295, Val Loss: 8530,  Lear. Rate: 0.00496, Train Grad.: 56.9\n",
      "Epoch 14201/150000, Train Loss: 8217, Val Loss: 8451,  Lear. Rate: 0.00496, Train Grad.: 56.6\n",
      "Epoch 14301/150000, Train Loss: 8140, Val Loss: 8373,  Lear. Rate: 0.00496, Train Grad.: 56.1\n",
      "Epoch 14401/150000, Train Loss: 8064, Val Loss: 8296,  Lear. Rate: 0.00496, Train Grad.: 55.7\n",
      "Epoch 14501/150000, Train Loss: 7988, Val Loss: 8218,  Lear. Rate: 0.00496, Train Grad.: 55.2\n",
      "Epoch 14601/150000, Train Loss: 7913, Val Loss: 8142,  Lear. Rate: 0.00496, Train Grad.: 54.7\n",
      "Epoch 14701/150000, Train Loss: 7838, Val Loss: 8066,  Lear. Rate: 0.00496, Train Grad.: 54.4\n",
      "Epoch 14801/150000, Train Loss: 7764, Val Loss: 7990,  Lear. Rate: 0.00497, Train Grad.: 54.5\n",
      "Epoch 14901/150000, Train Loss: 7690, Val Loss: 7915,  Lear. Rate: 0.00497, Train Grad.: 53.6\n",
      "Epoch 15001/150000, Train Loss: 7617, Val Loss: 7841,  Lear. Rate: 0.00497, Train Grad.: 53.3\n",
      "Epoch 15101/150000, Train Loss: 7544, Val Loss: 7767,  Lear. Rate: 0.00497, Train Grad.: 52.9\n",
      "Epoch 15201/150000, Train Loss: 7471, Val Loss: 7693,  Lear. Rate: 0.00497, Train Grad.: 52.5\n",
      "Epoch 15301/150000, Train Loss: 7399, Val Loss: 7620,  Lear. Rate: 0.00497, Train Grad.: 52.1\n",
      "Epoch 15401/150000, Train Loss: 7328, Val Loss: 7548,  Lear. Rate: 0.00497, Train Grad.: 51.7\n",
      "Epoch 15501/150000, Train Loss: 7257, Val Loss: 7476,  Lear. Rate: 0.00497, Train Grad.: 51.3\n",
      "Epoch 15601/150000, Train Loss: 7187, Val Loss: 7404,  Lear. Rate: 0.00497, Train Grad.: 51.0\n",
      "Epoch 15701/150000, Train Loss: 7117, Val Loss: 7333,  Lear. Rate: 0.00497, Train Grad.: 50.7\n",
      "Epoch 15801/150000, Train Loss: 7047, Val Loss: 7262,  Lear. Rate: 0.00497, Train Grad.: 50.3\n",
      "Epoch 15901/150000, Train Loss: 6978, Val Loss: 7192,  Lear. Rate: 0.00497, Train Grad.: 49.9\n",
      "Epoch 16001/150000, Train Loss: 6909, Val Loss: 7123,  Lear. Rate: 0.00497, Train Grad.: 49.5\n",
      "Epoch 16101/150000, Train Loss: 6841, Val Loss: 7054,  Lear. Rate: 0.00497, Train Grad.: 49.2\n",
      "Epoch 16201/150000, Train Loss: 6774, Val Loss: 6985,  Lear. Rate: 0.00497, Train Grad.: 48.9\n",
      "Epoch 16301/150000, Train Loss: 6707, Val Loss: 6917,  Lear. Rate: 0.00497, Train Grad.: 48.5\n",
      "Epoch 16401/150000, Train Loss: 6640, Val Loss: 6849,  Lear. Rate: 0.00497, Train Grad.: 48.1\n",
      "Epoch 16501/150000, Train Loss: 6574, Val Loss: 6782,  Lear. Rate: 0.00497, Train Grad.: 47.8\n",
      "Epoch 16601/150000, Train Loss: 6508, Val Loss: 6715,  Lear. Rate: 0.00498, Train Grad.: 47.5\n",
      "Epoch 16701/150000, Train Loss: 6442, Val Loss: 6649,  Lear. Rate: 0.00498, Train Grad.: 47.1\n",
      "Epoch 16801/150000, Train Loss: 6378, Val Loss: 6583,  Lear. Rate: 0.00498, Train Grad.: 46.8\n",
      "Epoch 16901/150000, Train Loss: 6313, Val Loss: 6518,  Lear. Rate: 0.00498, Train Grad.: 46.5\n",
      "Epoch 17001/150000, Train Loss: 6249, Val Loss: 6453,  Lear. Rate: 0.00498, Train Grad.: 46.1\n",
      "Epoch 17101/150000, Train Loss: 6186, Val Loss: 6388,  Lear. Rate: 0.00498, Train Grad.: 45.8\n",
      "Epoch 17201/150000, Train Loss: 6123, Val Loss: 6324,  Lear. Rate: 0.00498, Train Grad.: 45.5\n",
      "Epoch 17301/150000, Train Loss: 6060, Val Loss: 6261,  Lear. Rate: 0.00498, Train Grad.: 45.1\n",
      "Epoch 17401/150000, Train Loss: 5998, Val Loss: 6198,  Lear. Rate: 0.00498, Train Grad.: 44.8\n",
      "Epoch 17501/150000, Train Loss: 5936, Val Loss: 6135,  Lear. Rate: 0.00498, Train Grad.: 44.5\n",
      "Epoch 17601/150000, Train Loss: 5875, Val Loss: 6073,  Lear. Rate: 0.00498, Train Grad.: 44.1\n",
      "Epoch 17701/150000, Train Loss: 5814, Val Loss: 6012,  Lear. Rate: 0.00498, Train Grad.: 44.0\n",
      "Epoch 17801/150000, Train Loss: 5754, Val Loss: 5951,  Lear. Rate: 0.00498, Train Grad.: 43.5\n",
      "Epoch 17901/150000, Train Loss: 5694, Val Loss: 5890,  Lear. Rate: 0.00498, Train Grad.: 43.2\n",
      "Epoch 18001/150000, Train Loss: 5635, Val Loss: 5830,  Lear. Rate: 0.00498, Train Grad.: 42.8\n",
      "Epoch 18101/150000, Train Loss: 5576, Val Loss: 5770,  Lear. Rate: 0.00498, Train Grad.: 42.5\n",
      "Epoch 18201/150000, Train Loss: 5517, Val Loss: 5711,  Lear. Rate: 0.00498, Train Grad.: 42.2\n",
      "Epoch 18301/150000, Train Loss: 5459, Val Loss: 5652,  Lear. Rate: 0.00498, Train Grad.: 41.9\n",
      "Epoch 18401/150000, Train Loss: 5402, Val Loss: 5594,  Lear. Rate: 0.00498, Train Grad.: 41.5\n",
      "Epoch 18501/150000, Train Loss: 5345, Val Loss: 5536,  Lear. Rate: 0.00498, Train Grad.: 41.2\n",
      "Epoch 18601/150000, Train Loss: 5288, Val Loss: 5479,  Lear. Rate: 0.00498, Train Grad.: 40.9\n",
      "Epoch 18701/150000, Train Loss: 5232, Val Loss: 5422,  Lear. Rate: 0.00498, Train Grad.: 40.6\n",
      "Epoch 18801/150000, Train Loss: 5176, Val Loss: 5366,  Lear. Rate: 0.00498, Train Grad.: 40.2\n",
      "Epoch 18901/150000, Train Loss: 5121, Val Loss: 5310,  Lear. Rate: 0.00498, Train Grad.: 39.9\n",
      "Epoch 19001/150000, Train Loss: 5066, Val Loss: 5254,  Lear. Rate: 0.00498, Train Grad.: 39.6\n",
      "Epoch 19101/150000, Train Loss: 5012, Val Loss: 5199,  Lear. Rate: 0.00499, Train Grad.: 39.3\n",
      "Epoch 19201/150000, Train Loss: 4958, Val Loss: 5145,  Lear. Rate: 0.00499, Train Grad.: 39.0\n",
      "Epoch 19301/150000, Train Loss: 4904, Val Loss: 5091,  Lear. Rate: 0.00499, Train Grad.: 38.7\n",
      "Epoch 19401/150000, Train Loss: 4851, Val Loss: 5037,  Lear. Rate: 0.00499, Train Grad.: 38.4\n",
      "Epoch 19501/150000, Train Loss: 4798, Val Loss: 4984,  Lear. Rate: 0.00499, Train Grad.: 38.1\n",
      "Epoch 19601/150000, Train Loss: 4746, Val Loss: 4931,  Lear. Rate: 0.00499, Train Grad.: 38.0\n",
      "Epoch 19701/150000, Train Loss: 4694, Val Loss: 4878,  Lear. Rate: 0.00499, Train Grad.: 37.6\n",
      "Epoch 19801/150000, Train Loss: 4642, Val Loss: 4826,  Lear. Rate: 0.00499, Train Grad.: 37.3\n",
      "Epoch 19901/150000, Train Loss: 4591, Val Loss: 4775,  Lear. Rate: 0.00499, Train Grad.: 36.9\n",
      "Epoch 20001/150000, Train Loss: 4540, Val Loss: 4724,  Lear. Rate: 0.00499, Train Grad.: 36.7\n",
      "Epoch 20101/150000, Train Loss: 4490, Val Loss: 4673,  Lear. Rate: 0.00499, Train Grad.: 36.4\n",
      "Epoch 20201/150000, Train Loss: 4440, Val Loss: 4623,  Lear. Rate: 0.00499, Train Grad.: 36.1\n",
      "Epoch 20301/150000, Train Loss: 4390, Val Loss: 4573,  Lear. Rate: 0.00499, Train Grad.: 35.8\n",
      "Epoch 20401/150000, Train Loss: 4341, Val Loss: 4523,  Lear. Rate: 0.00499, Train Grad.: 35.8\n",
      "Epoch 20501/150000, Train Loss: 4292, Val Loss: 4474,  Lear. Rate: 0.00499, Train Grad.: 35.3\n",
      "Epoch 20601/150000, Train Loss: 4244, Val Loss: 4426,  Lear. Rate: 0.00499, Train Grad.: 34.5\n",
      "Epoch 20701/150000, Train Loss: 4196, Val Loss: 4377,  Lear. Rate: 0.00499, Train Grad.: 34.7\n",
      "Epoch 20801/150000, Train Loss: 4148, Val Loss: 4329,  Lear. Rate: 0.00499, Train Grad.: 34.4\n",
      "Epoch 20901/150000, Train Loss: 4101, Val Loss: 4282,  Lear. Rate: 0.00499, Train Grad.: 34.2\n",
      "Epoch 21001/150000, Train Loss: 4054, Val Loss: 4235,  Lear. Rate: 0.00499, Train Grad.: 33.9\n",
      "Epoch 21101/150000, Train Loss: 4008, Val Loss: 4188,  Lear. Rate: 0.00499, Train Grad.: 33.6\n",
      "Epoch 21201/150000, Train Loss: 3961, Val Loss: 4141,  Lear. Rate: 0.00499, Train Grad.: 33.3\n",
      "Epoch 21301/150000, Train Loss: 3916, Val Loss: 4095,  Lear. Rate: 0.00499, Train Grad.: 33.1\n",
      "Epoch 21401/150000, Train Loss: 3870, Val Loss: 4050,  Lear. Rate: 0.00499, Train Grad.: 32.8\n",
      "Epoch 21501/150000, Train Loss: 3826, Val Loss: 4005,  Lear. Rate: 0.00499, Train Grad.: 32.5\n",
      "Epoch 21601/150000, Train Loss: 3781, Val Loss: 3960,  Lear. Rate: 0.00499, Train Grad.: 32.2\n",
      "Epoch 21701/150000, Train Loss: 3737, Val Loss: 3916,  Lear. Rate: 0.00499, Train Grad.: 32.0\n",
      "Epoch 21801/150000, Train Loss: 3693, Val Loss: 3872,  Lear. Rate: 0.00499, Train Grad.: 31.7\n",
      "Epoch 21901/150000, Train Loss: 3650, Val Loss: 3828,  Lear. Rate: 0.00499, Train Grad.: 31.4\n",
      "Epoch 22001/150000, Train Loss: 3607, Val Loss: 3785,  Lear. Rate: 0.00499, Train Grad.: 31.2\n",
      "Epoch 22101/150000, Train Loss: 3564, Val Loss: 3742,  Lear. Rate: 0.00499, Train Grad.: 30.9\n",
      "Epoch 22201/150000, Train Loss: 3522, Val Loss: 3700,  Lear. Rate: 0.00499, Train Grad.: 30.6\n",
      "Epoch 22301/150000, Train Loss: 3480, Val Loss: 3658,  Lear. Rate: 0.00499, Train Grad.: 30.4\n",
      "Epoch 22401/150000, Train Loss: 3439, Val Loss: 3617,  Lear. Rate: 0.00499, Train Grad.: 30.1\n",
      "Epoch 22501/150000, Train Loss: 3398, Val Loss: 3576,  Lear. Rate: 0.00499, Train Grad.: 29.9\n",
      "Epoch 22601/150000, Train Loss: 3357, Val Loss: 3535,  Lear. Rate: 0.00499, Train Grad.: 29.6\n",
      "Epoch 22701/150000, Train Loss: 3317, Val Loss: 3495,  Lear. Rate: 0.00499, Train Grad.: 29.3\n",
      "Epoch 22801/150000, Train Loss: 3277, Val Loss: 3455,  Lear. Rate: 0.00499, Train Grad.: 29.1\n",
      "Epoch 22901/150000, Train Loss: 3237, Val Loss: 3415,  Lear. Rate: 0.00499, Train Grad.: 29.0\n",
      "Epoch 23001/150000, Train Loss: 3198, Val Loss: 3375,  Lear. Rate: 0.00499, Train Grad.: 28.6\n",
      "Epoch 23101/150000, Train Loss: 3159, Val Loss: 3336,  Lear. Rate: 0.00499, Train Grad.: 28.3\n",
      "Epoch 23201/150000, Train Loss: 3121, Val Loss: 3297,  Lear. Rate: 0.00499, Train Grad.: 28.0\n",
      "Epoch 23301/150000, Train Loss: 3083, Val Loss: 3259,  Lear. Rate: 0.00499, Train Grad.: 27.8\n",
      "Epoch 23401/150000, Train Loss: 3045, Val Loss: 3221,  Lear. Rate: 0.00499, Train Grad.: 27.6\n",
      "Epoch 23501/150000, Train Loss: 3007, Val Loss: 3183,  Lear. Rate: 0.00499, Train Grad.: 27.3\n",
      "Epoch 23601/150000, Train Loss: 2970, Val Loss: 3146,  Lear. Rate: 0.00499, Train Grad.: 27.1\n",
      "Epoch 23701/150000, Train Loss: 2934, Val Loss: 3109,  Lear. Rate: 0.00499, Train Grad.: 26.8\n",
      "Epoch 23801/150000, Train Loss: 2897, Val Loss: 3072,  Lear. Rate: 0.00499, Train Grad.: 26.6\n",
      "Epoch 23901/150000, Train Loss: 2861, Val Loss: 3036,  Lear. Rate: 0.00499, Train Grad.: 26.3\n",
      "Epoch 24001/150000, Train Loss: 2826, Val Loss: 3000,  Lear. Rate: 0.00500, Train Grad.: 25.9\n",
      "Epoch 24101/150000, Train Loss: 2790, Val Loss: 2965,  Lear. Rate: 0.00500, Train Grad.: 25.8\n",
      "Epoch 24201/150000, Train Loss: 2755, Val Loss: 2930,  Lear. Rate: 0.00500, Train Grad.: 25.6\n",
      "Epoch 24301/150000, Train Loss: 2721, Val Loss: 2895,  Lear. Rate: 0.00500, Train Grad.: 25.4\n",
      "Epoch 24401/150000, Train Loss: 2686, Val Loss: 2861,  Lear. Rate: 0.00500, Train Grad.: 25.2\n",
      "Epoch 24501/150000, Train Loss: 2652, Val Loss: 2827,  Lear. Rate: 0.00500, Train Grad.: 24.9\n",
      "Epoch 24601/150000, Train Loss: 2619, Val Loss: 2794,  Lear. Rate: 0.00500, Train Grad.: 24.9\n",
      "Epoch 24701/150000, Train Loss: 2585, Val Loss: 2761,  Lear. Rate: 0.00500, Train Grad.: 24.4\n",
      "Epoch 24801/150000, Train Loss: 2552, Val Loss: 2728,  Lear. Rate: 0.00500, Train Grad.: 24.2\n",
      "Epoch 24901/150000, Train Loss: 2520, Val Loss: 2695,  Lear. Rate: 0.00500, Train Grad.: 23.9\n",
      "Epoch 25001/150000, Train Loss: 2487, Val Loss: 2663,  Lear. Rate: 0.00500, Train Grad.: 23.7\n",
      "Epoch 25101/150000, Train Loss: 2455, Val Loss: 2631,  Lear. Rate: 0.00500, Train Grad.: 23.9\n",
      "Epoch 25201/150000, Train Loss: 2423, Val Loss: 2600,  Lear. Rate: 0.00500, Train Grad.: 23.3\n",
      "Epoch 25301/150000, Train Loss: 2392, Val Loss: 2569,  Lear. Rate: 0.00500, Train Grad.: 23.1\n",
      "Epoch 25401/150000, Train Loss: 2361, Val Loss: 2538,  Lear. Rate: 0.00500, Train Grad.: 22.9\n",
      "Epoch 25501/150000, Train Loss: 2330, Val Loss: 2507,  Lear. Rate: 0.00500, Train Grad.: 22.5\n",
      "Epoch 25601/150000, Train Loss: 2299, Val Loss: 2477,  Lear. Rate: 0.00500, Train Grad.: 22.4\n",
      "Epoch 25701/150000, Train Loss: 2269, Val Loss: 2447,  Lear. Rate: 0.00500, Train Grad.: 22.2\n",
      "Epoch 25801/150000, Train Loss: 2239, Val Loss: 2417,  Lear. Rate: 0.00500, Train Grad.: 22.0\n",
      "Epoch 25901/150000, Train Loss: 2209, Val Loss: 2388,  Lear. Rate: 0.00500, Train Grad.: 21.8\n",
      "Epoch 26001/150000, Train Loss: 2180, Val Loss: 2359,  Lear. Rate: 0.00500, Train Grad.: 21.6\n",
      "Epoch 26101/150000, Train Loss: 2151, Val Loss: 2330,  Lear. Rate: 0.00500, Train Grad.: 21.4\n",
      "Epoch 26201/150000, Train Loss: 2122, Val Loss: 2301,  Lear. Rate: 0.00500, Train Grad.: 21.2\n",
      "Epoch 26301/150000, Train Loss: 2093, Val Loss: 2273,  Lear. Rate: 0.00500, Train Grad.: 21.0\n",
      "Epoch 26401/150000, Train Loss: 2065, Val Loss: 2245,  Lear. Rate: 0.00500, Train Grad.: 20.5\n",
      "Epoch 26501/150000, Train Loss: 2037, Val Loss: 2217,  Lear. Rate: 0.00500, Train Grad.: 20.6\n",
      "Epoch 26601/150000, Train Loss: 2009, Val Loss: 2190,  Lear. Rate: 0.00500, Train Grad.: 20.4\n",
      "Epoch 26701/150000, Train Loss: 1982, Val Loss: 2162,  Lear. Rate: 0.00500, Train Grad.: 20.2\n",
      "Epoch 26801/150000, Train Loss: 1954, Val Loss: 2135,  Lear. Rate: 0.00500, Train Grad.: 20.0\n",
      "Epoch 26901/150000, Train Loss: 1927, Val Loss: 2109,  Lear. Rate: 0.00500, Train Grad.: 19.8\n",
      "Epoch 27001/150000, Train Loss: 1901, Val Loss: 2082,  Lear. Rate: 0.00500, Train Grad.: 19.6\n",
      "Epoch 27101/150000, Train Loss: 1874, Val Loss: 2056,  Lear. Rate: 0.00500, Train Grad.: 19.4\n",
      "Epoch 27201/150000, Train Loss: 1848, Val Loss: 2030,  Lear. Rate: 0.00500, Train Grad.: 19.2\n",
      "Epoch 27301/150000, Train Loss: 1822, Val Loss: 2005,  Lear. Rate: 0.00500, Train Grad.: 19.0\n",
      "Epoch 27401/150000, Train Loss: 1797, Val Loss: 1980,  Lear. Rate: 0.00500, Train Grad.: 18.6\n",
      "Epoch 27501/150000, Train Loss: 1772, Val Loss: 1955,  Lear. Rate: 0.00500, Train Grad.: 18.6\n",
      "Epoch 27601/150000, Train Loss: 1747, Val Loss: 1930,  Lear. Rate: 0.00500, Train Grad.: 18.5\n",
      "Epoch 27701/150000, Train Loss: 1722, Val Loss: 1906,  Lear. Rate: 0.00500, Train Grad.: 18.2\n",
      "Epoch 27801/150000, Train Loss: 1697, Val Loss: 1882,  Lear. Rate: 0.00500, Train Grad.: 18.1\n",
      "Epoch 27901/150000, Train Loss: 1673, Val Loss: 1858,  Lear. Rate: 0.00500, Train Grad.: 18.2\n",
      "Epoch 28001/150000, Train Loss: 1649, Val Loss: 1834,  Lear. Rate: 0.00500, Train Grad.: 17.7\n",
      "Epoch 28101/150000, Train Loss: 1626, Val Loss: 1811,  Lear. Rate: 0.00500, Train Grad.: 17.3\n",
      "Epoch 28201/150000, Train Loss: 1602, Val Loss: 1788,  Lear. Rate: 0.00500, Train Grad.: 17.3\n",
      "Epoch 28301/150000, Train Loss: 1579, Val Loss: 1765,  Lear. Rate: 0.00500, Train Grad.: 17.1\n",
      "Epoch 28401/150000, Train Loss: 1556, Val Loss: 1742,  Lear. Rate: 0.00500, Train Grad.: 16.9\n",
      "Epoch 28501/150000, Train Loss: 1534, Val Loss: 1720,  Lear. Rate: 0.00500, Train Grad.: 16.8\n",
      "Epoch 28601/150000, Train Loss: 1511, Val Loss: 1698,  Lear. Rate: 0.00500, Train Grad.: 16.6\n",
      "Epoch 28701/150000, Train Loss: 1489, Val Loss: 1676,  Lear. Rate: 0.00500, Train Grad.: 16.4\n",
      "Epoch 28801/150000, Train Loss: 1467, Val Loss: 1655,  Lear. Rate: 0.00500, Train Grad.: 16.3\n",
      "Epoch 28901/150000, Train Loss: 1446, Val Loss: 1634,  Lear. Rate: 0.00500, Train Grad.: 16.1\n",
      "Epoch 29001/150000, Train Loss: 1424, Val Loss: 1613,  Lear. Rate: 0.00500, Train Grad.: 16.1\n",
      "Epoch 29101/150000, Train Loss: 1403, Val Loss: 1593,  Lear. Rate: 0.00500, Train Grad.: 15.7\n",
      "Epoch 29201/150000, Train Loss: 1382, Val Loss: 1573,  Lear. Rate: 0.00500, Train Grad.: 15.6\n",
      "Epoch 29301/150000, Train Loss: 1361, Val Loss: 1553,  Lear. Rate: 0.00500, Train Grad.: 15.4\n",
      "Epoch 29401/150000, Train Loss: 1341, Val Loss: 1533,  Lear. Rate: 0.00500, Train Grad.: 15.4\n",
      "Epoch 29501/150000, Train Loss: 1321, Val Loss: 1513,  Lear. Rate: 0.00500, Train Grad.: 15.1\n",
      "Epoch 29601/150000, Train Loss: 1300, Val Loss: 1494,  Lear. Rate: 0.00500, Train Grad.: 14.9\n",
      "Epoch 29701/150000, Train Loss: 1281, Val Loss: 1475,  Lear. Rate: 0.00500, Train Grad.: 14.7\n",
      "Epoch 29801/150000, Train Loss: 1261, Val Loss: 1456,  Lear. Rate: 0.00500, Train Grad.: 14.6\n",
      "Epoch 29901/150000, Train Loss: 1242, Val Loss: 1437,  Lear. Rate: 0.00500, Train Grad.: 14.5\n",
      "Epoch 30001/150000, Train Loss: 1223, Val Loss: 1419,  Lear. Rate: 0.00500, Train Grad.: 14.3\n",
      "Epoch 30101/150000, Train Loss: 1204, Val Loss: 1401,  Lear. Rate: 0.00500, Train Grad.: 14.2\n",
      "Epoch 30201/150000, Train Loss: 1185, Val Loss: 1383,  Lear. Rate: 0.00500, Train Grad.: 13.9\n",
      "Epoch 30301/150000, Train Loss: 1166, Val Loss: 1365,  Lear. Rate: 0.00500, Train Grad.: 13.8\n",
      "Epoch 30401/150000, Train Loss: 1148, Val Loss: 1348,  Lear. Rate: 0.00500, Train Grad.: 13.6\n",
      "Epoch 30501/150000, Train Loss: 1130, Val Loss: 1330,  Lear. Rate: 0.00500, Train Grad.: 13.5\n",
      "Epoch 30601/150000, Train Loss: 1112, Val Loss: 1313,  Lear. Rate: 0.00500, Train Grad.: 13.2\n",
      "Epoch 30701/150000, Train Loss: 1095, Val Loss: 1296,  Lear. Rate: 0.00500, Train Grad.: 13.2\n",
      "Epoch 30801/150000, Train Loss: 1077, Val Loss: 1280,  Lear. Rate: 0.00500, Train Grad.: 13.0\n",
      "Epoch 30901/150000, Train Loss: 1060, Val Loss: 1264,  Lear. Rate: 0.00500, Train Grad.: 12.9\n",
      "Epoch 31001/150000, Train Loss: 1043, Val Loss: 1247,  Lear. Rate: 0.00500, Train Grad.: 12.7\n",
      "Epoch 31101/150000, Train Loss: 1026, Val Loss: 1232,  Lear. Rate: 0.00500, Train Grad.: 12.6\n",
      "Epoch 31201/150000, Train Loss: 1010, Val Loss: 1216,  Lear. Rate: 0.00500, Train Grad.: 12.4\n",
      "Epoch 31301/150000, Train Loss: 993, Val Loss: 1201,  Lear. Rate: 0.00500, Train Grad.: 12.2\n",
      "Epoch 31401/150000, Train Loss: 977, Val Loss: 1185,  Lear. Rate: 0.00500, Train Grad.: 12.1\n",
      "Epoch 31501/150000, Train Loss: 961, Val Loss: 1170,  Lear. Rate: 0.00500, Train Grad.: 11.9\n",
      "Epoch 31601/150000, Train Loss: 946, Val Loss: 1156,  Lear. Rate: 0.00500, Train Grad.: 11.8\n",
      "Epoch 31701/150000, Train Loss: 930, Val Loss: 1141,  Lear. Rate: 0.00500, Train Grad.: 11.7\n",
      "Epoch 31801/150000, Train Loss: 915, Val Loss: 1127,  Lear. Rate: 0.00500, Train Grad.: 11.6\n",
      "Epoch 31901/150000, Train Loss: 900, Val Loss: 1113,  Lear. Rate: 0.00500, Train Grad.: 11.4\n",
      "Epoch 32001/150000, Train Loss: 885, Val Loss: 1099,  Lear. Rate: 0.00500, Train Grad.: 11.2\n",
      "Epoch 32101/150000, Train Loss: 870, Val Loss: 1085,  Lear. Rate: 0.00500, Train Grad.: 11.1\n",
      "Epoch 32201/150000, Train Loss: 856, Val Loss: 1072,  Lear. Rate: 0.00500, Train Grad.: 11.0\n",
      "Epoch 32301/150000, Train Loss: 841, Val Loss: 1059,  Lear. Rate: 0.00500, Train Grad.: 10.9\n",
      "Epoch 32401/150000, Train Loss: 827, Val Loss: 1046,  Lear. Rate: 0.00500, Train Grad.: 10.7\n",
      "Epoch 32501/150000, Train Loss: 813, Val Loss: 1033,  Lear. Rate: 0.00500, Train Grad.: 10.7\n",
      "Epoch 32601/150000, Train Loss: 799, Val Loss: 1020,  Lear. Rate: 0.00500, Train Grad.: 10.4\n",
      "Epoch 32701/150000, Train Loss: 786, Val Loss: 1008,  Lear. Rate: 0.00500, Train Grad.: 10.3\n",
      "Epoch 32801/150000, Train Loss: 772, Val Loss: 996,  Lear. Rate: 0.00500, Train Grad.: 10.2\n",
      "Epoch 32901/150000, Train Loss: 759, Val Loss: 984,  Lear. Rate: 0.00500, Train Grad.: 10.0\n",
      "Epoch 33001/150000, Train Loss: 746, Val Loss: 972,  Lear. Rate: 0.00500, Train Grad.: 9.9\n",
      "Epoch 33101/150000, Train Loss: 733, Val Loss: 960,  Lear. Rate: 0.00500, Train Grad.: 9.8\n",
      "Epoch 33201/150000, Train Loss: 721, Val Loss: 948,  Lear. Rate: 0.00500, Train Grad.: 9.6\n",
      "Epoch 33301/150000, Train Loss: 708, Val Loss: 937,  Lear. Rate: 0.00500, Train Grad.: 9.3\n",
      "Epoch 33401/150000, Train Loss: 696, Val Loss: 926,  Lear. Rate: 0.00500, Train Grad.: 9.4\n",
      "Epoch 33501/150000, Train Loss: 684, Val Loss: 914,  Lear. Rate: 0.00500, Train Grad.: 9.3\n",
      "Epoch 33601/150000, Train Loss: 672, Val Loss: 903,  Lear. Rate: 0.00500, Train Grad.: 9.3\n",
      "Epoch 33701/150000, Train Loss: 660, Val Loss: 893,  Lear. Rate: 0.00500, Train Grad.: 9.0\n",
      "Epoch 33801/150000, Train Loss: 648, Val Loss: 882,  Lear. Rate: 0.00500, Train Grad.: 8.9\n",
      "Epoch 33901/150000, Train Loss: 637, Val Loss: 872,  Lear. Rate: 0.00500, Train Grad.: 8.8\n",
      "Epoch 34001/150000, Train Loss: 625, Val Loss: 862,  Lear. Rate: 0.00500, Train Grad.: 8.7\n",
      "Epoch 34101/150000, Train Loss: 614, Val Loss: 852,  Lear. Rate: 0.00500, Train Grad.: 8.1\n",
      "Epoch 34201/150000, Train Loss: 603, Val Loss: 842,  Lear. Rate: 0.00500, Train Grad.: 8.4\n",
      "Epoch 34301/150000, Train Loss: 592, Val Loss: 832,  Lear. Rate: 0.00500, Train Grad.: 8.3\n",
      "Epoch 34401/150000, Train Loss: 582, Val Loss: 822,  Lear. Rate: 0.00500, Train Grad.: 8.2\n",
      "Epoch 34501/150000, Train Loss: 571, Val Loss: 813,  Lear. Rate: 0.00500, Train Grad.: 8.1\n",
      "Epoch 34601/150000, Train Loss: 561, Val Loss: 804,  Lear. Rate: 0.00500, Train Grad.: 8.0\n",
      "Epoch 34701/150000, Train Loss: 550, Val Loss: 795,  Lear. Rate: 0.00500, Train Grad.: 7.9\n",
      "Epoch 34801/150000, Train Loss: 540, Val Loss: 786,  Lear. Rate: 0.00500, Train Grad.: 7.8\n",
      "Epoch 34901/150000, Train Loss: 530, Val Loss: 777,  Lear. Rate: 0.00500, Train Grad.: 7.6\n",
      "Epoch 35001/150000, Train Loss: 521, Val Loss: 769,  Lear. Rate: 0.00500, Train Grad.: 6.9\n",
      "Epoch 35101/150000, Train Loss: 511, Val Loss: 760,  Lear. Rate: 0.00500, Train Grad.: 7.4\n",
      "Epoch 35201/150000, Train Loss: 502, Val Loss: 752,  Lear. Rate: 0.00500, Train Grad.: 7.3\n",
      "Epoch 35301/150000, Train Loss: 492, Val Loss: 744,  Lear. Rate: 0.00500, Train Grad.: 7.2\n",
      "Epoch 35401/150000, Train Loss: 483, Val Loss: 736,  Lear. Rate: 0.00500, Train Grad.: 7.1\n",
      "Epoch 35501/150000, Train Loss: 474, Val Loss: 728,  Lear. Rate: 0.00500, Train Grad.: 6.8\n",
      "Epoch 35601/150000, Train Loss: 465, Val Loss: 720,  Lear. Rate: 0.00500, Train Grad.: 6.9\n",
      "Epoch 35701/150000, Train Loss: 456, Val Loss: 713,  Lear. Rate: 0.00500, Train Grad.: 6.8\n",
      "Epoch 35801/150000, Train Loss: 448, Val Loss: 705,  Lear. Rate: 0.00500, Train Grad.: 6.7\n",
      "Epoch 35901/150000, Train Loss: 439, Val Loss: 698,  Lear. Rate: 0.00500, Train Grad.: 6.6\n",
      "Epoch 36001/150000, Train Loss: 431, Val Loss: 691,  Lear. Rate: 0.00500, Train Grad.: 6.5\n",
      "Epoch 36101/150000, Train Loss: 423, Val Loss: 684,  Lear. Rate: 0.00500, Train Grad.: 6.3\n",
      "Epoch 36201/150000, Train Loss: 415, Val Loss: 677,  Lear. Rate: 0.00500, Train Grad.: 6.3\n",
      "Epoch 36301/150000, Train Loss: 407, Val Loss: 671,  Lear. Rate: 0.00500, Train Grad.: 6.4\n",
      "Epoch 36401/150000, Train Loss: 399, Val Loss: 664,  Lear. Rate: 0.00500, Train Grad.: 6.1\n",
      "Epoch 36501/150000, Train Loss: 392, Val Loss: 656,  Lear. Rate: 0.00500, Train Grad.: 6.0\n",
      "Epoch 36601/150000, Train Loss: 383, Val Loss: 645,  Lear. Rate: 0.00500, Train Grad.: 5.9\n",
      "Epoch 36701/150000, Train Loss: 376, Val Loss: 635,  Lear. Rate: 0.00500, Train Grad.: 5.8\n",
      "Epoch 36801/150000, Train Loss: 368, Val Loss: 626,  Lear. Rate: 0.00500, Train Grad.: 5.6\n",
      "Epoch 36901/150000, Train Loss: 361, Val Loss: 619,  Lear. Rate: 0.00500, Train Grad.: 5.6\n",
      "Epoch 37001/150000, Train Loss: 353, Val Loss: 611,  Lear. Rate: 0.00500, Train Grad.: 5.5\n",
      "Epoch 37101/150000, Train Loss: 346, Val Loss: 604,  Lear. Rate: 0.00500, Train Grad.: 5.5\n",
      "Epoch 37201/150000, Train Loss: 339, Val Loss: 597,  Lear. Rate: 0.00500, Train Grad.: 5.4\n",
      "Epoch 37301/150000, Train Loss: 333, Val Loss: 590,  Lear. Rate: 0.00500, Train Grad.: 5.2\n",
      "Epoch 37401/150000, Train Loss: 326, Val Loss: 584,  Lear. Rate: 0.00500, Train Grad.: 5.2\n",
      "Epoch 37501/150000, Train Loss: 320, Val Loss: 578,  Lear. Rate: 0.00500, Train Grad.: 5.1\n",
      "Epoch 37601/150000, Train Loss: 313, Val Loss: 572,  Lear. Rate: 0.00500, Train Grad.: 5.0\n",
      "Epoch 37701/150000, Train Loss: 307, Val Loss: 566,  Lear. Rate: 0.00500, Train Grad.: 4.9\n",
      "Epoch 37801/150000, Train Loss: 301, Val Loss: 561,  Lear. Rate: 0.00500, Train Grad.: 4.7\n",
      "Epoch 37901/150000, Train Loss: 295, Val Loss: 556,  Lear. Rate: 0.00500, Train Grad.: 4.7\n",
      "Epoch 38001/150000, Train Loss: 289, Val Loss: 551,  Lear. Rate: 0.00500, Train Grad.: 4.6\n",
      "Epoch 38101/150000, Train Loss: 283, Val Loss: 546,  Lear. Rate: 0.00500, Train Grad.: 4.5\n",
      "Epoch 38201/150000, Train Loss: 278, Val Loss: 541,  Lear. Rate: 0.00500, Train Grad.: 4.4\n",
      "Epoch 38301/150000, Train Loss: 272, Val Loss: 536,  Lear. Rate: 0.00500, Train Grad.: 4.4\n",
      "Epoch 38401/150000, Train Loss: 267, Val Loss: 532,  Lear. Rate: 0.00500, Train Grad.: 4.3\n",
      "Epoch 38501/150000, Train Loss: 262, Val Loss: 527,  Lear. Rate: 0.00500, Train Grad.: 4.2\n",
      "Epoch 38601/150000, Train Loss: 257, Val Loss: 522,  Lear. Rate: 0.00500, Train Grad.: 4.1\n",
      "Epoch 38701/150000, Train Loss: 252, Val Loss: 518,  Lear. Rate: 0.00500, Train Grad.: 4.0\n",
      "Epoch 38801/150000, Train Loss: 247, Val Loss: 514,  Lear. Rate: 0.00500, Train Grad.: 4.0\n",
      "Epoch 38901/150000, Train Loss: 242, Val Loss: 510,  Lear. Rate: 0.00500, Train Grad.: 3.8\n",
      "Epoch 39001/150000, Train Loss: 238, Val Loss: 506,  Lear. Rate: 0.00500, Train Grad.: 3.7\n",
      "Epoch 39101/150000, Train Loss: 233, Val Loss: 503,  Lear. Rate: 0.00500, Train Grad.: 3.7\n",
      "Epoch 39201/150000, Train Loss: 229, Val Loss: 499,  Lear. Rate: 0.00500, Train Grad.: 3.6\n",
      "Epoch 39301/150000, Train Loss: 225, Val Loss: 496,  Lear. Rate: 0.00500, Train Grad.: 3.6\n",
      "Epoch 39401/150000, Train Loss: 220, Val Loss: 492,  Lear. Rate: 0.00500, Train Grad.: 3.5\n",
      "Epoch 39501/150000, Train Loss: 216, Val Loss: 489,  Lear. Rate: 0.00500, Train Grad.: 3.3\n",
      "Epoch 39601/150000, Train Loss: 212, Val Loss: 486,  Lear. Rate: 0.00500, Train Grad.: 3.3\n",
      "Epoch 39701/150000, Train Loss: 208, Val Loss: 483,  Lear. Rate: 0.00500, Train Grad.: 2.7\n",
      "Epoch 39801/150000, Train Loss: 204, Val Loss: 480,  Lear. Rate: 0.00500, Train Grad.: 3.2\n",
      "Epoch 39901/150000, Train Loss: 201, Val Loss: 477,  Lear. Rate: 0.00500, Train Grad.: 3.1\n",
      "Epoch 40001/150000, Train Loss: 197, Val Loss: 475,  Lear. Rate: 0.00500, Train Grad.: 3.0\n",
      "Epoch 40101/150000, Train Loss: 193, Val Loss: 472,  Lear. Rate: 0.00500, Train Grad.: 3.0\n",
      "Epoch 40201/150000, Train Loss: 190, Val Loss: 470,  Lear. Rate: 0.00500, Train Grad.: 2.8\n",
      "Epoch 40301/150000, Train Loss: 186, Val Loss: 468,  Lear. Rate: 0.00500, Train Grad.: 2.8\n",
      "Epoch 40401/150000, Train Loss: 183, Val Loss: 466,  Lear. Rate: 0.00500, Train Grad.: 2.5\n",
      "Epoch 40501/150000, Train Loss: 180, Val Loss: 463,  Lear. Rate: 0.00500, Train Grad.: 2.7\n",
      "Epoch 40601/150000, Train Loss: 177, Val Loss: 461,  Lear. Rate: 0.00500, Train Grad.: 2.6\n",
      "Epoch 40701/150000, Train Loss: 173, Val Loss: 459,  Lear. Rate: 0.00500, Train Grad.: 2.6\n",
      "Epoch 40801/150000, Train Loss: 170, Val Loss: 457,  Lear. Rate: 0.00500, Train Grad.: 2.9\n",
      "Epoch 40901/150000, Train Loss: 167, Val Loss: 456,  Lear. Rate: 0.00500, Train Grad.: 2.5\n",
      "Epoch 41001/150000, Train Loss: 164, Val Loss: 454,  Lear. Rate: 0.00500, Train Grad.: 2.4\n",
      "Epoch 41101/150000, Train Loss: 162, Val Loss: 452,  Lear. Rate: 0.00500, Train Grad.: 2.4\n",
      "Epoch 41201/150000, Train Loss: 159, Val Loss: 451,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 41301/150000, Train Loss: 156, Val Loss: 450,  Lear. Rate: 0.00500, Train Grad.: 2.2\n",
      "Epoch 41401/150000, Train Loss: 153, Val Loss: 449,  Lear. Rate: 0.00500, Train Grad.: 2.2\n",
      "Epoch 41501/150000, Train Loss: 151, Val Loss: 448,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 41601/150000, Train Loss: 148, Val Loss: 448,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 41701/150000, Train Loss: 146, Val Loss: 447,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 41801/150000, Train Loss: 144, Val Loss: 447,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 41901/150000, Train Loss: 141, Val Loss: 447,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 42001/150000, Train Loss: 139, Val Loss: 447,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 42101/150000, Train Loss: 137, Val Loss: 447,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 42201/150000, Train Loss: 135, Val Loss: 448,  Lear. Rate: 0.00500, Train Grad.: 1.8\n",
      "Epoch 42301/150000, Train Loss: 133, Val Loss: 449,  Lear. Rate: 0.00500, Train Grad.: 1.8\n",
      "Epoch 42401/150000, Train Loss: 131, Val Loss: 450,  Lear. Rate: 0.00500, Train Grad.: 1.7\n",
      "Epoch 42501/150000, Train Loss: 129, Val Loss: 451,  Lear. Rate: 0.00500, Train Grad.: 1.7\n",
      "Epoch 42601/150000, Train Loss: 127, Val Loss: 451,  Lear. Rate: 0.00500, Train Grad.: 1.6\n",
      "Epoch 42701/150000, Train Loss: 125, Val Loss: 450,  Lear. Rate: 0.00500, Train Grad.: 1.6\n",
      "Epoch 42801/150000, Train Loss: 123, Val Loss: 449,  Lear. Rate: 0.00500, Train Grad.: 1.6\n",
      "Epoch 42901/150000, Train Loss: 121, Val Loss: 448,  Lear. Rate: 0.00500, Train Grad.: 1.5\n",
      "Epoch 43001/150000, Train Loss: 119, Val Loss: 447,  Lear. Rate: 0.00500, Train Grad.: 1.5\n",
      "Epoch 43101/150000, Train Loss: 118, Val Loss: 446,  Lear. Rate: 0.00500, Train Grad.: 1.4\n",
      "Epoch 43201/150000, Train Loss: 116, Val Loss: 445,  Lear. Rate: 0.00500, Train Grad.: 1.4\n",
      "Epoch 43301/150000, Train Loss: 115, Val Loss: 443,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
      "Epoch 43401/150000, Train Loss: 113, Val Loss: 442,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
      "Epoch 43501/150000, Train Loss: 112, Val Loss: 441,  Lear. Rate: 0.00500, Train Grad.: 1.4\n",
      "Epoch 43601/150000, Train Loss: 110, Val Loss: 440,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 43701/150000, Train Loss: 109, Val Loss: 439,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 43801/150000, Train Loss: 107, Val Loss: 438,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 43901/150000, Train Loss: 106, Val Loss: 436,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 44001/150000, Train Loss: 105, Val Loss: 435,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 44101/150000, Train Loss: 104, Val Loss: 434,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 44201/150000, Train Loss: 102, Val Loss: 433,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 44301/150000, Train Loss: 101, Val Loss: 433,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 44401/150000, Train Loss: 100, Val Loss: 432,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 44501/150000, Train Loss: 99, Val Loss: 432,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 44601/150000, Train Loss: 98, Val Loss: 431,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 44701/150000, Train Loss: 97, Val Loss: 429,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 44801/150000, Train Loss: 94, Val Loss: 423,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 44901/150000, Train Loss: 92, Val Loss: 420,  Lear. Rate: 0.00500, Train Grad.: 0.8\n",
      "Epoch 45001/150000, Train Loss: 91, Val Loss: 418,  Lear. Rate: 0.00500, Train Grad.: 0.8\n",
      "Epoch 45101/150000, Train Loss: 90, Val Loss: 417,  Lear. Rate: 0.00500, Train Grad.: 0.8\n",
      "Epoch 45201/150000, Train Loss: 89, Val Loss: 418,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 45301/150000, Train Loss: 88, Val Loss: 418,  Lear. Rate: 0.00500, Train Grad.: 0.8\n",
      "Epoch 45401/150000, Train Loss: 87, Val Loss: 419,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 45501/150000, Train Loss: 86, Val Loss: 421,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 45601/150000, Train Loss: 85, Val Loss: 425,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 45701/150000, Train Loss: 85, Val Loss: 427,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 45801/150000, Train Loss: 84, Val Loss: 430,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 45901/150000, Train Loss: 83, Val Loss: 433,  Lear. Rate: 0.00500, Train Grad.: 0.6\n",
      "Epoch 46001/150000, Train Loss: 82, Val Loss: 434,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 46101/150000, Train Loss: 82, Val Loss: 437,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 46201/150000, Train Loss: 81, Val Loss: 439,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 46301/150000, Train Loss: 81, Val Loss: 440,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 46401/150000, Train Loss: 80, Val Loss: 442,  Lear. Rate: 0.00500, Train Grad.: 0.4\n",
      "Epoch 46501/150000, Train Loss: 79, Val Loss: 444,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 46601/150000, Train Loss: 79, Val Loss: 446,  Lear. Rate: 0.00500, Train Grad.: 0.4\n",
      "Epoch 46701/150000, Train Loss: 78, Val Loss: 448,  Lear. Rate: 0.00500, Train Grad.: 0.4\n",
      "Epoch 46801/150000, Train Loss: 78, Val Loss: 450,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 46901/150000, Train Loss: 78, Val Loss: 454,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 47001/150000, Train Loss: 77, Val Loss: 456,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 47101/150000, Train Loss: 77, Val Loss: 458,  Lear. Rate: 0.00500, Train Grad.: 0.4\n",
      "Epoch 47201/150000, Train Loss: 76, Val Loss: 461,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 47301/150000, Train Loss: 76, Val Loss: 464,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 47401/150000, Train Loss: 76, Val Loss: 466,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 47501/150000, Train Loss: 76, Val Loss: 468,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 47601/150000, Train Loss: 75, Val Loss: 470,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 47701/150000, Train Loss: 75, Val Loss: 471,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 47801/150000, Train Loss: 75, Val Loss: 472,  Lear. Rate: 0.00500, Train Grad.: 0.3\n",
      "Epoch 47901/150000, Train Loss: 74, Val Loss: 473,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 48001/150000, Train Loss: 74, Val Loss: 475,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 48101/150000, Train Loss: 74, Val Loss: 477,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 48201/150000, Train Loss: 74, Val Loss: 477,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 48301/150000, Train Loss: 74, Val Loss: 478,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 48401/150000, Train Loss: 73, Val Loss: 479,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 48501/150000, Train Loss: 73, Val Loss: 479,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 48601/150000, Train Loss: 73, Val Loss: 479,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 48701/150000, Train Loss: 73, Val Loss: 479,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 48801/150000, Train Loss: 73, Val Loss: 479,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 48901/150000, Train Loss: 72, Val Loss: 476,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 49001/150000, Train Loss: 72, Val Loss: 472,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 49101/150000, Train Loss: 72, Val Loss: 470,  Lear. Rate: 0.00500, Train Grad.: 0.3\n",
      "Epoch 49201/150000, Train Loss: 72, Val Loss: 469,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 49301/150000, Train Loss: 72, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 49401/150000, Train Loss: 71, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 49501/150000, Train Loss: 71, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 49601/150000, Train Loss: 71, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 49701/150000, Train Loss: 71, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 49801/150000, Train Loss: 71, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 49901/150000, Train Loss: 71, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 50001/150000, Train Loss: 71, Val Loss: 466,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 50101/150000, Train Loss: 70, Val Loss: 466,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 50201/150000, Train Loss: 70, Val Loss: 465,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 50301/150000, Train Loss: 70, Val Loss: 465,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 50401/150000, Train Loss: 70, Val Loss: 463,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 50501/150000, Train Loss: 70, Val Loss: 461,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 50601/150000, Train Loss: 70, Val Loss: 461,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 50701/150000, Train Loss: 70, Val Loss: 461,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 50801/150000, Train Loss: 69, Val Loss: 460,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 50901/150000, Train Loss: 69, Val Loss: 460,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 51001/150000, Train Loss: 69, Val Loss: 459,  Lear. Rate: 0.00500, Train Grad.: 0.4\n",
      "Epoch 51101/150000, Train Loss: 69, Val Loss: 456,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 51201/150000, Train Loss: 69, Val Loss: 453,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 51301/150000, Train Loss: 68, Val Loss: 450,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 51401/150000, Train Loss: 68, Val Loss: 449,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 51501/150000, Train Loss: 68, Val Loss: 447,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 51601/150000, Train Loss: 68, Val Loss: 446,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 51701/150000, Train Loss: 67, Val Loss: 445,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 51801/150000, Train Loss: 67, Val Loss: 444,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 51901/150000, Train Loss: 67, Val Loss: 444,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 52001/150000, Train Loss: 66, Val Loss: 444,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 52101/150000, Train Loss: 66, Val Loss: 446,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 52201/150000, Train Loss: 66, Val Loss: 447,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 52301/150000, Train Loss: 66, Val Loss: 448,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 52401/150000, Train Loss: 65, Val Loss: 448,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 52501/150000, Train Loss: 65, Val Loss: 449,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 52601/150000, Train Loss: 65, Val Loss: 450,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 52701/150000, Train Loss: 65, Val Loss: 450,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 52801/150000, Train Loss: 65, Val Loss: 451,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 52901/150000, Train Loss: 64, Val Loss: 452,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 53001/150000, Train Loss: 64, Val Loss: 453,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 53101/150000, Train Loss: 64, Val Loss: 454,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 53201/150000, Train Loss: 64, Val Loss: 454,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 53301/150000, Train Loss: 64, Val Loss: 455,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 53401/150000, Train Loss: 64, Val Loss: 456,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 53501/150000, Train Loss: 63, Val Loss: 457,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 53601/150000, Train Loss: 63, Val Loss: 458,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 53701/150000, Train Loss: 63, Val Loss: 458,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 53801/150000, Train Loss: 63, Val Loss: 459,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 53901/150000, Train Loss: 63, Val Loss: 459,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 54001/150000, Train Loss: 63, Val Loss: 461,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 54101/150000, Train Loss: 63, Val Loss: 461,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 54201/150000, Train Loss: 62, Val Loss: 462,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 54301/150000, Train Loss: 62, Val Loss: 463,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 54401/150000, Train Loss: 62, Val Loss: 464,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 54501/150000, Train Loss: 62, Val Loss: 463,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 54601/150000, Train Loss: 62, Val Loss: 464,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 54701/150000, Train Loss: 62, Val Loss: 465,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 54801/150000, Train Loss: 62, Val Loss: 465,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 54901/150000, Train Loss: 61, Val Loss: 466,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 55001/150000, Train Loss: 61, Val Loss: 466,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 55101/150000, Train Loss: 61, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 55201/150000, Train Loss: 61, Val Loss: 466,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 55301/150000, Train Loss: 61, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 55401/150000, Train Loss: 61, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 55501/150000, Train Loss: 60, Val Loss: 467,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 55601/150000, Train Loss: 60, Val Loss: 468,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 55701/150000, Train Loss: 60, Val Loss: 468,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 55801/150000, Train Loss: 60, Val Loss: 469,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Epoch 55901/150000, Train Loss: 60, Val Loss: 469,  Lear. Rate: 0.00500, Train Grad.: 0.1\n",
      "Epoch 56001/150000, Train Loss: 59, Val Loss: 471,  Lear. Rate: 0.00500, Train Grad.: 0.0\n",
      "Early stopping at epoch 56084 with validation loss 472.47430419921875.\n",
      "Test Loss: 406.49298095703125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np  # Added this line\n",
    "\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [2]\n",
    "num_layers_list = [1]\n",
    "\n",
    "learning_rates = [0.005]\n",
    "\n",
    "window_sizes = [15, 20, 25]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "factors = [0.5]\n",
    "patience_lr = [10]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "def set_random_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    set_random_seeds(42)\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Lear. Rate: {current_lr:.5f}, Train Grad.: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=2, num_layers=1, learning_rate=0.005, window_size=15, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150000, Train Loss: 29946, Val Loss: 30347,  Lear. Rate: 0.00500, Train Grad.: 224.0\n",
      "Epoch 101/150000, Train Loss: 29482, Val Loss: 29879,  Lear. Rate: 0.00453, Train Grad.: 219.8\n",
      "Epoch 201/150000, Train Loss: 29135, Val Loss: 29530,  Lear. Rate: 0.00454, Train Grad.: 216.6\n",
      "Epoch 301/150000, Train Loss: 28822, Val Loss: 29214,  Lear. Rate: 0.00455, Train Grad.: 213.7\n",
      "Epoch 401/150000, Train Loss: 28523, Val Loss: 28912,  Lear. Rate: 0.00456, Train Grad.: 210.9\n",
      "Epoch 501/150000, Train Loss: 28233, Val Loss: 28620,  Lear. Rate: 0.00456, Train Grad.: 208.1\n",
      "Epoch 601/150000, Train Loss: 27950, Val Loss: 28334,  Lear. Rate: 0.00457, Train Grad.: 205.4\n",
      "Epoch 701/150000, Train Loss: 27674, Val Loss: 28056,  Lear. Rate: 0.00458, Train Grad.: 202.7\n",
      "Epoch 801/150000, Train Loss: 27404, Val Loss: 27783,  Lear. Rate: 0.00459, Train Grad.: 200.0\n",
      "Epoch 901/150000, Train Loss: 27139, Val Loss: 27515,  Lear. Rate: 0.00460, Train Grad.: 197.3\n",
      "Epoch 1001/150000, Train Loss: 26879, Val Loss: 27252,  Lear. Rate: 0.00460, Train Grad.: 194.7\n",
      "Epoch 1101/150000, Train Loss: 26623, Val Loss: 26995,  Lear. Rate: 0.00461, Train Grad.: 192.0\n",
      "Epoch 1201/150000, Train Loss: 26373, Val Loss: 26742,  Lear. Rate: 0.00462, Train Grad.: 189.4\n",
      "Epoch 1301/150000, Train Loss: 26127, Val Loss: 26493,  Lear. Rate: 0.00462, Train Grad.: 186.8\n",
      "Epoch 1401/150000, Train Loss: 25886, Val Loss: 26249,  Lear. Rate: 0.00463, Train Grad.: 184.2\n",
      "Epoch 1501/150000, Train Loss: 25649, Val Loss: 26010,  Lear. Rate: 0.00464, Train Grad.: 181.6\n",
      "Epoch 1601/150000, Train Loss: 25416, Val Loss: 25775,  Lear. Rate: 0.00464, Train Grad.: 179.0\n",
      "Epoch 1701/150000, Train Loss: 25187, Val Loss: 25544,  Lear. Rate: 0.00465, Train Grad.: 176.4\n",
      "Epoch 1801/150000, Train Loss: 24963, Val Loss: 25317,  Lear. Rate: 0.00466, Train Grad.: 173.9\n",
      "Epoch 1901/150000, Train Loss: 24743, Val Loss: 25094,  Lear. Rate: 0.00466, Train Grad.: 171.3\n",
      "Epoch 2001/150000, Train Loss: 24526, Val Loss: 24875,  Lear. Rate: 0.00467, Train Grad.: 168.8\n",
      "Epoch 2101/150000, Train Loss: 24314, Val Loss: 24660,  Lear. Rate: 0.00467, Train Grad.: 166.2\n",
      "Epoch 2201/150000, Train Loss: 24105, Val Loss: 24449,  Lear. Rate: 0.00468, Train Grad.: 163.7\n",
      "Epoch 2301/150000, Train Loss: 23901, Val Loss: 24242,  Lear. Rate: 0.00468, Train Grad.: 161.2\n",
      "Epoch 2401/150000, Train Loss: 23700, Val Loss: 24039,  Lear. Rate: 0.00469, Train Grad.: 158.7\n",
      "Epoch 2501/150000, Train Loss: 23503, Val Loss: 23840,  Lear. Rate: 0.00469, Train Grad.: 156.2\n",
      "Epoch 2601/150000, Train Loss: 23310, Val Loss: 23644,  Lear. Rate: 0.00470, Train Grad.: 153.7\n",
      "Epoch 2701/150000, Train Loss: 23120, Val Loss: 23452,  Lear. Rate: 0.00470, Train Grad.: 151.2\n",
      "Epoch 2801/150000, Train Loss: 22935, Val Loss: 23264,  Lear. Rate: 0.00471, Train Grad.: 148.7\n",
      "Epoch 2901/150000, Train Loss: 22752, Val Loss: 23079,  Lear. Rate: 0.00471, Train Grad.: 146.2\n",
      "Epoch 3001/150000, Train Loss: 22574, Val Loss: 22898,  Lear. Rate: 0.00472, Train Grad.: 143.8\n",
      "Epoch 3101/150000, Train Loss: 22282, Val Loss: 22620,  Lear. Rate: 0.00472, Train Grad.: 152.8\n",
      "Epoch 3201/150000, Train Loss: 22080, Val Loss: 22416,  Lear. Rate: 0.00473, Train Grad.: 151.4\n",
      "Epoch 3301/150000, Train Loss: 21881, Val Loss: 22215,  Lear. Rate: 0.00473, Train Grad.: 150.0\n",
      "Epoch 3401/150000, Train Loss: 21685, Val Loss: 22017,  Lear. Rate: 0.00474, Train Grad.: 148.5\n",
      "Epoch 3501/150000, Train Loss: 21490, Val Loss: 21821,  Lear. Rate: 0.00474, Train Grad.: 147.1\n",
      "Epoch 3601/150000, Train Loss: 21297, Val Loss: 21626,  Lear. Rate: 0.00475, Train Grad.: 145.7\n",
      "Epoch 3701/150000, Train Loss: 21107, Val Loss: 21434,  Lear. Rate: 0.00475, Train Grad.: 144.4\n",
      "Epoch 3801/150000, Train Loss: 20918, Val Loss: 21243,  Lear. Rate: 0.00476, Train Grad.: 143.0\n",
      "Epoch 3901/150000, Train Loss: 20731, Val Loss: 21054,  Lear. Rate: 0.00476, Train Grad.: 141.7\n",
      "Epoch 4001/150000, Train Loss: 20546, Val Loss: 20866,  Lear. Rate: 0.00476, Train Grad.: 140.4\n",
      "Epoch 4101/150000, Train Loss: 20363, Val Loss: 20681,  Lear. Rate: 0.00477, Train Grad.: 139.2\n",
      "Epoch 4201/150000, Train Loss: 20181, Val Loss: 20497,  Lear. Rate: 0.00477, Train Grad.: 137.9\n",
      "Epoch 4301/150000, Train Loss: 20001, Val Loss: 20314,  Lear. Rate: 0.00478, Train Grad.: 136.7\n",
      "Epoch 4401/150000, Train Loss: 19822, Val Loss: 20134,  Lear. Rate: 0.00478, Train Grad.: 135.4\n",
      "Epoch 4501/150000, Train Loss: 19645, Val Loss: 19955,  Lear. Rate: 0.00478, Train Grad.: 134.2\n",
      "Epoch 4601/150000, Train Loss: 19470, Val Loss: 19777,  Lear. Rate: 0.00479, Train Grad.: 133.0\n",
      "Epoch 4701/150000, Train Loss: 19296, Val Loss: 19601,  Lear. Rate: 0.00479, Train Grad.: 131.8\n",
      "Epoch 4801/150000, Train Loss: 19123, Val Loss: 19426,  Lear. Rate: 0.00480, Train Grad.: 130.7\n",
      "Epoch 4901/150000, Train Loss: 18952, Val Loss: 19253,  Lear. Rate: 0.00480, Train Grad.: 129.5\n",
      "Epoch 5001/150000, Train Loss: 18783, Val Loss: 19081,  Lear. Rate: 0.00480, Train Grad.: 128.4\n",
      "Epoch 5101/150000, Train Loss: 18615, Val Loss: 18910,  Lear. Rate: 0.00481, Train Grad.: 127.2\n",
      "Epoch 5201/150000, Train Loss: 18448, Val Loss: 18741,  Lear. Rate: 0.00481, Train Grad.: 126.1\n",
      "Epoch 5301/150000, Train Loss: 18283, Val Loss: 18574,  Lear. Rate: 0.00481, Train Grad.: 125.0\n",
      "Epoch 5401/150000, Train Loss: 18119, Val Loss: 18408,  Lear. Rate: 0.00482, Train Grad.: 123.9\n",
      "Epoch 5501/150000, Train Loss: 17957, Val Loss: 18243,  Lear. Rate: 0.00482, Train Grad.: 122.8\n",
      "Epoch 5601/150000, Train Loss: 17796, Val Loss: 18080,  Lear. Rate: 0.00482, Train Grad.: 121.7\n",
      "Epoch 5701/150000, Train Loss: 17636, Val Loss: 17918,  Lear. Rate: 0.00483, Train Grad.: 120.6\n",
      "Epoch 5801/150000, Train Loss: 17478, Val Loss: 17758,  Lear. Rate: 0.00483, Train Grad.: 119.5\n",
      "Epoch 5901/150000, Train Loss: 17321, Val Loss: 17599,  Lear. Rate: 0.00483, Train Grad.: 118.4\n",
      "Epoch 6001/150000, Train Loss: 17165, Val Loss: 17441,  Lear. Rate: 0.00484, Train Grad.: 117.3\n",
      "Epoch 6101/150000, Train Loss: 17011, Val Loss: 17285,  Lear. Rate: 0.00484, Train Grad.: 116.3\n",
      "Epoch 6201/150000, Train Loss: 16858, Val Loss: 17131,  Lear. Rate: 0.00484, Train Grad.: 115.2\n",
      "Epoch 6301/150000, Train Loss: 16707, Val Loss: 16977,  Lear. Rate: 0.00484, Train Grad.: 114.1\n",
      "Epoch 6401/150000, Train Loss: 16557, Val Loss: 16825,  Lear. Rate: 0.00485, Train Grad.: 113.1\n",
      "Epoch 6501/150000, Train Loss: 16409, Val Loss: 16675,  Lear. Rate: 0.00485, Train Grad.: 112.0\n",
      "Epoch 6601/150000, Train Loss: 16261, Val Loss: 16526,  Lear. Rate: 0.00485, Train Grad.: 110.9\n",
      "Epoch 6701/150000, Train Loss: 16116, Val Loss: 16378,  Lear. Rate: 0.00485, Train Grad.: 109.9\n",
      "Epoch 6801/150000, Train Loss: 15971, Val Loss: 16232,  Lear. Rate: 0.00486, Train Grad.: 108.9\n",
      "Epoch 6901/150000, Train Loss: 15828, Val Loss: 16087,  Lear. Rate: 0.00486, Train Grad.: 107.8\n",
      "Epoch 7001/150000, Train Loss: 15687, Val Loss: 15944,  Lear. Rate: 0.00486, Train Grad.: 106.8\n",
      "Epoch 7101/150000, Train Loss: 15547, Val Loss: 15802,  Lear. Rate: 0.00486, Train Grad.: 105.8\n",
      "Epoch 7201/150000, Train Loss: 15408, Val Loss: 15661,  Lear. Rate: 0.00487, Train Grad.: 104.7\n",
      "Epoch 7301/150000, Train Loss: 15270, Val Loss: 15522,  Lear. Rate: 0.00487, Train Grad.: 103.7\n",
      "Epoch 7401/150000, Train Loss: 15134, Val Loss: 15384,  Lear. Rate: 0.00487, Train Grad.: 102.7\n",
      "Epoch 7501/150000, Train Loss: 14999, Val Loss: 15248,  Lear. Rate: 0.00487, Train Grad.: 101.7\n",
      "Epoch 7601/150000, Train Loss: 14865, Val Loss: 15113,  Lear. Rate: 0.00488, Train Grad.: 100.7\n",
      "Epoch 7701/150000, Train Loss: 14733, Val Loss: 14978,  Lear. Rate: 0.00488, Train Grad.: 99.8\n",
      "Epoch 7801/150000, Train Loss: 14601, Val Loss: 14846,  Lear. Rate: 0.00488, Train Grad.: 98.9\n",
      "Epoch 7901/150000, Train Loss: 14471, Val Loss: 14715,  Lear. Rate: 0.00488, Train Grad.: 98.0\n",
      "Epoch 8001/150000, Train Loss: 14342, Val Loss: 14585,  Lear. Rate: 0.00488, Train Grad.: 97.0\n",
      "Epoch 8101/150000, Train Loss: 14215, Val Loss: 14456,  Lear. Rate: 0.00489, Train Grad.: 96.1\n",
      "Epoch 8201/150000, Train Loss: 14088, Val Loss: 14328,  Lear. Rate: 0.00489, Train Grad.: 95.2\n",
      "Epoch 8301/150000, Train Loss: 13963, Val Loss: 14201,  Lear. Rate: 0.00489, Train Grad.: 94.3\n",
      "Epoch 8401/150000, Train Loss: 13839, Val Loss: 14075,  Lear. Rate: 0.00489, Train Grad.: 93.4\n",
      "Epoch 8501/150000, Train Loss: 13716, Val Loss: 13951,  Lear. Rate: 0.00489, Train Grad.: 92.5\n",
      "Epoch 8601/150000, Train Loss: 13594, Val Loss: 13828,  Lear. Rate: 0.00490, Train Grad.: 91.6\n",
      "Epoch 8701/150000, Train Loss: 13473, Val Loss: 13706,  Lear. Rate: 0.00490, Train Grad.: 90.7\n",
      "Epoch 8801/150000, Train Loss: 13354, Val Loss: 13585,  Lear. Rate: 0.00490, Train Grad.: 89.8\n",
      "Epoch 8901/150000, Train Loss: 13235, Val Loss: 13465,  Lear. Rate: 0.00490, Train Grad.: 89.0\n",
      "Epoch 9001/150000, Train Loss: 13118, Val Loss: 13346,  Lear. Rate: 0.00490, Train Grad.: 88.2\n",
      "Epoch 9101/150000, Train Loss: 13002, Val Loss: 13229,  Lear. Rate: 0.00490, Train Grad.: 87.3\n",
      "Epoch 9201/150000, Train Loss: 12886, Val Loss: 13112,  Lear. Rate: 0.00491, Train Grad.: 86.5\n",
      "Epoch 9301/150000, Train Loss: 12772, Val Loss: 12996,  Lear. Rate: 0.00491, Train Grad.: 85.7\n",
      "Epoch 9401/150000, Train Loss: 12659, Val Loss: 12882,  Lear. Rate: 0.00491, Train Grad.: 84.9\n",
      "Epoch 9501/150000, Train Loss: 12547, Val Loss: 12768,  Lear. Rate: 0.00491, Train Grad.: 84.1\n",
      "Epoch 9601/150000, Train Loss: 12435, Val Loss: 12656,  Lear. Rate: 0.00491, Train Grad.: 83.3\n",
      "Epoch 9701/150000, Train Loss: 12325, Val Loss: 12544,  Lear. Rate: 0.00491, Train Grad.: 82.5\n",
      "Epoch 9801/150000, Train Loss: 12216, Val Loss: 12434,  Lear. Rate: 0.00492, Train Grad.: 81.8\n",
      "Epoch 9901/150000, Train Loss: 12108, Val Loss: 12325,  Lear. Rate: 0.00492, Train Grad.: 80.8\n",
      "Epoch 10001/150000, Train Loss: 12000, Val Loss: 12216,  Lear. Rate: 0.00492, Train Grad.: 80.3\n",
      "Epoch 10101/150000, Train Loss: 11894, Val Loss: 12109,  Lear. Rate: 0.00492, Train Grad.: 79.6\n",
      "Epoch 10201/150000, Train Loss: 11788, Val Loss: 12003,  Lear. Rate: 0.00492, Train Grad.: 78.9\n",
      "Epoch 10301/150000, Train Loss: 11683, Val Loss: 11897,  Lear. Rate: 0.00492, Train Grad.: 78.2\n",
      "Epoch 10401/150000, Train Loss: 11580, Val Loss: 11792,  Lear. Rate: 0.00492, Train Grad.: 77.4\n",
      "Epoch 10501/150000, Train Loss: 11477, Val Loss: 11688,  Lear. Rate: 0.00493, Train Grad.: 76.7\n",
      "Epoch 10601/150000, Train Loss: 11375, Val Loss: 11585,  Lear. Rate: 0.00493, Train Grad.: 75.8\n",
      "Epoch 10701/150000, Train Loss: 11274, Val Loss: 11483,  Lear. Rate: 0.00493, Train Grad.: 75.3\n",
      "Epoch 10801/150000, Train Loss: 11173, Val Loss: 11383,  Lear. Rate: 0.00493, Train Grad.: 74.6\n",
      "Epoch 10901/150000, Train Loss: 11074, Val Loss: 11282,  Lear. Rate: 0.00493, Train Grad.: 74.0\n",
      "Epoch 11001/150000, Train Loss: 10976, Val Loss: 11183,  Lear. Rate: 0.00493, Train Grad.: 73.2\n",
      "Epoch 11101/150000, Train Loss: 10878, Val Loss: 11084,  Lear. Rate: 0.00493, Train Grad.: 72.7\n",
      "Epoch 11201/150000, Train Loss: 10781, Val Loss: 10987,  Lear. Rate: 0.00493, Train Grad.: 72.0\n",
      "Epoch 11301/150000, Train Loss: 10685, Val Loss: 10889,  Lear. Rate: 0.00494, Train Grad.: 71.4\n",
      "Epoch 11401/150000, Train Loss: 10590, Val Loss: 10793,  Lear. Rate: 0.00494, Train Grad.: 70.7\n",
      "Epoch 11501/150000, Train Loss: 10495, Val Loss: 10698,  Lear. Rate: 0.00494, Train Grad.: 70.2\n",
      "Epoch 11601/150000, Train Loss: 10401, Val Loss: 10604,  Lear. Rate: 0.00494, Train Grad.: 69.6\n",
      "Epoch 11701/150000, Train Loss: 10308, Val Loss: 10510,  Lear. Rate: 0.00494, Train Grad.: 68.9\n",
      "Epoch 11801/150000, Train Loss: 10216, Val Loss: 10416,  Lear. Rate: 0.00494, Train Grad.: 68.4\n",
      "Epoch 11901/150000, Train Loss: 10125, Val Loss: 10324,  Lear. Rate: 0.00494, Train Grad.: 67.8\n",
      "Epoch 12001/150000, Train Loss: 10034, Val Loss: 10232,  Lear. Rate: 0.00494, Train Grad.: 67.1\n",
      "Epoch 12101/150000, Train Loss: 9944, Val Loss: 10141,  Lear. Rate: 0.00494, Train Grad.: 66.7\n",
      "Epoch 12201/150000, Train Loss: 9854, Val Loss: 10051,  Lear. Rate: 0.00494, Train Grad.: 66.1\n",
      "Epoch 12301/150000, Train Loss: 9766, Val Loss: 9962,  Lear. Rate: 0.00495, Train Grad.: 65.5\n",
      "Epoch 12401/150000, Train Loss: 9678, Val Loss: 9873,  Lear. Rate: 0.00495, Train Grad.: 65.0\n",
      "Epoch 12501/150000, Train Loss: 9591, Val Loss: 9785,  Lear. Rate: 0.00495, Train Grad.: 64.4\n",
      "Epoch 12601/150000, Train Loss: 9504, Val Loss: 9698,  Lear. Rate: 0.00495, Train Grad.: 63.7\n",
      "Epoch 12701/150000, Train Loss: 9419, Val Loss: 9611,  Lear. Rate: 0.00495, Train Grad.: 63.1\n",
      "Epoch 12801/150000, Train Loss: 9333, Val Loss: 9524,  Lear. Rate: 0.00495, Train Grad.: 62.8\n",
      "Epoch 12901/150000, Train Loss: 9249, Val Loss: 9439,  Lear. Rate: 0.00495, Train Grad.: 62.3\n",
      "Epoch 13001/150000, Train Loss: 9165, Val Loss: 9352,  Lear. Rate: 0.00495, Train Grad.: 61.8\n",
      "Epoch 13101/150000, Train Loss: 9082, Val Loss: 9268,  Lear. Rate: 0.00495, Train Grad.: 61.3\n",
      "Epoch 13201/150000, Train Loss: 8999, Val Loss: 9185,  Lear. Rate: 0.00495, Train Grad.: 60.9\n",
      "Epoch 13301/150000, Train Loss: 8917, Val Loss: 9102,  Lear. Rate: 0.00495, Train Grad.: 60.5\n",
      "Epoch 13401/150000, Train Loss: 8835, Val Loss: 9019,  Lear. Rate: 0.00496, Train Grad.: 59.9\n",
      "Epoch 13501/150000, Train Loss: 8754, Val Loss: 8938,  Lear. Rate: 0.00496, Train Grad.: 59.5\n",
      "Epoch 13601/150000, Train Loss: 8673, Val Loss: 8857,  Lear. Rate: 0.00496, Train Grad.: 59.0\n",
      "Epoch 13701/150000, Train Loss: 8593, Val Loss: 8776,  Lear. Rate: 0.00496, Train Grad.: 58.5\n",
      "Epoch 13801/150000, Train Loss: 8514, Val Loss: 8696,  Lear. Rate: 0.00496, Train Grad.: 58.1\n",
      "Epoch 13901/150000, Train Loss: 8435, Val Loss: 8616,  Lear. Rate: 0.00496, Train Grad.: 57.7\n",
      "Epoch 14001/150000, Train Loss: 8357, Val Loss: 8537,  Lear. Rate: 0.00496, Train Grad.: 57.2\n",
      "Epoch 14101/150000, Train Loss: 8279, Val Loss: 8459,  Lear. Rate: 0.00496, Train Grad.: 56.8\n",
      "Epoch 14201/150000, Train Loss: 8202, Val Loss: 8381,  Lear. Rate: 0.00496, Train Grad.: 56.3\n",
      "Epoch 14301/150000, Train Loss: 8125, Val Loss: 8304,  Lear. Rate: 0.00496, Train Grad.: 56.0\n",
      "Epoch 14401/150000, Train Loss: 8049, Val Loss: 8227,  Lear. Rate: 0.00496, Train Grad.: 55.5\n",
      "Epoch 14501/150000, Train Loss: 7973, Val Loss: 8150,  Lear. Rate: 0.00496, Train Grad.: 55.1\n",
      "Epoch 14601/150000, Train Loss: 7898, Val Loss: 8074,  Lear. Rate: 0.00496, Train Grad.: 54.6\n",
      "Epoch 14701/150000, Train Loss: 7823, Val Loss: 7999,  Lear. Rate: 0.00496, Train Grad.: 54.3\n",
      "Epoch 14801/150000, Train Loss: 7749, Val Loss: 7924,  Lear. Rate: 0.00497, Train Grad.: 53.9\n",
      "Epoch 14901/150000, Train Loss: 7675, Val Loss: 7850,  Lear. Rate: 0.00497, Train Grad.: 53.7\n",
      "Epoch 15001/150000, Train Loss: 7602, Val Loss: 7776,  Lear. Rate: 0.00497, Train Grad.: 53.3\n",
      "Epoch 15101/150000, Train Loss: 7529, Val Loss: 7703,  Lear. Rate: 0.00497, Train Grad.: 52.9\n",
      "Epoch 15201/150000, Train Loss: 7457, Val Loss: 7630,  Lear. Rate: 0.00497, Train Grad.: 52.4\n",
      "Epoch 15301/150000, Train Loss: 7385, Val Loss: 7558,  Lear. Rate: 0.00497, Train Grad.: 52.1\n",
      "Epoch 15401/150000, Train Loss: 7314, Val Loss: 7488,  Lear. Rate: 0.00497, Train Grad.: 51.7\n",
      "Epoch 15501/150000, Train Loss: 7243, Val Loss: 7418,  Lear. Rate: 0.00497, Train Grad.: 51.3\n",
      "Epoch 15601/150000, Train Loss: 7173, Val Loss: 7347,  Lear. Rate: 0.00497, Train Grad.: 50.9\n",
      "Epoch 15701/150000, Train Loss: 7103, Val Loss: 7276,  Lear. Rate: 0.00497, Train Grad.: 50.6\n",
      "Epoch 15801/150000, Train Loss: 7034, Val Loss: 7206,  Lear. Rate: 0.00497, Train Grad.: 50.4\n",
      "Epoch 15901/150000, Train Loss: 6965, Val Loss: 7137,  Lear. Rate: 0.00497, Train Grad.: 49.9\n",
      "Epoch 16001/150000, Train Loss: 6896, Val Loss: 7068,  Lear. Rate: 0.00497, Train Grad.: 49.4\n",
      "Epoch 16101/150000, Train Loss: 6828, Val Loss: 7000,  Lear. Rate: 0.00497, Train Grad.: 49.1\n",
      "Epoch 16201/150000, Train Loss: 6761, Val Loss: 6931,  Lear. Rate: 0.00497, Train Grad.: 48.8\n",
      "Epoch 16301/150000, Train Loss: 6693, Val Loss: 6864,  Lear. Rate: 0.00497, Train Grad.: 48.4\n",
      "Epoch 16401/150000, Train Loss: 6627, Val Loss: 6797,  Lear. Rate: 0.00497, Train Grad.: 48.0\n",
      "Epoch 16501/150000, Train Loss: 6561, Val Loss: 6730,  Lear. Rate: 0.00498, Train Grad.: 47.7\n",
      "Epoch 16601/150000, Train Loss: 6495, Val Loss: 6664,  Lear. Rate: 0.00498, Train Grad.: 47.3\n",
      "Epoch 16701/150000, Train Loss: 6430, Val Loss: 6598,  Lear. Rate: 0.00498, Train Grad.: 47.1\n",
      "Epoch 16801/150000, Train Loss: 6365, Val Loss: 6533,  Lear. Rate: 0.00498, Train Grad.: 46.8\n",
      "Epoch 16901/150000, Train Loss: 6301, Val Loss: 6467,  Lear. Rate: 0.00498, Train Grad.: 46.7\n",
      "Epoch 17001/150000, Train Loss: 6237, Val Loss: 6403,  Lear. Rate: 0.00498, Train Grad.: 45.9\n",
      "Epoch 17101/150000, Train Loss: 6173, Val Loss: 6339,  Lear. Rate: 0.00498, Train Grad.: 45.7\n",
      "Epoch 17201/150000, Train Loss: 6110, Val Loss: 6275,  Lear. Rate: 0.00498, Train Grad.: 45.3\n",
      "Epoch 17301/150000, Train Loss: 6048, Val Loss: 6212,  Lear. Rate: 0.00498, Train Grad.: 45.1\n",
      "Epoch 17401/150000, Train Loss: 5986, Val Loss: 6149,  Lear. Rate: 0.00498, Train Grad.: 44.7\n",
      "Epoch 17501/150000, Train Loss: 5924, Val Loss: 6087,  Lear. Rate: 0.00498, Train Grad.: 44.4\n",
      "Epoch 17601/150000, Train Loss: 5863, Val Loss: 6025,  Lear. Rate: 0.00498, Train Grad.: 44.1\n",
      "Epoch 17701/150000, Train Loss: 5802, Val Loss: 5964,  Lear. Rate: 0.00498, Train Grad.: 43.8\n",
      "Epoch 17801/150000, Train Loss: 5742, Val Loss: 5903,  Lear. Rate: 0.00498, Train Grad.: 43.5\n",
      "Epoch 17901/150000, Train Loss: 5682, Val Loss: 5843,  Lear. Rate: 0.00498, Train Grad.: 43.1\n",
      "Epoch 18001/150000, Train Loss: 5623, Val Loss: 5784,  Lear. Rate: 0.00498, Train Grad.: 42.9\n",
      "Epoch 18101/150000, Train Loss: 5564, Val Loss: 5725,  Lear. Rate: 0.00498, Train Grad.: 42.4\n",
      "Epoch 18201/150000, Train Loss: 5506, Val Loss: 5667,  Lear. Rate: 0.00498, Train Grad.: 41.9\n",
      "Epoch 18301/150000, Train Loss: 5448, Val Loss: 5609,  Lear. Rate: 0.00498, Train Grad.: 41.7\n",
      "Epoch 18401/150000, Train Loss: 5391, Val Loss: 5551,  Lear. Rate: 0.00498, Train Grad.: 41.5\n",
      "Epoch 18501/150000, Train Loss: 5334, Val Loss: 5494,  Lear. Rate: 0.00498, Train Grad.: 41.3\n",
      "Epoch 18601/150000, Train Loss: 5277, Val Loss: 5437,  Lear. Rate: 0.00498, Train Grad.: 40.8\n",
      "Epoch 18701/150000, Train Loss: 5221, Val Loss: 5381,  Lear. Rate: 0.00498, Train Grad.: 40.4\n",
      "Epoch 18801/150000, Train Loss: 5165, Val Loss: 5325,  Lear. Rate: 0.00498, Train Grad.: 40.1\n",
      "Epoch 18901/150000, Train Loss: 5110, Val Loss: 5270,  Lear. Rate: 0.00498, Train Grad.: 39.8\n",
      "Epoch 19001/150000, Train Loss: 5056, Val Loss: 5215,  Lear. Rate: 0.00499, Train Grad.: 39.4\n",
      "Epoch 19101/150000, Train Loss: 5001, Val Loss: 5161,  Lear. Rate: 0.00499, Train Grad.: 39.4\n",
      "Epoch 19201/150000, Train Loss: 4947, Val Loss: 5107,  Lear. Rate: 0.00499, Train Grad.: 39.0\n",
      "Epoch 19301/150000, Train Loss: 4894, Val Loss: 5054,  Lear. Rate: 0.00499, Train Grad.: 38.5\n",
      "Epoch 19401/150000, Train Loss: 4841, Val Loss: 5001,  Lear. Rate: 0.00499, Train Grad.: 38.3\n",
      "Epoch 19501/150000, Train Loss: 4788, Val Loss: 4948,  Lear. Rate: 0.00499, Train Grad.: 37.9\n",
      "Epoch 19601/150000, Train Loss: 4736, Val Loss: 4896,  Lear. Rate: 0.00499, Train Grad.: 37.9\n",
      "Epoch 19701/150000, Train Loss: 4684, Val Loss: 4845,  Lear. Rate: 0.00499, Train Grad.: 37.5\n",
      "Epoch 19801/150000, Train Loss: 4633, Val Loss: 4794,  Lear. Rate: 0.00499, Train Grad.: 37.2\n",
      "Epoch 19901/150000, Train Loss: 4581, Val Loss: 4743,  Lear. Rate: 0.00499, Train Grad.: 37.0\n",
      "Epoch 20001/150000, Train Loss: 4531, Val Loss: 4694,  Lear. Rate: 0.00499, Train Grad.: 36.8\n",
      "Epoch 20101/150000, Train Loss: 4480, Val Loss: 4644,  Lear. Rate: 0.00499, Train Grad.: 36.3\n",
      "Epoch 20201/150000, Train Loss: 4430, Val Loss: 4594,  Lear. Rate: 0.00499, Train Grad.: 36.2\n",
      "Epoch 20301/150000, Train Loss: 4381, Val Loss: 4545,  Lear. Rate: 0.00499, Train Grad.: 35.8\n",
      "Epoch 20401/150000, Train Loss: 4332, Val Loss: 4495,  Lear. Rate: 0.00499, Train Grad.: 35.4\n",
      "Epoch 20501/150000, Train Loss: 4283, Val Loss: 4447,  Lear. Rate: 0.00499, Train Grad.: 35.2\n",
      "Epoch 20601/150000, Train Loss: 4235, Val Loss: 4398,  Lear. Rate: 0.00499, Train Grad.: 35.0\n",
      "Epoch 20701/150000, Train Loss: 4187, Val Loss: 4350,  Lear. Rate: 0.00499, Train Grad.: 34.7\n",
      "Epoch 20801/150000, Train Loss: 4139, Val Loss: 4302,  Lear. Rate: 0.00499, Train Grad.: 34.2\n",
      "Epoch 20901/150000, Train Loss: 4092, Val Loss: 4255,  Lear. Rate: 0.00499, Train Grad.: 34.1\n",
      "Epoch 21001/150000, Train Loss: 4045, Val Loss: 4208,  Lear. Rate: 0.00499, Train Grad.: 33.5\n",
      "Epoch 21101/150000, Train Loss: 3999, Val Loss: 4157,  Lear. Rate: 0.00499, Train Grad.: 33.4\n",
      "Epoch 21201/150000, Train Loss: 3953, Val Loss: 4111,  Lear. Rate: 0.00499, Train Grad.: 33.3\n",
      "Epoch 21301/150000, Train Loss: 3908, Val Loss: 4066,  Lear. Rate: 0.00499, Train Grad.: 33.0\n",
      "Epoch 21401/150000, Train Loss: 3862, Val Loss: 4020,  Lear. Rate: 0.00499, Train Grad.: 32.7\n",
      "Epoch 21501/150000, Train Loss: 3817, Val Loss: 3975,  Lear. Rate: 0.00499, Train Grad.: 32.4\n",
      "Epoch 21601/150000, Train Loss: 3773, Val Loss: 3930,  Lear. Rate: 0.00499, Train Grad.: 32.2\n",
      "Epoch 21701/150000, Train Loss: 3729, Val Loss: 3886,  Lear. Rate: 0.00499, Train Grad.: 32.0\n",
      "Epoch 21801/150000, Train Loss: 3685, Val Loss: 3842,  Lear. Rate: 0.00499, Train Grad.: 31.6\n",
      "Epoch 21901/150000, Train Loss: 3642, Val Loss: 3798,  Lear. Rate: 0.00499, Train Grad.: 31.2\n",
      "Epoch 22001/150000, Train Loss: 3599, Val Loss: 3755,  Lear. Rate: 0.00499, Train Grad.: 31.1\n",
      "Epoch 22101/150000, Train Loss: 3557, Val Loss: 3713,  Lear. Rate: 0.00499, Train Grad.: 30.8\n",
      "Epoch 22201/150000, Train Loss: 3515, Val Loss: 3671,  Lear. Rate: 0.00499, Train Grad.: 30.6\n",
      "Epoch 22301/150000, Train Loss: 3473, Val Loss: 3629,  Lear. Rate: 0.00499, Train Grad.: 30.3\n",
      "Epoch 22401/150000, Train Loss: 3432, Val Loss: 3588,  Lear. Rate: 0.00499, Train Grad.: 30.0\n",
      "Epoch 22501/150000, Train Loss: 3391, Val Loss: 3548,  Lear. Rate: 0.00499, Train Grad.: 29.8\n",
      "Epoch 22601/150000, Train Loss: 3350, Val Loss: 3508,  Lear. Rate: 0.00499, Train Grad.: 29.5\n",
      "Epoch 22701/150000, Train Loss: 3310, Val Loss: 3468,  Lear. Rate: 0.00499, Train Grad.: 29.2\n",
      "Epoch 22801/150000, Train Loss: 3270, Val Loss: 3430,  Lear. Rate: 0.00499, Train Grad.: 29.0\n",
      "Epoch 22901/150000, Train Loss: 3231, Val Loss: 3392,  Lear. Rate: 0.00499, Train Grad.: 28.6\n",
      "Epoch 23001/150000, Train Loss: 3191, Val Loss: 3354,  Lear. Rate: 0.00499, Train Grad.: 28.5\n",
      "Epoch 23101/150000, Train Loss: 3153, Val Loss: 3316,  Lear. Rate: 0.00499, Train Grad.: 28.1\n",
      "Epoch 23201/150000, Train Loss: 3114, Val Loss: 3279,  Lear. Rate: 0.00499, Train Grad.: 28.0\n",
      "Epoch 23301/150000, Train Loss: 3077, Val Loss: 3236,  Lear. Rate: 0.00499, Train Grad.: 27.7\n",
      "Epoch 23401/150000, Train Loss: 3039, Val Loss: 3199,  Lear. Rate: 0.00499, Train Grad.: 27.5\n",
      "Epoch 23501/150000, Train Loss: 3001, Val Loss: 3162,  Lear. Rate: 0.00499, Train Grad.: 27.3\n",
      "Epoch 23601/150000, Train Loss: 2964, Val Loss: 3127,  Lear. Rate: 0.00499, Train Grad.: 27.0\n",
      "Epoch 23701/150000, Train Loss: 2928, Val Loss: 3091,  Lear. Rate: 0.00499, Train Grad.: 26.7\n",
      "Epoch 23801/150000, Train Loss: 2891, Val Loss: 3054,  Lear. Rate: 0.00499, Train Grad.: 26.5\n",
      "Epoch 23901/150000, Train Loss: 2856, Val Loss: 3018,  Lear. Rate: 0.00500, Train Grad.: 26.3\n",
      "Epoch 24001/150000, Train Loss: 2820, Val Loss: 2983,  Lear. Rate: 0.00500, Train Grad.: 26.7\n",
      "Epoch 24101/150000, Train Loss: 2785, Val Loss: 2948,  Lear. Rate: 0.00500, Train Grad.: 25.8\n",
      "Epoch 24201/150000, Train Loss: 2750, Val Loss: 2913,  Lear. Rate: 0.00500, Train Grad.: 25.5\n",
      "Epoch 24301/150000, Train Loss: 2715, Val Loss: 2879,  Lear. Rate: 0.00500, Train Grad.: 25.3\n",
      "Epoch 24401/150000, Train Loss: 2681, Val Loss: 2845,  Lear. Rate: 0.00500, Train Grad.: 25.1\n",
      "Epoch 24501/150000, Train Loss: 2647, Val Loss: 2812,  Lear. Rate: 0.00500, Train Grad.: 25.3\n",
      "Epoch 24601/150000, Train Loss: 2613, Val Loss: 2778,  Lear. Rate: 0.00500, Train Grad.: 24.6\n",
      "Epoch 24701/150000, Train Loss: 2580, Val Loss: 2746,  Lear. Rate: 0.00500, Train Grad.: 24.4\n",
      "Epoch 24801/150000, Train Loss: 2547, Val Loss: 2713,  Lear. Rate: 0.00500, Train Grad.: 24.2\n",
      "Epoch 24901/150000, Train Loss: 2514, Val Loss: 2681,  Lear. Rate: 0.00500, Train Grad.: 23.9\n",
      "Epoch 25001/150000, Train Loss: 2482, Val Loss: 2649,  Lear. Rate: 0.00500, Train Grad.: 24.0\n",
      "Epoch 25101/150000, Train Loss: 2450, Val Loss: 2613,  Lear. Rate: 0.00500, Train Grad.: 23.4\n",
      "Epoch 25201/150000, Train Loss: 2418, Val Loss: 2582,  Lear. Rate: 0.00500, Train Grad.: 23.2\n",
      "Epoch 25301/150000, Train Loss: 2387, Val Loss: 2551,  Lear. Rate: 0.00500, Train Grad.: 23.0\n",
      "Epoch 25401/150000, Train Loss: 2356, Val Loss: 2519,  Lear. Rate: 0.00500, Train Grad.: 22.8\n",
      "Epoch 25501/150000, Train Loss: 2325, Val Loss: 2488,  Lear. Rate: 0.00500, Train Grad.: 22.6\n",
      "Epoch 25601/150000, Train Loss: 2294, Val Loss: 2458,  Lear. Rate: 0.00500, Train Grad.: 22.4\n",
      "Epoch 25701/150000, Train Loss: 2264, Val Loss: 2428,  Lear. Rate: 0.00500, Train Grad.: 22.2\n",
      "Epoch 25801/150000, Train Loss: 2234, Val Loss: 2399,  Lear. Rate: 0.00500, Train Grad.: 22.0\n",
      "Epoch 25901/150000, Train Loss: 2205, Val Loss: 2370,  Lear. Rate: 0.00500, Train Grad.: 21.8\n",
      "Epoch 26001/150000, Train Loss: 2175, Val Loss: 2341,  Lear. Rate: 0.00500, Train Grad.: 21.6\n",
      "Epoch 26101/150000, Train Loss: 2146, Val Loss: 2312,  Lear. Rate: 0.00500, Train Grad.: 21.2\n",
      "Epoch 26201/150000, Train Loss: 2117, Val Loss: 2284,  Lear. Rate: 0.00500, Train Grad.: 21.2\n",
      "Epoch 26301/150000, Train Loss: 2089, Val Loss: 2256,  Lear. Rate: 0.00500, Train Grad.: 21.1\n",
      "Epoch 26401/150000, Train Loss: 2060, Val Loss: 2229,  Lear. Rate: 0.00500, Train Grad.: 20.8\n",
      "Epoch 26501/150000, Train Loss: 2032, Val Loss: 2201,  Lear. Rate: 0.00500, Train Grad.: 20.7\n",
      "Epoch 26601/150000, Train Loss: 2005, Val Loss: 2177,  Lear. Rate: 0.00500, Train Grad.: 20.4\n",
      "Epoch 26701/150000, Train Loss: 1977, Val Loss: 2153,  Lear. Rate: 0.00500, Train Grad.: 20.2\n",
      "Epoch 26801/150000, Train Loss: 1950, Val Loss: 2132,  Lear. Rate: 0.00500, Train Grad.: 20.0\n",
      "Epoch 26901/150000, Train Loss: 1923, Val Loss: 2122,  Lear. Rate: 0.00500, Train Grad.: 19.7\n",
      "Epoch 27001/150000, Train Loss: 1896, Val Loss: 2122,  Lear. Rate: 0.00500, Train Grad.: 19.6\n",
      "Early stopping at epoch 27100 with validation loss 2056.615478515625.\n",
      "Test Loss: 1995.321044921875\n",
      "Hyperparameters: input_size=7, hidden_size=2, num_layers=1, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 29964, Val Loss: 30459,  Lear. Rate: 0.00500, Train Grad.: 224.1\n",
      "Epoch 101/150000, Train Loss: 29500, Val Loss: 29990,  Lear. Rate: 0.00452, Train Grad.: 219.9\n",
      "Epoch 201/150000, Train Loss: 29153, Val Loss: 29640,  Lear. Rate: 0.00453, Train Grad.: 216.7\n",
      "Epoch 301/150000, Train Loss: 28840, Val Loss: 29323,  Lear. Rate: 0.00454, Train Grad.: 213.8\n",
      "Epoch 401/150000, Train Loss: 28541, Val Loss: 29021,  Lear. Rate: 0.00455, Train Grad.: 211.0\n",
      "Epoch 501/150000, Train Loss: 28251, Val Loss: 28728,  Lear. Rate: 0.00456, Train Grad.: 208.2\n",
      "Epoch 601/150000, Train Loss: 27969, Val Loss: 28442,  Lear. Rate: 0.00457, Train Grad.: 205.5\n",
      "Epoch 701/150000, Train Loss: 27693, Val Loss: 28162,  Lear. Rate: 0.00458, Train Grad.: 202.8\n",
      "Epoch 801/150000, Train Loss: 27422, Val Loss: 27888,  Lear. Rate: 0.00459, Train Grad.: 200.1\n",
      "Epoch 901/150000, Train Loss: 27157, Val Loss: 27620,  Lear. Rate: 0.00459, Train Grad.: 197.4\n",
      "Epoch 1001/150000, Train Loss: 26897, Val Loss: 27357,  Lear. Rate: 0.00460, Train Grad.: 194.8\n",
      "Epoch 1101/150000, Train Loss: 26642, Val Loss: 27098,  Lear. Rate: 0.00461, Train Grad.: 192.1\n",
      "Epoch 1201/150000, Train Loss: 26391, Val Loss: 26845,  Lear. Rate: 0.00462, Train Grad.: 189.5\n",
      "Epoch 1301/150000, Train Loss: 26145, Val Loss: 26596,  Lear. Rate: 0.00462, Train Grad.: 186.9\n",
      "Epoch 1401/150000, Train Loss: 25904, Val Loss: 26351,  Lear. Rate: 0.00463, Train Grad.: 184.3\n",
      "Epoch 1501/150000, Train Loss: 25667, Val Loss: 26111,  Lear. Rate: 0.00464, Train Grad.: 181.7\n",
      "Epoch 1601/150000, Train Loss: 25434, Val Loss: 25875,  Lear. Rate: 0.00464, Train Grad.: 179.1\n",
      "Epoch 1701/150000, Train Loss: 25206, Val Loss: 25643,  Lear. Rate: 0.00465, Train Grad.: 176.5\n",
      "Epoch 1801/150000, Train Loss: 24981, Val Loss: 25415,  Lear. Rate: 0.00465, Train Grad.: 174.0\n",
      "Epoch 1901/150000, Train Loss: 24761, Val Loss: 25192,  Lear. Rate: 0.00466, Train Grad.: 171.4\n",
      "Epoch 2001/150000, Train Loss: 24544, Val Loss: 24972,  Lear. Rate: 0.00467, Train Grad.: 168.9\n",
      "Epoch 2101/150000, Train Loss: 24332, Val Loss: 24756,  Lear. Rate: 0.00467, Train Grad.: 166.4\n",
      "Epoch 2201/150000, Train Loss: 24123, Val Loss: 24545,  Lear. Rate: 0.00468, Train Grad.: 163.8\n",
      "Epoch 2301/150000, Train Loss: 23919, Val Loss: 24337,  Lear. Rate: 0.00468, Train Grad.: 161.3\n",
      "Epoch 2401/150000, Train Loss: 23718, Val Loss: 24133,  Lear. Rate: 0.00469, Train Grad.: 158.8\n",
      "Epoch 2501/150000, Train Loss: 23521, Val Loss: 23933,  Lear. Rate: 0.00469, Train Grad.: 156.3\n",
      "Epoch 2601/150000, Train Loss: 23327, Val Loss: 23736,  Lear. Rate: 0.00470, Train Grad.: 153.8\n",
      "Epoch 2701/150000, Train Loss: 23138, Val Loss: 23544,  Lear. Rate: 0.00470, Train Grad.: 151.3\n",
      "Epoch 2801/150000, Train Loss: 22952, Val Loss: 23355,  Lear. Rate: 0.00471, Train Grad.: 148.8\n",
      "Epoch 2901/150000, Train Loss: 22769, Val Loss: 23169,  Lear. Rate: 0.00471, Train Grad.: 146.4\n",
      "Epoch 3001/150000, Train Loss: 22486, Val Loss: 22899,  Lear. Rate: 0.00472, Train Grad.: 154.3\n",
      "Epoch 3101/150000, Train Loss: 22284, Val Loss: 22695,  Lear. Rate: 0.00472, Train Grad.: 152.9\n",
      "Epoch 3201/150000, Train Loss: 22084, Val Loss: 22492,  Lear. Rate: 0.00473, Train Grad.: 151.4\n",
      "Epoch 3301/150000, Train Loss: 21886, Val Loss: 22292,  Lear. Rate: 0.00473, Train Grad.: 150.0\n",
      "Epoch 3401/150000, Train Loss: 21691, Val Loss: 22095,  Lear. Rate: 0.00474, Train Grad.: 148.5\n",
      "Epoch 3501/150000, Train Loss: 21497, Val Loss: 21898,  Lear. Rate: 0.00474, Train Grad.: 147.1\n",
      "Epoch 3601/150000, Train Loss: 21305, Val Loss: 21704,  Lear. Rate: 0.00475, Train Grad.: 145.8\n",
      "Epoch 3701/150000, Train Loss: 21116, Val Loss: 21511,  Lear. Rate: 0.00475, Train Grad.: 144.4\n",
      "Epoch 3801/150000, Train Loss: 20928, Val Loss: 21320,  Lear. Rate: 0.00475, Train Grad.: 143.1\n",
      "Epoch 3901/150000, Train Loss: 20741, Val Loss: 21131,  Lear. Rate: 0.00476, Train Grad.: 141.8\n",
      "Epoch 4001/150000, Train Loss: 20557, Val Loss: 20944,  Lear. Rate: 0.00476, Train Grad.: 140.5\n",
      "Epoch 4101/150000, Train Loss: 20374, Val Loss: 20758,  Lear. Rate: 0.00477, Train Grad.: 139.2\n",
      "Epoch 4201/150000, Train Loss: 20192, Val Loss: 20574,  Lear. Rate: 0.00477, Train Grad.: 138.0\n",
      "Epoch 4301/150000, Train Loss: 20013, Val Loss: 20391,  Lear. Rate: 0.00478, Train Grad.: 136.7\n",
      "Epoch 4401/150000, Train Loss: 19834, Val Loss: 20210,  Lear. Rate: 0.00478, Train Grad.: 135.5\n",
      "Epoch 4501/150000, Train Loss: 19658, Val Loss: 20030,  Lear. Rate: 0.00478, Train Grad.: 134.3\n",
      "Epoch 4601/150000, Train Loss: 19482, Val Loss: 19851,  Lear. Rate: 0.00479, Train Grad.: 133.1\n",
      "Epoch 4701/150000, Train Loss: 19309, Val Loss: 19675,  Lear. Rate: 0.00479, Train Grad.: 131.9\n",
      "Epoch 4801/150000, Train Loss: 19136, Val Loss: 19499,  Lear. Rate: 0.00479, Train Grad.: 130.8\n",
      "Epoch 4901/150000, Train Loss: 18965, Val Loss: 19326,  Lear. Rate: 0.00480, Train Grad.: 129.6\n",
      "Epoch 5001/150000, Train Loss: 18796, Val Loss: 19153,  Lear. Rate: 0.00480, Train Grad.: 128.5\n",
      "Epoch 5101/150000, Train Loss: 18628, Val Loss: 18982,  Lear. Rate: 0.00480, Train Grad.: 127.3\n",
      "Epoch 5201/150000, Train Loss: 18461, Val Loss: 18813,  Lear. Rate: 0.00481, Train Grad.: 126.2\n",
      "Epoch 5301/150000, Train Loss: 18296, Val Loss: 18645,  Lear. Rate: 0.00481, Train Grad.: 125.1\n",
      "Epoch 5401/150000, Train Loss: 18133, Val Loss: 18479,  Lear. Rate: 0.00482, Train Grad.: 124.0\n",
      "Epoch 5501/150000, Train Loss: 17970, Val Loss: 18314,  Lear. Rate: 0.00482, Train Grad.: 122.9\n",
      "Epoch 5601/150000, Train Loss: 17809, Val Loss: 18150,  Lear. Rate: 0.00482, Train Grad.: 121.8\n",
      "Epoch 5701/150000, Train Loss: 17650, Val Loss: 17989,  Lear. Rate: 0.00482, Train Grad.: 120.6\n",
      "Epoch 5801/150000, Train Loss: 17491, Val Loss: 17828,  Lear. Rate: 0.00483, Train Grad.: 119.6\n",
      "Epoch 5901/150000, Train Loss: 17335, Val Loss: 17669,  Lear. Rate: 0.00483, Train Grad.: 118.5\n",
      "Epoch 6001/150000, Train Loss: 17179, Val Loss: 17511,  Lear. Rate: 0.00483, Train Grad.: 117.4\n",
      "Epoch 6101/150000, Train Loss: 17025, Val Loss: 17355,  Lear. Rate: 0.00484, Train Grad.: 116.3\n",
      "Epoch 6201/150000, Train Loss: 16872, Val Loss: 17200,  Lear. Rate: 0.00484, Train Grad.: 115.3\n",
      "Epoch 6301/150000, Train Loss: 16721, Val Loss: 17046,  Lear. Rate: 0.00484, Train Grad.: 114.2\n",
      "Epoch 6401/150000, Train Loss: 16571, Val Loss: 16894,  Lear. Rate: 0.00485, Train Grad.: 113.1\n",
      "Epoch 6501/150000, Train Loss: 16423, Val Loss: 16743,  Lear. Rate: 0.00485, Train Grad.: 112.1\n",
      "Epoch 6601/150000, Train Loss: 16275, Val Loss: 16594,  Lear. Rate: 0.00485, Train Grad.: 111.0\n",
      "Epoch 6701/150000, Train Loss: 16130, Val Loss: 16446,  Lear. Rate: 0.00485, Train Grad.: 110.0\n",
      "Epoch 6801/150000, Train Loss: 15985, Val Loss: 16299,  Lear. Rate: 0.00486, Train Grad.: 108.9\n",
      "Epoch 6901/150000, Train Loss: 15842, Val Loss: 16154,  Lear. Rate: 0.00486, Train Grad.: 107.9\n",
      "Epoch 7001/150000, Train Loss: 15700, Val Loss: 16010,  Lear. Rate: 0.00486, Train Grad.: 106.9\n",
      "Epoch 7101/150000, Train Loss: 15560, Val Loss: 15867,  Lear. Rate: 0.00486, Train Grad.: 105.8\n",
      "Epoch 7201/150000, Train Loss: 15421, Val Loss: 15726,  Lear. Rate: 0.00487, Train Grad.: 104.8\n",
      "Epoch 7301/150000, Train Loss: 15283, Val Loss: 15586,  Lear. Rate: 0.00487, Train Grad.: 103.8\n",
      "Epoch 7401/150000, Train Loss: 15147, Val Loss: 15448,  Lear. Rate: 0.00487, Train Grad.: 102.8\n",
      "Epoch 7501/150000, Train Loss: 15012, Val Loss: 15311,  Lear. Rate: 0.00487, Train Grad.: 101.8\n",
      "Epoch 7601/150000, Train Loss: 14878, Val Loss: 15175,  Lear. Rate: 0.00487, Train Grad.: 100.8\n",
      "Epoch 7701/150000, Train Loss: 14746, Val Loss: 15040,  Lear. Rate: 0.00488, Train Grad.: 99.9\n",
      "Epoch 7801/150000, Train Loss: 14614, Val Loss: 14908,  Lear. Rate: 0.00488, Train Grad.: 99.0\n",
      "Epoch 7901/150000, Train Loss: 14484, Val Loss: 14776,  Lear. Rate: 0.00488, Train Grad.: 98.0\n",
      "Epoch 8001/150000, Train Loss: 14355, Val Loss: 14645,  Lear. Rate: 0.00488, Train Grad.: 97.1\n",
      "Epoch 8101/150000, Train Loss: 14227, Val Loss: 14515,  Lear. Rate: 0.00489, Train Grad.: 96.2\n",
      "Epoch 8201/150000, Train Loss: 14101, Val Loss: 14387,  Lear. Rate: 0.00489, Train Grad.: 95.3\n",
      "Epoch 8301/150000, Train Loss: 13975, Val Loss: 14260,  Lear. Rate: 0.00489, Train Grad.: 94.3\n",
      "Epoch 8401/150000, Train Loss: 13851, Val Loss: 14134,  Lear. Rate: 0.00489, Train Grad.: 93.4\n",
      "Epoch 8501/150000, Train Loss: 13728, Val Loss: 14009,  Lear. Rate: 0.00489, Train Grad.: 92.5\n",
      "Epoch 8601/150000, Train Loss: 13606, Val Loss: 13885,  Lear. Rate: 0.00489, Train Grad.: 91.7\n",
      "Epoch 8701/150000, Train Loss: 13485, Val Loss: 13763,  Lear. Rate: 0.00490, Train Grad.: 90.8\n",
      "Epoch 8801/150000, Train Loss: 13366, Val Loss: 13641,  Lear. Rate: 0.00490, Train Grad.: 89.9\n",
      "Epoch 8901/150000, Train Loss: 13247, Val Loss: 13521,  Lear. Rate: 0.00490, Train Grad.: 89.1\n",
      "Epoch 9001/150000, Train Loss: 13130, Val Loss: 13402,  Lear. Rate: 0.00490, Train Grad.: 88.2\n",
      "Epoch 9101/150000, Train Loss: 13013, Val Loss: 13284,  Lear. Rate: 0.00490, Train Grad.: 87.4\n",
      "Epoch 9201/150000, Train Loss: 12898, Val Loss: 13167,  Lear. Rate: 0.00491, Train Grad.: 86.6\n",
      "Early stopping at epoch 9211 with validation loss 13420.31640625.\n",
      "Test Loss: 13268.7529296875\n",
      "Hyperparameters: input_size=7, hidden_size=2, num_layers=1, learning_rate=0.005, window_size=25, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 29982, Val Loss: 30573,  Lear. Rate: 0.00500, Train Grad.: 224.2\n",
      "Epoch 101/150000, Train Loss: 29518, Val Loss: 30103,  Lear. Rate: 0.00452, Train Grad.: 220.0\n",
      "Epoch 201/150000, Train Loss: 29171, Val Loss: 29752,  Lear. Rate: 0.00453, Train Grad.: 216.8\n",
      "Epoch 301/150000, Train Loss: 28858, Val Loss: 29434,  Lear. Rate: 0.00454, Train Grad.: 213.9\n",
      "Epoch 401/150000, Train Loss: 28559, Val Loss: 29131,  Lear. Rate: 0.00455, Train Grad.: 211.1\n",
      "Epoch 501/150000, Train Loss: 28270, Val Loss: 28837,  Lear. Rate: 0.00456, Train Grad.: 208.3\n",
      "Epoch 601/150000, Train Loss: 27987, Val Loss: 28551,  Lear. Rate: 0.00457, Train Grad.: 205.6\n",
      "Epoch 701/150000, Train Loss: 27711, Val Loss: 28270,  Lear. Rate: 0.00457, Train Grad.: 202.9\n",
      "Epoch 801/150000, Train Loss: 27440, Val Loss: 27996,  Lear. Rate: 0.00458, Train Grad.: 200.2\n",
      "Epoch 901/150000, Train Loss: 27175, Val Loss: 27727,  Lear. Rate: 0.00459, Train Grad.: 197.5\n",
      "Epoch 1001/150000, Train Loss: 26915, Val Loss: 27462,  Lear. Rate: 0.00460, Train Grad.: 194.9\n",
      "Epoch 1101/150000, Train Loss: 26660, Val Loss: 27203,  Lear. Rate: 0.00460, Train Grad.: 192.3\n",
      "Epoch 1201/150000, Train Loss: 26410, Val Loss: 26949,  Lear. Rate: 0.00461, Train Grad.: 189.6\n",
      "Epoch 1301/150000, Train Loss: 26164, Val Loss: 26699,  Lear. Rate: 0.00462, Train Grad.: 187.0\n",
      "Epoch 1401/150000, Train Loss: 25922, Val Loss: 26453,  Lear. Rate: 0.00463, Train Grad.: 184.4\n",
      "Epoch 1501/150000, Train Loss: 25685, Val Loss: 26212,  Lear. Rate: 0.00463, Train Grad.: 181.8\n",
      "Epoch 1601/150000, Train Loss: 25452, Val Loss: 25975,  Lear. Rate: 0.00464, Train Grad.: 179.2\n",
      "Epoch 1701/150000, Train Loss: 25224, Val Loss: 25743,  Lear. Rate: 0.00465, Train Grad.: 176.7\n",
      "Epoch 1801/150000, Train Loss: 24999, Val Loss: 25514,  Lear. Rate: 0.00465, Train Grad.: 174.1\n",
      "Epoch 1901/150000, Train Loss: 24779, Val Loss: 25290,  Lear. Rate: 0.00466, Train Grad.: 171.6\n",
      "Epoch 2001/150000, Train Loss: 24562, Val Loss: 25069,  Lear. Rate: 0.00466, Train Grad.: 169.0\n",
      "Epoch 2101/150000, Train Loss: 24350, Val Loss: 24853,  Lear. Rate: 0.00467, Train Grad.: 166.5\n",
      "Epoch 2201/150000, Train Loss: 24141, Val Loss: 24641,  Lear. Rate: 0.00467, Train Grad.: 164.0\n",
      "Epoch 2301/150000, Train Loss: 23936, Val Loss: 24432,  Lear. Rate: 0.00468, Train Grad.: 161.4\n",
      "Epoch 2401/150000, Train Loss: 23735, Val Loss: 24227,  Lear. Rate: 0.00468, Train Grad.: 158.9\n",
      "Epoch 2501/150000, Train Loss: 23538, Val Loss: 24026,  Lear. Rate: 0.00469, Train Grad.: 156.4\n",
      "Epoch 2601/150000, Train Loss: 23345, Val Loss: 23829,  Lear. Rate: 0.00470, Train Grad.: 153.9\n",
      "Epoch 2701/150000, Train Loss: 23155, Val Loss: 23635,  Lear. Rate: 0.00470, Train Grad.: 151.5\n",
      "Epoch 2801/150000, Train Loss: 22969, Val Loss: 23445,  Lear. Rate: 0.00470, Train Grad.: 149.0\n",
      "Epoch 2901/150000, Train Loss: 22701, Val Loss: 23191,  Lear. Rate: 0.00471, Train Grad.: 156.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np  # Added this line\n",
    "\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [2]\n",
    "num_layers_list = [1]\n",
    "\n",
    "learning_rates = [0.005]\n",
    "\n",
    "window_sizes = [15, 20, 25]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "factors = [0.5]\n",
    "patience_lr = [10]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "def set_random_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    set_random_seeds(42)\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Lear. Rate: {current_lr:.5f}, Train Grad.: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.005, window_size=15, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150000, Train Loss: 29872, Val Loss: 30273,  Lear. Rate: 0.00500, Train Grad.: 223.3\n",
      "Epoch 101/150000, Train Loss: 29512, Val Loss: 29909,  Lear. Rate: 0.00453, Train Grad.: 220.1\n",
      "Epoch 201/150000, Train Loss: 28934, Val Loss: 29325,  Lear. Rate: 0.00454, Train Grad.: 214.8\n",
      "Epoch 301/150000, Train Loss: 28483, Val Loss: 28871,  Lear. Rate: 0.00456, Train Grad.: 210.5\n",
      "Epoch 401/150000, Train Loss: 28073, Val Loss: 28457,  Lear. Rate: 0.00457, Train Grad.: 206.6\n",
      "Epoch 501/150000, Train Loss: 27685, Val Loss: 28066,  Lear. Rate: 0.00458, Train Grad.: 202.8\n",
      "Epoch 601/150000, Train Loss: 27313, Val Loss: 27690,  Lear. Rate: 0.00459, Train Grad.: 199.1\n",
      "Epoch 701/150000, Train Loss: 26953, Val Loss: 27327,  Lear. Rate: 0.00460, Train Grad.: 195.4\n",
      "Epoch 801/150000, Train Loss: 26605, Val Loss: 26975,  Lear. Rate: 0.00461, Train Grad.: 191.8\n",
      "Epoch 901/150000, Train Loss: 26267, Val Loss: 26633,  Lear. Rate: 0.00462, Train Grad.: 188.3\n",
      "Epoch 1001/150000, Train Loss: 25938, Val Loss: 26301,  Lear. Rate: 0.00463, Train Grad.: 184.7\n",
      "Epoch 1101/150000, Train Loss: 25618, Val Loss: 25978,  Lear. Rate: 0.00464, Train Grad.: 181.2\n",
      "Epoch 1201/150000, Train Loss: 25307, Val Loss: 25664,  Lear. Rate: 0.00465, Train Grad.: 177.8\n",
      "Epoch 1301/150000, Train Loss: 25004, Val Loss: 25358,  Lear. Rate: 0.00466, Train Grad.: 174.3\n",
      "Epoch 1401/150000, Train Loss: 24710, Val Loss: 25060,  Lear. Rate: 0.00466, Train Grad.: 170.9\n",
      "Epoch 1501/150000, Train Loss: 24423, Val Loss: 24770,  Lear. Rate: 0.00467, Train Grad.: 167.5\n",
      "Epoch 1601/150000, Train Loss: 24143, Val Loss: 24487,  Lear. Rate: 0.00468, Train Grad.: 164.2\n",
      "Epoch 1701/150000, Train Loss: 23872, Val Loss: 24212,  Lear. Rate: 0.00469, Train Grad.: 160.8\n",
      "Epoch 1801/150000, Train Loss: 23607, Val Loss: 23944,  Lear. Rate: 0.00469, Train Grad.: 157.5\n",
      "Epoch 1901/150000, Train Loss: 23349, Val Loss: 23684,  Lear. Rate: 0.00470, Train Grad.: 154.2\n",
      "Epoch 2001/150000, Train Loss: 23099, Val Loss: 23430,  Lear. Rate: 0.00470, Train Grad.: 150.9\n",
      "Epoch 2101/150000, Train Loss: 22855, Val Loss: 23183,  Lear. Rate: 0.00471, Train Grad.: 147.6\n",
      "Epoch 2201/150000, Train Loss: 22618, Val Loss: 22943,  Lear. Rate: 0.00472, Train Grad.: 144.4\n",
      "Epoch 2301/150000, Train Loss: 22388, Val Loss: 22710,  Lear. Rate: 0.00472, Train Grad.: 141.2\n",
      "Epoch 2401/150000, Train Loss: 22164, Val Loss: 22483,  Lear. Rate: 0.00473, Train Grad.: 138.0\n",
      "Epoch 2501/150000, Train Loss: 21947, Val Loss: 22263,  Lear. Rate: 0.00473, Train Grad.: 134.8\n",
      "Epoch 2601/150000, Train Loss: 21570, Val Loss: 21909,  Lear. Rate: 0.00474, Train Grad.: 144.1\n",
      "Epoch 2701/150000, Train Loss: 21301, Val Loss: 21639,  Lear. Rate: 0.00475, Train Grad.: 145.5\n",
      "Epoch 2801/150000, Train Loss: 21044, Val Loss: 21370,  Lear. Rate: 0.00475, Train Grad.: 143.8\n",
      "Epoch 2901/150000, Train Loss: 20792, Val Loss: 21114,  Lear. Rate: 0.00476, Train Grad.: 142.1\n",
      "Epoch 3001/150000, Train Loss: 20545, Val Loss: 20863,  Lear. Rate: 0.00477, Train Grad.: 140.4\n",
      "Epoch 3101/150000, Train Loss: 20301, Val Loss: 20616,  Lear. Rate: 0.00477, Train Grad.: 138.7\n",
      "Epoch 3201/150000, Train Loss: 20061, Val Loss: 20373,  Lear. Rate: 0.00478, Train Grad.: 137.1\n",
      "Epoch 3301/150000, Train Loss: 19824, Val Loss: 20133,  Lear. Rate: 0.00478, Train Grad.: 135.5\n",
      "Epoch 3401/150000, Train Loss: 19591, Val Loss: 19897,  Lear. Rate: 0.00479, Train Grad.: 133.9\n",
      "Epoch 3501/150000, Train Loss: 19360, Val Loss: 19663,  Lear. Rate: 0.00479, Train Grad.: 132.3\n",
      "Epoch 3601/150000, Train Loss: 19133, Val Loss: 19432,  Lear. Rate: 0.00480, Train Grad.: 130.8\n",
      "Epoch 3701/150000, Train Loss: 18908, Val Loss: 19204,  Lear. Rate: 0.00480, Train Grad.: 129.2\n",
      "Epoch 3801/150000, Train Loss: 18686, Val Loss: 18979,  Lear. Rate: 0.00481, Train Grad.: 127.7\n",
      "Epoch 3901/150000, Train Loss: 18467, Val Loss: 18757,  Lear. Rate: 0.00481, Train Grad.: 126.3\n",
      "Epoch 4001/150000, Train Loss: 18251, Val Loss: 18538,  Lear. Rate: 0.00481, Train Grad.: 124.8\n",
      "Epoch 4101/150000, Train Loss: 18037, Val Loss: 18322,  Lear. Rate: 0.00482, Train Grad.: 123.3\n",
      "Epoch 4201/150000, Train Loss: 17826, Val Loss: 18108,  Lear. Rate: 0.00482, Train Grad.: 121.9\n",
      "Epoch 4301/150000, Train Loss: 17617, Val Loss: 17897,  Lear. Rate: 0.00483, Train Grad.: 120.5\n",
      "Epoch 4401/150000, Train Loss: 17411, Val Loss: 17688,  Lear. Rate: 0.00483, Train Grad.: 119.1\n",
      "Epoch 4501/150000, Train Loss: 17207, Val Loss: 17482,  Lear. Rate: 0.00483, Train Grad.: 117.6\n",
      "Epoch 4601/150000, Train Loss: 17006, Val Loss: 17278,  Lear. Rate: 0.00484, Train Grad.: 116.2\n",
      "Epoch 4701/150000, Train Loss: 16808, Val Loss: 17078,  Lear. Rate: 0.00484, Train Grad.: 114.8\n",
      "Epoch 4801/150000, Train Loss: 16612, Val Loss: 16879,  Lear. Rate: 0.00485, Train Grad.: 113.5\n",
      "Epoch 4901/150000, Train Loss: 16418, Val Loss: 16683,  Lear. Rate: 0.00485, Train Grad.: 112.1\n",
      "Epoch 5001/150000, Train Loss: 16227, Val Loss: 16490,  Lear. Rate: 0.00485, Train Grad.: 110.7\n",
      "Epoch 5101/150000, Train Loss: 16038, Val Loss: 16299,  Lear. Rate: 0.00486, Train Grad.: 109.3\n",
      "Epoch 5201/150000, Train Loss: 15852, Val Loss: 16110,  Lear. Rate: 0.00486, Train Grad.: 108.0\n",
      "Epoch 5301/150000, Train Loss: 15668, Val Loss: 15924,  Lear. Rate: 0.00486, Train Grad.: 106.6\n",
      "Epoch 5401/150000, Train Loss: 15487, Val Loss: 15740,  Lear. Rate: 0.00487, Train Grad.: 105.3\n",
      "Epoch 5501/150000, Train Loss: 15308, Val Loss: 15559,  Lear. Rate: 0.00487, Train Grad.: 104.0\n",
      "Epoch 5601/150000, Train Loss: 15131, Val Loss: 15380,  Lear. Rate: 0.00487, Train Grad.: 102.7\n",
      "Epoch 5701/150000, Train Loss: 14956, Val Loss: 15203,  Lear. Rate: 0.00487, Train Grad.: 101.4\n",
      "Epoch 5801/150000, Train Loss: 14784, Val Loss: 15029,  Lear. Rate: 0.00488, Train Grad.: 100.2\n",
      "Epoch 5901/150000, Train Loss: 14613, Val Loss: 14857,  Lear. Rate: 0.00488, Train Grad.: 99.0\n",
      "Epoch 6001/150000, Train Loss: 14445, Val Loss: 14686,  Lear. Rate: 0.00488, Train Grad.: 97.8\n",
      "Epoch 6101/150000, Train Loss: 14279, Val Loss: 14518,  Lear. Rate: 0.00489, Train Grad.: 96.6\n",
      "Epoch 6201/150000, Train Loss: 14115, Val Loss: 14352,  Lear. Rate: 0.00489, Train Grad.: 95.4\n",
      "Epoch 6301/150000, Train Loss: 13953, Val Loss: 14188,  Lear. Rate: 0.00489, Train Grad.: 94.2\n",
      "Epoch 6401/150000, Train Loss: 13793, Val Loss: 14026,  Lear. Rate: 0.00489, Train Grad.: 93.0\n",
      "Epoch 6501/150000, Train Loss: 13634, Val Loss: 13866,  Lear. Rate: 0.00490, Train Grad.: 91.9\n",
      "Epoch 6601/150000, Train Loss: 13478, Val Loss: 13708,  Lear. Rate: 0.00490, Train Grad.: 90.7\n",
      "Epoch 6701/150000, Train Loss: 13324, Val Loss: 13552,  Lear. Rate: 0.00490, Train Grad.: 89.6\n",
      "Epoch 6801/150000, Train Loss: 13172, Val Loss: 13398,  Lear. Rate: 0.00490, Train Grad.: 88.6\n",
      "Epoch 6901/150000, Train Loss: 13021, Val Loss: 13246,  Lear. Rate: 0.00490, Train Grad.: 87.5\n",
      "Epoch 7001/150000, Train Loss: 12872, Val Loss: 13096,  Lear. Rate: 0.00491, Train Grad.: 86.4\n",
      "Epoch 7101/150000, Train Loss: 12725, Val Loss: 12947,  Lear. Rate: 0.00491, Train Grad.: 85.4\n",
      "Epoch 7201/150000, Train Loss: 12580, Val Loss: 12800,  Lear. Rate: 0.00491, Train Grad.: 84.3\n",
      "Epoch 7301/150000, Train Loss: 12436, Val Loss: 12655,  Lear. Rate: 0.00491, Train Grad.: 83.3\n",
      "Epoch 7401/150000, Train Loss: 12294, Val Loss: 12511,  Lear. Rate: 0.00491, Train Grad.: 82.4\n",
      "Epoch 7501/150000, Train Loss: 12154, Val Loss: 12369,  Lear. Rate: 0.00492, Train Grad.: 81.4\n",
      "Epoch 7601/150000, Train Loss: 12015, Val Loss: 12229,  Lear. Rate: 0.00492, Train Grad.: 80.4\n",
      "Epoch 7701/150000, Train Loss: 11877, Val Loss: 12090,  Lear. Rate: 0.00492, Train Grad.: 79.5\n",
      "Epoch 7801/150000, Train Loss: 11742, Val Loss: 11953,  Lear. Rate: 0.00492, Train Grad.: 78.6\n",
      "Epoch 7901/150000, Train Loss: 11607, Val Loss: 11818,  Lear. Rate: 0.00492, Train Grad.: 77.6\n",
      "Epoch 8001/150000, Train Loss: 11474, Val Loss: 11684,  Lear. Rate: 0.00493, Train Grad.: 76.7\n",
      "Epoch 8101/150000, Train Loss: 11343, Val Loss: 11552,  Lear. Rate: 0.00493, Train Grad.: 75.8\n",
      "Epoch 8201/150000, Train Loss: 11213, Val Loss: 11421,  Lear. Rate: 0.00493, Train Grad.: 74.9\n",
      "Epoch 8301/150000, Train Loss: 11085, Val Loss: 11290,  Lear. Rate: 0.00493, Train Grad.: 74.2\n",
      "Epoch 8401/150000, Train Loss: 10958, Val Loss: 11162,  Lear. Rate: 0.00493, Train Grad.: 73.2\n",
      "Epoch 8501/150000, Train Loss: 10832, Val Loss: 11035,  Lear. Rate: 0.00493, Train Grad.: 72.3\n",
      "Epoch 8601/150000, Train Loss: 10708, Val Loss: 10909,  Lear. Rate: 0.00494, Train Grad.: 71.6\n",
      "Epoch 8701/150000, Train Loss: 10585, Val Loss: 10785,  Lear. Rate: 0.00494, Train Grad.: 70.8\n",
      "Epoch 8801/150000, Train Loss: 10463, Val Loss: 10662,  Lear. Rate: 0.00494, Train Grad.: 70.0\n",
      "Epoch 8901/150000, Train Loss: 10343, Val Loss: 10540,  Lear. Rate: 0.00494, Train Grad.: 69.2\n",
      "Epoch 9001/150000, Train Loss: 10224, Val Loss: 10420,  Lear. Rate: 0.00494, Train Grad.: 68.4\n",
      "Epoch 9101/150000, Train Loss: 10106, Val Loss: 10301,  Lear. Rate: 0.00494, Train Grad.: 67.4\n",
      "Epoch 9201/150000, Train Loss: 9989, Val Loss: 10183,  Lear. Rate: 0.00494, Train Grad.: 66.9\n",
      "Epoch 9301/150000, Train Loss: 9874, Val Loss: 10066,  Lear. Rate: 0.00494, Train Grad.: 66.2\n",
      "Epoch 9401/150000, Train Loss: 9759, Val Loss: 9950,  Lear. Rate: 0.00495, Train Grad.: 65.4\n",
      "Epoch 9501/150000, Train Loss: 9646, Val Loss: 9836,  Lear. Rate: 0.00495, Train Grad.: 64.7\n",
      "Epoch 9601/150000, Train Loss: 9534, Val Loss: 9723,  Lear. Rate: 0.00495, Train Grad.: 64.1\n",
      "Epoch 9701/150000, Train Loss: 9423, Val Loss: 9611,  Lear. Rate: 0.00495, Train Grad.: 63.3\n",
      "Epoch 9801/150000, Train Loss: 9313, Val Loss: 9500,  Lear. Rate: 0.00495, Train Grad.: 62.7\n",
      "Epoch 9901/150000, Train Loss: 9205, Val Loss: 9390,  Lear. Rate: 0.00495, Train Grad.: 62.0\n",
      "Epoch 10001/150000, Train Loss: 9097, Val Loss: 9281,  Lear. Rate: 0.00495, Train Grad.: 61.4\n",
      "Epoch 10101/150000, Train Loss: 8990, Val Loss: 9173,  Lear. Rate: 0.00495, Train Grad.: 60.7\n",
      "Epoch 10201/150000, Train Loss: 8884, Val Loss: 9067,  Lear. Rate: 0.00496, Train Grad.: 60.2\n",
      "Epoch 10301/150000, Train Loss: 8779, Val Loss: 8961,  Lear. Rate: 0.00496, Train Grad.: 59.6\n",
      "Epoch 10401/150000, Train Loss: 8675, Val Loss: 8856,  Lear. Rate: 0.00496, Train Grad.: 59.0\n",
      "Epoch 10501/150000, Train Loss: 8572, Val Loss: 8752,  Lear. Rate: 0.00496, Train Grad.: 58.3\n",
      "Epoch 10601/150000, Train Loss: 8469, Val Loss: 8649,  Lear. Rate: 0.00496, Train Grad.: 57.9\n",
      "Epoch 10701/150000, Train Loss: 8368, Val Loss: 8547,  Lear. Rate: 0.00496, Train Grad.: 57.0\n",
      "Epoch 10801/150000, Train Loss: 8267, Val Loss: 8445,  Lear. Rate: 0.00496, Train Grad.: 56.7\n",
      "Epoch 10901/150000, Train Loss: 8168, Val Loss: 8345,  Lear. Rate: 0.00496, Train Grad.: 56.2\n",
      "Epoch 11001/150000, Train Loss: 8069, Val Loss: 8245,  Lear. Rate: 0.00496, Train Grad.: 55.7\n",
      "Epoch 11101/150000, Train Loss: 7971, Val Loss: 8146,  Lear. Rate: 0.00496, Train Grad.: 55.1\n",
      "Epoch 11201/150000, Train Loss: 7873, Val Loss: 8048,  Lear. Rate: 0.00496, Train Grad.: 54.6\n",
      "Epoch 11301/150000, Train Loss: 7777, Val Loss: 7951,  Lear. Rate: 0.00497, Train Grad.: 54.1\n",
      "Epoch 11401/150000, Train Loss: 7682, Val Loss: 7855,  Lear. Rate: 0.00497, Train Grad.: 53.6\n",
      "Epoch 11501/150000, Train Loss: 7587, Val Loss: 7759,  Lear. Rate: 0.00497, Train Grad.: 53.1\n",
      "Epoch 11601/150000, Train Loss: 7493, Val Loss: 7664,  Lear. Rate: 0.00497, Train Grad.: 52.4\n",
      "Epoch 11701/150000, Train Loss: 7400, Val Loss: 7570,  Lear. Rate: 0.00497, Train Grad.: 52.1\n",
      "Epoch 11801/150000, Train Loss: 7307, Val Loss: 7477,  Lear. Rate: 0.00497, Train Grad.: 51.6\n",
      "Epoch 11901/150000, Train Loss: 7216, Val Loss: 7385,  Lear. Rate: 0.00497, Train Grad.: 51.2\n",
      "Epoch 12001/150000, Train Loss: 7125, Val Loss: 7294,  Lear. Rate: 0.00497, Train Grad.: 50.7\n",
      "Epoch 12101/150000, Train Loss: 7035, Val Loss: 7203,  Lear. Rate: 0.00497, Train Grad.: 50.3\n",
      "Epoch 12201/150000, Train Loss: 6946, Val Loss: 7113,  Lear. Rate: 0.00497, Train Grad.: 49.8\n",
      "Epoch 12301/150000, Train Loss: 6857, Val Loss: 7024,  Lear. Rate: 0.00497, Train Grad.: 49.6\n",
      "Epoch 12401/150000, Train Loss: 6769, Val Loss: 6935,  Lear. Rate: 0.00497, Train Grad.: 48.8\n",
      "Epoch 12501/150000, Train Loss: 6682, Val Loss: 6848,  Lear. Rate: 0.00497, Train Grad.: 48.4\n",
      "Epoch 12601/150000, Train Loss: 6596, Val Loss: 6761,  Lear. Rate: 0.00497, Train Grad.: 47.9\n",
      "Epoch 12701/150000, Train Loss: 6511, Val Loss: 6675,  Lear. Rate: 0.00498, Train Grad.: 47.5\n",
      "Epoch 12801/150000, Train Loss: 6426, Val Loss: 6589,  Lear. Rate: 0.00498, Train Grad.: 47.1\n",
      "Epoch 12901/150000, Train Loss: 6342, Val Loss: 6505,  Lear. Rate: 0.00498, Train Grad.: 46.6\n",
      "Epoch 13001/150000, Train Loss: 6259, Val Loss: 6421,  Lear. Rate: 0.00498, Train Grad.: 46.2\n",
      "Epoch 13101/150000, Train Loss: 6177, Val Loss: 6338,  Lear. Rate: 0.00498, Train Grad.: 45.7\n",
      "Epoch 13201/150000, Train Loss: 6095, Val Loss: 6256,  Lear. Rate: 0.00498, Train Grad.: 45.3\n",
      "Epoch 13301/150000, Train Loss: 6014, Val Loss: 6174,  Lear. Rate: 0.00498, Train Grad.: 44.9\n",
      "Epoch 13401/150000, Train Loss: 5934, Val Loss: 6094,  Lear. Rate: 0.00498, Train Grad.: 44.5\n",
      "Epoch 13501/150000, Train Loss: 5855, Val Loss: 6014,  Lear. Rate: 0.00498, Train Grad.: 44.1\n",
      "Epoch 13601/150000, Train Loss: 5776, Val Loss: 5935,  Lear. Rate: 0.00498, Train Grad.: 43.6\n",
      "Epoch 13701/150000, Train Loss: 5698, Val Loss: 5856,  Lear. Rate: 0.00498, Train Grad.: 43.5\n",
      "Epoch 13801/150000, Train Loss: 5621, Val Loss: 5779,  Lear. Rate: 0.00498, Train Grad.: 42.8\n",
      "Epoch 13901/150000, Train Loss: 5545, Val Loss: 5702,  Lear. Rate: 0.00498, Train Grad.: 42.3\n",
      "Epoch 14001/150000, Train Loss: 5470, Val Loss: 5626,  Lear. Rate: 0.00498, Train Grad.: 41.9\n",
      "Epoch 14101/150000, Train Loss: 5395, Val Loss: 5551,  Lear. Rate: 0.00498, Train Grad.: 41.5\n",
      "Epoch 14201/150000, Train Loss: 5321, Val Loss: 5477,  Lear. Rate: 0.00498, Train Grad.: 41.1\n",
      "Epoch 14301/150000, Train Loss: 5248, Val Loss: 5403,  Lear. Rate: 0.00498, Train Grad.: 40.7\n",
      "Epoch 14401/150000, Train Loss: 5175, Val Loss: 5331,  Lear. Rate: 0.00498, Train Grad.: 40.2\n",
      "Epoch 14501/150000, Train Loss: 5104, Val Loss: 5259,  Lear. Rate: 0.00498, Train Grad.: 39.7\n",
      "Epoch 14601/150000, Train Loss: 5033, Val Loss: 5187,  Lear. Rate: 0.00499, Train Grad.: 39.4\n",
      "Epoch 14701/150000, Train Loss: 4963, Val Loss: 5117,  Lear. Rate: 0.00499, Train Grad.: 39.0\n",
      "Epoch 14801/150000, Train Loss: 4893, Val Loss: 5047,  Lear. Rate: 0.00499, Train Grad.: 38.6\n",
      "Epoch 14901/150000, Train Loss: 4825, Val Loss: 4978,  Lear. Rate: 0.00499, Train Grad.: 38.2\n",
      "Epoch 15001/150000, Train Loss: 4756, Val Loss: 4910,  Lear. Rate: 0.00499, Train Grad.: 37.9\n",
      "Epoch 15101/150000, Train Loss: 4689, Val Loss: 4842,  Lear. Rate: 0.00499, Train Grad.: 37.6\n",
      "Epoch 15201/150000, Train Loss: 4622, Val Loss: 4775,  Lear. Rate: 0.00499, Train Grad.: 37.1\n",
      "Epoch 15301/150000, Train Loss: 4556, Val Loss: 4709,  Lear. Rate: 0.00499, Train Grad.: 36.9\n",
      "Epoch 15401/150000, Train Loss: 4490, Val Loss: 4643,  Lear. Rate: 0.00499, Train Grad.: 36.4\n",
      "Epoch 15501/150000, Train Loss: 4425, Val Loss: 4579,  Lear. Rate: 0.00499, Train Grad.: 36.1\n",
      "Epoch 15601/150000, Train Loss: 4361, Val Loss: 4515,  Lear. Rate: 0.00499, Train Grad.: 35.7\n",
      "Epoch 15701/150000, Train Loss: 4298, Val Loss: 4451,  Lear. Rate: 0.00499, Train Grad.: 35.3\n",
      "Epoch 15801/150000, Train Loss: 4235, Val Loss: 4388,  Lear. Rate: 0.00499, Train Grad.: 35.0\n",
      "Epoch 15901/150000, Train Loss: 4173, Val Loss: 4326,  Lear. Rate: 0.00499, Train Grad.: 34.6\n",
      "Epoch 16001/150000, Train Loss: 4111, Val Loss: 4265,  Lear. Rate: 0.00499, Train Grad.: 34.2\n",
      "Epoch 16101/150000, Train Loss: 4050, Val Loss: 4204,  Lear. Rate: 0.00499, Train Grad.: 34.0\n",
      "Epoch 16201/150000, Train Loss: 3990, Val Loss: 4144,  Lear. Rate: 0.00499, Train Grad.: 33.5\n",
      "Epoch 16301/150000, Train Loss: 3931, Val Loss: 4085,  Lear. Rate: 0.00499, Train Grad.: 33.2\n",
      "Epoch 16401/150000, Train Loss: 3872, Val Loss: 4026,  Lear. Rate: 0.00499, Train Grad.: 32.8\n",
      "Epoch 16501/150000, Train Loss: 3814, Val Loss: 3968,  Lear. Rate: 0.00499, Train Grad.: 32.4\n",
      "Epoch 16601/150000, Train Loss: 3756, Val Loss: 3911,  Lear. Rate: 0.00499, Train Grad.: 32.2\n",
      "Epoch 16701/150000, Train Loss: 3700, Val Loss: 3854,  Lear. Rate: 0.00499, Train Grad.: 31.8\n",
      "Epoch 16801/150000, Train Loss: 3643, Val Loss: 3798,  Lear. Rate: 0.00499, Train Grad.: 31.5\n",
      "Epoch 16901/150000, Train Loss: 3588, Val Loss: 3743,  Lear. Rate: 0.00499, Train Grad.: 31.1\n",
      "Epoch 17001/150000, Train Loss: 3533, Val Loss: 3688,  Lear. Rate: 0.00499, Train Grad.: 30.7\n",
      "Epoch 17101/150000, Train Loss: 3479, Val Loss: 3634,  Lear. Rate: 0.00499, Train Grad.: 30.3\n",
      "Epoch 17201/150000, Train Loss: 3425, Val Loss: 3581,  Lear. Rate: 0.00499, Train Grad.: 30.0\n",
      "Epoch 17301/150000, Train Loss: 3372, Val Loss: 3529,  Lear. Rate: 0.00499, Train Grad.: 29.2\n",
      "Epoch 17401/150000, Train Loss: 3320, Val Loss: 3476,  Lear. Rate: 0.00499, Train Grad.: 29.3\n",
      "Epoch 17501/150000, Train Loss: 3268, Val Loss: 3425,  Lear. Rate: 0.00499, Train Grad.: 29.0\n",
      "Epoch 17601/150000, Train Loss: 3217, Val Loss: 3374,  Lear. Rate: 0.00499, Train Grad.: 28.7\n",
      "Epoch 17701/150000, Train Loss: 3167, Val Loss: 3324,  Lear. Rate: 0.00499, Train Grad.: 28.4\n",
      "Epoch 17801/150000, Train Loss: 3117, Val Loss: 3274,  Lear. Rate: 0.00499, Train Grad.: 28.1\n",
      "Epoch 17901/150000, Train Loss: 3068, Val Loss: 3225,  Lear. Rate: 0.00499, Train Grad.: 27.8\n",
      "Epoch 18001/150000, Train Loss: 3019, Val Loss: 3175,  Lear. Rate: 0.00499, Train Grad.: 27.4\n",
      "Epoch 18101/150000, Train Loss: 2971, Val Loss: 3126,  Lear. Rate: 0.00499, Train Grad.: 27.0\n",
      "Epoch 18201/150000, Train Loss: 2923, Val Loss: 3079,  Lear. Rate: 0.00499, Train Grad.: 26.6\n",
      "Epoch 18301/150000, Train Loss: 2877, Val Loss: 3032,  Lear. Rate: 0.00499, Train Grad.: 26.8\n",
      "Epoch 18401/150000, Train Loss: 2830, Val Loss: 2986,  Lear. Rate: 0.00500, Train Grad.: 26.1\n",
      "Epoch 18501/150000, Train Loss: 2785, Val Loss: 2940,  Lear. Rate: 0.00500, Train Grad.: 25.8\n",
      "Epoch 18601/150000, Train Loss: 2739, Val Loss: 2895,  Lear. Rate: 0.00500, Train Grad.: 25.5\n",
      "Epoch 18701/150000, Train Loss: 2695, Val Loss: 2851,  Lear. Rate: 0.00500, Train Grad.: 25.2\n",
      "Epoch 18801/150000, Train Loss: 2651, Val Loss: 2808,  Lear. Rate: 0.00500, Train Grad.: 24.4\n",
      "Epoch 18901/150000, Train Loss: 2607, Val Loss: 2764,  Lear. Rate: 0.00500, Train Grad.: 24.6\n",
      "Epoch 19001/150000, Train Loss: 2565, Val Loss: 2721,  Lear. Rate: 0.00500, Train Grad.: 24.3\n",
      "Epoch 19101/150000, Train Loss: 2522, Val Loss: 2679,  Lear. Rate: 0.00500, Train Grad.: 24.1\n",
      "Epoch 19201/150000, Train Loss: 2480, Val Loss: 2638,  Lear. Rate: 0.00500, Train Grad.: 23.7\n",
      "Epoch 19301/150000, Train Loss: 2439, Val Loss: 2598,  Lear. Rate: 0.00500, Train Grad.: 22.6\n",
      "Epoch 19401/150000, Train Loss: 2398, Val Loss: 2556,  Lear. Rate: 0.00500, Train Grad.: 23.1\n",
      "Epoch 19501/150000, Train Loss: 2358, Val Loss: 2517,  Lear. Rate: 0.00500, Train Grad.: 22.9\n",
      "Epoch 19601/150000, Train Loss: 2318, Val Loss: 2477,  Lear. Rate: 0.00500, Train Grad.: 22.7\n",
      "Epoch 19701/150000, Train Loss: 2279, Val Loss: 2438,  Lear. Rate: 0.00500, Train Grad.: 22.3\n",
      "Epoch 19801/150000, Train Loss: 2240, Val Loss: 2400,  Lear. Rate: 0.00500, Train Grad.: 21.9\n",
      "Epoch 19901/150000, Train Loss: 2202, Val Loss: 2362,  Lear. Rate: 0.00500, Train Grad.: 21.8\n",
      "Epoch 20001/150000, Train Loss: 2164, Val Loss: 2325,  Lear. Rate: 0.00500, Train Grad.: 21.5\n",
      "Epoch 20101/150000, Train Loss: 2127, Val Loss: 2288,  Lear. Rate: 0.00500, Train Grad.: 21.2\n",
      "Epoch 20201/150000, Train Loss: 2090, Val Loss: 2251,  Lear. Rate: 0.00500, Train Grad.: 21.0\n",
      "Epoch 20301/150000, Train Loss: 2053, Val Loss: 2215,  Lear. Rate: 0.00500, Train Grad.: 21.4\n",
      "Epoch 20401/150000, Train Loss: 2017, Val Loss: 2180,  Lear. Rate: 0.00500, Train Grad.: 20.5\n",
      "Epoch 20501/150000, Train Loss: 1982, Val Loss: 2145,  Lear. Rate: 0.00500, Train Grad.: 20.2\n",
      "Epoch 20601/150000, Train Loss: 1947, Val Loss: 2110,  Lear. Rate: 0.00500, Train Grad.: 19.8\n",
      "Epoch 20701/150000, Train Loss: 1912, Val Loss: 2076,  Lear. Rate: 0.00500, Train Grad.: 19.7\n",
      "Epoch 20801/150000, Train Loss: 1878, Val Loss: 2042,  Lear. Rate: 0.00500, Train Grad.: 20.4\n",
      "Epoch 20901/150000, Train Loss: 1844, Val Loss: 2009,  Lear. Rate: 0.00500, Train Grad.: 19.2\n",
      "Epoch 21001/150000, Train Loss: 1811, Val Loss: 1977,  Lear. Rate: 0.00500, Train Grad.: 19.0\n",
      "Epoch 21101/150000, Train Loss: 1778, Val Loss: 1945,  Lear. Rate: 0.00500, Train Grad.: 18.6\n",
      "Epoch 21201/150000, Train Loss: 1746, Val Loss: 1913,  Lear. Rate: 0.00500, Train Grad.: 18.5\n",
      "Epoch 21301/150000, Train Loss: 1714, Val Loss: 1881,  Lear. Rate: 0.00500, Train Grad.: 18.6\n",
      "Epoch 21401/150000, Train Loss: 1683, Val Loss: 1851,  Lear. Rate: 0.00500, Train Grad.: 18.0\n",
      "Epoch 21501/150000, Train Loss: 1652, Val Loss: 1821,  Lear. Rate: 0.00500, Train Grad.: 17.7\n",
      "Epoch 21601/150000, Train Loss: 1621, Val Loss: 1791,  Lear. Rate: 0.00500, Train Grad.: 17.5\n",
      "Epoch 21701/150000, Train Loss: 1591, Val Loss: 1761,  Lear. Rate: 0.00500, Train Grad.: 17.2\n",
      "Epoch 21801/150000, Train Loss: 1562, Val Loss: 1732,  Lear. Rate: 0.00500, Train Grad.: 17.0\n",
      "Epoch 21901/150000, Train Loss: 1533, Val Loss: 1704,  Lear. Rate: 0.00500, Train Grad.: 16.8\n",
      "Epoch 22001/150000, Train Loss: 1504, Val Loss: 1675,  Lear. Rate: 0.00500, Train Grad.: 17.4\n",
      "Epoch 22101/150000, Train Loss: 1476, Val Loss: 1648,  Lear. Rate: 0.00500, Train Grad.: 16.3\n",
      "Epoch 22201/150000, Train Loss: 1448, Val Loss: 1621,  Lear. Rate: 0.00500, Train Grad.: 16.1\n",
      "Epoch 22301/150000, Train Loss: 1420, Val Loss: 1594,  Lear. Rate: 0.00500, Train Grad.: 15.9\n",
      "Epoch 22401/150000, Train Loss: 1393, Val Loss: 1568,  Lear. Rate: 0.00500, Train Grad.: 15.7\n",
      "Epoch 22501/150000, Train Loss: 1366, Val Loss: 1542,  Lear. Rate: 0.00500, Train Grad.: 15.4\n",
      "Epoch 22601/150000, Train Loss: 1340, Val Loss: 1517,  Lear. Rate: 0.00500, Train Grad.: 15.3\n",
      "Epoch 22701/150000, Train Loss: 1314, Val Loss: 1492,  Lear. Rate: 0.00500, Train Grad.: 15.1\n",
      "Epoch 22801/150000, Train Loss: 1288, Val Loss: 1467,  Lear. Rate: 0.00500, Train Grad.: 14.8\n",
      "Epoch 22901/150000, Train Loss: 1263, Val Loss: 1443,  Lear. Rate: 0.00500, Train Grad.: 14.6\n",
      "Epoch 23001/150000, Train Loss: 1238, Val Loss: 1418,  Lear. Rate: 0.00500, Train Grad.: 14.7\n",
      "Epoch 23101/150000, Train Loss: 1213, Val Loss: 1395,  Lear. Rate: 0.00500, Train Grad.: 14.2\n",
      "Epoch 23201/150000, Train Loss: 1189, Val Loss: 1372,  Lear. Rate: 0.00500, Train Grad.: 14.0\n",
      "Epoch 23301/150000, Train Loss: 1166, Val Loss: 1349,  Lear. Rate: 0.00500, Train Grad.: 13.8\n",
      "Epoch 23401/150000, Train Loss: 1142, Val Loss: 1326,  Lear. Rate: 0.00500, Train Grad.: 13.6\n",
      "Epoch 23501/150000, Train Loss: 1119, Val Loss: 1305,  Lear. Rate: 0.00500, Train Grad.: 13.2\n",
      "Epoch 23601/150000, Train Loss: 1096, Val Loss: 1283,  Lear. Rate: 0.00500, Train Grad.: 13.2\n",
      "Epoch 23701/150000, Train Loss: 1074, Val Loss: 1262,  Lear. Rate: 0.00500, Train Grad.: 12.4\n",
      "Epoch 23801/150000, Train Loss: 1052, Val Loss: 1241,  Lear. Rate: 0.00500, Train Grad.: 12.8\n",
      "Epoch 23901/150000, Train Loss: 1030, Val Loss: 1220,  Lear. Rate: 0.00500, Train Grad.: 12.6\n",
      "Epoch 24001/150000, Train Loss: 1009, Val Loss: 1200,  Lear. Rate: 0.00500, Train Grad.: 12.4\n",
      "Epoch 24101/150000, Train Loss: 988, Val Loss: 1181,  Lear. Rate: 0.00500, Train Grad.: 12.3\n",
      "Epoch 24201/150000, Train Loss: 968, Val Loss: 1161,  Lear. Rate: 0.00500, Train Grad.: 11.9\n",
      "Epoch 24301/150000, Train Loss: 947, Val Loss: 1142,  Lear. Rate: 0.00500, Train Grad.: 11.9\n",
      "Epoch 24401/150000, Train Loss: 928, Val Loss: 1124,  Lear. Rate: 0.00500, Train Grad.: 11.9\n",
      "Epoch 24501/150000, Train Loss: 908, Val Loss: 1105,  Lear. Rate: 0.00500, Train Grad.: 11.5\n",
      "Epoch 24601/150000, Train Loss: 889, Val Loss: 1087,  Lear. Rate: 0.00500, Train Grad.: 11.6\n",
      "Epoch 24701/150000, Train Loss: 870, Val Loss: 1070,  Lear. Rate: 0.00500, Train Grad.: 11.1\n",
      "Epoch 24801/150000, Train Loss: 852, Val Loss: 1054,  Lear. Rate: 0.00500, Train Grad.: 9.9\n",
      "Epoch 24901/150000, Train Loss: 833, Val Loss: 1037,  Lear. Rate: 0.00500, Train Grad.: 10.8\n",
      "Epoch 25001/150000, Train Loss: 815, Val Loss: 1020,  Lear. Rate: 0.00500, Train Grad.: 10.6\n",
      "Epoch 25101/150000, Train Loss: 797, Val Loss: 1004,  Lear. Rate: 0.00500, Train Grad.: 10.4\n",
      "Epoch 25201/150000, Train Loss: 780, Val Loss: 988,  Lear. Rate: 0.00500, Train Grad.: 10.3\n",
      "Epoch 25301/150000, Train Loss: 763, Val Loss: 973,  Lear. Rate: 0.00500, Train Grad.: 9.9\n",
      "Epoch 25401/150000, Train Loss: 746, Val Loss: 957,  Lear. Rate: 0.00500, Train Grad.: 9.9\n",
      "Epoch 25501/150000, Train Loss: 730, Val Loss: 942,  Lear. Rate: 0.00500, Train Grad.: 9.8\n",
      "Epoch 25601/150000, Train Loss: 714, Val Loss: 927,  Lear. Rate: 0.00500, Train Grad.: 9.6\n",
      "Epoch 25701/150000, Train Loss: 698, Val Loss: 913,  Lear. Rate: 0.00500, Train Grad.: 9.5\n",
      "Epoch 25801/150000, Train Loss: 682, Val Loss: 898,  Lear. Rate: 0.00500, Train Grad.: 9.6\n",
      "Epoch 25901/150000, Train Loss: 667, Val Loss: 885,  Lear. Rate: 0.00500, Train Grad.: 9.1\n",
      "Epoch 26001/150000, Train Loss: 652, Val Loss: 871,  Lear. Rate: 0.00500, Train Grad.: 9.0\n",
      "Epoch 26101/150000, Train Loss: 637, Val Loss: 858,  Lear. Rate: 0.00500, Train Grad.: 8.9\n",
      "Epoch 26201/150000, Train Loss: 623, Val Loss: 844,  Lear. Rate: 0.00500, Train Grad.: 8.7\n",
      "Epoch 26301/150000, Train Loss: 609, Val Loss: 832,  Lear. Rate: 0.00500, Train Grad.: 8.5\n",
      "Epoch 26401/150000, Train Loss: 595, Val Loss: 819,  Lear. Rate: 0.00500, Train Grad.: 8.4\n",
      "Epoch 26501/150000, Train Loss: 581, Val Loss: 807,  Lear. Rate: 0.00500, Train Grad.: 8.2\n",
      "Epoch 26601/150000, Train Loss: 567, Val Loss: 794,  Lear. Rate: 0.00500, Train Grad.: 8.2\n",
      "Epoch 26701/150000, Train Loss: 554, Val Loss: 783,  Lear. Rate: 0.00500, Train Grad.: 8.0\n",
      "Epoch 26801/150000, Train Loss: 541, Val Loss: 771,  Lear. Rate: 0.00500, Train Grad.: 7.5\n",
      "Epoch 26901/150000, Train Loss: 529, Val Loss: 760,  Lear. Rate: 0.00500, Train Grad.: 7.7\n",
      "Epoch 27001/150000, Train Loss: 516, Val Loss: 748,  Lear. Rate: 0.00500, Train Grad.: 7.5\n",
      "Epoch 27101/150000, Train Loss: 504, Val Loss: 737,  Lear. Rate: 0.00500, Train Grad.: 7.4\n",
      "Epoch 27201/150000, Train Loss: 492, Val Loss: 727,  Lear. Rate: 0.00500, Train Grad.: 7.3\n",
      "Epoch 27301/150000, Train Loss: 480, Val Loss: 716,  Lear. Rate: 0.00500, Train Grad.: 7.1\n",
      "Epoch 27401/150000, Train Loss: 469, Val Loss: 706,  Lear. Rate: 0.00500, Train Grad.: 7.0\n",
      "Epoch 27501/150000, Train Loss: 458, Val Loss: 696,  Lear. Rate: 0.00500, Train Grad.: 6.8\n",
      "Epoch 27601/150000, Train Loss: 447, Val Loss: 686,  Lear. Rate: 0.00500, Train Grad.: 6.7\n",
      "Epoch 27701/150000, Train Loss: 436, Val Loss: 677,  Lear. Rate: 0.00500, Train Grad.: 6.0\n",
      "Epoch 27801/150000, Train Loss: 425, Val Loss: 667,  Lear. Rate: 0.00500, Train Grad.: 6.5\n",
      "Epoch 27901/150000, Train Loss: 415, Val Loss: 658,  Lear. Rate: 0.00500, Train Grad.: 6.3\n",
      "Epoch 28001/150000, Train Loss: 405, Val Loss: 650,  Lear. Rate: 0.00500, Train Grad.: 6.2\n",
      "Epoch 28101/150000, Train Loss: 395, Val Loss: 641,  Lear. Rate: 0.00500, Train Grad.: 5.8\n",
      "Epoch 28201/150000, Train Loss: 386, Val Loss: 633,  Lear. Rate: 0.00500, Train Grad.: 6.0\n",
      "Epoch 28301/150000, Train Loss: 376, Val Loss: 625,  Lear. Rate: 0.00500, Train Grad.: 5.8\n",
      "Epoch 28401/150000, Train Loss: 367, Val Loss: 617,  Lear. Rate: 0.00500, Train Grad.: 5.7\n",
      "Epoch 28501/150000, Train Loss: 358, Val Loss: 609,  Lear. Rate: 0.00500, Train Grad.: 5.8\n",
      "Epoch 28601/150000, Train Loss: 350, Val Loss: 602,  Lear. Rate: 0.00500, Train Grad.: 4.9\n",
      "Epoch 28701/150000, Train Loss: 341, Val Loss: 594,  Lear. Rate: 0.00500, Train Grad.: 5.6\n",
      "Epoch 28801/150000, Train Loss: 333, Val Loss: 587,  Lear. Rate: 0.00500, Train Grad.: 5.2\n",
      "Epoch 28901/150000, Train Loss: 325, Val Loss: 580,  Lear. Rate: 0.00500, Train Grad.: 5.1\n",
      "Epoch 29001/150000, Train Loss: 317, Val Loss: 574,  Lear. Rate: 0.00500, Train Grad.: 5.0\n",
      "Epoch 29101/150000, Train Loss: 309, Val Loss: 567,  Lear. Rate: 0.00500, Train Grad.: 4.8\n",
      "Epoch 29201/150000, Train Loss: 302, Val Loss: 561,  Lear. Rate: 0.00500, Train Grad.: 4.6\n",
      "Epoch 29301/150000, Train Loss: 295, Val Loss: 555,  Lear. Rate: 0.00500, Train Grad.: 4.5\n",
      "Epoch 29401/150000, Train Loss: 288, Val Loss: 549,  Lear. Rate: 0.00500, Train Grad.: 4.5\n",
      "Epoch 29501/150000, Train Loss: 281, Val Loss: 543,  Lear. Rate: 0.00500, Train Grad.: 4.7\n",
      "Epoch 29601/150000, Train Loss: 275, Val Loss: 538,  Lear. Rate: 0.00500, Train Grad.: 4.2\n",
      "Epoch 29701/150000, Train Loss: 269, Val Loss: 533,  Lear. Rate: 0.00500, Train Grad.: 4.1\n",
      "Epoch 29801/150000, Train Loss: 262, Val Loss: 528,  Lear. Rate: 0.00500, Train Grad.: 4.0\n",
      "Epoch 29901/150000, Train Loss: 257, Val Loss: 523,  Lear. Rate: 0.00500, Train Grad.: 3.7\n",
      "Epoch 30001/150000, Train Loss: 250, Val Loss: 518,  Lear. Rate: 0.00500, Train Grad.: 3.9\n",
      "Epoch 30101/150000, Train Loss: 240, Val Loss: 512,  Lear. Rate: 0.00500, Train Grad.: 3.9\n",
      "Epoch 30201/150000, Train Loss: 233, Val Loss: 506,  Lear. Rate: 0.00500, Train Grad.: 3.7\n",
      "Epoch 30301/150000, Train Loss: 227, Val Loss: 501,  Lear. Rate: 0.00500, Train Grad.: 3.6\n",
      "Epoch 30401/150000, Train Loss: 222, Val Loss: 497,  Lear. Rate: 0.00500, Train Grad.: 3.5\n",
      "Epoch 30501/150000, Train Loss: 217, Val Loss: 493,  Lear. Rate: 0.00500, Train Grad.: 3.4\n",
      "Epoch 30601/150000, Train Loss: 212, Val Loss: 488,  Lear. Rate: 0.00500, Train Grad.: 4.1\n",
      "Epoch 30701/150000, Train Loss: 206, Val Loss: 485,  Lear. Rate: 0.00500, Train Grad.: 3.3\n",
      "Epoch 30801/150000, Train Loss: 202, Val Loss: 481,  Lear. Rate: 0.00500, Train Grad.: 3.3\n",
      "Epoch 30901/150000, Train Loss: 197, Val Loss: 478,  Lear. Rate: 0.00500, Train Grad.: 3.2\n",
      "Epoch 31001/150000, Train Loss: 193, Val Loss: 474,  Lear. Rate: 0.00500, Train Grad.: 3.1\n",
      "Epoch 31101/150000, Train Loss: 188, Val Loss: 471,  Lear. Rate: 0.00500, Train Grad.: 3.0\n",
      "Epoch 31201/150000, Train Loss: 184, Val Loss: 468,  Lear. Rate: 0.00500, Train Grad.: 3.0\n",
      "Epoch 31301/150000, Train Loss: 180, Val Loss: 465,  Lear. Rate: 0.00500, Train Grad.: 2.9\n",
      "Epoch 31401/150000, Train Loss: 176, Val Loss: 463,  Lear. Rate: 0.00500, Train Grad.: 2.8\n",
      "Epoch 31501/150000, Train Loss: 172, Val Loss: 460,  Lear. Rate: 0.00500, Train Grad.: 2.7\n",
      "Epoch 31601/150000, Train Loss: 168, Val Loss: 458,  Lear. Rate: 0.00500, Train Grad.: 2.7\n",
      "Epoch 31701/150000, Train Loss: 165, Val Loss: 456,  Lear. Rate: 0.00500, Train Grad.: 2.8\n",
      "Epoch 31801/150000, Train Loss: 161, Val Loss: 454,  Lear. Rate: 0.00500, Train Grad.: 2.4\n",
      "Epoch 31901/150000, Train Loss: 158, Val Loss: 453,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 32001/150000, Train Loss: 155, Val Loss: 451,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 32101/150000, Train Loss: 151, Val Loss: 449,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 32201/150000, Train Loss: 148, Val Loss: 447,  Lear. Rate: 0.00500, Train Grad.: 2.2\n",
      "Epoch 32301/150000, Train Loss: 145, Val Loss: 445,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 32401/150000, Train Loss: 142, Val Loss: 443,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 32501/150000, Train Loss: 140, Val Loss: 442,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 32601/150000, Train Loss: 137, Val Loss: 440,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 32701/150000, Train Loss: 134, Val Loss: 439,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 32801/150000, Train Loss: 132, Val Loss: 438,  Lear. Rate: 0.00500, Train Grad.: 1.8\n",
      "Epoch 32901/150000, Train Loss: 129, Val Loss: 437,  Lear. Rate: 0.00500, Train Grad.: 1.8\n",
      "Epoch 33001/150000, Train Loss: 127, Val Loss: 435,  Lear. Rate: 0.00500, Train Grad.: 1.7\n",
      "Epoch 33101/150000, Train Loss: 124, Val Loss: 434,  Lear. Rate: 0.00500, Train Grad.: 1.7\n",
      "Epoch 33201/150000, Train Loss: 122, Val Loss: 433,  Lear. Rate: 0.00500, Train Grad.: 1.6\n",
      "Epoch 33301/150000, Train Loss: 120, Val Loss: 432,  Lear. Rate: 0.00500, Train Grad.: 1.6\n",
      "Epoch 33401/150000, Train Loss: 118, Val Loss: 431,  Lear. Rate: 0.00500, Train Grad.: 1.5\n",
      "Epoch 33501/150000, Train Loss: 115, Val Loss: 430,  Lear. Rate: 0.00500, Train Grad.: 1.5\n",
      "Epoch 33601/150000, Train Loss: 113, Val Loss: 430,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 33701/150000, Train Loss: 112, Val Loss: 429,  Lear. Rate: 0.00500, Train Grad.: 1.4\n",
      "Epoch 33801/150000, Train Loss: 110, Val Loss: 428,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 33901/150000, Train Loss: 108, Val Loss: 428,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
      "Epoch 34001/150000, Train Loss: 106, Val Loss: 427,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 34101/150000, Train Loss: 105, Val Loss: 427,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 34201/150000, Train Loss: 103, Val Loss: 426,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 34301/150000, Train Loss: 102, Val Loss: 426,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 34401/150000, Train Loss: 100, Val Loss: 425,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 34501/150000, Train Loss: 99, Val Loss: 425,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 34601/150000, Train Loss: 97, Val Loss: 424,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 34701/150000, Train Loss: 96, Val Loss: 424,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 34801/150000, Train Loss: 95, Val Loss: 424,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 34901/150000, Train Loss: 94, Val Loss: 423,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 35001/150000, Train Loss: 93, Val Loss: 423,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 35101/150000, Train Loss: 92, Val Loss: 423,  Lear. Rate: 0.00500, Train Grad.: 0.6\n",
      "Epoch 35201/150000, Train Loss: 91, Val Loss: 422,  Lear. Rate: 0.00500, Train Grad.: 0.8\n",
      "Epoch 35301/150000, Train Loss: 90, Val Loss: 422,  Lear. Rate: 0.00500, Train Grad.: 0.8\n",
      "Epoch 35401/150000, Train Loss: 89, Val Loss: 421,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Early stopping at epoch 35480 with validation loss 424.79327392578125.\n",
      "Test Loss: 398.8932800292969\n",
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.005, window_size=20, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 30105, Val Loss: 30601,  Lear. Rate: 0.00500, Train Grad.: 225.3\n",
      "Epoch 101/150000, Train Loss: 29510, Val Loss: 29994,  Lear. Rate: 0.00452, Train Grad.: 220.2\n",
      "Epoch 201/150000, Train Loss: 28909, Val Loss: 29391,  Lear. Rate: 0.00454, Train Grad.: 214.5\n",
      "Epoch 301/150000, Train Loss: 28461, Val Loss: 28938,  Lear. Rate: 0.00455, Train Grad.: 210.3\n",
      "Epoch 401/150000, Train Loss: 28051, Val Loss: 28524,  Lear. Rate: 0.00457, Train Grad.: 206.3\n",
      "Epoch 501/150000, Train Loss: 27663, Val Loss: 28131,  Lear. Rate: 0.00458, Train Grad.: 202.5\n",
      "Epoch 601/150000, Train Loss: 27290, Val Loss: 27754,  Lear. Rate: 0.00459, Train Grad.: 198.8\n",
      "Epoch 701/150000, Train Loss: 26930, Val Loss: 27390,  Lear. Rate: 0.00460, Train Grad.: 195.1\n",
      "Epoch 801/150000, Train Loss: 26582, Val Loss: 27037,  Lear. Rate: 0.00461, Train Grad.: 191.5\n",
      "Epoch 901/150000, Train Loss: 26244, Val Loss: 26694,  Lear. Rate: 0.00462, Train Grad.: 188.0\n",
      "Epoch 1001/150000, Train Loss: 25916, Val Loss: 26362,  Lear. Rate: 0.00463, Train Grad.: 184.4\n",
      "Epoch 1101/150000, Train Loss: 25596, Val Loss: 26038,  Lear. Rate: 0.00464, Train Grad.: 180.9\n",
      "Epoch 1201/150000, Train Loss: 25285, Val Loss: 25723,  Lear. Rate: 0.00465, Train Grad.: 177.5\n",
      "Epoch 1301/150000, Train Loss: 24983, Val Loss: 25416,  Lear. Rate: 0.00465, Train Grad.: 174.0\n",
      "Epoch 1401/150000, Train Loss: 24689, Val Loss: 25118,  Lear. Rate: 0.00466, Train Grad.: 170.6\n",
      "Epoch 1501/150000, Train Loss: 24402, Val Loss: 24827,  Lear. Rate: 0.00467, Train Grad.: 167.2\n",
      "Epoch 1601/150000, Train Loss: 24123, Val Loss: 24544,  Lear. Rate: 0.00468, Train Grad.: 163.8\n",
      "Epoch 1701/150000, Train Loss: 23852, Val Loss: 24269,  Lear. Rate: 0.00468, Train Grad.: 160.5\n",
      "Epoch 1801/150000, Train Loss: 23588, Val Loss: 24001,  Lear. Rate: 0.00469, Train Grad.: 157.2\n",
      "Epoch 1901/150000, Train Loss: 23331, Val Loss: 23739,  Lear. Rate: 0.00470, Train Grad.: 153.9\n",
      "Epoch 2001/150000, Train Loss: 23081, Val Loss: 23485,  Lear. Rate: 0.00470, Train Grad.: 150.6\n",
      "Epoch 2101/150000, Train Loss: 22838, Val Loss: 23238,  Lear. Rate: 0.00471, Train Grad.: 147.3\n",
      "Epoch 2201/150000, Train Loss: 22602, Val Loss: 22998,  Lear. Rate: 0.00472, Train Grad.: 144.1\n",
      "Epoch 2301/150000, Train Loss: 22372, Val Loss: 22764,  Lear. Rate: 0.00472, Train Grad.: 140.8\n",
      "Epoch 2401/150000, Train Loss: 22149, Val Loss: 22537,  Lear. Rate: 0.00473, Train Grad.: 137.6\n",
      "Epoch 2501/150000, Train Loss: 21767, Val Loss: 22179,  Lear. Rate: 0.00473, Train Grad.: 148.3\n",
      "Epoch 2601/150000, Train Loss: 21505, Val Loss: 21915,  Lear. Rate: 0.00474, Train Grad.: 147.1\n",
      "Epoch 2701/150000, Train Loss: 21250, Val Loss: 21656,  Lear. Rate: 0.00475, Train Grad.: 145.5\n",
      "Epoch 2801/150000, Train Loss: 20999, Val Loss: 21401,  Lear. Rate: 0.00475, Train Grad.: 143.8\n",
      "Epoch 2901/150000, Train Loss: 20751, Val Loss: 21150,  Lear. Rate: 0.00476, Train Grad.: 141.5\n",
      "Epoch 3001/150000, Train Loss: 20507, Val Loss: 20902,  Lear. Rate: 0.00476, Train Grad.: 140.0\n",
      "Epoch 3101/150000, Train Loss: 20266, Val Loss: 20659,  Lear. Rate: 0.00477, Train Grad.: 138.4\n",
      "Epoch 3201/150000, Train Loss: 20029, Val Loss: 20418,  Lear. Rate: 0.00477, Train Grad.: 137.8\n",
      "Epoch 3301/150000, Train Loss: 19794, Val Loss: 20181,  Lear. Rate: 0.00478, Train Grad.: 134.9\n",
      "Epoch 3401/150000, Train Loss: 19563, Val Loss: 19948,  Lear. Rate: 0.00478, Train Grad.: 132.6\n",
      "Epoch 3501/150000, Train Loss: 19334, Val Loss: 19715,  Lear. Rate: 0.00479, Train Grad.: 132.4\n",
      "Epoch 3601/150000, Train Loss: 19108, Val Loss: 19487,  Lear. Rate: 0.00479, Train Grad.: 130.9\n",
      "Epoch 3701/150000, Train Loss: 18885, Val Loss: 19261,  Lear. Rate: 0.00480, Train Grad.: 128.8\n",
      "Epoch 3801/150000, Train Loss: 18664, Val Loss: 19037,  Lear. Rate: 0.00480, Train Grad.: 126.8\n",
      "Epoch 3901/150000, Train Loss: 18447, Val Loss: 18816,  Lear. Rate: 0.00481, Train Grad.: 126.0\n",
      "Epoch 4001/150000, Train Loss: 18231, Val Loss: 18598,  Lear. Rate: 0.00481, Train Grad.: 124.3\n",
      "Epoch 4101/150000, Train Loss: 18019, Val Loss: 18382,  Lear. Rate: 0.00482, Train Grad.: 123.3\n",
      "Epoch 4201/150000, Train Loss: 17809, Val Loss: 18170,  Lear. Rate: 0.00482, Train Grad.: 122.7\n",
      "Epoch 4301/150000, Train Loss: 17601, Val Loss: 17958,  Lear. Rate: 0.00483, Train Grad.: 120.4\n",
      "Epoch 4401/150000, Train Loss: 17396, Val Loss: 17750,  Lear. Rate: 0.00483, Train Grad.: 118.7\n",
      "Epoch 4501/150000, Train Loss: 17193, Val Loss: 17544,  Lear. Rate: 0.00483, Train Grad.: 117.5\n",
      "Epoch 4601/150000, Train Loss: 16993, Val Loss: 17342,  Lear. Rate: 0.00484, Train Grad.: 116.3\n",
      "Epoch 4701/150000, Train Loss: 16795, Val Loss: 17141,  Lear. Rate: 0.00484, Train Grad.: 114.6\n",
      "Epoch 4801/150000, Train Loss: 16600, Val Loss: 16942,  Lear. Rate: 0.00484, Train Grad.: 113.6\n",
      "Epoch 4901/150000, Train Loss: 16407, Val Loss: 16747,  Lear. Rate: 0.00485, Train Grad.: 111.2\n",
      "Epoch 5001/150000, Train Loss: 16217, Val Loss: 16553,  Lear. Rate: 0.00485, Train Grad.: 110.3\n",
      "Epoch 5101/150000, Train Loss: 16028, Val Loss: 16362,  Lear. Rate: 0.00485, Train Grad.: 109.0\n",
      "Epoch 5201/150000, Train Loss: 15843, Val Loss: 16173,  Lear. Rate: 0.00486, Train Grad.: 108.0\n",
      "Epoch 5301/150000, Train Loss: 15660, Val Loss: 15988,  Lear. Rate: 0.00486, Train Grad.: 105.1\n",
      "Epoch 5401/150000, Train Loss: 15478, Val Loss: 15800,  Lear. Rate: 0.00486, Train Grad.: 105.7\n",
      "Epoch 5501/150000, Train Loss: 15299, Val Loss: 15619,  Lear. Rate: 0.00487, Train Grad.: 103.8\n",
      "Epoch 5601/150000, Train Loss: 15123, Val Loss: 15440,  Lear. Rate: 0.00487, Train Grad.: 101.7\n",
      "Epoch 5701/150000, Train Loss: 14948, Val Loss: 15262,  Lear. Rate: 0.00487, Train Grad.: 101.2\n",
      "Epoch 5801/150000, Train Loss: 14776, Val Loss: 15087,  Lear. Rate: 0.00488, Train Grad.: 100.1\n",
      "Epoch 5901/150000, Train Loss: 14608, Val Loss: 14916,  Lear. Rate: 0.00488, Train Grad.: 96.9\n",
      "Epoch 6001/150000, Train Loss: 14439, Val Loss: 14745,  Lear. Rate: 0.00488, Train Grad.: 97.3\n",
      "Epoch 6101/150000, Train Loss: 14274, Val Loss: 14577,  Lear. Rate: 0.00488, Train Grad.: 96.2\n",
      "Epoch 6201/150000, Train Loss: 14111, Val Loss: 14412,  Lear. Rate: 0.00489, Train Grad.: 95.4\n",
      "Epoch 6301/150000, Train Loss: 13949, Val Loss: 14249,  Lear. Rate: 0.00489, Train Grad.: 93.9\n",
      "Epoch 6401/150000, Train Loss: 13789, Val Loss: 14088,  Lear. Rate: 0.00489, Train Grad.: 93.9\n",
      "Epoch 6501/150000, Train Loss: 13631, Val Loss: 13928,  Lear. Rate: 0.00489, Train Grad.: 91.6\n",
      "Epoch 6601/150000, Train Loss: 13475, Val Loss: 13770,  Lear. Rate: 0.00490, Train Grad.: 90.6\n",
      "Epoch 6701/150000, Train Loss: 13321, Val Loss: 13614,  Lear. Rate: 0.00490, Train Grad.: 89.0\n",
      "Epoch 6801/150000, Train Loss: 13169, Val Loss: 13459,  Lear. Rate: 0.00490, Train Grad.: 88.5\n",
      "Epoch 6901/150000, Train Loss: 13019, Val Loss: 13307,  Lear. Rate: 0.00490, Train Grad.: 87.3\n",
      "Epoch 7001/150000, Train Loss: 12870, Val Loss: 13156,  Lear. Rate: 0.00491, Train Grad.: 86.2\n",
      "Epoch 7101/150000, Train Loss: 12723, Val Loss: 13007,  Lear. Rate: 0.00491, Train Grad.: 84.9\n",
      "Epoch 7201/150000, Train Loss: 12578, Val Loss: 12860,  Lear. Rate: 0.00491, Train Grad.: 84.0\n",
      "Epoch 7301/150000, Train Loss: 12434, Val Loss: 12716,  Lear. Rate: 0.00491, Train Grad.: 83.1\n",
      "Epoch 7401/150000, Train Loss: 12292, Val Loss: 12572,  Lear. Rate: 0.00491, Train Grad.: 82.1\n",
      "Epoch 7501/150000, Train Loss: 12151, Val Loss: 12430,  Lear. Rate: 0.00492, Train Grad.: 81.1\n",
      "Epoch 7601/150000, Train Loss: 12013, Val Loss: 12289,  Lear. Rate: 0.00492, Train Grad.: 80.0\n",
      "Epoch 7701/150000, Train Loss: 11875, Val Loss: 12151,  Lear. Rate: 0.00492, Train Grad.: 79.8\n",
      "Epoch 7801/150000, Train Loss: 12230, Val Loss: 13196,  Lear. Rate: 0.00492, Train Grad.: 46.9\n",
      "Early stopping at epoch 7802 with validation loss 12296.3173828125.\n",
      "Test Loss: 12148.6513671875\n",
      "Hyperparameters: input_size=7, hidden_size=3, num_layers=1, learning_rate=0.005, window_size=25, dropout_prob=0, weight_decay=0, factor=0.5, patience=10\n",
      "Epoch 1/150000, Train Loss: 29891, Val Loss: 30482,  Lear. Rate: 0.00500, Train Grad.: 223.3\n",
      "Epoch 101/150000, Train Loss: 29339, Val Loss: 29921,  Lear. Rate: 0.00452, Train Grad.: 218.4\n",
      "Epoch 201/150000, Train Loss: 28892, Val Loss: 29468,  Lear. Rate: 0.00454, Train Grad.: 214.2\n",
      "Epoch 301/150000, Train Loss: 28484, Val Loss: 29054,  Lear. Rate: 0.00455, Train Grad.: 210.4\n",
      "Epoch 401/150000, Train Loss: 28095, Val Loss: 28659,  Lear. Rate: 0.00456, Train Grad.: 206.7\n",
      "Epoch 501/150000, Train Loss: 27720, Val Loss: 28279,  Lear. Rate: 0.00457, Train Grad.: 203.0\n",
      "Epoch 601/150000, Train Loss: 27357, Val Loss: 27910,  Lear. Rate: 0.00458, Train Grad.: 199.4\n",
      "Epoch 701/150000, Train Loss: 27004, Val Loss: 27552,  Lear. Rate: 0.00460, Train Grad.: 195.8\n",
      "Epoch 801/150000, Train Loss: 26661, Val Loss: 27203,  Lear. Rate: 0.00460, Train Grad.: 192.3\n",
      "Epoch 901/150000, Train Loss: 26326, Val Loss: 26863,  Lear. Rate: 0.00461, Train Grad.: 188.7\n",
      "Epoch 1001/150000, Train Loss: 26001, Val Loss: 26532,  Lear. Rate: 0.00462, Train Grad.: 185.3\n",
      "Epoch 1101/150000, Train Loss: 25683, Val Loss: 26209,  Lear. Rate: 0.00463, Train Grad.: 181.8\n",
      "Epoch 1201/150000, Train Loss: 25374, Val Loss: 25895,  Lear. Rate: 0.00464, Train Grad.: 178.4\n",
      "Epoch 1301/150000, Train Loss: 25072, Val Loss: 25588,  Lear. Rate: 0.00465, Train Grad.: 175.0\n",
      "Epoch 1401/150000, Train Loss: 24778, Val Loss: 25289,  Lear. Rate: 0.00466, Train Grad.: 171.6\n",
      "Epoch 1501/150000, Train Loss: 24492, Val Loss: 24997,  Lear. Rate: 0.00467, Train Grad.: 168.2\n",
      "Epoch 1601/150000, Train Loss: 24213, Val Loss: 24713,  Lear. Rate: 0.00467, Train Grad.: 164.8\n",
      "Epoch 1701/150000, Train Loss: 23941, Val Loss: 24436,  Lear. Rate: 0.00468, Train Grad.: 161.5\n",
      "Epoch 1801/150000, Train Loss: 23676, Val Loss: 24166,  Lear. Rate: 0.00469, Train Grad.: 158.2\n",
      "Epoch 1901/150000, Train Loss: 23419, Val Loss: 23903,  Lear. Rate: 0.00469, Train Grad.: 154.9\n",
      "Epoch 2001/150000, Train Loss: 23168, Val Loss: 23647,  Lear. Rate: 0.00470, Train Grad.: 151.6\n",
      "Epoch 2101/150000, Train Loss: 22923, Val Loss: 23398,  Lear. Rate: 0.00471, Train Grad.: 148.4\n",
      "Epoch 2201/150000, Train Loss: 22686, Val Loss: 23156,  Lear. Rate: 0.00471, Train Grad.: 145.1\n",
      "Epoch 2301/150000, Train Loss: 22455, Val Loss: 22920,  Lear. Rate: 0.00472, Train Grad.: 141.9\n",
      "Epoch 2401/150000, Train Loss: 22231, Val Loss: 22691,  Lear. Rate: 0.00472, Train Grad.: 138.7\n",
      "Epoch 2501/150000, Train Loss: 22012, Val Loss: 22467,  Lear. Rate: 0.00473, Train Grad.: 135.6\n",
      "Epoch 2601/150000, Train Loss: 21612, Val Loss: 22090,  Lear. Rate: 0.00474, Train Grad.: 147.9\n",
      "Epoch 2701/150000, Train Loss: 21355, Val Loss: 21828,  Lear. Rate: 0.00474, Train Grad.: 146.0\n",
      "Epoch 2801/150000, Train Loss: 21101, Val Loss: 21571,  Lear. Rate: 0.00475, Train Grad.: 144.3\n",
      "Epoch 2901/150000, Train Loss: 20852, Val Loss: 21317,  Lear. Rate: 0.00475, Train Grad.: 142.5\n",
      "Epoch 3001/150000, Train Loss: 20606, Val Loss: 21067,  Lear. Rate: 0.00476, Train Grad.: 140.8\n",
      "Epoch 3101/150000, Train Loss: 20363, Val Loss: 20821,  Lear. Rate: 0.00477, Train Grad.: 139.1\n",
      "Epoch 3201/150000, Train Loss: 20124, Val Loss: 20577,  Lear. Rate: 0.00477, Train Grad.: 137.5\n",
      "Epoch 3301/150000, Train Loss: 19888, Val Loss: 20336,  Lear. Rate: 0.00478, Train Grad.: 135.9\n",
      "Epoch 3401/150000, Train Loss: 19654, Val Loss: 20098,  Lear. Rate: 0.00478, Train Grad.: 134.3\n",
      "Epoch 3501/150000, Train Loss: 19424, Val Loss: 19863,  Lear. Rate: 0.00479, Train Grad.: 132.7\n",
      "Epoch 3601/150000, Train Loss: 19197, Val Loss: 19631,  Lear. Rate: 0.00479, Train Grad.: 131.2\n",
      "Epoch 3701/150000, Train Loss: 18972, Val Loss: 19400,  Lear. Rate: 0.00480, Train Grad.: 129.7\n",
      "Epoch 3801/150000, Train Loss: 18750, Val Loss: 19172,  Lear. Rate: 0.00480, Train Grad.: 128.2\n",
      "Epoch 3901/150000, Train Loss: 18531, Val Loss: 18946,  Lear. Rate: 0.00481, Train Grad.: 126.7\n",
      "Epoch 4001/150000, Train Loss: 18314, Val Loss: 18724,  Lear. Rate: 0.00481, Train Grad.: 125.2\n",
      "Epoch 4101/150000, Train Loss: 18100, Val Loss: 18505,  Lear. Rate: 0.00481, Train Grad.: 123.8\n",
      "Epoch 4201/150000, Train Loss: 17889, Val Loss: 18289,  Lear. Rate: 0.00482, Train Grad.: 122.3\n",
      "Epoch 4301/150000, Train Loss: 17680, Val Loss: 18076,  Lear. Rate: 0.00482, Train Grad.: 120.9\n",
      "Epoch 4401/150000, Train Loss: 17473, Val Loss: 17866,  Lear. Rate: 0.00483, Train Grad.: 119.5\n",
      "Epoch 4501/150000, Train Loss: 17269, Val Loss: 17658,  Lear. Rate: 0.00483, Train Grad.: 118.1\n",
      "Epoch 4601/150000, Train Loss: 17068, Val Loss: 17453,  Lear. Rate: 0.00483, Train Grad.: 116.7\n",
      "Epoch 4701/150000, Train Loss: 16869, Val Loss: 17250,  Lear. Rate: 0.00484, Train Grad.: 115.3\n",
      "Epoch 4801/150000, Train Loss: 16672, Val Loss: 17050,  Lear. Rate: 0.00484, Train Grad.: 113.9\n",
      "Epoch 4901/150000, Train Loss: 16478, Val Loss: 16852,  Lear. Rate: 0.00485, Train Grad.: 112.5\n",
      "Epoch 5001/150000, Train Loss: 16287, Val Loss: 16657,  Lear. Rate: 0.00485, Train Grad.: 111.1\n",
      "Epoch 5101/150000, Train Loss: 16097, Val Loss: 16465,  Lear. Rate: 0.00485, Train Grad.: 109.8\n",
      "Epoch 5201/150000, Train Loss: 15911, Val Loss: 16274,  Lear. Rate: 0.00486, Train Grad.: 108.4\n",
      "Epoch 5301/150000, Train Loss: 15726, Val Loss: 16087,  Lear. Rate: 0.00486, Train Grad.: 107.1\n",
      "Epoch 5401/150000, Train Loss: 15544, Val Loss: 15901,  Lear. Rate: 0.00486, Train Grad.: 105.7\n",
      "Epoch 5501/150000, Train Loss: 15365, Val Loss: 15718,  Lear. Rate: 0.00487, Train Grad.: 104.4\n",
      "Epoch 5601/150000, Train Loss: 15187, Val Loss: 15538,  Lear. Rate: 0.00487, Train Grad.: 103.1\n",
      "Epoch 5701/150000, Train Loss: 15012, Val Loss: 15359,  Lear. Rate: 0.00487, Train Grad.: 101.8\n",
      "Epoch 5801/150000, Train Loss: 14840, Val Loss: 15183,  Lear. Rate: 0.00487, Train Grad.: 100.5\n",
      "Epoch 5901/150000, Train Loss: 14669, Val Loss: 15009,  Lear. Rate: 0.00488, Train Grad.: 99.2\n",
      "Epoch 6001/150000, Train Loss: 14500, Val Loss: 14838,  Lear. Rate: 0.00488, Train Grad.: 98.1\n",
      "Epoch 6101/150000, Train Loss: 14334, Val Loss: 14668,  Lear. Rate: 0.00488, Train Grad.: 96.9\n",
      "Epoch 6201/150000, Train Loss: 14169, Val Loss: 14500,  Lear. Rate: 0.00489, Train Grad.: 95.7\n",
      "Epoch 6301/150000, Train Loss: 14006, Val Loss: 14335,  Lear. Rate: 0.00489, Train Grad.: 94.5\n",
      "Epoch 6401/150000, Train Loss: 13846, Val Loss: 14171,  Lear. Rate: 0.00489, Train Grad.: 93.4\n",
      "Epoch 6501/150000, Train Loss: 13687, Val Loss: 14010,  Lear. Rate: 0.00489, Train Grad.: 92.2\n",
      "Epoch 6601/150000, Train Loss: 13530, Val Loss: 13850,  Lear. Rate: 0.00490, Train Grad.: 91.1\n",
      "Epoch 6701/150000, Train Loss: 13375, Val Loss: 13692,  Lear. Rate: 0.00490, Train Grad.: 90.0\n",
      "Epoch 6801/150000, Train Loss: 13222, Val Loss: 13537,  Lear. Rate: 0.00490, Train Grad.: 88.9\n",
      "Epoch 6901/150000, Train Loss: 13071, Val Loss: 13383,  Lear. Rate: 0.00490, Train Grad.: 87.8\n",
      "Epoch 7001/150000, Train Loss: 12922, Val Loss: 13231,  Lear. Rate: 0.00490, Train Grad.: 86.8\n",
      "Epoch 7101/150000, Train Loss: 12774, Val Loss: 13080,  Lear. Rate: 0.00491, Train Grad.: 85.7\n",
      "Epoch 7201/150000, Train Loss: 12628, Val Loss: 12932,  Lear. Rate: 0.00491, Train Grad.: 84.7\n",
      "Epoch 7301/150000, Train Loss: 12484, Val Loss: 12785,  Lear. Rate: 0.00491, Train Grad.: 83.7\n",
      "Epoch 7401/150000, Train Loss: 12341, Val Loss: 12640,  Lear. Rate: 0.00491, Train Grad.: 82.7\n",
      "Epoch 7501/150000, Train Loss: 12200, Val Loss: 12496,  Lear. Rate: 0.00491, Train Grad.: 81.7\n",
      "Epoch 7601/150000, Train Loss: 12061, Val Loss: 12355,  Lear. Rate: 0.00492, Train Grad.: 80.7\n",
      "Epoch 7701/150000, Train Loss: 11923, Val Loss: 12215,  Lear. Rate: 0.00492, Train Grad.: 79.8\n",
      "Epoch 7801/150000, Train Loss: 11787, Val Loss: 12076,  Lear. Rate: 0.00492, Train Grad.: 78.9\n",
      "Epoch 7901/150000, Train Loss: 11652, Val Loss: 11939,  Lear. Rate: 0.00492, Train Grad.: 77.9\n",
      "Epoch 8001/150000, Train Loss: 11519, Val Loss: 11804,  Lear. Rate: 0.00492, Train Grad.: 77.0\n",
      "Epoch 8101/150000, Train Loss: 11387, Val Loss: 11670,  Lear. Rate: 0.00493, Train Grad.: 76.1\n",
      "Epoch 8201/150000, Train Loss: 11257, Val Loss: 11537,  Lear. Rate: 0.00493, Train Grad.: 75.2\n",
      "Epoch 8301/150000, Train Loss: 11128, Val Loss: 11406,  Lear. Rate: 0.00493, Train Grad.: 74.3\n",
      "Epoch 8401/150000, Train Loss: 11000, Val Loss: 11277,  Lear. Rate: 0.00493, Train Grad.: 73.5\n",
      "Epoch 8501/150000, Train Loss: 10874, Val Loss: 11149,  Lear. Rate: 0.00493, Train Grad.: 72.6\n",
      "Epoch 8601/150000, Train Loss: 10749, Val Loss: 11022,  Lear. Rate: 0.00493, Train Grad.: 71.9\n",
      "Epoch 8701/150000, Train Loss: 10626, Val Loss: 10897,  Lear. Rate: 0.00494, Train Grad.: 71.2\n",
      "Epoch 8801/150000, Train Loss: 10504, Val Loss: 10773,  Lear. Rate: 0.00494, Train Grad.: 70.2\n",
      "Epoch 8901/150000, Train Loss: 10383, Val Loss: 10650,  Lear. Rate: 0.00494, Train Grad.: 69.4\n",
      "Epoch 9001/150000, Train Loss: 10264, Val Loss: 10528,  Lear. Rate: 0.00494, Train Grad.: 68.6\n",
      "Epoch 9101/150000, Train Loss: 10145, Val Loss: 10408,  Lear. Rate: 0.00494, Train Grad.: 68.0\n",
      "Epoch 9201/150000, Train Loss: 10028, Val Loss: 10288,  Lear. Rate: 0.00494, Train Grad.: 67.2\n",
      "Epoch 9301/150000, Train Loss: 9912, Val Loss: 10170,  Lear. Rate: 0.00494, Train Grad.: 66.3\n",
      "Epoch 9401/150000, Train Loss: 9798, Val Loss: 10053,  Lear. Rate: 0.00494, Train Grad.: 65.7\n",
      "Epoch 9501/150000, Train Loss: 9684, Val Loss: 9938,  Lear. Rate: 0.00495, Train Grad.: 65.0\n",
      "Epoch 9601/150000, Train Loss: 9572, Val Loss: 9823,  Lear. Rate: 0.00495, Train Grad.: 64.3\n",
      "Epoch 9701/150000, Train Loss: 9460, Val Loss: 9710,  Lear. Rate: 0.00495, Train Grad.: 63.6\n",
      "Epoch 9801/150000, Train Loss: 9350, Val Loss: 9598,  Lear. Rate: 0.00495, Train Grad.: 62.9\n",
      "Epoch 9901/150000, Train Loss: 9241, Val Loss: 9486,  Lear. Rate: 0.00495, Train Grad.: 62.3\n",
      "Epoch 10001/150000, Train Loss: 9133, Val Loss: 9376,  Lear. Rate: 0.00495, Train Grad.: 61.6\n",
      "Epoch 10101/150000, Train Loss: 9025, Val Loss: 9268,  Lear. Rate: 0.00495, Train Grad.: 61.0\n",
      "Epoch 10201/150000, Train Loss: 8919, Val Loss: 9160,  Lear. Rate: 0.00495, Train Grad.: 60.4\n",
      "Epoch 10301/150000, Train Loss: 8814, Val Loss: 9053,  Lear. Rate: 0.00496, Train Grad.: 59.8\n",
      "Epoch 10401/150000, Train Loss: 8709, Val Loss: 8947,  Lear. Rate: 0.00496, Train Grad.: 59.2\n",
      "Epoch 10501/150000, Train Loss: 8606, Val Loss: 8842,  Lear. Rate: 0.00496, Train Grad.: 58.9\n",
      "Epoch 10601/150000, Train Loss: 8503, Val Loss: 8737,  Lear. Rate: 0.00496, Train Grad.: 58.1\n",
      "Epoch 10701/150000, Train Loss: 8402, Val Loss: 8634,  Lear. Rate: 0.00496, Train Grad.: 57.5\n",
      "Epoch 10801/150000, Train Loss: 8301, Val Loss: 8531,  Lear. Rate: 0.00496, Train Grad.: 56.9\n",
      "Epoch 10901/150000, Train Loss: 8201, Val Loss: 8430,  Lear. Rate: 0.00496, Train Grad.: 56.4\n",
      "Epoch 11001/150000, Train Loss: 8101, Val Loss: 8329,  Lear. Rate: 0.00496, Train Grad.: 55.8\n",
      "Epoch 11101/150000, Train Loss: 8003, Val Loss: 8229,  Lear. Rate: 0.00496, Train Grad.: 55.3\n",
      "Epoch 11201/150000, Train Loss: 7906, Val Loss: 8130,  Lear. Rate: 0.00496, Train Grad.: 54.8\n",
      "Epoch 11301/150000, Train Loss: 7809, Val Loss: 8032,  Lear. Rate: 0.00496, Train Grad.: 54.3\n",
      "Epoch 11401/150000, Train Loss: 7713, Val Loss: 7935,  Lear. Rate: 0.00497, Train Grad.: 53.6\n",
      "Epoch 11501/150000, Train Loss: 7618, Val Loss: 7839,  Lear. Rate: 0.00497, Train Grad.: 53.3\n",
      "Epoch 11601/150000, Train Loss: 7524, Val Loss: 7743,  Lear. Rate: 0.00497, Train Grad.: 52.5\n",
      "Epoch 11701/150000, Train Loss: 7430, Val Loss: 7649,  Lear. Rate: 0.00497, Train Grad.: 52.3\n",
      "Epoch 11801/150000, Train Loss: 7338, Val Loss: 7555,  Lear. Rate: 0.00497, Train Grad.: 51.8\n",
      "Epoch 11901/150000, Train Loss: 7246, Val Loss: 7462,  Lear. Rate: 0.00497, Train Grad.: 51.3\n",
      "Epoch 12001/150000, Train Loss: 7155, Val Loss: 7369,  Lear. Rate: 0.00497, Train Grad.: 50.8\n",
      "Epoch 12101/150000, Train Loss: 7065, Val Loss: 7278,  Lear. Rate: 0.00497, Train Grad.: 50.3\n",
      "Epoch 12201/150000, Train Loss: 6975, Val Loss: 7187,  Lear. Rate: 0.00497, Train Grad.: 49.9\n",
      "Epoch 12301/150000, Train Loss: 6886, Val Loss: 7097,  Lear. Rate: 0.00497, Train Grad.: 49.6\n",
      "Epoch 12401/150000, Train Loss: 6798, Val Loss: 7008,  Lear. Rate: 0.00497, Train Grad.: 49.0\n",
      "Epoch 12501/150000, Train Loss: 6711, Val Loss: 6919,  Lear. Rate: 0.00497, Train Grad.: 48.5\n",
      "Epoch 12601/150000, Train Loss: 6625, Val Loss: 6832,  Lear. Rate: 0.00497, Train Grad.: 48.1\n",
      "Epoch 12701/150000, Train Loss: 6539, Val Loss: 6745,  Lear. Rate: 0.00498, Train Grad.: 47.6\n",
      "Epoch 12801/150000, Train Loss: 6454, Val Loss: 6659,  Lear. Rate: 0.00498, Train Grad.: 47.2\n",
      "Epoch 12901/150000, Train Loss: 6370, Val Loss: 6574,  Lear. Rate: 0.00498, Train Grad.: 46.8\n",
      "Epoch 13001/150000, Train Loss: 6286, Val Loss: 6489,  Lear. Rate: 0.00498, Train Grad.: 46.2\n",
      "Epoch 13101/150000, Train Loss: 6204, Val Loss: 6405,  Lear. Rate: 0.00498, Train Grad.: 45.9\n",
      "Epoch 13201/150000, Train Loss: 6122, Val Loss: 6322,  Lear. Rate: 0.00498, Train Grad.: 45.4\n",
      "Epoch 13301/150000, Train Loss: 6041, Val Loss: 6240,  Lear. Rate: 0.00498, Train Grad.: 45.0\n",
      "Epoch 13401/150000, Train Loss: 5960, Val Loss: 6159,  Lear. Rate: 0.00498, Train Grad.: 44.6\n",
      "Epoch 13501/150000, Train Loss: 5881, Val Loss: 6078,  Lear. Rate: 0.00498, Train Grad.: 44.1\n",
      "Epoch 13601/150000, Train Loss: 5802, Val Loss: 5999,  Lear. Rate: 0.00498, Train Grad.: 43.7\n",
      "Epoch 13701/150000, Train Loss: 5724, Val Loss: 5919,  Lear. Rate: 0.00498, Train Grad.: 43.6\n",
      "Epoch 13801/150000, Train Loss: 5647, Val Loss: 5841,  Lear. Rate: 0.00498, Train Grad.: 42.9\n",
      "Epoch 13901/150000, Train Loss: 5570, Val Loss: 5764,  Lear. Rate: 0.00498, Train Grad.: 42.5\n",
      "Epoch 14001/150000, Train Loss: 5494, Val Loss: 5687,  Lear. Rate: 0.00498, Train Grad.: 42.0\n",
      "Epoch 14101/150000, Train Loss: 5419, Val Loss: 5611,  Lear. Rate: 0.00498, Train Grad.: 41.6\n",
      "Epoch 14201/150000, Train Loss: 5345, Val Loss: 5536,  Lear. Rate: 0.00498, Train Grad.: 41.2\n",
      "Epoch 14301/150000, Train Loss: 5272, Val Loss: 5462,  Lear. Rate: 0.00498, Train Grad.: 40.8\n",
      "Epoch 14401/150000, Train Loss: 5199, Val Loss: 5388,  Lear. Rate: 0.00498, Train Grad.: 40.3\n",
      "Epoch 14501/150000, Train Loss: 5127, Val Loss: 5315,  Lear. Rate: 0.00498, Train Grad.: 39.9\n",
      "Epoch 14601/150000, Train Loss: 5056, Val Loss: 5243,  Lear. Rate: 0.00498, Train Grad.: 39.2\n",
      "Epoch 14701/150000, Train Loss: 4986, Val Loss: 5172,  Lear. Rate: 0.00499, Train Grad.: 39.2\n",
      "Epoch 14801/150000, Train Loss: 4916, Val Loss: 5102,  Lear. Rate: 0.00499, Train Grad.: 38.7\n",
      "Epoch 14901/150000, Train Loss: 4847, Val Loss: 5033,  Lear. Rate: 0.00499, Train Grad.: 38.4\n",
      "Epoch 15001/150000, Train Loss: 4778, Val Loss: 4964,  Lear. Rate: 0.00499, Train Grad.: 38.0\n",
      "Epoch 15101/150000, Train Loss: 4711, Val Loss: 4896,  Lear. Rate: 0.00499, Train Grad.: 37.7\n",
      "Epoch 15201/150000, Train Loss: 4644, Val Loss: 4828,  Lear. Rate: 0.00499, Train Grad.: 37.2\n",
      "Epoch 15301/150000, Train Loss: 4577, Val Loss: 4762,  Lear. Rate: 0.00499, Train Grad.: 36.9\n",
      "Epoch 15401/150000, Train Loss: 4512, Val Loss: 4696,  Lear. Rate: 0.00499, Train Grad.: 36.5\n",
      "Epoch 15501/150000, Train Loss: 4447, Val Loss: 4630,  Lear. Rate: 0.00499, Train Grad.: 36.2\n",
      "Epoch 15601/150000, Train Loss: 4382, Val Loss: 4566,  Lear. Rate: 0.00499, Train Grad.: 35.8\n",
      "Epoch 15701/150000, Train Loss: 4319, Val Loss: 4502,  Lear. Rate: 0.00499, Train Grad.: 35.0\n",
      "Epoch 15801/150000, Train Loss: 4255, Val Loss: 4438,  Lear. Rate: 0.00499, Train Grad.: 35.1\n",
      "Epoch 15901/150000, Train Loss: 4193, Val Loss: 4376,  Lear. Rate: 0.00499, Train Grad.: 34.7\n",
      "Epoch 16001/150000, Train Loss: 4131, Val Loss: 4314,  Lear. Rate: 0.00499, Train Grad.: 34.3\n",
      "Epoch 16101/150000, Train Loss: 4070, Val Loss: 4253,  Lear. Rate: 0.00499, Train Grad.: 34.0\n",
      "Epoch 16201/150000, Train Loss: 4010, Val Loss: 4192,  Lear. Rate: 0.00499, Train Grad.: 33.8\n",
      "Epoch 16301/150000, Train Loss: 3950, Val Loss: 4133,  Lear. Rate: 0.00499, Train Grad.: 33.3\n",
      "Epoch 16401/150000, Train Loss: 3891, Val Loss: 4074,  Lear. Rate: 0.00499, Train Grad.: 32.9\n",
      "Epoch 16501/150000, Train Loss: 3833, Val Loss: 4015,  Lear. Rate: 0.00499, Train Grad.: 32.5\n",
      "Epoch 16601/150000, Train Loss: 3775, Val Loss: 3957,  Lear. Rate: 0.00499, Train Grad.: 32.2\n",
      "Epoch 16701/150000, Train Loss: 3718, Val Loss: 3900,  Lear. Rate: 0.00499, Train Grad.: 31.9\n",
      "Epoch 16801/150000, Train Loss: 3662, Val Loss: 3844,  Lear. Rate: 0.00499, Train Grad.: 31.5\n",
      "Epoch 16901/150000, Train Loss: 3606, Val Loss: 3788,  Lear. Rate: 0.00499, Train Grad.: 31.1\n",
      "Epoch 17001/150000, Train Loss: 3551, Val Loss: 3733,  Lear. Rate: 0.00499, Train Grad.: 30.8\n",
      "Epoch 17101/150000, Train Loss: 3497, Val Loss: 3679,  Lear. Rate: 0.00499, Train Grad.: 30.5\n",
      "Epoch 17201/150000, Train Loss: 3443, Val Loss: 3625,  Lear. Rate: 0.00499, Train Grad.: 29.7\n",
      "Epoch 17301/150000, Train Loss: 3390, Val Loss: 3572,  Lear. Rate: 0.00499, Train Grad.: 29.8\n",
      "Epoch 17401/150000, Train Loss: 3337, Val Loss: 3520,  Lear. Rate: 0.00499, Train Grad.: 29.4\n",
      "Epoch 17501/150000, Train Loss: 3285, Val Loss: 3468,  Lear. Rate: 0.00499, Train Grad.: 29.1\n",
      "Epoch 17601/150000, Train Loss: 3234, Val Loss: 3416,  Lear. Rate: 0.00499, Train Grad.: 28.8\n",
      "Epoch 17701/150000, Train Loss: 3183, Val Loss: 3365,  Lear. Rate: 0.00499, Train Grad.: 28.3\n",
      "Epoch 17801/150000, Train Loss: 3133, Val Loss: 3315,  Lear. Rate: 0.00499, Train Grad.: 28.1\n",
      "Epoch 17901/150000, Train Loss: 3084, Val Loss: 3265,  Lear. Rate: 0.00499, Train Grad.: 27.8\n",
      "Epoch 18001/150000, Train Loss: 3035, Val Loss: 3216,  Lear. Rate: 0.00499, Train Grad.: 27.4\n",
      "Epoch 18101/150000, Train Loss: 2987, Val Loss: 3168,  Lear. Rate: 0.00499, Train Grad.: 27.1\n",
      "Epoch 18201/150000, Train Loss: 2939, Val Loss: 3121,  Lear. Rate: 0.00499, Train Grad.: 27.0\n",
      "Epoch 18301/150000, Train Loss: 2892, Val Loss: 3074,  Lear. Rate: 0.00499, Train Grad.: 26.5\n",
      "Epoch 18401/150000, Train Loss: 2846, Val Loss: 3028,  Lear. Rate: 0.00499, Train Grad.: 26.2\n",
      "Epoch 18501/150000, Train Loss: 2800, Val Loss: 2982,  Lear. Rate: 0.00500, Train Grad.: 25.9\n",
      "Epoch 18601/150000, Train Loss: 2755, Val Loss: 2937,  Lear. Rate: 0.00500, Train Grad.: 25.6\n",
      "Epoch 18701/150000, Train Loss: 2710, Val Loss: 2892,  Lear. Rate: 0.00500, Train Grad.: 25.3\n",
      "Epoch 18801/150000, Train Loss: 2666, Val Loss: 2849,  Lear. Rate: 0.00500, Train Grad.: 25.0\n",
      "Epoch 18901/150000, Train Loss: 2622, Val Loss: 2805,  Lear. Rate: 0.00500, Train Grad.: 24.7\n",
      "Epoch 19001/150000, Train Loss: 2579, Val Loss: 2763,  Lear. Rate: 0.00500, Train Grad.: 24.4\n",
      "Epoch 19101/150000, Train Loss: 2537, Val Loss: 2721,  Lear. Rate: 0.00500, Train Grad.: 24.1\n",
      "Epoch 19201/150000, Train Loss: 2495, Val Loss: 2679,  Lear. Rate: 0.00500, Train Grad.: 23.8\n",
      "Epoch 19301/150000, Train Loss: 2453, Val Loss: 2638,  Lear. Rate: 0.00500, Train Grad.: 23.5\n",
      "Epoch 19401/150000, Train Loss: 2412, Val Loss: 2598,  Lear. Rate: 0.00500, Train Grad.: 23.2\n",
      "Epoch 19501/150000, Train Loss: 2372, Val Loss: 2558,  Lear. Rate: 0.00500, Train Grad.: 22.9\n",
      "Epoch 19601/150000, Train Loss: 2332, Val Loss: 2518,  Lear. Rate: 0.00500, Train Grad.: 22.6\n",
      "Epoch 19701/150000, Train Loss: 2293, Val Loss: 2480,  Lear. Rate: 0.00500, Train Grad.: 22.1\n",
      "Epoch 19801/150000, Train Loss: 2254, Val Loss: 2441,  Lear. Rate: 0.00500, Train Grad.: 22.1\n",
      "Epoch 19901/150000, Train Loss: 2215, Val Loss: 2403,  Lear. Rate: 0.00500, Train Grad.: 21.8\n",
      "Epoch 20001/150000, Train Loss: 2177, Val Loss: 2365,  Lear. Rate: 0.00500, Train Grad.: 21.6\n",
      "Epoch 20101/150000, Train Loss: 2140, Val Loss: 2328,  Lear. Rate: 0.00500, Train Grad.: 21.3\n",
      "Epoch 20201/150000, Train Loss: 2103, Val Loss: 2292,  Lear. Rate: 0.00500, Train Grad.: 21.0\n",
      "Epoch 20301/150000, Train Loss: 2066, Val Loss: 2256,  Lear. Rate: 0.00500, Train Grad.: 20.8\n",
      "Epoch 20401/150000, Train Loss: 2030, Val Loss: 2220,  Lear. Rate: 0.00500, Train Grad.: 20.5\n",
      "Epoch 20501/150000, Train Loss: 1994, Val Loss: 2185,  Lear. Rate: 0.00500, Train Grad.: 20.3\n",
      "Epoch 20601/150000, Train Loss: 1959, Val Loss: 2150,  Lear. Rate: 0.00500, Train Grad.: 20.0\n",
      "Epoch 20701/150000, Train Loss: 1925, Val Loss: 2116,  Lear. Rate: 0.00500, Train Grad.: 19.7\n",
      "Epoch 20801/150000, Train Loss: 1890, Val Loss: 2082,  Lear. Rate: 0.00500, Train Grad.: 19.5\n",
      "Epoch 20901/150000, Train Loss: 1856, Val Loss: 2049,  Lear. Rate: 0.00500, Train Grad.: 19.3\n",
      "Epoch 21001/150000, Train Loss: 1823, Val Loss: 2016,  Lear. Rate: 0.00500, Train Grad.: 19.0\n",
      "Epoch 21101/150000, Train Loss: 1790, Val Loss: 1984,  Lear. Rate: 0.00500, Train Grad.: 18.8\n",
      "Epoch 21201/150000, Train Loss: 1758, Val Loss: 1952,  Lear. Rate: 0.00500, Train Grad.: 18.4\n",
      "Epoch 21301/150000, Train Loss: 1726, Val Loss: 1921,  Lear. Rate: 0.00500, Train Grad.: 18.3\n",
      "Epoch 21401/150000, Train Loss: 1695, Val Loss: 1890,  Lear. Rate: 0.00500, Train Grad.: 18.0\n",
      "Epoch 21501/150000, Train Loss: 1663, Val Loss: 1860,  Lear. Rate: 0.00500, Train Grad.: 17.8\n",
      "Epoch 21601/150000, Train Loss: 1633, Val Loss: 1830,  Lear. Rate: 0.00500, Train Grad.: 17.5\n",
      "Epoch 21701/150000, Train Loss: 1603, Val Loss: 1801,  Lear. Rate: 0.00500, Train Grad.: 17.2\n",
      "Epoch 21801/150000, Train Loss: 1573, Val Loss: 1772,  Lear. Rate: 0.00500, Train Grad.: 17.0\n",
      "Epoch 21901/150000, Train Loss: 1544, Val Loss: 1743,  Lear. Rate: 0.00500, Train Grad.: 16.8\n",
      "Epoch 22001/150000, Train Loss: 1515, Val Loss: 1715,  Lear. Rate: 0.00500, Train Grad.: 16.5\n",
      "Epoch 22101/150000, Train Loss: 1487, Val Loss: 1688,  Lear. Rate: 0.00500, Train Grad.: 16.4\n",
      "Epoch 22201/150000, Train Loss: 1459, Val Loss: 1661,  Lear. Rate: 0.00500, Train Grad.: 16.1\n",
      "Epoch 22301/150000, Train Loss: 1431, Val Loss: 1635,  Lear. Rate: 0.00500, Train Grad.: 15.9\n",
      "Epoch 22401/150000, Train Loss: 1404, Val Loss: 1609,  Lear. Rate: 0.00500, Train Grad.: 15.7\n",
      "Epoch 22501/150000, Train Loss: 1377, Val Loss: 1583,  Lear. Rate: 0.00500, Train Grad.: 15.5\n",
      "Epoch 22601/150000, Train Loss: 1351, Val Loss: 1558,  Lear. Rate: 0.00500, Train Grad.: 15.3\n",
      "Epoch 22701/150000, Train Loss: 1325, Val Loss: 1533,  Lear. Rate: 0.00500, Train Grad.: 15.1\n",
      "Epoch 22801/150000, Train Loss: 1299, Val Loss: 1509,  Lear. Rate: 0.00500, Train Grad.: 15.4\n",
      "Epoch 22901/150000, Train Loss: 1274, Val Loss: 1485,  Lear. Rate: 0.00500, Train Grad.: 14.6\n",
      "Epoch 23001/150000, Train Loss: 1249, Val Loss: 1461,  Lear. Rate: 0.00500, Train Grad.: 14.4\n",
      "Epoch 23101/150000, Train Loss: 1224, Val Loss: 1438,  Lear. Rate: 0.00500, Train Grad.: 13.9\n",
      "Epoch 23201/150000, Train Loss: 1200, Val Loss: 1415,  Lear. Rate: 0.00500, Train Grad.: 14.0\n",
      "Epoch 23301/150000, Train Loss: 1176, Val Loss: 1392,  Lear. Rate: 0.00500, Train Grad.: 13.8\n",
      "Epoch 23401/150000, Train Loss: 1153, Val Loss: 1370,  Lear. Rate: 0.00500, Train Grad.: 13.6\n",
      "Epoch 23501/150000, Train Loss: 1130, Val Loss: 1348,  Lear. Rate: 0.00500, Train Grad.: 13.4\n",
      "Epoch 23601/150000, Train Loss: 1107, Val Loss: 1327,  Lear. Rate: 0.00500, Train Grad.: 13.2\n",
      "Epoch 23701/150000, Train Loss: 1084, Val Loss: 1305,  Lear. Rate: 0.00500, Train Grad.: 13.1\n",
      "Epoch 23801/150000, Train Loss: 1062, Val Loss: 1285,  Lear. Rate: 0.00500, Train Grad.: 12.7\n",
      "Epoch 23901/150000, Train Loss: 1041, Val Loss: 1264,  Lear. Rate: 0.00500, Train Grad.: 12.7\n",
      "Epoch 24001/150000, Train Loss: 1019, Val Loss: 1244,  Lear. Rate: 0.00500, Train Grad.: 12.5\n",
      "Epoch 24101/150000, Train Loss: 998, Val Loss: 1224,  Lear. Rate: 0.00500, Train Grad.: 12.2\n",
      "Epoch 24201/150000, Train Loss: 978, Val Loss: 1205,  Lear. Rate: 0.00500, Train Grad.: 11.8\n",
      "Epoch 24301/150000, Train Loss: 957, Val Loss: 1186,  Lear. Rate: 0.00500, Train Grad.: 11.9\n",
      "Epoch 24401/150000, Train Loss: 937, Val Loss: 1167,  Lear. Rate: 0.00500, Train Grad.: 11.6\n",
      "Epoch 24501/150000, Train Loss: 918, Val Loss: 1149,  Lear. Rate: 0.00500, Train Grad.: 11.5\n",
      "Epoch 24601/150000, Train Loss: 898, Val Loss: 1132,  Lear. Rate: 0.00500, Train Grad.: 11.3\n",
      "Epoch 24701/150000, Train Loss: 880, Val Loss: 1114,  Lear. Rate: 0.00500, Train Grad.: 11.1\n",
      "Epoch 24801/150000, Train Loss: 861, Val Loss: 1098,  Lear. Rate: 0.00500, Train Grad.: 11.0\n",
      "Epoch 24901/150000, Train Loss: 843, Val Loss: 1081,  Lear. Rate: 0.00500, Train Grad.: 10.8\n",
      "Epoch 25001/150000, Train Loss: 825, Val Loss: 1065,  Lear. Rate: 0.00500, Train Grad.: 10.6\n",
      "Epoch 25101/150000, Train Loss: 807, Val Loss: 1049,  Lear. Rate: 0.00500, Train Grad.: 10.4\n",
      "Epoch 25201/150000, Train Loss: 790, Val Loss: 1033,  Lear. Rate: 0.00500, Train Grad.: 10.3\n",
      "Epoch 25301/150000, Train Loss: 773, Val Loss: 1018,  Lear. Rate: 0.00500, Train Grad.: 10.1\n",
      "Epoch 25401/150000, Train Loss: 756, Val Loss: 1003,  Lear. Rate: 0.00500, Train Grad.: 9.9\n",
      "Epoch 25501/150000, Train Loss: 739, Val Loss: 988,  Lear. Rate: 0.00500, Train Grad.: 9.8\n",
      "Epoch 25601/150000, Train Loss: 723, Val Loss: 974,  Lear. Rate: 0.00500, Train Grad.: 9.5\n",
      "Epoch 25701/150000, Train Loss: 707, Val Loss: 959,  Lear. Rate: 0.00500, Train Grad.: 9.4\n",
      "Epoch 25801/150000, Train Loss: 692, Val Loss: 946,  Lear. Rate: 0.00500, Train Grad.: 8.8\n",
      "Epoch 25901/150000, Train Loss: 676, Val Loss: 932,  Lear. Rate: 0.00500, Train Grad.: 9.1\n",
      "Epoch 26001/150000, Train Loss: 661, Val Loss: 919,  Lear. Rate: 0.00500, Train Grad.: 9.0\n",
      "Epoch 26101/150000, Train Loss: 647, Val Loss: 905,  Lear. Rate: 0.00500, Train Grad.: 8.8\n",
      "Epoch 26201/150000, Train Loss: 632, Val Loss: 893,  Lear. Rate: 0.00500, Train Grad.: 8.7\n",
      "Epoch 26301/150000, Train Loss: 618, Val Loss: 880,  Lear. Rate: 0.00500, Train Grad.: 8.5\n",
      "Epoch 26401/150000, Train Loss: 604, Val Loss: 868,  Lear. Rate: 0.00500, Train Grad.: 8.4\n",
      "Epoch 26501/150000, Train Loss: 590, Val Loss: 856,  Lear. Rate: 0.00500, Train Grad.: 8.3\n",
      "Epoch 26601/150000, Train Loss: 577, Val Loss: 844,  Lear. Rate: 0.00500, Train Grad.: 8.1\n",
      "Epoch 26701/150000, Train Loss: 564, Val Loss: 832,  Lear. Rate: 0.00500, Train Grad.: 8.3\n",
      "Epoch 26801/150000, Train Loss: 551, Val Loss: 822,  Lear. Rate: 0.00500, Train Grad.: 7.8\n",
      "Epoch 26901/150000, Train Loss: 538, Val Loss: 811,  Lear. Rate: 0.00500, Train Grad.: 7.6\n",
      "Epoch 27001/150000, Train Loss: 526, Val Loss: 800,  Lear. Rate: 0.00500, Train Grad.: 7.5\n",
      "Epoch 27101/150000, Train Loss: 514, Val Loss: 790,  Lear. Rate: 0.00500, Train Grad.: 7.4\n",
      "Epoch 27201/150000, Train Loss: 502, Val Loss: 779,  Lear. Rate: 0.00500, Train Grad.: 7.2\n",
      "Epoch 27301/150000, Train Loss: 490, Val Loss: 769,  Lear. Rate: 0.00500, Train Grad.: 7.1\n",
      "Epoch 27401/150000, Train Loss: 479, Val Loss: 760,  Lear. Rate: 0.00500, Train Grad.: 6.9\n",
      "Epoch 27501/150000, Train Loss: 467, Val Loss: 750,  Lear. Rate: 0.00500, Train Grad.: 6.8\n",
      "Epoch 27601/150000, Train Loss: 456, Val Loss: 741,  Lear. Rate: 0.00500, Train Grad.: 6.7\n",
      "Epoch 27701/150000, Train Loss: 446, Val Loss: 732,  Lear. Rate: 0.00500, Train Grad.: 6.5\n",
      "Epoch 27801/150000, Train Loss: 435, Val Loss: 723,  Lear. Rate: 0.00500, Train Grad.: 6.6\n",
      "Epoch 27901/150000, Train Loss: 425, Val Loss: 715,  Lear. Rate: 0.00500, Train Grad.: 6.3\n",
      "Epoch 28001/150000, Train Loss: 415, Val Loss: 707,  Lear. Rate: 0.00500, Train Grad.: 6.0\n",
      "Epoch 28101/150000, Train Loss: 405, Val Loss: 699,  Lear. Rate: 0.00500, Train Grad.: 6.0\n",
      "Epoch 28201/150000, Train Loss: 396, Val Loss: 691,  Lear. Rate: 0.00500, Train Grad.: 5.9\n",
      "Epoch 28301/150000, Train Loss: 386, Val Loss: 684,  Lear. Rate: 0.00500, Train Grad.: 5.8\n",
      "Epoch 28401/150000, Train Loss: 377, Val Loss: 675,  Lear. Rate: 0.00500, Train Grad.: 5.7\n",
      "Epoch 28501/150000, Train Loss: 367, Val Loss: 663,  Lear. Rate: 0.00500, Train Grad.: 5.6\n",
      "Epoch 28601/150000, Train Loss: 358, Val Loss: 652,  Lear. Rate: 0.00500, Train Grad.: 5.5\n",
      "Epoch 28701/150000, Train Loss: 349, Val Loss: 644,  Lear. Rate: 0.00500, Train Grad.: 5.3\n",
      "Epoch 28801/150000, Train Loss: 340, Val Loss: 636,  Lear. Rate: 0.00500, Train Grad.: 5.4\n",
      "Epoch 28901/150000, Train Loss: 332, Val Loss: 630,  Lear. Rate: 0.00500, Train Grad.: 5.1\n",
      "Epoch 29001/150000, Train Loss: 324, Val Loss: 623,  Lear. Rate: 0.00500, Train Grad.: 5.0\n",
      "Epoch 29101/150000, Train Loss: 316, Val Loss: 617,  Lear. Rate: 0.00500, Train Grad.: 4.9\n",
      "Epoch 29201/150000, Train Loss: 309, Val Loss: 611,  Lear. Rate: 0.00500, Train Grad.: 4.7\n",
      "Epoch 29301/150000, Train Loss: 302, Val Loss: 605,  Lear. Rate: 0.00500, Train Grad.: 4.7\n",
      "Epoch 29401/150000, Train Loss: 294, Val Loss: 600,  Lear. Rate: 0.00500, Train Grad.: 4.4\n",
      "Epoch 29501/150000, Train Loss: 287, Val Loss: 595,  Lear. Rate: 0.00500, Train Grad.: 4.4\n",
      "Epoch 29601/150000, Train Loss: 281, Val Loss: 590,  Lear. Rate: 0.00500, Train Grad.: 4.3\n",
      "Epoch 29701/150000, Train Loss: 274, Val Loss: 585,  Lear. Rate: 0.00500, Train Grad.: 4.2\n",
      "Epoch 29801/150000, Train Loss: 268, Val Loss: 580,  Lear. Rate: 0.00500, Train Grad.: 3.5\n",
      "Epoch 29901/150000, Train Loss: 262, Val Loss: 576,  Lear. Rate: 0.00500, Train Grad.: 4.0\n",
      "Epoch 30001/150000, Train Loss: 256, Val Loss: 572,  Lear. Rate: 0.00500, Train Grad.: 3.9\n",
      "Epoch 30101/150000, Train Loss: 250, Val Loss: 567,  Lear. Rate: 0.00500, Train Grad.: 3.8\n",
      "Epoch 30201/150000, Train Loss: 244, Val Loss: 563,  Lear. Rate: 0.00500, Train Grad.: 3.7\n",
      "Epoch 30301/150000, Train Loss: 239, Val Loss: 560,  Lear. Rate: 0.00500, Train Grad.: 3.6\n",
      "Epoch 30401/150000, Train Loss: 234, Val Loss: 556,  Lear. Rate: 0.00500, Train Grad.: 3.5\n",
      "Epoch 30501/150000, Train Loss: 228, Val Loss: 553,  Lear. Rate: 0.00500, Train Grad.: 3.4\n",
      "Epoch 30601/150000, Train Loss: 223, Val Loss: 550,  Lear. Rate: 0.00500, Train Grad.: 3.3\n",
      "Epoch 30701/150000, Train Loss: 219, Val Loss: 547,  Lear. Rate: 0.00500, Train Grad.: 3.2\n",
      "Epoch 30801/150000, Train Loss: 214, Val Loss: 544,  Lear. Rate: 0.00500, Train Grad.: 3.1\n",
      "Epoch 30901/150000, Train Loss: 209, Val Loss: 541,  Lear. Rate: 0.00500, Train Grad.: 3.0\n",
      "Epoch 31001/150000, Train Loss: 205, Val Loss: 538,  Lear. Rate: 0.00500, Train Grad.: 2.9\n",
      "Epoch 31101/150000, Train Loss: 201, Val Loss: 535,  Lear. Rate: 0.00500, Train Grad.: 2.9\n",
      "Epoch 31201/150000, Train Loss: 197, Val Loss: 533,  Lear. Rate: 0.00500, Train Grad.: 2.8\n",
      "Epoch 31301/150000, Train Loss: 192, Val Loss: 531,  Lear. Rate: 0.00500, Train Grad.: 2.7\n",
      "Epoch 31401/150000, Train Loss: 189, Val Loss: 528,  Lear. Rate: 0.00500, Train Grad.: 2.6\n",
      "Epoch 31501/150000, Train Loss: 185, Val Loss: 526,  Lear. Rate: 0.00500, Train Grad.: 2.6\n",
      "Epoch 31601/150000, Train Loss: 181, Val Loss: 524,  Lear. Rate: 0.00500, Train Grad.: 2.5\n",
      "Epoch 31701/150000, Train Loss: 178, Val Loss: 522,  Lear. Rate: 0.00500, Train Grad.: 2.4\n",
      "Epoch 31801/150000, Train Loss: 174, Val Loss: 520,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 31901/150000, Train Loss: 171, Val Loss: 519,  Lear. Rate: 0.00500, Train Grad.: 2.3\n",
      "Epoch 32001/150000, Train Loss: 168, Val Loss: 517,  Lear. Rate: 0.00500, Train Grad.: 2.2\n",
      "Epoch 32101/150000, Train Loss: 164, Val Loss: 515,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 32201/150000, Train Loss: 161, Val Loss: 514,  Lear. Rate: 0.00500, Train Grad.: 2.1\n",
      "Epoch 32301/150000, Train Loss: 158, Val Loss: 512,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 32401/150000, Train Loss: 156, Val Loss: 511,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 32501/150000, Train Loss: 153, Val Loss: 509,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 32601/150000, Train Loss: 150, Val Loss: 508,  Lear. Rate: 0.00500, Train Grad.: 1.8\n",
      "Epoch 32701/150000, Train Loss: 148, Val Loss: 507,  Lear. Rate: 0.00500, Train Grad.: 2.0\n",
      "Epoch 32801/150000, Train Loss: 145, Val Loss: 506,  Lear. Rate: 0.00500, Train Grad.: 1.7\n",
      "Epoch 32901/150000, Train Loss: 143, Val Loss: 505,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
      "Epoch 33001/150000, Train Loss: 140, Val Loss: 503,  Lear. Rate: 0.00500, Train Grad.: 1.6\n",
      "Epoch 33101/150000, Train Loss: 138, Val Loss: 502,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 33201/150000, Train Loss: 136, Val Loss: 501,  Lear. Rate: 0.00500, Train Grad.: 1.5\n",
      "Epoch 33301/150000, Train Loss: 134, Val Loss: 500,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 33401/150000, Train Loss: 131, Val Loss: 499,  Lear. Rate: 0.00500, Train Grad.: 1.5\n",
      "Epoch 33501/150000, Train Loss: 129, Val Loss: 497,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 33601/150000, Train Loss: 127, Val Loss: 496,  Lear. Rate: 0.00500, Train Grad.: 1.4\n",
      "Epoch 33701/150000, Train Loss: 126, Val Loss: 493,  Lear. Rate: 0.00500, Train Grad.: 1.9\n",
      "Epoch 33801/150000, Train Loss: 123, Val Loss: 489,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
      "Epoch 33901/150000, Train Loss: 119, Val Loss: 470,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
      "Epoch 34001/150000, Train Loss: 113, Val Loss: 446,  Lear. Rate: 0.00500, Train Grad.: 1.3\n",
      "Epoch 34101/150000, Train Loss: 110, Val Loss: 435,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 34201/150000, Train Loss: 108, Val Loss: 427,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 34301/150000, Train Loss: 105, Val Loss: 419,  Lear. Rate: 0.00500, Train Grad.: 1.2\n",
      "Epoch 34401/150000, Train Loss: 103, Val Loss: 415,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 34501/150000, Train Loss: 101, Val Loss: 413,  Lear. Rate: 0.00500, Train Grad.: 1.1\n",
      "Epoch 34601/150000, Train Loss: 100, Val Loss: 412,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 34701/150000, Train Loss: 99, Val Loss: 411,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 34801/150000, Train Loss: 97, Val Loss: 411,  Lear. Rate: 0.00500, Train Grad.: 1.0\n",
      "Epoch 34901/150000, Train Loss: 96, Val Loss: 410,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 35001/150000, Train Loss: 95, Val Loss: 410,  Lear. Rate: 0.00500, Train Grad.: 0.9\n",
      "Epoch 35101/150000, Train Loss: 94, Val Loss: 409,  Lear. Rate: 0.00500, Train Grad.: 0.8\n",
      "Epoch 35201/150000, Train Loss: 92, Val Loss: 409,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 35301/150000, Train Loss: 91, Val Loss: 408,  Lear. Rate: 0.00500, Train Grad.: 0.8\n",
      "Epoch 35401/150000, Train Loss: 91, Val Loss: 408,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 35501/150000, Train Loss: 89, Val Loss: 408,  Lear. Rate: 0.00500, Train Grad.: 0.8\n",
      "Epoch 35601/150000, Train Loss: 89, Val Loss: 407,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 35701/150000, Train Loss: 88, Val Loss: 407,  Lear. Rate: 0.00500, Train Grad.: 0.6\n",
      "Epoch 35801/150000, Train Loss: 87, Val Loss: 407,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 35901/150000, Train Loss: 86, Val Loss: 407,  Lear. Rate: 0.00500, Train Grad.: 0.7\n",
      "Epoch 36001/150000, Train Loss: 85, Val Loss: 407,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 36101/150000, Train Loss: 85, Val Loss: 407,  Lear. Rate: 0.00500, Train Grad.: 0.6\n",
      "Epoch 36201/150000, Train Loss: 84, Val Loss: 407,  Lear. Rate: 0.00500, Train Grad.: 0.6\n",
      "Epoch 36301/150000, Train Loss: 83, Val Loss: 407,  Lear. Rate: 0.00500, Train Grad.: 0.2\n",
      "Epoch 36401/150000, Train Loss: 82, Val Loss: 407,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 36501/150000, Train Loss: 82, Val Loss: 408,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 36601/150000, Train Loss: 79, Val Loss: 413,  Lear. Rate: 0.00500, Train Grad.: 0.5\n",
      "Epoch 36701/150000, Train Loss: 76, Val Loss: 423,  Lear. Rate: 0.00500, Train Grad.: 0.6\n",
      "Epoch 36801/150000, Train Loss: 74, Val Loss: 426,  Lear. Rate: 0.00500, Train Grad.: 0.4\n",
      "Epoch 36901/150000, Train Loss: 72, Val Loss: 427,  Lear. Rate: 0.00500, Train Grad.: 0.4\n",
      "Early stopping at epoch 36947 with validation loss 485.7889404296875.\n",
      "Test Loss: 786.464599609375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np  # Added this line\n",
    "\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [3]\n",
    "num_layers_list = [1]\n",
    "\n",
    "learning_rates = [0.005]\n",
    "\n",
    "window_sizes = [15, 20, 25]\n",
    "dropout_probs = [0]\n",
    "weight_decays = [0]\n",
    "factors = [0.5]\n",
    "patience_lr = [10]\n",
    "\n",
    "num_epochs = 150000\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes, dropout_probs, weight_decays, factors, patience_lr))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob, weight_decay, factor, patience = hyperparams\n",
    "\n",
    "\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}, dropout_prob={dropout_prob}, weight_decay={weight_decay}, factor={factor}, patience={patience}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size, dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize the scheduler after defining the optimizer\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "\n",
    "    # segment sequential data into smaller windows using a sliding window approach,\n",
    "    # ensuring temporal coherence, and returns the windows as tensors for training sequential models.\n",
    "    def split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size):\n",
    "        # Check if the lengths of x_train_tensor and y_train_tensor match\n",
    "        if len(x_train_tensor) != len(y_train_tensor):\n",
    "            raise ValueError(\"Lengths of x_train_tensor and y_train_tensor must match.\")\n",
    "\n",
    "        # Initialize lists to store sequential windows\n",
    "        x_seq_list, y_seq_list = [], []\n",
    "\n",
    "        # Iterate through the data with the specified window size\n",
    "        for i in range(len(x_train_tensor) - window_size):\n",
    "            # Extract a window of input features and target output\n",
    "            x_window = x_train_tensor[i:i+window_size]\n",
    "            y_window = y_train_tensor[i+window_size]  # Next entry as target output\n",
    "\n",
    "            x_seq_list.append(x_window)\n",
    "            y_seq_list.append(y_window)\n",
    "\n",
    "        # Concatenate the lists into tensors\n",
    "        x_seq = torch.stack(x_seq_list)\n",
    "        y_seq = torch.stack(y_seq_list)\n",
    "\n",
    "        return x_seq, y_seq\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Split the training data into sliding windows\n",
    "        x_train_seq, y_train_seq = split_data_with_sliding_window(x_train_tensor, y_train_tensor, window_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_train_seq)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_train_seq)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                # Monitor gradients\n",
    "        if epoch % 100 == 0:\n",
    "            (\"Gradients:\")    # print(\"Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    (f\"{name}: {param.grad.norm().item()}\") # print(f\"{name}: {param.grad.norm().item()}\")\n",
    "\n",
    "                # Monitor Learning Rate\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Validate the model\n",
    "        with torch.no_grad():\n",
    "            # Split validation data into sliding windows\n",
    "            x_val_seq, y_val_seq = split_data_with_sliding_window(x_val_tensor, y_val_tensor, window_size)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = model(x_val_seq)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss = criterion(val_outputs, y_val_seq)\n",
    "\n",
    "            #Update the scheduler within the training loop after calculating the validation loss:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} with validation loss {val_loss}.\")\n",
    "                    break\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.0f}, Val Loss: {val_loss.item():.0f},  Lear. Rate: {current_lr:.5f}, Train Grad.: {param.grad.norm().item():.1f}\")\n",
    "\n",
    "    # After the training loop, you can evaluate the model on the test data\n",
    "    # Split test data into sliding windows\n",
    "    x_test_seq, y_test_seq = split_data_with_sliding_window(x_test_tensor, y_test_tensor, window_size)\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test_seq)\n",
    "        test_loss = criterion(test_outputs, y_test_seq)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
