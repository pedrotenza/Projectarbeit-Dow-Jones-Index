{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Data Loading and Preprocessing:\n",
    "\n",
    "- The code starts by importing necessary libraries, including Pandas, NumPy, Matplotlib, and PyTorch.\n",
    "- It reads a CSV file (aapl_raw_data.csv) into a Pandas DataFrame and performs initial data exploration (data.shape and data.tail(1)).\n",
    "- It removes rows beyond the first 10,747 rows (data = data.iloc[:10747]) and handles missing values by filling them with zeros (data=data.fillna(0)).\n",
    "- The data types of certain columns (open, high, low, volume, etc.) are explicitly converted to float.\n",
    "- The script standardizes selected columns using StandardScaler from scikit-learn.\n",
    "\n",
    "Embedding Layer for Date:\n",
    "- Another set of code is provided, starting with the creation of a new DataFrame (data2) containing a 'date' column with dates ranging from '1980-12-12' to '2023-07-31'.\n",
    "- The 'date' column is converted to a datetime format.\n",
    "- Day, month, and year values are extracted from the 'date' column.\n",
    "- Embedding layers are created for day, month, and year using PyTorch's nn.Embedding.\n",
    "- Embeddings are generated for the day, month, and year values, and these embeddings are concatenated into a single tensor (date_embeddings).\n",
    "\n",
    "Printing the Resulting Embeddings:\n",
    "- The script prints the resulting embeddings (print(date_embeddings)).\n",
    "\n",
    "\n",
    "\n",
    "Note: The provided code assumes specific embedding dimensions (embedding_dim = 1). You may need to adjust this dimension based on your specific use case and the size of your dataset.\n",
    "\n",
    "If you have a specific question or if there's something specific you would like to do with the embeddings, please provide more details so that I can assist you further.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2244,  1.8514,  1.4517],\n",
      "        [ 0.6590,  1.8514,  1.4517],\n",
      "        [-1.4248,  1.8514,  1.4517],\n",
      "        ...,\n",
      "        [ 1.1204,  0.1840,  0.4753],\n",
      "        [ 0.2789,  0.1840,  0.4753],\n",
      "        [-0.5472,  0.1840,  0.4753]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "#data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "# Specify the columns you want to standardize\n",
    "columns_to_standardize = [\"open\", \"high\", \"low\", \"volume\", \"adjusted_close\", \"change_percent\", \"avg_vol_20d\"]\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Loop through the columns and standardize each one\n",
    "for column in columns_to_standardize:\n",
    "    data[column] = scaler.fit_transform(data[[column]])\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with a 'date' column containing dates from 1980-12-12 to 2023-07-31\n",
    "data2 = pd.DataFrame({'date': pd.date_range(start='1980-12-12', end='2023-07-31')})\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "data2['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Extract day, month, and year from the date column\n",
    "data['day'] = data2['date'].dt.day\n",
    "data['month'] = data2['date'].dt.month\n",
    "data['year'] = data2['date'].dt.year\n",
    "\n",
    "# Define the embedding dimensions\n",
    "embedding_dim = 1  # You can adjust this dimension as needed\n",
    "\n",
    "# Create embedding layers for day, month, and year\n",
    "day_embedding = nn.Embedding(32, embedding_dim)  # 0-31 days\n",
    "month_embedding = nn.Embedding(13, embedding_dim)  # 1-12 months\n",
    "year_embedding = nn.Embedding(44, embedding_dim)  # Embedding for years from 1980 to 2023\n",
    "\n",
    "# Convert day, month, and year to tensors with Long data type\n",
    "day_tensor = torch.LongTensor(data['day'].values)\n",
    "month_tensor = torch.LongTensor(data['month'].values)\n",
    "year_tensor = torch.LongTensor(data['year'].values - 1980)  # Convert years to an index from 0 to 43\n",
    "\n",
    "\n",
    "# Pass tensors through embedding layers to get embeddings\n",
    "day_embeddings = day_embedding(day_tensor)\n",
    "month_embeddings = month_embedding(month_tensor)\n",
    "year_embeddings = year_embedding(year_tensor)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "date_embeddings = torch.cat((day_embeddings, month_embeddings, year_embeddings), dim=1)\n",
    "\n",
    "# Print the resulting embeddings\n",
    "print(date_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport pickle\\n\\n# Define the path where you want to save the pickle file\\npickle_path = \"../data/data/date_embeddings.pkl\"\\n\\n# Convert tensor to numpy array before saving (optional)\\ndate_embeddings_np = date_embeddings.detach().numpy()\\n\\n# Save the embeddings to a pickle file\\nwith open(pickle_path, \\'wb\\') as f:\\n    pickle.dump(date_embeddings_np, f)\\n\\nprint(f\"Date embeddings saved to {pickle_path}\")\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Define the path where you want to save the pickle file\n",
    "pickle_path = \"../data/data/date_embeddings.pkl\"\n",
    "\n",
    "# Convert tensor to numpy array before saving (optional)\n",
    "date_embeddings_np = date_embeddings.detach().numpy()\n",
    "\n",
    "# Save the embeddings to a pickle file\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(date_embeddings_np, f)\n",
    "\n",
    "print(f\"Date embeddings saved to {pickle_path}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of a Long Short-Term Memory (LSTM) neural network for time series prediction, specifically applied to stock price prediction:\n",
    "\n",
    "\n",
    "\n",
    "Data Preprocessing:\n",
    "Training and testing data are loaded and split using train_test_split.\n",
    "A MinMaxScaler is applied to normalize the input features.\n",
    "Date embeddings are concatenated with the training and testing data.\n",
    "\n",
    "Model Definition:\n",
    "An LSTM model is defined using the LSTMModel class, which extends nn.Module.\n",
    "The LSTM model takes input features, date embeddings, and other hyperparameters such as hidden dimensions, number of layers, and sequence length.\n",
    "The forward method processes the input through the LSTM layer and a fully connected layer (fc).\n",
    "\n",
    "Training Loop:\n",
    "Hyperparameters such as learning rate, number of epochs, and batch size are defined.\n",
    "The training loop iterates over epochs, batches, and performs forward and backward passes.\n",
    "The model is optimized using the Adam optimizer and Mean Squared Error (MSE) loss.\n",
    "Validation loss is printed periodically during training.\n",
    "\n",
    "Feature Extraction:\n",
    "After training, the model is evaluated on the test set, and the hidden states are extracted from the LSTM layer.\n",
    "\n",
    "Printing Hidden States:\n",
    "The hidden states are printed after training.\n",
    "\n",
    "\n",
    "\n",
    "It's worth noting a couple of potential improvements and considerations:\n",
    "The LSTM model's architecture and hyperparameters should be fine-tuned based on the specific problem and dataset.\n",
    "The number of layers (n_layers) is set to 4, which may be too high for some applications. Consider adjusting it based on the complexity of your data.\n",
    "The model is trained using Mean Squared Error (MSE) loss, which may be appropriate for regression tasks. For classification tasks, a different loss function might be needed.\n",
    "Ensure that the input data, especially the sequence length, aligns with the model architecture. The sequence length is set to 65, but it's crucial to match it with the characteristics of the time series data.\n",
    "Finally, if you have a specific question or need further clarification on any part of the code, feel free to ask!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pickle\\n\\n# Define the path from where you want to load the pickle file\\npickle_path = \"../data/data/date_embeddings.pkl\"\\n\\n# Load the embeddings from the pickle file\\nwith open(pickle_path, \\'rb\\') as f:\\n    loaded_date_embeddings = pickle.load(f)\\n\\n# Print a confirmation message\\nprint(f\"Date embeddings loaded from {pickle_path}\")\\n\\n# Now, the variable loaded_date_embeddings contains the embeddings loaded from the pickle file.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pickle\n",
    "\n",
    "# Define the path from where you want to load the pickle file\n",
    "pickle_path = \"../data/data/date_embeddings.pkl\"\n",
    "\n",
    "# Load the embeddings from the pickle file\n",
    "with open(pickle_path, 'rb') as f:\n",
    "    loaded_date_embeddings = pickle.load(f)\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"Date embeddings loaded from {pickle_path}\")\n",
    "\n",
    "# Now, the variable loaded_date_embeddings contains the embeddings loaded from the pickle file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: Learning Rate=0.00015, Sequence Length=65, Batch Size=64, Input Size=7, Date Embedding Dim=3, Hidden Dim=16,Layers=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/4500], Loss: 18978.9629, Val Loss: 25122.5371\n",
      "Epoch [200/4500], Loss: 12384.5264, Val Loss: 21844.5273\n",
      "Epoch [300/4500], Loss: 12511.6289, Val Loss: 18733.0840\n",
      "Epoch [400/4500], Loss: 8806.0029, Val Loss: 16179.9658\n",
      "Epoch [500/4500], Loss: 6498.1045, Val Loss: 14008.1309\n",
      "Epoch [600/4500], Loss: 22702.8613, Val Loss: 12159.3135\n",
      "Epoch [700/4500], Loss: 4821.3374, Val Loss: 10576.2061\n",
      "Epoch [800/4500], Loss: 16599.7891, Val Loss: 9203.2500\n",
      "Epoch [900/4500], Loss: 194.1398, Val Loss: 7996.8320\n",
      "Epoch [1000/4500], Loss: 5888.9082, Val Loss: 6917.7910\n",
      "Epoch [1100/4500], Loss: 8622.8945, Val Loss: 5949.5586\n",
      "Epoch [1200/4500], Loss: 6299.5659, Val Loss: 5083.8535\n",
      "Epoch [1300/4500], Loss: 3943.5681, Val Loss: 4312.1958\n",
      "Epoch [1400/4500], Loss: 1222.7190, Val Loss: 3628.8413\n",
      "Epoch [1500/4500], Loss: 1927.1990, Val Loss: 3030.5356\n",
      "Epoch [1600/4500], Loss: 3081.5498, Val Loss: 2509.4121\n",
      "Epoch [1700/4500], Loss: 0.3001, Val Loss: 2059.1221\n",
      "Epoch [1800/4500], Loss: 1275.9719, Val Loss: 1671.3286\n",
      "Epoch [1900/4500], Loss: 1325.1143, Val Loss: 1341.8975\n",
      "Epoch [2000/4500], Loss: 983.6615, Val Loss: 1063.5125\n",
      "Epoch [2100/4500], Loss: 1178.1841, Val Loss: 830.7096\n",
      "Epoch [2200/4500], Loss: 279.1671, Val Loss: 639.0305\n",
      "Epoch [2300/4500], Loss: 441.7575, Val Loss: 483.3402\n",
      "Epoch [2400/4500], Loss: 2616.8264, Val Loss: 358.3274\n",
      "Epoch [2500/4500], Loss: 53.8224, Val Loss: 261.4714\n",
      "Epoch [2600/4500], Loss: 227.5712, Val Loss: 188.0642\n",
      "Epoch [2700/4500], Loss: 145.0329, Val Loss: 133.8014\n",
      "Epoch [2800/4500], Loss: 1.1942, Val Loss: 93.8877\n",
      "Epoch [2900/4500], Loss: 51.8614, Val Loss: 64.9057\n",
      "Epoch [3000/4500], Loss: 0.3079, Val Loss: 44.7258\n",
      "Epoch [3100/4500], Loss: 0.5024, Val Loss: 30.7606\n",
      "Epoch [3200/4500], Loss: 1.7272, Val Loss: 21.4570\n",
      "Epoch [3300/4500], Loss: 0.6589, Val Loss: 15.1511\n",
      "Epoch [3400/4500], Loss: 0.5251, Val Loss: 10.9573\n",
      "Epoch [3500/4500], Loss: 0.5106, Val Loss: 7.8206\n",
      "Epoch [3600/4500], Loss: 0.4341, Val Loss: 5.7507\n",
      "Epoch [3700/4500], Loss: 0.5978, Val Loss: 4.3414\n",
      "Epoch [3800/4500], Loss: 10.7511, Val Loss: 3.4696\n",
      "Epoch [3900/4500], Loss: 0.4538, Val Loss: 2.7093\n",
      "Epoch [4000/4500], Loss: 0.3240, Val Loss: 2.3354\n",
      "Epoch [4100/4500], Loss: 1.2545, Val Loss: 1.9582\n",
      "Epoch [4200/4500], Loss: 0.9203, Val Loss: 1.9948\n",
      "Epoch [4300/4500], Loss: 0.7822, Val Loss: 1.7736\n",
      "Epoch [4400/4500], Loss: 0.4506, Val Loss: 1.5821\n",
      "Epoch [4500/4500], Loss: 0.4132, Val Loss: 1.7199\n",
      "tensor([[[ 0.0716,  0.2341, -0.4217,  ..., -0.3828,  0.0114,  0.0692]],\n",
      "\n",
      "        [[ 0.0763,  0.2344, -0.4212,  ..., -0.3806,  0.0090,  0.0707]],\n",
      "\n",
      "        [[ 0.0570,  0.2086, -0.4193,  ..., -0.3784,  0.0291,  0.0484]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1668,  0.3518, -0.4245,  ..., -0.3901, -0.0910,  0.1808]],\n",
      "\n",
      "        [[ 0.0559,  0.2081, -0.4193,  ..., -0.3785,  0.0300,  0.0477]],\n",
      "\n",
      "        [[ 0.2534,  0.4353, -0.4111,  ..., -0.3821, -0.1849,  0.2750]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the x_train and y_train data\n",
    "x_train = data[['open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].to_numpy()\n",
    "y_train = data[\"close\"].to_numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate date embeddings with your training data\n",
    "x_train_feature_tensors = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_train_date_embeddings = date_embeddings[:len(x_train_scaled)]  # Use the same length as your training data\n",
    "x_train_combined = torch.cat((x_train_feature_tensors, x_train_date_embeddings), dim=1)\n",
    "\n",
    "# Concatenate date embeddings with your testing data\n",
    "x_test_feature_tensors = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "x_test_date_embeddings = date_embeddings[len(x_train_scaled):]  # Use the remaining embeddings for testing data\n",
    "x_test_combined = torch.cat((x_test_feature_tensors, x_test_date_embeddings), dim=1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Convert the combined testing data to PyTorch tensors\n",
    " x_test_tensor = torch.tensor(x_test_combined, dtype=torch.float32)\n",
    "\n",
    "# Create y_train_tensor and y_test_tensor\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Copy x_train_combined to create x_train_tensor\n",
    "x_train_tensor = x_train_combined.clone().detach()\n",
    "x_train_tensor = x_train_tensor.to(torch.float32)\n",
    "\n",
    "# Copy x_test_combined to create x_test_tensor\n",
    "x_test_tensor = x_test_combined.clone().detach()\n",
    "x_test_tensor = x_test_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inside your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size + date_embedding_dim  # Updated input size to include date embeddings\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Define the LSTM layer as a class attribute\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Ensure input has the shape [batch_size, sequence_length, input_size]\n",
    "        x = x.view(batch_size, 1, self.input_size)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 7  # Adjust based on the number of input features (excluding change_percent)\n",
    "date_embedding_dim = 3  # Adjust based on the dimension of your date embeddings\n",
    "output_size = 1\n",
    "hidden_dim = 16\n",
    "n_layers = 4\n",
    "sequence_length = 65  # Keep this as 1 for your input data\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader for batch training\n",
    "train_data = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, date_embedding_dim, hidden_dim, n_layers, output_size, sequence_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.00015\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Hyperparameters: Learning Rate={learning_rate}, Sequence Length={sequence_length}, Batch Size={batch_size}, Input Size={input_size}, Date Embedding Dim={date_embedding_dim}, Hidden Dim={hidden_dim},'\n",
    "              f'Layers={n_layers}')\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 4500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, batch_y.view(-1, 1))  # Ensure batch_y has the right shape\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Extract features on the testing set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Extract features from the hidden states\n",
    "            hidden_states, _ = model.lstm(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "            val_outputs = model(x_test_tensor)\n",
    "            val_loss = loss_function(val_outputs, y_test_tensor.view(-1, 1))  # Ensure y_test_tensor has the right shape\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "        # You can now use 'hidden_states' as the feature representations of your sequences\n",
    "        # The shape of 'hidden_states' will be (batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "# Print hidden_states after training\n",
    "print(hidden_states)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train_feature_tensors: torch.Size([7200, 7])\n",
      "Shape of x_train_date_embeddings: torch.Size([7200, 3])\n",
      "Shape of x_test_feature_tensors:  torch.Size([3547, 7])\n",
      "Shape of x_test_date_embeddings:  torch.Size([3547, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of x_train_feature_tensors:\", x_train_feature_tensors.shape)\n",
    "print(\"Shape of x_train_date_embeddings:\", x_train_date_embeddings.shape)\n",
    "\n",
    "print(\"Shape of x_test_feature_tensors: \", x_test_feature_tensors.shape)\n",
    "print(\"Shape of x_test_date_embeddings: \", x_test_date_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (7200, 7)\n",
      "Shape of x_test: (3547, 7)\n",
      "Shape of y_train: (2837, 1)\n",
      "Shape of y_test: (710, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nimport pickle\\n\\n# Save hidden_states using pickle\\nwith open('hidden_states.pkl', 'wb') as f:\\n    pickle.dump(hidden_states, f)\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save hidden_states using pickle\n",
    "with open('hidden_states.pkl', 'wb') as f:\n",
    "    pickle.dump(hidden_states, f)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4743,  0.3444, -0.5437,  ..., -0.5118, -0.0398,  0.0840]],\n",
      "\n",
      "        [[ 0.4728,  0.3463, -0.5443,  ..., -0.5133, -0.0426,  0.0871]],\n",
      "\n",
      "        [[ 0.4406,  0.3176, -0.5339,  ..., -0.5004, -0.0123,  0.0586]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5916,  0.4664, -0.5911,  ..., -0.5719, -0.2051,  0.2398]],\n",
      "\n",
      "        [[ 0.4406,  0.3168, -0.5336,  ..., -0.4998, -0.0113,  0.0576]],\n",
      "\n",
      "        [[ 0.6520,  0.5440, -0.6259,  ..., -0.6183, -0.3507,  0.3760]]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pickle\n",
    "\n",
    "# Load hidden_states from the saved file\n",
    "with open('hidden_states.pkl', 'rb') as f:\n",
    "    loaded_hidden_states = pickle.load(f)\n",
    "\n",
    "# Now, you can use loaded_hidden_states as you would normally use hidden_states\n",
    "print(loaded_hidden_states)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The code segments demonstrate the integration of LSTM features and date embeddings into a Random Forest Regressor for predicting the closing stock prices. Random Forest model is trained on the combined features obtained from the LSTM model and the one-hot encoding for the target date. This approach leverages the strengths of both models for time series prediction.\n",
    "\n",
    "\n",
    "- Generate Date Embedding for Target Date (11th August):\n",
    "    -The code first defines the target date as \"11th August\" and creates one-hot encodings for day, month, and year.\n",
    "    -It then maps the target date to a one-hot encoding and concatenates these encodings to create the date embedding for the target date.\n",
    "\n",
    "- Combine Hidden States with Date Embedding:\n",
    "    -The date embedding for the target date is broadcasted to match the shape of the hidden states obtained from the LSTM model.\n",
    "    -The hidden states and the broadcasted date embedding are concatenated along the third dimension to create a new tensor (combined_states).\n",
    "\n",
    "- Train Random Forest Regressor:\n",
    "    - The code initializes a Random Forest Regressor and fits it to the combined feature tensor (combined_states_2d) and the predicted close values (close_values).\n",
    "\n",
    "- Predict Close Value for 11th August 2023:\n",
    "    -The trained Random Forest model is then used to predict the close value for the target date (11th August 2023) based on the combined features.\n",
    "    \n",
    "- Print Predicted Close Value:\n",
    "    -The predicted close value for 11th August 2023 is printed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    hidden_states, _ = model.lstm(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "    val_outputs = model(x_test_tensor)\n",
    "    y_test_predictions = val_outputs\n",
    "\n",
    "# Define a function to generate date embeddings\n",
    "def generate_date_embedding(target_date):\n",
    "    day_index = int(target_date.split(\" \")[0].replace(\"st\", \"\").replace(\"nd\", \"\").replace(\"rd\", \"\").replace(\"th\", \"\")) - 1\n",
    "    month_index = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "    max_day = 31\n",
    "    max_month = 12\n",
    "    max_year = 100\n",
    "\n",
    "    day_encoding = torch.zeros(max_day)\n",
    "    month_encoding = torch.zeros(max_month)\n",
    "    year_encoding = torch.zeros(max_year + 1)\n",
    "\n",
    "    day_encoding[day_index] = 1\n",
    "    month_encoding[month_index] = 1\n",
    "    year_encoding[43] = 1\n",
    "\n",
    "    date_embedding = torch.cat((day_encoding, month_encoding, year_encoding), dim=0)\n",
    "     # Broadcast the date embedding to match the shape of hidden_states\n",
    "    date_embedding_broadcasted = date_embedding.unsqueeze(0).repeat(hidden_states.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.8511\n",
      "Epoch [2/100], Loss: 1.2965\n",
      "Epoch [3/100], Loss: 1.0295\n",
      "Epoch [4/100], Loss: 0.5330\n",
      "Epoch [5/100], Loss: 1.5196\n",
      "Epoch [6/100], Loss: 1.1157\n",
      "Epoch [7/100], Loss: 1.0258\n",
      "Epoch [8/100], Loss: 1.2604\n",
      "Epoch [9/100], Loss: 0.8456\n",
      "Epoch [10/100], Loss: 1.0653\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the JointModel\n",
    "class JointModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(JointModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the JointModel\n",
    "class JointModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(JointModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.date_embedding_layer = nn.Linear(43 + 12 + 101, 64)  # Adjust dimensions as needed\n",
    "        self.fc = nn.Linear(hidden_size + 64, output_size)\n",
    "\n",
    "    def forward(self, x, date_embedding):\n",
    "        _, (hidden_states, _) = self.lstm(x)\n",
    "        date_embedding_transformed = self.date_embedding_layer(date_embedding)\n",
    "        combined_features = torch.cat((hidden_states.squeeze(0), date_embedding_transformed), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Adjustable Parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "accumulation_steps = 4\n",
    "input_size = 10\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "# Initialize the joint model\n",
    "joint_model = JointModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(joint_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward and backward passes with gradient accumulation\n",
    "    for i in range(0, 7200, batch_size * accumulation_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for j in range(accumulation_steps):\n",
    "            # Generate dummy tensors for demonstration (replace with actual data)\n",
    "            x_batch = torch.randn(batch_size, 1, input_size)\n",
    "            date_embedding_batch = torch.randn(batch_size, 43 + 12 + 101)\n",
    "\n",
    "            outputs = joint_model(x_batch, date_embedding_batch)\n",
    "            loss = criterion(outputs, torch.randn(batch_size, 1))  # Example target tensor\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for monitoring\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# After training, you can proceed with the evaluation and prediction steps\n",
    "# Please adapt the evaluation and prediction code based on your specific requirements\n",
    "\n",
    "        self.date_embedding_layer = nn.Linear(43 + 12 + 101, 64)  # Adjust dimensions as needed\n",
    "        self.fc = nn.Linear(hidden_size + 64, output_size)\n",
    "\n",
    "    def forward(self, x, date_embedding):\n",
    "        _, (hidden_states, _) = self.lstm(x)\n",
    "        date_embedding_transformed = self.date_embedding_layer(date_embedding)\n",
    "        combined_features = torch.cat((hidden_states.squeeze(0), date_embedding_transformed), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Adjustable Parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "accumulation_steps = 4\n",
    "input_size = 10\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "# Initialize the joint model\n",
    "joint_model = JointModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(joint_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward and backward passes with gradient accumulation\n",
    "    for i in range(0, 7200, batch_size * accumulation_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for j in range(accumulation_steps):\n",
    "            # Generate dummy tensors for demonstration (replace with actual data)\n",
    "            x_batch = torch.randn(batch_size, 1, input_size)\n",
    "            date_embedding_batch = torch.randn(batch_size, 43 + 12 + 101)\n",
    "\n",
    "            outputs = joint_model(x_batch, date_embedding_batch)\n",
    "            loss = criterion(outputs, torch.randn(batch_size, 1))  # Example target tensor\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for monitoring\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# After training, you can proceed with the evaluation and prediction steps\n",
    "# Please adapt the evaluation and prediction code based on your specific requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'date_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming `tensor` is your tensor of interest\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdate_embedding\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(date_embedding\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'date_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming `tensor` is your tensor of interest\n",
    "print(date_embedding.shape)\n",
    "print(date_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7200, 10])\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the input tensor\n",
    "print(x_train_tensor.size())\n",
    "\n",
    "# Initialize the joint model with the correct input size\n",
    "input_size = x_train_tensor.size(-1)  # Update the input size based on the actual size of the input tensor\n",
    "hidden_size = 64  # LSTM hidden size\n",
    "output_size = 1  # Output size\n",
    "joint_model = JointModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# Continue with the training loop and other steps as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dinamic Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 1, 16])\n",
      "torch.Size([5000, 1, 67])\n",
      "torch.Size([5000, 1, 83])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states.shape)\n",
    "print(date_embedding_broadcasted.shape)\n",
    "print(combined_states.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2139630920647225\n",
      "Predicted Close Value for 1st August: [ 41.62678303  43.11470383  28.55921614 ... 112.76144005  28.0626162\n",
      " 174.71596466]\n",
      "\n",
      "MSE: 0.2139630920647225\n",
      "Predicted Close Value for 15th August: [ 41.62678303  43.11470383  28.55921614 ... 112.76144005  28.0626162\n",
      " 174.71596466]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    hidden_states, _ = model.lstm(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "    val_outputs = model(x_test_tensor)\n",
    "    y_test_predictions = val_outputs\n",
    "\n",
    "# Define a function to generate date embeddings\n",
    "def generate_date_embedding(target_date):\n",
    "    day_index = int(target_date.split(\" \")[0].replace(\"st\", \"\").replace(\"nd\", \"\").replace(\"rd\", \"\").replace(\"th\", \"\")) - 1\n",
    "    month_index = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "    max_day = 31\n",
    "    max_month = 12\n",
    "    max_year = 100\n",
    "\n",
    "    day_encoding = torch.zeros(max_day)\n",
    "    month_encoding = torch.zeros(max_month)\n",
    "    year_encoding = torch.zeros(max_year + 1)\n",
    "\n",
    "    day_encoding[day_index] = 1\n",
    "    month_encoding[month_index] = 1\n",
    "    year_encoding[43] = 1\n",
    "\n",
    "    date_embedding = torch.cat((day_encoding, month_encoding, year_encoding), dim=0)\n",
    "    return date_embedding\n",
    "\n",
    "\n",
    "# List of target dates for evaluation\n",
    "list_of_dates = [\"1st August\", \"15th August\"]\n",
    "\n",
    "for target_date in list_of_dates:\n",
    "    # Generate date embedding for the target_date\n",
    "    date_embedding = generate_date_embedding(target_date)\n",
    "\n",
    "    # Broadcast the date embedding to match the shape of hidden_states\n",
    "    date_embedding_broadcasted = date_embedding.unsqueeze(0).repeat(hidden_states.shape[0], 1, 1)\n",
    "\n",
    "    # Combine hidden_states and date_embedding\n",
    "    combined_states = torch.cat((hidden_states, date_embedding_broadcasted), dim=2)\n",
    "\n",
    "    # Convert y_test_predictions to numpy array (assuming it's a tensor)\n",
    "    close_values = y_test_predictions.numpy()\n",
    "\n",
    "    # Reshape combined_states to remove the extra dimension\n",
    "    combined_states_reshaped = combined_states.reshape(-1, combined_states.shape[-1])\n",
    "\n",
    "    # Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_states_reshaped, close_values, test_size=0.2, random_state=43)\n",
    "\n",
    "    # Initialize and train the RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=43)\n",
    "    rf_model.fit(X_train, y_train.ravel())  # Using ravel() to avoid DataConversionWarning\n",
    "\n",
    "    # Predict on the reshaped test data (X_test)\n",
    "    predicted_close_test = rf_model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics for the test predictions\n",
    "    mae_test = mean_absolute_error(y_test, predicted_close_test)\n",
    "    mse_test = mean_squared_error(y_test, predicted_close_test)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    r2_test = r2_score(y_test, predicted_close_test)\n",
    "\n",
    "    # Print the metrics for the test predictions\n",
    "    #print(f\"Evaluation metrics for {target_date} (Test Set):\")\n",
    "    #print(f\"MAE: {mae_test}\")\n",
    "    print(f\"MSE: {mse_test}\")\n",
    "    #print(f\"RMSE: {rmse_test}\")\n",
    "    #print(f\"R2 Score: {r2_test}\")\n",
    "\n",
    "    # Predict the 'close' value for the combined_states_reshaped for the target date\n",
    "    predicted_close = rf_model.predict(combined_states_reshaped)\n",
    "\n",
    "    # Print the predicted 'close' value for the target date\n",
    "    print(f\"Predicted Close Value for {target_date}: {predicted_close}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.13244102817911238\n",
      "Predicted Close Value for 1st August: [ 41.75436844  42.91783333  28.43019012 ... 112.78553429  28.0711599\n",
      " 174.27684525]\n",
      "\n",
      "MSE: 0.13244102817911238\n",
      "Predicted Close Value for 15th August: [ 41.75436844  42.91783333  28.43019012 ... 112.78553429  28.0711599\n",
      " 174.27684525]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    hidden_states, _ = model.lstm(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "    val_outputs = model(x_test_tensor)\n",
    "    y_test_predictions = val_outputs\n",
    "\n",
    "# Define a function to generate date embeddings\n",
    "def generate_date_embedding(target_date):\n",
    "    day_index = int(target_date.split(\" \")[0].replace(\"st\", \"\").replace(\"nd\", \"\").replace(\"rd\", \"\").replace(\"th\", \"\")) - 1\n",
    "    month_index = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "    max_day = 31\n",
    "    max_month = 12\n",
    "    max_year = 100\n",
    "\n",
    "    day_encoding = torch.zeros(max_day)\n",
    "    month_encoding = torch.zeros(max_month)\n",
    "    year_encoding = torch.zeros(max_year + 1)\n",
    "\n",
    "    day_encoding[day_index] = 1\n",
    "    month_encoding[month_index] = 1\n",
    "    year_encoding[43] = 1\n",
    "\n",
    "    date_embedding = torch.cat((day_encoding, month_encoding, year_encoding), dim=0)\n",
    "    return date_embedding\n",
    "\n",
    "\n",
    "# List of target dates for evaluation\n",
    "list_of_dates = [\"1st August\", \"15th August\"]\n",
    "\n",
    "for target_date in list_of_dates:\n",
    "    # Generate date embedding for the target_date\n",
    "    date_embedding = generate_date_embedding(target_date)\n",
    "\n",
    "    # Broadcast the date embedding to match the shape of hidden_states\n",
    "    date_embedding_broadcasted = date_embedding.unsqueeze(0).repeat(hidden_states.shape[0], 1, 1)\n",
    "\n",
    "    # Combine hidden_states and date_embedding\n",
    "    combined_states = torch.cat((hidden_states, date_embedding_broadcasted), dim=2)\n",
    "\n",
    "    # Convert y_test_predictions to numpy array (assuming it's a tensor)\n",
    "    close_values = y_test_predictions.numpy()\n",
    "\n",
    "    # Reshape combined_states to remove the extra dimension\n",
    "    combined_states_reshaped = combined_states.reshape(-1, combined_states.shape[-1])\n",
    "\n",
    "    # Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_states_reshaped, close_values, test_size=0.2, random_state=43)\n",
    "\n",
    "    # Initialize and train the RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=43)\n",
    "    rf_model.fit(X_train, y_train.ravel())  # Using ravel() to avoid DataConversionWarning\n",
    "\n",
    "    # Predict on the reshaped test data (X_test)\n",
    "    predicted_close_test = rf_model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics for the test predictions\n",
    "    mae_test = mean_absolute_error(y_test, predicted_close_test)\n",
    "    mse_test = mean_squared_error(y_test, predicted_close_test)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    r2_test = r2_score(y_test, predicted_close_test)\n",
    "\n",
    "    # Print the metrics for the test predictions\n",
    "    #print(f\"Evaluation metrics for {target_date} (Test Set):\")\n",
    "    #print(f\"MAE: {mae_test}\")\n",
    "    print(f\"MSE: {mse_test}\")\n",
    "    #print(f\"RMSE: {rmse_test}\")\n",
    "    #print(f\"R2 Score: {r2_test}\")\n",
    "\n",
    "    # Predict the 'close' value for the combined_states_reshaped for the target date\n",
    "    predicted_close = rf_model.predict(combined_states_reshaped)\n",
    "\n",
    "    # Print the predicted 'close' value for the target date\n",
    "    print(f\"Predicted Close Value for {target_date}: {predicted_close}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.13244102817911238\n",
      "Predicted Close Value for 1st August: [ 41.75436844  42.91783333  28.43019012 ... 112.78553429  28.0711599\n",
      " 174.27684525]\n",
      "\n",
      "MSE: 0.13244102817911238\n",
      "Predicted Close Value for 15th August: [ 41.75436844  42.91783333  28.43019012 ... 112.78553429  28.0711599\n",
      " 174.27684525]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    hidden_states, _ = model.lstm(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "    val_outputs = model(x_test_tensor)\n",
    "    y_test_predictions = val_outputs\n",
    "\n",
    "# Define a function to generate date embeddings\n",
    "def generate_date_embedding(target_date):\n",
    "    day_index = int(target_date.split(\" \")[0].replace(\"st\", \"\").replace(\"nd\", \"\").replace(\"rd\", \"\").replace(\"th\", \"\")) - 1\n",
    "    month_index = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "    max_day = 31\n",
    "    max_month = 12\n",
    "    max_year = 100\n",
    "\n",
    "    day_encoding = torch.zeros(max_day)\n",
    "    month_encoding = torch.zeros(max_month)\n",
    "    year_encoding = torch.zeros(max_year + 1)\n",
    "\n",
    "    day_encoding[day_index] = 1\n",
    "    month_encoding[month_index] = 1\n",
    "    year_encoding[43] = 1\n",
    "\n",
    "    date_embedding = torch.cat((day_encoding, month_encoding, year_encoding), dim=0)\n",
    "    return date_embedding\n",
    "\n",
    "# Load your AAPL stock data (Make sure to adjust the path as per your directory structure)\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "# List of target dates for evaluation\n",
    "list_of_dates = [\"1st August\", \"15th August\"]\n",
    "\n",
    "for target_date in list_of_dates:\n",
    "    # Generate date embedding for the target_date\n",
    "    date_embedding = generate_date_embedding(target_date)\n",
    "\n",
    "    # Broadcast the date embedding to match the shape of hidden_states\n",
    "    date_embedding_broadcasted = date_embedding.unsqueeze(0).repeat(hidden_states.shape[0], 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "    # Convert y_test_predictions to numpy array (assuming it's a tensor)\n",
    "    close_values = y_test_predictions.numpy()\n",
    "\n",
    "    # Reshape combined_states to remove the extra dimension\n",
    "    combined_states_reshaped = combined_states.reshape(-1, combined_states.shape[-1])\n",
    "\n",
    "    # Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_states_reshaped, close_values, test_size=0.2, random_state=43)\n",
    "\n",
    "    # Initialize and train the RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=43)\n",
    "    rf_model.fit(X_train, y_train.ravel())  # Using ravel() to avoid DataConversionWarning\n",
    "\n",
    "    # Predict on the reshaped test data (X_test)\n",
    "    predicted_close_test = rf_model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics for the test predictions\n",
    "    mae_test = mean_absolute_error(y_test, predicted_close_test)\n",
    "    mse_test = mean_squared_error(y_test, predicted_close_test)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    r2_test = r2_score(y_test, predicted_close_test)\n",
    "\n",
    "    # Print the metrics for the test predictions\n",
    "    #print(f\"Evaluation metrics for {target_date} (Test Set):\")\n",
    "    #print(f\"MAE: {mae_test}\")\n",
    "    print(f\"MSE: {mse_test}\")\n",
    "    #print(f\"RMSE: {rmse_test}\")\n",
    "    #print(f\"R2 Score: {r2_test}\")\n",
    "\n",
    "    # Predict the 'close' value for the combined_states_reshaped for the target date\n",
    "    predicted_close = rf_model.predict(combined_states_reshaped)\n",
    "\n",
    "    # Print the predicted 'close' value for the target date\n",
    "    print(f\"Predicted Close Value for {target_date}: {predicted_close}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of hidden_states: torch.Size([3547, 1, 16])\n",
      "Shape of date_embedding_broadcasted: torch.Size([3547, 1, 144])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of hidden_states:\", hidden_states.shape)\n",
    "print(\"Shape of date_embedding_broadcasted:\", date_embedding_broadcasted.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined_states_reshaped: torch.Size([3547, 160])\n",
      "Shape of close_values: (3547, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Close Value for 1st August: [[-0.0596607   0.0196106   0.03657901 ...  0.23620727 -0.06977356\n",
      "  -0.03142575]\n",
      " [ 0.03013961 -0.12598329  0.08704979 ...  0.0418684  -0.09994098\n",
      "   0.00280301]\n",
      " [-0.06291041 -0.06442257  0.03296388 ... -0.0788666   0.0711974\n",
      "  -0.07564339]\n",
      " ...\n",
      " [ 0.14101837  0.03464901  0.11194071 ...  0.13066217  0.03582651\n",
      "   0.16494781]\n",
      " [ 0.13453363 -0.05943837  0.3274678  ... -0.02344289 -0.14869018\n",
      "   0.29203854]\n",
      " [ 0.07976641 -0.05209781  0.1851694  ...  0.12625204 -0.08235208\n",
      "   0.16313479]]\n",
      "\n",
      "Shape of combined_states_reshaped: torch.Size([3547, 160])\n",
      "Shape of close_values: (3547, 16)\n",
      "Predicted Close Value for 15th August: [[-0.0596607   0.0196106   0.03657901 ...  0.23620727 -0.06977356\n",
      "  -0.03142575]\n",
      " [ 0.03013961 -0.12598329  0.08704979 ...  0.0418684  -0.09994098\n",
      "   0.00280301]\n",
      " [-0.06291041 -0.06442257  0.03296388 ... -0.0788666   0.0711974\n",
      "  -0.07564339]\n",
      " ...\n",
      " [ 0.14101837  0.03464901  0.11194071 ...  0.13066217  0.03582651\n",
      "   0.16494781]\n",
      " [ 0.13453363 -0.05943837  0.3274678  ... -0.02344289 -0.14869018\n",
      "   0.29203854]\n",
      " [ 0.07976641 -0.05209781  0.1851694  ...  0.12625204 -0.08235208\n",
      "   0.16313479]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a mock LSTM model for demonstration\n",
    "class MockLSTMModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MockLSTMModel, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size=16, hidden_size=16, num_layers=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lstm(x)\n",
    "\n",
    "# Instantiate the mock LSTM model\n",
    "model = MockLSTMModel()\n",
    "model.eval()\n",
    "\n",
    "# Generate mock hidden_states and x_test_tensor for demonstration\n",
    "hidden_states = torch.randn(3547, 1, 16)\n",
    "x_test_tensor = torch.randn(3547, 16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden_states, _ = model.lstm(x_test_tensor.view(x_test_tensor.size(0), 1, -1))\n",
    "    val_outputs = model(x_test_tensor)\n",
    "    y_test_predictions = val_outputs[0]\n",
    "\n",
    "# Function to generate date embeddings\n",
    "def generate_date_embedding(target_date):\n",
    "    day_index = int(target_date.split(\" \")[0].replace(\"st\", \"\").replace(\"nd\", \"\").replace(\"rd\", \"\").replace(\"th\", \"\")) - 1\n",
    "    month_index = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "    max_day = 31\n",
    "    max_month = 12\n",
    "    max_year = 100\n",
    "\n",
    "    day_encoding = torch.zeros(max_day)\n",
    "    month_encoding = torch.zeros(max_month)\n",
    "    year_encoding = torch.zeros(max_year + 1)\n",
    "\n",
    "    day_encoding[day_index] = 1\n",
    "    month_encoding[month_index] = 1\n",
    "    year_encoding[43] = 1\n",
    "\n",
    "    date_embedding = torch.cat((day_encoding, month_encoding, year_encoding), dim=0)\n",
    "    return date_embedding\n",
    "\n",
    "# List of target dates for evaluation\n",
    "list_of_dates = [\"1st August\", \"15th August\"]\n",
    "\n",
    "for target_date in list_of_dates:\n",
    "    date_embedding = generate_date_embedding(target_date)\n",
    "    date_embedding_broadcasted = date_embedding.unsqueeze(0).repeat(hidden_states.shape[0], 1, 1)\n",
    "\n",
    "    combined_states = torch.cat((hidden_states, date_embedding_broadcasted), dim=2)\n",
    "    combined_states_reshaped = combined_states.view(-1, combined_states.shape[-1])\n",
    "\n",
    "    close_values = y_test_predictions.squeeze().numpy()\n",
    "\n",
    "    # Debugging: Print the shapes for debugging\n",
    "    print(f\"Shape of combined_states_reshaped: {combined_states_reshaped.shape}\")\n",
    "    print(f\"Shape of close_values: {close_values.shape}\")\n",
    "\n",
    "    assert combined_states_reshaped.shape[0] == close_values.shape[0], \"Number of samples mismatch!\"\n",
    "\n",
    "    # Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_states_reshaped.numpy(), close_values, test_size=0.2, random_state=43)\n",
    "\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=43)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the 'close' value for the combined_states_reshaped for the target date\n",
    "    predicted_close = rf_model.predict(combined_states_reshaped.numpy())\n",
    "\n",
    "    print(f\"Predicted Close Value for {target_date}: {predicted_close}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 st August 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Close Value for 1st August: [[-0.0596607   0.0196106   0.03657901 ...  0.23620727 -0.06977356\n",
      "  -0.03142575]\n",
      " [ 0.03013961 -0.12598329  0.08704979 ...  0.0418684  -0.09994098\n",
      "   0.00280301]\n",
      " [-0.06291041 -0.06442257  0.03296388 ... -0.0788666   0.0711974\n",
      "  -0.07564339]\n",
      " ...\n",
      " [ 0.14101837  0.03464901  0.11194071 ...  0.13066217  0.03582651\n",
      "   0.16494781]\n",
      " [ 0.13453363 -0.05943837  0.3274678  ... -0.02344289 -0.14869018\n",
      "   0.29203854]\n",
      " [ 0.07976641 -0.05209781  0.1851694  ...  0.12625204 -0.08235208\n",
      "   0.16313479]]\n",
      "Closing 1st August 2023: 195.61\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate Date Embedding for 1st August:\n",
    "target_date_1st = \"1st August\"\n",
    "day_index_1st = int(target_date_1st.split(\" \")[0].replace(\"st\", \"\").replace(\"nd\", \"\").replace(\"rd\", \"\").replace(\"th\", \"\")) - 1\n",
    "month_index_1st = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "# Assuming max_day, max_month, max_year, and hidden_states are defined elsewhere\n",
    "max_day = 31\n",
    "max_month = 12\n",
    "max_year = 100\n",
    "\n",
    "day_encoding_1st = torch.zeros(max_day)\n",
    "month_encoding_1st = torch.zeros(max_month)\n",
    "year_encoding_1st = torch.zeros(max_year + 1)\n",
    "\n",
    "day_encoding_1st[day_index_1st] = 1\n",
    "month_encoding_1st[month_index_1st] = 1\n",
    "year_encoding_1st[43] = 1\n",
    "\n",
    "date_embedding_1st = torch.cat((day_encoding_1st, month_encoding_1st, year_encoding_1st), dim=0)\n",
    "date_embedding_1st_broadcasted = date_embedding_1st.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "combined_states_1st = torch.cat((hidden_states, date_embedding_1st_broadcasted), dim=2)\n",
    "\n",
    "# Extract predicted 'close' values as a numpy array for 1st August\n",
    "close_values_1st = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape combined_states_1st to remove the extra dimension\n",
    "combined_states_1st_2d = combined_states_1st.reshape(-1, combined_states_1st.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets for 1st August with a different random_state\n",
    "X_train_1st, X_test_1st, y_train_1st, y_test_1st = train_test_split(combined_states_1st_2d, close_values_1st, test_size=0.2, random_state=43)  # Changed random_state to 43\n",
    "\n",
    "# Initialize the Random Forest Regressor for 1st August\n",
    "rf_model_1st = RandomForestRegressor(n_estimators=100, random_state=43)\n",
    "rf_model_1st.fit(X_train_1st, y_train_1st)\n",
    "y_pred_1st = rf_model_1st.predict(X_test_1st)\n",
    "predicted_close_1st = rf_model_1st.predict(combined_states_1st_2d)\n",
    "\n",
    "# Calculate evaluation metrics for 1st August\n",
    "mae_1st = mean_absolute_error(y_test_1st, y_pred_1st)\n",
    "mse_1st = mean_squared_error(y_test_1st, y_pred_1st)\n",
    "rmse_1st = np.sqrt(mse_1st)\n",
    "r2_1st = r2_score(y_test_1st, y_pred_1st)\n",
    "\n",
    "# Print the predicted \"close\" value for 1st August\n",
    "print(\"Predicted Close Value for 1st August:\", predicted_close_1st)\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "# Print the value of the \"close\" column for the date 1st August 2023\n",
    "date_filter = data[data['date'] == '2023-08-01']\n",
    "close_value = date_filter['close'].values[0]\n",
    "print(f\"Closing 1st August 2023: {close_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11th August 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.00524252483473877\n",
      "Predicted Close Value for 11th August: [[-0.06268863 -0.00119187  0.00773499 ...  0.24643942 -0.0882733\n",
      "  -0.03402392]\n",
      " [ 0.03761899 -0.12755952  0.10113487 ...  0.05272629 -0.10312968\n",
      "  -0.00848138]\n",
      " [-0.05485251 -0.06256528  0.02657036 ... -0.05956581  0.06104317\n",
      "  -0.04874376]\n",
      " ...\n",
      " [ 0.14344806  0.02752012  0.11326391 ...  0.12436044  0.03839121\n",
      "   0.15973297]\n",
      " [ 0.1126018  -0.05501558  0.31566225 ... -0.01838417 -0.15746938\n",
      "   0.26818339]\n",
      " [ 0.07721089 -0.0653956   0.20254177 ...  0.12405528 -0.09182393\n",
      "   0.22451101]]\n",
      "The closing value on 11th August 2023 was: 177.79\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate Date Embedding for 11th August:\n",
    "target_date_11th = \"11th August\"\n",
    "day_index_11th = int(target_date_11th.split(\" \")[0].replace(\"st\", \"\").replace(\"nd\", \"\").replace(\"rd\", \"\").replace(\"th\", \"\")) - 1\n",
    "month_index_11th = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "# Assuming max_day, max_month, max_year, and hidden_states are defined elsewhere\n",
    "max_day = 31  # Example value, change as needed\n",
    "max_month = 12\n",
    "max_year = 100  # Example value, change as needed\n",
    "\n",
    "day_encoding_11th = torch.zeros(max_day)\n",
    "month_encoding_11th = torch.zeros(max_month)\n",
    "year_encoding_11th = torch.zeros(max_year + 1)\n",
    "\n",
    "day_encoding_11th[day_index_11th] = 1\n",
    "month_encoding_11th[month_index_11th] = 1\n",
    "year_encoding_11th[43] = 1\n",
    "\n",
    "date_embedding_11th = torch.cat((day_encoding_11th, month_encoding_11th, year_encoding_11th), dim=0)\n",
    "date_embedding_11th_broadcasted = date_embedding_11th.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "combined_states_11th = torch.cat((hidden_states, date_embedding_11th_broadcasted), dim=2)\n",
    "\n",
    "# Extract predicted 'close' values as a numpy array for 11th August\n",
    "close_values_11th = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape combined_states_11th to remove the extra dimension\n",
    "combined_states_11th_2d = combined_states_11th.reshape(-1, combined_states_11th.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets for 11th August with a different random_state\n",
    "X_train_11th, X_test_11th, y_train_11th, y_test_11th = train_test_split(combined_states_11th_2d, close_values_11th, test_size=0.2, random_state=42)  # Changed random_state to 42\n",
    "\n",
    "# Initialize the Random Forest Regressor for 11th August\n",
    "rf_model_11th = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_11th.fit(X_train_11th, y_train_11th)\n",
    "y_pred_11th = rf_model_11th.predict(X_test_11th)\n",
    "predicted_close_11th = rf_model_11th.predict(combined_states_11th_2d)\n",
    "\n",
    "# Calculate evaluation metrics for 11th August\n",
    "mae_11th = mean_absolute_error(y_test_11th, y_pred_11th)\n",
    "mse_11th = mean_squared_error(y_test_11th, y_pred_11th)\n",
    "rmse_11th = np.sqrt(mse_11th)\n",
    "r2_11th = r2_score(y_test_11th, y_pred_11th)\n",
    "\n",
    "#print(\"Metrics for 11th August:\")\n",
    "#print(f'Mean Absolute Error (MAE): {mae_11th}')\n",
    "print(f'Mean Squared Error (MSE): {mse_11th}')\n",
    "#print(f'Root Mean Squared Error (RMSE): {rmse_11th}')\n",
    "#print(f'R-squared (R^2): {r2_11th}')\n",
    "\n",
    "# Print the predicted \"close\" value for 11th August\n",
    "print(\"Predicted Close Value for 11th August:\", predicted_close_11th)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "\n",
    "# Print the value of the \"close\" column for the date 11th August 2023\n",
    "date_filter = data[data['date'] == '2023-08-11']\n",
    "close_value = date_filter['close'].values[0]\n",
    "print(f\"The closing value on 11th August 2023 was: {close_value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31th August 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.0056149470474229825\n",
      "Predicted Close Value for 31st August: [[-0.0596607   0.0196106   0.03657901 ...  0.23620727 -0.06977356\n",
      "  -0.03142575]\n",
      " [ 0.03013961 -0.12598329  0.08704979 ...  0.0418684  -0.09994098\n",
      "   0.00280301]\n",
      " [-0.06291041 -0.06442257  0.03296388 ... -0.0788666   0.0711974\n",
      "  -0.07564339]\n",
      " ...\n",
      " [ 0.14101837  0.03464901  0.11194071 ...  0.13066217  0.03582651\n",
      "   0.16494781]\n",
      " [ 0.13453363 -0.05943837  0.3274678  ... -0.02344289 -0.14869018\n",
      "   0.29203854]\n",
      " [ 0.07976641 -0.05209781  0.1851694  ...  0.12625204 -0.08235208\n",
      "   0.16313479]]\n",
      "The closing value on 31st August 2023 was: 187.87\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate Date Embedding for 31st August:\n",
    "target_date_31st = \"31st August\"\n",
    "day_index_31st = int(target_date_31st.split(\" \")[0].replace(\"st\", \"\").replace(\"nd\", \"\").replace(\"rd\", \"\").replace(\"th\", \"\")) - 1\n",
    "month_index_31st = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "# Assuming max_day, max_month, max_year, and hidden_states are defined elsewhere\n",
    "max_day = 31  # Example value, change as needed\n",
    "max_month = 12\n",
    "max_year = 100  # Example value, change as needed\n",
    "\n",
    "day_encoding_31st = torch.zeros(max_day)\n",
    "month_encoding_31st = torch.zeros(max_month)\n",
    "year_encoding_31st = torch.zeros(max_year + 1)\n",
    "\n",
    "day_encoding_31st[day_index_31st] = 1\n",
    "month_encoding_31st[month_index_31st] = 1\n",
    "year_encoding_31st[43] = 1\n",
    "\n",
    "date_embedding_31st = torch.cat((day_encoding_31st, month_encoding_31st, year_encoding_31st), dim=0)\n",
    "date_embedding_31st_broadcasted = date_embedding_31st.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "combined_states_31st = torch.cat((hidden_states, date_embedding_31st_broadcasted), dim=2)\n",
    "\n",
    "# Extract predicted 'close' values as a numpy array for 31st August\n",
    "close_values_31st = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape combined_states_31st to remove the extra dimension\n",
    "combined_states_31st_2d = combined_states_31st.reshape(-1, combined_states_31st.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets for 31st August with a different random_state\n",
    "X_train_31st, X_test_31st, y_train_31st, y_test_31st = train_test_split(combined_states_31st_2d, close_values_31st, test_size=0.2, random_state=43)  # Changed random_state to 43\n",
    "\n",
    "# Initialize the Random Forest Regressor for 31st August\n",
    "rf_model_31st = RandomForestRegressor(n_estimators=100, random_state=43)\n",
    "rf_model_31st.fit(X_train_31st, y_train_31st)\n",
    "y_pred_31st = rf_model_31st.predict(X_test_31st)\n",
    "predicted_close_31st = rf_model_31st.predict(combined_states_31st_2d)\n",
    "\n",
    "# Calculate evaluation metrics for 31st August\n",
    "mae_31st = mean_absolute_error(y_test_31st, y_pred_31st)\n",
    "mse_31st = mean_squared_error(y_test_31st, y_pred_31st)\n",
    "rmse_31st = np.sqrt(mse_31st)\n",
    "r2_31st = r2_score(y_test_31st, y_pred_31st)\n",
    "\n",
    "#print(\"Metrics for 31st August:\")\n",
    "#print(f'Mean Absolute Error (MAE): {mae_31st}')\n",
    "print(f'Mean Squared Error (MSE): {mse_31st}')\n",
    "#print(f'Root Mean Squared Error (RMSE): {rmse_31st}')\n",
    "#print(f'R-squared (R^2): {r2_31st}')\n",
    "\n",
    "# Print the predicted \"close\" value for 31st August\n",
    "print(\"Predicted Close Value for 31st August:\", predicted_close_31st)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "\n",
    "# Print the value of the \"close\" column for the date 31th August 2023\n",
    "date_filter = data[data['date'] == '2023-08-31']\n",
    "close_value = date_filter['close'].values[0]\n",
    "print(f\"The closing value on 31st August 2023 was: {close_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for 31st August:\n",
      "Mean Absolute Error (MAE): 0.053133741373833074\n",
      "Mean Squared Error (MSE): 0.00524252483473877\n",
      "Root Mean Squared Error (RMSE): 0.07240528181520164\n",
      "R-squared (R^2): 0.68463254978573\n",
      "Predicted Close Value for 31st August: [[-0.06268863 -0.00119187  0.00773499 ...  0.24643942 -0.0882733\n",
      "  -0.03402392]\n",
      " [ 0.03761899 -0.12755952  0.10113487 ...  0.05272629 -0.10312968\n",
      "  -0.00848138]\n",
      " [-0.05485251 -0.06256528  0.02657036 ... -0.05956581  0.06104317\n",
      "  -0.04874376]\n",
      " ...\n",
      " [ 0.14344806  0.02752012  0.11326391 ...  0.12436044  0.03839121\n",
      "   0.15973297]\n",
      " [ 0.1126018  -0.05501558  0.31566225 ... -0.01838417 -0.15746938\n",
      "   0.26818339]\n",
      " [ 0.07721089 -0.0653956   0.20254177 ...  0.12405528 -0.09182393\n",
      "   0.22451101]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate Date Embedding for 31st August:\n",
    "target_date_31st = \"31st August\"\n",
    "# Extract Day Index for 31st August\n",
    "day_index_31st = int(target_date_31st.split(\" \")[0][:-2]) - 1\n",
    "month_index_31st = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "day_encoding_31st = torch.zeros(max_day)\n",
    "month_encoding_31st = torch.zeros(max_month)\n",
    "year_encoding_31st = torch.zeros(max_year + 1)\n",
    "day_encoding_31st[day_index_31st] = 1\n",
    "month_encoding_31st[month_index_31st] = 1\n",
    "year_encoding_31st[43] = 1\n",
    "\n",
    "date_embedding_31st = torch.cat((day_encoding_31st, month_encoding_31st, year_encoding_31st), dim=0)\n",
    "date_embedding_31st_broadcasted = date_embedding_31st.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "combined_states_31st = torch.cat((hidden_states, date_embedding_31st_broadcasted), dim=2)\n",
    "\n",
    "# Extract predicted 'close' values as a numpy array for 31st August\n",
    "close_values_31st = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape combined_states_31st to remove the extra dimension\n",
    "combined_states_31st_2d = combined_states_31st.reshape(-1, combined_states_31st.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets for 31st August\n",
    "X_train_31st, X_test_31st, y_train_31st, y_test_31st = train_test_split(combined_states_31st_2d, close_values_31st, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor for 31st August\n",
    "rf_model_31st = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_31st.fit(X_train_31st, y_train_31st)\n",
    "y_pred_31st = rf_model_31st.predict(X_test_31st)\n",
    "predicted_close_31st = rf_model_31st.predict(combined_states_31st_2d)\n",
    "\n",
    "# Calculate evaluation metrics for 31st August\n",
    "mae_31st = mean_absolute_error(y_test_31st, y_pred_31st)\n",
    "mse_31st = mean_squared_error(y_test_31st, y_pred_31st)\n",
    "rmse_31st = np.sqrt(mse_31st)\n",
    "r2_31st = r2_score(y_test_31st, y_pred_31st)\n",
    "\n",
    "print(\"\\nMetrics for 31st August:\")\n",
    "print(f'Mean Absolute Error (MAE): {mae_31st}')\n",
    "print(f'Mean Squared Error (MSE): {mse_31st}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_31st}')\n",
    "print(f'R-squared (R^2): {r2_31st}')\n",
    "\n",
    "# Print the predicted \"close\" value for 31st August\n",
    "print(\"Predicted Close Value for 31st August:\", predicted_close_31st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2837, 45392]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Initialize the Random Forest Regressor for 11th August\u001b[39;00m\n\u001b[0;32m     35\u001b[0m rf_model_11th \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[43mrf_model_11th\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_11th\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_11th\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m y_pred_11th \u001b[38;5;241m=\u001b[39m rf_model_11th\u001b[38;5;241m.\u001b[39mpredict(X_test_11th)\n\u001b[0;32m     38\u001b[0m predicted_close_11th \u001b[38;5;241m=\u001b[39m rf_model_11th\u001b[38;5;241m.\u001b[39mpredict(combined_states_11th_2d)\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:327\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 327\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:981\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    964\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    965\u001b[0m     X,\n\u001b[0;32m    966\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    977\u001b[0m )\n\u001b[0;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[1;32m--> 981\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2837, 45392]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Generate Date Embedding for 11th August:\n",
    "target_date_11th = \"11th August\"\n",
    "day_index_11th = int(target_date_11th.split(\" \")[0].replace(\"st\", \"\").replace(\"nd\", \"\").replace(\"rd\", \"\").replace(\"th\", \"\")) - 1\n",
    "month_index_11th = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "day_encoding_11th = torch.zeros(max_day)\n",
    "month_encoding_11th = torch.zeros(max_month)\n",
    "year_encoding_11th = torch.zeros(max_year + 1)\n",
    "day_encoding_11th[day_index_11th] = 1\n",
    "month_encoding_11th[month_index_11th] = 1\n",
    "year_encoding_11th[43] = 1\n",
    "\n",
    "date_embedding_11th = torch.cat((day_encoding_11th, month_encoding_11th, year_encoding_11th), dim=0)\n",
    "date_embedding_11th_broadcasted = date_embedding_11th.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "combined_states_11th = torch.cat((hidden_states, date_embedding_11th_broadcasted), dim=2)\n",
    "\n",
    "# Extract predicted 'close' values as a numpy array for 11th August\n",
    "close_values_11th = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape combined_states_11th to remove the extra dimension\n",
    "combined_states_11th_2d = combined_states_11th.reshape(-1, combined_states_11th.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets for 11th August\n",
    "X_train_11th, X_test_11th, y_train_11th, y_test_11th = train_test_split(combined_states_11th_2d, close_values_11th, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor for 11th August\n",
    "rf_model_11th = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_11th.fit(X_train_11th, y_train_11th.ravel())\n",
    "y_pred_11th = rf_model_11th.predict(X_test_11th)\n",
    "predicted_close_11th = rf_model_11th.predict(combined_states_11th_2d)\n",
    "\n",
    "# Calculate evaluation metrics for 11th August\n",
    "mae_11th = mean_absolute_error(y_test_11th, y_pred_11th)\n",
    "mse_11th = mean_squared_error(y_test_11th, y_pred_11th)\n",
    "rmse_11th = np.sqrt(mse_11th)\n",
    "r2_11th = r2_score(y_test_11th, y_pred_11th)\n",
    "\n",
    "print(\"Metrics for 11th August:\")\n",
    "print(f'Mean Absolute Error (MAE): {mae_11th}')\n",
    "print(f'Mean Squared Error (MSE): {mse_11th}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_11th}')\n",
    "print(f'R-squared (R^2): {r2_11th}')\n",
    "\n",
    "# Print the predicted \"close\" value for 11th August\n",
    "print(\"Predicted Close Value for 11th August:\", predicted_close_11th)\n",
    "\n",
    "\n",
    "# Generate Date Embedding for 31st August:\n",
    "target_date_31st = \"31st August\"\n",
    "# Extract Day Index for 31st August\n",
    "day_index_31st = int(target_date_31st.split(\" \")[0][:-2]) - 1\n",
    "month_index_31st = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "day_encoding_31st = torch.zeros(max_day)\n",
    "month_encoding_31st = torch.zeros(max_month)\n",
    "year_encoding_31st = torch.zeros(max_year + 1)\n",
    "day_encoding_31st[day_index_31st] = 1\n",
    "month_encoding_31st[month_index_31st] = 1\n",
    "year_encoding_31st[43] = 1\n",
    "\n",
    "date_embedding_31st = torch.cat((day_encoding_31st, month_encoding_31st, year_encoding_31st), dim=0)\n",
    "date_embedding_31st_broadcasted = date_embedding_31st.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "combined_states_31st = torch.cat((hidden_states, date_embedding_31st_broadcasted), dim=2)\n",
    "\n",
    "# Extract predicted 'close' values as a numpy array for 31st August\n",
    "close_values_31st = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape combined_states_31st to remove the extra dimension\n",
    "combined_states_31st_2d = combined_states_31st.reshape(-1, combined_states_31st.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets for 31st August\n",
    "X_train_31st, X_test_31st, y_train_31st, y_test_31st = train_test_split(combined_states_31st_2d, close_values_31st, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor for 31st August\n",
    "rf_model_31st = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_31st.fit(X_train_31st, y_train_31st.ravel())\n",
    "y_pred_31st = rf_model_31st.predict(X_test_31st)\n",
    "predicted_close_31st = rf_model_31st.predict(combined_states_31st_2d)\n",
    "\n",
    "# Calculate evaluation metrics for 31st August\n",
    "mae_31st = mean_absolute_error(y_test_31st, y_pred_31st)\n",
    "mse_31st = mean_squared_error(y_test_31st, y_pred_31st)\n",
    "rmse_31st = np.sqrt(mse_31st)\n",
    "r2_31st = r2_score(y_test_31st, y_pred_31st)\n",
    "\n",
    "print(\"\\nMetrics for 31st August:\")\n",
    "print(f'Mean Absolute Error (MAE): {mae_31st}')\n",
    "print(f'Mean Squared Error (MSE): {mse_31st}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_31st}')\n",
    "print(f'R-squared (R^2): {r2_31st}')\n",
    "\n",
    "# Print the predicted \"close\" value for 31st August\n",
    "print(\"Predicted Close Value for 31st August:\", predicted_close_31st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for 11th August:\n",
      "Mean Absolute Error (MAE): 0.1839889506219137\n",
      "Mean Squared Error (MSE): 0.20189083672486538\n",
      "Root Mean Squared Error (RMSE): 0.4493226421235251\n",
      "R-squared (R^2): 0.9999893254992325\n",
      "Predicted Close Value for 11th August: [ 41.63775166  43.09688847  28.30034241 ... 112.93842918  28.06641165\n",
      " 174.56737762]\n",
      "\n",
      "Metrics for 31st August:\n",
      "Mean Absolute Error (MAE): 0.1839889506219137\n",
      "Mean Squared Error (MSE): 0.20189083672486538\n",
      "Root Mean Squared Error (RMSE): 0.4493226421235251\n",
      "R-squared (R^2): 0.9999893254992325\n",
      "Predicted Close Value for 31st August: [ 41.63775166  43.09688847  28.30034241 ... 112.93842918  28.06641165\n",
      " 174.56737762]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Generate Date Embedding for 11th August:\n",
    "target_date_11th = \"11th August\"\n",
    "day_index_11th = int(target_date_11th.split(\" \")[0].replace(\"st\", \"\").replace(\"nd\", \"\").replace(\"rd\", \"\").replace(\"th\", \"\")) - 1\n",
    "month_index_11th = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "day_encoding_11th = torch.zeros(max_day)\n",
    "month_encoding_11th = torch.zeros(max_month)\n",
    "year_encoding_11th = torch.zeros(max_year + 1)\n",
    "day_encoding_11th[day_index_11th] = 1\n",
    "month_encoding_11th[month_index_11th] = 1\n",
    "year_encoding_11th[43] = 1\n",
    "\n",
    "date_embedding_11th = torch.cat((day_encoding_11th, month_encoding_11th, year_encoding_11th), dim=0)\n",
    "date_embedding_11th_broadcasted = date_embedding_11th.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "combined_states_11th = torch.cat((hidden_states, date_embedding_11th_broadcasted), dim=2)\n",
    "\n",
    "# Extract predicted 'close' values as a numpy array for 11th August\n",
    "close_values_11th = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape combined_states_11th to remove the extra dimension\n",
    "combined_states_11th_2d = combined_states_11th.reshape(-1, combined_states_11th.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets for 11th August\n",
    "X_train_11th, X_test_11th, y_train_11th, y_test_11th = train_test_split(combined_states_11th_2d, close_values_11th, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor for 11th August\n",
    "rf_model_11th = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_11th.fit(X_train_11th, y_train_11th.ravel())\n",
    "y_pred_11th = rf_model_11th.predict(X_test_11th)\n",
    "predicted_close_11th = rf_model_11th.predict(combined_states_11th_2d)\n",
    "\n",
    "# Calculate evaluation metrics for 11th August\n",
    "mae_11th = mean_absolute_error(y_test_11th, y_pred_11th)\n",
    "mse_11th = mean_squared_error(y_test_11th, y_pred_11th)\n",
    "rmse_11th = np.sqrt(mse_11th)\n",
    "r2_11th = r2_score(y_test_11th, y_pred_11th)\n",
    "\n",
    "print(\"Metrics for 11th August:\")\n",
    "print(f'Mean Absolute Error (MAE): {mae_11th}')\n",
    "print(f'Mean Squared Error (MSE): {mse_11th}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_11th}')\n",
    "print(f'R-squared (R^2): {r2_11th}')\n",
    "\n",
    "# Print the predicted \"close\" value for 11th August\n",
    "print(\"Predicted Close Value for 11th August:\", predicted_close_11th)\n",
    "\n",
    "\n",
    "# Generate Date Embedding for 31st August:\n",
    "target_date_31st = \"31st August\"\n",
    "# Extract Day Index for 31st August\n",
    "day_index_31st = int(target_date_31st.split(\" \")[0][:-2]) - 1\n",
    "month_index_31st = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "day_encoding_31st = torch.zeros(max_day)\n",
    "month_encoding_31st = torch.zeros(max_month)\n",
    "year_encoding_31st = torch.zeros(max_year + 1)\n",
    "day_encoding_31st[day_index_31st] = 1\n",
    "month_encoding_31st[month_index_31st] = 1\n",
    "year_encoding_31st[43] = 1\n",
    "\n",
    "date_embedding_31st = torch.cat((day_encoding_31st, month_encoding_31st, year_encoding_31st), dim=0)\n",
    "date_embedding_31st_broadcasted = date_embedding_31st.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "combined_states_31st = torch.cat((hidden_states, date_embedding_31st_broadcasted), dim=2)\n",
    "\n",
    "# Extract predicted 'close' values as a numpy array for 31st August\n",
    "close_values_31st = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape combined_states_31st to remove the extra dimension\n",
    "combined_states_31st_2d = combined_states_31st.reshape(-1, combined_states_31st.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets for 31st August\n",
    "X_train_31st, X_test_31st, y_train_31st, y_test_31st = train_test_split(combined_states_31st_2d, close_values_31st, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor for 31st August\n",
    "rf_model_31st = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_31st.fit(X_train_31st, y_train_31st.ravel())\n",
    "y_pred_31st = rf_model_31st.predict(X_test_31st)\n",
    "predicted_close_31st = rf_model_31st.predict(combined_states_31st_2d)\n",
    "\n",
    "# Calculate evaluation metrics for 31st August\n",
    "mae_31st = mean_absolute_error(y_test_31st, y_pred_31st)\n",
    "mse_31st = mean_squared_error(y_test_31st, y_pred_31st)\n",
    "rmse_31st = np.sqrt(mse_31st)\n",
    "r2_31st = r2_score(y_test_31st, y_pred_31st)\n",
    "\n",
    "print(\"\\nMetrics for 31st August:\")\n",
    "print(f'Mean Absolute Error (MAE): {mae_31st}')\n",
    "print(f'Mean Squared Error (MSE): {mse_31st}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_31st}')\n",
    "print(f'R-squared (R^2): {r2_31st}')\n",
    "\n",
    "# Print the predicted \"close\" value for 31st August\n",
    "print(\"Predicted Close Value for 31st August:\", predicted_close_31st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for 11th August:\n",
      "Mean Absolute Error (MAE): 0.1839889506219137\n",
      "Mean Squared Error (MSE): 0.20189083672486538\n",
      "Root Mean Squared Error (RMSE): 0.4493226421235251\n",
      "R-squared (R^2): 0.9999893254992325\n",
      "Predicted Close Value for 11th August: [ 41.63775166  43.09688847  28.30034241 ... 112.93842918  28.06641165\n",
      " 174.56737762]\n",
      "\n",
      "Metrics for 31st August:\n",
      "Mean Absolute Error (MAE): 0.1839889506219137\n",
      "Mean Squared Error (MSE): 0.20189083672486538\n",
      "Root Mean Squared Error (RMSE): 0.4493226421235251\n",
      "R-squared (R^2): 0.9999893254992325\n",
      "Predicted Close Value for 31st August: [ 41.63775166  43.09688847  28.30034241 ... 112.93842918  28.06641165\n",
      " 174.56737762]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Define the target dates\n",
    "target_date_11th = \"11th August\"\n",
    "target_date_31st = \"31st August\"\n",
    "\n",
    "# Function to generate date embedding for a given target date\n",
    "def generate_date_embedding(target_date):\n",
    "    # Extract day and month from the target date\n",
    "    day_index = int(target_date.split(\" \")[0][:-2]) - 1\n",
    "    month_index = 7  # August is the 8th month (0-based index)\n",
    "\n",
    "    # Create one-hot encodings for day, month, and year\n",
    "    day_encoding = torch.zeros(max_day)\n",
    "    month_encoding = torch.zeros(max_month)\n",
    "    year_encoding = torch.zeros(max_year + 1)\n",
    "    day_encoding[day_index] = 1\n",
    "    month_encoding[month_index] = 1\n",
    "    year_encoding[43] = 1  # 43 corresponds to the year 2023\n",
    "\n",
    "    # Concatenate the day, month, and year encodings to get the date embedding\n",
    "    date_embedding = torch.cat((day_encoding, month_encoding, year_encoding), dim=0)\n",
    "\n",
    "    return date_embedding\n",
    "\n",
    "# Generate Date Embedding for 11th August\n",
    "date_embedding_11th = generate_date_embedding(target_date_11th)\n",
    "\n",
    "# Broadcast date_embedding_11th to match the shape of hidden_states\n",
    "date_embedding_11th_broadcasted = date_embedding_11th.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "\n",
    "# Combine hidden_states and date_embedding_11th_broadcasted\n",
    "combined_states_11th = torch.cat((hidden_states, date_embedding_11th_broadcasted), dim=2)\n",
    "\n",
    "# Extract predicted 'close' values as a numpy array for 11th August\n",
    "close_values_11th = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape combined_states_11th to remove the extra dimension\n",
    "combined_states_11th_2d = combined_states_11th.reshape(-1, combined_states_11th.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets for 11th August\n",
    "X_train_11th, X_test_11th, y_train_11th, y_test_11th = train_test_split(combined_states_11th_2d, close_values_11th, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor for 11th August\n",
    "rf_model_11th = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_11th.fit(X_train_11th, y_train_11th.ravel())\n",
    "y_pred_11th = rf_model_11th.predict(X_test_11th)\n",
    "predicted_close_11th = rf_model_11th.predict(combined_states_11th_2d)\n",
    "\n",
    "# Calculate evaluation metrics for 11th August\n",
    "mae_11th = mean_absolute_error(y_test_11th, y_pred_11th)\n",
    "mse_11th = mean_squared_error(y_test_11th, y_pred_11th)\n",
    "rmse_11th = np.sqrt(mse_11th)\n",
    "r2_11th = r2_score(y_test_11th, y_pred_11th)\n",
    "\n",
    "print(\"Metrics for 11th August:\")\n",
    "print(f'Mean Absolute Error (MAE): {mae_11th}')\n",
    "print(f'Mean Squared Error (MSE): {mse_11th}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_11th}')\n",
    "print(f'R-squared (R^2): {r2_11th}')\n",
    "\n",
    "# Print the predicted \"close\" value for 11th August\n",
    "print(\"Predicted Close Value for 11th August:\", predicted_close_11th)\n",
    "\n",
    "\n",
    "# Generate Date Embedding for 31st August\n",
    "date_embedding_31st = generate_date_embedding(target_date_31st)\n",
    "\n",
    "# Broadcast date_embedding_31st to match the shape of hidden_states\n",
    "date_embedding_31st_broadcasted = date_embedding_31st.reshape(1, 1, -1).expand(hidden_states.shape[0], 1, -1)\n",
    "\n",
    "# Combine hidden_states and date_embedding_31st_broadcasted\n",
    "combined_states_31st = torch.cat((hidden_states, date_embedding_31st_broadcasted), dim=2)\n",
    "\n",
    "# Extract predicted 'close' values as a numpy array for 31st August\n",
    "close_values_31st = y_test_predictions.numpy()\n",
    "\n",
    "# Reshape combined_states_31st to remove the extra dimension\n",
    "combined_states_31st_2d = combined_states_31st.reshape(-1, combined_states_31st.shape[-1])\n",
    "\n",
    "# Split the data into training and testing sets for 31st August\n",
    "X_train_31st, X_test_31st, y_train_31st, y_test_31st = train_test_split(combined_states_31st_2d, close_values_31st, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor for 31st August\n",
    "rf_model_31st = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_31st.fit(X_train_31st, y_train_31st.ravel())\n",
    "y_pred_31st = rf_model_31st.predict(X_test_31st)\n",
    "predicted_close_31st = rf_model_31st.predict(combined_states_31st_2d)\n",
    "\n",
    "# Calculate evaluation metrics for 31st August\n",
    "mae_31st = mean_absolute_error(y_test_31st, y_pred_31st)\n",
    "mse_31st = mean_squared_error(y_test_31st, y_pred_31st)\n",
    "rmse_31st = np.sqrt(mse_31st)\n",
    "r2_31st = r2_score(y_test_31st, y_pred_31st)\n",
    "\n",
    "print(\"\\nMetrics for 31st August:\")\n",
    "print(f'Mean Absolute Error (MAE): {mae_31st}')\n",
    "print(f'Mean Squared Error (MSE): {mse_31st}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_31st}')\n",
    "print(f'R-squared (R^2): {r2_31st}')\n",
    "\n",
    "# Print the predicted \"close\" value for 31st August\n",
    "print(\"Predicted Close Value for 31st August:\", predicted_close_31st)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
