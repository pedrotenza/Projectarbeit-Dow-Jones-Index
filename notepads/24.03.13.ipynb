{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the window size for training\n",
    "train_window_size = 20\n",
    "\n",
    "# Initialize lists to store training and temporary sets\n",
    "x_train_list, y_train_list, x_temp_list, y_temp_list = [], [], [], []\n",
    "\n",
    "# Iterate through the data with the specified window size\n",
    "for i in range(0, len(x_data) - train_window_size, train_window_size + 1):\n",
    "    x_train_temp = x_data.iloc[i:i+train_window_size+1]\n",
    "    y_train_temp = y_data.iloc[i:i+train_window_size+1]\n",
    "\n",
    "    # Separate the last row for the temporary set\n",
    "    x_train = x_train_temp.iloc[:-1]\n",
    "    y_train = y_train_temp.iloc[:-1]\n",
    "\n",
    "    x_temp = x_train_temp.iloc[-1:]\n",
    "    y_temp = y_train_temp.iloc[-1:]\n",
    "\n",
    "    x_train_list.append(x_train)\n",
    "    y_train_list.append(y_train)\n",
    "    x_temp_list.append(x_temp)\n",
    "    y_temp_list.append(y_temp)\n",
    "\n",
    "# Concatenate the lists into pandas DataFrames\n",
    "x_train = pd.concat(x_train_list)\n",
    "y_train = pd.concat(y_train_list)\n",
    "x_temp = pd.concat(x_temp_list)\n",
    "y_temp = pd.concat(y_temp_list)\n",
    "\n",
    "# print(y_train.head(50))\n",
    "x_temp_train, x_temp_val, y_temp_train, y_temp_val = train_test_split(x_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Split x_temp and y_temp into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 4\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 8, 'batch_evaluation_frequency': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3636\\2606247059.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_window = torch.tensor(x_train_tensor[start_idx:end_idx], dtype=torch.float32)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3636\\2606247059.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_window = torch.tensor(y_train_tensor[start_idx:end_idx], dtype=torch.float32)\n",
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3636\\2606247059.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_window = torch.tensor(x_val_tensor[:params['window_size']], dtype=torch.float32)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3636\\2606247059.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_window = torch.tensor(y_val_tensor[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000, Train Loss: 332.5583801269531, Validation Loss: 9417.529296875\n",
      "Iteration 2000, Train Loss: 427.4424743652344, Validation Loss: 8965.142578125\n",
      "Iteration 3000, Train Loss: 1652.799072265625, Validation Loss: 8539.595703125\n",
      "Iteration 4000, Train Loss: 2.128542423248291, Validation Loss: 8366.58203125\n",
      "Iteration 5000, Train Loss: 19.744075775146484, Validation Loss: 8042.1376953125\n",
      "Iteration 6000, Train Loss: 1954.186279296875, Validation Loss: 7925.7958984375\n",
      "Iteration 7000, Train Loss: 31323.359375, Validation Loss: 7336.06640625\n",
      "Iteration 8000, Train Loss: 257966.65625, Validation Loss: 6757.73046875\n",
      "Iteration 9000, Train Loss: 24523.64453125, Validation Loss: 6639.24755859375\n",
      "Iteration 10000, Train Loss: 16394.1015625, Validation Loss: 6427.556640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3636\\2606247059.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_window = torch.tensor(x_test_tensor[:params['window_size']], dtype=torch.float32)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_3636\\2606247059.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_window = torch.tensor(y_test_tensor[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for parameters {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 8, 'batch_evaluation_frequency': 5}: 1881.4996337890625\n",
      "Hyperparameters: {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 8, 'batch_evaluation_frequency': 10}\n",
      "Iteration 1000, Train Loss: 313.94427490234375, Validation Loss: 9337.021484375\n",
      "Iteration 2000, Train Loss: 406.4568786621094, Validation Loss: 8888.2822265625\n",
      "Iteration 3000, Train Loss: 1611.148193359375, Validation Loss: 8465.4970703125\n",
      "Iteration 4000, Train Loss: 3.6939234733581543, Validation Loss: 8297.640625\n",
      "Iteration 5000, Train Loss: 24.149866104125977, Validation Loss: 7977.3095703125\n",
      "Iteration 6000, Train Loss: 1915.3638916015625, Validation Loss: 7866.20068359375\n",
      "Iteration 7000, Train Loss: 31164.994140625, Validation Loss: 7279.61474609375\n",
      "Iteration 8000, Train Loss: 257508.375, Validation Loss: 6705.1650390625\n",
      "Iteration 9000, Train Loss: 24383.345703125, Validation Loss: 6587.91015625\n",
      "Iteration 10000, Train Loss: 16279.9921875, Validation Loss: 6378.134765625\n",
      "Test Loss for parameters {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 8, 'batch_evaluation_frequency': 10}: 1862.048583984375\n",
      "Hyperparameters: {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 8, 'batch_evaluation_frequency': 15}\n",
      "Iteration 3000, Train Loss: 1640.2974853515625, Validation Loss: 8517.4013671875\n",
      "Iteration 6000, Train Loss: 1942.5806884765625, Validation Loss: 7908.00146484375\n",
      "Iteration 9000, Train Loss: 24481.84765625, Validation Loss: 6623.9248046875\n",
      "Test Loss for parameters {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 8, 'batch_evaluation_frequency': 15}: 1875.665771484375\n",
      "Hyperparameters: {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 16, 'batch_evaluation_frequency': 5}\n",
      "Iteration 1000, Train Loss: 332.3416442871094, Validation Loss: 9416.771484375\n",
      "Iteration 2000, Train Loss: 427.11077880859375, Validation Loss: 8964.001953125\n",
      "Iteration 3000, Train Loss: 1652.362548828125, Validation Loss: 8538.85546875\n",
      "Iteration 4000, Train Loss: 2.129943370819092, Validation Loss: 8366.560546875\n",
      "Iteration 5000, Train Loss: 19.631053924560547, Validation Loss: 8043.97119140625\n",
      "Iteration 6000, Train Loss: 1955.6654052734375, Validation Loss: 7928.0654296875\n",
      "Iteration 7000, Train Loss: 31335.55078125, Validation Loss: 7340.4208984375\n",
      "Iteration 8000, Train Loss: 258020.40625, Validation Loss: 6763.92822265625\n",
      "Iteration 9000, Train Loss: 24541.37890625, Validation Loss: 6645.75390625\n",
      "Iteration 10000, Train Loss: 16410.38671875, Validation Loss: 6434.62744140625\n",
      "Test Loss for parameters {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 16, 'batch_evaluation_frequency': 5}: 1884.3619384765625\n",
      "Hyperparameters: {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 16, 'batch_evaluation_frequency': 10}\n",
      "Iteration 1000, Train Loss: 335.888427734375, Validation Loss: 9431.9130859375\n",
      "Iteration 2000, Train Loss: 428.6988220214844, Validation Loss: 8969.7587890625\n",
      "Iteration 3000, Train Loss: 1654.7056884765625, Validation Loss: 8543.009765625\n",
      "Iteration 4000, Train Loss: 2.0766348838806152, Validation Loss: 8369.357421875\n",
      "Iteration 5000, Train Loss: 19.605379104614258, Validation Loss: 8044.3701171875\n",
      "Iteration 6000, Train Loss: 1955.343505859375, Validation Loss: 7927.5693359375\n",
      "Iteration 7000, Train Loss: 31322.037109375, Validation Loss: 7335.5869140625\n",
      "Iteration 8000, Train Loss: 257943.875, Validation Loss: 6755.1123046875\n",
      "Iteration 9000, Train Loss: 24515.43359375, Validation Loss: 6636.2353515625\n",
      "Iteration 10000, Train Loss: 16385.55859375, Validation Loss: 6423.849609375\n",
      "Test Loss for parameters {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 16, 'batch_evaluation_frequency': 10}: 1879.9716796875\n",
      "Hyperparameters: {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 16, 'batch_evaluation_frequency': 15}\n",
      "Iteration 3000, Train Loss: 1660.3265380859375, Validation Loss: 8552.982421875\n",
      "Iteration 6000, Train Loss: 1965.9058837890625, Validation Loss: 7943.7529296875\n",
      "Iteration 9000, Train Loss: 24584.88671875, Validation Loss: 6661.73681640625\n",
      "Test Loss for parameters {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 256, 'n_layers': 16, 'batch_evaluation_frequency': 15}: 1890.8548583984375\n",
      "Hyperparameters: {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 512, 'n_layers': 8, 'batch_evaluation_frequency': 5}\n",
      "Iteration 1000, Train Loss: 149.22140502929688, Validation Loss: 8514.291015625\n",
      "Iteration 2000, Train Loss: 155.4208984375, Validation Loss: 7800.42724609375\n",
      "Iteration 3000, Train Loss: 898.2604370117188, Validation Loss: 7112.2490234375\n",
      "Iteration 4000, Train Loss: 130.5247344970703, Validation Loss: 7042.61083984375\n",
      "Iteration 5000, Train Loss: 242.46347045898438, Validation Loss: 6637.08349609375\n",
      "Iteration 6000, Train Loss: 1160.115966796875, Validation Loss: 6653.78759765625\n",
      "Iteration 7000, Train Loss: 26355.431640625, Validation Loss: 5698.47705078125\n",
      "Iteration 8000, Train Loss: 238521.296875, Validation Loss: 4856.2001953125\n",
      "Iteration 9000, Train Loss: 18586.01171875, Validation Loss: 4721.28857421875\n",
      "Iteration 10000, Train Loss: 11270.9375, Validation Loss: 4474.41552734375\n",
      "Test Loss for parameters {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 512, 'n_layers': 8, 'batch_evaluation_frequency': 5}: 1395.911865234375\n",
      "Hyperparameters: {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 512, 'n_layers': 8, 'batch_evaluation_frequency': 10}\n",
      "Iteration 1000, Train Loss: 149.59835815429688, Validation Loss: 8516.537109375\n",
      "Iteration 2000, Train Loss: 156.48721313476562, Validation Loss: 7806.140625\n",
      "Iteration 3000, Train Loss: 902.1500854492188, Validation Loss: 7120.22509765625\n",
      "Iteration 4000, Train Loss: 129.31570434570312, Validation Loss: 7049.07177734375\n",
      "Iteration 5000, Train Loss: 240.4909210205078, Validation Loss: 6644.3837890625\n",
      "Iteration 6000, Train Loss: 1163.638916015625, Validation Loss: 6659.7451171875\n",
      "Iteration 7000, Train Loss: 26383.90625, Validation Loss: 5707.0068359375\n",
      "Iteration 8000, Train Loss: 238635.28125, Validation Loss: 4865.2978515625\n",
      "Iteration 9000, Train Loss: 18617.05078125, Validation Loss: 4729.76171875\n",
      "Iteration 10000, Train Loss: 11295.4423828125, Validation Loss: 4482.20166015625\n",
      "Test Loss for parameters {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 512, 'n_layers': 8, 'batch_evaluation_frequency': 10}: 1395.897216796875\n",
      "Hyperparameters: {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 512, 'n_layers': 8, 'batch_evaluation_frequency': 15}\n",
      "Iteration 3000, Train Loss: 919.8429565429688, Validation Loss: 7156.359375\n",
      "Iteration 6000, Train Loss: 1174.313720703125, Validation Loss: 6677.7646484375\n",
      "Iteration 9000, Train Loss: 18664.169921875, Validation Loss: 4742.65966796875\n",
      "Test Loss for parameters {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 512, 'n_layers': 8, 'batch_evaluation_frequency': 15}: 1395.928955078125\n",
      "Hyperparameters: {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 512, 'n_layers': 16, 'batch_evaluation_frequency': 5}\n",
      "Iteration 1000, Train Loss: 149.1954345703125, Validation Loss: 8514.3359375\n",
      "Iteration 2000, Train Loss: 155.77293395996094, Validation Loss: 7802.3798828125\n",
      "Iteration 3000, Train Loss: 898.4879760742188, Validation Loss: 7112.77587890625\n",
      "Iteration 4000, Train Loss: 130.4655303955078, Validation Loss: 7043.01708984375\n",
      "Iteration 5000, Train Loss: 242.3079833984375, Validation Loss: 6637.73046875\n",
      "Iteration 6000, Train Loss: 1160.455810546875, Validation Loss: 6654.36572265625\n",
      "Iteration 7000, Train Loss: 26351.66796875, Validation Loss: 5697.345703125\n",
      "Iteration 8000, Train Loss: 238491.5, Validation Loss: 4853.8232421875\n",
      "Iteration 9000, Train Loss: 18576.87109375, Validation Loss: 4718.796875\n",
      "Iteration 10000, Train Loss: 11262.4736328125, Validation Loss: 4471.7294921875\n",
      "Test Loss for parameters {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 512, 'n_layers': 16, 'batch_evaluation_frequency': 5}: 1395.9232177734375\n",
      "Hyperparameters: {'learning_rate': 1e-05, 'window_size': 5, 'hidden_dim': 512, 'n_layers': 16, 'batch_evaluation_frequency': 10}\n",
      "Iteration 1000, Train Loss: 151.25003051757812, Validation Loss: 8526.416015625\n",
      "Iteration 2000, Train Loss: 158.9231414794922, Validation Loss: 7819.19287109375\n",
      "Iteration 3000, Train Loss: 907.7979736328125, Validation Loss: 7131.8408203125\n",
      "Iteration 4000, Train Loss: 127.62632751464844, Validation Loss: 7058.2490234375\n",
      "Iteration 5000, Train Loss: 238.0286102294922, Validation Loss: 6653.625\n",
      "Iteration 6000, Train Loss: 1168.4844970703125, Validation Loss: 6667.91943359375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from itertools import product\n",
    "\n",
    "# Assuming x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled, x_test_scaled, y_test_scaled,\n",
    "# input_size, output_size, test_window_size, scaler are available\n",
    "\n",
    "\n",
    "    # Print hyperparameters only once\n",
    "    print(f\"Hyperparameters: {params}\")\n",
    "\n",
    "    input_size = 7\n",
    "    output_size = 1\n",
    "\n",
    "    model = LSTMModel(input_size, params['hidden_dim'], params['n_layers'], output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Training using Walk-Forward Validation\n",
    "    for i in range(params['window_size'], len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_idx = i - params['window_size']\n",
    "        end_idx = i\n",
    "        x_window = torch.tensor(x_train_tensor[start_idx:end_idx], dtype=torch.float32)\n",
    "        y_window = torch.tensor(y_train_tensor[start_idx:end_idx], dtype=torch.float32)\n",
    "        x_window = x_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        outputs, hidden = model(x_window, hidden)\n",
    "        loss = criterion(outputs, y_window)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % params['batch_evaluation_frequency'] == 0:\n",
    "            with torch.no_grad():\n",
    "                x_val_window = torch.tensor(x_val_tensor[:params['window_size']], dtype=torch.float32)\n",
    "                y_val_window = torch.tensor(y_val_tensor[:params['window_size']], dtype=torch.float32)\n",
    "                x_val_window = x_val_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "                hidden = model.init_hidden(1)\n",
    "                val_outputs, _ = model(x_val_window, hidden)\n",
    "                val_loss = criterion(val_outputs, y_val_window)\n",
    "\n",
    "                if i % 1000 == 0:  # Print every 1000 iterations\n",
    "                    print(f\"Iteration {i}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # Test set evaluation\n",
    "    with torch.no_grad():\n",
    "        x_test_window = torch.tensor(x_test_tensor[:params['window_size']], dtype=torch.float32)\n",
    "        y_test_window = torch.tensor(y_test_tensor[:params['window_size']], dtype=torch.float32)\n",
    "        x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        test_outputs, _ = model(x_test_window, hidden)\n",
    "        test_loss = criterion(test_outputs, y_test_window)\n",
    "\n",
    "    print(f\"Test Loss for parameters {params}: {test_loss.item()}\")\n",
    "\n",
    "    # Update the best_params if the current model performs better on the test set\n",
    "    if best_params is None:\n",
    "        best_params = params.copy()\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "    elif test_loss < best_params['test_loss']:\n",
    "        best_params = params.copy()\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
