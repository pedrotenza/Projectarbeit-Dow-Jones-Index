{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "#print(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "x_data = data[[ 'open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "y_data = data[\"close\"].values.reshape(-1, 1)  # Reshape y_data if it's 1D\n",
    "\n",
    "\n",
    "\n",
    "# Convert data to Tensors\n",
    "x_feature_tensors = torch.tensor(x_data, dtype=torch.float32)\n",
    "y_feature_tensors = torch.tensor(y_data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(x_feature_tensors) * 0.67)\n",
    "x_train, x_test = x_feature_tensors[:train_size], x_feature_tensors[train_size:]\n",
    "y_train, y_test = y_feature_tensors[:train_size], y_feature_tensors[train_size:]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fitting the MinMaxScaler to training data using fit_transform and then\n",
    "#  test data only transform\n",
    "x_train_scaled = scaler.fit_transform(x_train.detach().numpy())\n",
    "x_test_scaled = scaler.transform(x_test.detach().numpy())  # Use transform, not fit_transform\n",
    "\n",
    "y_train_scaled = scaler.fit_transform(y_train.detach().numpy())\n",
    "y_test_scaled = scaler.transform(y_test.detach().numpy())  # Use transform, not fit_transform\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 4\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train_scaled) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train_scaled.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2082185232.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 24\u001b[1;36m\u001b[0m\n\u001b[1;33m    Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256,\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled, x_test_scaled, y_test_scaled,\n",
    "# input_size, output_size, test_window_size, scaler are available\n",
    "\n",
    "# Splitting the dataset into training, validation, and test sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_train_scaled, y_train_scaled, test_size=0.2, shuffle=False)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'learning_rate': [0.00001, 0.00005, 0.0001],\n",
    "    'window_size': [3, 5, 6],\n",
    "    'hidden_dim': [ 128, 256, 512],\n",
    "    'n_layers': [10, 11, 12],\n",
    "    'batch_evaluation_frequency': [2,3, 4]\n",
    "}\n",
    "\n",
    "Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256,\n",
    "                  'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 6463.20556640625}\n",
    "\n",
    "\n",
    "# Number of random search iterations\n",
    "num_iterations = 10\n",
    "\n",
    "best_params = None\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    params = {\n",
    "        'learning_rate': random.choice(search_space['learning_rate']),\n",
    "        'window_size': random.choice(search_space['window_size']),\n",
    "        'hidden_dim': random.choice(search_space['hidden_dim']),\n",
    "        'n_layers': random.choice(search_space['n_layers']),\n",
    "        'batch_evaluation_frequency': random.choice(search_space['batch_evaluation_frequency'])\n",
    "    }\n",
    "\n",
    "    model = LSTMModel(input_size, params['hidden_dim'], params['n_layers'], output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Training using Walk-Forward Validation\n",
    "    for i in range(params['window_size'], len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_idx = i - params['window_size']\n",
    "        end_idx = i\n",
    "        x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        y_window = torch.tensor(y_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        x_window = x_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        outputs, hidden = model(x_window, hidden)\n",
    "        loss = criterion(outputs, y_window)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % params['batch_evaluation_frequency'] == 0:\n",
    "            with torch.no_grad():\n",
    "                x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n",
    "                y_val_window = torch.tensor(y_val[:params['window_size']], dtype=torch.float32)\n",
    "                x_val_window = x_val_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "                hidden = model.init_hidden(1)\n",
    "                val_outputs, _ = model(x_val_window, hidden)\n",
    "                val_loss = criterion(val_outputs, y_val_window)\n",
    "\n",
    "                if i % 1000 == 0:  # Print every 1000 iterations\n",
    "                    print(f\"Iteration {i}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # Test set evaluation\n",
    "    with torch.no_grad():\n",
    "        x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n",
    "        y_test_window = torch.tensor(y_test[:params['window_size']], dtype=torch.float32)\n",
    "        x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        test_outputs, _ = model(x_test_window, hidden)\n",
    "        test_loss = criterion(test_outputs, y_test_window)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "    # Update the best_params if the current model performs better on the test set\n",
    "    if best_params is None:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "    elif test_loss < best_params['test_loss']:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last entries of the last window in the test set:\n",
      "Input values:\n",
      "[[0.66118854 0.6558897  0.65246195 0.19039789 0.6685383  0.5804368\n",
      "  0.31263226]\n",
      " [0.6560159  0.65875053 0.66305935 0.1308405  0.67966837 0.62846404\n",
      "  0.30835122]\n",
      " [0.6577598  0.6728215  0.6649329  0.15884562 0.69799155 0.64044154\n",
      "  0.3110624 ]]\n",
      "Actual output values:\n",
      "[0.65143836 0.6631376  0.682414  ]\n",
      "Predicted output values:\n",
      "0.046670124\n"
     ]
    }
   ],
   "source": [
    "# ...\n",
    "\n",
    "# Test set evaluation\n",
    "with torch.no_grad():\n",
    "    x_test_window = torch.tensor(x_test[-params['window_size']:], dtype=torch.float32)\n",
    "    y_test_window = torch.tensor(y_test[-params['window_size']:], dtype=torch.float32)\n",
    "    x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "    hidden = model.init_hidden(1)\n",
    "    test_outputs, _ = model(x_test_window, hidden)\n",
    "    test_loss = criterion(test_outputs, y_test_window)\n",
    "\n",
    "# Print the last entries of the last window\n",
    "print(f\"Last entries of the last window in the test set:\")\n",
    "print(\"Input values:\")\n",
    "print(x_test_window.squeeze().numpy())\n",
    "print(\"Actual output values:\")\n",
    "print(y_test_window.squeeze().numpy())\n",
    "print(\"Predicted output values:\")\n",
    "print(test_outputs.squeeze().numpy())\n",
    "\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the scaler on the training data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m scaler\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Transform the training and test data using the fitted scaler\u001b[39;00m\n\u001b[0;32m      5\u001b[0m x_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(x_train\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "# Fit the scaler on the training data\n",
    "scaler.fit(x_train.detach().numpy())\n",
    "\n",
    "# Transform the training and test data using the fitted scaler\n",
    "x_train_scaled = scaler.transform(x_train.detach().numpy())\n",
    "x_test_scaled = scaler.transform(x_test.detach().numpy())\n",
    "\n",
    "y_train_scaled = scaler.transform(y_train.detach().numpy())\n",
    "y_test_scaled = scaler.transform(y_test.detach().numpy())\n",
    "\n",
    "# Inverse transform the scaled test data to get the original values\n",
    "x_test_original = scaler.inverse_transform(x_test_scaled)\n",
    "y_test_original = scaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# Extract the last entries of the last window\n",
    "last_window_size = params['window_size']\n",
    "x_test_last_window = x_test_original[-last_window_size:]\n",
    "y_test_last_window = y_test_original[-last_window_size:]\n",
    "\n",
    "# Print the last entries of the last window in the original scale\n",
    "print(\"Last entries of the last window in the test set (original scale):\")\n",
    "print(\"Input values:\")\n",
    "print(x_test_last_window)\n",
    "print(\"Actual output values:\")\n",
    "print(y_test_last_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ... (Your model training and testing code here)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Use the already fitted scaler to transform the test data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x_test_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Directly use NumPy array\u001b[39;00m\n\u001b[0;32m      5\u001b[0m y_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(y_test)  \u001b[38;5;66;03m# Directly use NumPy array\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Inverse transform the scaled test data to get the original values\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:494\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    482\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scale features of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03m        Transformed data.\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    497\u001b[0m         X,\n\u001b[0;32m    498\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    502\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1222\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1217\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1218\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1219\u001b[0m     ]\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[1;32m-> 1222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# ... (Your model training and testing code here)\n",
    "\n",
    "# Use the already fitted scaler to transform the test data\n",
    "x_test_scaled = scaler.transform(x_test)  # Directly use NumPy array\n",
    "y_test_scaled = scaler.transform(y_test)  # Directly use NumPy array\n",
    "\n",
    "# Inverse transform the scaled test data to get the original values\n",
    "x_test_original = scaler.inverse_transform(x_test_scaled)\n",
    "y_test_original = scaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# Extract the last entries of the last window\n",
    "last_window_size = params['window_size']\n",
    "x_test_last_window = x_test_original[-last_window_size:]\n",
    "y_test_last_window = y_test_original[-last_window_size:]\n",
    "\n",
    "# Print the last entries of the last window in the original scale\n",
    "print(\"Last entries of the last window in the test set (original scale):\")\n",
    "print(\"Input values:\")\n",
    "print(x_test_last_window)\n",
    "print(\"Actual output values:\")\n",
    "print(y_test_last_window)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
