{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# x_train_tensor inverse\\n\\nx_test_original = scaler.inverse_transform(x_train_tensor.numpy())\\nprint(\"\\nFirst row of x_test_original:\")\\nprint(x_test_original[0])\\n\\nprint(\"\\nFirst row of x_train:\")\\nprint(x_train.head(1))\\n\\n\\n\\nprint(\"\\nLast row of x_test_original:\")\\nprint(x_test_original[-1])\\n\\nprint(\"\\nLast row of x_train:\")\\nprint(x_train.tail(1))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the window size for training\n",
    "train_window_size = 20\n",
    "\n",
    "# Initialize lists to store training and temporary sets\n",
    "x_train_list, y_train_list, x_temp_list, y_temp_list = [], [], [], []\n",
    "\n",
    "# Iterate through the data with the specified window size\n",
    "for i in range(0, len(x_data) - train_window_size, train_window_size + 1):\n",
    "    x_train_temp = x_data.iloc[i:i+train_window_size+1]\n",
    "    y_train_temp = y_data.iloc[i:i+train_window_size+1]\n",
    "\n",
    "    # Separate the last row for the temporary set\n",
    "    x_train = x_train_temp.iloc[:-1]\n",
    "    y_train = y_train_temp.iloc[:-1]\n",
    "\n",
    "    x_temp = x_train_temp.iloc[-1:]\n",
    "    y_temp = y_train_temp.iloc[-1:]\n",
    "\n",
    "    x_train_list.append(x_train)\n",
    "    y_train_list.append(y_train)\n",
    "    x_temp_list.append(x_temp)\n",
    "    y_temp_list.append(y_temp)\n",
    "\n",
    "# Concatenate the lists into pandas DataFrames\n",
    "x_train = pd.concat(x_train_list)\n",
    "y_train = pd.concat(y_train_list)\n",
    "x_temp = pd.concat(x_temp_list)\n",
    "y_temp = pd.concat(y_temp_list)\n",
    "\n",
    "# print(y_train.head(50))\n",
    "x_temp_train, x_temp_val, y_temp_train, y_temp_val = train_test_split(x_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Split x_temp and y_temp into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 22351.461821838264, Validation Loss: 22762.787109375\n",
      "Epoch [2/50], Training Loss: 19530.393213252326, Validation Loss: 20687.96484375\n",
      "Epoch [3/50], Training Loss: 16522.507419584123, Validation Loss: 19053.494140625\n",
      "Epoch [4/50], Training Loss: 14229.592356875843, Validation Loss: 17442.82421875\n",
      "Epoch [5/50], Training Loss: 12442.620746553148, Validation Loss: 14169.869140625\n",
      "Epoch [6/50], Training Loss: 11001.347510759988, Validation Loss: 11881.30859375\n",
      "Epoch [7/50], Training Loss: 9835.767752316897, Validation Loss: 10697.4609375\n",
      "Epoch [8/50], Training Loss: 8900.346939763613, Validation Loss: 9903.060546875\n",
      "Epoch [9/50], Training Loss: 8018.447919958429, Validation Loss: 12919.5048828125\n",
      "Epoch [10/50], Training Loss: 7232.683605896568, Validation Loss: 11749.7705078125\n",
      "Epoch [11/50], Training Loss: 6530.987028128451, Validation Loss: 11055.7822265625\n",
      "Epoch [12/50], Training Loss: 5885.8008603381895, Validation Loss: 10155.583984375\n",
      "Epoch [13/50], Training Loss: 5292.018844600343, Validation Loss: 9040.8017578125\n",
      "Epoch [14/50], Training Loss: 4768.82865219979, Validation Loss: 7786.9765625\n",
      "Epoch [15/50], Training Loss: 4305.486858419076, Validation Loss: 6694.7099609375\n",
      "Epoch [16/50], Training Loss: 3877.153458359282, Validation Loss: 5799.884765625\n",
      "Epoch [17/50], Training Loss: 3480.498972800864, Validation Loss: 4935.26806640625\n",
      "Epoch [18/50], Training Loss: 3121.302270583679, Validation Loss: 4234.43408203125\n",
      "Epoch [19/50], Training Loss: 2804.496014729552, Validation Loss: 3716.86181640625\n",
      "Epoch [20/50], Training Loss: 2523.640838133025, Validation Loss: 3279.0107421875\n",
      "Epoch [21/50], Training Loss: 2284.3261241543782, Validation Loss: 2943.8857421875\n",
      "Epoch [22/50], Training Loss: 2076.6176860184864, Validation Loss: 2654.995361328125\n",
      "Epoch [23/50], Training Loss: 1888.6999813083028, Validation Loss: 2394.571533203125\n",
      "Epoch [24/50], Training Loss: 1717.4069171625017, Validation Loss: 2153.72802734375\n",
      "Epoch [25/50], Training Loss: 1564.913523069531, Validation Loss: 1941.173828125\n",
      "Epoch [26/50], Training Loss: 1433.051805072117, Validation Loss: 1770.6693115234375\n",
      "Epoch [27/50], Training Loss: 1317.7631363888524, Validation Loss: 1639.8095703125\n",
      "Epoch [28/50], Training Loss: 1212.0213808900023, Validation Loss: 1518.92578125\n",
      "Epoch [29/50], Training Loss: 1115.0924164544806, Validation Loss: 1404.7037353515625\n",
      "Epoch [30/50], Training Loss: 1026.2339644209446, Validation Loss: 1300.1632080078125\n",
      "Epoch [31/50], Training Loss: 944.3444260378516, Validation Loss: 1207.1385498046875\n",
      "Epoch [32/50], Training Loss: 869.2348033600283, Validation Loss: 1119.859619140625\n",
      "Epoch [33/50], Training Loss: 800.481424887133, Validation Loss: 1028.574462890625\n",
      "Epoch [34/50], Training Loss: 736.6551891883358, Validation Loss: 948.8116455078125\n",
      "Epoch [35/50], Training Loss: 677.9635513492783, Validation Loss: 870.254150390625\n",
      "Epoch [36/50], Training Loss: 623.5364120407872, Validation Loss: 795.5739135742188\n",
      "Epoch [37/50], Training Loss: 572.3371396488816, Validation Loss: 735.0459594726562\n",
      "Epoch [38/50], Training Loss: 524.9602169883169, Validation Loss: 694.6967163085938\n",
      "Epoch [39/50], Training Loss: 480.9106187445094, Validation Loss: 648.5195922851562\n",
      "Epoch [40/50], Training Loss: 440.2509115838328, Validation Loss: 598.38671875\n",
      "Epoch [41/50], Training Loss: 403.3057199827609, Validation Loss: 547.25390625\n",
      "Epoch [42/50], Training Loss: 369.1002050337658, Validation Loss: 501.8952941894531\n",
      "Epoch [43/50], Training Loss: 337.56894402351264, Validation Loss: 458.9345397949219\n",
      "Epoch [44/50], Training Loss: 308.0590996598455, Validation Loss: 429.0405578613281\n",
      "Epoch [45/50], Training Loss: 281.1540629707708, Validation Loss: 398.1173095703125\n",
      "Epoch [46/50], Training Loss: 256.19673952056485, Validation Loss: 367.9533996582031\n",
      "Epoch [47/50], Training Loss: 233.59810931425488, Validation Loss: 337.6575012207031\n",
      "Epoch [48/50], Training Loss: 213.35789470501396, Validation Loss: 309.4978332519531\n",
      "Epoch [49/50], Training Loss: 195.1311967381659, Validation Loss: 283.1171875\n",
      "Epoch [50/50], Training Loss: 178.60063451140562, Validation Loss: 256.2901916503906\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, learning_rate=0.001, window_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model with specified parameters\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "input_size = 7  # Number of features\n",
    "hidden_size = 64  # Number of LSTM units\n",
    "num_layers = 4\n",
    "\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "window_size = 1 # Window size\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 22319.8968181098, Validation Loss: 22740.08984375\n",
      "Epoch [2/50], Training Loss: 19575.82829728923, Validation Loss: 20715.751953125\n",
      "Epoch [3/50], Training Loss: 16560.932099565358, Validation Loss: 19093.005859375\n",
      "Epoch [4/50], Training Loss: 14259.677255592298, Validation Loss: 17747.6875\n",
      "Epoch [5/50], Training Loss: 12471.883559377153, Validation Loss: 14551.69140625\n",
      "Epoch [6/50], Training Loss: 11028.106706517407, Validation Loss: 12054.23046875\n",
      "Epoch [7/50], Training Loss: 9862.15668895215, Validation Loss: 10690.443359375\n",
      "Epoch [8/50], Training Loss: 8923.620887357007, Validation Loss: 9940.3759765625\n",
      "Epoch [9/50], Training Loss: 8032.467596803317, Validation Loss: 12909.0341796875\n",
      "Epoch [10/50], Training Loss: 7250.105078278011, Validation Loss: 11969.3408203125\n",
      "Epoch [11/50], Training Loss: 6550.6564129727285, Validation Loss: 11309.580078125\n",
      "Epoch [12/50], Training Loss: 5906.232188705084, Validation Loss: 10684.69140625\n",
      "Epoch [13/50], Training Loss: 5314.403153054144, Validation Loss: 9858.21875\n",
      "Epoch [14/50], Training Loss: 4791.314837535332, Validation Loss: 8936.0693359375\n",
      "Epoch [15/50], Training Loss: 4329.22696371876, Validation Loss: 7926.833984375\n",
      "Epoch [16/50], Training Loss: 3902.3090437628407, Validation Loss: 6942.05322265625\n",
      "Epoch [17/50], Training Loss: 3504.756304983436, Validation Loss: 5926.2314453125\n",
      "Epoch [18/50], Training Loss: 3140.7790346876013, Validation Loss: 4942.115234375\n",
      "Epoch [19/50], Training Loss: 2816.563014279342, Validation Loss: 4181.71142578125\n",
      "Epoch [20/50], Training Loss: 2531.6050636384643, Validation Loss: 3601.8369140625\n",
      "Epoch [21/50], Training Loss: 2289.0079706034307, Validation Loss: 3148.130859375\n",
      "Epoch [22/50], Training Loss: 2082.2005152208144, Validation Loss: 2819.571044921875\n",
      "Epoch [23/50], Training Loss: 1893.9366151649056, Validation Loss: 2524.60498046875\n",
      "Epoch [24/50], Training Loss: 1721.556428529828, Validation Loss: 2260.689453125\n",
      "Epoch [25/50], Training Loss: 1568.2597107649385, Validation Loss: 2030.8385009765625\n",
      "Epoch [26/50], Training Loss: 1435.60198945447, Validation Loss: 1843.7392578125\n",
      "Epoch [27/50], Training Loss: 1320.0627475553563, Validation Loss: 1697.8165283203125\n",
      "Epoch [28/50], Training Loss: 1214.9071695823193, Validation Loss: 1568.443359375\n",
      "Epoch [29/50], Training Loss: 1117.8096991330904, Validation Loss: 1446.2928466796875\n",
      "Epoch [30/50], Training Loss: 1028.4865407538623, Validation Loss: 1331.845947265625\n",
      "Epoch [31/50], Training Loss: 946.2662031915535, Validation Loss: 1226.5015869140625\n",
      "Epoch [32/50], Training Loss: 870.6870528009398, Validation Loss: 1128.088623046875\n",
      "Epoch [33/50], Training Loss: 801.1965092153583, Validation Loss: 1031.8935546875\n",
      "Epoch [34/50], Training Loss: 736.8971783119124, Validation Loss: 946.5982055664062\n",
      "Epoch [35/50], Training Loss: 677.5865964734239, Validation Loss: 868.3018188476562\n",
      "Epoch [36/50], Training Loss: 622.3384792600864, Validation Loss: 802.9839477539062\n",
      "Epoch [37/50], Training Loss: 571.4467866514119, Validation Loss: 753.9932861328125\n",
      "Epoch [38/50], Training Loss: 523.7078976795226, Validation Loss: 696.5630493164062\n",
      "Epoch [39/50], Training Loss: 478.9355653535995, Validation Loss: 633.7838134765625\n",
      "Epoch [40/50], Training Loss: 438.1229896330277, Validation Loss: 576.6395263671875\n",
      "Epoch [41/50], Training Loss: 400.53902727987855, Validation Loss: 533.9266967773438\n",
      "Epoch [42/50], Training Loss: 365.98318592172, Validation Loss: 508.0982360839844\n",
      "Epoch [43/50], Training Loss: 334.2753520664949, Validation Loss: 478.0914001464844\n",
      "Epoch [44/50], Training Loss: 304.5409156466077, Validation Loss: 434.9052429199219\n",
      "Epoch [45/50], Training Loss: 277.15825157391424, Validation Loss: 386.6178894042969\n",
      "Epoch [46/50], Training Loss: 252.20852852598298, Validation Loss: 350.4332580566406\n",
      "Epoch [47/50], Training Loss: 229.59544694570846, Validation Loss: 324.7398986816406\n",
      "Epoch [48/50], Training Loss: 209.22999724412745, Validation Loss: 304.0986633300781\n",
      "Epoch [49/50], Training Loss: 190.8917954856769, Validation Loss: 281.4367980957031\n",
      "Epoch [50/50], Training Loss: 174.36921353230483, Validation Loss: 255.6691131591797\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, learning_rate=0.001, window_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model with specified parameters\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "window_size = 4\n",
    "\n",
    "input_size = 7  # Number of features\n",
    "hidden_size = 64  # Number of LSTM units\n",
    "num_layers = 4\n",
    "\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "window_size = 1 # Window size\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 22368.88652702448, Validation Loss: 22775.37109375\n",
      "Epoch [2/50], Training Loss: 19616.243053061364, Validation Loss: 20743.466796875\n",
      "Epoch [3/50], Training Loss: 16566.98257264973, Validation Loss: 19091.939453125\n",
      "Epoch [4/50], Training Loss: 14270.991436727287, Validation Loss: 17720.076171875\n",
      "Epoch [5/50], Training Loss: 12479.091879509158, Validation Loss: 14486.3017578125\n",
      "Epoch [6/50], Training Loss: 11032.177518863627, Validation Loss: 12007.5517578125\n",
      "Epoch [7/50], Training Loss: 9863.860659070362, Validation Loss: 10707.0263671875\n",
      "Epoch [8/50], Training Loss: 8924.358268312853, Validation Loss: 9945.9072265625\n",
      "Epoch [9/50], Training Loss: 8042.379863454362, Validation Loss: 13071.2744140625\n",
      "Epoch [10/50], Training Loss: 7256.935003655125, Validation Loss: 12077.9697265625\n",
      "Epoch [11/50], Training Loss: 6556.560700647632, Validation Loss: 11557.6591796875\n",
      "Epoch [12/50], Training Loss: 5912.539038845866, Validation Loss: 10946.0390625\n",
      "Epoch [13/50], Training Loss: 5320.802138191754, Validation Loss: 10155.3994140625\n",
      "Epoch [14/50], Training Loss: 4796.30747982877, Validation Loss: 9238.626953125\n",
      "Epoch [15/50], Training Loss: 4332.782611311391, Validation Loss: 8328.9873046875\n",
      "Epoch [16/50], Training Loss: 3908.2428343531283, Validation Loss: 7305.30517578125\n",
      "Epoch [17/50], Training Loss: 3510.7223846776865, Validation Loss: 6382.57958984375\n",
      "Epoch [18/50], Training Loss: 3146.3990216778257, Validation Loss: 5382.220703125\n",
      "Epoch [19/50], Training Loss: 2820.3421854496846, Validation Loss: 4438.9677734375\n",
      "Epoch [20/50], Training Loss: 2533.730884748138, Validation Loss: 3807.66845703125\n",
      "Epoch [21/50], Training Loss: 2289.693346695276, Validation Loss: 3302.379150390625\n",
      "Epoch [22/50], Training Loss: 2083.2145080478135, Validation Loss: 2918.181396484375\n",
      "Epoch [23/50], Training Loss: 1893.6817438329185, Validation Loss: 2579.653564453125\n",
      "Epoch [24/50], Training Loss: 1720.4008919054259, Validation Loss: 2285.799072265625\n",
      "Epoch [25/50], Training Loss: 1566.53937362091, Validation Loss: 2039.92138671875\n",
      "Epoch [26/50], Training Loss: 1433.0411196094215, Validation Loss: 1842.5008544921875\n",
      "Epoch [27/50], Training Loss: 1316.5303231613254, Validation Loss: 1688.9925537109375\n",
      "Epoch [28/50], Training Loss: 1210.6966284393839, Validation Loss: 1557.6619873046875\n",
      "Epoch [29/50], Training Loss: 1113.405688658463, Validation Loss: 1435.742919921875\n",
      "Epoch [30/50], Training Loss: 1024.0778826333897, Validation Loss: 1319.6324462890625\n",
      "Epoch [31/50], Training Loss: 941.9264388230241, Validation Loss: 1212.0137939453125\n",
      "Epoch [32/50], Training Loss: 866.505405105967, Validation Loss: 1112.1610107421875\n",
      "Epoch [33/50], Training Loss: 797.1407156780617, Validation Loss: 1019.9695434570312\n",
      "Epoch [34/50], Training Loss: 733.2242320568972, Validation Loss: 938.9606323242188\n",
      "Epoch [35/50], Training Loss: 673.7829934579734, Validation Loss: 867.9912719726562\n",
      "Epoch [36/50], Training Loss: 618.9537931877663, Validation Loss: 811.5647583007812\n",
      "Epoch [37/50], Training Loss: 567.358473129838, Validation Loss: 743.8140258789062\n",
      "Epoch [38/50], Training Loss: 519.3123904107738, Validation Loss: 671.9132080078125\n",
      "Epoch [39/50], Training Loss: 475.33875307359835, Validation Loss: 621.012451171875\n",
      "Epoch [40/50], Training Loss: 434.7423936574532, Validation Loss: 584.0537109375\n",
      "Epoch [41/50], Training Loss: 397.3246171837639, Validation Loss: 549.4262084960938\n",
      "Epoch [42/50], Training Loss: 362.19230999538354, Validation Loss: 498.6319885253906\n",
      "Epoch [43/50], Training Loss: 329.901162923159, Validation Loss: 440.0425720214844\n",
      "Epoch [44/50], Training Loss: 300.5104333606727, Validation Loss: 400.6473388671875\n",
      "Epoch [45/50], Training Loss: 273.6455423494944, Validation Loss: 372.892333984375\n",
      "Epoch [46/50], Training Loss: 249.20262812236888, Validation Loss: 347.1794128417969\n",
      "Epoch [47/50], Training Loss: 227.02030760118566, Validation Loss: 319.1231384277344\n",
      "Epoch [48/50], Training Loss: 206.96309385345592, Validation Loss: 289.19775390625\n",
      "Epoch [49/50], Training Loss: 188.91817923320443, Validation Loss: 261.142822265625\n",
      "Epoch [50/50], Training Loss: 172.75356346847406, Validation Loss: 245.2568359375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, learning_rate=0.001, window_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model with specified parameters\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "window_size = 7\n",
    "\n",
    "input_size = 7  # Number of features\n",
    "hidden_size = 64  # Number of LSTM units\n",
    "num_layers = 4\n",
    "\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "window_size = 1 # Window size\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 22317.428971511974, Validation Loss: 22738.310546875\n",
      "Epoch [2/50], Training Loss: 19572.714614360568, Validation Loss: 20713.8046875\n",
      "Epoch [3/50], Training Loss: 16554.31648854807, Validation Loss: 19095.16796875\n",
      "Epoch [4/50], Training Loss: 14263.27642229091, Validation Loss: 17919.138671875\n",
      "Epoch [5/50], Training Loss: 12482.496839357547, Validation Loss: 14918.21875\n",
      "Epoch [6/50], Training Loss: 11040.810606993069, Validation Loss: 12291.623046875\n",
      "Epoch [7/50], Training Loss: 9875.969814177493, Validation Loss: 10688.41015625\n",
      "Epoch [8/50], Training Loss: 8935.643862821089, Validation Loss: 9922.2998046875\n",
      "Epoch [9/50], Training Loss: 8057.432918739009, Validation Loss: 13169.2451171875\n",
      "Epoch [10/50], Training Loss: 7269.159717453258, Validation Loss: 12144.662109375\n",
      "Epoch [11/50], Training Loss: 6568.936487477045, Validation Loss: 11621.5478515625\n",
      "Epoch [12/50], Training Loss: 5924.049974036686, Validation Loss: 11027.7607421875\n",
      "Epoch [13/50], Training Loss: 5331.278020269662, Validation Loss: 10306.6357421875\n",
      "Epoch [14/50], Training Loss: 4806.929845042875, Validation Loss: 9443.1005859375\n",
      "Epoch [15/50], Training Loss: 4342.732886399815, Validation Loss: 8539.78515625\n",
      "Epoch [16/50], Training Loss: 3914.855914411976, Validation Loss: 7617.68017578125\n",
      "Epoch [17/50], Training Loss: 3515.975906018151, Validation Loss: 6708.97705078125\n",
      "Epoch [18/50], Training Loss: 3150.6684271979857, Validation Loss: 5769.1787109375\n",
      "Epoch [19/50], Training Loss: 2824.9105805485337, Validation Loss: 4945.96826171875\n",
      "Epoch [20/50], Training Loss: 2538.509278704115, Validation Loss: 4204.54345703125\n",
      "Epoch [21/50], Training Loss: 2293.5083949172267, Validation Loss: 3570.404052734375\n",
      "Epoch [22/50], Training Loss: 2086.0814601033685, Validation Loss: 3133.270751953125\n",
      "Epoch [23/50], Training Loss: 1895.8036241702432, Validation Loss: 2750.19189453125\n",
      "Epoch [24/50], Training Loss: 1722.0243478239986, Validation Loss: 2411.38427734375\n",
      "Epoch [25/50], Training Loss: 1567.7443191404095, Validation Loss: 2127.83251953125\n",
      "Epoch [26/50], Training Loss: 1433.647043098785, Validation Loss: 1899.069091796875\n",
      "Epoch [27/50], Training Loss: 1316.527342955145, Validation Loss: 1722.2537841796875\n",
      "Epoch [28/50], Training Loss: 1210.3487554286826, Validation Loss: 1577.375\n",
      "Epoch [29/50], Training Loss: 1112.9496444657455, Validation Loss: 1449.8543701171875\n",
      "Epoch [30/50], Training Loss: 1023.6748864592846, Validation Loss: 1332.175048828125\n",
      "Epoch [31/50], Training Loss: 941.6214926865116, Validation Loss: 1222.618408203125\n",
      "Epoch [32/50], Training Loss: 866.3039412331915, Validation Loss: 1120.1634521484375\n",
      "Epoch [33/50], Training Loss: 797.0658195734355, Validation Loss: 1028.0938720703125\n",
      "Epoch [34/50], Training Loss: 733.2701296533163, Validation Loss: 948.1976318359375\n",
      "Epoch [35/50], Training Loss: 673.901965051092, Validation Loss: 880.3665771484375\n",
      "Epoch [36/50], Training Loss: 619.0254636311577, Validation Loss: 824.292724609375\n",
      "Epoch [37/50], Training Loss: 567.3409140157898, Validation Loss: 743.8944702148438\n",
      "Epoch [38/50], Training Loss: 519.7669393273605, Validation Loss: 677.2731323242188\n",
      "Epoch [39/50], Training Loss: 475.84057280675074, Validation Loss: 630.213623046875\n",
      "Epoch [40/50], Training Loss: 435.27592819080013, Validation Loss: 596.3909912109375\n",
      "Epoch [41/50], Training Loss: 397.8173302339049, Validation Loss: 562.6756591796875\n",
      "Epoch [42/50], Training Loss: 362.8527460422105, Validation Loss: 516.49462890625\n",
      "Epoch [43/50], Training Loss: 330.65285375607647, Validation Loss: 461.2607116699219\n",
      "Epoch [44/50], Training Loss: 301.2582086654793, Validation Loss: 420.4504699707031\n",
      "Epoch [45/50], Training Loss: 274.36816807129685, Validation Loss: 390.97650146484375\n",
      "Epoch [46/50], Training Loss: 249.91598452215308, Validation Loss: 363.8357238769531\n",
      "Epoch [47/50], Training Loss: 227.79015068546596, Validation Loss: 333.741943359375\n",
      "Epoch [48/50], Training Loss: 207.82990458494766, Validation Loss: 300.809326171875\n",
      "Epoch [49/50], Training Loss: 189.8458568024557, Validation Loss: 269.3791809082031\n",
      "Epoch [50/50], Training Loss: 173.66036480349132, Validation Loss: 245.52479553222656\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, learning_rate=0.001, window_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model with specified parameters\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "input_size = 7  # Number of features\n",
    "hidden_size = 64  # Number of LSTM units\n",
    "num_layers = 4\n",
    "\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "window_size = 1 # Window size\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 22378.59537625562, Validation Loss: 22782.4140625\n",
      "Epoch [2/50], Training Loss: 19631.858954196523, Validation Loss: 20753.775390625\n",
      "Epoch [3/50], Training Loss: 16583.19557265353, Validation Loss: 19101.595703125\n",
      "Epoch [4/50], Training Loss: 14284.49371309695, Validation Loss: 17785.65234375\n",
      "Epoch [5/50], Training Loss: 12493.65131585586, Validation Loss: 14659.44140625\n",
      "Epoch [6/50], Training Loss: 11046.250683409366, Validation Loss: 12117.2666015625\n",
      "Epoch [7/50], Training Loss: 9876.927548880853, Validation Loss: 10698.1474609375\n",
      "Epoch [8/50], Training Loss: 8935.102401780161, Validation Loss: 9948.486328125\n",
      "Epoch [9/50], Training Loss: 8044.144197207675, Validation Loss: 12721.01953125\n",
      "Epoch [10/50], Training Loss: 7259.599931496834, Validation Loss: 11837.876953125\n",
      "Epoch [11/50], Training Loss: 6554.783877510416, Validation Loss: 11162.9189453125\n",
      "Epoch [12/50], Training Loss: 5901.113314770404, Validation Loss: 10239.109375\n",
      "Epoch [13/50], Training Loss: 5303.2307090747945, Validation Loss: 9429.9189453125\n",
      "Epoch [14/50], Training Loss: 4777.660487466305, Validation Loss: 8584.9111328125\n",
      "Epoch [15/50], Training Loss: 4314.160456108738, Validation Loss: 7806.02880859375\n",
      "Epoch [16/50], Training Loss: 3888.7741707630216, Validation Loss: 6858.6923828125\n",
      "Epoch [17/50], Training Loss: 3495.1280546243775, Validation Loss: 5851.27783203125\n",
      "Epoch [18/50], Training Loss: 3136.719669650889, Validation Loss: 4845.947265625\n",
      "Epoch [19/50], Training Loss: 2816.7991413758114, Validation Loss: 4012.628173828125\n",
      "Epoch [20/50], Training Loss: 2533.5058851192493, Validation Loss: 3377.7333984375\n",
      "Epoch [21/50], Training Loss: 2290.3868455222464, Validation Loss: 2916.2275390625\n",
      "Epoch [22/50], Training Loss: 2082.726673551825, Validation Loss: 2588.94970703125\n",
      "Epoch [23/50], Training Loss: 1893.1414578024692, Validation Loss: 2316.853271484375\n",
      "Epoch [24/50], Training Loss: 1720.4581400555765, Validation Loss: 2088.860107421875\n",
      "Epoch [25/50], Training Loss: 1566.7944639416717, Validation Loss: 1905.8883056640625\n",
      "Epoch [26/50], Training Loss: 1432.5647383073433, Validation Loss: 1754.6488037109375\n",
      "Epoch [27/50], Training Loss: 1314.9234447033384, Validation Loss: 1626.880859375\n",
      "Epoch [28/50], Training Loss: 1208.803902464223, Validation Loss: 1520.58837890625\n",
      "Epoch [29/50], Training Loss: 1111.165159974708, Validation Loss: 1425.0074462890625\n",
      "Epoch [30/50], Training Loss: 1021.7701893906955, Validation Loss: 1331.7294921875\n",
      "Epoch [31/50], Training Loss: 939.86017856987, Validation Loss: 1237.6097412109375\n",
      "Epoch [32/50], Training Loss: 864.7015942695997, Validation Loss: 1143.4715576171875\n",
      "Epoch [33/50], Training Loss: 795.6439993789357, Validation Loss: 1056.3055419921875\n",
      "Epoch [34/50], Training Loss: 732.0619721874217, Validation Loss: 978.0111694335938\n",
      "Epoch [35/50], Training Loss: 673.264933826214, Validation Loss: 906.688232421875\n",
      "Epoch [36/50], Training Loss: 618.3039264415303, Validation Loss: 841.7780151367188\n",
      "Epoch [37/50], Training Loss: 567.3265188744421, Validation Loss: 777.75537109375\n",
      "Epoch [38/50], Training Loss: 519.7990563050165, Validation Loss: 701.3026123046875\n",
      "Epoch [39/50], Training Loss: 475.96550891665396, Validation Loss: 644.5406494140625\n",
      "Epoch [40/50], Training Loss: 435.36183999357775, Validation Loss: 601.2236328125\n",
      "Epoch [41/50], Training Loss: 397.80769801855234, Validation Loss: 565.0579223632812\n",
      "Epoch [42/50], Training Loss: 363.2265923882926, Validation Loss: 526.8107299804688\n",
      "Epoch [43/50], Training Loss: 331.39404144881263, Validation Loss: 479.4933166503906\n",
      "Epoch [44/50], Training Loss: 302.0678137058882, Validation Loss: 429.7863464355469\n",
      "Epoch [45/50], Training Loss: 275.206912646024, Validation Loss: 388.6384582519531\n",
      "Epoch [46/50], Training Loss: 250.76563945294953, Validation Loss: 357.072021484375\n",
      "Epoch [47/50], Training Loss: 228.5780030712509, Validation Loss: 330.6966857910156\n",
      "Epoch [48/50], Training Loss: 208.46030805032478, Validation Loss: 305.5006408691406\n",
      "Epoch [49/50], Training Loss: 190.22249978704278, Validation Loss: 279.5374755859375\n",
      "Epoch [50/50], Training Loss: 173.67557876733946, Validation Loss: 253.16986083984375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, learning_rate=0.001, window_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model with specified parameters\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "window_size = 12\n",
    "\n",
    "input_size = 7  # Number of features\n",
    "hidden_size = 64  # Number of LSTM units\n",
    "num_layers = 4\n",
    "\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "window_size = 1 # Window size\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '채' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43m채\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name '채' is not defined"
     ]
    }
   ],
   "source": [
    "채"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, learning_rate=0.001, window_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model with specified parameters\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "input_size = 7  # Number of features\n",
    "hidden_size = 64  # Number of LSTM units\n",
    "num_layers = 4\n",
    "\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "window_size = 1 # Window size\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 22362.390420705764, Validation Loss: 22770.517578125\n",
      "Epoch [2/50], Training Loss: 19625.774810691677, Validation Loss: 20748.962890625\n",
      "Epoch [3/50], Training Loss: 16572.38601870388, Validation Loss: 19089.185546875\n",
      "Epoch [4/50], Training Loss: 14275.918697014538, Validation Loss: 17655.80078125\n",
      "Epoch [5/50], Training Loss: 12485.246166992663, Validation Loss: 14549.3955078125\n",
      "Epoch [6/50], Training Loss: 11038.983618900053, Validation Loss: 12068.533203125\n",
      "Epoch [7/50], Training Loss: 9869.84869106432, Validation Loss: 10690.11328125\n",
      "Epoch [8/50], Training Loss: 8928.599015030866, Validation Loss: 9931.947265625\n",
      "Epoch [9/50], Training Loss: 8037.044813161406, Validation Loss: 12673.654296875\n",
      "Epoch [10/50], Training Loss: 7255.179926998209, Validation Loss: 12002.94140625\n",
      "Epoch [11/50], Training Loss: 6556.284719373862, Validation Loss: 11445.2900390625\n",
      "Epoch [12/50], Training Loss: 5911.89182833453, Validation Loss: 10921.658203125\n",
      "Epoch [13/50], Training Loss: 5320.117903131434, Validation Loss: 10247.7822265625\n",
      "Epoch [14/50], Training Loss: 4797.729920495353, Validation Loss: 9584.3056640625\n",
      "Epoch [15/50], Training Loss: 4336.088397493913, Validation Loss: 8843.166015625\n",
      "Epoch [16/50], Training Loss: 3909.583470110346, Validation Loss: 8068.23291015625\n",
      "Epoch [17/50], Training Loss: 3513.8680925611093, Validation Loss: 7151.57666015625\n",
      "Epoch [18/50], Training Loss: 3149.6377443497277, Validation Loss: 6114.34423828125\n",
      "Epoch [19/50], Training Loss: 2823.337236742641, Validation Loss: 5084.23681640625\n",
      "Epoch [20/50], Training Loss: 2536.549062909324, Validation Loss: 4267.41455078125\n",
      "Epoch [21/50], Training Loss: 2292.4235938660745, Validation Loss: 3643.070556640625\n",
      "Epoch [22/50], Training Loss: 2085.076192302458, Validation Loss: 3217.234130859375\n",
      "Epoch [23/50], Training Loss: 1894.5363032273112, Validation Loss: 2847.663818359375\n",
      "Epoch [24/50], Training Loss: 1720.4862943171493, Validation Loss: 2509.660888671875\n",
      "Epoch [25/50], Training Loss: 1566.0619757148027, Validation Loss: 2236.25830078125\n",
      "Epoch [26/50], Training Loss: 1431.6754493403034, Validation Loss: 2012.73486328125\n",
      "Epoch [27/50], Training Loss: 1314.0194277398484, Validation Loss: 1832.1717529296875\n",
      "Epoch [28/50], Training Loss: 1207.3019374229164, Validation Loss: 1674.2462158203125\n",
      "Epoch [29/50], Training Loss: 1109.4451092312254, Validation Loss: 1533.1348876953125\n",
      "Epoch [30/50], Training Loss: 1019.9530693681446, Validation Loss: 1402.291259765625\n",
      "Epoch [31/50], Training Loss: 937.9095298282576, Validation Loss: 1278.8564453125\n",
      "Epoch [32/50], Training Loss: 862.6844438059865, Validation Loss: 1164.606689453125\n",
      "Epoch [33/50], Training Loss: 793.5027732093215, Validation Loss: 1064.8358154296875\n",
      "Epoch [34/50], Training Loss: 729.505768655393, Validation Loss: 975.1271362304688\n",
      "Epoch [35/50], Training Loss: 670.3315607257274, Validation Loss: 892.9746704101562\n",
      "Epoch [36/50], Training Loss: 615.2047766731112, Validation Loss: 810.7774047851562\n",
      "Epoch [37/50], Training Loss: 563.8081849018273, Validation Loss: 734.1559448242188\n",
      "Epoch [38/50], Training Loss: 516.4810041647359, Validation Loss: 675.8056030273438\n",
      "Epoch [39/50], Training Loss: 472.7964275851365, Validation Loss: 630.2666015625\n",
      "Epoch [40/50], Training Loss: 432.3814662877075, Validation Loss: 577.4074096679688\n",
      "Epoch [41/50], Training Loss: 394.72259757052785, Validation Loss: 525.5070190429688\n",
      "Epoch [42/50], Training Loss: 359.71859966581815, Validation Loss: 479.8736267089844\n",
      "Epoch [43/50], Training Loss: 327.47540444114753, Validation Loss: 438.6639404296875\n",
      "Epoch [44/50], Training Loss: 297.8697493375047, Validation Loss: 404.3544616699219\n",
      "Epoch [45/50], Training Loss: 270.7237971068893, Validation Loss: 375.0688171386719\n",
      "Epoch [46/50], Training Loss: 246.04102121122443, Validation Loss: 345.7400817871094\n",
      "Epoch [47/50], Training Loss: 223.8932916963145, Validation Loss: 313.768310546875\n",
      "Epoch [48/50], Training Loss: 204.11464041937725, Validation Loss: 281.74700927734375\n",
      "Epoch [49/50], Training Loss: 186.43498968091623, Validation Loss: 254.00843811035156\n",
      "Epoch [50/50], Training Loss: 170.608885164991, Validation Loss: 233.06101989746094\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, learning_rate=0.001, window_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model with specified parameters\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "input_size = 7  # Number of features\n",
    "hidden_size = 64  # Number of LSTM units\n",
    "num_layers = 4\n",
    "\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "window_size = 1 # Window size\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the size of the sliding window\n",
    "# sliding_window_size = 1\n",
    "\n",
    "# Walk-forward validation training with sliding window\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using sliding window approach\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        # Determine the end index of the current window\n",
    "        window_end = min(i + window_size, len(x_train_tensor))\n",
    "\n",
    "        # Extract the current window of data\n",
    "        inputs = x_train_tensor[i:window_end].unsqueeze(0)  # Add extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[window_end - 1]  # Label is the last value in the window\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            # Determine the end index of the current window\n",
    "            window_end = min(i + window_size, len(x_val_tensor))\n",
    "\n",
    "            inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '채' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43m채\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name '채' is not defined"
     ]
    }
   ],
   "source": [
    "채"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_layers = 16 to slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 22327.921713422566, Validation Loss: 22746.3828125\n",
      "Epoch [2/50], Training Loss: 19808.118937096144, Validation Loss: 20861.662109375\n",
      "Epoch [3/50], Training Loss: 18020.592938645852, Validation Loss: 19660.25390625\n",
      "Epoch [4/50], Training Loss: 15919.224945660948, Validation Loss: 18331.97265625\n",
      "Epoch [5/50], Training Loss: 13533.551605080094, Validation Loss: 17175.2890625\n",
      "Epoch [6/50], Training Loss: 12019.459947555484, Validation Loss: 17480.8515625\n",
      "Epoch [7/50], Training Loss: 10777.384772162273, Validation Loss: 19364.748046875\n",
      "Epoch [8/50], Training Loss: 9817.751082037768, Validation Loss: 19100.451171875\n",
      "Epoch [9/50], Training Loss: 9099.111255824613, Validation Loss: 19494.08203125\n",
      "Epoch [10/50], Training Loss: 8383.335022393132, Validation Loss: 20797.99609375\n",
      "Epoch [11/50], Training Loss: 7790.24467633839, Validation Loss: 21700.71484375\n",
      "Epoch [12/50], Training Loss: 8141.076041850022, Validation Loss: 28721.216796875\n",
      "Epoch [13/50], Training Loss: 8298.214665567933, Validation Loss: 22598.580078125\n",
      "Epoch [14/50], Training Loss: 7642.684330352393, Validation Loss: 23083.60546875\n",
      "Epoch [15/50], Training Loss: 7456.625723605129, Validation Loss: 22837.88671875\n",
      "Epoch [16/50], Training Loss: 6965.271538404498, Validation Loss: 26434.078125\n",
      "Epoch [17/50], Training Loss: 5411.356332433209, Validation Loss: 21511.537109375\n",
      "Epoch [18/50], Training Loss: 6829.637414877072, Validation Loss: 37211.0390625\n",
      "Epoch [19/50], Training Loss: 6820.225606677146, Validation Loss: 23572.587890625\n",
      "Epoch [20/50], Training Loss: 4214.69780800128, Validation Loss: 15948.4326171875\n",
      "Epoch [21/50], Training Loss: 3900.042393495003, Validation Loss: 20467.7421875\n",
      "Epoch [22/50], Training Loss: 3776.981446128945, Validation Loss: 22451.77734375\n",
      "Epoch [23/50], Training Loss: 3113.0218045048123, Validation Loss: 21240.7578125\n",
      "Epoch [24/50], Training Loss: 2778.614282487358, Validation Loss: 19343.048828125\n",
      "Epoch [25/50], Training Loss: 2504.8332114374984, Validation Loss: 14343.6142578125\n",
      "Epoch [26/50], Training Loss: 2261.8743118672223, Validation Loss: 13671.3837890625\n",
      "Epoch [27/50], Training Loss: 2082.090912294271, Validation Loss: 9160.400390625\n",
      "Epoch [28/50], Training Loss: 1861.289818705502, Validation Loss: 10012.8623046875\n",
      "Epoch [29/50], Training Loss: 1720.7870859945072, Validation Loss: 8950.8779296875\n",
      "Epoch [30/50], Training Loss: 1564.110277996182, Validation Loss: 8660.0380859375\n",
      "Epoch [31/50], Training Loss: 1447.0507979846832, Validation Loss: 7979.724609375\n",
      "Epoch [32/50], Training Loss: 1336.93357679008, Validation Loss: 8116.41259765625\n",
      "Epoch [33/50], Training Loss: 1233.0351778185961, Validation Loss: 7938.79638671875\n",
      "Epoch [34/50], Training Loss: 1143.377039002824, Validation Loss: 7322.068359375\n",
      "Epoch [35/50], Training Loss: 1055.5605661661673, Validation Loss: 6215.775390625\n",
      "Epoch [36/50], Training Loss: 963.6535774514924, Validation Loss: 6245.474609375\n",
      "Epoch [37/50], Training Loss: 892.1880287619641, Validation Loss: 6963.71826171875\n",
      "Epoch [38/50], Training Loss: 826.4130761948031, Validation Loss: 6752.81591796875\n",
      "Epoch [39/50], Training Loss: 783.8266302717109, Validation Loss: 6361.3359375\n",
      "Epoch [40/50], Training Loss: 707.400269466122, Validation Loss: 6482.4658203125\n",
      "Epoch [41/50], Training Loss: 672.0539140586699, Validation Loss: 6407.89404296875\n",
      "Epoch [42/50], Training Loss: 611.0462113264854, Validation Loss: 6219.12353515625\n",
      "Epoch [43/50], Training Loss: 576.8681119265567, Validation Loss: 6468.4638671875\n",
      "Epoch [44/50], Training Loss: 541.0491671820718, Validation Loss: 6367.0556640625\n",
      "Epoch [45/50], Training Loss: 500.9472620980835, Validation Loss: 6660.45556640625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     58\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 59\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Validate the model after each epoch using x_val_tensor and y_val_tensor\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\optim\\adam.py:345\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    344\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 345\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[0;32m    348\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, learning_rate=0.001, window_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model with specified parameters\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "input_size = 7  # Number of features\n",
    "hidden_size = 64  # Number of LSTM units\n",
    "num_layers = 16\n",
    "# Number of LSTM layers\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "window_size = 20  # Window size\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Walk-forward validation training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using x_train_tensor and y_train_tensor\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        inputs = x_train_tensor[i].unsqueeze(0).unsqueeze(0)  # Add two extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[i]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            inputs = x_val_tensor[i].unsqueeze(0).unsqueeze(0)\n",
    "            labels = y_val_tensor[i]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "# After training, you can test the model similarly using x_test_tensor and y_test_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "채"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 22360.31667383806, Validation Loss: 22769.455078125\n",
      "Epoch [2/50], Training Loss: 19505.713305504094, Validation Loss: 20672.65234375\n",
      "Epoch [3/50], Training Loss: 16501.247543511898, Validation Loss: 19049.033203125\n",
      "Epoch [4/50], Training Loss: 14218.50668195493, Validation Loss: 17513.68359375\n",
      "Epoch [5/50], Training Loss: 12438.109628592638, Validation Loss: 14365.9365234375\n",
      "Epoch [6/50], Training Loss: 11001.18178465093, Validation Loss: 11959.9912109375\n",
      "Epoch [7/50], Training Loss: 9838.843309614864, Validation Loss: 10677.056640625\n",
      "Epoch [8/50], Training Loss: 8904.14371472347, Validation Loss: 9903.6171875\n",
      "Epoch [9/50], Training Loss: 8020.264743066753, Validation Loss: 12834.7744140625\n",
      "Epoch [10/50], Training Loss: 7236.501935498802, Validation Loss: 11874.6171875\n",
      "Epoch [11/50], Training Loss: 6536.458665778439, Validation Loss: 11239.4130859375\n",
      "Epoch [12/50], Training Loss: 5891.431617533891, Validation Loss: 10446.9951171875\n",
      "Epoch [13/50], Training Loss: 5298.031714918126, Validation Loss: 9632.755859375\n",
      "Epoch [14/50], Training Loss: 4774.765875893797, Validation Loss: 8570.75\n",
      "Epoch [15/50], Training Loss: 4310.883544968414, Validation Loss: 7524.04833984375\n",
      "Epoch [16/50], Training Loss: 3882.3949333228084, Validation Loss: 6545.7734375\n",
      "Epoch [17/50], Training Loss: 3484.9149410920527, Validation Loss: 5484.22265625\n",
      "Epoch [18/50], Training Loss: 3124.832796301612, Validation Loss: 4591.04345703125\n",
      "Epoch [19/50], Training Loss: 2805.8530463077454, Validation Loss: 3909.86669921875\n",
      "Epoch [20/50], Training Loss: 2523.7362670457833, Validation Loss: 3371.23583984375\n",
      "Epoch [21/50], Training Loss: 2283.648210326295, Validation Loss: 2990.227294921875\n",
      "Epoch [22/50], Training Loss: 2075.481805439297, Validation Loss: 2656.1240234375\n",
      "Epoch [23/50], Training Loss: 1887.098895584606, Validation Loss: 2372.397216796875\n",
      "Epoch [24/50], Training Loss: 1715.5633411719461, Validation Loss: 2129.499755859375\n",
      "Epoch [25/50], Training Loss: 1563.1112786939007, Validation Loss: 1920.1640625\n",
      "Epoch [26/50], Training Loss: 1431.1574707767588, Validation Loss: 1751.0406494140625\n",
      "Epoch [27/50], Training Loss: 1315.4747081353094, Validation Loss: 1618.721923828125\n",
      "Epoch [28/50], Training Loss: 1209.4846458410293, Validation Loss: 1498.0364990234375\n",
      "Epoch [29/50], Training Loss: 1112.4696194820046, Validation Loss: 1386.7098388671875\n",
      "Epoch [30/50], Training Loss: 1023.6563800509707, Validation Loss: 1285.991455078125\n",
      "Epoch [31/50], Training Loss: 941.7769493780505, Validation Loss: 1196.494873046875\n",
      "Epoch [32/50], Training Loss: 866.7432758159418, Validation Loss: 1101.174560546875\n",
      "Epoch [33/50], Training Loss: 797.7068637391744, Validation Loss: 1005.8161010742188\n",
      "Epoch [34/50], Training Loss: 734.1338760722485, Validation Loss: 918.271728515625\n",
      "Epoch [35/50], Training Loss: 674.651757120111, Validation Loss: 843.4484252929688\n",
      "Epoch [36/50], Training Loss: 619.3059659386405, Validation Loss: 783.9195556640625\n",
      "Epoch [37/50], Training Loss: 567.8798494912639, Validation Loss: 730.8792114257812\n",
      "Epoch [38/50], Training Loss: 520.0591053192129, Validation Loss: 670.6781005859375\n",
      "Epoch [39/50], Training Loss: 476.21018014814587, Validation Loss: 616.6112060546875\n",
      "Epoch [40/50], Training Loss: 435.5817569350125, Validation Loss: 569.3174438476562\n",
      "Epoch [41/50], Training Loss: 397.98705182271, Validation Loss: 530.0982055664062\n",
      "Epoch [42/50], Training Loss: 363.43059440909315, Validation Loss: 492.2107849121094\n",
      "Epoch [43/50], Training Loss: 331.4915973190045, Validation Loss: 450.71209716796875\n",
      "Epoch [44/50], Training Loss: 301.96891894508934, Validation Loss: 405.9750671386719\n",
      "Epoch [45/50], Training Loss: 275.08768265269964, Validation Loss: 367.8995056152344\n",
      "Epoch [46/50], Training Loss: 250.594411159443, Validation Loss: 335.2222595214844\n",
      "Epoch [47/50], Training Loss: 228.26662612213596, Validation Loss: 304.4356384277344\n",
      "Epoch [48/50], Training Loss: 208.03575223821704, Validation Loss: 275.6087646484375\n",
      "Epoch [49/50], Training Loss: 189.78486299043516, Validation Loss: 249.3068084716797\n",
      "Epoch [50/50], Training Loss: 173.27458214634703, Validation Loss: 226.76976013183594\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, learning_rate=0.001, window_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model with specified parameters\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "input_size = 7  # Number of features\n",
    "hidden_size = 64  # Number of LSTM units\n",
    "num_layers = 4\n",
    "# Number of LSTM layers\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "window_size = 20  # Window size\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Walk-forward validation training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Train the model using x_train_tensor and y_train_tensor\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        inputs = x_train_tensor[i].unsqueeze(0).unsqueeze(0)  # Add two extra dimensions for batch and sequence length\n",
    "        labels = y_train_tensor[i]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate the model after each epoch using x_val_tensor and y_val_tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_val_tensor)):\n",
    "            inputs = x_val_tensor[i].unsqueeze(0).unsqueeze(0)\n",
    "            labels = y_val_tensor[i]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "# After training, you can test the model similarly using x_test_tensor and y_test_tensor\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
