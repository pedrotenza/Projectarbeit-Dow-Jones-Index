{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with target variable: open\n",
      "\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0003, window_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Training Loss: 27801.95051053062, Validation Loss: 27333.595703125\n",
      "Epoch [2/1500], Training Loss: 26070.488700325834, Validation Loss: 25755.142578125\n",
      "Epoch [3/1500], Training Loss: 24662.73662761667, Validation Loss: 24430.470703125\n",
      "Epoch [4/1500], Training Loss: 23505.198705265935, Validation Loss: 23341.79296875\n",
      "Epoch [5/1500], Training Loss: 22545.683825416607, Validation Loss: 22402.91015625\n",
      "Epoch [6/1500], Training Loss: 21548.952729497578, Validation Loss: 21434.84375\n",
      "Epoch [7/1500], Training Loss: 20618.004249594615, Validation Loss: 20512.015625\n",
      "Epoch [8/1500], Training Loss: 19748.32208769504, Validation Loss: 19656.845703125\n",
      "Epoch [9/1500], Training Loss: 18944.06635768439, Validation Loss: 18862.791015625\n",
      "Epoch [10/1500], Training Loss: 18192.59623123343, Validation Loss: 18121.794921875\n",
      "Epoch [11/1500], Training Loss: 17488.50295575279, Validation Loss: 17427.73046875\n",
      "Epoch [12/1500], Training Loss: 16826.384332044327, Validation Loss: 16777.42578125\n",
      "Epoch [13/1500], Training Loss: 16203.487578943357, Validation Loss: 16166.689453125\n",
      "Epoch [14/1500], Training Loss: 15618.363785422049, Validation Loss: 15586.0625\n",
      "Epoch [15/1500], Training Loss: 15067.892054164537, Validation Loss: 15033.779296875\n",
      "Epoch [16/1500], Training Loss: 14548.706019640253, Validation Loss: 14518.271484375\n",
      "Epoch [17/1500], Training Loss: 14058.39449644329, Validation Loss: 14070.9755859375\n",
      "Epoch [18/1500], Training Loss: 13592.599092998473, Validation Loss: 13627.5517578125\n",
      "Epoch [19/1500], Training Loss: 13150.667901191879, Validation Loss: 13190.509765625\n",
      "Epoch [20/1500], Training Loss: 12731.454670766132, Validation Loss: 12768.87890625\n",
      "Epoch [21/1500], Training Loss: 12333.807943122294, Validation Loss: 12367.255859375\n",
      "Epoch [22/1500], Training Loss: 11956.4218471037, Validation Loss: 11987.72265625\n",
      "Epoch [23/1500], Training Loss: 11597.960793599805, Validation Loss: 11630.107421875\n",
      "Epoch [24/1500], Training Loss: 11257.242001093466, Validation Loss: 11293.556640625\n",
      "Epoch [25/1500], Training Loss: 10933.313541849528, Validation Loss: 10977.0234375\n",
      "Epoch [26/1500], Training Loss: 10625.52919226289, Validation Loss: 10679.03515625\n",
      "Epoch [27/1500], Training Loss: 10333.358593750429, Validation Loss: 10398.966796875\n",
      "Epoch [28/1500], Training Loss: 10056.297752146194, Validation Loss: 10138.0654296875\n",
      "Epoch [29/1500], Training Loss: 9793.865530941244, Validation Loss: 9901.126953125\n",
      "Epoch [30/1500], Training Loss: 9545.089696049228, Validation Loss: 9734.748046875\n",
      "Epoch [31/1500], Training Loss: 9289.408331399718, Validation Loss: 9960.474609375\n",
      "Epoch [32/1500], Training Loss: 9094.550108638505, Validation Loss: 9423.0048828125\n",
      "Epoch [33/1500], Training Loss: 8813.174056957494, Validation Loss: 9159.763671875\n",
      "Epoch [34/1500], Training Loss: 8565.356198366888, Validation Loss: 8958.697265625\n",
      "Epoch [35/1500], Training Loss: 8326.645855549774, Validation Loss: 8750.8701171875\n",
      "Epoch [36/1500], Training Loss: 8097.050138251876, Validation Loss: 8510.0419921875\n",
      "Epoch [37/1500], Training Loss: 7877.144160479401, Validation Loss: 8257.185546875\n",
      "Epoch [38/1500], Training Loss: 7668.555651909866, Validation Loss: 8005.43408203125\n",
      "Epoch [39/1500], Training Loss: 7470.049879839407, Validation Loss: 7760.7734375\n",
      "Epoch [40/1500], Training Loss: 7279.250774198653, Validation Loss: 7528.57470703125\n",
      "Epoch [41/1500], Training Loss: 7094.6682536149, Validation Loss: 7309.73095703125\n",
      "Epoch [42/1500], Training Loss: 6915.424698050894, Validation Loss: 7104.39013671875\n",
      "Epoch [43/1500], Training Loss: 6741.107863629332, Validation Loss: 6912.916015625\n",
      "Epoch [44/1500], Training Loss: 6571.419743000477, Validation Loss: 6733.73486328125\n",
      "Epoch [45/1500], Training Loss: 6406.010389095382, Validation Loss: 6562.19140625\n",
      "Epoch [46/1500], Training Loss: 6244.574793987071, Validation Loss: 6392.87353515625\n",
      "Epoch [47/1500], Training Loss: 6087.020149159417, Validation Loss: 6224.6611328125\n",
      "Epoch [48/1500], Training Loss: 5933.4068873191, Validation Loss: 6061.43310546875\n",
      "Epoch [49/1500], Training Loss: 5783.791747507731, Validation Loss: 5906.12255859375\n",
      "Epoch [50/1500], Training Loss: 5638.284437841984, Validation Loss: 5759.37890625\n",
      "Epoch [51/1500], Training Loss: 5496.938504143411, Validation Loss: 5620.78759765625\n",
      "Epoch [52/1500], Training Loss: 5359.763036104386, Validation Loss: 5488.9306640625\n",
      "Epoch [53/1500], Training Loss: 5226.763400345868, Validation Loss: 5362.35888671875\n",
      "Epoch [54/1500], Training Loss: 5097.861611431979, Validation Loss: 5240.0634765625\n",
      "Epoch [55/1500], Training Loss: 4972.886702338084, Validation Loss: 5121.1083984375\n",
      "Epoch [56/1500], Training Loss: 4851.4891646965025, Validation Loss: 5004.15185546875\n",
      "Epoch [57/1500], Training Loss: 4733.2123915451075, Validation Loss: 4887.57080078125\n",
      "Epoch [58/1500], Training Loss: 4617.59423964161, Validation Loss: 4771.001953125\n",
      "Epoch [59/1500], Training Loss: 4504.347254073226, Validation Loss: 4655.2861328125\n",
      "Epoch [60/1500], Training Loss: 4393.306884901124, Validation Loss: 4541.0830078125\n",
      "Epoch [61/1500], Training Loss: 4284.377780899617, Validation Loss: 4428.93017578125\n",
      "Epoch [62/1500], Training Loss: 4177.541482865162, Validation Loss: 4319.29541015625\n",
      "Epoch [63/1500], Training Loss: 4072.84575769069, Validation Loss: 4212.41015625\n",
      "Epoch [64/1500], Training Loss: 3970.3436970615653, Validation Loss: 4108.3466796875\n",
      "Epoch [65/1500], Training Loss: 3870.1098323149554, Validation Loss: 4007.0673828125\n",
      "Epoch [66/1500], Training Loss: 3772.205266418298, Validation Loss: 3908.560791015625\n",
      "Epoch [67/1500], Training Loss: 3676.6752499222416, Validation Loss: 3812.78369140625\n",
      "Epoch [68/1500], Training Loss: 3583.535276030868, Validation Loss: 3719.7431640625\n",
      "Epoch [69/1500], Training Loss: 3492.8003857488575, Validation Loss: 3629.43212890625\n",
      "Epoch [70/1500], Training Loss: 3404.455656215145, Validation Loss: 3541.80078125\n",
      "Epoch [71/1500], Training Loss: 3318.501531550361, Validation Loss: 3456.866455078125\n",
      "Epoch [72/1500], Training Loss: 3234.918303444478, Validation Loss: 3374.609130859375\n",
      "Epoch [73/1500], Training Loss: 3153.692078607223, Validation Loss: 3295.00146484375\n",
      "Epoch [74/1500], Training Loss: 3074.7815704757463, Validation Loss: 3217.954833984375\n",
      "Epoch [75/1500], Training Loss: 2998.1463315687356, Validation Loss: 3143.324951171875\n",
      "Epoch [76/1500], Training Loss: 2923.71767592728, Validation Loss: 3070.962890625\n",
      "Epoch [77/1500], Training Loss: 2851.397309415118, Validation Loss: 3000.774658203125\n",
      "Epoch [78/1500], Training Loss: 2781.0695467179776, Validation Loss: 2932.693115234375\n",
      "Epoch [79/1500], Training Loss: 2712.6840429304875, Validation Loss: 2866.742431640625\n",
      "Epoch [80/1500], Training Loss: 2646.2719972078557, Validation Loss: 2802.94287109375\n",
      "Epoch [81/1500], Training Loss: 2581.945734625389, Validation Loss: 2741.3310546875\n",
      "Epoch [82/1500], Training Loss: 2519.7904051046253, Validation Loss: 2681.90771484375\n",
      "Epoch [83/1500], Training Loss: 2459.8584560352524, Validation Loss: 2624.6455078125\n",
      "Epoch [84/1500], Training Loss: 2402.1028504723777, Validation Loss: 2569.498779296875\n",
      "Epoch [85/1500], Training Loss: 2346.352392255452, Validation Loss: 2516.287109375\n",
      "Epoch [86/1500], Training Loss: 2292.393400798235, Validation Loss: 2464.81396484375\n",
      "Epoch [87/1500], Training Loss: 2240.028704054113, Validation Loss: 2414.908203125\n",
      "Epoch [88/1500], Training Loss: 2189.1093456155145, Validation Loss: 2366.369384765625\n",
      "Epoch [89/1500], Training Loss: 2139.547136836459, Validation Loss: 2319.0419921875\n",
      "Epoch [90/1500], Training Loss: 2091.2550217102103, Validation Loss: 2272.787109375\n",
      "Epoch [91/1500], Training Loss: 2044.151035812659, Validation Loss: 2227.480224609375\n",
      "Epoch [92/1500], Training Loss: 1998.1877785994993, Validation Loss: 2183.071044921875\n",
      "Epoch [93/1500], Training Loss: 1953.3231897185544, Validation Loss: 2139.486328125\n",
      "Epoch [94/1500], Training Loss: 1909.4922039944022, Validation Loss: 2096.663330078125\n",
      "Epoch [95/1500], Training Loss: 1866.6840521144416, Validation Loss: 2054.612060546875\n",
      "Epoch [96/1500], Training Loss: 1824.911740066723, Validation Loss: 2013.3031005859375\n",
      "Epoch [97/1500], Training Loss: 1784.1728513909754, Validation Loss: 1972.7635498046875\n",
      "Epoch [98/1500], Training Loss: 1744.508371161294, Validation Loss: 1932.9720458984375\n",
      "Epoch [99/1500], Training Loss: 1705.9254070293796, Validation Loss: 1893.9053955078125\n",
      "Epoch [100/1500], Training Loss: 1668.4240156712963, Validation Loss: 1855.530029296875\n",
      "Epoch [101/1500], Training Loss: 1631.9829846527887, Validation Loss: 1817.7701416015625\n",
      "Epoch [102/1500], Training Loss: 1596.5683452413123, Validation Loss: 1780.5726318359375\n",
      "Epoch [103/1500], Training Loss: 1562.133956722389, Validation Loss: 1743.88720703125\n",
      "Epoch [104/1500], Training Loss: 1528.634242280034, Validation Loss: 1707.6630859375\n",
      "Epoch [105/1500], Training Loss: 1496.00953321892, Validation Loss: 1671.8575439453125\n",
      "Epoch [106/1500], Training Loss: 1464.1895429410315, Validation Loss: 1636.416259765625\n",
      "Epoch [107/1500], Training Loss: 1433.1257181607066, Validation Loss: 1601.3458251953125\n",
      "Epoch [108/1500], Training Loss: 1402.7584852418179, Validation Loss: 1566.6898193359375\n",
      "Epoch [109/1500], Training Loss: 1373.055519795562, Validation Loss: 1532.572265625\n",
      "Epoch [110/1500], Training Loss: 1344.026265271946, Validation Loss: 1499.052978515625\n",
      "Epoch [111/1500], Training Loss: 1315.6752064225075, Validation Loss: 1466.0611572265625\n",
      "Epoch [112/1500], Training Loss: 1287.978076966246, Validation Loss: 1433.44970703125\n",
      "Epoch [113/1500], Training Loss: 1260.9225650588653, Validation Loss: 1401.069580078125\n",
      "Epoch [114/1500], Training Loss: 1234.470306164682, Validation Loss: 1368.796142578125\n",
      "Epoch [115/1500], Training Loss: 1208.578088487931, Validation Loss: 1336.5628662109375\n",
      "Epoch [116/1500], Training Loss: 1183.2168802589977, Validation Loss: 1304.3756103515625\n",
      "Epoch [117/1500], Training Loss: 1158.348355933028, Validation Loss: 1272.36474609375\n",
      "Epoch [118/1500], Training Loss: 1133.9672560002712, Validation Loss: 1240.78369140625\n",
      "Epoch [119/1500], Training Loss: 1110.0793637313893, Validation Loss: 1209.9365234375\n",
      "Epoch [120/1500], Training Loss: 1086.7190725951057, Validation Loss: 1180.1258544921875\n",
      "Epoch [121/1500], Training Loss: 1063.9089738875655, Validation Loss: 1151.589599609375\n",
      "Epoch [122/1500], Training Loss: 1041.65563995291, Validation Loss: 1124.4603271484375\n",
      "Epoch [123/1500], Training Loss: 1019.9595598017837, Validation Loss: 1098.7796630859375\n",
      "Epoch [124/1500], Training Loss: 998.8241582068662, Validation Loss: 1074.5208740234375\n",
      "Epoch [125/1500], Training Loss: 978.2305336786595, Validation Loss: 1051.5587158203125\n",
      "Epoch [126/1500], Training Loss: 958.1594643549723, Validation Loss: 1029.755126953125\n",
      "Epoch [127/1500], Training Loss: 938.592031428126, Validation Loss: 1008.961669921875\n",
      "Epoch [128/1500], Training Loss: 919.5035664381517, Validation Loss: 989.0239868164062\n",
      "Epoch [129/1500], Training Loss: 900.8639148351983, Validation Loss: 969.8209228515625\n",
      "Epoch [130/1500], Training Loss: 882.6540290947481, Validation Loss: 951.2493286132812\n",
      "Epoch [131/1500], Training Loss: 864.8507317609017, Validation Loss: 933.2529907226562\n",
      "Epoch [132/1500], Training Loss: 847.4412554727611, Validation Loss: 915.7666625976562\n",
      "Epoch [133/1500], Training Loss: 830.3974018044007, Validation Loss: 898.7431030273438\n",
      "Epoch [134/1500], Training Loss: 813.7024036356739, Validation Loss: 882.143798828125\n",
      "Epoch [135/1500], Training Loss: 797.3450404574169, Validation Loss: 865.9537353515625\n",
      "Epoch [136/1500], Training Loss: 781.3160315629646, Validation Loss: 850.14111328125\n",
      "Epoch [137/1500], Training Loss: 765.5972453356081, Validation Loss: 834.68701171875\n",
      "Epoch [138/1500], Training Loss: 750.1832212841161, Validation Loss: 819.5695190429688\n",
      "Epoch [139/1500], Training Loss: 735.0666907814332, Validation Loss: 804.7799072265625\n",
      "Epoch [140/1500], Training Loss: 720.2373604151594, Validation Loss: 790.3018188476562\n",
      "Epoch [141/1500], Training Loss: 705.6874212354447, Validation Loss: 776.1170654296875\n",
      "Epoch [142/1500], Training Loss: 691.4111561049325, Validation Loss: 762.229248046875\n",
      "Epoch [143/1500], Training Loss: 677.4057587907107, Validation Loss: 748.6243896484375\n",
      "Epoch [144/1500], Training Loss: 663.6681589179769, Validation Loss: 735.2967529296875\n",
      "Epoch [145/1500], Training Loss: 650.1943463459342, Validation Loss: 722.24365234375\n",
      "Epoch [146/1500], Training Loss: 636.9846075245587, Validation Loss: 709.458740234375\n",
      "Epoch [147/1500], Training Loss: 624.027879748446, Validation Loss: 696.9285888671875\n",
      "Epoch [148/1500], Training Loss: 611.3174012508749, Validation Loss: 684.6489868164062\n",
      "Epoch [149/1500], Training Loss: 598.8560659812495, Validation Loss: 672.6195068359375\n",
      "Epoch [150/1500], Training Loss: 586.6330799639114, Validation Loss: 660.8311767578125\n",
      "Epoch [151/1500], Training Loss: 574.6470321817857, Validation Loss: 649.2838134765625\n",
      "Epoch [152/1500], Training Loss: 562.8950692826859, Validation Loss: 637.9764404296875\n",
      "Epoch [153/1500], Training Loss: 551.3748264281077, Validation Loss: 626.8989868164062\n",
      "Epoch [154/1500], Training Loss: 540.0746053501346, Validation Loss: 616.0475463867188\n",
      "Epoch [155/1500], Training Loss: 528.9949110667919, Validation Loss: 605.421875\n",
      "Epoch [156/1500], Training Loss: 518.1315044127715, Validation Loss: 595.0130615234375\n",
      "Epoch [157/1500], Training Loss: 507.4795537697107, Validation Loss: 584.82275390625\n",
      "Epoch [158/1500], Training Loss: 497.0356153417522, Validation Loss: 574.8485107421875\n",
      "Epoch [159/1500], Training Loss: 486.7940765163326, Validation Loss: 565.0855102539062\n",
      "Epoch [160/1500], Training Loss: 476.7525357027432, Validation Loss: 555.5333862304688\n",
      "Epoch [161/1500], Training Loss: 466.9114726327188, Validation Loss: 546.1923828125\n",
      "Epoch [162/1500], Training Loss: 457.2654815552764, Validation Loss: 537.0625610351562\n",
      "Epoch [163/1500], Training Loss: 447.81428679476033, Validation Loss: 528.1337890625\n",
      "Epoch [164/1500], Training Loss: 438.5486417899896, Validation Loss: 519.4083862304688\n",
      "Epoch [165/1500], Training Loss: 429.47211307299307, Validation Loss: 510.8813171386719\n",
      "Epoch [166/1500], Training Loss: 420.58043152936517, Validation Loss: 502.5509948730469\n",
      "Epoch [167/1500], Training Loss: 411.871503338406, Validation Loss: 494.41162109375\n",
      "Epoch [168/1500], Training Loss: 403.3391475009259, Validation Loss: 486.4576110839844\n",
      "Epoch [169/1500], Training Loss: 394.9848917383835, Validation Loss: 478.6926574707031\n",
      "Epoch [170/1500], Training Loss: 386.80664698653254, Validation Loss: 471.1075439453125\n",
      "Epoch [171/1500], Training Loss: 378.79820259162886, Validation Loss: 463.6993103027344\n",
      "Epoch [172/1500], Training Loss: 370.96248879581407, Validation Loss: 456.4710388183594\n",
      "Epoch [173/1500], Training Loss: 363.2951083124899, Validation Loss: 449.40911865234375\n",
      "Epoch [174/1500], Training Loss: 355.79194730758684, Validation Loss: 442.5157165527344\n",
      "Epoch [175/1500], Training Loss: 348.44896792378046, Validation Loss: 435.7826232910156\n",
      "Epoch [176/1500], Training Loss: 341.2684095768904, Validation Loss: 429.2105407714844\n",
      "Epoch [177/1500], Training Loss: 334.2440079560749, Validation Loss: 422.7938232421875\n",
      "Epoch [178/1500], Training Loss: 327.3747856047515, Validation Loss: 416.5273132324219\n",
      "Epoch [179/1500], Training Loss: 320.6585650916125, Validation Loss: 410.411865234375\n",
      "Epoch [180/1500], Training Loss: 314.09353809218567, Validation Loss: 404.44097900390625\n",
      "Epoch [181/1500], Training Loss: 307.6731222937119, Validation Loss: 398.6087951660156\n",
      "Epoch [182/1500], Training Loss: 301.39601027573667, Validation Loss: 392.9166564941406\n",
      "Epoch [183/1500], Training Loss: 295.2591241969653, Validation Loss: 387.3556823730469\n",
      "Epoch [184/1500], Training Loss: 289.2601251207391, Validation Loss: 381.9281921386719\n",
      "Epoch [185/1500], Training Loss: 283.39578404529277, Validation Loss: 376.626220703125\n",
      "Epoch [186/1500], Training Loss: 277.6648938042516, Validation Loss: 371.45263671875\n",
      "Epoch [187/1500], Training Loss: 272.0662854400213, Validation Loss: 366.4022521972656\n",
      "Epoch [188/1500], Training Loss: 266.5949344619822, Validation Loss: 361.4719543457031\n",
      "Epoch [189/1500], Training Loss: 261.24924568029996, Validation Loss: 356.65802001953125\n",
      "Epoch [190/1500], Training Loss: 256.02568315733185, Validation Loss: 351.960693359375\n",
      "Epoch [191/1500], Training Loss: 250.9246353754547, Validation Loss: 347.3760986328125\n",
      "Epoch [192/1500], Training Loss: 245.9425210205219, Validation Loss: 342.9021301269531\n",
      "Epoch [193/1500], Training Loss: 241.07971386145613, Validation Loss: 338.5370788574219\n",
      "Epoch [194/1500], Training Loss: 236.33317892773667, Validation Loss: 334.28033447265625\n",
      "Epoch [195/1500], Training Loss: 231.69881222407759, Validation Loss: 330.123291015625\n",
      "Epoch [196/1500], Training Loss: 227.17430820156753, Validation Loss: 326.05078125\n",
      "Epoch [197/1500], Training Loss: 222.7543399456805, Validation Loss: 322.0654602050781\n",
      "Epoch [198/1500], Training Loss: 218.43950481385667, Validation Loss: 318.1661682128906\n",
      "Epoch [199/1500], Training Loss: 214.2231696119506, Validation Loss: 314.3504333496094\n",
      "Epoch [200/1500], Training Loss: 210.10583997626503, Validation Loss: 310.6125793457031\n",
      "Epoch [201/1500], Training Loss: 206.08221741504545, Validation Loss: 306.9504089355469\n",
      "Epoch [202/1500], Training Loss: 202.15181418203701, Validation Loss: 303.3612365722656\n",
      "Epoch [203/1500], Training Loss: 198.3119472999158, Validation Loss: 299.8431091308594\n",
      "Epoch [204/1500], Training Loss: 194.56004767155252, Validation Loss: 296.39239501953125\n",
      "Epoch [205/1500], Training Loss: 190.8929008065449, Validation Loss: 293.0053405761719\n",
      "Epoch [206/1500], Training Loss: 187.30702039192892, Validation Loss: 289.677001953125\n",
      "Epoch [207/1500], Training Loss: 183.79855332066143, Validation Loss: 286.4037170410156\n",
      "Epoch [208/1500], Training Loss: 180.3650804342922, Validation Loss: 283.18084716796875\n",
      "Epoch [209/1500], Training Loss: 177.00321448364892, Validation Loss: 280.0029602050781\n",
      "Epoch [210/1500], Training Loss: 173.70783539240924, Validation Loss: 276.86474609375\n",
      "Epoch [211/1500], Training Loss: 170.47535103787274, Validation Loss: 273.7615661621094\n",
      "Epoch [212/1500], Training Loss: 167.30152034309344, Validation Loss: 270.6893615722656\n",
      "Epoch [213/1500], Training Loss: 164.18174499739254, Validation Loss: 267.6502685546875\n",
      "Epoch [214/1500], Training Loss: 161.11108759638284, Validation Loss: 264.64727783203125\n",
      "Epoch [215/1500], Training Loss: 158.08602734579216, Validation Loss: 261.6961669921875\n",
      "Epoch [216/1500], Training Loss: 155.1068936249443, Validation Loss: 258.8124084472656\n",
      "Epoch [217/1500], Training Loss: 152.17706133949795, Validation Loss: 255.99729919433594\n",
      "Epoch [218/1500], Training Loss: 149.29888440737764, Validation Loss: 253.2342529296875\n",
      "Epoch [219/1500], Training Loss: 146.47317907645538, Validation Loss: 250.50779724121094\n",
      "Epoch [220/1500], Training Loss: 143.70138112864, Validation Loss: 247.806640625\n",
      "Epoch [221/1500], Training Loss: 140.9831983347697, Validation Loss: 245.1246795654297\n",
      "Epoch [222/1500], Training Loss: 138.31580386682003, Validation Loss: 242.46218872070312\n",
      "Epoch [223/1500], Training Loss: 135.69698158440931, Validation Loss: 239.8219451904297\n",
      "Epoch [224/1500], Training Loss: 133.1280462872781, Validation Loss: 237.2042694091797\n",
      "Epoch [225/1500], Training Loss: 130.6052961170542, Validation Loss: 234.60675048828125\n",
      "Epoch [226/1500], Training Loss: 128.12774926747977, Validation Loss: 232.02610778808594\n",
      "Epoch [227/1500], Training Loss: 125.69499372378485, Validation Loss: 229.45944213867188\n",
      "Epoch [228/1500], Training Loss: 123.30525488729265, Validation Loss: 226.90138244628906\n",
      "Epoch [229/1500], Training Loss: 120.95723460626412, Validation Loss: 224.3480987548828\n",
      "Epoch [230/1500], Training Loss: 118.6489538401354, Validation Loss: 221.7953338623047\n",
      "Epoch [231/1500], Training Loss: 116.38046789661963, Validation Loss: 219.2425537109375\n",
      "Epoch [232/1500], Training Loss: 114.15020487828804, Validation Loss: 216.6873321533203\n",
      "Epoch [233/1500], Training Loss: 111.95720587139317, Validation Loss: 214.12850952148438\n",
      "Epoch [234/1500], Training Loss: 109.8000658693624, Validation Loss: 211.5640411376953\n",
      "Epoch [235/1500], Training Loss: 107.67802535703044, Validation Loss: 208.9940643310547\n",
      "Epoch [236/1500], Training Loss: 105.59059125199803, Validation Loss: 206.41688537597656\n",
      "Epoch [237/1500], Training Loss: 103.537424258581, Validation Loss: 203.8317413330078\n",
      "Epoch [238/1500], Training Loss: 101.51644780311688, Validation Loss: 201.2356414794922\n",
      "Epoch [239/1500], Training Loss: 99.52569323117558, Validation Loss: 198.62620544433594\n",
      "Epoch [240/1500], Training Loss: 97.56571778009877, Validation Loss: 196.00213623046875\n",
      "Epoch [241/1500], Training Loss: 95.63608420859717, Validation Loss: 193.3621368408203\n",
      "Epoch [242/1500], Training Loss: 93.73514295154828, Validation Loss: 190.706298828125\n",
      "Epoch [243/1500], Training Loss: 91.86337766969662, Validation Loss: 188.0302734375\n",
      "Epoch [244/1500], Training Loss: 90.02063409519991, Validation Loss: 185.3345947265625\n",
      "Epoch [245/1500], Training Loss: 88.20583846765828, Validation Loss: 182.6173095703125\n",
      "Epoch [246/1500], Training Loss: 86.41801984174244, Validation Loss: 179.8783721923828\n",
      "Epoch [247/1500], Training Loss: 84.65794038058826, Validation Loss: 177.1167449951172\n",
      "Epoch [248/1500], Training Loss: 82.92472328130472, Validation Loss: 174.3313751220703\n",
      "Epoch [249/1500], Training Loss: 81.21773568841203, Validation Loss: 171.52374267578125\n",
      "Epoch [250/1500], Training Loss: 79.53619332142516, Validation Loss: 168.69325256347656\n",
      "Epoch [251/1500], Training Loss: 77.88143113270708, Validation Loss: 165.8430938720703\n",
      "Epoch [252/1500], Training Loss: 76.253792541569, Validation Loss: 162.97329711914062\n",
      "Epoch [253/1500], Training Loss: 74.65282787206587, Validation Loss: 160.08677673339844\n",
      "Epoch [254/1500], Training Loss: 73.08012005381147, Validation Loss: 157.18545532226562\n",
      "Epoch [255/1500], Training Loss: 71.53497156300536, Validation Loss: 154.2729034423828\n",
      "Epoch [256/1500], Training Loss: 70.01853754197218, Validation Loss: 151.35289001464844\n",
      "Epoch [257/1500], Training Loss: 68.5314397686863, Validation Loss: 148.42816162109375\n",
      "Epoch [258/1500], Training Loss: 67.07282356819161, Validation Loss: 145.50267028808594\n",
      "Epoch [259/1500], Training Loss: 65.6427616179578, Validation Loss: 142.58106994628906\n",
      "Epoch [260/1500], Training Loss: 64.24330537307061, Validation Loss: 139.67178344726562\n",
      "Epoch [261/1500], Training Loss: 62.87392641332196, Validation Loss: 136.7760772705078\n",
      "Epoch [262/1500], Training Loss: 61.534012051262515, Validation Loss: 133.9014892578125\n",
      "Epoch [263/1500], Training Loss: 60.223855796001736, Validation Loss: 131.05189514160156\n",
      "Epoch [264/1500], Training Loss: 58.944169311313395, Validation Loss: 128.2315673828125\n",
      "Epoch [265/1500], Training Loss: 57.693675785783576, Validation Loss: 125.44503784179688\n",
      "Epoch [266/1500], Training Loss: 56.47249122438095, Validation Loss: 122.69638061523438\n",
      "Epoch [267/1500], Training Loss: 55.28044390054296, Validation Loss: 119.98909759521484\n",
      "Epoch [268/1500], Training Loss: 54.116922938898874, Validation Loss: 117.32970428466797\n",
      "Epoch [269/1500], Training Loss: 52.98266181825524, Validation Loss: 114.72103881835938\n",
      "Epoch [270/1500], Training Loss: 51.87513411942109, Validation Loss: 112.1672592163086\n",
      "Epoch [271/1500], Training Loss: 50.79492152756681, Validation Loss: 109.67131805419922\n",
      "Epoch [272/1500], Training Loss: 49.741481985584926, Validation Loss: 107.23429107666016\n",
      "Epoch [273/1500], Training Loss: 48.71459906944202, Validation Loss: 104.85932922363281\n",
      "Epoch [274/1500], Training Loss: 47.713603269684825, Validation Loss: 102.54692077636719\n",
      "Epoch [275/1500], Training Loss: 46.73741521432807, Validation Loss: 100.29702758789062\n",
      "Epoch [276/1500], Training Loss: 45.78631196399037, Validation Loss: 98.11024475097656\n",
      "Epoch [277/1500], Training Loss: 44.85954813955896, Validation Loss: 95.98550415039062\n",
      "Epoch [278/1500], Training Loss: 43.95619096250158, Validation Loss: 93.92118835449219\n",
      "Epoch [279/1500], Training Loss: 43.075889400653146, Validation Loss: 91.91680145263672\n",
      "Epoch [280/1500], Training Loss: 42.21810687296834, Validation Loss: 89.97037506103516\n",
      "Epoch [281/1500], Training Loss: 41.38206392179611, Validation Loss: 88.08147430419922\n",
      "Epoch [282/1500], Training Loss: 40.56721031701962, Validation Loss: 86.2469253540039\n",
      "Epoch [283/1500], Training Loss: 39.77321119594286, Validation Loss: 84.46653747558594\n",
      "Epoch [284/1500], Training Loss: 38.99971415551535, Validation Loss: 82.73849487304688\n",
      "Epoch [285/1500], Training Loss: 38.24635453492467, Validation Loss: 81.06184387207031\n",
      "Epoch [286/1500], Training Loss: 37.511827817350806, Validation Loss: 79.43360900878906\n",
      "Epoch [287/1500], Training Loss: 36.79662788813681, Validation Loss: 77.8541259765625\n",
      "Epoch [288/1500], Training Loss: 36.099736701507766, Validation Loss: 76.3203125\n",
      "Epoch [289/1500], Training Loss: 35.42065282417301, Validation Loss: 74.83126831054688\n",
      "Epoch [290/1500], Training Loss: 34.75906524436923, Validation Loss: 73.38510131835938\n",
      "Epoch [291/1500], Training Loss: 34.11454007599423, Validation Loss: 71.9815673828125\n",
      "Epoch [292/1500], Training Loss: 33.4864287375347, Validation Loss: 70.6180419921875\n",
      "Epoch [293/1500], Training Loss: 32.87435128083797, Validation Loss: 69.29391479492188\n",
      "Epoch [294/1500], Training Loss: 32.277768925069026, Validation Loss: 68.008056640625\n",
      "Epoch [295/1500], Training Loss: 31.6961788681584, Validation Loss: 66.75936889648438\n",
      "Epoch [296/1500], Training Loss: 31.129252955731303, Validation Loss: 65.54624938964844\n",
      "Epoch [297/1500], Training Loss: 30.576619043122392, Validation Loss: 64.36869049072266\n",
      "Epoch [298/1500], Training Loss: 30.03813378235688, Validation Loss: 63.22502517700195\n",
      "Epoch [299/1500], Training Loss: 29.513552139481348, Validation Loss: 62.1146354675293\n",
      "Epoch [300/1500], Training Loss: 29.00208995816495, Validation Loss: 61.03611373901367\n",
      "Epoch [301/1500], Training Loss: 28.50308779006497, Validation Loss: 59.988441467285156\n",
      "Epoch [302/1500], Training Loss: 28.016448164508763, Validation Loss: 58.970767974853516\n",
      "Epoch [303/1500], Training Loss: 27.54156427096658, Validation Loss: 57.982078552246094\n",
      "Epoch [304/1500], Training Loss: 27.078542496511226, Validation Loss: 57.021663665771484\n",
      "Epoch [305/1500], Training Loss: 26.626944549178777, Validation Loss: 56.088619232177734\n",
      "Epoch [306/1500], Training Loss: 26.186335461265255, Validation Loss: 55.18202590942383\n",
      "Epoch [307/1500], Training Loss: 25.756673031446724, Validation Loss: 54.30136489868164\n",
      "Epoch [308/1500], Training Loss: 25.337605399528805, Validation Loss: 53.445369720458984\n",
      "Epoch [309/1500], Training Loss: 24.928937614086333, Validation Loss: 52.61341094970703\n",
      "Epoch [310/1500], Training Loss: 24.529734850686648, Validation Loss: 51.804420471191406\n",
      "Epoch [311/1500], Training Loss: 24.140197445034307, Validation Loss: 51.01789474487305\n",
      "Epoch [312/1500], Training Loss: 23.7599760387015, Validation Loss: 50.25343322753906\n",
      "Epoch [313/1500], Training Loss: 23.388831993082004, Validation Loss: 49.51006317138672\n",
      "Epoch [314/1500], Training Loss: 23.026551392579005, Validation Loss: 48.787071228027344\n",
      "Epoch [315/1500], Training Loss: 22.673056817713253, Validation Loss: 48.08445739746094\n",
      "Epoch [316/1500], Training Loss: 22.32791398393724, Validation Loss: 47.4011344909668\n",
      "Epoch [317/1500], Training Loss: 21.990919826572306, Validation Loss: 46.736671447753906\n",
      "Epoch [318/1500], Training Loss: 21.6618783629461, Validation Loss: 46.09025573730469\n",
      "Epoch [319/1500], Training Loss: 21.340578958975183, Validation Loss: 45.461666107177734\n",
      "Epoch [320/1500], Training Loss: 21.02668112144106, Validation Loss: 44.850440979003906\n",
      "Epoch [321/1500], Training Loss: 20.720495654498905, Validation Loss: 44.256072998046875\n",
      "Epoch [322/1500], Training Loss: 20.421502133902784, Validation Loss: 43.67817306518555\n",
      "Epoch [323/1500], Training Loss: 20.129571504338344, Validation Loss: 43.11623764038086\n",
      "Epoch [324/1500], Training Loss: 19.844396934590616, Validation Loss: 42.569419860839844\n",
      "Epoch [325/1500], Training Loss: 19.56559692224621, Validation Loss: 42.03704833984375\n",
      "Epoch [326/1500], Training Loss: 19.293006755959844, Validation Loss: 41.51902389526367\n",
      "Epoch [327/1500], Training Loss: 19.026754693886677, Validation Loss: 41.01485824584961\n",
      "Epoch [328/1500], Training Loss: 18.766517550873342, Validation Loss: 40.52404022216797\n",
      "Epoch [329/1500], Training Loss: 18.512266521170936, Validation Loss: 40.04629135131836\n",
      "Epoch [330/1500], Training Loss: 18.26377658104113, Validation Loss: 39.580772399902344\n",
      "Epoch [331/1500], Training Loss: 18.020624652245665, Validation Loss: 39.12716293334961\n",
      "Epoch [332/1500], Training Loss: 17.782948988637834, Validation Loss: 38.6850700378418\n",
      "Epoch [333/1500], Training Loss: 17.550440784703177, Validation Loss: 38.25389862060547\n",
      "Epoch [334/1500], Training Loss: 17.32289805980067, Validation Loss: 37.83314514160156\n",
      "Epoch [335/1500], Training Loss: 17.100261580269905, Validation Loss: 37.42253112792969\n",
      "Epoch [336/1500], Training Loss: 16.88220184659963, Validation Loss: 37.021575927734375\n",
      "Epoch [337/1500], Training Loss: 16.668828582763275, Validation Loss: 36.63011932373047\n",
      "Epoch [338/1500], Training Loss: 16.459964924002907, Validation Loss: 36.24769592285156\n",
      "Epoch [339/1500], Training Loss: 16.25551579481052, Validation Loss: 35.874061584472656\n",
      "Epoch [340/1500], Training Loss: 16.05525595943758, Validation Loss: 35.508602142333984\n",
      "Epoch [341/1500], Training Loss: 15.859068303765756, Validation Loss: 35.15138244628906\n",
      "Epoch [342/1500], Training Loss: 15.667084505712069, Validation Loss: 34.80210494995117\n",
      "Epoch [343/1500], Training Loss: 15.47887276702821, Validation Loss: 34.46022033691406\n",
      "Epoch [344/1500], Training Loss: 15.294600819410526, Validation Loss: 34.12555694580078\n",
      "Epoch [345/1500], Training Loss: 15.114066538090649, Validation Loss: 33.797977447509766\n",
      "Epoch [346/1500], Training Loss: 14.93697556282728, Validation Loss: 33.47709655761719\n",
      "Epoch [347/1500], Training Loss: 14.763396981062302, Validation Loss: 33.16285705566406\n",
      "Epoch [348/1500], Training Loss: 14.593177590911905, Validation Loss: 32.85499572753906\n",
      "Epoch [349/1500], Training Loss: 14.42636505744746, Validation Loss: 32.55351638793945\n",
      "Epoch [350/1500], Training Loss: 14.262822101200236, Validation Loss: 32.25798416137695\n",
      "Epoch [351/1500], Training Loss: 14.102231649470946, Validation Loss: 31.968116760253906\n",
      "Epoch [352/1500], Training Loss: 13.944563802300626, Validation Loss: 31.68389320373535\n",
      "Epoch [353/1500], Training Loss: 13.78989463644992, Validation Loss: 31.40518569946289\n",
      "Epoch [354/1500], Training Loss: 13.63826864662303, Validation Loss: 31.132104873657227\n",
      "Epoch [355/1500], Training Loss: 13.489460182267017, Validation Loss: 30.864482879638672\n",
      "Epoch [356/1500], Training Loss: 13.343429667090993, Validation Loss: 30.601844787597656\n",
      "Epoch [357/1500], Training Loss: 13.200188524556399, Validation Loss: 30.344552993774414\n",
      "Epoch [358/1500], Training Loss: 13.059525897834359, Validation Loss: 30.092071533203125\n",
      "Epoch [359/1500], Training Loss: 12.921484901242938, Validation Loss: 29.844585418701172\n",
      "Epoch [360/1500], Training Loss: 12.785932038973895, Validation Loss: 29.601778030395508\n",
      "Epoch [361/1500], Training Loss: 12.652795065057822, Validation Loss: 29.363540649414062\n",
      "Epoch [362/1500], Training Loss: 12.522036244207644, Validation Loss: 29.129962921142578\n",
      "Epoch [363/1500], Training Loss: 12.393544382195627, Validation Loss: 28.900758743286133\n",
      "Epoch [364/1500], Training Loss: 12.267279809175948, Validation Loss: 28.67580223083496\n",
      "Epoch [365/1500], Training Loss: 12.143166029764869, Validation Loss: 28.455047607421875\n",
      "Epoch [366/1500], Training Loss: 12.021141770902732, Validation Loss: 28.238584518432617\n",
      "Epoch [367/1500], Training Loss: 11.90122550887145, Validation Loss: 28.025968551635742\n",
      "Epoch [368/1500], Training Loss: 11.783378017276558, Validation Loss: 27.817481994628906\n",
      "Epoch [369/1500], Training Loss: 11.667592486743443, Validation Loss: 27.61305809020996\n",
      "Epoch [370/1500], Training Loss: 11.553797320129936, Validation Loss: 27.412378311157227\n",
      "Epoch [371/1500], Training Loss: 11.441939089663194, Validation Loss: 27.21553611755371\n",
      "Epoch [372/1500], Training Loss: 11.3320575056429, Validation Loss: 27.022308349609375\n",
      "Epoch [373/1500], Training Loss: 11.223974747844345, Validation Loss: 26.8327693939209\n",
      "Epoch [374/1500], Training Loss: 11.117701998563742, Validation Loss: 26.646682739257812\n",
      "Epoch [375/1500], Training Loss: 11.013250351458165, Validation Loss: 26.464122772216797\n",
      "Epoch [376/1500], Training Loss: 10.910501525601752, Validation Loss: 26.284900665283203\n",
      "Epoch [377/1500], Training Loss: 10.809446473422314, Validation Loss: 26.1088809967041\n",
      "Epoch [378/1500], Training Loss: 10.710169020154463, Validation Loss: 25.936098098754883\n",
      "Epoch [379/1500], Training Loss: 10.612460902240928, Validation Loss: 25.76658821105957\n",
      "Epoch [380/1500], Training Loss: 10.516318847401928, Validation Loss: 25.59998321533203\n",
      "Epoch [381/1500], Training Loss: 10.421711690009452, Validation Loss: 25.436309814453125\n",
      "Epoch [382/1500], Training Loss: 10.32861660750043, Validation Loss: 25.275575637817383\n",
      "Epoch [383/1500], Training Loss: 10.236948531087643, Validation Loss: 25.117612838745117\n",
      "Epoch [384/1500], Training Loss: 10.146670205716458, Validation Loss: 24.96232795715332\n",
      "Epoch [385/1500], Training Loss: 10.057815378950334, Validation Loss: 24.809757232666016\n",
      "Epoch [386/1500], Training Loss: 9.970390453112524, Validation Loss: 24.65981101989746\n",
      "Epoch [387/1500], Training Loss: 9.884289879665522, Validation Loss: 24.51241683959961\n",
      "Epoch [388/1500], Training Loss: 9.799531376095235, Validation Loss: 24.367496490478516\n",
      "Epoch [389/1500], Training Loss: 9.716049817741245, Validation Loss: 24.22490692138672\n",
      "Epoch [390/1500], Training Loss: 9.633844803990888, Validation Loss: 24.084732055664062\n",
      "Epoch [391/1500], Training Loss: 9.552938201519387, Validation Loss: 23.946863174438477\n",
      "Epoch [392/1500], Training Loss: 9.4732511855961, Validation Loss: 23.81114959716797\n",
      "Epoch [393/1500], Training Loss: 9.394710460026031, Validation Loss: 23.67770004272461\n",
      "Epoch [394/1500], Training Loss: 9.317395427206863, Validation Loss: 23.546411514282227\n",
      "Epoch [395/1500], Training Loss: 9.241238195333342, Validation Loss: 23.417198181152344\n",
      "Epoch [396/1500], Training Loss: 9.166313233064225, Validation Loss: 23.289976119995117\n",
      "Epoch [397/1500], Training Loss: 9.092451404957206, Validation Loss: 23.16482925415039\n",
      "Epoch [398/1500], Training Loss: 9.019742045253526, Validation Loss: 23.04153060913086\n",
      "Epoch [399/1500], Training Loss: 8.94805539325115, Validation Loss: 22.920103073120117\n",
      "Epoch [400/1500], Training Loss: 8.877413067082376, Validation Loss: 22.80064582824707\n",
      "Epoch [401/1500], Training Loss: 8.807864890391691, Validation Loss: 22.68290138244629\n",
      "Epoch [402/1500], Training Loss: 8.73933532711054, Validation Loss: 22.567007064819336\n",
      "Epoch [403/1500], Training Loss: 8.671834174437894, Validation Loss: 22.45281982421875\n",
      "Epoch [404/1500], Training Loss: 8.605317500686928, Validation Loss: 22.340242385864258\n",
      "Epoch [405/1500], Training Loss: 8.539729851550527, Validation Loss: 22.22919464111328\n",
      "Epoch [406/1500], Training Loss: 8.474877236571471, Validation Loss: 22.119579315185547\n",
      "Epoch [407/1500], Training Loss: 8.410878331463927, Validation Loss: 22.01155662536621\n",
      "Epoch [408/1500], Training Loss: 8.34786366200935, Validation Loss: 21.9051513671875\n",
      "Epoch [409/1500], Training Loss: 8.285799707683973, Validation Loss: 21.800243377685547\n",
      "Epoch [410/1500], Training Loss: 8.224501952943285, Validation Loss: 21.696773529052734\n",
      "Epoch [411/1500], Training Loss: 8.164129356192484, Validation Loss: 21.59476089477539\n",
      "Epoch [412/1500], Training Loss: 8.104598474506997, Validation Loss: 21.49418067932129\n",
      "Epoch [413/1500], Training Loss: 8.045966198274753, Validation Loss: 21.395187377929688\n",
      "Epoch [414/1500], Training Loss: 7.988224086631837, Validation Loss: 21.297510147094727\n",
      "Epoch [415/1500], Training Loss: 7.931358512478868, Validation Loss: 21.2012882232666\n",
      "Epoch [416/1500], Training Loss: 7.875295683234801, Validation Loss: 21.106542587280273\n",
      "Epoch [417/1500], Training Loss: 7.819963932917042, Validation Loss: 21.013025283813477\n",
      "Epoch [418/1500], Training Loss: 7.765396273013523, Validation Loss: 20.920738220214844\n",
      "Epoch [419/1500], Training Loss: 7.711602528010888, Validation Loss: 20.82976531982422\n",
      "Epoch [420/1500], Training Loss: 7.658561264744556, Validation Loss: 20.740018844604492\n",
      "Epoch [421/1500], Training Loss: 7.606201242387033, Validation Loss: 20.651519775390625\n",
      "Epoch [422/1500], Training Loss: 7.5545378094439775, Validation Loss: 20.564151763916016\n",
      "Epoch [423/1500], Training Loss: 7.503601650119647, Validation Loss: 20.4780330657959\n",
      "Epoch [424/1500], Training Loss: 7.453344569169571, Validation Loss: 20.392993927001953\n",
      "Epoch [425/1500], Training Loss: 7.403758306290361, Validation Loss: 20.309160232543945\n",
      "Epoch [426/1500], Training Loss: 7.3547900365216154, Validation Loss: 20.22641944885254\n",
      "Epoch [427/1500], Training Loss: 7.3064703256722465, Validation Loss: 20.144819259643555\n",
      "Epoch [428/1500], Training Loss: 7.258762460677508, Validation Loss: 20.064266204833984\n",
      "Epoch [429/1500], Training Loss: 7.211604060383344, Validation Loss: 19.984649658203125\n",
      "Epoch [430/1500], Training Loss: 7.165139179711058, Validation Loss: 19.906158447265625\n",
      "Epoch [431/1500], Training Loss: 7.119260800368699, Validation Loss: 19.828659057617188\n",
      "Epoch [432/1500], Training Loss: 7.074099270088469, Validation Loss: 19.752241134643555\n",
      "Epoch [433/1500], Training Loss: 7.029439817408895, Validation Loss: 19.676712036132812\n",
      "Epoch [434/1500], Training Loss: 6.985301508277902, Validation Loss: 19.602081298828125\n",
      "Epoch [435/1500], Training Loss: 6.941787704934824, Validation Loss: 19.528396606445312\n",
      "Epoch [436/1500], Training Loss: 6.898780157524642, Validation Loss: 19.455562591552734\n",
      "Epoch [437/1500], Training Loss: 6.856267560638734, Validation Loss: 19.383586883544922\n",
      "Epoch [438/1500], Training Loss: 6.814322942133838, Validation Loss: 19.312456130981445\n",
      "Epoch [439/1500], Training Loss: 6.772971288500965, Validation Loss: 19.24231719970703\n",
      "Epoch [440/1500], Training Loss: 6.7321469533412115, Validation Loss: 19.172876358032227\n",
      "Epoch [441/1500], Training Loss: 6.691801543918523, Validation Loss: 19.10419273376465\n",
      "Epoch [442/1500], Training Loss: 6.65189972800694, Validation Loss: 19.036174774169922\n",
      "Epoch [443/1500], Training Loss: 6.612460672388529, Validation Loss: 18.96908187866211\n",
      "Epoch [444/1500], Training Loss: 6.573506770480785, Validation Loss: 18.902576446533203\n",
      "Epoch [445/1500], Training Loss: 6.535031643646933, Validation Loss: 18.836841583251953\n",
      "Epoch [446/1500], Training Loss: 6.497062350435888, Validation Loss: 18.771780014038086\n",
      "Epoch [447/1500], Training Loss: 6.459528159971315, Validation Loss: 18.707216262817383\n",
      "Epoch [448/1500], Training Loss: 6.422432335526623, Validation Loss: 18.64332389831543\n",
      "Epoch [449/1500], Training Loss: 6.38578144953611, Validation Loss: 18.580007553100586\n",
      "Epoch [450/1500], Training Loss: 6.349501610660112, Validation Loss: 18.517078399658203\n",
      "Epoch [451/1500], Training Loss: 6.313682996079448, Validation Loss: 18.45478057861328\n",
      "Epoch [452/1500], Training Loss: 6.278286772812014, Validation Loss: 18.393033981323242\n",
      "Epoch [453/1500], Training Loss: 6.243346161934497, Validation Loss: 18.331735610961914\n",
      "Epoch [454/1500], Training Loss: 6.20871321850146, Validation Loss: 18.27083396911621\n",
      "Epoch [455/1500], Training Loss: 6.174461789324627, Validation Loss: 18.210254669189453\n",
      "Epoch [456/1500], Training Loss: 6.140569265596383, Validation Loss: 18.150043487548828\n",
      "Epoch [457/1500], Training Loss: 6.107066407829354, Validation Loss: 18.09027671813965\n",
      "Epoch [458/1500], Training Loss: 6.073914530843734, Validation Loss: 18.0307559967041\n",
      "Epoch [459/1500], Training Loss: 6.0411283066824835, Validation Loss: 17.971527099609375\n",
      "Epoch [460/1500], Training Loss: 6.008687733428382, Validation Loss: 17.912586212158203\n",
      "Epoch [461/1500], Training Loss: 5.9766350575687355, Validation Loss: 17.85384750366211\n",
      "Epoch [462/1500], Training Loss: 5.944891001149026, Validation Loss: 17.79526138305664\n",
      "Epoch [463/1500], Training Loss: 5.913400895490465, Validation Loss: 17.736661911010742\n",
      "Epoch [464/1500], Training Loss: 5.882264878955703, Validation Loss: 17.67836570739746\n",
      "Epoch [465/1500], Training Loss: 5.851495742648622, Validation Loss: 17.620031356811523\n",
      "Epoch [466/1500], Training Loss: 5.8209588945122235, Validation Loss: 17.561756134033203\n",
      "Epoch [467/1500], Training Loss: 5.79066744010638, Validation Loss: 17.50350570678711\n",
      "Epoch [468/1500], Training Loss: 5.760704227427601, Validation Loss: 17.44512367248535\n",
      "Epoch [469/1500], Training Loss: 5.731039820206796, Validation Loss: 17.38679313659668\n",
      "Epoch [470/1500], Training Loss: 5.701645091844961, Validation Loss: 17.328269958496094\n",
      "Epoch [471/1500], Training Loss: 5.672513168872587, Validation Loss: 17.269493103027344\n",
      "Epoch [472/1500], Training Loss: 5.643605103611713, Validation Loss: 17.21050453186035\n",
      "Epoch [473/1500], Training Loss: 5.614906395659268, Validation Loss: 17.151208877563477\n",
      "Epoch [474/1500], Training Loss: 5.586402725468735, Validation Loss: 17.091665267944336\n",
      "Epoch [475/1500], Training Loss: 5.558179332380866, Validation Loss: 17.031700134277344\n",
      "Epoch [476/1500], Training Loss: 5.5302040523133495, Validation Loss: 16.971513748168945\n",
      "Epoch [477/1500], Training Loss: 5.502451365144129, Validation Loss: 16.910982131958008\n",
      "Epoch [478/1500], Training Loss: 5.4749055233697534, Validation Loss: 16.8498477935791\n",
      "Epoch [479/1500], Training Loss: 5.447603074730816, Validation Loss: 16.78829002380371\n",
      "Epoch [480/1500], Training Loss: 5.420457222929663, Validation Loss: 16.726369857788086\n",
      "Epoch [481/1500], Training Loss: 5.393529003494713, Validation Loss: 16.663846969604492\n",
      "Epoch [482/1500], Training Loss: 5.366846992810535, Validation Loss: 16.600780487060547\n",
      "Epoch [483/1500], Training Loss: 5.340336952037095, Validation Loss: 16.53711700439453\n",
      "Epoch [484/1500], Training Loss: 5.314036234559732, Validation Loss: 16.472843170166016\n",
      "Epoch [485/1500], Training Loss: 5.287901300742433, Validation Loss: 16.40786361694336\n",
      "Epoch [486/1500], Training Loss: 5.261844329680552, Validation Loss: 16.342266082763672\n",
      "Epoch [487/1500], Training Loss: 5.2359766392951395, Validation Loss: 16.27593231201172\n",
      "Epoch [488/1500], Training Loss: 5.210324089838414, Validation Loss: 16.209096908569336\n",
      "Epoch [489/1500], Training Loss: 5.1848578142153015, Validation Loss: 16.14150047302246\n",
      "Epoch [490/1500], Training Loss: 5.159549944697304, Validation Loss: 16.073211669921875\n",
      "Epoch [491/1500], Training Loss: 5.134456974730763, Validation Loss: 16.004350662231445\n",
      "Epoch [492/1500], Training Loss: 5.109495407423608, Validation Loss: 15.934825897216797\n",
      "Epoch [493/1500], Training Loss: 5.084710316296245, Validation Loss: 15.86479663848877\n",
      "Epoch [494/1500], Training Loss: 5.060080587130672, Validation Loss: 15.794191360473633\n",
      "Epoch [495/1500], Training Loss: 5.03564863394407, Validation Loss: 15.7230224609375\n",
      "Epoch [496/1500], Training Loss: 5.011337045323339, Validation Loss: 15.651410102844238\n",
      "Epoch [497/1500], Training Loss: 4.987164160189822, Validation Loss: 15.579183578491211\n",
      "Epoch [498/1500], Training Loss: 4.963179027128146, Validation Loss: 15.506726264953613\n",
      "Epoch [499/1500], Training Loss: 4.939385829977808, Validation Loss: 15.43386459350586\n",
      "Epoch [500/1500], Training Loss: 4.915834190000064, Validation Loss: 15.36093807220459\n",
      "Epoch [501/1500], Training Loss: 4.892484027602022, Validation Loss: 15.287816047668457\n",
      "Epoch [502/1500], Training Loss: 4.8693843318146515, Validation Loss: 15.214698791503906\n",
      "Epoch [503/1500], Training Loss: 4.846461683744795, Validation Loss: 15.141602516174316\n",
      "Epoch [504/1500], Training Loss: 4.8237912116148625, Validation Loss: 15.068577766418457\n",
      "Epoch [505/1500], Training Loss: 4.801313431844622, Validation Loss: 14.995798110961914\n",
      "Epoch [506/1500], Training Loss: 4.779141136975964, Validation Loss: 14.923544883728027\n",
      "Epoch [507/1500], Training Loss: 4.757263365789848, Validation Loss: 14.851704597473145\n",
      "Epoch [508/1500], Training Loss: 4.735756088838469, Validation Loss: 14.780549049377441\n",
      "Epoch [509/1500], Training Loss: 4.714543316441954, Validation Loss: 14.709890365600586\n",
      "Epoch [510/1500], Training Loss: 4.693630822607032, Validation Loss: 14.639971733093262\n",
      "Epoch [511/1500], Training Loss: 4.673053052687312, Validation Loss: 14.570712089538574\n",
      "Epoch [512/1500], Training Loss: 4.652783735800227, Validation Loss: 14.502150535583496\n",
      "Epoch [513/1500], Training Loss: 4.632832153050702, Validation Loss: 14.434735298156738\n",
      "Epoch [514/1500], Training Loss: 4.613300882662374, Validation Loss: 14.367789268493652\n",
      "Epoch [515/1500], Training Loss: 4.594112457532571, Validation Loss: 14.301985740661621\n",
      "Epoch [516/1500], Training Loss: 4.57522777775282, Validation Loss: 14.236978530883789\n",
      "Epoch [517/1500], Training Loss: 4.556660479106129, Validation Loss: 14.173025131225586\n",
      "Epoch [518/1500], Training Loss: 4.538351432511918, Validation Loss: 14.109992980957031\n",
      "Epoch [519/1500], Training Loss: 4.520376899367445, Validation Loss: 14.047584533691406\n",
      "Epoch [520/1500], Training Loss: 4.50263130468532, Validation Loss: 13.986394882202148\n",
      "Epoch [521/1500], Training Loss: 4.485143311023495, Validation Loss: 13.926011085510254\n",
      "Epoch [522/1500], Training Loss: 4.467866093471687, Validation Loss: 13.86668872833252\n",
      "Epoch [523/1500], Training Loss: 4.450831757643189, Validation Loss: 13.808432579040527\n",
      "Epoch [524/1500], Training Loss: 4.43403040665418, Validation Loss: 13.751320838928223\n",
      "Epoch [525/1500], Training Loss: 4.417437653639065, Validation Loss: 13.695277214050293\n",
      "Epoch [526/1500], Training Loss: 4.401063238134193, Validation Loss: 13.640558242797852\n",
      "Epoch [527/1500], Training Loss: 4.384908242148729, Validation Loss: 13.586840629577637\n",
      "Epoch [528/1500], Training Loss: 4.368910158960454, Validation Loss: 13.534148216247559\n",
      "Epoch [529/1500], Training Loss: 4.353072067505419, Validation Loss: 13.482583045959473\n",
      "Epoch [530/1500], Training Loss: 4.337381158219066, Validation Loss: 13.431822776794434\n",
      "Epoch [531/1500], Training Loss: 4.321833466285105, Validation Loss: 13.38187026977539\n",
      "Epoch [532/1500], Training Loss: 4.306456859558087, Validation Loss: 13.33284854888916\n",
      "Epoch [533/1500], Training Loss: 4.291220527320734, Validation Loss: 13.284383773803711\n",
      "Epoch [534/1500], Training Loss: 4.2761032919477495, Validation Loss: 13.236831665039062\n",
      "Epoch [535/1500], Training Loss: 4.261156671714018, Validation Loss: 13.189885139465332\n",
      "Epoch [536/1500], Training Loss: 4.24637363778254, Validation Loss: 13.143509864807129\n",
      "Epoch [537/1500], Training Loss: 4.231754555331247, Validation Loss: 13.097789764404297\n",
      "Epoch [538/1500], Training Loss: 4.217301929908898, Validation Loss: 13.052642822265625\n",
      "Epoch [539/1500], Training Loss: 4.202988802331754, Validation Loss: 13.007967948913574\n",
      "Epoch [540/1500], Training Loss: 4.188806480120064, Validation Loss: 12.963912963867188\n",
      "Epoch [541/1500], Training Loss: 4.174698339653372, Validation Loss: 12.92023754119873\n",
      "Epoch [542/1500], Training Loss: 4.160743928080216, Validation Loss: 12.876900672912598\n",
      "Epoch [543/1500], Training Loss: 4.14689532785823, Validation Loss: 12.834020614624023\n",
      "Epoch [544/1500], Training Loss: 4.133168481403193, Validation Loss: 12.791464805603027\n",
      "Epoch [545/1500], Training Loss: 4.119554201351669, Validation Loss: 12.74931812286377\n",
      "Epoch [546/1500], Training Loss: 4.1061012328976005, Validation Loss: 12.707430839538574\n",
      "Epoch [547/1500], Training Loss: 4.092740063775201, Validation Loss: 12.665765762329102\n",
      "Epoch [548/1500], Training Loss: 4.079531489633322, Validation Loss: 12.624481201171875\n",
      "Epoch [549/1500], Training Loss: 4.066474912467474, Validation Loss: 12.583479881286621\n",
      "Epoch [550/1500], Training Loss: 4.0535672023239515, Validation Loss: 12.542926788330078\n",
      "Epoch [551/1500], Training Loss: 4.040770313086679, Validation Loss: 12.502575874328613\n",
      "Epoch [552/1500], Training Loss: 4.02812802465873, Validation Loss: 12.462567329406738\n",
      "Epoch [553/1500], Training Loss: 4.01562464959457, Validation Loss: 12.422798156738281\n",
      "Epoch [554/1500], Training Loss: 4.003240196665284, Validation Loss: 12.3832368850708\n",
      "Epoch [555/1500], Training Loss: 3.990991779846207, Validation Loss: 12.343894958496094\n",
      "Epoch [556/1500], Training Loss: 3.978849461968551, Validation Loss: 12.304965019226074\n",
      "Epoch [557/1500], Training Loss: 3.9668398281541877, Validation Loss: 12.266173362731934\n",
      "Epoch [558/1500], Training Loss: 3.9549262907749574, Validation Loss: 12.227617263793945\n",
      "Epoch [559/1500], Training Loss: 3.943126553348722, Validation Loss: 12.189245223999023\n",
      "Epoch [560/1500], Training Loss: 3.9314252537225327, Validation Loss: 12.151046752929688\n",
      "Epoch [561/1500], Training Loss: 3.9198035725777287, Validation Loss: 12.113107681274414\n",
      "Epoch [562/1500], Training Loss: 3.9082774012740322, Validation Loss: 12.07539176940918\n",
      "Epoch [563/1500], Training Loss: 3.896885251320755, Validation Loss: 12.037869453430176\n",
      "Epoch [564/1500], Training Loss: 3.885570282308974, Validation Loss: 12.000432968139648\n",
      "Epoch [565/1500], Training Loss: 3.8743149304650926, Validation Loss: 11.963163375854492\n",
      "Epoch [566/1500], Training Loss: 3.8631544713104145, Validation Loss: 11.926153182983398\n",
      "Epoch [567/1500], Training Loss: 3.8520821894139683, Validation Loss: 11.889410972595215\n",
      "Epoch [568/1500], Training Loss: 3.841133251253005, Validation Loss: 11.852753639221191\n",
      "Epoch [569/1500], Training Loss: 3.8302722828908076, Validation Loss: 11.816319465637207\n",
      "Epoch [570/1500], Training Loss: 3.8194810427580164, Validation Loss: 11.780158996582031\n",
      "Epoch [571/1500], Training Loss: 3.808794458725019, Validation Loss: 11.744206428527832\n",
      "Epoch [572/1500], Training Loss: 3.798222222110296, Validation Loss: 11.708395957946777\n",
      "Epoch [573/1500], Training Loss: 3.7877456453351135, Validation Loss: 11.672842025756836\n",
      "Epoch [574/1500], Training Loss: 3.777366356250997, Validation Loss: 11.637510299682617\n",
      "Epoch [575/1500], Training Loss: 3.7670435905316024, Validation Loss: 11.60232925415039\n",
      "Epoch [576/1500], Training Loss: 3.7568133157663053, Validation Loss: 11.56735610961914\n",
      "Epoch [577/1500], Training Loss: 3.7466647006450806, Validation Loss: 11.53260612487793\n",
      "Epoch [578/1500], Training Loss: 3.7365868891482834, Validation Loss: 11.498087882995605\n",
      "Epoch [579/1500], Training Loss: 3.726585209068535, Validation Loss: 11.463715553283691\n",
      "Epoch [580/1500], Training Loss: 3.7166798916640187, Validation Loss: 11.429527282714844\n",
      "Epoch [581/1500], Training Loss: 3.7068577801321996, Validation Loss: 11.395605087280273\n",
      "Epoch [582/1500], Training Loss: 3.697105021076125, Validation Loss: 11.361903190612793\n",
      "Epoch [583/1500], Training Loss: 3.6874310701630693, Validation Loss: 11.328420639038086\n",
      "Epoch [584/1500], Training Loss: 3.677839090071281, Validation Loss: 11.295151710510254\n",
      "Epoch [585/1500], Training Loss: 3.6683045835715204, Validation Loss: 11.262039184570312\n",
      "Epoch [586/1500], Training Loss: 3.6588531800846744, Validation Loss: 11.229227066040039\n",
      "Epoch [587/1500], Training Loss: 3.6494843885337214, Validation Loss: 11.196630477905273\n",
      "Epoch [588/1500], Training Loss: 3.64019561034526, Validation Loss: 11.164266586303711\n",
      "Epoch [589/1500], Training Loss: 3.6309925861843837, Validation Loss: 11.132147789001465\n",
      "Epoch [590/1500], Training Loss: 3.6218310325950953, Validation Loss: 11.100229263305664\n",
      "Epoch [591/1500], Training Loss: 3.612742312844987, Validation Loss: 11.06846809387207\n",
      "Epoch [592/1500], Training Loss: 3.6037243242435877, Validation Loss: 11.036909103393555\n",
      "Epoch [593/1500], Training Loss: 3.594754920725412, Validation Loss: 11.005584716796875\n",
      "Epoch [594/1500], Training Loss: 3.585842620354976, Validation Loss: 10.974491119384766\n",
      "Epoch [595/1500], Training Loss: 3.577010400829465, Validation Loss: 10.943634033203125\n",
      "Epoch [596/1500], Training Loss: 3.568253016157821, Validation Loss: 10.91297435760498\n",
      "Epoch [597/1500], Training Loss: 3.559553750699304, Validation Loss: 10.88264274597168\n",
      "Epoch [598/1500], Training Loss: 3.5509570782045747, Validation Loss: 10.852577209472656\n",
      "Epoch [599/1500], Training Loss: 3.5424330663942523, Validation Loss: 10.82275676727295\n",
      "Epoch [600/1500], Training Loss: 3.5339844235888402, Validation Loss: 10.793179512023926\n",
      "Epoch [601/1500], Training Loss: 3.5256103200287776, Validation Loss: 10.763896942138672\n",
      "Epoch [602/1500], Training Loss: 3.5172983037111054, Validation Loss: 10.734848022460938\n",
      "Epoch [603/1500], Training Loss: 3.5090519306404473, Validation Loss: 10.706001281738281\n",
      "Epoch [604/1500], Training Loss: 3.500858142589247, Validation Loss: 10.677512168884277\n",
      "Epoch [605/1500], Training Loss: 3.4927357114651, Validation Loss: 10.649261474609375\n",
      "Epoch [606/1500], Training Loss: 3.4846651366393577, Validation Loss: 10.621185302734375\n",
      "Epoch [607/1500], Training Loss: 3.4766685772663566, Validation Loss: 10.59338665008545\n",
      "Epoch [608/1500], Training Loss: 3.4687368546175477, Validation Loss: 10.565949440002441\n",
      "Epoch [609/1500], Training Loss: 3.460865246187948, Validation Loss: 10.538697242736816\n",
      "Epoch [610/1500], Training Loss: 3.4530406417707673, Validation Loss: 10.511787414550781\n",
      "Epoch [611/1500], Training Loss: 3.4452913738783555, Validation Loss: 10.485071182250977\n",
      "Epoch [612/1500], Training Loss: 3.437589862366579, Validation Loss: 10.458695411682129\n",
      "Epoch [613/1500], Training Loss: 3.4299509606122505, Validation Loss: 10.43252182006836\n",
      "Epoch [614/1500], Training Loss: 3.4223808993643248, Validation Loss: 10.406662940979004\n",
      "Epoch [615/1500], Training Loss: 3.4148655118101683, Validation Loss: 10.381081581115723\n",
      "Epoch [616/1500], Training Loss: 3.4074062568810235, Validation Loss: 10.355782508850098\n",
      "Epoch [617/1500], Training Loss: 3.400021690501589, Validation Loss: 10.330803871154785\n",
      "Epoch [618/1500], Training Loss: 3.3926773060362816, Validation Loss: 10.3060884475708\n",
      "Epoch [619/1500], Training Loss: 3.3853865000815846, Validation Loss: 10.281700134277344\n",
      "Epoch [620/1500], Training Loss: 3.378150579261945, Validation Loss: 10.257556915283203\n",
      "Epoch [621/1500], Training Loss: 3.370966593621284, Validation Loss: 10.233702659606934\n",
      "Epoch [622/1500], Training Loss: 3.363838382131814, Validation Loss: 10.210152626037598\n",
      "Epoch [623/1500], Training Loss: 3.356781376165528, Validation Loss: 10.186885833740234\n",
      "Epoch [624/1500], Training Loss: 3.3497881615648417, Validation Loss: 10.163888931274414\n",
      "Epoch [625/1500], Training Loss: 3.3428555735186816, Validation Loss: 10.141212463378906\n",
      "Epoch [626/1500], Training Loss: 3.3359686880287676, Validation Loss: 10.118851661682129\n",
      "Epoch [627/1500], Training Loss: 3.329143134838723, Validation Loss: 10.096726417541504\n",
      "Epoch [628/1500], Training Loss: 3.3223637440979235, Validation Loss: 10.074995040893555\n",
      "Epoch [629/1500], Training Loss: 3.3156510384271662, Validation Loss: 10.053576469421387\n",
      "Epoch [630/1500], Training Loss: 3.3090113144536084, Validation Loss: 10.032413482666016\n",
      "Epoch [631/1500], Training Loss: 3.302437815396562, Validation Loss: 10.011571884155273\n",
      "Epoch [632/1500], Training Loss: 3.2959324337567115, Validation Loss: 9.991034507751465\n",
      "Epoch [633/1500], Training Loss: 3.2894697518715845, Validation Loss: 9.970845222473145\n",
      "Epoch [634/1500], Training Loss: 3.283050114086176, Validation Loss: 9.950925827026367\n",
      "Epoch [635/1500], Training Loss: 3.2766858592802808, Validation Loss: 9.931356430053711\n",
      "Epoch [636/1500], Training Loss: 3.2703917440567856, Validation Loss: 9.912117958068848\n",
      "Epoch [637/1500], Training Loss: 3.26414269039803, Validation Loss: 9.893113136291504\n",
      "Epoch [638/1500], Training Loss: 3.257938364098869, Validation Loss: 9.8744535446167\n",
      "Epoch [639/1500], Training Loss: 3.2518232019933144, Validation Loss: 9.856106758117676\n",
      "Epoch [640/1500], Training Loss: 3.2457760238815805, Validation Loss: 9.838150024414062\n",
      "Epoch [641/1500], Training Loss: 3.2397721778665614, Validation Loss: 9.820536613464355\n",
      "Epoch [642/1500], Training Loss: 3.233811118866634, Validation Loss: 9.803215026855469\n",
      "Epoch [643/1500], Training Loss: 3.2279073423387734, Validation Loss: 9.786134719848633\n",
      "Epoch [644/1500], Training Loss: 3.222056085067084, Validation Loss: 9.769415855407715\n",
      "Epoch [645/1500], Training Loss: 3.2162352462050112, Validation Loss: 9.75296688079834\n",
      "Epoch [646/1500], Training Loss: 3.2104801759189225, Validation Loss: 9.73682689666748\n",
      "Epoch [647/1500], Training Loss: 3.204761952922542, Validation Loss: 9.720998764038086\n",
      "Epoch [648/1500], Training Loss: 3.199097596792611, Validation Loss: 9.705474853515625\n",
      "Epoch [649/1500], Training Loss: 3.193494487758889, Validation Loss: 9.690361022949219\n",
      "Epoch [650/1500], Training Loss: 3.1879340778263514, Validation Loss: 9.675518989562988\n",
      "Epoch [651/1500], Training Loss: 3.182431544472181, Validation Loss: 9.66092586517334\n",
      "Epoch [652/1500], Training Loss: 3.1769770068353784, Validation Loss: 9.646682739257812\n",
      "Epoch [653/1500], Training Loss: 3.1715590419774493, Validation Loss: 9.632734298706055\n",
      "Epoch [654/1500], Training Loss: 3.1661859458622392, Validation Loss: 9.61911392211914\n",
      "Epoch [655/1500], Training Loss: 3.1608799979078483, Validation Loss: 9.605805397033691\n",
      "Epoch [656/1500], Training Loss: 3.1556129030176225, Validation Loss: 9.592740058898926\n",
      "Epoch [657/1500], Training Loss: 3.1503924727010304, Validation Loss: 9.5800199508667\n",
      "Epoch [658/1500], Training Loss: 3.145233946464101, Validation Loss: 9.56761646270752\n",
      "Epoch [659/1500], Training Loss: 3.1401424105224938, Validation Loss: 9.555551528930664\n",
      "Epoch [660/1500], Training Loss: 3.1350945680887423, Validation Loss: 9.543800354003906\n",
      "Epoch [661/1500], Training Loss: 3.1300985127981082, Validation Loss: 9.532343864440918\n",
      "Epoch [662/1500], Training Loss: 3.125136441816689, Validation Loss: 9.521195411682129\n",
      "Epoch [663/1500], Training Loss: 3.1202200311804913, Validation Loss: 9.510403633117676\n",
      "Epoch [664/1500], Training Loss: 3.1153578695815956, Validation Loss: 9.499844551086426\n",
      "Epoch [665/1500], Training Loss: 3.1105296839663192, Validation Loss: 9.4895658493042\n",
      "Epoch [666/1500], Training Loss: 3.1057355525171926, Validation Loss: 9.47960376739502\n",
      "Epoch [667/1500], Training Loss: 3.100992188798119, Validation Loss: 9.469951629638672\n",
      "Epoch [668/1500], Training Loss: 3.0962778092309113, Validation Loss: 9.46064567565918\n",
      "Epoch [669/1500], Training Loss: 3.0916132398408247, Validation Loss: 9.45158576965332\n",
      "Epoch [670/1500], Training Loss: 3.0869888620484063, Validation Loss: 9.442805290222168\n",
      "Epoch [671/1500], Training Loss: 3.0824161916831896, Validation Loss: 9.434406280517578\n",
      "Epoch [672/1500], Training Loss: 3.0778819657270566, Validation Loss: 9.42626667022705\n",
      "Epoch [673/1500], Training Loss: 3.0733826856080073, Validation Loss: 9.418413162231445\n",
      "Epoch [674/1500], Training Loss: 3.0689147676077666, Validation Loss: 9.410821914672852\n",
      "Epoch [675/1500], Training Loss: 3.064459156477073, Validation Loss: 9.403520584106445\n",
      "Epoch [676/1500], Training Loss: 3.0600534150225416, Validation Loss: 9.396537780761719\n",
      "Epoch [677/1500], Training Loss: 3.05568277614329, Validation Loss: 9.389857292175293\n",
      "Epoch [678/1500], Training Loss: 3.051348842158426, Validation Loss: 9.383438110351562\n",
      "Epoch [679/1500], Training Loss: 3.0470561784482606, Validation Loss: 9.377276420593262\n",
      "Epoch [680/1500], Training Loss: 3.042793191619231, Validation Loss: 9.37144660949707\n",
      "Epoch [681/1500], Training Loss: 3.038570449218592, Validation Loss: 9.365915298461914\n",
      "Epoch [682/1500], Training Loss: 3.034396039728977, Validation Loss: 9.360651969909668\n",
      "Epoch [683/1500], Training Loss: 3.0302392214340865, Validation Loss: 9.355709075927734\n",
      "Epoch [684/1500], Training Loss: 3.0261237514429324, Validation Loss: 9.351070404052734\n",
      "Epoch [685/1500], Training Loss: 3.022039748387368, Validation Loss: 9.346711158752441\n",
      "Epoch [686/1500], Training Loss: 3.0179962982078554, Validation Loss: 9.342663764953613\n",
      "Epoch [687/1500], Training Loss: 3.0140134022364853, Validation Loss: 9.338844299316406\n",
      "Epoch [688/1500], Training Loss: 3.010069596698986, Validation Loss: 9.335402488708496\n",
      "Epoch [689/1500], Training Loss: 3.0061555275242338, Validation Loss: 9.332169532775879\n",
      "Epoch [690/1500], Training Loss: 3.0022769014409207, Validation Loss: 9.329323768615723\n",
      "Epoch [691/1500], Training Loss: 2.9984232480199364, Validation Loss: 9.326753616333008\n",
      "Epoch [692/1500], Training Loss: 2.994604107754754, Validation Loss: 9.324355125427246\n",
      "Epoch [693/1500], Training Loss: 2.9908167906180676, Validation Loss: 9.322310447692871\n",
      "Epoch [694/1500], Training Loss: 2.987064111015592, Validation Loss: 9.320494651794434\n",
      "Epoch [695/1500], Training Loss: 2.9833517415772555, Validation Loss: 9.319003105163574\n",
      "Epoch [696/1500], Training Loss: 2.9796641680288167, Validation Loss: 9.317652702331543\n",
      "Epoch [697/1500], Training Loss: 2.976012912770977, Validation Loss: 9.316580772399902\n",
      "Epoch [698/1500], Training Loss: 2.9723929681738195, Validation Loss: 9.315722465515137\n",
      "Epoch [699/1500], Training Loss: 2.968802111079341, Validation Loss: 9.315075874328613\n",
      "Epoch [700/1500], Training Loss: 2.965237527722333, Validation Loss: 9.314613342285156\n",
      "Epoch [701/1500], Training Loss: 2.96169120093709, Validation Loss: 9.314346313476562\n",
      "Epoch [702/1500], Training Loss: 2.9581786159427073, Validation Loss: 9.314218521118164\n",
      "Epoch [703/1500], Training Loss: 2.9546773407556897, Validation Loss: 9.314230918884277\n",
      "Epoch [704/1500], Training Loss: 2.9512102642154363, Validation Loss: 9.314360618591309\n",
      "Epoch [705/1500], Training Loss: 2.9477690639480785, Validation Loss: 9.314618110656738\n",
      "Epoch [706/1500], Training Loss: 2.9443505437076296, Validation Loss: 9.314984321594238\n",
      "Epoch [707/1500], Training Loss: 2.940944958422275, Validation Loss: 9.31534194946289\n",
      "Epoch [708/1500], Training Loss: 2.93754224823028, Validation Loss: 9.315743446350098\n",
      "Epoch [709/1500], Training Loss: 2.9341694230635684, Validation Loss: 9.316118240356445\n",
      "Epoch [710/1500], Training Loss: 2.930823340663893, Validation Loss: 9.316561698913574\n",
      "Epoch [711/1500], Training Loss: 2.927507227381803, Validation Loss: 9.316983222961426\n",
      "Early stopping at epoch 711\n",
      "Final Test Loss: 11.158198356628418\n",
      "Training model with target variable: high\n",
      "\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0003, window_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Training Loss: 28281.659234074155, Validation Loss: 27806.3359375\n",
      "Epoch [2/1500], Training Loss: 26533.99723871147, Validation Loss: 26215.443359375\n",
      "Epoch [3/1500], Training Loss: 25113.788917967962, Validation Loss: 24877.890625\n",
      "Epoch [4/1500], Training Loss: 23943.234187347804, Validation Loss: 23775.986328125\n",
      "Epoch [5/1500], Training Loss: 22936.79370956132, Validation Loss: 22783.927734375\n",
      "Epoch [6/1500], Training Loss: 21892.648935449328, Validation Loss: 21768.2734375\n",
      "Epoch [7/1500], Training Loss: 20945.055619894592, Validation Loss: 20830.08984375\n",
      "Epoch [8/1500], Training Loss: 20071.324757329847, Validation Loss: 19967.435546875\n",
      "Epoch [9/1500], Training Loss: 19256.12814365151, Validation Loss: 19163.798828125\n",
      "Epoch [10/1500], Training Loss: 18494.212879692055, Validation Loss: 18413.2890625\n",
      "Epoch [11/1500], Training Loss: 17779.80272702703, Validation Loss: 17711.71875\n",
      "Epoch [12/1500], Training Loss: 17108.456250595966, Validation Loss: 17054.474609375\n",
      "Epoch [13/1500], Training Loss: 16477.841661980772, Validation Loss: 16432.5234375\n",
      "Epoch [14/1500], Training Loss: 15885.459830028512, Validation Loss: 15840.740234375\n",
      "Epoch [15/1500], Training Loss: 15328.027326348285, Validation Loss: 15282.2861328125\n",
      "Epoch [16/1500], Training Loss: 14802.39398928679, Validation Loss: 14762.13671875\n",
      "Epoch [17/1500], Training Loss: 14306.084199448951, Validation Loss: 14293.5546875\n",
      "Epoch [18/1500], Training Loss: 13834.435866084563, Validation Loss: 13874.4033203125\n",
      "Epoch [19/1500], Training Loss: 13386.830913528507, Validation Loss: 13440.0625\n",
      "Epoch [20/1500], Training Loss: 12962.136484671793, Validation Loss: 13070.240234375\n",
      "Epoch [21/1500], Training Loss: 12559.240865580055, Validation Loss: 12673.458984375\n",
      "Epoch [22/1500], Training Loss: 12176.784489710022, Validation Loss: 12283.25\n",
      "Epoch [23/1500], Training Loss: 11813.40440851658, Validation Loss: 11917.1376953125\n",
      "Epoch [24/1500], Training Loss: 11467.9006230115, Validation Loss: 11575.4189453125\n",
      "Epoch [25/1500], Training Loss: 11139.390389722426, Validation Loss: 11258.2646484375\n",
      "Epoch [26/1500], Training Loss: 10827.088754220405, Validation Loss: 10958.9267578125\n",
      "Epoch [27/1500], Training Loss: 10530.43954100051, Validation Loss: 10671.677734375\n",
      "Epoch [28/1500], Training Loss: 10248.9982292592, Validation Loss: 10405.1611328125\n",
      "Epoch [29/1500], Training Loss: 9981.18895092533, Validation Loss: 10264.4228515625\n",
      "Epoch [30/1500], Training Loss: 9714.392376997208, Validation Loss: 10170.087890625\n",
      "Epoch [31/1500], Training Loss: 9474.508366543185, Validation Loss: 10037.2333984375\n",
      "Epoch [32/1500], Training Loss: 9239.235631875177, Validation Loss: 9429.5849609375\n",
      "Epoch [33/1500], Training Loss: 8934.85383769718, Validation Loss: 9472.126953125\n",
      "Epoch [34/1500], Training Loss: 8757.726477519225, Validation Loss: 8932.10546875\n",
      "Epoch [35/1500], Training Loss: 8451.200757205632, Validation Loss: 8852.4091796875\n",
      "Epoch [36/1500], Training Loss: 8222.114049360875, Validation Loss: 8601.251953125\n",
      "Epoch [37/1500], Training Loss: 8040.132432231287, Validation Loss: 8288.9140625\n",
      "Epoch [38/1500], Training Loss: 7789.397700611968, Validation Loss: 8085.4208984375\n",
      "Epoch [39/1500], Training Loss: 7577.55729976153, Validation Loss: 7922.3896484375\n",
      "Epoch [40/1500], Training Loss: 7372.00463883942, Validation Loss: 7684.59033203125\n",
      "Epoch [41/1500], Training Loss: 7168.003814110071, Validation Loss: 7396.00830078125\n",
      "Epoch [42/1500], Training Loss: 6977.106781288751, Validation Loss: 7183.28662109375\n",
      "Epoch [43/1500], Training Loss: 6795.288017418784, Validation Loss: 6964.9990234375\n",
      "Epoch [44/1500], Training Loss: 6619.453916382271, Validation Loss: 6760.31884765625\n",
      "Epoch [45/1500], Training Loss: 6450.457611209026, Validation Loss: 6553.486328125\n",
      "Epoch [46/1500], Training Loss: 6286.97660872322, Validation Loss: 6368.82568359375\n",
      "Epoch [47/1500], Training Loss: 6128.142076091865, Validation Loss: 6198.8310546875\n",
      "Epoch [48/1500], Training Loss: 5973.745779043188, Validation Loss: 6039.09912109375\n",
      "Epoch [49/1500], Training Loss: 5823.640061845474, Validation Loss: 5886.3701171875\n",
      "Epoch [50/1500], Training Loss: 5677.789938608077, Validation Loss: 5739.78271484375\n",
      "Epoch [51/1500], Training Loss: 5536.221610816092, Validation Loss: 5598.791015625\n",
      "Epoch [52/1500], Training Loss: 5398.954568753183, Validation Loss: 5462.92529296875\n",
      "Epoch [53/1500], Training Loss: 5265.988823719795, Validation Loss: 5331.84423828125\n",
      "Epoch [54/1500], Training Loss: 5137.175136221252, Validation Loss: 5205.18798828125\n",
      "Epoch [55/1500], Training Loss: 5012.144932363323, Validation Loss: 5082.46435546875\n",
      "Epoch [56/1500], Training Loss: 4890.3508923125955, Validation Loss: 4963.07275390625\n",
      "Epoch [57/1500], Training Loss: 4771.349209777317, Validation Loss: 4846.52880859375\n",
      "Epoch [58/1500], Training Loss: 4654.886956299184, Validation Loss: 4732.564453125\n",
      "Epoch [59/1500], Training Loss: 4540.821254507931, Validation Loss: 4621.123046875\n",
      "Epoch [60/1500], Training Loss: 4429.076906025539, Validation Loss: 4512.11376953125\n",
      "Epoch [61/1500], Training Loss: 4319.62357669223, Validation Loss: 4405.48193359375\n",
      "Epoch [62/1500], Training Loss: 4212.42579379724, Validation Loss: 4301.1220703125\n",
      "Epoch [63/1500], Training Loss: 4107.51222586397, Validation Loss: 4198.99365234375\n",
      "Epoch [64/1500], Training Loss: 4004.892281476304, Validation Loss: 4099.08447265625\n",
      "Epoch [65/1500], Training Loss: 3904.5939718402456, Validation Loss: 4001.42626953125\n",
      "Epoch [66/1500], Training Loss: 3806.624137345651, Validation Loss: 3906.0849609375\n",
      "Epoch [67/1500], Training Loss: 3710.995750048795, Validation Loss: 3813.097900390625\n",
      "Epoch [68/1500], Training Loss: 3617.738292364143, Validation Loss: 3722.439697265625\n",
      "Epoch [69/1500], Training Loss: 3526.854497244369, Validation Loss: 3634.113525390625\n",
      "Epoch [70/1500], Training Loss: 3438.3260228976683, Validation Loss: 3548.215087890625\n",
      "Epoch [71/1500], Training Loss: 3352.1393600082924, Validation Loss: 3464.74267578125\n",
      "Epoch [72/1500], Training Loss: 3268.28118778401, Validation Loss: 3383.621337890625\n",
      "Epoch [73/1500], Training Loss: 3186.778025225235, Validation Loss: 3304.779296875\n",
      "Epoch [74/1500], Training Loss: 3107.619001864267, Validation Loss: 3228.192138671875\n",
      "Epoch [75/1500], Training Loss: 3030.764691841105, Validation Loss: 3153.80224609375\n",
      "Epoch [76/1500], Training Loss: 2956.089057104847, Validation Loss: 3081.524169921875\n",
      "Epoch [77/1500], Training Loss: 2883.475662468267, Validation Loss: 3011.267578125\n",
      "Epoch [78/1500], Training Loss: 2812.829540131664, Validation Loss: 2942.994140625\n",
      "Epoch [79/1500], Training Loss: 2744.1232682764635, Validation Loss: 2876.68310546875\n",
      "Epoch [80/1500], Training Loss: 2677.4231031896056, Validation Loss: 2812.423583984375\n",
      "Epoch [81/1500], Training Loss: 2612.7866877533684, Validation Loss: 2750.31787109375\n",
      "Epoch [82/1500], Training Loss: 2550.2658467557117, Validation Loss: 2690.492919921875\n",
      "Epoch [83/1500], Training Loss: 2489.8722829042977, Validation Loss: 2632.79833984375\n",
      "Epoch [84/1500], Training Loss: 2431.584322094505, Validation Loss: 2576.84130859375\n",
      "Epoch [85/1500], Training Loss: 2375.2375904304827, Validation Loss: 2522.474609375\n",
      "Epoch [86/1500], Training Loss: 2320.611388510238, Validation Loss: 2469.673828125\n",
      "Epoch [87/1500], Training Loss: 2267.51756014909, Validation Loss: 2418.35498046875\n",
      "Epoch [88/1500], Training Loss: 2215.8215966656776, Validation Loss: 2368.46435546875\n",
      "Epoch [89/1500], Training Loss: 2165.4402444203274, Validation Loss: 2319.926513671875\n",
      "Epoch [90/1500], Training Loss: 2116.295483602707, Validation Loss: 2272.642333984375\n",
      "Epoch [91/1500], Training Loss: 2068.3367695704783, Validation Loss: 2226.522216796875\n",
      "Epoch [92/1500], Training Loss: 2021.4875235269644, Validation Loss: 2181.481689453125\n",
      "Epoch [93/1500], Training Loss: 1975.7211599909883, Validation Loss: 2137.49658203125\n",
      "Epoch [94/1500], Training Loss: 1931.0415367428025, Validation Loss: 2094.580078125\n",
      "Epoch [95/1500], Training Loss: 1887.4371708316046, Validation Loss: 2052.70947265625\n",
      "Epoch [96/1500], Training Loss: 1844.9315367361621, Validation Loss: 2011.9251708984375\n",
      "Epoch [97/1500], Training Loss: 1803.521990082018, Validation Loss: 1972.2298583984375\n",
      "Epoch [98/1500], Training Loss: 1763.2284116545384, Validation Loss: 1933.652587890625\n",
      "Epoch [99/1500], Training Loss: 1724.0590901361072, Validation Loss: 1896.19921875\n",
      "Epoch [100/1500], Training Loss: 1686.0309070774833, Validation Loss: 1859.8929443359375\n",
      "Epoch [101/1500], Training Loss: 1649.1464996533161, Validation Loss: 1824.728759765625\n",
      "Epoch [102/1500], Training Loss: 1613.4345062309123, Validation Loss: 1790.70556640625\n",
      "Epoch [103/1500], Training Loss: 1578.9001232657617, Validation Loss: 1757.81884765625\n",
      "Epoch [104/1500], Training Loss: 1545.5385254228374, Validation Loss: 1726.02685546875\n",
      "Epoch [105/1500], Training Loss: 1513.2801969276265, Validation Loss: 1695.260009765625\n",
      "Epoch [106/1500], Training Loss: 1482.0519527505078, Validation Loss: 1665.451171875\n",
      "Epoch [107/1500], Training Loss: 1451.7996941714196, Validation Loss: 1636.5426025390625\n",
      "Epoch [108/1500], Training Loss: 1422.4555686020278, Validation Loss: 1608.449951171875\n",
      "Epoch [109/1500], Training Loss: 1393.9657314496872, Validation Loss: 1581.1094970703125\n",
      "Epoch [110/1500], Training Loss: 1366.2939731546517, Validation Loss: 1554.4560546875\n",
      "Epoch [111/1500], Training Loss: 1339.3933146199179, Validation Loss: 1528.4434814453125\n",
      "Epoch [112/1500], Training Loss: 1313.2213681472836, Validation Loss: 1503.0167236328125\n",
      "Epoch [113/1500], Training Loss: 1287.7437138231774, Validation Loss: 1478.142822265625\n",
      "Epoch [114/1500], Training Loss: 1262.9064680936854, Validation Loss: 1453.787841796875\n",
      "Epoch [115/1500], Training Loss: 1238.6665582886565, Validation Loss: 1429.9041748046875\n",
      "Epoch [116/1500], Training Loss: 1214.975333334003, Validation Loss: 1406.4522705078125\n",
      "Epoch [117/1500], Training Loss: 1191.7948034704746, Validation Loss: 1383.394775390625\n",
      "Epoch [118/1500], Training Loss: 1169.0853788700933, Validation Loss: 1360.6773681640625\n",
      "Epoch [119/1500], Training Loss: 1146.8081964418752, Validation Loss: 1338.242431640625\n",
      "Epoch [120/1500], Training Loss: 1124.9307741268756, Validation Loss: 1316.03369140625\n",
      "Epoch [121/1500], Training Loss: 1103.4344961141821, Validation Loss: 1293.989501953125\n",
      "Epoch [122/1500], Training Loss: 1082.288770887092, Validation Loss: 1272.0322265625\n",
      "Epoch [123/1500], Training Loss: 1061.4798842559242, Validation Loss: 1250.0870361328125\n",
      "Epoch [124/1500], Training Loss: 1040.9880121117703, Validation Loss: 1228.093505859375\n",
      "Epoch [125/1500], Training Loss: 1020.8104692090346, Validation Loss: 1206.057861328125\n",
      "Epoch [126/1500], Training Loss: 1000.9513625859877, Validation Loss: 1184.0457763671875\n",
      "Epoch [127/1500], Training Loss: 981.4295516985006, Validation Loss: 1162.1865234375\n",
      "Epoch [128/1500], Training Loss: 962.2650183407029, Validation Loss: 1140.595458984375\n",
      "Epoch [129/1500], Training Loss: 943.4598095597499, Validation Loss: 1119.3441162109375\n",
      "Epoch [130/1500], Training Loss: 925.0085430432968, Validation Loss: 1098.4915771484375\n",
      "Epoch [131/1500], Training Loss: 906.9252207953414, Validation Loss: 1078.0703125\n",
      "Epoch [132/1500], Training Loss: 889.2028667968452, Validation Loss: 1058.09814453125\n",
      "Epoch [133/1500], Training Loss: 871.8416810477651, Validation Loss: 1038.580322265625\n",
      "Epoch [134/1500], Training Loss: 854.8289538639619, Validation Loss: 1019.5138549804688\n",
      "Epoch [135/1500], Training Loss: 838.1652633215838, Validation Loss: 1000.8900146484375\n",
      "Epoch [136/1500], Training Loss: 821.8475339550672, Validation Loss: 982.6968383789062\n",
      "Epoch [137/1500], Training Loss: 805.8702659032715, Validation Loss: 964.911865234375\n",
      "Epoch [138/1500], Training Loss: 790.2174792943323, Validation Loss: 947.4902954101562\n",
      "Epoch [139/1500], Training Loss: 774.8821913787693, Validation Loss: 930.4191284179688\n",
      "Epoch [140/1500], Training Loss: 759.8561076051046, Validation Loss: 913.659423828125\n",
      "Epoch [141/1500], Training Loss: 745.1229508010343, Validation Loss: 897.17822265625\n",
      "Epoch [142/1500], Training Loss: 730.6686875984701, Validation Loss: 880.9296264648438\n",
      "Epoch [143/1500], Training Loss: 716.4817835476366, Validation Loss: 864.8895263671875\n",
      "Epoch [144/1500], Training Loss: 702.5532388710567, Validation Loss: 849.017333984375\n",
      "Epoch [145/1500], Training Loss: 688.8684612036963, Validation Loss: 833.278564453125\n",
      "Epoch [146/1500], Training Loss: 675.4138430275862, Validation Loss: 817.6483764648438\n",
      "Epoch [147/1500], Training Loss: 662.1791301069065, Validation Loss: 802.0938110351562\n",
      "Epoch [148/1500], Training Loss: 649.1506221784163, Validation Loss: 786.59716796875\n",
      "Epoch [149/1500], Training Loss: 636.3226736009465, Validation Loss: 771.1587524414062\n",
      "Epoch [150/1500], Training Loss: 623.6829304042303, Validation Loss: 755.76904296875\n",
      "Epoch [151/1500], Training Loss: 611.2178215405708, Validation Loss: 740.4378051757812\n",
      "Epoch [152/1500], Training Loss: 598.9308904938641, Validation Loss: 725.1880493164062\n",
      "Epoch [153/1500], Training Loss: 586.8167778271003, Validation Loss: 710.047119140625\n",
      "Epoch [154/1500], Training Loss: 574.875478652227, Validation Loss: 695.0574951171875\n",
      "Epoch [155/1500], Training Loss: 563.1091745888796, Validation Loss: 680.2657470703125\n",
      "Epoch [156/1500], Training Loss: 551.521996943828, Validation Loss: 665.7190551757812\n",
      "Epoch [157/1500], Training Loss: 540.1161718908137, Validation Loss: 651.48291015625\n",
      "Epoch [158/1500], Training Loss: 528.9000586736094, Validation Loss: 637.6115112304688\n",
      "Epoch [159/1500], Training Loss: 517.8786316268412, Validation Loss: 624.1560668945312\n",
      "Epoch [160/1500], Training Loss: 507.0506147957302, Validation Loss: 611.1499633789062\n",
      "Epoch [161/1500], Training Loss: 496.4233252817335, Validation Loss: 598.6102905273438\n",
      "Epoch [162/1500], Training Loss: 485.99460388977866, Validation Loss: 586.52587890625\n",
      "Epoch [163/1500], Training Loss: 475.7652074989498, Validation Loss: 574.8738403320312\n",
      "Epoch [164/1500], Training Loss: 465.72811149792966, Validation Loss: 563.616455078125\n",
      "Epoch [165/1500], Training Loss: 455.88293952841036, Validation Loss: 552.727783203125\n",
      "Epoch [166/1500], Training Loss: 446.2288690375455, Validation Loss: 542.1719970703125\n",
      "Epoch [167/1500], Training Loss: 436.7624826915805, Validation Loss: 531.929931640625\n",
      "Epoch [168/1500], Training Loss: 427.4748413984668, Validation Loss: 521.97705078125\n",
      "Epoch [169/1500], Training Loss: 418.36506510178, Validation Loss: 512.302978515625\n",
      "Epoch [170/1500], Training Loss: 409.43070490401436, Validation Loss: 502.8915710449219\n",
      "Epoch [171/1500], Training Loss: 400.6674127764801, Validation Loss: 493.7264404296875\n",
      "Epoch [172/1500], Training Loss: 392.0733311384285, Validation Loss: 484.8013916015625\n",
      "Epoch [173/1500], Training Loss: 383.64430192873505, Validation Loss: 476.0998840332031\n",
      "Epoch [174/1500], Training Loss: 375.37884671913093, Validation Loss: 467.6129150390625\n",
      "Epoch [175/1500], Training Loss: 367.2747000225559, Validation Loss: 459.32958984375\n",
      "Epoch [176/1500], Training Loss: 359.3265163003129, Validation Loss: 451.23687744140625\n",
      "Epoch [177/1500], Training Loss: 351.5324966749249, Validation Loss: 443.3316650390625\n",
      "Epoch [178/1500], Training Loss: 343.8950734306691, Validation Loss: 435.60736083984375\n",
      "Epoch [179/1500], Training Loss: 336.4092764532695, Validation Loss: 428.0542297363281\n",
      "Epoch [180/1500], Training Loss: 329.07441215625033, Validation Loss: 420.6703796386719\n",
      "Epoch [181/1500], Training Loss: 321.8920555792056, Validation Loss: 413.4495544433594\n",
      "Epoch [182/1500], Training Loss: 314.8584812696778, Validation Loss: 406.3848571777344\n",
      "Epoch [183/1500], Training Loss: 307.97114280343567, Validation Loss: 399.46820068359375\n",
      "Epoch [184/1500], Training Loss: 301.22931930393923, Validation Loss: 392.695556640625\n",
      "Epoch [185/1500], Training Loss: 294.6360587455842, Validation Loss: 386.0615539550781\n",
      "Epoch [186/1500], Training Loss: 288.18541222183643, Validation Loss: 379.55755615234375\n",
      "Epoch [187/1500], Training Loss: 281.87518722486186, Validation Loss: 373.18060302734375\n",
      "Epoch [188/1500], Training Loss: 275.7113080163307, Validation Loss: 366.92620849609375\n",
      "Epoch [189/1500], Training Loss: 269.6919529956055, Validation Loss: 360.79022216796875\n",
      "Epoch [190/1500], Training Loss: 263.8144020773704, Validation Loss: 354.767822265625\n",
      "Epoch [191/1500], Training Loss: 258.07563051418833, Validation Loss: 348.84722900390625\n",
      "Epoch [192/1500], Training Loss: 252.47103127617405, Validation Loss: 343.0276184082031\n",
      "Epoch [193/1500], Training Loss: 247.00163125038304, Validation Loss: 337.30499267578125\n",
      "Epoch [194/1500], Training Loss: 241.66229742290056, Validation Loss: 331.67303466796875\n",
      "Epoch [195/1500], Training Loss: 236.45297232837837, Validation Loss: 326.1289367675781\n",
      "Epoch [196/1500], Training Loss: 231.36922578123628, Validation Loss: 320.6695556640625\n",
      "Epoch [197/1500], Training Loss: 226.4071747228456, Validation Loss: 315.28961181640625\n",
      "Epoch [198/1500], Training Loss: 221.56339469988635, Validation Loss: 309.9871520996094\n",
      "Epoch [199/1500], Training Loss: 216.83534287049625, Validation Loss: 304.7575378417969\n",
      "Epoch [200/1500], Training Loss: 212.21981535348786, Validation Loss: 299.6006164550781\n",
      "Epoch [201/1500], Training Loss: 207.7143222581017, Validation Loss: 294.509765625\n",
      "Epoch [202/1500], Training Loss: 203.31466550172848, Validation Loss: 289.4847717285156\n",
      "Epoch [203/1500], Training Loss: 199.01837181047574, Validation Loss: 284.5238952636719\n",
      "Epoch [204/1500], Training Loss: 194.82244547056456, Validation Loss: 279.62506103515625\n",
      "Epoch [205/1500], Training Loss: 190.72464261557477, Validation Loss: 274.7857971191406\n",
      "Epoch [206/1500], Training Loss: 186.71944566404053, Validation Loss: 270.00616455078125\n",
      "Epoch [207/1500], Training Loss: 182.80521353594338, Validation Loss: 265.28717041015625\n",
      "Epoch [208/1500], Training Loss: 178.98020526153306, Validation Loss: 260.6307678222656\n",
      "Epoch [209/1500], Training Loss: 175.24092645303236, Validation Loss: 256.0377502441406\n",
      "Epoch [210/1500], Training Loss: 171.58226851770524, Validation Loss: 251.5075225830078\n",
      "Epoch [211/1500], Training Loss: 168.00313778551805, Validation Loss: 247.04087829589844\n",
      "Epoch [212/1500], Training Loss: 164.5008958496087, Validation Loss: 242.63839721679688\n",
      "Epoch [213/1500], Training Loss: 161.07155635210873, Validation Loss: 238.29502868652344\n",
      "Epoch [214/1500], Training Loss: 157.710649269424, Validation Loss: 234.01046752929688\n",
      "Epoch [215/1500], Training Loss: 154.41741048099934, Validation Loss: 229.7764129638672\n",
      "Epoch [216/1500], Training Loss: 151.18622285402682, Validation Loss: 225.58935546875\n",
      "Epoch [217/1500], Training Loss: 148.01599903377002, Validation Loss: 221.44334411621094\n",
      "Epoch [218/1500], Training Loss: 144.90427162937277, Validation Loss: 217.33409118652344\n",
      "Epoch [219/1500], Training Loss: 141.84992847543475, Validation Loss: 213.25779724121094\n",
      "Epoch [220/1500], Training Loss: 138.850544716247, Validation Loss: 209.209228515625\n",
      "Epoch [221/1500], Training Loss: 135.9055786207265, Validation Loss: 205.1854248046875\n",
      "Epoch [222/1500], Training Loss: 133.0123206312852, Validation Loss: 201.1846160888672\n",
      "Epoch [223/1500], Training Loss: 130.1700849892493, Validation Loss: 197.2058563232422\n",
      "Epoch [224/1500], Training Loss: 127.37656403949715, Validation Loss: 193.24822998046875\n",
      "Epoch [225/1500], Training Loss: 124.6320953766545, Validation Loss: 189.31475830078125\n",
      "Epoch [226/1500], Training Loss: 121.93743904367385, Validation Loss: 185.41030883789062\n",
      "Epoch [227/1500], Training Loss: 119.29175509131204, Validation Loss: 181.5345001220703\n",
      "Epoch [228/1500], Training Loss: 116.69316889804422, Validation Loss: 177.68614196777344\n",
      "Epoch [229/1500], Training Loss: 114.14207511747837, Validation Loss: 173.85610961914062\n",
      "Epoch [230/1500], Training Loss: 111.63665363371238, Validation Loss: 170.03549194335938\n",
      "Epoch [231/1500], Training Loss: 109.17464775517793, Validation Loss: 166.2136688232422\n",
      "Epoch [232/1500], Training Loss: 106.7562503309104, Validation Loss: 162.38502502441406\n",
      "Epoch [233/1500], Training Loss: 104.37790099112935, Validation Loss: 158.54403686523438\n",
      "Epoch [234/1500], Training Loss: 102.03813256915232, Validation Loss: 154.68431091308594\n",
      "Epoch [235/1500], Training Loss: 99.73556023892208, Validation Loss: 150.80838012695312\n",
      "Epoch [236/1500], Training Loss: 97.46823614625417, Validation Loss: 146.91424560546875\n",
      "Epoch [237/1500], Training Loss: 95.23428664774589, Validation Loss: 143.0022735595703\n",
      "Epoch [238/1500], Training Loss: 93.03195326118717, Validation Loss: 139.07826232910156\n",
      "Epoch [239/1500], Training Loss: 90.86070049162791, Validation Loss: 135.15087890625\n",
      "Epoch [240/1500], Training Loss: 88.72054079826988, Validation Loss: 131.23074340820312\n",
      "Epoch [241/1500], Training Loss: 86.61142044912059, Validation Loss: 127.33287048339844\n",
      "Epoch [242/1500], Training Loss: 84.53335008685977, Validation Loss: 123.47522735595703\n",
      "Epoch [243/1500], Training Loss: 82.49030001912176, Validation Loss: 119.67829895019531\n",
      "Epoch [244/1500], Training Loss: 80.48432436558005, Validation Loss: 115.96163177490234\n",
      "Epoch [245/1500], Training Loss: 78.51728473924597, Validation Loss: 112.34635925292969\n",
      "Epoch [246/1500], Training Loss: 76.5929780819088, Validation Loss: 108.85133361816406\n",
      "Epoch [247/1500], Training Loss: 74.71525160689696, Validation Loss: 105.49118041992188\n",
      "Epoch [248/1500], Training Loss: 72.88751137549504, Validation Loss: 102.27954864501953\n",
      "Epoch [249/1500], Training Loss: 71.11061433687905, Validation Loss: 99.2226333618164\n",
      "Epoch [250/1500], Training Loss: 69.38711092121159, Validation Loss: 96.3246078491211\n",
      "Epoch [251/1500], Training Loss: 67.71643745267365, Validation Loss: 93.58287048339844\n",
      "Epoch [252/1500], Training Loss: 66.09811312217752, Validation Loss: 90.99465942382812\n",
      "Epoch [253/1500], Training Loss: 64.53167681989753, Validation Loss: 88.55244445800781\n",
      "Epoch [254/1500], Training Loss: 63.016358976034766, Validation Loss: 86.24797821044922\n",
      "Epoch [255/1500], Training Loss: 61.54928553421622, Validation Loss: 84.07064056396484\n",
      "Epoch [256/1500], Training Loss: 60.129437531595954, Validation Loss: 82.00943756103516\n",
      "Epoch [257/1500], Training Loss: 58.75329907206733, Validation Loss: 80.05368041992188\n",
      "Epoch [258/1500], Training Loss: 57.41940777368215, Validation Loss: 78.19349670410156\n",
      "Epoch [259/1500], Training Loss: 56.126317078023256, Validation Loss: 76.41951751708984\n",
      "Epoch [260/1500], Training Loss: 54.87150402973225, Validation Loss: 74.72348022460938\n",
      "Epoch [261/1500], Training Loss: 53.65367275462819, Validation Loss: 73.09750366210938\n",
      "Epoch [262/1500], Training Loss: 52.47156287974501, Validation Loss: 71.53413391113281\n",
      "Epoch [263/1500], Training Loss: 51.32341207495448, Validation Loss: 70.02912902832031\n",
      "Epoch [264/1500], Training Loss: 50.2086802161828, Validation Loss: 68.5771713256836\n",
      "Epoch [265/1500], Training Loss: 49.124964552699645, Validation Loss: 67.17323303222656\n",
      "Epoch [266/1500], Training Loss: 48.07113734983112, Validation Loss: 65.81425476074219\n",
      "Epoch [267/1500], Training Loss: 47.04650719759986, Validation Loss: 64.49732971191406\n",
      "Epoch [268/1500], Training Loss: 46.04980728847392, Validation Loss: 63.22054672241211\n",
      "Epoch [269/1500], Training Loss: 45.080016277256654, Validation Loss: 61.98057174682617\n",
      "Epoch [270/1500], Training Loss: 44.135894937004295, Validation Loss: 60.77630615234375\n",
      "Epoch [271/1500], Training Loss: 43.216401821247224, Validation Loss: 59.60588073730469\n",
      "Epoch [272/1500], Training Loss: 42.321248980259156, Validation Loss: 58.46805191040039\n",
      "Epoch [273/1500], Training Loss: 41.44893688609705, Validation Loss: 57.36073684692383\n",
      "Epoch [274/1500], Training Loss: 40.59856347109873, Validation Loss: 56.28275680541992\n",
      "Epoch [275/1500], Training Loss: 39.76923725876304, Validation Loss: 55.23345184326172\n",
      "Epoch [276/1500], Training Loss: 38.96093443036354, Validation Loss: 54.2110481262207\n",
      "Epoch [277/1500], Training Loss: 38.172068793686094, Validation Loss: 53.21559524536133\n",
      "Epoch [278/1500], Training Loss: 37.402170346481604, Validation Loss: 52.24489212036133\n",
      "Epoch [279/1500], Training Loss: 36.65090562365445, Validation Loss: 51.299659729003906\n",
      "Epoch [280/1500], Training Loss: 35.91807950932331, Validation Loss: 50.37904357910156\n",
      "Epoch [281/1500], Training Loss: 35.202332605786076, Validation Loss: 49.480682373046875\n",
      "Epoch [282/1500], Training Loss: 34.50292495297093, Validation Loss: 48.60560989379883\n",
      "Epoch [283/1500], Training Loss: 33.81963697000464, Validation Loss: 47.75246810913086\n",
      "Epoch [284/1500], Training Loss: 33.15196980164953, Validation Loss: 46.92057418823242\n",
      "Epoch [285/1500], Training Loss: 32.499868939532035, Validation Loss: 46.10948944091797\n",
      "Epoch [286/1500], Training Loss: 31.862341127387037, Validation Loss: 45.318382263183594\n",
      "Epoch [287/1500], Training Loss: 31.23900077399005, Validation Loss: 44.5472297668457\n",
      "Epoch [288/1500], Training Loss: 30.62977843573554, Validation Loss: 43.7950439453125\n",
      "Epoch [289/1500], Training Loss: 30.034375503369468, Validation Loss: 43.06194305419922\n",
      "Epoch [290/1500], Training Loss: 29.452660646663205, Validation Loss: 42.346961975097656\n",
      "Epoch [291/1500], Training Loss: 28.884229527983564, Validation Loss: 41.64969253540039\n",
      "Epoch [292/1500], Training Loss: 28.328274786470196, Validation Loss: 40.96950149536133\n",
      "Epoch [293/1500], Training Loss: 27.785244525378452, Validation Loss: 40.30656051635742\n",
      "Epoch [294/1500], Training Loss: 27.254477884566906, Validation Loss: 39.65956497192383\n",
      "Epoch [295/1500], Training Loss: 26.735592624867138, Validation Loss: 39.027896881103516\n",
      "Epoch [296/1500], Training Loss: 26.228470709623263, Validation Loss: 38.4116325378418\n",
      "Epoch [297/1500], Training Loss: 25.733054389357402, Validation Loss: 37.81029510498047\n",
      "Epoch [298/1500], Training Loss: 25.24881634331812, Validation Loss: 37.22297668457031\n",
      "Epoch [299/1500], Training Loss: 24.775641357485398, Validation Loss: 36.64944839477539\n",
      "Epoch [300/1500], Training Loss: 24.31341395231708, Validation Loss: 36.089378356933594\n",
      "Epoch [301/1500], Training Loss: 23.86161634620331, Validation Loss: 35.541873931884766\n",
      "Epoch [302/1500], Training Loss: 23.41992332216955, Validation Loss: 35.006874084472656\n",
      "Epoch [303/1500], Training Loss: 22.98840069691533, Validation Loss: 34.48372268676758\n",
      "Epoch [304/1500], Training Loss: 22.56678849946974, Validation Loss: 33.97254180908203\n",
      "Epoch [305/1500], Training Loss: 22.154997688313873, Validation Loss: 33.472782135009766\n",
      "Epoch [306/1500], Training Loss: 21.752776273118016, Validation Loss: 32.98419952392578\n",
      "Epoch [307/1500], Training Loss: 21.35985395616631, Validation Loss: 32.50679016113281\n",
      "Epoch [308/1500], Training Loss: 20.97570935331272, Validation Loss: 32.04014205932617\n",
      "Epoch [309/1500], Training Loss: 20.600647221354432, Validation Loss: 31.583866119384766\n",
      "Epoch [310/1500], Training Loss: 20.23429782801779, Validation Loss: 31.138137817382812\n",
      "Epoch [311/1500], Training Loss: 19.876086241107917, Validation Loss: 30.702816009521484\n",
      "Epoch [312/1500], Training Loss: 19.526539710099797, Validation Loss: 30.277996063232422\n",
      "Epoch [313/1500], Training Loss: 19.185052198923977, Validation Loss: 29.863636016845703\n",
      "Epoch [314/1500], Training Loss: 18.851599706824064, Validation Loss: 29.45972442626953\n",
      "Epoch [315/1500], Training Loss: 18.52559373172645, Validation Loss: 29.066667556762695\n",
      "Epoch [316/1500], Training Loss: 18.20737037845604, Validation Loss: 28.68439292907715\n",
      "Epoch [317/1500], Training Loss: 17.89658639442834, Validation Loss: 28.313491821289062\n",
      "Epoch [318/1500], Training Loss: 17.59323328871617, Validation Loss: 27.954212188720703\n",
      "Epoch [319/1500], Training Loss: 17.29688767869398, Validation Loss: 27.606449127197266\n",
      "Epoch [320/1500], Training Loss: 17.007286431047078, Validation Loss: 27.270605087280273\n",
      "Epoch [321/1500], Training Loss: 16.724610536766964, Validation Loss: 26.94643783569336\n",
      "Epoch [322/1500], Training Loss: 16.449019822522036, Validation Loss: 26.633880615234375\n",
      "Epoch [323/1500], Training Loss: 16.180225240737062, Validation Loss: 26.33234214782715\n",
      "Epoch [324/1500], Training Loss: 15.917659604758198, Validation Loss: 26.040624618530273\n",
      "Epoch [325/1500], Training Loss: 15.661457782949087, Validation Loss: 25.759353637695312\n",
      "Epoch [326/1500], Training Loss: 15.411328790490957, Validation Loss: 25.487089157104492\n",
      "Epoch [327/1500], Training Loss: 15.167156378993521, Validation Loss: 25.223997116088867\n",
      "Epoch [328/1500], Training Loss: 14.928768027721004, Validation Loss: 24.969070434570312\n",
      "Epoch [329/1500], Training Loss: 14.695838359754616, Validation Loss: 24.722034454345703\n",
      "Epoch [330/1500], Training Loss: 14.468674296714093, Validation Loss: 24.482736587524414\n",
      "Epoch [331/1500], Training Loss: 14.247051186133916, Validation Loss: 24.25048065185547\n",
      "Epoch [332/1500], Training Loss: 14.030757636599528, Validation Loss: 24.024599075317383\n",
      "Epoch [333/1500], Training Loss: 13.81960550909211, Validation Loss: 23.805265426635742\n",
      "Epoch [334/1500], Training Loss: 13.613481659165293, Validation Loss: 23.591516494750977\n",
      "Epoch [335/1500], Training Loss: 13.412260624331749, Validation Loss: 23.383054733276367\n",
      "Epoch [336/1500], Training Loss: 13.215863102304079, Validation Loss: 23.179548263549805\n",
      "Epoch [337/1500], Training Loss: 13.02402703779369, Validation Loss: 22.98053741455078\n",
      "Epoch [338/1500], Training Loss: 12.83664509856355, Validation Loss: 22.78570556640625\n",
      "Epoch [339/1500], Training Loss: 12.653540907135497, Validation Loss: 22.59465980529785\n",
      "Epoch [340/1500], Training Loss: 12.474504938540285, Validation Loss: 22.407217025756836\n",
      "Epoch [341/1500], Training Loss: 12.299575110696201, Validation Loss: 22.222782135009766\n",
      "Epoch [342/1500], Training Loss: 12.128579945744528, Validation Loss: 22.04155731201172\n",
      "Epoch [343/1500], Training Loss: 11.96136243831318, Validation Loss: 21.863143920898438\n",
      "Epoch [344/1500], Training Loss: 11.797835847246684, Validation Loss: 21.68685531616211\n",
      "Epoch [345/1500], Training Loss: 11.637893841375908, Validation Loss: 21.512561798095703\n",
      "Epoch [346/1500], Training Loss: 11.481408196966145, Validation Loss: 21.340486526489258\n",
      "Epoch [347/1500], Training Loss: 11.328290652294173, Validation Loss: 21.17062759399414\n",
      "Epoch [348/1500], Training Loss: 11.178442320502288, Validation Loss: 21.002674102783203\n",
      "Epoch [349/1500], Training Loss: 11.031824309291526, Validation Loss: 20.836641311645508\n",
      "Epoch [350/1500], Training Loss: 10.888336996035454, Validation Loss: 20.671871185302734\n",
      "Epoch [351/1500], Training Loss: 10.74777698554129, Validation Loss: 20.509138107299805\n",
      "Epoch [352/1500], Training Loss: 10.610131506743627, Validation Loss: 20.347789764404297\n",
      "Epoch [353/1500], Training Loss: 10.475344933870819, Validation Loss: 20.187990188598633\n",
      "Epoch [354/1500], Training Loss: 10.343290874644774, Validation Loss: 20.03028106689453\n",
      "Epoch [355/1500], Training Loss: 10.213810947589787, Validation Loss: 19.874187469482422\n",
      "Epoch [356/1500], Training Loss: 10.086935409754426, Validation Loss: 19.71935272216797\n",
      "Epoch [357/1500], Training Loss: 9.962561681332486, Validation Loss: 19.566654205322266\n",
      "Epoch [358/1500], Training Loss: 9.840632677830326, Validation Loss: 19.41579818725586\n",
      "Epoch [359/1500], Training Loss: 9.720990099451491, Validation Loss: 19.266490936279297\n",
      "Epoch [360/1500], Training Loss: 9.603506498219014, Validation Loss: 19.118907928466797\n",
      "Epoch [361/1500], Training Loss: 9.488190795974976, Validation Loss: 18.97344207763672\n",
      "Epoch [362/1500], Training Loss: 9.375056619791904, Validation Loss: 18.830116271972656\n",
      "Epoch [363/1500], Training Loss: 9.263988909682457, Validation Loss: 18.6888427734375\n",
      "Epoch [364/1500], Training Loss: 9.155111101106325, Validation Loss: 18.54983901977539\n",
      "Epoch [365/1500], Training Loss: 9.048317833164043, Validation Loss: 18.412830352783203\n",
      "Epoch [366/1500], Training Loss: 8.943349295156173, Validation Loss: 18.27818489074707\n",
      "Epoch [367/1500], Training Loss: 8.84020615929161, Validation Loss: 18.145587921142578\n",
      "Epoch [368/1500], Training Loss: 8.738954255929402, Validation Loss: 18.015199661254883\n",
      "Epoch [369/1500], Training Loss: 8.639416287423003, Validation Loss: 17.886911392211914\n",
      "Epoch [370/1500], Training Loss: 8.541661207387053, Validation Loss: 17.760557174682617\n",
      "Epoch [371/1500], Training Loss: 8.445689077930782, Validation Loss: 17.63628387451172\n",
      "Epoch [372/1500], Training Loss: 8.351305324702569, Validation Loss: 17.51402473449707\n",
      "Epoch [373/1500], Training Loss: 8.258508058889019, Validation Loss: 17.39379119873047\n",
      "Epoch [374/1500], Training Loss: 8.167253040049497, Validation Loss: 17.27544593811035\n",
      "Epoch [375/1500], Training Loss: 8.077590441712164, Validation Loss: 17.15886688232422\n",
      "Epoch [376/1500], Training Loss: 7.989436710305792, Validation Loss: 17.04410743713379\n",
      "Epoch [377/1500], Training Loss: 7.90278662117468, Validation Loss: 16.930805206298828\n",
      "Epoch [378/1500], Training Loss: 7.817501426903941, Validation Loss: 16.81926155090332\n",
      "Epoch [379/1500], Training Loss: 7.733727799964616, Validation Loss: 16.709306716918945\n",
      "Epoch [380/1500], Training Loss: 7.6512482006707705, Validation Loss: 16.600849151611328\n",
      "Epoch [381/1500], Training Loss: 7.5700957881547515, Validation Loss: 16.49390411376953\n",
      "Epoch [382/1500], Training Loss: 7.490338425497566, Validation Loss: 16.388277053833008\n",
      "Epoch [383/1500], Training Loss: 7.411951550333313, Validation Loss: 16.283916473388672\n",
      "Epoch [384/1500], Training Loss: 7.334775346443578, Validation Loss: 16.180994033813477\n",
      "Epoch [385/1500], Training Loss: 7.258721850574281, Validation Loss: 16.07925033569336\n",
      "Epoch [386/1500], Training Loss: 7.183718257974545, Validation Loss: 15.978471755981445\n",
      "Epoch [387/1500], Training Loss: 7.10999045693309, Validation Loss: 15.878928184509277\n",
      "Epoch [388/1500], Training Loss: 7.037267863990528, Validation Loss: 15.780186653137207\n",
      "Epoch [389/1500], Training Loss: 6.9655985307321195, Validation Loss: 15.682482719421387\n",
      "Epoch [390/1500], Training Loss: 6.895067752926717, Validation Loss: 15.586028099060059\n",
      "Epoch [391/1500], Training Loss: 6.825545118411934, Validation Loss: 15.490107536315918\n",
      "Epoch [392/1500], Training Loss: 6.756972313820875, Validation Loss: 15.39529037475586\n",
      "Epoch [393/1500], Training Loss: 6.68936296883908, Validation Loss: 15.300971984863281\n",
      "Epoch [394/1500], Training Loss: 6.622746368255442, Validation Loss: 15.20765495300293\n",
      "Epoch [395/1500], Training Loss: 6.55708792691017, Validation Loss: 15.11518383026123\n",
      "Epoch [396/1500], Training Loss: 6.492464458676447, Validation Loss: 15.02376651763916\n",
      "Epoch [397/1500], Training Loss: 6.42867791121701, Validation Loss: 14.933062553405762\n",
      "Epoch [398/1500], Training Loss: 6.365700285595086, Validation Loss: 14.843057632446289\n",
      "Epoch [399/1500], Training Loss: 6.303606368639056, Validation Loss: 14.753859519958496\n",
      "Epoch [400/1500], Training Loss: 6.242389794660297, Validation Loss: 14.665337562561035\n",
      "Epoch [401/1500], Training Loss: 6.182031145556557, Validation Loss: 14.577751159667969\n",
      "Epoch [402/1500], Training Loss: 6.122474028469018, Validation Loss: 14.49094009399414\n",
      "Epoch [403/1500], Training Loss: 6.0638471676323915, Validation Loss: 14.405034065246582\n",
      "Epoch [404/1500], Training Loss: 6.006083219448812, Validation Loss: 14.319788932800293\n",
      "Epoch [405/1500], Training Loss: 5.949067533502273, Validation Loss: 14.235026359558105\n",
      "Epoch [406/1500], Training Loss: 5.892632624623525, Validation Loss: 14.15097713470459\n",
      "Epoch [407/1500], Training Loss: 5.8370002638969805, Validation Loss: 14.067660331726074\n",
      "Epoch [408/1500], Training Loss: 5.782174119410582, Validation Loss: 13.985037803649902\n",
      "Epoch [409/1500], Training Loss: 5.728134149781051, Validation Loss: 13.903447151184082\n",
      "Epoch [410/1500], Training Loss: 5.674781817200374, Validation Loss: 13.822240829467773\n",
      "Epoch [411/1500], Training Loss: 5.622053656126775, Validation Loss: 13.74181079864502\n",
      "Epoch [412/1500], Training Loss: 5.569971232269257, Validation Loss: 13.661927223205566\n",
      "Epoch [413/1500], Training Loss: 5.518568867116612, Validation Loss: 13.582859992980957\n",
      "Epoch [414/1500], Training Loss: 5.467880592079422, Validation Loss: 13.504490852355957\n",
      "Epoch [415/1500], Training Loss: 5.417895949758096, Validation Loss: 13.426740646362305\n",
      "Epoch [416/1500], Training Loss: 5.368410485659099, Validation Loss: 13.349676132202148\n",
      "Epoch [417/1500], Training Loss: 5.319532954504571, Validation Loss: 13.273462295532227\n",
      "Epoch [418/1500], Training Loss: 5.271255305360897, Validation Loss: 13.197604179382324\n",
      "Epoch [419/1500], Training Loss: 5.22355529530113, Validation Loss: 13.12260913848877\n",
      "Epoch [420/1500], Training Loss: 5.176457613525398, Validation Loss: 13.048112869262695\n",
      "Epoch [421/1500], Training Loss: 5.129995072701936, Validation Loss: 12.974308013916016\n",
      "Epoch [422/1500], Training Loss: 5.084063988177439, Validation Loss: 12.901430130004883\n",
      "Epoch [423/1500], Training Loss: 5.0386547164532915, Validation Loss: 12.828936576843262\n",
      "Epoch [424/1500], Training Loss: 4.993811871224601, Validation Loss: 12.757306098937988\n",
      "Epoch [425/1500], Training Loss: 4.949629436666261, Validation Loss: 12.686432838439941\n",
      "Epoch [426/1500], Training Loss: 4.90600605237556, Validation Loss: 12.61622142791748\n",
      "Epoch [427/1500], Training Loss: 4.8629537148214395, Validation Loss: 12.546769142150879\n",
      "Epoch [428/1500], Training Loss: 4.820517660660371, Validation Loss: 12.478160858154297\n",
      "Epoch [429/1500], Training Loss: 4.778563597703959, Validation Loss: 12.410064697265625\n",
      "Epoch [430/1500], Training Loss: 4.7371897740541815, Validation Loss: 12.342572212219238\n",
      "Epoch [431/1500], Training Loss: 4.696318111194348, Validation Loss: 12.275668144226074\n",
      "Epoch [432/1500], Training Loss: 4.655898839948882, Validation Loss: 12.20928955078125\n",
      "Epoch [433/1500], Training Loss: 4.615826007705899, Validation Loss: 12.143340110778809\n",
      "Epoch [434/1500], Training Loss: 4.5762019673430325, Validation Loss: 12.077964782714844\n",
      "Epoch [435/1500], Training Loss: 4.537151520611147, Validation Loss: 12.013429641723633\n",
      "Epoch [436/1500], Training Loss: 4.498612761566188, Validation Loss: 11.949419975280762\n",
      "Epoch [437/1500], Training Loss: 4.46046732089832, Validation Loss: 11.88565731048584\n",
      "Epoch [438/1500], Training Loss: 4.422697930366677, Validation Loss: 11.822436332702637\n",
      "Epoch [439/1500], Training Loss: 4.385400657209143, Validation Loss: 11.759791374206543\n",
      "Epoch [440/1500], Training Loss: 4.348552957875768, Validation Loss: 11.697699546813965\n",
      "Epoch [441/1500], Training Loss: 4.312178703308096, Validation Loss: 11.636025428771973\n",
      "Epoch [442/1500], Training Loss: 4.276252642393979, Validation Loss: 11.574647903442383\n",
      "Epoch [443/1500], Training Loss: 4.2406995956818125, Validation Loss: 11.513681411743164\n",
      "Epoch [444/1500], Training Loss: 4.205503446103449, Validation Loss: 11.453248977661133\n",
      "Epoch [445/1500], Training Loss: 4.170711295032036, Validation Loss: 11.392972946166992\n",
      "Epoch [446/1500], Training Loss: 4.136290633263019, Validation Loss: 11.332987785339355\n",
      "Epoch [447/1500], Training Loss: 4.102316626587411, Validation Loss: 11.273371696472168\n",
      "Epoch [448/1500], Training Loss: 4.068596012346589, Validation Loss: 11.213761329650879\n",
      "Epoch [449/1500], Training Loss: 4.035159537416939, Validation Loss: 11.1544828414917\n",
      "Epoch [450/1500], Training Loss: 4.002090249576363, Validation Loss: 11.095128059387207\n",
      "Epoch [451/1500], Training Loss: 3.969478990811229, Validation Loss: 11.036040306091309\n",
      "Epoch [452/1500], Training Loss: 3.937301853102069, Validation Loss: 10.97710132598877\n",
      "Epoch [453/1500], Training Loss: 3.905577312663919, Validation Loss: 10.91835880279541\n",
      "Epoch [454/1500], Training Loss: 3.8742640140049867, Validation Loss: 10.859763145446777\n",
      "Epoch [455/1500], Training Loss: 3.8432761080538556, Validation Loss: 10.800925254821777\n",
      "Epoch [456/1500], Training Loss: 3.8125578697503317, Validation Loss: 10.74239730834961\n",
      "Epoch [457/1500], Training Loss: 3.782121802233691, Validation Loss: 10.683708190917969\n",
      "Epoch [458/1500], Training Loss: 3.752020312809525, Validation Loss: 10.624822616577148\n",
      "Epoch [459/1500], Training Loss: 3.7222146700524275, Validation Loss: 10.56601333618164\n",
      "Epoch [460/1500], Training Loss: 3.6927913928176634, Validation Loss: 10.507125854492188\n",
      "Epoch [461/1500], Training Loss: 3.663658041008608, Validation Loss: 10.448027610778809\n",
      "Epoch [462/1500], Training Loss: 3.634837206892261, Validation Loss: 10.3888578414917\n",
      "Epoch [463/1500], Training Loss: 3.6063373008922137, Validation Loss: 10.329804420471191\n",
      "Epoch [464/1500], Training Loss: 3.578062541895286, Validation Loss: 10.270393371582031\n",
      "Epoch [465/1500], Training Loss: 3.550034188221409, Validation Loss: 10.21110725402832\n",
      "Epoch [466/1500], Training Loss: 3.5222946873472454, Validation Loss: 10.151772499084473\n",
      "Epoch [467/1500], Training Loss: 3.494835565454306, Validation Loss: 10.092239379882812\n",
      "Epoch [468/1500], Training Loss: 3.4676953247387576, Validation Loss: 10.032758712768555\n",
      "Epoch [469/1500], Training Loss: 3.4408414794927267, Validation Loss: 9.973196029663086\n",
      "Epoch [470/1500], Training Loss: 3.414285163905705, Validation Loss: 9.913782119750977\n",
      "Epoch [471/1500], Training Loss: 3.3880160059772755, Validation Loss: 9.854191780090332\n",
      "Epoch [472/1500], Training Loss: 3.362107968748395, Validation Loss: 9.794837951660156\n",
      "Epoch [473/1500], Training Loss: 3.3365345219493627, Validation Loss: 9.735488891601562\n",
      "Epoch [474/1500], Training Loss: 3.311345118911827, Validation Loss: 9.676532745361328\n",
      "Epoch [475/1500], Training Loss: 3.2864361615170394, Validation Loss: 9.617788314819336\n",
      "Epoch [476/1500], Training Loss: 3.2618476775215366, Validation Loss: 9.55903434753418\n",
      "Epoch [477/1500], Training Loss: 3.2375211360123393, Validation Loss: 9.500598907470703\n",
      "Epoch [478/1500], Training Loss: 3.2134868209069674, Validation Loss: 9.442387580871582\n",
      "Epoch [479/1500], Training Loss: 3.1897375775552375, Validation Loss: 9.384401321411133\n",
      "Epoch [480/1500], Training Loss: 3.166272289598781, Validation Loss: 9.326512336730957\n",
      "Epoch [481/1500], Training Loss: 3.1430955193478884, Validation Loss: 9.269002914428711\n",
      "Epoch [482/1500], Training Loss: 3.1202323360170916, Validation Loss: 9.211817741394043\n",
      "Epoch [483/1500], Training Loss: 3.097671811922696, Validation Loss: 9.154956817626953\n",
      "Epoch [484/1500], Training Loss: 3.0754184276821164, Validation Loss: 9.098340034484863\n",
      "Epoch [485/1500], Training Loss: 3.05337894410375, Validation Loss: 9.042116165161133\n",
      "Epoch [486/1500], Training Loss: 3.031647156654239, Validation Loss: 8.986260414123535\n",
      "Epoch [487/1500], Training Loss: 3.0102772583457833, Validation Loss: 8.930859565734863\n",
      "Epoch [488/1500], Training Loss: 2.98911724387851, Validation Loss: 8.875598907470703\n",
      "Epoch [489/1500], Training Loss: 2.968257885587823, Validation Loss: 8.82097053527832\n",
      "Epoch [490/1500], Training Loss: 2.9476479964659967, Validation Loss: 8.766657829284668\n",
      "Epoch [491/1500], Training Loss: 2.927255115495983, Validation Loss: 8.71258544921875\n",
      "Epoch [492/1500], Training Loss: 2.9071520648374736, Validation Loss: 8.659156799316406\n",
      "Epoch [493/1500], Training Loss: 2.8872759970153847, Validation Loss: 8.605979919433594\n",
      "Epoch [494/1500], Training Loss: 2.8675990307280923, Validation Loss: 8.553281784057617\n",
      "Epoch [495/1500], Training Loss: 2.8482189175433588, Validation Loss: 8.501069068908691\n",
      "Epoch [496/1500], Training Loss: 2.829089030622784, Validation Loss: 8.449263572692871\n",
      "Epoch [497/1500], Training Loss: 2.8101655975636417, Validation Loss: 8.39785099029541\n",
      "Epoch [498/1500], Training Loss: 2.791546969757038, Validation Loss: 8.346892356872559\n",
      "Epoch [499/1500], Training Loss: 2.773144066052199, Validation Loss: 8.296611785888672\n",
      "Epoch [500/1500], Training Loss: 2.7549958077640535, Validation Loss: 8.246636390686035\n",
      "Epoch [501/1500], Training Loss: 2.737066659479076, Validation Loss: 8.196969985961914\n",
      "Epoch [502/1500], Training Loss: 2.7193426405472456, Validation Loss: 8.14775562286377\n",
      "Epoch [503/1500], Training Loss: 2.7018961868546887, Validation Loss: 8.099117279052734\n",
      "Epoch [504/1500], Training Loss: 2.684688238007357, Validation Loss: 8.050981521606445\n",
      "Epoch [505/1500], Training Loss: 2.6677331179345485, Validation Loss: 8.003158569335938\n",
      "Epoch [506/1500], Training Loss: 2.6509988587817714, Validation Loss: 7.955977439880371\n",
      "Epoch [507/1500], Training Loss: 2.6345172245346444, Validation Loss: 7.909415245056152\n",
      "Epoch [508/1500], Training Loss: 2.6182998811494667, Validation Loss: 7.863196849822998\n",
      "Epoch [509/1500], Training Loss: 2.6022960108821485, Validation Loss: 7.817532539367676\n",
      "Epoch [510/1500], Training Loss: 2.5865028743070697, Validation Loss: 7.772330284118652\n",
      "Epoch [511/1500], Training Loss: 2.5709308362000836, Validation Loss: 7.727663516998291\n",
      "Epoch [512/1500], Training Loss: 2.555594814800585, Validation Loss: 7.6833930015563965\n",
      "Epoch [513/1500], Training Loss: 2.5404567446493553, Validation Loss: 7.639543533325195\n",
      "Epoch [514/1500], Training Loss: 2.525489472514994, Validation Loss: 7.596106052398682\n",
      "Epoch [515/1500], Training Loss: 2.5107090613099774, Validation Loss: 7.553081035614014\n",
      "Epoch [516/1500], Training Loss: 2.4961081494895616, Validation Loss: 7.5105719566345215\n",
      "Epoch [517/1500], Training Loss: 2.481724245556905, Validation Loss: 7.468472957611084\n",
      "Epoch [518/1500], Training Loss: 2.467525214647251, Validation Loss: 7.426907062530518\n",
      "Epoch [519/1500], Training Loss: 2.4535498986078097, Validation Loss: 7.38577127456665\n",
      "Epoch [520/1500], Training Loss: 2.4397752891294355, Validation Loss: 7.345139503479004\n",
      "Epoch [521/1500], Training Loss: 2.4261700507493007, Validation Loss: 7.304905414581299\n",
      "Epoch [522/1500], Training Loss: 2.4127381450281407, Validation Loss: 7.265180587768555\n",
      "Epoch [523/1500], Training Loss: 2.399464026425451, Validation Loss: 7.225824356079102\n",
      "Epoch [524/1500], Training Loss: 2.386362635185937, Validation Loss: 7.186919689178467\n",
      "Epoch [525/1500], Training Loss: 2.37343475053411, Validation Loss: 7.148260116577148\n",
      "Epoch [526/1500], Training Loss: 2.360678553571116, Validation Loss: 7.110137939453125\n",
      "Epoch [527/1500], Training Loss: 2.3480754648795177, Validation Loss: 7.072402477264404\n",
      "Epoch [528/1500], Training Loss: 2.335624429957859, Validation Loss: 7.034969329833984\n",
      "Epoch [529/1500], Training Loss: 2.323369671609662, Validation Loss: 6.998049736022949\n",
      "Epoch [530/1500], Training Loss: 2.3112848836264277, Validation Loss: 6.961657524108887\n",
      "Epoch [531/1500], Training Loss: 2.2993190672540984, Validation Loss: 6.925667762756348\n",
      "Epoch [532/1500], Training Loss: 2.287523527082478, Validation Loss: 6.889966011047363\n",
      "Epoch [533/1500], Training Loss: 2.2758699709777583, Validation Loss: 6.854909420013428\n",
      "Epoch [534/1500], Training Loss: 2.2643703839368854, Validation Loss: 6.820165634155273\n",
      "Epoch [535/1500], Training Loss: 2.253055580305922, Validation Loss: 6.785931587219238\n",
      "Epoch [536/1500], Training Loss: 2.241919634429215, Validation Loss: 6.752315998077393\n",
      "Epoch [537/1500], Training Loss: 2.230897959422568, Validation Loss: 6.718808174133301\n",
      "Epoch [538/1500], Training Loss: 2.2200052159146964, Validation Loss: 6.6858720779418945\n",
      "Epoch [539/1500], Training Loss: 2.2092745879402913, Validation Loss: 6.653285503387451\n",
      "Epoch [540/1500], Training Loss: 2.1986449266453545, Validation Loss: 6.621053218841553\n",
      "Epoch [541/1500], Training Loss: 2.1881297646256352, Validation Loss: 6.589418411254883\n",
      "Epoch [542/1500], Training Loss: 2.177759036141005, Validation Loss: 6.558196067810059\n",
      "Epoch [543/1500], Training Loss: 2.1675365980820978, Validation Loss: 6.52730655670166\n",
      "Epoch [544/1500], Training Loss: 2.157426463037768, Validation Loss: 6.496804714202881\n",
      "Epoch [545/1500], Training Loss: 2.1474259730305993, Validation Loss: 6.466757774353027\n",
      "Epoch [546/1500], Training Loss: 2.1375673365621966, Validation Loss: 6.437069416046143\n",
      "Epoch [547/1500], Training Loss: 2.1278221626765235, Validation Loss: 6.40779447555542\n",
      "Epoch [548/1500], Training Loss: 2.118206763029244, Validation Loss: 6.378986358642578\n",
      "Epoch [549/1500], Training Loss: 2.1087100004268247, Validation Loss: 6.350567817687988\n",
      "Epoch [550/1500], Training Loss: 2.099306504325245, Validation Loss: 6.322638034820557\n",
      "Epoch [551/1500], Training Loss: 2.08999170257842, Validation Loss: 6.294961929321289\n",
      "Epoch [552/1500], Training Loss: 2.080786548830077, Validation Loss: 6.267727375030518\n",
      "Epoch [553/1500], Training Loss: 2.071674508531534, Validation Loss: 6.240828514099121\n",
      "Epoch [554/1500], Training Loss: 2.0626708920256305, Validation Loss: 6.214397430419922\n",
      "Epoch [555/1500], Training Loss: 2.0537154740765193, Validation Loss: 6.188299655914307\n",
      "Epoch [556/1500], Training Loss: 2.0448809276432267, Validation Loss: 6.162515640258789\n",
      "Epoch [557/1500], Training Loss: 2.036128035282746, Validation Loss: 6.1371870040893555\n",
      "Epoch [558/1500], Training Loss: 2.027484052185598, Validation Loss: 6.112303256988525\n",
      "Epoch [559/1500], Training Loss: 2.018925449126378, Validation Loss: 6.087713718414307\n",
      "Epoch [560/1500], Training Loss: 2.010479544456832, Validation Loss: 6.063544750213623\n",
      "Epoch [561/1500], Training Loss: 2.0021061553396633, Validation Loss: 6.039655685424805\n",
      "Epoch [562/1500], Training Loss: 1.9938073990541787, Validation Loss: 6.0161261558532715\n",
      "Epoch [563/1500], Training Loss: 1.9856058059105708, Validation Loss: 5.9930195808410645\n",
      "Epoch [564/1500], Training Loss: 1.9774659647266641, Validation Loss: 5.970212936401367\n",
      "Epoch [565/1500], Training Loss: 1.9694148466708468, Validation Loss: 5.9477715492248535\n",
      "Epoch [566/1500], Training Loss: 1.9614544249378478, Validation Loss: 5.925790786743164\n",
      "Epoch [567/1500], Training Loss: 1.9535984914977864, Validation Loss: 5.904078006744385\n",
      "Epoch [568/1500], Training Loss: 1.9458267519293002, Validation Loss: 5.882580757141113\n",
      "Epoch [569/1500], Training Loss: 1.9381169907127276, Validation Loss: 5.8615264892578125\n",
      "Epoch [570/1500], Training Loss: 1.9304772571250404, Validation Loss: 5.84074592590332\n",
      "Epoch [571/1500], Training Loss: 1.9229264308033918, Validation Loss: 5.820279598236084\n",
      "Epoch [572/1500], Training Loss: 1.9154283977786608, Validation Loss: 5.800181865692139\n",
      "Epoch [573/1500], Training Loss: 1.9079701675327607, Validation Loss: 5.7802534103393555\n",
      "Epoch [574/1500], Training Loss: 1.9006110021918414, Validation Loss: 5.7607011795043945\n",
      "Epoch [575/1500], Training Loss: 1.8933452310803682, Validation Loss: 5.74151611328125\n",
      "Epoch [576/1500], Training Loss: 1.886131162743701, Validation Loss: 5.722566604614258\n",
      "Epoch [577/1500], Training Loss: 1.8789738449351796, Validation Loss: 5.703866481781006\n",
      "Epoch [578/1500], Training Loss: 1.8718898205556544, Validation Loss: 5.6856489181518555\n",
      "Epoch [579/1500], Training Loss: 1.86487599083995, Validation Loss: 5.667591571807861\n",
      "Epoch [580/1500], Training Loss: 1.8579222311254868, Validation Loss: 5.6497979164123535\n",
      "Epoch [581/1500], Training Loss: 1.8510518840344858, Validation Loss: 5.632359981536865\n",
      "Epoch [582/1500], Training Loss: 1.8442628901241453, Validation Loss: 5.6151123046875\n",
      "Epoch [583/1500], Training Loss: 1.8375183217485156, Validation Loss: 5.598172664642334\n",
      "Epoch [584/1500], Training Loss: 1.8308277252875167, Validation Loss: 5.5814995765686035\n",
      "Epoch [585/1500], Training Loss: 1.8242193796566428, Validation Loss: 5.565056800842285\n",
      "Epoch [586/1500], Training Loss: 1.8176504619838654, Validation Loss: 5.548799991607666\n",
      "Epoch [587/1500], Training Loss: 1.8111407875675616, Validation Loss: 5.53289270401001\n",
      "Epoch [588/1500], Training Loss: 1.8046988931974584, Validation Loss: 5.517168998718262\n",
      "Epoch [589/1500], Training Loss: 1.7983095825599928, Validation Loss: 5.501742839813232\n",
      "Epoch [590/1500], Training Loss: 1.7919694677356819, Validation Loss: 5.486541748046875\n",
      "Epoch [591/1500], Training Loss: 1.7856934028619313, Validation Loss: 5.471579551696777\n",
      "Epoch [592/1500], Training Loss: 1.7794549331716112, Validation Loss: 5.456761360168457\n",
      "Epoch [593/1500], Training Loss: 1.7732467959556795, Validation Loss: 5.442154884338379\n",
      "Epoch [594/1500], Training Loss: 1.7670753604514635, Validation Loss: 5.42775297164917\n",
      "Epoch [595/1500], Training Loss: 1.7609513350559225, Validation Loss: 5.413604259490967\n",
      "Epoch [596/1500], Training Loss: 1.75489908551968, Validation Loss: 5.399616718292236\n",
      "Epoch [597/1500], Training Loss: 1.7488889178443903, Validation Loss: 5.385934829711914\n",
      "Epoch [598/1500], Training Loss: 1.7429399861032373, Validation Loss: 5.372345924377441\n",
      "Epoch [599/1500], Training Loss: 1.7370602464408103, Validation Loss: 5.359028339385986\n",
      "Epoch [600/1500], Training Loss: 1.7312336612559642, Validation Loss: 5.345832347869873\n",
      "Epoch [601/1500], Training Loss: 1.7254834211569852, Validation Loss: 5.3328633308410645\n",
      "Epoch [602/1500], Training Loss: 1.7197685874425668, Validation Loss: 5.319933891296387\n",
      "Epoch [603/1500], Training Loss: 1.7140730154575985, Validation Loss: 5.307056427001953\n",
      "Epoch [604/1500], Training Loss: 1.7084116537359906, Validation Loss: 5.294301509857178\n",
      "Epoch [605/1500], Training Loss: 1.7027774309979806, Validation Loss: 5.281783103942871\n",
      "Epoch [606/1500], Training Loss: 1.697191011954386, Validation Loss: 5.269222736358643\n",
      "Epoch [607/1500], Training Loss: 1.6916421455523853, Validation Loss: 5.256824493408203\n",
      "Epoch [608/1500], Training Loss: 1.6861426176665262, Validation Loss: 5.244478225708008\n",
      "Epoch [609/1500], Training Loss: 1.6806810006103599, Validation Loss: 5.232204914093018\n",
      "Epoch [610/1500], Training Loss: 1.6752516800218797, Validation Loss: 5.2200493812561035\n",
      "Epoch [611/1500], Training Loss: 1.6698458909175897, Validation Loss: 5.2077813148498535\n",
      "Epoch [612/1500], Training Loss: 1.6644993800836578, Validation Loss: 5.195552825927734\n",
      "Epoch [613/1500], Training Loss: 1.6592106286201946, Validation Loss: 5.183380126953125\n",
      "Epoch [614/1500], Training Loss: 1.653962251559405, Validation Loss: 5.171145915985107\n",
      "Epoch [615/1500], Training Loss: 1.6487332067784373, Validation Loss: 5.158906936645508\n",
      "Epoch [616/1500], Training Loss: 1.6435695808495008, Validation Loss: 5.146509170532227\n",
      "Epoch [617/1500], Training Loss: 1.6384168276134612, Validation Loss: 5.134137153625488\n",
      "Epoch [618/1500], Training Loss: 1.6333101731888104, Validation Loss: 5.121735095977783\n",
      "Epoch [619/1500], Training Loss: 1.6282310197504655, Validation Loss: 5.109198093414307\n",
      "Epoch [620/1500], Training Loss: 1.6231819254288093, Validation Loss: 5.096531867980957\n",
      "Epoch [621/1500], Training Loss: 1.6181487104343342, Validation Loss: 5.083614826202393\n",
      "Epoch [622/1500], Training Loss: 1.6131342346760624, Validation Loss: 5.070664405822754\n",
      "Epoch [623/1500], Training Loss: 1.6081253641854516, Validation Loss: 5.05756139755249\n",
      "Epoch [624/1500], Training Loss: 1.6031590315438757, Validation Loss: 5.0444159507751465\n",
      "Epoch [625/1500], Training Loss: 1.5982168046357694, Validation Loss: 5.030863285064697\n",
      "Epoch [626/1500], Training Loss: 1.5932887637748212, Validation Loss: 5.01730489730835\n",
      "Epoch [627/1500], Training Loss: 1.5883940103928156, Validation Loss: 5.003588676452637\n",
      "Epoch [628/1500], Training Loss: 1.583515963463154, Validation Loss: 4.989688873291016\n",
      "Epoch [629/1500], Training Loss: 1.5786581261887522, Validation Loss: 4.97587776184082\n",
      "Epoch [630/1500], Training Loss: 1.5738103399754158, Validation Loss: 4.961822986602783\n",
      "Epoch [631/1500], Training Loss: 1.5689909922124505, Validation Loss: 4.947935104370117\n",
      "Epoch [632/1500], Training Loss: 1.5641841146330577, Validation Loss: 4.933708190917969\n",
      "Epoch [633/1500], Training Loss: 1.5594067654950554, Validation Loss: 4.920041561126709\n",
      "Epoch [634/1500], Training Loss: 1.5546632406926613, Validation Loss: 4.906458377838135\n",
      "Epoch [635/1500], Training Loss: 1.5499309579317608, Validation Loss: 4.893035888671875\n",
      "Epoch [636/1500], Training Loss: 1.5452440286604974, Validation Loss: 4.88005256652832\n",
      "Epoch [637/1500], Training Loss: 1.5405925322582772, Validation Loss: 4.86751127243042\n",
      "Epoch [638/1500], Training Loss: 1.5359717769715115, Validation Loss: 4.855719566345215\n",
      "Epoch [639/1500], Training Loss: 1.5313995509971832, Validation Loss: 4.844399929046631\n",
      "Epoch [640/1500], Training Loss: 1.5268702778698298, Validation Loss: 4.833972930908203\n",
      "Epoch [641/1500], Training Loss: 1.5223534505512588, Validation Loss: 4.824581146240234\n",
      "Epoch [642/1500], Training Loss: 1.5179061882426932, Validation Loss: 4.815820217132568\n",
      "Epoch [643/1500], Training Loss: 1.5135357822189985, Validation Loss: 4.807844638824463\n",
      "Epoch [644/1500], Training Loss: 1.5092263683204374, Validation Loss: 4.800829887390137\n",
      "Epoch [645/1500], Training Loss: 1.5049556256339005, Validation Loss: 4.794033050537109\n",
      "Epoch [646/1500], Training Loss: 1.5007327612547596, Validation Loss: 4.7878031730651855\n",
      "Epoch [647/1500], Training Loss: 1.4965529113624905, Validation Loss: 4.7823381423950195\n",
      "Epoch [648/1500], Training Loss: 1.4924211429041785, Validation Loss: 4.7771897315979\n",
      "Epoch [649/1500], Training Loss: 1.4883164438758834, Validation Loss: 4.771896839141846\n",
      "Epoch [650/1500], Training Loss: 1.4842709752509282, Validation Loss: 4.767303466796875\n",
      "Epoch [651/1500], Training Loss: 1.4802671214351362, Validation Loss: 4.76280403137207\n",
      "Epoch [652/1500], Training Loss: 1.4763103204693162, Validation Loss: 4.75834846496582\n",
      "Epoch [653/1500], Training Loss: 1.4723767652507078, Validation Loss: 4.754400253295898\n",
      "Epoch [654/1500], Training Loss: 1.4684823715915523, Validation Loss: 4.750248908996582\n",
      "Epoch [655/1500], Training Loss: 1.464613225802277, Validation Loss: 4.74657678604126\n",
      "Epoch [656/1500], Training Loss: 1.4607933989013904, Validation Loss: 4.742962837219238\n",
      "Epoch [657/1500], Training Loss: 1.4570088993079864, Validation Loss: 4.739682197570801\n",
      "Epoch [658/1500], Training Loss: 1.4532704699482768, Validation Loss: 4.736523151397705\n",
      "Epoch [659/1500], Training Loss: 1.4495500453672325, Validation Loss: 4.732794761657715\n",
      "Epoch [660/1500], Training Loss: 1.4458634288806202, Validation Loss: 4.729464054107666\n",
      "Epoch [661/1500], Training Loss: 1.4422077993423454, Validation Loss: 4.726002216339111\n",
      "Epoch [662/1500], Training Loss: 1.438589406783364, Validation Loss: 4.722550392150879\n",
      "Epoch [663/1500], Training Loss: 1.4349911644316706, Validation Loss: 4.718695640563965\n",
      "Epoch [664/1500], Training Loss: 1.4314096682177755, Validation Loss: 4.714685440063477\n",
      "Epoch [665/1500], Training Loss: 1.4278573840651894, Validation Loss: 4.710691928863525\n",
      "Epoch [666/1500], Training Loss: 1.4243290989347723, Validation Loss: 4.706302165985107\n",
      "Epoch [667/1500], Training Loss: 1.420829328331376, Validation Loss: 4.701265811920166\n",
      "Epoch [668/1500], Training Loss: 1.4173563299963499, Validation Loss: 4.695885181427002\n",
      "Epoch [669/1500], Training Loss: 1.4138966705018994, Validation Loss: 4.690207004547119\n",
      "Epoch [670/1500], Training Loss: 1.4104878068962303, Validation Loss: 4.684327602386475\n",
      "Epoch [671/1500], Training Loss: 1.4071051992854815, Validation Loss: 4.67791748046875\n",
      "Epoch [672/1500], Training Loss: 1.4037502120680923, Validation Loss: 4.671035289764404\n",
      "Epoch [673/1500], Training Loss: 1.4004101767488644, Validation Loss: 4.663721561431885\n",
      "Epoch [674/1500], Training Loss: 1.3970998296356276, Validation Loss: 4.656404972076416\n",
      "Epoch [675/1500], Training Loss: 1.3938160122352174, Validation Loss: 4.648360252380371\n",
      "Epoch [676/1500], Training Loss: 1.3905548415336775, Validation Loss: 4.639986038208008\n",
      "Epoch [677/1500], Training Loss: 1.3873265632755496, Validation Loss: 4.631180286407471\n",
      "Epoch [678/1500], Training Loss: 1.3841171120988478, Validation Loss: 4.622091293334961\n",
      "Epoch [679/1500], Training Loss: 1.380940653850087, Validation Loss: 4.612641334533691\n",
      "Epoch [680/1500], Training Loss: 1.377789689150947, Validation Loss: 4.602875232696533\n",
      "Epoch [681/1500], Training Loss: 1.374660066610748, Validation Loss: 4.592753887176514\n",
      "Epoch [682/1500], Training Loss: 1.3715496878848976, Validation Loss: 4.582556247711182\n",
      "Epoch [683/1500], Training Loss: 1.3684620755044956, Validation Loss: 4.572007656097412\n",
      "Epoch [684/1500], Training Loss: 1.3653924824528494, Validation Loss: 4.561055660247803\n",
      "Epoch [685/1500], Training Loss: 1.362334279345806, Validation Loss: 4.550120830535889\n",
      "Epoch [686/1500], Training Loss: 1.3593253487234196, Validation Loss: 4.538754940032959\n",
      "Epoch [687/1500], Training Loss: 1.356325066022169, Validation Loss: 4.527076244354248\n",
      "Epoch [688/1500], Training Loss: 1.353350392424588, Validation Loss: 4.515536785125732\n",
      "Epoch [689/1500], Training Loss: 1.3504039129818604, Validation Loss: 4.504065036773682\n",
      "Epoch [690/1500], Training Loss: 1.3474958331950628, Validation Loss: 4.492157936096191\n",
      "Epoch [691/1500], Training Loss: 1.3446057307215125, Validation Loss: 4.480323314666748\n",
      "Epoch [692/1500], Training Loss: 1.341747243896623, Validation Loss: 4.468442440032959\n",
      "Epoch [693/1500], Training Loss: 1.338904881095783, Validation Loss: 4.45626163482666\n",
      "Epoch [694/1500], Training Loss: 1.336089390689466, Validation Loss: 4.444273948669434\n",
      "Epoch [695/1500], Training Loss: 1.3332989499157044, Validation Loss: 4.432060718536377\n",
      "Epoch [696/1500], Training Loss: 1.3305233447207094, Validation Loss: 4.419809341430664\n",
      "Epoch [697/1500], Training Loss: 1.3277725915573175, Validation Loss: 4.407761573791504\n",
      "Epoch [698/1500], Training Loss: 1.3250562047137835, Validation Loss: 4.395822525024414\n",
      "Epoch [699/1500], Training Loss: 1.3223622710467984, Validation Loss: 4.383420944213867\n",
      "Epoch [700/1500], Training Loss: 1.3196794361168762, Validation Loss: 4.3711113929748535\n",
      "Epoch [701/1500], Training Loss: 1.3170129006041822, Validation Loss: 4.358975887298584\n",
      "Epoch [702/1500], Training Loss: 1.3143547138269078, Validation Loss: 4.347257614135742\n",
      "Epoch [703/1500], Training Loss: 1.3117312569668704, Validation Loss: 4.335233211517334\n",
      "Epoch [704/1500], Training Loss: 1.3091270479922923, Validation Loss: 4.32308292388916\n",
      "Epoch [705/1500], Training Loss: 1.306562371036701, Validation Loss: 4.311500072479248\n",
      "Epoch [706/1500], Training Loss: 1.304011946780879, Validation Loss: 4.299999237060547\n",
      "Epoch [707/1500], Training Loss: 1.3014720672761562, Validation Loss: 4.288388252258301\n",
      "Epoch [708/1500], Training Loss: 1.2989495545721126, Validation Loss: 4.276853084564209\n",
      "Epoch [709/1500], Training Loss: 1.296456472294602, Validation Loss: 4.265552043914795\n",
      "Epoch [710/1500], Training Loss: 1.293983166689018, Validation Loss: 4.254345893859863\n",
      "Epoch [711/1500], Training Loss: 1.2915285697056205, Validation Loss: 4.242952823638916\n",
      "Epoch [712/1500], Training Loss: 1.2891066936427302, Validation Loss: 4.232029438018799\n",
      "Epoch [713/1500], Training Loss: 1.2866996917714548, Validation Loss: 4.22114372253418\n",
      "Epoch [714/1500], Training Loss: 1.2843093955807672, Validation Loss: 4.2100830078125\n",
      "Epoch [715/1500], Training Loss: 1.2819346130380715, Validation Loss: 4.199653625488281\n",
      "Epoch [716/1500], Training Loss: 1.2795916405344074, Validation Loss: 4.188992977142334\n",
      "Epoch [717/1500], Training Loss: 1.2772684876682199, Validation Loss: 4.178748607635498\n",
      "Epoch [718/1500], Training Loss: 1.2749640347782736, Validation Loss: 4.1686811447143555\n",
      "Epoch [719/1500], Training Loss: 1.2726611179740122, Validation Loss: 4.15842866897583\n",
      "Epoch [720/1500], Training Loss: 1.2703651449208906, Validation Loss: 4.148331165313721\n",
      "Epoch [721/1500], Training Loss: 1.2680961641820088, Validation Loss: 4.138299465179443\n",
      "Epoch [722/1500], Training Loss: 1.2658570174903727, Validation Loss: 4.12883186340332\n",
      "Epoch [723/1500], Training Loss: 1.2636169783088367, Validation Loss: 4.119083404541016\n",
      "Epoch [724/1500], Training Loss: 1.2614156864614983, Validation Loss: 4.109760284423828\n",
      "Epoch [725/1500], Training Loss: 1.2592300003741812, Validation Loss: 4.100714206695557\n",
      "Epoch [726/1500], Training Loss: 1.2570703792339815, Validation Loss: 4.091412544250488\n",
      "Epoch [727/1500], Training Loss: 1.2549338956086376, Validation Loss: 4.082557678222656\n",
      "Epoch [728/1500], Training Loss: 1.252821501787553, Validation Loss: 4.073608875274658\n",
      "Epoch [729/1500], Training Loss: 1.250716063716732, Validation Loss: 4.065037250518799\n",
      "Epoch [730/1500], Training Loss: 1.2486177131949225, Validation Loss: 4.056591510772705\n",
      "Epoch [731/1500], Training Loss: 1.246539507741709, Validation Loss: 4.047930717468262\n",
      "Epoch [732/1500], Training Loss: 1.24446979373786, Validation Loss: 4.039418697357178\n",
      "Epoch [733/1500], Training Loss: 1.2424104827175217, Validation Loss: 4.031115531921387\n",
      "Epoch [734/1500], Training Loss: 1.2403746360294488, Validation Loss: 4.023197650909424\n",
      "Epoch [735/1500], Training Loss: 1.2383497811294453, Validation Loss: 4.015122413635254\n",
      "Epoch [736/1500], Training Loss: 1.2363417246936814, Validation Loss: 4.007216453552246\n",
      "Epoch [737/1500], Training Loss: 1.2343309872942818, Validation Loss: 3.9997570514678955\n",
      "Epoch [738/1500], Training Loss: 1.2323430768232022, Validation Loss: 3.991956949234009\n",
      "Epoch [739/1500], Training Loss: 1.23037606869044, Validation Loss: 3.9843173027038574\n",
      "Epoch [740/1500], Training Loss: 1.2284186032316893, Validation Loss: 3.97691011428833\n",
      "Epoch [741/1500], Training Loss: 1.2264754640163709, Validation Loss: 3.9698264598846436\n",
      "Epoch [742/1500], Training Loss: 1.224549804177306, Validation Loss: 3.9625070095062256\n",
      "Epoch [743/1500], Training Loss: 1.2226313965102247, Validation Loss: 3.9554083347320557\n",
      "Epoch [744/1500], Training Loss: 1.220732944309672, Validation Loss: 3.9483981132507324\n",
      "Epoch [745/1500], Training Loss: 1.21884009109278, Validation Loss: 3.9417166709899902\n",
      "Epoch [746/1500], Training Loss: 1.216959459768675, Validation Loss: 3.935021162033081\n",
      "Epoch [747/1500], Training Loss: 1.2150904524834005, Validation Loss: 3.9286692142486572\n",
      "Epoch [748/1500], Training Loss: 1.2132360259340738, Validation Loss: 3.9222137928009033\n",
      "Epoch [749/1500], Training Loss: 1.2113899726535182, Validation Loss: 3.9157614707946777\n",
      "Epoch [750/1500], Training Loss: 1.209554221404345, Validation Loss: 3.909520387649536\n",
      "Epoch [751/1500], Training Loss: 1.2077308072428081, Validation Loss: 3.903489112854004\n",
      "Epoch [752/1500], Training Loss: 1.2059180938144634, Validation Loss: 3.897481918334961\n",
      "Epoch [753/1500], Training Loss: 1.2041182635116081, Validation Loss: 3.8915247917175293\n",
      "Epoch [754/1500], Training Loss: 1.2023190724994353, Validation Loss: 3.8855538368225098\n",
      "Epoch [755/1500], Training Loss: 1.2005372952565114, Validation Loss: 3.8797271251678467\n",
      "Epoch [756/1500], Training Loss: 1.1987640937540862, Validation Loss: 3.8741612434387207\n",
      "Epoch [757/1500], Training Loss: 1.1970005074129297, Validation Loss: 3.8687562942504883\n",
      "Epoch [758/1500], Training Loss: 1.195257445766865, Validation Loss: 3.863333225250244\n",
      "Epoch [759/1500], Training Loss: 1.1935270330992451, Validation Loss: 3.858100652694702\n",
      "Epoch [760/1500], Training Loss: 1.1918163812044233, Validation Loss: 3.8527302742004395\n",
      "Epoch [761/1500], Training Loss: 1.190112076104052, Validation Loss: 3.8478121757507324\n",
      "Epoch [762/1500], Training Loss: 1.1884248714865033, Validation Loss: 3.842715263366699\n",
      "Epoch [763/1500], Training Loss: 1.1867579520362588, Validation Loss: 3.8377230167388916\n",
      "Epoch [764/1500], Training Loss: 1.1851042374686507, Validation Loss: 3.832827091217041\n",
      "Epoch [765/1500], Training Loss: 1.1834641992863173, Validation Loss: 3.827895164489746\n",
      "Epoch [766/1500], Training Loss: 1.1818343028375708, Validation Loss: 3.8232691287994385\n",
      "Epoch [767/1500], Training Loss: 1.1802171720998256, Validation Loss: 3.818683624267578\n",
      "Epoch [768/1500], Training Loss: 1.1786150131858848, Validation Loss: 3.8140010833740234\n",
      "Epoch [769/1500], Training Loss: 1.177016160293629, Validation Loss: 3.809657573699951\n",
      "Epoch [770/1500], Training Loss: 1.1754330804761994, Validation Loss: 3.805396795272827\n",
      "Epoch [771/1500], Training Loss: 1.1738543819111311, Validation Loss: 3.80104923248291\n",
      "Epoch [772/1500], Training Loss: 1.1722941098567314, Validation Loss: 3.796762704849243\n",
      "Epoch [773/1500], Training Loss: 1.1707383840775354, Validation Loss: 3.793039321899414\n",
      "Epoch [774/1500], Training Loss: 1.1691962762249382, Validation Loss: 3.788874626159668\n",
      "Epoch [775/1500], Training Loss: 1.1676726981045624, Validation Loss: 3.7850072383880615\n",
      "Epoch [776/1500], Training Loss: 1.1661564959618544, Validation Loss: 3.781050443649292\n",
      "Epoch [777/1500], Training Loss: 1.164646874346628, Validation Loss: 3.7774720191955566\n",
      "Epoch [778/1500], Training Loss: 1.1631633384372213, Validation Loss: 3.7736966609954834\n",
      "Epoch [779/1500], Training Loss: 1.1616941281696564, Validation Loss: 3.7699549198150635\n",
      "Epoch [780/1500], Training Loss: 1.1602246059534798, Validation Loss: 3.7667458057403564\n",
      "Epoch [781/1500], Training Loss: 1.1587707868770694, Validation Loss: 3.763359308242798\n",
      "Epoch [782/1500], Training Loss: 1.1573306333217228, Validation Loss: 3.759964942932129\n",
      "Epoch [783/1500], Training Loss: 1.1559044323367003, Validation Loss: 3.756815195083618\n",
      "Epoch [784/1500], Training Loss: 1.1544953532874134, Validation Loss: 3.7535619735717773\n",
      "Epoch [785/1500], Training Loss: 1.1530993786414228, Validation Loss: 3.7504770755767822\n",
      "Epoch [786/1500], Training Loss: 1.1517071816884628, Validation Loss: 3.747445821762085\n",
      "Epoch [787/1500], Training Loss: 1.1503329832195557, Validation Loss: 3.7446787357330322\n",
      "Epoch [788/1500], Training Loss: 1.1489604210659463, Validation Loss: 3.7418322563171387\n",
      "Epoch [789/1500], Training Loss: 1.147597870920537, Validation Loss: 3.7388851642608643\n",
      "Epoch [790/1500], Training Loss: 1.1462438989523396, Validation Loss: 3.7358994483947754\n",
      "Epoch [791/1500], Training Loss: 1.1448847168647178, Validation Loss: 3.7333009243011475\n",
      "Epoch [792/1500], Training Loss: 1.143540663587366, Validation Loss: 3.73045015335083\n",
      "Epoch [793/1500], Training Loss: 1.1422069635179808, Validation Loss: 3.7276387214660645\n",
      "Epoch [794/1500], Training Loss: 1.140880874039712, Validation Loss: 3.7250068187713623\n",
      "Epoch [795/1500], Training Loss: 1.1395632634803445, Validation Loss: 3.7226758003234863\n",
      "Epoch [796/1500], Training Loss: 1.1382515747178927, Validation Loss: 3.7201449871063232\n",
      "Epoch [797/1500], Training Loss: 1.1369531812449647, Validation Loss: 3.7177088260650635\n",
      "Epoch [798/1500], Training Loss: 1.1356567365137702, Validation Loss: 3.715362310409546\n",
      "Epoch [799/1500], Training Loss: 1.1343740691050155, Validation Loss: 3.7130396366119385\n",
      "Epoch [800/1500], Training Loss: 1.1331011605931043, Validation Loss: 3.710693836212158\n",
      "Epoch [801/1500], Training Loss: 1.1318314494513122, Validation Loss: 3.7083547115325928\n",
      "Epoch [802/1500], Training Loss: 1.1305676069911341, Validation Loss: 3.706174612045288\n",
      "Epoch [803/1500], Training Loss: 1.129305007628438, Validation Loss: 3.7043683528900146\n",
      "Epoch [804/1500], Training Loss: 1.1280643684401594, Validation Loss: 3.7022557258605957\n",
      "Epoch [805/1500], Training Loss: 1.1268329479179469, Validation Loss: 3.700198173522949\n",
      "Epoch [806/1500], Training Loss: 1.1256170503908762, Validation Loss: 3.6982686519622803\n",
      "Epoch [807/1500], Training Loss: 1.1244032164697986, Validation Loss: 3.696547269821167\n",
      "Epoch [808/1500], Training Loss: 1.1232008869572672, Validation Loss: 3.6946890354156494\n",
      "Epoch [809/1500], Training Loss: 1.1220008272553939, Validation Loss: 3.692915201187134\n",
      "Epoch [810/1500], Training Loss: 1.1208098861245468, Validation Loss: 3.6912152767181396\n",
      "Epoch [811/1500], Training Loss: 1.11963580112185, Validation Loss: 3.6895880699157715\n",
      "Epoch [812/1500], Training Loss: 1.1184688597151693, Validation Loss: 3.6880016326904297\n",
      "Epoch [813/1500], Training Loss: 1.1173113607724872, Validation Loss: 3.686563491821289\n",
      "Epoch [814/1500], Training Loss: 1.116159380223604, Validation Loss: 3.685002326965332\n",
      "Epoch [815/1500], Training Loss: 1.11501169015704, Validation Loss: 3.6835389137268066\n",
      "Epoch [816/1500], Training Loss: 1.1138767500286446, Validation Loss: 3.681886672973633\n",
      "Epoch [817/1500], Training Loss: 1.1127287707500553, Validation Loss: 3.6805319786071777\n",
      "Epoch [818/1500], Training Loss: 1.1115948638374702, Validation Loss: 3.6791293621063232\n",
      "Epoch [819/1500], Training Loss: 1.110462281853855, Validation Loss: 3.6777522563934326\n",
      "Epoch [820/1500], Training Loss: 1.109336597347418, Validation Loss: 3.6764371395111084\n",
      "Epoch [821/1500], Training Loss: 1.108218558272685, Validation Loss: 3.6750354766845703\n",
      "Epoch [822/1500], Training Loss: 1.1071068210399646, Validation Loss: 3.6741347312927246\n",
      "Epoch [823/1500], Training Loss: 1.1060035516008921, Validation Loss: 3.6729326248168945\n",
      "Epoch [824/1500], Training Loss: 1.1049120797798235, Validation Loss: 3.6718015670776367\n",
      "Epoch [825/1500], Training Loss: 1.1038286889010975, Validation Loss: 3.6709256172180176\n",
      "Epoch [826/1500], Training Loss: 1.1027581025773836, Validation Loss: 3.6699047088623047\n",
      "Epoch [827/1500], Training Loss: 1.101691609322149, Validation Loss: 3.6690850257873535\n",
      "Epoch [828/1500], Training Loss: 1.1006251861330045, Validation Loss: 3.6681723594665527\n",
      "Epoch [829/1500], Training Loss: 1.099567809561638, Validation Loss: 3.667402982711792\n",
      "Epoch [830/1500], Training Loss: 1.0985093665796741, Validation Loss: 3.666562080383301\n",
      "Epoch [831/1500], Training Loss: 1.0974667706883734, Validation Loss: 3.6657261848449707\n",
      "Epoch [832/1500], Training Loss: 1.0964348553506096, Validation Loss: 3.664872169494629\n",
      "Epoch [833/1500], Training Loss: 1.095405835023335, Validation Loss: 3.6641287803649902\n",
      "Epoch [834/1500], Training Loss: 1.0943795330008803, Validation Loss: 3.663393497467041\n",
      "Epoch [835/1500], Training Loss: 1.0933628546419012, Validation Loss: 3.662626028060913\n",
      "Epoch [836/1500], Training Loss: 1.0923552479848746, Validation Loss: 3.662177324295044\n",
      "Epoch [837/1500], Training Loss: 1.0913490970396744, Validation Loss: 3.6617026329040527\n",
      "Epoch [838/1500], Training Loss: 1.0903522460623145, Validation Loss: 3.6611974239349365\n",
      "Epoch [839/1500], Training Loss: 1.0893655509164488, Validation Loss: 3.6606783866882324\n",
      "Epoch [840/1500], Training Loss: 1.088373940523378, Validation Loss: 3.6602845191955566\n",
      "Epoch [841/1500], Training Loss: 1.0873963941435787, Validation Loss: 3.65981388092041\n",
      "Epoch [842/1500], Training Loss: 1.0864187369623772, Validation Loss: 3.6594173908233643\n",
      "Epoch [843/1500], Training Loss: 1.0854490373444832, Validation Loss: 3.6590890884399414\n",
      "Epoch [844/1500], Training Loss: 1.084475584520441, Validation Loss: 3.658827304840088\n",
      "Epoch [845/1500], Training Loss: 1.0835171841021876, Validation Loss: 3.6586482524871826\n",
      "Epoch [846/1500], Training Loss: 1.0825654773127538, Validation Loss: 3.658437967300415\n",
      "Epoch [847/1500], Training Loss: 1.0816081557687205, Validation Loss: 3.6582095623016357\n",
      "Epoch [848/1500], Training Loss: 1.0806593591542644, Validation Loss: 3.658095121383667\n",
      "Epoch [849/1500], Training Loss: 1.079714140208093, Validation Loss: 3.6580305099487305\n",
      "Epoch [850/1500], Training Loss: 1.0787726888306208, Validation Loss: 3.657992362976074\n",
      "Epoch [851/1500], Training Loss: 1.077833357403849, Validation Loss: 3.6581361293792725\n",
      "Epoch [852/1500], Training Loss: 1.0768955900160777, Validation Loss: 3.658311367034912\n",
      "Epoch [853/1500], Training Loss: 1.0759536276309924, Validation Loss: 3.658625602722168\n",
      "Epoch [854/1500], Training Loss: 1.0750278688473487, Validation Loss: 3.6588051319122314\n",
      "Epoch [855/1500], Training Loss: 1.0740935024983809, Validation Loss: 3.6590793132781982\n",
      "Epoch [856/1500], Training Loss: 1.0731680679888045, Validation Loss: 3.6595494747161865\n",
      "Epoch [857/1500], Training Loss: 1.0722470528671666, Validation Loss: 3.66009259223938\n",
      "Epoch [858/1500], Training Loss: 1.0713206867502274, Validation Loss: 3.6604955196380615\n",
      "Epoch [859/1500], Training Loss: 1.0704040587662564, Validation Loss: 3.6611807346343994\n",
      "Early stopping at epoch 859\n",
      "Final Test Loss: 3.6501095294952393\n",
      "Training model with target variable: low\n",
      "\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0003, window_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Training Loss: 27127.337133915556, Validation Loss: 26666.6796875\n",
      "Epoch [2/1500], Training Loss: 25431.52050930531, Validation Loss: 25123.58984375\n",
      "Epoch [3/1500], Training Loss: 24059.307048660845, Validation Loss: 23833.494140625\n",
      "Epoch [4/1500], Training Loss: 22935.944802061855, Validation Loss: 22777.708984375\n",
      "Epoch [5/1500], Training Loss: 21977.99566765365, Validation Loss: 21833.84375\n",
      "Epoch [6/1500], Training Loss: 21001.929491792416, Validation Loss: 20887.74609375\n",
      "Epoch [7/1500], Training Loss: 20084.7975334564, Validation Loss: 19982.294921875\n",
      "Epoch [8/1500], Training Loss: 19238.10773718441, Validation Loss: 19144.90625\n",
      "Epoch [9/1500], Training Loss: 18451.63924005573, Validation Loss: 18368.87109375\n",
      "Epoch [10/1500], Training Loss: 17717.03197906525, Validation Loss: 17645.03515625\n",
      "Epoch [11/1500], Training Loss: 17028.20943644479, Validation Loss: 16968.404296875\n",
      "Epoch [12/1500], Training Loss: 16380.780460223341, Validation Loss: 16334.9111328125\n",
      "Epoch [13/1500], Training Loss: 15772.257358145487, Validation Loss: 15737.7451171875\n",
      "Epoch [14/1500], Training Loss: 15201.196502714749, Validation Loss: 15167.92578125\n",
      "Epoch [15/1500], Training Loss: 14664.008291088043, Validation Loss: 14628.13671875\n",
      "Epoch [16/1500], Training Loss: 14157.228376099845, Validation Loss: 14123.9521484375\n",
      "Epoch [17/1500], Training Loss: 13678.592112626353, Validation Loss: 13685.234375\n",
      "Epoch [18/1500], Training Loss: 13223.808636910288, Validation Loss: 13242.1787109375\n",
      "Epoch [19/1500], Training Loss: 12792.265800104407, Validation Loss: 12814.2724609375\n",
      "Epoch [20/1500], Training Loss: 12383.149266325281, Validation Loss: 12403.134765625\n",
      "Epoch [21/1500], Training Loss: 11995.092508353771, Validation Loss: 12011.4755859375\n",
      "Epoch [22/1500], Training Loss: 11626.61515623696, Validation Loss: 11640.5400390625\n",
      "Epoch [23/1500], Training Loss: 11276.466039616, Validation Loss: 11291.9326171875\n",
      "Epoch [24/1500], Training Loss: 10943.793435827123, Validation Loss: 10964.318359375\n",
      "Epoch [25/1500], Training Loss: 10627.8844274304, Validation Loss: 10655.302734375\n",
      "Epoch [26/1500], Training Loss: 10328.095157610614, Validation Loss: 10363.50390625\n",
      "Epoch [27/1500], Training Loss: 10043.90114789725, Validation Loss: 10089.3134765625\n",
      "Epoch [28/1500], Training Loss: 9774.770636172341, Validation Loss: 9832.490234375\n",
      "Epoch [29/1500], Training Loss: 9520.106778762416, Validation Loss: 9595.947265625\n",
      "Epoch [30/1500], Training Loss: 9279.01807002024, Validation Loss: 9407.943359375\n",
      "Epoch [31/1500], Training Loss: 9039.474049602879, Validation Loss: 9557.5078125\n",
      "Epoch [32/1500], Training Loss: 8810.347336030587, Validation Loss: 9232.7314453125\n",
      "Epoch [33/1500], Training Loss: 8566.72701758785, Validation Loss: 8919.099609375\n",
      "Epoch [34/1500], Training Loss: 8320.869117696046, Validation Loss: 8670.4541015625\n",
      "Epoch [35/1500], Training Loss: 8087.932210045779, Validation Loss: 8445.7822265625\n",
      "Epoch [36/1500], Training Loss: 7862.079555698497, Validation Loss: 8182.3095703125\n",
      "Epoch [37/1500], Training Loss: 7644.517190207684, Validation Loss: 7911.6669921875\n",
      "Epoch [38/1500], Training Loss: 7437.951200831985, Validation Loss: 7665.216796875\n",
      "Epoch [39/1500], Training Loss: 7242.791318888812, Validation Loss: 7432.3759765625\n",
      "Epoch [40/1500], Training Loss: 7055.2367909859195, Validation Loss: 7209.46142578125\n",
      "Epoch [41/1500], Training Loss: 6873.189639470356, Validation Loss: 7001.3330078125\n",
      "Epoch [42/1500], Training Loss: 6696.528747327549, Validation Loss: 6807.06103515625\n",
      "Epoch [43/1500], Training Loss: 6525.100125379586, Validation Loss: 6623.060546875\n",
      "Epoch [44/1500], Training Loss: 6358.485267049929, Validation Loss: 6448.67822265625\n",
      "Epoch [45/1500], Training Loss: 6196.398104539427, Validation Loss: 6285.591796875\n",
      "Epoch [46/1500], Training Loss: 6038.695052803554, Validation Loss: 6137.26953125\n",
      "Epoch [47/1500], Training Loss: 5884.9275291827225, Validation Loss: 6001.2451171875\n",
      "Epoch [48/1500], Training Loss: 5734.303545711375, Validation Loss: 5839.20263671875\n",
      "Epoch [49/1500], Training Loss: 5588.109895784246, Validation Loss: 5678.3828125\n",
      "Epoch [50/1500], Training Loss: 5446.436877341246, Validation Loss: 5535.05615234375\n",
      "Epoch [51/1500], Training Loss: 5309.076712833761, Validation Loss: 5396.7314453125\n",
      "Epoch [52/1500], Training Loss: 5175.971885849261, Validation Loss: 5263.59814453125\n",
      "Epoch [53/1500], Training Loss: 5047.150261451986, Validation Loss: 5135.373046875\n",
      "Epoch [54/1500], Training Loss: 4922.381538714926, Validation Loss: 5011.75634765625\n",
      "Epoch [55/1500], Training Loss: 4801.230692989955, Validation Loss: 4892.06396484375\n",
      "Epoch [56/1500], Training Loss: 4683.203677752135, Validation Loss: 4775.60107421875\n",
      "Epoch [57/1500], Training Loss: 4567.904419630607, Validation Loss: 4661.77880859375\n",
      "Epoch [58/1500], Training Loss: 4455.042076966899, Validation Loss: 4550.34228515625\n",
      "Epoch [59/1500], Training Loss: 4344.421703707458, Validation Loss: 4441.19580078125\n",
      "Epoch [60/1500], Training Loss: 4235.90330775662, Validation Loss: 4334.32275390625\n",
      "Epoch [61/1500], Training Loss: 4129.40356563468, Validation Loss: 4229.69140625\n",
      "Epoch [62/1500], Training Loss: 4024.9099783536285, Validation Loss: 4127.31982421875\n",
      "Epoch [63/1500], Training Loss: 3922.4968109578813, Validation Loss: 4027.25048828125\n",
      "Epoch [64/1500], Training Loss: 3822.2495797002475, Validation Loss: 3929.52001953125\n",
      "Epoch [65/1500], Training Loss: 3724.273744122399, Validation Loss: 3834.20849609375\n",
      "Epoch [66/1500], Training Loss: 3628.640828915556, Validation Loss: 3741.357666015625\n",
      "Epoch [67/1500], Training Loss: 3535.392196858948, Validation Loss: 3651.009765625\n",
      "Epoch [68/1500], Training Loss: 3444.53211467212, Validation Loss: 3563.146728515625\n",
      "Epoch [69/1500], Training Loss: 3356.0653263997824, Validation Loss: 3477.789306640625\n",
      "Epoch [70/1500], Training Loss: 3269.9950995419445, Validation Loss: 3394.93701171875\n",
      "Epoch [71/1500], Training Loss: 3186.3551980769744, Validation Loss: 3314.602783203125\n",
      "Epoch [72/1500], Training Loss: 3105.1569915279824, Validation Loss: 3236.80126953125\n",
      "Epoch [73/1500], Training Loss: 3026.4056519162427, Validation Loss: 3161.4794921875\n",
      "Epoch [74/1500], Training Loss: 2950.0589132209516, Validation Loss: 3088.546875\n",
      "Epoch [75/1500], Training Loss: 2876.0246989923244, Validation Loss: 3017.849853515625\n",
      "Epoch [76/1500], Training Loss: 2804.161022080871, Validation Loss: 2949.219482421875\n",
      "Epoch [77/1500], Training Loss: 2734.322283015013, Validation Loss: 2882.522705078125\n",
      "Epoch [78/1500], Training Loss: 2666.4148151947047, Validation Loss: 2817.745361328125\n",
      "Epoch [79/1500], Training Loss: 2600.459377862582, Validation Loss: 2754.947021484375\n",
      "Epoch [80/1500], Training Loss: 2536.563907352467, Validation Loss: 2694.218994140625\n",
      "Epoch [81/1500], Training Loss: 2474.811174743013, Validation Loss: 2635.56884765625\n",
      "Epoch [82/1500], Training Loss: 2415.239457626455, Validation Loss: 2579.010986328125\n",
      "Epoch [83/1500], Training Loss: 2357.8324362521016, Validation Loss: 2524.503173828125\n",
      "Epoch [84/1500], Training Loss: 2302.4863785339494, Validation Loss: 2471.90673828125\n",
      "Epoch [85/1500], Training Loss: 2249.016599985841, Validation Loss: 2421.087890625\n",
      "Epoch [86/1500], Training Loss: 2197.2196754973284, Validation Loss: 2371.875\n",
      "Epoch [87/1500], Training Loss: 2146.9066794861337, Validation Loss: 2324.115478515625\n",
      "Epoch [88/1500], Training Loss: 2097.9260462136017, Validation Loss: 2277.64111328125\n",
      "Epoch [89/1500], Training Loss: 2050.1541327960213, Validation Loss: 2232.31640625\n",
      "Epoch [90/1500], Training Loss: 2003.5240332728556, Validation Loss: 2188.12353515625\n",
      "Epoch [91/1500], Training Loss: 1957.9911392436695, Validation Loss: 2145.0517578125\n",
      "Epoch [92/1500], Training Loss: 1913.5360331823442, Validation Loss: 2103.0576171875\n",
      "Epoch [93/1500], Training Loss: 1870.1697992782697, Validation Loss: 2062.080322265625\n",
      "Epoch [94/1500], Training Loss: 1827.9140303508104, Validation Loss: 2022.08154296875\n",
      "Epoch [95/1500], Training Loss: 1786.796309454628, Validation Loss: 1983.05029296875\n",
      "Epoch [96/1500], Training Loss: 1746.8408590905328, Validation Loss: 1945.00146484375\n",
      "Epoch [97/1500], Training Loss: 1708.0694888509722, Validation Loss: 1908.01025390625\n",
      "Epoch [98/1500], Training Loss: 1670.4823943931885, Validation Loss: 1872.1153564453125\n",
      "Epoch [99/1500], Training Loss: 1634.0655445348596, Validation Loss: 1837.31982421875\n",
      "Epoch [100/1500], Training Loss: 1598.8018930708777, Validation Loss: 1803.5755615234375\n",
      "Epoch [101/1500], Training Loss: 1564.6499335573453, Validation Loss: 1770.7957763671875\n",
      "Epoch [102/1500], Training Loss: 1531.5502971254014, Validation Loss: 1738.8905029296875\n",
      "Epoch [103/1500], Training Loss: 1499.4350542988013, Validation Loss: 1707.766357421875\n",
      "Epoch [104/1500], Training Loss: 1468.228176391724, Validation Loss: 1677.325927734375\n",
      "Epoch [105/1500], Training Loss: 1437.8423124560502, Validation Loss: 1647.5\n",
      "Epoch [106/1500], Training Loss: 1408.2047657673265, Validation Loss: 1618.2427978515625\n",
      "Epoch [107/1500], Training Loss: 1379.2663637477258, Validation Loss: 1589.5518798828125\n",
      "Epoch [108/1500], Training Loss: 1351.0007247642034, Validation Loss: 1561.4300537109375\n",
      "Epoch [109/1500], Training Loss: 1323.387901464459, Validation Loss: 1534.0084228515625\n",
      "Epoch [110/1500], Training Loss: 1296.4561366244166, Validation Loss: 1507.549072265625\n",
      "Epoch [111/1500], Training Loss: 1270.2160268990133, Validation Loss: 1482.1531982421875\n",
      "Epoch [112/1500], Training Loss: 1244.6222422191756, Validation Loss: 1457.724365234375\n",
      "Epoch [113/1500], Training Loss: 1219.6448751410387, Validation Loss: 1434.155517578125\n",
      "Epoch [114/1500], Training Loss: 1195.2625025395105, Validation Loss: 1411.3575439453125\n",
      "Epoch [115/1500], Training Loss: 1171.451647583351, Validation Loss: 1389.2601318359375\n",
      "Epoch [116/1500], Training Loss: 1148.1966500190656, Validation Loss: 1367.813720703125\n",
      "Epoch [117/1500], Training Loss: 1125.4757230469334, Validation Loss: 1346.9490966796875\n",
      "Epoch [118/1500], Training Loss: 1103.2683778457454, Validation Loss: 1326.6168212890625\n",
      "Epoch [119/1500], Training Loss: 1081.5622670956952, Validation Loss: 1306.7777099609375\n",
      "Epoch [120/1500], Training Loss: 1060.3405789483145, Validation Loss: 1287.3887939453125\n",
      "Epoch [121/1500], Training Loss: 1039.591705222068, Validation Loss: 1268.390625\n",
      "Epoch [122/1500], Training Loss: 1019.2927102398978, Validation Loss: 1249.7403564453125\n",
      "Epoch [123/1500], Training Loss: 999.4264033202438, Validation Loss: 1231.3680419921875\n",
      "Epoch [124/1500], Training Loss: 979.9767052774683, Validation Loss: 1213.2178955078125\n",
      "Epoch [125/1500], Training Loss: 960.9266190318284, Validation Loss: 1195.2242431640625\n",
      "Epoch [126/1500], Training Loss: 942.2489094795218, Validation Loss: 1177.30712890625\n",
      "Epoch [127/1500], Training Loss: 923.9231057539184, Validation Loss: 1159.396728515625\n",
      "Epoch [128/1500], Training Loss: 905.9404595959508, Validation Loss: 1141.3902587890625\n",
      "Epoch [129/1500], Training Loss: 888.2647877307081, Validation Loss: 1123.14990234375\n",
      "Epoch [130/1500], Training Loss: 870.8762427828341, Validation Loss: 1104.5330810546875\n",
      "Epoch [131/1500], Training Loss: 853.759336416309, Validation Loss: 1085.3892822265625\n",
      "Epoch [132/1500], Training Loss: 836.8822328711797, Validation Loss: 1065.584228515625\n",
      "Epoch [133/1500], Training Loss: 820.2148756784321, Validation Loss: 1045.1146240234375\n",
      "Epoch [134/1500], Training Loss: 803.7260016895214, Validation Loss: 1024.1075439453125\n",
      "Epoch [135/1500], Training Loss: 787.392231011165, Validation Loss: 1002.7714233398438\n",
      "Epoch [136/1500], Training Loss: 771.2085067569134, Validation Loss: 981.4198608398438\n",
      "Epoch [137/1500], Training Loss: 755.2140521726496, Validation Loss: 960.2611694335938\n",
      "Epoch [138/1500], Training Loss: 739.4319236820261, Validation Loss: 939.5354614257812\n",
      "Epoch [139/1500], Training Loss: 723.8515115765844, Validation Loss: 919.426025390625\n",
      "Epoch [140/1500], Training Loss: 708.4607772098415, Validation Loss: 899.5686645507812\n",
      "Epoch [141/1500], Training Loss: 693.2555646694996, Validation Loss: 879.5636596679688\n",
      "Epoch [142/1500], Training Loss: 678.2349668472128, Validation Loss: 859.366455078125\n",
      "Epoch [143/1500], Training Loss: 663.3859775947031, Validation Loss: 839.0874633789062\n",
      "Epoch [144/1500], Training Loss: 648.7152946452215, Validation Loss: 818.846435546875\n",
      "Epoch [145/1500], Training Loss: 634.2364184929114, Validation Loss: 798.747314453125\n",
      "Epoch [146/1500], Training Loss: 619.9735482606303, Validation Loss: 778.8637084960938\n",
      "Epoch [147/1500], Training Loss: 605.9529049320727, Validation Loss: 759.19482421875\n",
      "Epoch [148/1500], Training Loss: 592.2002184225739, Validation Loss: 739.7406005859375\n",
      "Epoch [149/1500], Training Loss: 578.7452897898758, Validation Loss: 720.6829223632812\n",
      "Epoch [150/1500], Training Loss: 565.625184527708, Validation Loss: 702.484619140625\n",
      "Epoch [151/1500], Training Loss: 552.8922953236311, Validation Loss: 685.547119140625\n",
      "Epoch [152/1500], Training Loss: 540.5181276071371, Validation Loss: 669.789794921875\n",
      "Epoch [153/1500], Training Loss: 528.4341526043651, Validation Loss: 654.8651123046875\n",
      "Epoch [154/1500], Training Loss: 516.5936280645144, Validation Loss: 640.421630859375\n",
      "Epoch [155/1500], Training Loss: 504.9778461261032, Validation Loss: 626.2122802734375\n",
      "Epoch [156/1500], Training Loss: 493.5655849449496, Validation Loss: 612.1499633789062\n",
      "Epoch [157/1500], Training Loss: 482.35760705803386, Validation Loss: 598.2728271484375\n",
      "Epoch [158/1500], Training Loss: 471.3573978704883, Validation Loss: 584.7050170898438\n",
      "Epoch [159/1500], Training Loss: 460.59245092984344, Validation Loss: 571.5847778320312\n",
      "Epoch [160/1500], Training Loss: 450.0827034775104, Validation Loss: 558.981689453125\n",
      "Epoch [161/1500], Training Loss: 439.83729236528, Validation Loss: 546.8865356445312\n",
      "Epoch [162/1500], Training Loss: 429.85470756660413, Validation Loss: 535.2520751953125\n",
      "Epoch [163/1500], Training Loss: 420.1286405439545, Validation Loss: 524.0255737304688\n",
      "Epoch [164/1500], Training Loss: 410.6406124440579, Validation Loss: 513.1535034179688\n",
      "Epoch [165/1500], Training Loss: 401.37865094185213, Validation Loss: 502.59539794921875\n",
      "Epoch [166/1500], Training Loss: 392.3291433904193, Validation Loss: 492.3199462890625\n",
      "Epoch [167/1500], Training Loss: 383.48071744441444, Validation Loss: 482.2959899902344\n",
      "Epoch [168/1500], Training Loss: 374.82800770774423, Validation Loss: 472.5097351074219\n",
      "Epoch [169/1500], Training Loss: 366.3587100018758, Validation Loss: 462.942626953125\n",
      "Epoch [170/1500], Training Loss: 358.0695232846065, Validation Loss: 453.5867004394531\n",
      "Epoch [171/1500], Training Loss: 349.9539385745434, Validation Loss: 444.43280029296875\n",
      "Epoch [172/1500], Training Loss: 342.00914209555384, Validation Loss: 435.4776611328125\n",
      "Epoch [173/1500], Training Loss: 334.23265126878675, Validation Loss: 426.72039794921875\n",
      "Epoch [174/1500], Training Loss: 326.6251061421486, Validation Loss: 418.156494140625\n",
      "Epoch [175/1500], Training Loss: 319.1849583754102, Validation Loss: 409.78277587890625\n",
      "Epoch [176/1500], Training Loss: 311.90823380033777, Validation Loss: 401.5935363769531\n",
      "Epoch [177/1500], Training Loss: 304.79491758208997, Validation Loss: 393.5862731933594\n",
      "Epoch [178/1500], Training Loss: 297.8400008166453, Validation Loss: 385.7580871582031\n",
      "Epoch [179/1500], Training Loss: 291.04661248393285, Validation Loss: 378.11212158203125\n",
      "Epoch [180/1500], Training Loss: 284.4117078876468, Validation Loss: 370.6440124511719\n",
      "Epoch [181/1500], Training Loss: 277.93320928234573, Validation Loss: 363.3552551269531\n",
      "Epoch [182/1500], Training Loss: 271.6068034018071, Validation Loss: 356.2460632324219\n",
      "Epoch [183/1500], Training Loss: 265.4315711772794, Validation Loss: 349.3182373046875\n",
      "Epoch [184/1500], Training Loss: 259.40086627447795, Validation Loss: 342.5655822753906\n",
      "Epoch [185/1500], Training Loss: 253.5148029081484, Validation Loss: 335.9825744628906\n",
      "Epoch [186/1500], Training Loss: 247.76743223639585, Validation Loss: 329.56329345703125\n",
      "Epoch [187/1500], Training Loss: 242.15499904693564, Validation Loss: 323.29815673828125\n",
      "Epoch [188/1500], Training Loss: 236.67644969784828, Validation Loss: 317.18603515625\n",
      "Epoch [189/1500], Training Loss: 231.3306284428731, Validation Loss: 311.2226257324219\n",
      "Epoch [190/1500], Training Loss: 226.11364745006412, Validation Loss: 305.408203125\n",
      "Epoch [191/1500], Training Loss: 221.0268345686914, Validation Loss: 299.7414245605469\n",
      "Epoch [192/1500], Training Loss: 216.0655004608102, Validation Loss: 294.2280578613281\n",
      "Epoch [193/1500], Training Loss: 211.23081566251795, Validation Loss: 288.8661193847656\n",
      "Epoch [194/1500], Training Loss: 206.52067321894242, Validation Loss: 283.6612854003906\n",
      "Epoch [195/1500], Training Loss: 201.93481886713252, Validation Loss: 278.614013671875\n",
      "Epoch [196/1500], Training Loss: 197.46978544788604, Validation Loss: 273.7204284667969\n",
      "Epoch [197/1500], Training Loss: 193.12349352630096, Validation Loss: 268.97991943359375\n",
      "Epoch [198/1500], Training Loss: 188.89582259089977, Validation Loss: 264.39263916015625\n",
      "Epoch [199/1500], Training Loss: 184.78205463401335, Validation Loss: 259.95166015625\n",
      "Epoch [200/1500], Training Loss: 180.78166389724208, Validation Loss: 255.65309143066406\n",
      "Epoch [201/1500], Training Loss: 176.89144331219222, Validation Loss: 251.4912109375\n",
      "Epoch [202/1500], Training Loss: 173.10755154805787, Validation Loss: 247.45806884765625\n",
      "Epoch [203/1500], Training Loss: 169.4271665903643, Validation Loss: 243.5451202392578\n",
      "Epoch [204/1500], Training Loss: 165.84644031319655, Validation Loss: 239.74327087402344\n",
      "Epoch [205/1500], Training Loss: 162.36175360019368, Validation Loss: 236.04112243652344\n",
      "Epoch [206/1500], Training Loss: 158.9669950502855, Validation Loss: 232.42694091796875\n",
      "Epoch [207/1500], Training Loss: 155.65997038756126, Validation Loss: 228.89256286621094\n",
      "Epoch [208/1500], Training Loss: 152.43596445485576, Validation Loss: 225.42684936523438\n",
      "Epoch [209/1500], Training Loss: 149.28920907904478, Validation Loss: 222.02561950683594\n",
      "Epoch [210/1500], Training Loss: 146.2178835588417, Validation Loss: 218.678466796875\n",
      "Epoch [211/1500], Training Loss: 143.21449504034743, Validation Loss: 215.3590850830078\n",
      "Epoch [212/1500], Training Loss: 140.27059257422664, Validation Loss: 212.02980041503906\n",
      "Epoch [213/1500], Training Loss: 137.37915119233082, Validation Loss: 208.65093994140625\n",
      "Epoch [214/1500], Training Loss: 134.53050705066218, Validation Loss: 205.18382263183594\n",
      "Epoch [215/1500], Training Loss: 131.71786476744893, Validation Loss: 201.59893798828125\n",
      "Epoch [216/1500], Training Loss: 128.9354825974284, Validation Loss: 197.87197875976562\n",
      "Epoch [217/1500], Training Loss: 126.17656659207945, Validation Loss: 193.99217224121094\n",
      "Epoch [218/1500], Training Loss: 123.44047585533679, Validation Loss: 189.96705627441406\n",
      "Epoch [219/1500], Training Loss: 120.72721972394258, Validation Loss: 185.82095336914062\n",
      "Epoch [220/1500], Training Loss: 118.03919234448219, Validation Loss: 181.59100341796875\n",
      "Epoch [221/1500], Training Loss: 115.38163747434577, Validation Loss: 177.32752990722656\n",
      "Epoch [222/1500], Training Loss: 112.76310340889339, Validation Loss: 173.08932495117188\n",
      "Epoch [223/1500], Training Loss: 110.19199689412525, Validation Loss: 168.93228149414062\n",
      "Epoch [224/1500], Training Loss: 107.67612705414828, Validation Loss: 164.90469360351562\n",
      "Epoch [225/1500], Training Loss: 105.22101939460491, Validation Loss: 161.04164123535156\n",
      "Epoch [226/1500], Training Loss: 102.83352442698029, Validation Loss: 157.35577392578125\n",
      "Epoch [227/1500], Training Loss: 100.51490563373119, Validation Loss: 153.84815979003906\n",
      "Epoch [228/1500], Training Loss: 98.26513613771078, Validation Loss: 150.50587463378906\n",
      "Epoch [229/1500], Training Loss: 96.08191118790543, Validation Loss: 147.31317138671875\n",
      "Epoch [230/1500], Training Loss: 93.96261552959005, Validation Loss: 144.25320434570312\n",
      "Epoch [231/1500], Training Loss: 91.90353556473663, Validation Loss: 141.315673828125\n",
      "Epoch [232/1500], Training Loss: 89.9019085207059, Validation Loss: 138.49029541015625\n",
      "Epoch [233/1500], Training Loss: 87.95551303406499, Validation Loss: 135.77154541015625\n",
      "Epoch [234/1500], Training Loss: 86.06265928557805, Validation Loss: 133.1544647216797\n",
      "Epoch [235/1500], Training Loss: 84.22240498130874, Validation Loss: 130.6402587890625\n",
      "Epoch [236/1500], Training Loss: 82.43254515051936, Validation Loss: 128.2253875732422\n",
      "Epoch [237/1500], Training Loss: 80.69296947576218, Validation Loss: 125.91194915771484\n",
      "Epoch [238/1500], Training Loss: 79.00378373754326, Validation Loss: 123.70011138916016\n",
      "Epoch [239/1500], Training Loss: 77.36375577947233, Validation Loss: 121.5892333984375\n",
      "Epoch [240/1500], Training Loss: 75.77251228434831, Validation Loss: 119.57833099365234\n",
      "Epoch [241/1500], Training Loss: 74.22988664034605, Validation Loss: 117.66511535644531\n",
      "Epoch [242/1500], Training Loss: 72.73394383991585, Validation Loss: 115.8460922241211\n",
      "Epoch [243/1500], Training Loss: 71.28313861458383, Validation Loss: 114.11487579345703\n",
      "Epoch [244/1500], Training Loss: 69.87624164567983, Validation Loss: 112.46814727783203\n",
      "Epoch [245/1500], Training Loss: 68.51260503137729, Validation Loss: 110.8988265991211\n",
      "Epoch [246/1500], Training Loss: 67.18933075157503, Validation Loss: 109.40168762207031\n",
      "Epoch [247/1500], Training Loss: 65.90675404473069, Validation Loss: 107.97126007080078\n",
      "Epoch [248/1500], Training Loss: 64.66162884010083, Validation Loss: 106.60179138183594\n",
      "Epoch [249/1500], Training Loss: 63.45301457243381, Validation Loss: 105.28794860839844\n",
      "Epoch [250/1500], Training Loss: 62.2788655747491, Validation Loss: 104.02513885498047\n",
      "Epoch [251/1500], Training Loss: 61.13795743061419, Validation Loss: 102.8090591430664\n",
      "Epoch [252/1500], Training Loss: 60.0296589245487, Validation Loss: 101.63726043701172\n",
      "Epoch [253/1500], Training Loss: 58.952851477362486, Validation Loss: 100.50821685791016\n",
      "Epoch [254/1500], Training Loss: 57.90593840962199, Validation Loss: 99.41766357421875\n",
      "Epoch [255/1500], Training Loss: 56.888569330381834, Validation Loss: 98.3648452758789\n",
      "Epoch [256/1500], Training Loss: 55.89891306060898, Validation Loss: 97.34740447998047\n",
      "Epoch [257/1500], Training Loss: 54.93691556941368, Validation Loss: 96.36553955078125\n",
      "Epoch [258/1500], Training Loss: 54.000984242631056, Validation Loss: 95.4162826538086\n",
      "Epoch [259/1500], Training Loss: 53.09042484718377, Validation Loss: 94.50017547607422\n",
      "Epoch [260/1500], Training Loss: 52.204145000232344, Validation Loss: 93.61515045166016\n",
      "Epoch [261/1500], Training Loss: 51.34167302202165, Validation Loss: 92.76139831542969\n",
      "Epoch [262/1500], Training Loss: 50.50220037610219, Validation Loss: 91.93738555908203\n",
      "Epoch [263/1500], Training Loss: 49.6852274741045, Validation Loss: 91.14239501953125\n",
      "Epoch [264/1500], Training Loss: 48.889534915109465, Validation Loss: 90.37512969970703\n",
      "Epoch [265/1500], Training Loss: 48.114939992104674, Validation Loss: 89.63490295410156\n",
      "Epoch [266/1500], Training Loss: 47.360244126946874, Validation Loss: 88.92097473144531\n",
      "Epoch [267/1500], Training Loss: 46.625406301721135, Validation Loss: 88.23155975341797\n",
      "Epoch [268/1500], Training Loss: 45.90931597369796, Validation Loss: 87.5661849975586\n",
      "Epoch [269/1500], Training Loss: 45.21125599534412, Validation Loss: 86.92306518554688\n",
      "Epoch [270/1500], Training Loss: 44.53117744996605, Validation Loss: 86.30178833007812\n",
      "Epoch [271/1500], Training Loss: 43.868203127459445, Validation Loss: 85.70159912109375\n",
      "Epoch [272/1500], Training Loss: 43.22190448536282, Validation Loss: 85.12123107910156\n",
      "Epoch [273/1500], Training Loss: 42.59181699776106, Validation Loss: 84.56021118164062\n",
      "Epoch [274/1500], Training Loss: 41.97712341107909, Validation Loss: 84.01801300048828\n",
      "Epoch [275/1500], Training Loss: 41.37740640722379, Validation Loss: 83.49227142333984\n",
      "Epoch [276/1500], Training Loss: 40.792058226155966, Validation Loss: 82.98353576660156\n",
      "Epoch [277/1500], Training Loss: 40.22122873770116, Validation Loss: 82.49121856689453\n",
      "Epoch [278/1500], Training Loss: 39.66351285431474, Validation Loss: 82.01311492919922\n",
      "Epoch [279/1500], Training Loss: 39.11892160992662, Validation Loss: 81.55015563964844\n",
      "Epoch [280/1500], Training Loss: 38.58752325356553, Validation Loss: 81.10083770751953\n",
      "Epoch [281/1500], Training Loss: 38.06901034137603, Validation Loss: 80.66463470458984\n",
      "Epoch [282/1500], Training Loss: 37.562385209052664, Validation Loss: 80.24080657958984\n",
      "Epoch [283/1500], Training Loss: 37.06713563334424, Validation Loss: 79.82856750488281\n",
      "Epoch [284/1500], Training Loss: 36.583313913971196, Validation Loss: 79.42817687988281\n",
      "Epoch [285/1500], Training Loss: 36.11071735153919, Validation Loss: 79.03852081298828\n",
      "Epoch [286/1500], Training Loss: 35.649079150863244, Validation Loss: 78.66018676757812\n",
      "Epoch [287/1500], Training Loss: 35.198396263857894, Validation Loss: 78.29230499267578\n",
      "Epoch [288/1500], Training Loss: 34.75773708422938, Validation Loss: 77.9344253540039\n",
      "Epoch [289/1500], Training Loss: 34.32679800493675, Validation Loss: 77.58692169189453\n",
      "Epoch [290/1500], Training Loss: 33.90555331175858, Validation Loss: 77.24896240234375\n",
      "Epoch [291/1500], Training Loss: 33.494061365170765, Validation Loss: 76.92134857177734\n",
      "Epoch [292/1500], Training Loss: 33.09199269068274, Validation Loss: 76.60350799560547\n",
      "Epoch [293/1500], Training Loss: 32.698930790493336, Validation Loss: 76.29535675048828\n",
      "Epoch [294/1500], Training Loss: 32.31486507359697, Validation Loss: 75.99777221679688\n",
      "Epoch [295/1500], Training Loss: 31.93962039310413, Validation Loss: 75.70973205566406\n",
      "Epoch [296/1500], Training Loss: 31.57283826834688, Validation Loss: 75.43101501464844\n",
      "Epoch [297/1500], Training Loss: 31.21453510381329, Validation Loss: 75.16236877441406\n",
      "Epoch [298/1500], Training Loss: 30.86432348838132, Validation Loss: 74.9024429321289\n",
      "Epoch [299/1500], Training Loss: 30.522432977963966, Validation Loss: 74.65086364746094\n",
      "Epoch [300/1500], Training Loss: 30.18837440570827, Validation Loss: 74.40819549560547\n",
      "Epoch [301/1500], Training Loss: 29.86191288217554, Validation Loss: 74.17366790771484\n",
      "Epoch [302/1500], Training Loss: 29.54305043594746, Validation Loss: 73.94660186767578\n",
      "Epoch [303/1500], Training Loss: 29.23147862730765, Validation Loss: 73.72602081298828\n",
      "Epoch [304/1500], Training Loss: 28.92723168920803, Validation Loss: 73.51214599609375\n",
      "Epoch [305/1500], Training Loss: 28.629868013407563, Validation Loss: 73.30451965332031\n",
      "Epoch [306/1500], Training Loss: 28.339503052647046, Validation Loss: 73.10201263427734\n",
      "Epoch [307/1500], Training Loss: 28.05584268043557, Validation Loss: 72.90351867675781\n",
      "Epoch [308/1500], Training Loss: 27.778891720751375, Validation Loss: 72.71028900146484\n",
      "Epoch [309/1500], Training Loss: 27.50859235959249, Validation Loss: 72.52054595947266\n",
      "Epoch [310/1500], Training Loss: 27.244438747530133, Validation Loss: 72.3345947265625\n",
      "Epoch [311/1500], Training Loss: 26.986430899043384, Validation Loss: 72.152099609375\n",
      "Epoch [312/1500], Training Loss: 26.734489952202836, Validation Loss: 71.97261047363281\n",
      "Epoch [313/1500], Training Loss: 26.488291039879613, Validation Loss: 71.79610443115234\n",
      "Epoch [314/1500], Training Loss: 26.247929023611047, Validation Loss: 71.62271881103516\n",
      "Epoch [315/1500], Training Loss: 26.013054908026692, Validation Loss: 71.45204162597656\n",
      "Epoch [316/1500], Training Loss: 25.783353246106163, Validation Loss: 71.28404235839844\n",
      "Epoch [317/1500], Training Loss: 25.558701630701318, Validation Loss: 71.11927795410156\n",
      "Epoch [318/1500], Training Loss: 25.339160975494128, Validation Loss: 70.9568862915039\n",
      "Epoch [319/1500], Training Loss: 25.124418498260077, Validation Loss: 70.79754638671875\n",
      "Epoch [320/1500], Training Loss: 24.91435848676302, Validation Loss: 70.64064025878906\n",
      "Epoch [321/1500], Training Loss: 24.708934869690978, Validation Loss: 70.48726654052734\n",
      "Epoch [322/1500], Training Loss: 24.507834678689598, Validation Loss: 70.33623504638672\n",
      "Epoch [323/1500], Training Loss: 24.310697548980897, Validation Loss: 70.18759155273438\n",
      "Epoch [324/1500], Training Loss: 24.117662027395042, Validation Loss: 70.0418472290039\n",
      "Epoch [325/1500], Training Loss: 23.92849564292544, Validation Loss: 69.8978271484375\n",
      "Epoch [326/1500], Training Loss: 23.74320944305313, Validation Loss: 69.75697326660156\n",
      "Epoch [327/1500], Training Loss: 23.56161014857474, Validation Loss: 69.617919921875\n",
      "Epoch [328/1500], Training Loss: 23.38362984547046, Validation Loss: 69.48194122314453\n",
      "Epoch [329/1500], Training Loss: 23.20907497586235, Validation Loss: 69.34832000732422\n",
      "Epoch [330/1500], Training Loss: 23.037897995732035, Validation Loss: 69.2173080444336\n",
      "Epoch [331/1500], Training Loss: 22.869966009897414, Validation Loss: 69.08834838867188\n",
      "Epoch [332/1500], Training Loss: 22.705155379709595, Validation Loss: 68.96144104003906\n",
      "Epoch [333/1500], Training Loss: 22.54327058771618, Validation Loss: 68.83678436279297\n",
      "Epoch [334/1500], Training Loss: 22.38433736394175, Validation Loss: 68.71390533447266\n",
      "Epoch [335/1500], Training Loss: 22.228226314782354, Validation Loss: 68.59271240234375\n",
      "Epoch [336/1500], Training Loss: 22.074950099134384, Validation Loss: 68.47356414794922\n",
      "Epoch [337/1500], Training Loss: 21.924392286471566, Validation Loss: 68.3562240600586\n",
      "Epoch [338/1500], Training Loss: 21.776349795891324, Validation Loss: 68.24095153808594\n",
      "Epoch [339/1500], Training Loss: 21.63094253854265, Validation Loss: 68.12723541259766\n",
      "Epoch [340/1500], Training Loss: 21.487895426382362, Validation Loss: 68.01502227783203\n",
      "Epoch [341/1500], Training Loss: 21.34725664920551, Validation Loss: 67.90443420410156\n",
      "Epoch [342/1500], Training Loss: 21.2087717191822, Validation Loss: 67.794677734375\n",
      "Epoch [343/1500], Training Loss: 21.072438781687477, Validation Loss: 67.68644714355469\n",
      "Epoch [344/1500], Training Loss: 20.9382242624702, Validation Loss: 67.57943725585938\n",
      "Epoch [345/1500], Training Loss: 20.80609403163752, Validation Loss: 67.47344970703125\n",
      "Epoch [346/1500], Training Loss: 20.675960961852144, Validation Loss: 67.36793518066406\n",
      "Epoch [347/1500], Training Loss: 20.547781369229796, Validation Loss: 67.26367950439453\n",
      "Epoch [348/1500], Training Loss: 20.421313995696647, Validation Loss: 67.16014862060547\n",
      "Epoch [349/1500], Training Loss: 20.296520398440894, Validation Loss: 67.05696105957031\n",
      "Epoch [350/1500], Training Loss: 20.173508839649937, Validation Loss: 66.9543228149414\n",
      "Epoch [351/1500], Training Loss: 20.05204630689773, Validation Loss: 66.85203552246094\n",
      "Epoch [352/1500], Training Loss: 19.93232424690517, Validation Loss: 66.74916076660156\n",
      "Epoch [353/1500], Training Loss: 19.814150802799645, Validation Loss: 66.6470718383789\n",
      "Epoch [354/1500], Training Loss: 19.69730683001553, Validation Loss: 66.54541015625\n",
      "Epoch [355/1500], Training Loss: 19.582011893659228, Validation Loss: 66.44282531738281\n",
      "Epoch [356/1500], Training Loss: 19.468181318171354, Validation Loss: 66.34025573730469\n",
      "Epoch [357/1500], Training Loss: 19.355716297056553, Validation Loss: 66.23714447021484\n",
      "Epoch [358/1500], Training Loss: 19.244398491185116, Validation Loss: 66.13335418701172\n",
      "Epoch [359/1500], Training Loss: 19.134184829139222, Validation Loss: 66.02906036376953\n",
      "Epoch [360/1500], Training Loss: 19.025183268289144, Validation Loss: 65.9238510131836\n",
      "Epoch [361/1500], Training Loss: 18.917384568755782, Validation Loss: 65.8179702758789\n",
      "Epoch [362/1500], Training Loss: 18.81066148524012, Validation Loss: 65.71072387695312\n",
      "Epoch [363/1500], Training Loss: 18.704847969791217, Validation Loss: 65.6023178100586\n",
      "Epoch [364/1500], Training Loss: 18.60007285998569, Validation Loss: 65.4925308227539\n",
      "Epoch [365/1500], Training Loss: 18.496265984080406, Validation Loss: 65.38094329833984\n",
      "Epoch [366/1500], Training Loss: 18.393176177462614, Validation Loss: 65.26834106445312\n",
      "Epoch [367/1500], Training Loss: 18.290859332878824, Validation Loss: 65.15394592285156\n",
      "Epoch [368/1500], Training Loss: 18.189466107548686, Validation Loss: 65.03787231445312\n",
      "Epoch [369/1500], Training Loss: 18.088826602343623, Validation Loss: 64.92035675048828\n",
      "Epoch [370/1500], Training Loss: 17.988983787151426, Validation Loss: 64.80089569091797\n",
      "Epoch [371/1500], Training Loss: 17.88999441908806, Validation Loss: 64.68109893798828\n",
      "Epoch [372/1500], Training Loss: 17.791766803027354, Validation Loss: 64.55970001220703\n",
      "Epoch [373/1500], Training Loss: 17.694341569024918, Validation Loss: 64.4382095336914\n",
      "Epoch [374/1500], Training Loss: 17.59771308574955, Validation Loss: 64.31726837158203\n",
      "Epoch [375/1500], Training Loss: 17.501737213690408, Validation Loss: 64.19725036621094\n",
      "Epoch [376/1500], Training Loss: 17.40661050599358, Validation Loss: 64.08063507080078\n",
      "Epoch [377/1500], Training Loss: 17.312446290265942, Validation Loss: 63.968284606933594\n",
      "Epoch [378/1500], Training Loss: 17.219242241253447, Validation Loss: 63.858333587646484\n",
      "Epoch [379/1500], Training Loss: 17.127069394057028, Validation Loss: 63.743507385253906\n",
      "Epoch [380/1500], Training Loss: 17.035778372262868, Validation Loss: 63.625057220458984\n",
      "Epoch [381/1500], Training Loss: 16.94514561706505, Validation Loss: 63.49692153930664\n",
      "Epoch [382/1500], Training Loss: 16.85543593575735, Validation Loss: 63.346553802490234\n",
      "Epoch [383/1500], Training Loss: 16.766487109198614, Validation Loss: 63.172760009765625\n",
      "Epoch [384/1500], Training Loss: 16.678111317326877, Validation Loss: 62.98072814941406\n",
      "Epoch [385/1500], Training Loss: 16.59023201416685, Validation Loss: 62.77626037597656\n",
      "Epoch [386/1500], Training Loss: 16.50282697040499, Validation Loss: 62.56381607055664\n",
      "Epoch [387/1500], Training Loss: 16.415779774629446, Validation Loss: 62.345237731933594\n",
      "Epoch [388/1500], Training Loss: 16.329241296185497, Validation Loss: 62.121986389160156\n",
      "Epoch [389/1500], Training Loss: 16.242979182523214, Validation Loss: 61.89421844482422\n",
      "Epoch [390/1500], Training Loss: 16.157036937115343, Validation Loss: 61.6619758605957\n",
      "Epoch [391/1500], Training Loss: 16.07143147689538, Validation Loss: 61.42487716674805\n",
      "Epoch [392/1500], Training Loss: 15.986094755451225, Validation Loss: 61.18367385864258\n",
      "Epoch [393/1500], Training Loss: 15.900941173831049, Validation Loss: 60.93794631958008\n",
      "Epoch [394/1500], Training Loss: 15.816121908554763, Validation Loss: 60.687591552734375\n",
      "Epoch [395/1500], Training Loss: 15.731478503310774, Validation Loss: 60.43325424194336\n",
      "Epoch [396/1500], Training Loss: 15.647053072902185, Validation Loss: 60.17499923706055\n",
      "Epoch [397/1500], Training Loss: 15.562639223945201, Validation Loss: 59.912635803222656\n",
      "Epoch [398/1500], Training Loss: 15.478313686937723, Validation Loss: 59.6466178894043\n",
      "Epoch [399/1500], Training Loss: 15.39400272330794, Validation Loss: 59.37683868408203\n",
      "Epoch [400/1500], Training Loss: 15.309718504891215, Validation Loss: 59.10319900512695\n",
      "Epoch [401/1500], Training Loss: 15.225573537166166, Validation Loss: 58.82587814331055\n",
      "Epoch [402/1500], Training Loss: 15.141243205140444, Validation Loss: 58.54465103149414\n",
      "Epoch [403/1500], Training Loss: 15.056811956961281, Validation Loss: 58.25980758666992\n",
      "Epoch [404/1500], Training Loss: 14.972148148293673, Validation Loss: 57.971309661865234\n",
      "Epoch [405/1500], Training Loss: 14.88725824461657, Validation Loss: 57.679229736328125\n",
      "Epoch [406/1500], Training Loss: 14.802070267694601, Validation Loss: 57.38258743286133\n",
      "Epoch [407/1500], Training Loss: 14.716511183571342, Validation Loss: 57.08204650878906\n",
      "Epoch [408/1500], Training Loss: 14.630509626564619, Validation Loss: 56.77724075317383\n",
      "Epoch [409/1500], Training Loss: 14.543990122677378, Validation Loss: 56.46723175048828\n",
      "Epoch [410/1500], Training Loss: 14.456958043579727, Validation Loss: 56.15202713012695\n",
      "Epoch [411/1500], Training Loss: 14.369444719044767, Validation Loss: 55.83167266845703\n",
      "Epoch [412/1500], Training Loss: 14.281343618313878, Validation Loss: 55.50589370727539\n",
      "Epoch [413/1500], Training Loss: 14.192653918922659, Validation Loss: 55.17425537109375\n",
      "Epoch [414/1500], Training Loss: 14.103373350077616, Validation Loss: 54.836605072021484\n",
      "Epoch [415/1500], Training Loss: 14.013531863847394, Validation Loss: 54.49318313598633\n",
      "Epoch [416/1500], Training Loss: 13.923018476179811, Validation Loss: 54.14309310913086\n",
      "Epoch [417/1500], Training Loss: 13.831966922418117, Validation Loss: 53.78739929199219\n",
      "Epoch [418/1500], Training Loss: 13.740339324925285, Validation Loss: 53.42599868774414\n",
      "Epoch [419/1500], Training Loss: 13.648183958612934, Validation Loss: 53.058101654052734\n",
      "Epoch [420/1500], Training Loss: 13.555479780772076, Validation Loss: 52.684871673583984\n",
      "Epoch [421/1500], Training Loss: 13.462418082448929, Validation Loss: 52.30671310424805\n",
      "Epoch [422/1500], Training Loss: 13.368854717580355, Validation Loss: 51.92327117919922\n",
      "Epoch [423/1500], Training Loss: 13.274975695409749, Validation Loss: 51.53571701049805\n",
      "Epoch [424/1500], Training Loss: 13.18083302378563, Validation Loss: 51.14440155029297\n",
      "Epoch [425/1500], Training Loss: 13.08630499568777, Validation Loss: 50.74965286254883\n",
      "Epoch [426/1500], Training Loss: 12.991553422094347, Validation Loss: 50.351924896240234\n",
      "Epoch [427/1500], Training Loss: 12.896746646397565, Validation Loss: 49.951873779296875\n",
      "Epoch [428/1500], Training Loss: 12.801917894177373, Validation Loss: 49.54991149902344\n",
      "Epoch [429/1500], Training Loss: 12.707087784907928, Validation Loss: 49.147342681884766\n",
      "Epoch [430/1500], Training Loss: 12.612359961758937, Validation Loss: 48.743858337402344\n",
      "Epoch [431/1500], Training Loss: 12.517672550253867, Validation Loss: 48.34069061279297\n",
      "Epoch [432/1500], Training Loss: 12.423171342711539, Validation Loss: 47.93766784667969\n",
      "Epoch [433/1500], Training Loss: 12.328936203088315, Validation Loss: 47.53605651855469\n",
      "Epoch [434/1500], Training Loss: 12.235079979626184, Validation Loss: 47.13558578491211\n",
      "Epoch [435/1500], Training Loss: 12.141629200225745, Validation Loss: 46.73710250854492\n",
      "Epoch [436/1500], Training Loss: 12.048504935670202, Validation Loss: 46.34117889404297\n",
      "Epoch [437/1500], Training Loss: 11.955966040322316, Validation Loss: 45.94801712036133\n",
      "Epoch [438/1500], Training Loss: 11.863836996569079, Validation Loss: 45.558135986328125\n",
      "Epoch [439/1500], Training Loss: 11.772355287576708, Validation Loss: 45.17115783691406\n",
      "Epoch [440/1500], Training Loss: 11.681520796021696, Validation Loss: 44.788230895996094\n",
      "Epoch [441/1500], Training Loss: 11.59130032587702, Validation Loss: 44.409114837646484\n",
      "Epoch [442/1500], Training Loss: 11.501760559945474, Validation Loss: 44.03382110595703\n",
      "Epoch [443/1500], Training Loss: 11.412817548764973, Validation Loss: 43.662635803222656\n",
      "Epoch [444/1500], Training Loss: 11.324602999274155, Validation Loss: 43.29566955566406\n",
      "Epoch [445/1500], Training Loss: 11.237193120536437, Validation Loss: 42.933345794677734\n",
      "Epoch [446/1500], Training Loss: 11.150441738903686, Validation Loss: 42.57548141479492\n",
      "Epoch [447/1500], Training Loss: 11.064428945064236, Validation Loss: 42.22186279296875\n",
      "Epoch [448/1500], Training Loss: 10.979241571338404, Validation Loss: 41.87271499633789\n",
      "Epoch [449/1500], Training Loss: 10.89480316831209, Validation Loss: 41.52831268310547\n",
      "Epoch [450/1500], Training Loss: 10.81110657537177, Validation Loss: 41.18838882446289\n",
      "Epoch [451/1500], Training Loss: 10.728167480599751, Validation Loss: 40.853004455566406\n",
      "Epoch [452/1500], Training Loss: 10.6460681973154, Validation Loss: 40.52189254760742\n",
      "Epoch [453/1500], Training Loss: 10.564670281452614, Validation Loss: 40.19527053833008\n",
      "Epoch [454/1500], Training Loss: 10.48414499323012, Validation Loss: 39.872802734375\n",
      "Epoch [455/1500], Training Loss: 10.404428221835659, Validation Loss: 39.55451965332031\n",
      "Epoch [456/1500], Training Loss: 10.325553063969258, Validation Loss: 39.24037170410156\n",
      "Epoch [457/1500], Training Loss: 10.247438064678308, Validation Loss: 38.93036651611328\n",
      "Epoch [458/1500], Training Loss: 10.170213599455103, Validation Loss: 38.62446594238281\n",
      "Epoch [459/1500], Training Loss: 10.093808193315168, Validation Loss: 38.32258605957031\n",
      "Epoch [460/1500], Training Loss: 10.018271703144698, Validation Loss: 38.02480697631836\n",
      "Epoch [461/1500], Training Loss: 9.943502968134428, Validation Loss: 37.73107147216797\n",
      "Epoch [462/1500], Training Loss: 9.86946734997698, Validation Loss: 37.44104766845703\n",
      "Epoch [463/1500], Training Loss: 9.796263047465446, Validation Loss: 37.15457534790039\n",
      "Epoch [464/1500], Training Loss: 9.723888979029068, Validation Loss: 36.87205123901367\n",
      "Epoch [465/1500], Training Loss: 9.652326645411316, Validation Loss: 36.59300994873047\n",
      "Epoch [466/1500], Training Loss: 9.581533159995478, Validation Loss: 36.31789779663086\n",
      "Epoch [467/1500], Training Loss: 9.511454788860851, Validation Loss: 36.046443939208984\n",
      "Epoch [468/1500], Training Loss: 9.442137977926441, Validation Loss: 35.77818298339844\n",
      "Epoch [469/1500], Training Loss: 9.373666772853747, Validation Loss: 35.51342010498047\n",
      "Epoch [470/1500], Training Loss: 9.3060717612606, Validation Loss: 35.252437591552734\n",
      "Epoch [471/1500], Training Loss: 9.239331653534618, Validation Loss: 34.99519729614258\n",
      "Epoch [472/1500], Training Loss: 9.173446780990801, Validation Loss: 34.74162292480469\n",
      "Epoch [473/1500], Training Loss: 9.108399628390027, Validation Loss: 34.49220657348633\n",
      "Epoch [474/1500], Training Loss: 9.044185062497494, Validation Loss: 34.24626541137695\n",
      "Epoch [475/1500], Training Loss: 8.980911095905624, Validation Loss: 34.00471878051758\n",
      "Epoch [476/1500], Training Loss: 8.918510935759898, Validation Loss: 33.76709747314453\n",
      "Epoch [477/1500], Training Loss: 8.85696057301622, Validation Loss: 33.533607482910156\n",
      "Epoch [478/1500], Training Loss: 8.796358381207607, Validation Loss: 33.30439758300781\n",
      "Epoch [479/1500], Training Loss: 8.736755300819203, Validation Loss: 33.07997512817383\n",
      "Epoch [480/1500], Training Loss: 8.67814505204256, Validation Loss: 32.86001968383789\n",
      "Epoch [481/1500], Training Loss: 8.620472418211438, Validation Loss: 32.64468765258789\n",
      "Epoch [482/1500], Training Loss: 8.563848059937811, Validation Loss: 32.434471130371094\n",
      "Epoch [483/1500], Training Loss: 8.508334535121223, Validation Loss: 32.229400634765625\n",
      "Epoch [484/1500], Training Loss: 8.453881514004035, Validation Loss: 32.02960205078125\n",
      "Epoch [485/1500], Training Loss: 8.40049457327003, Validation Loss: 31.834814071655273\n",
      "Epoch [486/1500], Training Loss: 8.348172174383516, Validation Loss: 31.64558219909668\n",
      "Epoch [487/1500], Training Loss: 8.296970922231063, Validation Loss: 31.462188720703125\n",
      "Epoch [488/1500], Training Loss: 8.246884379320543, Validation Loss: 31.283971786499023\n",
      "Epoch [489/1500], Training Loss: 8.197976024207266, Validation Loss: 31.111160278320312\n",
      "Epoch [490/1500], Training Loss: 8.150208107549139, Validation Loss: 30.944364547729492\n",
      "Epoch [491/1500], Training Loss: 8.103599291597682, Validation Loss: 30.783193588256836\n",
      "Epoch [492/1500], Training Loss: 8.058016480956644, Validation Loss: 30.627593994140625\n",
      "Epoch [493/1500], Training Loss: 8.013591214780664, Validation Loss: 30.478010177612305\n",
      "Epoch [494/1500], Training Loss: 7.970318276757783, Validation Loss: 30.333925247192383\n",
      "Epoch [495/1500], Training Loss: 7.928084209552632, Validation Loss: 30.19478988647461\n",
      "Epoch [496/1500], Training Loss: 7.88684798455459, Validation Loss: 30.061302185058594\n",
      "Epoch [497/1500], Training Loss: 7.846720780514922, Validation Loss: 29.933082580566406\n",
      "Epoch [498/1500], Training Loss: 7.807686528227128, Validation Loss: 29.809894561767578\n",
      "Epoch [499/1500], Training Loss: 7.7696359658678755, Validation Loss: 29.69204330444336\n",
      "Epoch [500/1500], Training Loss: 7.7326362169917635, Validation Loss: 29.57878303527832\n",
      "Epoch [501/1500], Training Loss: 7.69670118494914, Validation Loss: 29.470436096191406\n",
      "Epoch [502/1500], Training Loss: 7.661722222299341, Validation Loss: 29.366390228271484\n",
      "Epoch [503/1500], Training Loss: 7.627686246627337, Validation Loss: 29.26631736755371\n",
      "Epoch [504/1500], Training Loss: 7.594506260918954, Validation Loss: 29.170013427734375\n",
      "Epoch [505/1500], Training Loss: 7.562285880643294, Validation Loss: 29.078079223632812\n",
      "Epoch [506/1500], Training Loss: 7.530898406952463, Validation Loss: 28.989179611206055\n",
      "Epoch [507/1500], Training Loss: 7.500354633998842, Validation Loss: 28.903663635253906\n",
      "Epoch [508/1500], Training Loss: 7.470642087011553, Validation Loss: 28.821353912353516\n",
      "Epoch [509/1500], Training Loss: 7.441741769117648, Validation Loss: 28.741931915283203\n",
      "Epoch [510/1500], Training Loss: 7.4136968606797655, Validation Loss: 28.665142059326172\n",
      "Epoch [511/1500], Training Loss: 7.386355250888617, Validation Loss: 28.591306686401367\n",
      "Epoch [512/1500], Training Loss: 7.359694102448371, Validation Loss: 28.519319534301758\n",
      "Epoch [513/1500], Training Loss: 7.333722746492547, Validation Loss: 28.449989318847656\n",
      "Epoch [514/1500], Training Loss: 7.308456640139473, Validation Loss: 28.383201599121094\n",
      "Epoch [515/1500], Training Loss: 7.283770050912548, Validation Loss: 28.31835174560547\n",
      "Epoch [516/1500], Training Loss: 7.259722362053841, Validation Loss: 28.25597381591797\n",
      "Epoch [517/1500], Training Loss: 7.236305208804006, Validation Loss: 28.196115493774414\n",
      "Epoch [518/1500], Training Loss: 7.213455489499947, Validation Loss: 28.13850212097168\n",
      "Epoch [519/1500], Training Loss: 7.191271941253439, Validation Loss: 28.08414077758789\n",
      "Epoch [520/1500], Training Loss: 7.169669677026883, Validation Loss: 28.03215217590332\n",
      "Epoch [521/1500], Training Loss: 7.148575949619922, Validation Loss: 27.982934951782227\n",
      "Epoch [522/1500], Training Loss: 7.128017809923928, Validation Loss: 27.9366512298584\n",
      "Epoch [523/1500], Training Loss: 7.107907845866042, Validation Loss: 27.89320182800293\n",
      "Epoch [524/1500], Training Loss: 7.088239417040761, Validation Loss: 27.8522891998291\n",
      "Epoch [525/1500], Training Loss: 7.068924222773733, Validation Loss: 27.813636779785156\n",
      "Epoch [526/1500], Training Loss: 7.050071640046038, Validation Loss: 27.776683807373047\n",
      "Epoch [527/1500], Training Loss: 7.031496755330427, Validation Loss: 27.741849899291992\n",
      "Epoch [528/1500], Training Loss: 7.013118774504119, Validation Loss: 27.707651138305664\n",
      "Epoch [529/1500], Training Loss: 6.994981742765605, Validation Loss: 27.67428970336914\n",
      "Epoch [530/1500], Training Loss: 6.977084238285603, Validation Loss: 27.641183853149414\n",
      "Epoch [531/1500], Training Loss: 6.959270170207426, Validation Loss: 27.608631134033203\n",
      "Epoch [532/1500], Training Loss: 6.9416952945429236, Validation Loss: 27.57589340209961\n",
      "Epoch [533/1500], Training Loss: 6.924247312786173, Validation Loss: 27.54368782043457\n",
      "Epoch [534/1500], Training Loss: 6.906885594771506, Validation Loss: 27.511425018310547\n",
      "Epoch [535/1500], Training Loss: 6.889682781868402, Validation Loss: 27.479156494140625\n",
      "Epoch [536/1500], Training Loss: 6.872551862412185, Validation Loss: 27.44661521911621\n",
      "Epoch [537/1500], Training Loss: 6.855558361292898, Validation Loss: 27.41414451599121\n",
      "Epoch [538/1500], Training Loss: 6.838655382388991, Validation Loss: 27.38150978088379\n",
      "Epoch [539/1500], Training Loss: 6.821785479348716, Validation Loss: 27.348854064941406\n",
      "Epoch [540/1500], Training Loss: 6.80503545229715, Validation Loss: 27.31591796875\n",
      "Epoch [541/1500], Training Loss: 6.788394230967534, Validation Loss: 27.281864166259766\n",
      "Epoch [542/1500], Training Loss: 6.771800340096615, Validation Loss: 27.247562408447266\n",
      "Epoch [543/1500], Training Loss: 6.755300425017665, Validation Loss: 27.21284294128418\n",
      "Epoch [544/1500], Training Loss: 6.738890074339345, Validation Loss: 27.177785873413086\n",
      "Epoch [545/1500], Training Loss: 6.722556279435237, Validation Loss: 27.141820907592773\n",
      "Epoch [546/1500], Training Loss: 6.706245056177699, Validation Loss: 27.105073928833008\n",
      "Epoch [547/1500], Training Loss: 6.689976532612965, Validation Loss: 27.06785774230957\n",
      "Epoch [548/1500], Training Loss: 6.673770268768724, Validation Loss: 27.029470443725586\n",
      "Epoch [549/1500], Training Loss: 6.657624913473372, Validation Loss: 26.990192413330078\n",
      "Epoch [550/1500], Training Loss: 6.6415294622651535, Validation Loss: 26.95030975341797\n",
      "Epoch [551/1500], Training Loss: 6.625527424213963, Validation Loss: 26.90946388244629\n",
      "Epoch [552/1500], Training Loss: 6.609610696682652, Validation Loss: 26.867477416992188\n",
      "Epoch [553/1500], Training Loss: 6.5937749388149065, Validation Loss: 26.82468032836914\n",
      "Epoch [554/1500], Training Loss: 6.578001474477714, Validation Loss: 26.781089782714844\n",
      "Epoch [555/1500], Training Loss: 6.56232289843007, Validation Loss: 26.73682403564453\n",
      "Epoch [556/1500], Training Loss: 6.546686480435811, Validation Loss: 26.69179916381836\n",
      "Epoch [557/1500], Training Loss: 6.531174429842858, Validation Loss: 26.646060943603516\n",
      "Epoch [558/1500], Training Loss: 6.5157099382018, Validation Loss: 26.59971046447754\n",
      "Epoch [559/1500], Training Loss: 6.500350980951131, Validation Loss: 26.553049087524414\n",
      "Epoch [560/1500], Training Loss: 6.485050603599412, Validation Loss: 26.505828857421875\n",
      "Epoch [561/1500], Training Loss: 6.4698957652371165, Validation Loss: 26.457935333251953\n",
      "Epoch [562/1500], Training Loss: 6.4548391822195335, Validation Loss: 26.409961700439453\n",
      "Epoch [563/1500], Training Loss: 6.439892530563336, Validation Loss: 26.36142921447754\n",
      "Epoch [564/1500], Training Loss: 6.4250603907505495, Validation Loss: 26.312673568725586\n",
      "Epoch [565/1500], Training Loss: 6.410336235712419, Validation Loss: 26.263931274414062\n",
      "Epoch [566/1500], Training Loss: 6.395752204743819, Validation Loss: 26.215038299560547\n",
      "Epoch [567/1500], Training Loss: 6.381347774921811, Validation Loss: 26.166013717651367\n",
      "Epoch [568/1500], Training Loss: 6.367064914761304, Validation Loss: 26.117496490478516\n",
      "Epoch [569/1500], Training Loss: 6.352899580944126, Validation Loss: 26.068735122680664\n",
      "Epoch [570/1500], Training Loss: 6.338891668820949, Validation Loss: 26.020151138305664\n",
      "Epoch [571/1500], Training Loss: 6.325040050390622, Validation Loss: 25.97144889831543\n",
      "Epoch [572/1500], Training Loss: 6.311322650821704, Validation Loss: 25.923128128051758\n",
      "Epoch [573/1500], Training Loss: 6.297763604477025, Validation Loss: 25.874862670898438\n",
      "Epoch [574/1500], Training Loss: 6.284341778040936, Validation Loss: 25.827259063720703\n",
      "Epoch [575/1500], Training Loss: 6.270990228537718, Validation Loss: 25.779415130615234\n",
      "Epoch [576/1500], Training Loss: 6.257773825832837, Validation Loss: 25.731727600097656\n",
      "Epoch [577/1500], Training Loss: 6.244689195281928, Validation Loss: 25.68463706970215\n",
      "Epoch [578/1500], Training Loss: 6.23175745946998, Validation Loss: 25.6375732421875\n",
      "Epoch [579/1500], Training Loss: 6.2189324516477535, Validation Loss: 25.590707778930664\n",
      "Epoch [580/1500], Training Loss: 6.206200244169799, Validation Loss: 25.544431686401367\n",
      "Epoch [581/1500], Training Loss: 6.19356017568298, Validation Loss: 25.49820899963379\n",
      "Epoch [582/1500], Training Loss: 6.180976909421793, Validation Loss: 25.452356338500977\n",
      "Epoch [583/1500], Training Loss: 6.168484388840679, Validation Loss: 25.406705856323242\n",
      "Epoch [584/1500], Training Loss: 6.156085738581089, Validation Loss: 25.361801147460938\n",
      "Epoch [585/1500], Training Loss: 6.143784717989809, Validation Loss: 25.316987991333008\n",
      "Epoch [586/1500], Training Loss: 6.13153678809744, Validation Loss: 25.2724552154541\n",
      "Epoch [587/1500], Training Loss: 6.11938109920597, Validation Loss: 25.228212356567383\n",
      "Epoch [588/1500], Training Loss: 6.107286501752405, Validation Loss: 25.184585571289062\n",
      "Epoch [589/1500], Training Loss: 6.095269431147288, Validation Loss: 25.141427993774414\n",
      "Epoch [590/1500], Training Loss: 6.083328109863275, Validation Loss: 25.098865509033203\n",
      "Epoch [591/1500], Training Loss: 6.071480656931288, Validation Loss: 25.057016372680664\n",
      "Epoch [592/1500], Training Loss: 6.059674021789689, Validation Loss: 25.015453338623047\n",
      "Epoch [593/1500], Training Loss: 6.047921977254659, Validation Loss: 24.974468231201172\n",
      "Epoch [594/1500], Training Loss: 6.036247993881833, Validation Loss: 24.93398666381836\n",
      "Epoch [595/1500], Training Loss: 6.024693178128482, Validation Loss: 24.894254684448242\n",
      "Epoch [596/1500], Training Loss: 6.013227935504336, Validation Loss: 24.85504722595215\n",
      "Epoch [597/1500], Training Loss: 6.001864032853957, Validation Loss: 24.816123962402344\n",
      "Epoch [598/1500], Training Loss: 5.9906128838145465, Validation Loss: 24.777799606323242\n",
      "Epoch [599/1500], Training Loss: 5.979492227433234, Validation Loss: 24.739967346191406\n",
      "Epoch [600/1500], Training Loss: 5.968425212559576, Validation Loss: 24.702674865722656\n",
      "Epoch [601/1500], Training Loss: 5.957450207277079, Validation Loss: 24.665729522705078\n",
      "Epoch [602/1500], Training Loss: 5.946564766785763, Validation Loss: 24.629240036010742\n",
      "Epoch [603/1500], Training Loss: 5.935784495804796, Validation Loss: 24.5931453704834\n",
      "Epoch [604/1500], Training Loss: 5.925111893065237, Validation Loss: 24.55779457092285\n",
      "Epoch [605/1500], Training Loss: 5.9145122624850135, Validation Loss: 24.522890090942383\n",
      "Epoch [606/1500], Training Loss: 5.9039977696228965, Validation Loss: 24.488052368164062\n",
      "Epoch [607/1500], Training Loss: 5.8935688777673025, Validation Loss: 24.453861236572266\n",
      "Epoch [608/1500], Training Loss: 5.88323674639333, Validation Loss: 24.419946670532227\n",
      "Epoch [609/1500], Training Loss: 5.872995635218667, Validation Loss: 24.386079788208008\n",
      "Epoch [610/1500], Training Loss: 5.862856517377547, Validation Loss: 24.35272216796875\n",
      "Epoch [611/1500], Training Loss: 5.852791585238955, Validation Loss: 24.319536209106445\n",
      "Epoch [612/1500], Training Loss: 5.842826967360603, Validation Loss: 24.28678321838379\n",
      "Epoch [613/1500], Training Loss: 5.832910211291506, Validation Loss: 24.25410270690918\n",
      "Epoch [614/1500], Training Loss: 5.823103380052961, Validation Loss: 24.221691131591797\n",
      "Epoch [615/1500], Training Loss: 5.813347059180576, Validation Loss: 24.18943214416504\n",
      "Epoch [616/1500], Training Loss: 5.803699315397957, Validation Loss: 24.15779685974121\n",
      "Epoch [617/1500], Training Loss: 5.794133861290048, Validation Loss: 24.126018524169922\n",
      "Epoch [618/1500], Training Loss: 5.78466229455809, Validation Loss: 24.094518661499023\n",
      "Epoch [619/1500], Training Loss: 5.775320534088446, Validation Loss: 24.063379287719727\n",
      "Epoch [620/1500], Training Loss: 5.76609327116532, Validation Loss: 24.032329559326172\n",
      "Epoch [621/1500], Training Loss: 5.756973369465439, Validation Loss: 24.0013484954834\n",
      "Epoch [622/1500], Training Loss: 5.747955604632032, Validation Loss: 23.97073745727539\n",
      "Epoch [623/1500], Training Loss: 5.739060917239187, Validation Loss: 23.94044303894043\n",
      "Epoch [624/1500], Training Loss: 5.730241058813052, Validation Loss: 23.9103946685791\n",
      "Epoch [625/1500], Training Loss: 5.721526569349094, Validation Loss: 23.880502700805664\n",
      "Epoch [626/1500], Training Loss: 5.712950488428343, Validation Loss: 23.851076126098633\n",
      "Epoch [627/1500], Training Loss: 5.7044828986094895, Validation Loss: 23.821765899658203\n",
      "Epoch [628/1500], Training Loss: 5.696141140553767, Validation Loss: 23.792715072631836\n",
      "Epoch [629/1500], Training Loss: 5.687867847005528, Validation Loss: 23.764020919799805\n",
      "Epoch [630/1500], Training Loss: 5.679721357831226, Validation Loss: 23.73560905456543\n",
      "Epoch [631/1500], Training Loss: 5.671680598587639, Validation Loss: 23.707551956176758\n",
      "Epoch [632/1500], Training Loss: 5.663706529611012, Validation Loss: 23.67960548400879\n",
      "Epoch [633/1500], Training Loss: 5.655854537267225, Validation Loss: 23.651813507080078\n",
      "Epoch [634/1500], Training Loss: 5.648104761577814, Validation Loss: 23.624759674072266\n",
      "Epoch [635/1500], Training Loss: 5.6404284834284875, Validation Loss: 23.597686767578125\n",
      "Epoch [636/1500], Training Loss: 5.632833736499037, Validation Loss: 23.570783615112305\n",
      "Epoch [637/1500], Training Loss: 5.625306567228056, Validation Loss: 23.544288635253906\n",
      "Epoch [638/1500], Training Loss: 5.617840563580393, Validation Loss: 23.51791763305664\n",
      "Epoch [639/1500], Training Loss: 5.6104503303860875, Validation Loss: 23.49163246154785\n",
      "Epoch [640/1500], Training Loss: 5.603096375032154, Validation Loss: 23.465293884277344\n",
      "Epoch [641/1500], Training Loss: 5.595789196310262, Validation Loss: 23.439186096191406\n",
      "Epoch [642/1500], Training Loss: 5.588518278362155, Validation Loss: 23.413116455078125\n",
      "Epoch [643/1500], Training Loss: 5.581266625468467, Validation Loss: 23.387107849121094\n",
      "Epoch [644/1500], Training Loss: 5.574030672905175, Validation Loss: 23.360971450805664\n",
      "Epoch [645/1500], Training Loss: 5.566829477240545, Validation Loss: 23.334779739379883\n",
      "Epoch [646/1500], Training Loss: 5.559631747436116, Validation Loss: 23.308422088623047\n",
      "Epoch [647/1500], Training Loss: 5.552465653449651, Validation Loss: 23.281936645507812\n",
      "Epoch [648/1500], Training Loss: 5.545282840966542, Validation Loss: 23.255252838134766\n",
      "Epoch [649/1500], Training Loss: 5.538117528371931, Validation Loss: 23.228364944458008\n",
      "Epoch [650/1500], Training Loss: 5.530936912356886, Validation Loss: 23.20104217529297\n",
      "Epoch [651/1500], Training Loss: 5.52375833894482, Validation Loss: 23.173826217651367\n",
      "Epoch [652/1500], Training Loss: 5.516551873381965, Validation Loss: 23.146278381347656\n",
      "Epoch [653/1500], Training Loss: 5.509274230108971, Validation Loss: 23.118350982666016\n",
      "Epoch [654/1500], Training Loss: 5.501974398629396, Validation Loss: 23.09006118774414\n",
      "Epoch [655/1500], Training Loss: 5.494630237944747, Validation Loss: 23.061626434326172\n",
      "Epoch [656/1500], Training Loss: 5.487221732333663, Validation Loss: 23.03258514404297\n",
      "Epoch [657/1500], Training Loss: 5.479773062159994, Validation Loss: 23.003223419189453\n",
      "Epoch [658/1500], Training Loss: 5.472287416194433, Validation Loss: 22.973583221435547\n",
      "Epoch [659/1500], Training Loss: 5.46472075581018, Validation Loss: 22.943355560302734\n",
      "Epoch [660/1500], Training Loss: 5.457092192115077, Validation Loss: 22.9127254486084\n",
      "Epoch [661/1500], Training Loss: 5.44936315751383, Validation Loss: 22.881521224975586\n",
      "Epoch [662/1500], Training Loss: 5.4415306491933615, Validation Loss: 22.849945068359375\n",
      "Epoch [663/1500], Training Loss: 5.433565086619273, Validation Loss: 22.81806182861328\n",
      "Epoch [664/1500], Training Loss: 5.425546161252498, Validation Loss: 22.785484313964844\n",
      "Epoch [665/1500], Training Loss: 5.417402504715352, Validation Loss: 22.752321243286133\n",
      "Epoch [666/1500], Training Loss: 5.409145255170292, Validation Loss: 22.718568801879883\n",
      "Epoch [667/1500], Training Loss: 5.400737890008395, Validation Loss: 22.684585571289062\n",
      "Epoch [668/1500], Training Loss: 5.392201960501821, Validation Loss: 22.649707794189453\n",
      "Epoch [669/1500], Training Loss: 5.383533260925369, Validation Loss: 22.614227294921875\n",
      "Epoch [670/1500], Training Loss: 5.374705855806481, Validation Loss: 22.577911376953125\n",
      "Epoch [671/1500], Training Loss: 5.365725181492663, Validation Loss: 22.540781021118164\n",
      "Epoch [672/1500], Training Loss: 5.356566981200903, Validation Loss: 22.502948760986328\n",
      "Epoch [673/1500], Training Loss: 5.347211001540163, Validation Loss: 22.4644775390625\n",
      "Epoch [674/1500], Training Loss: 5.337696503743381, Validation Loss: 22.425189971923828\n",
      "Epoch [675/1500], Training Loss: 5.327983809236836, Validation Loss: 22.38490104675293\n",
      "Epoch [676/1500], Training Loss: 5.318053185041535, Validation Loss: 22.34421157836914\n",
      "Epoch [677/1500], Training Loss: 5.307913135669535, Validation Loss: 22.30233383178711\n",
      "Epoch [678/1500], Training Loss: 5.297556632170165, Validation Loss: 22.259403228759766\n",
      "Epoch [679/1500], Training Loss: 5.28693397361747, Validation Loss: 22.215576171875\n",
      "Epoch [680/1500], Training Loss: 5.276090915650289, Validation Loss: 22.170717239379883\n",
      "Epoch [681/1500], Training Loss: 5.264989336170937, Validation Loss: 22.124675750732422\n",
      "Epoch [682/1500], Training Loss: 5.253612731734885, Validation Loss: 22.077558517456055\n",
      "Epoch [683/1500], Training Loss: 5.2419746482270755, Validation Loss: 22.029258728027344\n",
      "Epoch [684/1500], Training Loss: 5.230068835567605, Validation Loss: 21.97941780090332\n",
      "Epoch [685/1500], Training Loss: 5.217852086644216, Validation Loss: 21.92851448059082\n",
      "Epoch [686/1500], Training Loss: 5.205350943114273, Validation Loss: 21.876245498657227\n",
      "Epoch [687/1500], Training Loss: 5.192554462520381, Validation Loss: 21.822406768798828\n",
      "Epoch [688/1500], Training Loss: 5.17943265717079, Validation Loss: 21.76715850830078\n",
      "Epoch [689/1500], Training Loss: 5.166032753914914, Validation Loss: 21.71039581298828\n",
      "Epoch [690/1500], Training Loss: 5.152302652818836, Validation Loss: 21.652240753173828\n",
      "Epoch [691/1500], Training Loss: 5.138250628223202, Validation Loss: 21.592254638671875\n",
      "Epoch [692/1500], Training Loss: 5.123885757123653, Validation Loss: 21.530668258666992\n",
      "Epoch [693/1500], Training Loss: 5.109173241313928, Validation Loss: 21.46748161315918\n",
      "Epoch [694/1500], Training Loss: 5.094148729564924, Validation Loss: 21.402420043945312\n",
      "Epoch [695/1500], Training Loss: 5.078822090028286, Validation Loss: 21.335609436035156\n",
      "Epoch [696/1500], Training Loss: 5.063161943496911, Validation Loss: 21.267053604125977\n",
      "Epoch [697/1500], Training Loss: 5.047171926996199, Validation Loss: 21.196598052978516\n",
      "Epoch [698/1500], Training Loss: 5.030845615861536, Validation Loss: 21.1242733001709\n",
      "Epoch [699/1500], Training Loss: 5.01417653255495, Validation Loss: 21.050033569335938\n",
      "Epoch [700/1500], Training Loss: 4.997174174537294, Validation Loss: 20.97395896911621\n",
      "Epoch [701/1500], Training Loss: 4.979838598268214, Validation Loss: 20.895904541015625\n",
      "Epoch [702/1500], Training Loss: 4.962177679526644, Validation Loss: 20.81595802307129\n",
      "Epoch [703/1500], Training Loss: 4.944161271748574, Validation Loss: 20.733837127685547\n",
      "Epoch [704/1500], Training Loss: 4.925807562721856, Validation Loss: 20.649751663208008\n",
      "Epoch [705/1500], Training Loss: 4.907115891062511, Validation Loss: 20.5634765625\n",
      "Epoch [706/1500], Training Loss: 4.888059598038041, Validation Loss: 20.475061416625977\n",
      "Epoch [707/1500], Training Loss: 4.868685601637909, Validation Loss: 20.384563446044922\n",
      "Epoch [708/1500], Training Loss: 4.848964231336857, Validation Loss: 20.291894912719727\n",
      "Epoch [709/1500], Training Loss: 4.828942139657197, Validation Loss: 20.196834564208984\n",
      "Epoch [710/1500], Training Loss: 4.808614826265327, Validation Loss: 20.099273681640625\n",
      "Epoch [711/1500], Training Loss: 4.7880237743391705, Validation Loss: 19.999235153198242\n",
      "Epoch [712/1500], Training Loss: 4.767157917662894, Validation Loss: 19.89710235595703\n",
      "Epoch [713/1500], Training Loss: 4.7460484764911595, Validation Loss: 19.79241371154785\n",
      "Epoch [714/1500], Training Loss: 4.7246605225560785, Validation Loss: 19.685333251953125\n",
      "Epoch [715/1500], Training Loss: 4.703039668887885, Validation Loss: 19.575586318969727\n",
      "Epoch [716/1500], Training Loss: 4.6811906719377525, Validation Loss: 19.46367835998535\n",
      "Epoch [717/1500], Training Loss: 4.659136054138543, Validation Loss: 19.3492488861084\n",
      "Epoch [718/1500], Training Loss: 4.636879537128907, Validation Loss: 19.232730865478516\n",
      "Epoch [719/1500], Training Loss: 4.614432146402481, Validation Loss: 19.113365173339844\n",
      "Epoch [720/1500], Training Loss: 4.591810892940917, Validation Loss: 18.991756439208984\n",
      "Epoch [721/1500], Training Loss: 4.569048127871207, Validation Loss: 18.867752075195312\n",
      "Epoch [722/1500], Training Loss: 4.546118357407603, Validation Loss: 18.741413116455078\n",
      "Epoch [723/1500], Training Loss: 4.5230493486769845, Validation Loss: 18.61258316040039\n",
      "Epoch [724/1500], Training Loss: 4.499852719979227, Validation Loss: 18.481582641601562\n",
      "Epoch [725/1500], Training Loss: 4.476521186767741, Validation Loss: 18.348657608032227\n",
      "Epoch [726/1500], Training Loss: 4.45306026099896, Validation Loss: 18.213220596313477\n",
      "Epoch [727/1500], Training Loss: 4.4294562520264655, Validation Loss: 18.076095581054688\n",
      "Epoch [728/1500], Training Loss: 4.405730670980329, Validation Loss: 17.93668556213379\n",
      "Epoch [729/1500], Training Loss: 4.381874479232978, Validation Loss: 17.79557228088379\n",
      "Epoch [730/1500], Training Loss: 4.357893792802569, Validation Loss: 17.652450561523438\n",
      "Epoch [731/1500], Training Loss: 4.333794589637683, Validation Loss: 17.507986068725586\n",
      "Epoch [732/1500], Training Loss: 4.309570913587038, Validation Loss: 17.361526489257812\n",
      "Epoch [733/1500], Training Loss: 4.285288170663915, Validation Loss: 17.214366912841797\n",
      "Epoch [734/1500], Training Loss: 4.26090797567405, Validation Loss: 17.065404891967773\n",
      "Epoch [735/1500], Training Loss: 4.2364826406764795, Validation Loss: 16.916044235229492\n",
      "Epoch [736/1500], Training Loss: 4.212062409220284, Validation Loss: 16.76634407043457\n",
      "Epoch [737/1500], Training Loss: 4.187709491197212, Validation Loss: 16.61738395690918\n",
      "Epoch [738/1500], Training Loss: 4.163466515503409, Validation Loss: 16.469669342041016\n",
      "Epoch [739/1500], Training Loss: 4.13940104430532, Validation Loss: 16.323625564575195\n",
      "Epoch [740/1500], Training Loss: 4.11559516284579, Validation Loss: 16.180416107177734\n",
      "Epoch [741/1500], Training Loss: 4.092185195521392, Validation Loss: 16.040420532226562\n",
      "Epoch [742/1500], Training Loss: 4.069182360032955, Validation Loss: 15.904399871826172\n",
      "Epoch [743/1500], Training Loss: 4.046641950526183, Validation Loss: 15.77261734008789\n",
      "Epoch [744/1500], Training Loss: 4.024615083351745, Validation Loss: 15.645447731018066\n",
      "Epoch [745/1500], Training Loss: 4.003100279345725, Validation Loss: 15.522930145263672\n",
      "Epoch [746/1500], Training Loss: 3.982143491562587, Validation Loss: 15.405383110046387\n",
      "Epoch [747/1500], Training Loss: 3.961695394815873, Validation Loss: 15.29260540008545\n",
      "Epoch [748/1500], Training Loss: 3.9418220051646555, Validation Loss: 15.184870719909668\n",
      "Epoch [749/1500], Training Loss: 3.9224571462297324, Validation Loss: 15.082048416137695\n",
      "Epoch [750/1500], Training Loss: 3.9035923556620995, Validation Loss: 14.983906745910645\n",
      "Epoch [751/1500], Training Loss: 3.8852039483803957, Validation Loss: 14.890503883361816\n",
      "Epoch [752/1500], Training Loss: 3.8672614055548076, Validation Loss: 14.80108642578125\n",
      "Epoch [753/1500], Training Loss: 3.84978210529693, Validation Loss: 14.716146469116211\n",
      "Epoch [754/1500], Training Loss: 3.8326773486720263, Validation Loss: 14.63547420501709\n",
      "Epoch [755/1500], Training Loss: 3.815950206335096, Validation Loss: 14.558552742004395\n",
      "Epoch [756/1500], Training Loss: 3.799591658458157, Validation Loss: 14.485108375549316\n",
      "Epoch [757/1500], Training Loss: 3.783552080030183, Validation Loss: 14.415656089782715\n",
      "Epoch [758/1500], Training Loss: 3.7678122416690023, Validation Loss: 14.348861694335938\n",
      "Epoch [759/1500], Training Loss: 3.7523668557370375, Validation Loss: 14.285188674926758\n",
      "Epoch [760/1500], Training Loss: 3.7371824412326586, Validation Loss: 14.224302291870117\n",
      "Epoch [761/1500], Training Loss: 3.7222343679383116, Validation Loss: 14.165816307067871\n",
      "Epoch [762/1500], Training Loss: 3.7075218977173834, Validation Loss: 14.109798431396484\n",
      "Epoch [763/1500], Training Loss: 3.693030303973788, Validation Loss: 14.055925369262695\n",
      "Epoch [764/1500], Training Loss: 3.678726655310587, Validation Loss: 14.003676414489746\n",
      "Epoch [765/1500], Training Loss: 3.664597948618484, Validation Loss: 13.953424453735352\n",
      "Epoch [766/1500], Training Loss: 3.6506567458483903, Validation Loss: 13.904903411865234\n",
      "Epoch [767/1500], Training Loss: 3.636862962897512, Validation Loss: 13.857236862182617\n",
      "Epoch [768/1500], Training Loss: 3.62319815752435, Validation Loss: 13.81068229675293\n",
      "Epoch [769/1500], Training Loss: 3.609680795867476, Validation Loss: 13.765421867370605\n",
      "Epoch [770/1500], Training Loss: 3.5962844062704624, Validation Loss: 13.721440315246582\n",
      "Epoch [771/1500], Training Loss: 3.5830357281998633, Validation Loss: 13.67784309387207\n",
      "Epoch [772/1500], Training Loss: 3.5699347794043583, Validation Loss: 13.635051727294922\n",
      "Epoch [773/1500], Training Loss: 3.5569487618018782, Validation Loss: 13.59261417388916\n",
      "Epoch [774/1500], Training Loss: 3.544072822219617, Validation Loss: 13.550338745117188\n",
      "Epoch [775/1500], Training Loss: 3.531341851185665, Validation Loss: 13.50890064239502\n",
      "Epoch [776/1500], Training Loss: 3.5186757291449062, Validation Loss: 13.467257499694824\n",
      "Epoch [777/1500], Training Loss: 3.506114897453344, Validation Loss: 13.42611312866211\n",
      "Epoch [778/1500], Training Loss: 3.4936592805235094, Validation Loss: 13.384623527526855\n",
      "Epoch [779/1500], Training Loss: 3.4813093102144066, Validation Loss: 13.342827796936035\n",
      "Epoch [780/1500], Training Loss: 3.4690407790967224, Validation Loss: 13.300366401672363\n",
      "Epoch [781/1500], Training Loss: 3.456886691981109, Validation Loss: 13.256997108459473\n",
      "Epoch [782/1500], Training Loss: 3.4448451519107537, Validation Loss: 13.213541984558105\n",
      "Epoch [783/1500], Training Loss: 3.432880136731206, Validation Loss: 13.16899299621582\n",
      "Epoch [784/1500], Training Loss: 3.421007375396731, Validation Loss: 13.123143196105957\n",
      "Epoch [785/1500], Training Loss: 3.4091901084903986, Validation Loss: 13.076586723327637\n",
      "Epoch [786/1500], Training Loss: 3.397440170494898, Validation Loss: 13.028288841247559\n",
      "Epoch [787/1500], Training Loss: 3.3857334056827413, Validation Loss: 12.978692054748535\n",
      "Epoch [788/1500], Training Loss: 3.3740348805632623, Validation Loss: 12.92728042602539\n",
      "Epoch [789/1500], Training Loss: 3.3623288765476436, Validation Loss: 12.873896598815918\n",
      "Epoch [790/1500], Training Loss: 3.3506266496359207, Validation Loss: 12.818497657775879\n",
      "Epoch [791/1500], Training Loss: 3.338909511313792, Validation Loss: 12.760970115661621\n",
      "Epoch [792/1500], Training Loss: 3.3271714145751448, Validation Loss: 12.701102256774902\n",
      "Epoch [793/1500], Training Loss: 3.3153788515306366, Validation Loss: 12.638805389404297\n",
      "Epoch [794/1500], Training Loss: 3.3035608204438853, Validation Loss: 12.57401180267334\n",
      "Epoch [795/1500], Training Loss: 3.291670585690599, Validation Loss: 12.506904602050781\n",
      "Epoch [796/1500], Training Loss: 3.27970434660687, Validation Loss: 12.438283920288086\n",
      "Epoch [797/1500], Training Loss: 3.2677022077109004, Validation Loss: 12.367392539978027\n",
      "Epoch [798/1500], Training Loss: 3.255630206763007, Validation Loss: 12.294706344604492\n",
      "Epoch [799/1500], Training Loss: 3.243496515876449, Validation Loss: 12.221345901489258\n",
      "Epoch [800/1500], Training Loss: 3.231323835459896, Validation Loss: 12.146238327026367\n",
      "Epoch [801/1500], Training Loss: 3.2191361309003086, Validation Loss: 12.071331977844238\n",
      "Epoch [802/1500], Training Loss: 3.207018564587619, Validation Loss: 11.995586395263672\n",
      "Epoch [803/1500], Training Loss: 3.195046518290652, Validation Loss: 11.919889450073242\n",
      "Epoch [804/1500], Training Loss: 3.183271485416658, Validation Loss: 11.846046447753906\n",
      "Epoch [805/1500], Training Loss: 3.17175890590089, Validation Loss: 11.774359703063965\n",
      "Epoch [806/1500], Training Loss: 3.1605910860798447, Validation Loss: 11.705288887023926\n",
      "Epoch [807/1500], Training Loss: 3.149770083571887, Validation Loss: 11.638628959655762\n",
      "Epoch [808/1500], Training Loss: 3.139327207775486, Validation Loss: 11.574952125549316\n",
      "Epoch [809/1500], Training Loss: 3.129243307393259, Validation Loss: 11.513957977294922\n",
      "Epoch [810/1500], Training Loss: 3.119471063476099, Validation Loss: 11.454465866088867\n",
      "Epoch [811/1500], Training Loss: 3.1099568855772306, Validation Loss: 11.396622657775879\n",
      "Epoch [812/1500], Training Loss: 3.100656159197334, Validation Loss: 11.339499473571777\n",
      "Epoch [813/1500], Training Loss: 3.0915217126608776, Validation Loss: 11.282034873962402\n",
      "Epoch [814/1500], Training Loss: 3.0824843356901823, Validation Loss: 11.224555015563965\n",
      "Epoch [815/1500], Training Loss: 3.0735202800882018, Validation Loss: 11.166950225830078\n",
      "Epoch [816/1500], Training Loss: 3.0646542087358064, Validation Loss: 11.10876178741455\n",
      "Epoch [817/1500], Training Loss: 3.0558163937375893, Validation Loss: 11.050200462341309\n",
      "Epoch [818/1500], Training Loss: 3.0470258462091864, Validation Loss: 10.992156982421875\n",
      "Epoch [819/1500], Training Loss: 3.038248413828384, Validation Loss: 10.933708190917969\n",
      "Epoch [820/1500], Training Loss: 3.029482031982088, Validation Loss: 10.875354766845703\n",
      "Epoch [821/1500], Training Loss: 3.0207324034686516, Validation Loss: 10.817503929138184\n",
      "Epoch [822/1500], Training Loss: 3.011998303871504, Validation Loss: 10.75877857208252\n",
      "Epoch [823/1500], Training Loss: 3.003257849688731, Validation Loss: 10.700160026550293\n",
      "Epoch [824/1500], Training Loss: 2.9945039950445205, Validation Loss: 10.641563415527344\n",
      "Epoch [825/1500], Training Loss: 2.985755120998664, Validation Loss: 10.5826416015625\n",
      "Epoch [826/1500], Training Loss: 2.976984578423586, Validation Loss: 10.52377986907959\n",
      "Epoch [827/1500], Training Loss: 2.9682126288529074, Validation Loss: 10.46465015411377\n",
      "Epoch [828/1500], Training Loss: 2.9594306888621267, Validation Loss: 10.405311584472656\n",
      "Epoch [829/1500], Training Loss: 2.9506382972475325, Validation Loss: 10.346089363098145\n",
      "Epoch [830/1500], Training Loss: 2.9418569374326267, Validation Loss: 10.286895751953125\n",
      "Epoch [831/1500], Training Loss: 2.933057592616124, Validation Loss: 10.2282075881958\n",
      "Epoch [832/1500], Training Loss: 2.9242531257571147, Validation Loss: 10.169536590576172\n",
      "Epoch [833/1500], Training Loss: 2.9154427887015033, Validation Loss: 10.111001014709473\n",
      "Epoch [834/1500], Training Loss: 2.9066365466438544, Validation Loss: 10.052830696105957\n",
      "Epoch [835/1500], Training Loss: 2.8978155213955663, Validation Loss: 9.994453430175781\n",
      "Epoch [836/1500], Training Loss: 2.889006600897407, Validation Loss: 9.936711311340332\n",
      "Epoch [837/1500], Training Loss: 2.880187150330152, Validation Loss: 9.879067420959473\n",
      "Epoch [838/1500], Training Loss: 2.8714042883399653, Validation Loss: 9.822202682495117\n",
      "Epoch [839/1500], Training Loss: 2.862617579538641, Validation Loss: 9.765168190002441\n",
      "Epoch [840/1500], Training Loss: 2.853850158289803, Validation Loss: 9.708540916442871\n",
      "Epoch [841/1500], Training Loss: 2.845084354023601, Validation Loss: 9.652488708496094\n",
      "Epoch [842/1500], Training Loss: 2.8363476391798526, Validation Loss: 9.596242904663086\n",
      "Epoch [843/1500], Training Loss: 2.8276181193802112, Validation Loss: 9.540095329284668\n",
      "Epoch [844/1500], Training Loss: 2.8189315920233033, Validation Loss: 9.484538078308105\n",
      "Epoch [845/1500], Training Loss: 2.810270370306733, Validation Loss: 9.428933143615723\n",
      "Epoch [846/1500], Training Loss: 2.801665839933494, Validation Loss: 9.373982429504395\n",
      "Epoch [847/1500], Training Loss: 2.793103274752939, Validation Loss: 9.319131851196289\n",
      "Epoch [848/1500], Training Loss: 2.784576325492233, Validation Loss: 9.264778137207031\n",
      "Epoch [849/1500], Training Loss: 2.776111522079945, Validation Loss: 9.211213111877441\n",
      "Epoch [850/1500], Training Loss: 2.767694264828744, Validation Loss: 9.15779972076416\n",
      "Epoch [851/1500], Training Loss: 2.7593623598419272, Validation Loss: 9.104815483093262\n",
      "Epoch [852/1500], Training Loss: 2.7510866081555716, Validation Loss: 9.052557945251465\n",
      "Epoch [853/1500], Training Loss: 2.742881835157492, Validation Loss: 9.000930786132812\n",
      "Epoch [854/1500], Training Loss: 2.7347574703735935, Validation Loss: 8.949722290039062\n",
      "Epoch [855/1500], Training Loss: 2.7267311323126977, Validation Loss: 8.899133682250977\n",
      "Epoch [856/1500], Training Loss: 2.718825665265089, Validation Loss: 8.849624633789062\n",
      "Epoch [857/1500], Training Loss: 2.711030382502853, Validation Loss: 8.80104923248291\n",
      "Epoch [858/1500], Training Loss: 2.7033429658440378, Validation Loss: 8.753255844116211\n",
      "Epoch [859/1500], Training Loss: 2.6957670928130635, Validation Loss: 8.70600414276123\n",
      "Epoch [860/1500], Training Loss: 2.688318289847352, Validation Loss: 8.659809112548828\n",
      "Epoch [861/1500], Training Loss: 2.68100502607075, Validation Loss: 8.614645004272461\n",
      "Epoch [862/1500], Training Loss: 2.6738199041054735, Validation Loss: 8.570497512817383\n",
      "Epoch [863/1500], Training Loss: 2.666763422301352, Validation Loss: 8.526996612548828\n",
      "Epoch [864/1500], Training Loss: 2.659836971810662, Validation Loss: 8.484896659851074\n",
      "Epoch [865/1500], Training Loss: 2.65305340897059, Validation Loss: 8.44385051727295\n",
      "Epoch [866/1500], Training Loss: 2.6464068499296616, Validation Loss: 8.403976440429688\n",
      "Epoch [867/1500], Training Loss: 2.639908094207775, Validation Loss: 8.364986419677734\n",
      "Epoch [868/1500], Training Loss: 2.633530231795176, Validation Loss: 8.32707405090332\n",
      "Epoch [869/1500], Training Loss: 2.627285698842718, Validation Loss: 8.290102005004883\n",
      "Epoch [870/1500], Training Loss: 2.621176761979544, Validation Loss: 8.254499435424805\n",
      "Epoch [871/1500], Training Loss: 2.615192578662957, Validation Loss: 8.219615936279297\n",
      "Epoch [872/1500], Training Loss: 2.609356624370784, Validation Loss: 8.186004638671875\n",
      "Epoch [873/1500], Training Loss: 2.6036355464111454, Validation Loss: 8.15328598022461\n",
      "Epoch [874/1500], Training Loss: 2.5980582345337946, Validation Loss: 8.12158203125\n",
      "Epoch [875/1500], Training Loss: 2.592609956350201, Validation Loss: 8.090959548950195\n",
      "Epoch [876/1500], Training Loss: 2.5872984147539495, Validation Loss: 8.06098461151123\n",
      "Epoch [877/1500], Training Loss: 2.5821015955617996, Validation Loss: 8.032055854797363\n",
      "Epoch [878/1500], Training Loss: 2.577029578673839, Validation Loss: 8.004189491271973\n",
      "Epoch [879/1500], Training Loss: 2.57208724825602, Validation Loss: 7.977131366729736\n",
      "Epoch [880/1500], Training Loss: 2.567252182031795, Validation Loss: 7.950506687164307\n",
      "Epoch [881/1500], Training Loss: 2.5625485893807993, Validation Loss: 7.924599647521973\n",
      "Epoch [882/1500], Training Loss: 2.5579605872105344, Validation Loss: 7.899521827697754\n",
      "Epoch [883/1500], Training Loss: 2.5534839919890615, Validation Loss: 7.875247001647949\n",
      "Epoch [884/1500], Training Loss: 2.5490963436541083, Validation Loss: 7.851870059967041\n",
      "Epoch [885/1500], Training Loss: 2.5448250484298667, Validation Loss: 7.82869291305542\n",
      "Epoch [886/1500], Training Loss: 2.5406455627138547, Validation Loss: 7.806637763977051\n",
      "Epoch [887/1500], Training Loss: 2.5365631622805, Validation Loss: 7.785031795501709\n",
      "Epoch [888/1500], Training Loss: 2.532589677784776, Validation Loss: 7.763925075531006\n",
      "Epoch [889/1500], Training Loss: 2.528696757600986, Validation Loss: 7.7432780265808105\n",
      "Epoch [890/1500], Training Loss: 2.5248989893161675, Validation Loss: 7.7234015464782715\n",
      "Epoch [891/1500], Training Loss: 2.5211866942009444, Validation Loss: 7.704234600067139\n",
      "Epoch [892/1500], Training Loss: 2.5175545095190186, Validation Loss: 7.685473442077637\n",
      "Epoch [893/1500], Training Loss: 2.514007301493253, Validation Loss: 7.667538166046143\n",
      "Epoch [894/1500], Training Loss: 2.5105251176665897, Validation Loss: 7.649962902069092\n",
      "Epoch [895/1500], Training Loss: 2.507127680257015, Validation Loss: 7.632693290710449\n",
      "Epoch [896/1500], Training Loss: 2.503809998932894, Validation Loss: 7.616334438323975\n",
      "Epoch [897/1500], Training Loss: 2.500559743245984, Validation Loss: 7.6005167961120605\n",
      "Epoch [898/1500], Training Loss: 2.497360140929622, Validation Loss: 7.585091590881348\n",
      "Epoch [899/1500], Training Loss: 2.4942286225328134, Validation Loss: 7.569946765899658\n",
      "Epoch [900/1500], Training Loss: 2.4911645567077385, Validation Loss: 7.555769443511963\n",
      "Epoch [901/1500], Training Loss: 2.488158328129577, Validation Loss: 7.5415849685668945\n",
      "Epoch [902/1500], Training Loss: 2.4852087599798227, Validation Loss: 7.528111457824707\n",
      "Epoch [903/1500], Training Loss: 2.4823235818935907, Validation Loss: 7.514824390411377\n",
      "Epoch [904/1500], Training Loss: 2.47950184004052, Validation Loss: 7.501893520355225\n",
      "Epoch [905/1500], Training Loss: 2.4767346114525233, Validation Loss: 7.489775657653809\n",
      "Epoch [906/1500], Training Loss: 2.474007125360912, Validation Loss: 7.477807998657227\n",
      "Epoch [907/1500], Training Loss: 2.4713215839193348, Validation Loss: 7.466650485992432\n",
      "Epoch [908/1500], Training Loss: 2.4686819436267062, Validation Loss: 7.456136703491211\n",
      "Epoch [909/1500], Training Loss: 2.466097732495058, Validation Loss: 7.446184158325195\n",
      "Epoch [910/1500], Training Loss: 2.4635451822380725, Validation Loss: 7.436155319213867\n",
      "Epoch [911/1500], Training Loss: 2.4610307268795566, Validation Loss: 7.426940441131592\n",
      "Epoch [912/1500], Training Loss: 2.458567151721066, Validation Loss: 7.4179229736328125\n",
      "Epoch [913/1500], Training Loss: 2.456162252889725, Validation Loss: 7.409704685211182\n",
      "Epoch [914/1500], Training Loss: 2.4537818734135604, Validation Loss: 7.40186071395874\n",
      "Epoch [915/1500], Training Loss: 2.451435042447223, Validation Loss: 7.394545078277588\n",
      "Epoch [916/1500], Training Loss: 2.4491211827005395, Validation Loss: 7.387936115264893\n",
      "Epoch [917/1500], Training Loss: 2.4468316159115777, Validation Loss: 7.381674289703369\n",
      "Epoch [918/1500], Training Loss: 2.4445777099728203, Validation Loss: 7.375792026519775\n",
      "Epoch [919/1500], Training Loss: 2.4423495427305117, Validation Loss: 7.370102882385254\n",
      "Epoch [920/1500], Training Loss: 2.4401492296948923, Validation Loss: 7.365306854248047\n",
      "Epoch [921/1500], Training Loss: 2.437979052613398, Validation Loss: 7.3606438636779785\n",
      "Epoch [922/1500], Training Loss: 2.4358382313053935, Validation Loss: 7.356487274169922\n",
      "Epoch [923/1500], Training Loss: 2.4337075991865387, Validation Loss: 7.35281229019165\n",
      "Epoch [924/1500], Training Loss: 2.4316011027193207, Validation Loss: 7.349309921264648\n",
      "Epoch [925/1500], Training Loss: 2.4295044523609746, Validation Loss: 7.346584320068359\n",
      "Epoch [926/1500], Training Loss: 2.427436073073421, Validation Loss: 7.344364643096924\n",
      "Epoch [927/1500], Training Loss: 2.4253910990169016, Validation Loss: 7.342151165008545\n",
      "Epoch [928/1500], Training Loss: 2.423380602240081, Validation Loss: 7.3406219482421875\n",
      "Epoch [929/1500], Training Loss: 2.4213885707655884, Validation Loss: 7.339651107788086\n",
      "Epoch [930/1500], Training Loss: 2.419406598484766, Validation Loss: 7.339013576507568\n",
      "Epoch [931/1500], Training Loss: 2.4174463351469186, Validation Loss: 7.339054107666016\n",
      "Epoch [932/1500], Training Loss: 2.4154882124668426, Validation Loss: 7.339555740356445\n",
      "Epoch [933/1500], Training Loss: 2.4135546707017568, Validation Loss: 7.340438365936279\n",
      "Epoch [934/1500], Training Loss: 2.4116150886385093, Validation Loss: 7.341808319091797\n",
      "Epoch [935/1500], Training Loss: 2.4096990905314253, Validation Loss: 7.34337854385376\n",
      "Epoch [936/1500], Training Loss: 2.407801423182156, Validation Loss: 7.345092296600342\n",
      "Epoch [937/1500], Training Loss: 2.405895092030068, Validation Loss: 7.347320079803467\n",
      "Epoch [938/1500], Training Loss: 2.4040135157705453, Validation Loss: 7.350271224975586\n",
      "Epoch [939/1500], Training Loss: 2.4021383513068724, Validation Loss: 7.353236198425293\n",
      "Early stopping at epoch 939\n",
      "Final Test Loss: 7.891520023345947\n",
      "Training model with target variable: close\n",
      "\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0003, window_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Training Loss: 27660.41091913312, Validation Loss: 27200.44140625\n",
      "Epoch [2/1500], Training Loss: 25948.54856126681, Validation Loss: 25640.982421875\n",
      "Epoch [3/1500], Training Loss: 24560.14757714108, Validation Loss: 24334.330078125\n",
      "Epoch [4/1500], Training Loss: 23420.33017742317, Validation Loss: 23261.955078125\n",
      "Epoch [5/1500], Training Loss: 22486.369191083235, Validation Loss: 22362.85546875\n",
      "Epoch [6/1500], Training Loss: 21464.82079566286, Validation Loss: 21366.92578125\n",
      "Epoch [7/1500], Training Loss: 20526.33781095909, Validation Loss: 20427.46875\n",
      "Epoch [8/1500], Training Loss: 19667.860242012714, Validation Loss: 19577.041015625\n",
      "Epoch [9/1500], Training Loss: 18866.27071641596, Validation Loss: 18784.291015625\n",
      "Epoch [10/1500], Training Loss: 18115.629528030822, Validation Loss: 18046.47265625\n",
      "Epoch [11/1500], Training Loss: 17411.965850229015, Validation Loss: 17362.724609375\n",
      "Epoch [12/1500], Training Loss: 16751.3747150233, Validation Loss: 16721.66015625\n",
      "Epoch [13/1500], Training Loss: 16131.216067853167, Validation Loss: 16111.6318359375\n",
      "Epoch [14/1500], Training Loss: 15549.031543684057, Validation Loss: 15525.9072265625\n",
      "Epoch [15/1500], Training Loss: 15001.456341639878, Validation Loss: 14966.2470703125\n",
      "Epoch [16/1500], Training Loss: 14485.045649083624, Validation Loss: 14445.7646484375\n",
      "Epoch [17/1500], Training Loss: 13997.025579068057, Validation Loss: 13973.630859375\n",
      "Epoch [18/1500], Training Loss: 13533.265980117105, Validation Loss: 13544.181640625\n",
      "Epoch [19/1500], Training Loss: 13092.91269991694, Validation Loss: 13128.064453125\n",
      "Epoch [20/1500], Training Loss: 12675.122929253095, Validation Loss: 12714.6298828125\n",
      "Epoch [21/1500], Training Loss: 12278.689371818076, Validation Loss: 12314.8525390625\n",
      "Epoch [22/1500], Training Loss: 11902.563105097539, Validation Loss: 11932.9560546875\n",
      "Epoch [23/1500], Training Loss: 11545.486201367867, Validation Loss: 11572.5224609375\n",
      "Epoch [24/1500], Training Loss: 11206.157860879694, Validation Loss: 11232.84375\n",
      "Epoch [25/1500], Training Loss: 10883.419327088986, Validation Loss: 10914.3857421875\n",
      "Epoch [26/1500], Training Loss: 10576.693366499607, Validation Loss: 10618.619140625\n",
      "Epoch [27/1500], Training Loss: 10285.560897646079, Validation Loss: 10345.033203125\n",
      "Epoch [28/1500], Training Loss: 10009.443914840744, Validation Loss: 10093.3134765625\n",
      "Epoch [29/1500], Training Loss: 9747.755338675051, Validation Loss: 9887.7880859375\n",
      "Epoch [30/1500], Training Loss: 9486.44241222192, Validation Loss: 10469.64453125\n",
      "Epoch [31/1500], Training Loss: 9251.057670796537, Validation Loss: 9599.9892578125\n",
      "Epoch [32/1500], Training Loss: 8984.741787181782, Validation Loss: 9365.0908203125\n",
      "Epoch [33/1500], Training Loss: 8732.9311014355, Validation Loss: 9116.28515625\n",
      "Epoch [34/1500], Training Loss: 8495.17769277109, Validation Loss: 8842.8583984375\n",
      "Epoch [35/1500], Training Loss: 8267.07853118142, Validation Loss: 8570.0849609375\n",
      "Epoch [36/1500], Training Loss: 8048.101031068123, Validation Loss: 8306.2080078125\n",
      "Epoch [37/1500], Training Loss: 7838.489352860459, Validation Loss: 8055.8251953125\n",
      "Epoch [38/1500], Training Loss: 7637.237504602227, Validation Loss: 7819.62109375\n",
      "Epoch [39/1500], Training Loss: 7443.215827459617, Validation Loss: 7598.23583984375\n",
      "Epoch [40/1500], Training Loss: 7255.5991971576395, Validation Loss: 7389.28515625\n",
      "Epoch [41/1500], Training Loss: 7073.568743022665, Validation Loss: 7188.45703125\n",
      "Epoch [42/1500], Training Loss: 6896.546646769109, Validation Loss: 6996.041015625\n",
      "Epoch [43/1500], Training Loss: 6724.12562843157, Validation Loss: 6812.58251953125\n",
      "Epoch [44/1500], Training Loss: 6556.01137269647, Validation Loss: 6637.09619140625\n",
      "Epoch [45/1500], Training Loss: 6392.124138145642, Validation Loss: 6469.3916015625\n",
      "Epoch [46/1500], Training Loss: 6232.425735342953, Validation Loss: 6309.00244140625\n",
      "Epoch [47/1500], Training Loss: 6076.913961642937, Validation Loss: 6154.5234375\n",
      "Epoch [48/1500], Training Loss: 5925.490480872984, Validation Loss: 6005.615234375\n",
      "Epoch [49/1500], Training Loss: 5777.564527478994, Validation Loss: 5865.6357421875\n",
      "Epoch [50/1500], Training Loss: 5632.19190639779, Validation Loss: 5721.8291015625\n",
      "Epoch [51/1500], Training Loss: 5490.376365128301, Validation Loss: 5574.74462890625\n",
      "Epoch [52/1500], Training Loss: 5353.186726536293, Validation Loss: 5435.8828125\n",
      "Epoch [53/1500], Training Loss: 5220.490155133608, Validation Loss: 5301.9423828125\n",
      "Epoch [54/1500], Training Loss: 5091.99627451372, Validation Loss: 5173.11376953125\n",
      "Epoch [55/1500], Training Loss: 4967.404815634022, Validation Loss: 5049.349609375\n",
      "Epoch [56/1500], Training Loss: 4846.25194641755, Validation Loss: 4930.06591796875\n",
      "Epoch [57/1500], Training Loss: 4728.009904429038, Validation Loss: 4814.49755859375\n",
      "Epoch [58/1500], Training Loss: 4612.272351306059, Validation Loss: 4701.93896484375\n",
      "Epoch [59/1500], Training Loss: 4498.833748984479, Validation Loss: 4591.91845703125\n",
      "Epoch [60/1500], Training Loss: 4387.5939881274535, Validation Loss: 4484.1748046875\n",
      "Epoch [61/1500], Training Loss: 4278.512363630509, Validation Loss: 4378.5458984375\n",
      "Epoch [62/1500], Training Loss: 4171.570273027642, Validation Loss: 4274.99365234375\n",
      "Epoch [63/1500], Training Loss: 4066.792116809346, Validation Loss: 4173.5\n",
      "Epoch [64/1500], Training Loss: 3964.2370596452556, Validation Loss: 4074.0810546875\n",
      "Epoch [65/1500], Training Loss: 3863.933531562184, Validation Loss: 3976.78173828125\n",
      "Epoch [66/1500], Training Loss: 3765.923325540702, Validation Loss: 3881.651123046875\n",
      "Epoch [67/1500], Training Loss: 3670.2296610126614, Validation Loss: 3788.784423828125\n",
      "Epoch [68/1500], Training Loss: 3576.8953021279276, Validation Loss: 3698.259033203125\n",
      "Epoch [69/1500], Training Loss: 3485.9375890088054, Validation Loss: 3610.19482421875\n",
      "Epoch [70/1500], Training Loss: 3397.3758983466196, Validation Loss: 3524.60791015625\n",
      "Epoch [71/1500], Training Loss: 3311.2181248347524, Validation Loss: 3441.54345703125\n",
      "Epoch [72/1500], Training Loss: 3227.465281001335, Validation Loss: 3360.9794921875\n",
      "Epoch [73/1500], Training Loss: 3146.1256226893997, Validation Loss: 3282.89013671875\n",
      "Epoch [74/1500], Training Loss: 3067.1930991058184, Validation Loss: 3207.253173828125\n",
      "Epoch [75/1500], Training Loss: 2990.634035076614, Validation Loss: 3133.991455078125\n",
      "Epoch [76/1500], Training Loss: 2916.3684706653994, Validation Loss: 3062.972900390625\n",
      "Epoch [77/1500], Training Loss: 2844.283626355885, Validation Loss: 2994.025390625\n",
      "Epoch [78/1500], Training Loss: 2774.245628774965, Validation Loss: 2927.00244140625\n",
      "Epoch [79/1500], Training Loss: 2706.128581932547, Validation Loss: 2861.807373046875\n",
      "Epoch [80/1500], Training Loss: 2639.864423849933, Validation Loss: 2798.490966796875\n",
      "Epoch [81/1500], Training Loss: 2575.4987811781166, Validation Loss: 2737.10009765625\n",
      "Epoch [82/1500], Training Loss: 2513.155841009737, Validation Loss: 2677.7109375\n",
      "Epoch [83/1500], Training Loss: 2452.9421010512565, Validation Loss: 2620.303955078125\n",
      "Epoch [84/1500], Training Loss: 2394.8836575305268, Validation Loss: 2564.865966796875\n",
      "Epoch [85/1500], Training Loss: 2338.8948388987774, Validation Loss: 2511.273193359375\n",
      "Epoch [86/1500], Training Loss: 2284.773609474138, Validation Loss: 2459.33642578125\n",
      "Epoch [87/1500], Training Loss: 2232.283267026451, Validation Loss: 2408.883056640625\n",
      "Epoch [88/1500], Training Loss: 2181.227392348791, Validation Loss: 2359.80078125\n",
      "Epoch [89/1500], Training Loss: 2131.480089451487, Validation Loss: 2311.9833984375\n",
      "Epoch [90/1500], Training Loss: 2082.9653933238433, Validation Loss: 2265.40087890625\n",
      "Epoch [91/1500], Training Loss: 2035.6140162976776, Validation Loss: 2219.947021484375\n",
      "Epoch [92/1500], Training Loss: 1989.3773709835161, Validation Loss: 2175.55517578125\n",
      "Epoch [93/1500], Training Loss: 1944.2116845894307, Validation Loss: 2132.185302734375\n",
      "Epoch [94/1500], Training Loss: 1900.125453823256, Validation Loss: 2089.862060546875\n",
      "Epoch [95/1500], Training Loss: 1857.1152599873906, Validation Loss: 2048.5888671875\n",
      "Epoch [96/1500], Training Loss: 1815.1951225377418, Validation Loss: 2008.4290771484375\n",
      "Epoch [97/1500], Training Loss: 1774.4040738505716, Validation Loss: 1969.3974609375\n",
      "Epoch [98/1500], Training Loss: 1734.7570321818466, Validation Loss: 1931.533935546875\n",
      "Epoch [99/1500], Training Loss: 1696.2901770013896, Validation Loss: 1894.898681640625\n",
      "Epoch [100/1500], Training Loss: 1659.0179272666028, Validation Loss: 1859.48974609375\n",
      "Epoch [101/1500], Training Loss: 1622.9329924916124, Validation Loss: 1825.2777099609375\n",
      "Epoch [102/1500], Training Loss: 1588.0323212817789, Validation Loss: 1792.2369384765625\n",
      "Epoch [103/1500], Training Loss: 1554.2744195975897, Validation Loss: 1760.29443359375\n",
      "Epoch [104/1500], Training Loss: 1521.6180392736715, Validation Loss: 1729.3963623046875\n",
      "Epoch [105/1500], Training Loss: 1490.0081022428715, Validation Loss: 1699.50146484375\n",
      "Epoch [106/1500], Training Loss: 1459.4052215040647, Validation Loss: 1670.5660400390625\n",
      "Epoch [107/1500], Training Loss: 1429.746988284022, Validation Loss: 1642.5452880859375\n",
      "Epoch [108/1500], Training Loss: 1400.976767778783, Validation Loss: 1615.375244140625\n",
      "Epoch [109/1500], Training Loss: 1373.0373399356852, Validation Loss: 1589.007080078125\n",
      "Epoch [110/1500], Training Loss: 1345.8661916458268, Validation Loss: 1563.3624267578125\n",
      "Epoch [111/1500], Training Loss: 1319.4145678262882, Validation Loss: 1538.3560791015625\n",
      "Epoch [112/1500], Training Loss: 1293.62690247341, Validation Loss: 1513.9071044921875\n",
      "Epoch [113/1500], Training Loss: 1268.4700228778313, Validation Loss: 1489.9627685546875\n",
      "Epoch [114/1500], Training Loss: 1243.9000701923685, Validation Loss: 1466.52734375\n",
      "Epoch [115/1500], Training Loss: 1219.8825849239056, Validation Loss: 1443.6510009765625\n",
      "Epoch [116/1500], Training Loss: 1196.3929623082713, Validation Loss: 1421.3511962890625\n",
      "Epoch [117/1500], Training Loss: 1173.401745869492, Validation Loss: 1399.608154296875\n",
      "Epoch [118/1500], Training Loss: 1150.8879157491817, Validation Loss: 1378.3961181640625\n",
      "Epoch [119/1500], Training Loss: 1128.8435876973083, Validation Loss: 1357.6900634765625\n",
      "Epoch [120/1500], Training Loss: 1107.2410422080018, Validation Loss: 1337.4508056640625\n",
      "Epoch [121/1500], Training Loss: 1086.0737778010318, Validation Loss: 1317.67138671875\n",
      "Epoch [122/1500], Training Loss: 1065.3244199776912, Validation Loss: 1298.324462890625\n",
      "Epoch [123/1500], Training Loss: 1044.9819235512002, Validation Loss: 1279.4012451171875\n",
      "Epoch [124/1500], Training Loss: 1025.0390147005678, Validation Loss: 1260.8824462890625\n",
      "Epoch [125/1500], Training Loss: 1005.4799925939402, Validation Loss: 1242.7486572265625\n",
      "Epoch [126/1500], Training Loss: 986.301685504655, Validation Loss: 1224.9970703125\n",
      "Epoch [127/1500], Training Loss: 967.4951673978911, Validation Loss: 1207.6129150390625\n",
      "Epoch [128/1500], Training Loss: 949.0619832340032, Validation Loss: 1190.588134765625\n",
      "Epoch [129/1500], Training Loss: 930.990923373983, Validation Loss: 1173.90673828125\n",
      "Epoch [130/1500], Training Loss: 913.2693001407007, Validation Loss: 1157.5550537109375\n",
      "Epoch [131/1500], Training Loss: 895.890513250823, Validation Loss: 1141.52734375\n",
      "Epoch [132/1500], Training Loss: 878.850250504702, Validation Loss: 1125.79833984375\n",
      "Epoch [133/1500], Training Loss: 862.1303542448294, Validation Loss: 1110.356689453125\n",
      "Epoch [134/1500], Training Loss: 845.7341980712287, Validation Loss: 1095.1944580078125\n",
      "Epoch [135/1500], Training Loss: 829.6514626748035, Validation Loss: 1080.2890625\n",
      "Epoch [136/1500], Training Loss: 813.8737112329536, Validation Loss: 1065.6258544921875\n",
      "Epoch [137/1500], Training Loss: 798.3900131652463, Validation Loss: 1051.1903076171875\n",
      "Epoch [138/1500], Training Loss: 783.1955854427543, Validation Loss: 1036.962158203125\n",
      "Epoch [139/1500], Training Loss: 768.2856153974807, Validation Loss: 1022.9461059570312\n",
      "Epoch [140/1500], Training Loss: 753.6607846760152, Validation Loss: 1009.1294555664062\n",
      "Epoch [141/1500], Training Loss: 739.310128936358, Validation Loss: 995.5042724609375\n",
      "Epoch [142/1500], Training Loss: 725.2248051969541, Validation Loss: 982.0711059570312\n",
      "Epoch [143/1500], Training Loss: 711.3968581147047, Validation Loss: 968.8246459960938\n",
      "Epoch [144/1500], Training Loss: 697.8227713022544, Validation Loss: 955.76171875\n",
      "Epoch [145/1500], Training Loss: 684.4975777867321, Validation Loss: 942.8746948242188\n",
      "Epoch [146/1500], Training Loss: 671.4121302195433, Validation Loss: 930.1535034179688\n",
      "Epoch [147/1500], Training Loss: 658.5663132341992, Validation Loss: 917.5939331054688\n",
      "Epoch [148/1500], Training Loss: 645.9545606460114, Validation Loss: 905.1758422851562\n",
      "Epoch [149/1500], Training Loss: 633.5690401578895, Validation Loss: 892.8845825195312\n",
      "Epoch [150/1500], Training Loss: 621.4039255866107, Validation Loss: 880.7109985351562\n",
      "Epoch [151/1500], Training Loss: 609.4506929087836, Validation Loss: 868.6409301757812\n",
      "Epoch [152/1500], Training Loss: 597.7089536451997, Validation Loss: 856.6689453125\n",
      "Epoch [153/1500], Training Loss: 586.1768540784327, Validation Loss: 844.7933959960938\n",
      "Epoch [154/1500], Training Loss: 574.8467850802703, Validation Loss: 833.0074462890625\n",
      "Epoch [155/1500], Training Loss: 563.7199474111632, Validation Loss: 821.3161010742188\n",
      "Epoch [156/1500], Training Loss: 552.7885766362855, Validation Loss: 809.7230224609375\n",
      "Epoch [157/1500], Training Loss: 542.0514511417755, Validation Loss: 798.237548828125\n",
      "Epoch [158/1500], Training Loss: 531.5106791810264, Validation Loss: 786.8780517578125\n",
      "Epoch [159/1500], Training Loss: 521.1645828975779, Validation Loss: 775.6493530273438\n",
      "Epoch [160/1500], Training Loss: 511.0062556369813, Validation Loss: 764.5665283203125\n",
      "Epoch [161/1500], Training Loss: 501.0394704803384, Validation Loss: 753.6359252929688\n",
      "Epoch [162/1500], Training Loss: 491.2577151039585, Validation Loss: 742.86572265625\n",
      "Epoch [163/1500], Training Loss: 481.65808182327316, Validation Loss: 732.2610473632812\n",
      "Epoch [164/1500], Training Loss: 472.23886396063153, Validation Loss: 721.8323364257812\n",
      "Epoch [165/1500], Training Loss: 463.00032695293135, Validation Loss: 711.582763671875\n",
      "Epoch [166/1500], Training Loss: 453.9396368441344, Validation Loss: 701.5134887695312\n",
      "Epoch [167/1500], Training Loss: 445.0533266520636, Validation Loss: 691.61865234375\n",
      "Epoch [168/1500], Training Loss: 436.3379307497271, Validation Loss: 681.8858032226562\n",
      "Epoch [169/1500], Training Loss: 427.79541061656977, Validation Loss: 672.2989501953125\n",
      "Epoch [170/1500], Training Loss: 419.4181321696548, Validation Loss: 662.8358154296875\n",
      "Epoch [171/1500], Training Loss: 411.203596878485, Validation Loss: 653.47265625\n",
      "Epoch [172/1500], Training Loss: 403.1483627713464, Validation Loss: 644.2001342773438\n",
      "Epoch [173/1500], Training Loss: 395.2464273929174, Validation Loss: 635.0039672851562\n",
      "Epoch [174/1500], Training Loss: 387.49644113817203, Validation Loss: 625.8865966796875\n",
      "Epoch [175/1500], Training Loss: 379.8955398848948, Validation Loss: 616.8427734375\n",
      "Epoch [176/1500], Training Loss: 372.4361604439811, Validation Loss: 607.864013671875\n",
      "Epoch [177/1500], Training Loss: 365.11590600490746, Validation Loss: 598.942138671875\n",
      "Epoch [178/1500], Training Loss: 357.93299812261745, Validation Loss: 590.0604248046875\n",
      "Epoch [179/1500], Training Loss: 350.8828897633785, Validation Loss: 581.201416015625\n",
      "Epoch [180/1500], Training Loss: 343.9613162129965, Validation Loss: 572.337890625\n",
      "Epoch [181/1500], Training Loss: 337.16401698294374, Validation Loss: 563.4472045898438\n",
      "Epoch [182/1500], Training Loss: 330.4851425093804, Validation Loss: 554.4978637695312\n",
      "Epoch [183/1500], Training Loss: 323.9205739436086, Validation Loss: 545.4696044921875\n",
      "Epoch [184/1500], Training Loss: 317.46813655295506, Validation Loss: 536.35107421875\n",
      "Epoch [185/1500], Training Loss: 311.123099548996, Validation Loss: 527.149658203125\n",
      "Epoch [186/1500], Training Loss: 304.882525423271, Validation Loss: 517.8938598632812\n",
      "Epoch [187/1500], Training Loss: 298.74568488669206, Validation Loss: 508.626953125\n",
      "Epoch [188/1500], Training Loss: 292.7102618210315, Validation Loss: 499.3843688964844\n",
      "Epoch [189/1500], Training Loss: 286.77497633332484, Validation Loss: 490.19989013671875\n",
      "Epoch [190/1500], Training Loss: 280.9381086057341, Validation Loss: 481.0899658203125\n",
      "Epoch [191/1500], Training Loss: 275.1993069692244, Validation Loss: 472.0696716308594\n",
      "Epoch [192/1500], Training Loss: 269.55557013119494, Validation Loss: 463.1498107910156\n",
      "Epoch [193/1500], Training Loss: 264.00520761170213, Validation Loss: 454.3402404785156\n",
      "Epoch [194/1500], Training Loss: 258.54710438570623, Validation Loss: 445.652099609375\n",
      "Epoch [195/1500], Training Loss: 253.17775254945067, Validation Loss: 437.0920104980469\n",
      "Epoch [196/1500], Training Loss: 247.89456215183776, Validation Loss: 428.67022705078125\n",
      "Epoch [197/1500], Training Loss: 242.7008270758751, Validation Loss: 420.40087890625\n",
      "Epoch [198/1500], Training Loss: 237.5984383818385, Validation Loss: 412.2899169921875\n",
      "Epoch [199/1500], Training Loss: 232.5864625956889, Validation Loss: 404.34417724609375\n",
      "Epoch [200/1500], Training Loss: 227.6648280421631, Validation Loss: 396.5671081542969\n",
      "Epoch [201/1500], Training Loss: 222.83350109342194, Validation Loss: 388.9629211425781\n",
      "Epoch [202/1500], Training Loss: 218.09358473979805, Validation Loss: 381.5326843261719\n",
      "Epoch [203/1500], Training Loss: 213.4420993035586, Validation Loss: 374.27679443359375\n",
      "Epoch [204/1500], Training Loss: 208.87809394081995, Validation Loss: 367.1921081542969\n",
      "Epoch [205/1500], Training Loss: 204.40155309066597, Validation Loss: 360.2805480957031\n",
      "Epoch [206/1500], Training Loss: 200.01012029598814, Validation Loss: 353.537841796875\n",
      "Epoch [207/1500], Training Loss: 195.70470336526935, Validation Loss: 346.9613037109375\n",
      "Epoch [208/1500], Training Loss: 191.48347091700532, Validation Loss: 340.5480651855469\n",
      "Epoch [209/1500], Training Loss: 187.34515799167798, Validation Loss: 334.29266357421875\n",
      "Epoch [210/1500], Training Loss: 183.28978008781297, Validation Loss: 328.1928405761719\n",
      "Epoch [211/1500], Training Loss: 179.3163504743843, Validation Loss: 322.2408142089844\n",
      "Epoch [212/1500], Training Loss: 175.4229842885492, Validation Loss: 316.4361572265625\n",
      "Epoch [213/1500], Training Loss: 171.6113632688663, Validation Loss: 310.77728271484375\n",
      "Epoch [214/1500], Training Loss: 167.8801661280926, Validation Loss: 305.25848388671875\n",
      "Epoch [215/1500], Training Loss: 164.22729231002194, Validation Loss: 299.876708984375\n",
      "Epoch [216/1500], Training Loss: 160.65210386046158, Validation Loss: 294.62579345703125\n",
      "Epoch [217/1500], Training Loss: 157.15262035388594, Validation Loss: 289.5010986328125\n",
      "Epoch [218/1500], Training Loss: 153.72821294777643, Validation Loss: 284.49847412109375\n",
      "Epoch [219/1500], Training Loss: 150.37601919914744, Validation Loss: 279.6055603027344\n",
      "Epoch [220/1500], Training Loss: 147.09352084086746, Validation Loss: 274.8162841796875\n",
      "Epoch [221/1500], Training Loss: 143.8798188819585, Validation Loss: 270.1244201660156\n",
      "Epoch [222/1500], Training Loss: 140.73608535575937, Validation Loss: 265.52203369140625\n",
      "Epoch [223/1500], Training Loss: 137.65883238568384, Validation Loss: 260.9963684082031\n",
      "Epoch [224/1500], Training Loss: 134.64883551834023, Validation Loss: 256.54180908203125\n",
      "Epoch [225/1500], Training Loss: 131.7039357371351, Validation Loss: 252.14356994628906\n",
      "Epoch [226/1500], Training Loss: 128.82401814811013, Validation Loss: 247.7955780029297\n",
      "Epoch [227/1500], Training Loss: 126.0104669144639, Validation Loss: 243.4895477294922\n",
      "Epoch [228/1500], Training Loss: 123.25979738129595, Validation Loss: 239.2171630859375\n",
      "Epoch [229/1500], Training Loss: 120.57176168030601, Validation Loss: 234.978515625\n",
      "Epoch [230/1500], Training Loss: 117.9468975683992, Validation Loss: 230.77273559570312\n",
      "Epoch [231/1500], Training Loss: 115.38367734633535, Validation Loss: 226.60581970214844\n",
      "Epoch [232/1500], Training Loss: 112.88176606970192, Validation Loss: 222.48214721679688\n",
      "Epoch [233/1500], Training Loss: 110.43961447277248, Validation Loss: 218.4072265625\n",
      "Epoch [234/1500], Training Loss: 108.05565238849233, Validation Loss: 214.38807678222656\n",
      "Epoch [235/1500], Training Loss: 105.72997927458341, Validation Loss: 210.43252563476562\n",
      "Epoch [236/1500], Training Loss: 103.46146459969707, Validation Loss: 206.5455322265625\n",
      "Epoch [237/1500], Training Loss: 101.24636865232826, Validation Loss: 202.73399353027344\n",
      "Epoch [238/1500], Training Loss: 99.0874079799239, Validation Loss: 199.00302124023438\n",
      "Epoch [239/1500], Training Loss: 96.98077400764824, Validation Loss: 195.35336303710938\n",
      "Epoch [240/1500], Training Loss: 94.92490198311448, Validation Loss: 191.7871551513672\n",
      "Epoch [241/1500], Training Loss: 92.91973681185563, Validation Loss: 188.3077392578125\n",
      "Epoch [242/1500], Training Loss: 90.96259845434074, Validation Loss: 184.91490173339844\n",
      "Epoch [243/1500], Training Loss: 89.05391165664534, Validation Loss: 181.6065216064453\n",
      "Epoch [244/1500], Training Loss: 87.19089987254718, Validation Loss: 178.3826446533203\n",
      "Epoch [245/1500], Training Loss: 85.37324410022111, Validation Loss: 175.23849487304688\n",
      "Epoch [246/1500], Training Loss: 83.59938973698405, Validation Loss: 172.16943359375\n",
      "Epoch [247/1500], Training Loss: 81.8687064157764, Validation Loss: 169.17111206054688\n",
      "Epoch [248/1500], Training Loss: 80.17839510592691, Validation Loss: 166.2360382080078\n",
      "Epoch [249/1500], Training Loss: 78.52810133211482, Validation Loss: 163.3600311279297\n",
      "Epoch [250/1500], Training Loss: 76.91679959169474, Validation Loss: 160.5382537841797\n",
      "Epoch [251/1500], Training Loss: 75.34315299511583, Validation Loss: 157.7654266357422\n",
      "Epoch [252/1500], Training Loss: 73.80651306547155, Validation Loss: 155.03822326660156\n",
      "Epoch [253/1500], Training Loss: 72.3063812467653, Validation Loss: 152.35128784179688\n",
      "Epoch [254/1500], Training Loss: 70.84103487544537, Validation Loss: 149.70208740234375\n",
      "Epoch [255/1500], Training Loss: 69.41056693799747, Validation Loss: 147.0867919921875\n",
      "Epoch [256/1500], Training Loss: 68.01355918264316, Validation Loss: 144.4991455078125\n",
      "Epoch [257/1500], Training Loss: 66.64890183362375, Validation Loss: 141.93704223632812\n",
      "Epoch [258/1500], Training Loss: 65.31498571738659, Validation Loss: 139.39596557617188\n",
      "Epoch [259/1500], Training Loss: 64.01218154958052, Validation Loss: 136.87240600585938\n",
      "Epoch [260/1500], Training Loss: 62.73866456673641, Validation Loss: 134.36279296875\n",
      "Epoch [261/1500], Training Loss: 61.49425459731462, Validation Loss: 131.86341857910156\n",
      "Epoch [262/1500], Training Loss: 60.27788490536357, Validation Loss: 129.37246704101562\n",
      "Epoch [263/1500], Training Loss: 59.08900023263344, Validation Loss: 126.8935317993164\n",
      "Epoch [264/1500], Training Loss: 57.92652181889328, Validation Loss: 124.43348693847656\n",
      "Epoch [265/1500], Training Loss: 56.79198250228152, Validation Loss: 122.00516510009766\n",
      "Epoch [266/1500], Training Loss: 55.684854098884145, Validation Loss: 119.62532806396484\n",
      "Epoch [267/1500], Training Loss: 54.60408256912073, Validation Loss: 117.30783081054688\n",
      "Epoch [268/1500], Training Loss: 53.54975854672901, Validation Loss: 115.06266021728516\n",
      "Epoch [269/1500], Training Loss: 52.5224779973486, Validation Loss: 112.8884506225586\n",
      "Epoch [270/1500], Training Loss: 51.521072429851635, Validation Loss: 110.78548431396484\n",
      "Epoch [271/1500], Training Loss: 50.545586513499764, Validation Loss: 108.75115966796875\n",
      "Epoch [272/1500], Training Loss: 49.596138582753504, Validation Loss: 106.78399658203125\n",
      "Epoch [273/1500], Training Loss: 48.67162432395698, Validation Loss: 104.88317108154297\n",
      "Epoch [274/1500], Training Loss: 47.77133173298609, Validation Loss: 103.04730224609375\n",
      "Epoch [275/1500], Training Loss: 46.89478180507411, Validation Loss: 101.2761459350586\n",
      "Epoch [276/1500], Training Loss: 46.04123989169366, Validation Loss: 99.5663070678711\n",
      "Epoch [277/1500], Training Loss: 45.20985436691393, Validation Loss: 97.9146957397461\n",
      "Epoch [278/1500], Training Loss: 44.399895349936614, Validation Loss: 96.31934356689453\n",
      "Epoch [279/1500], Training Loss: 43.61043516182715, Validation Loss: 94.77484893798828\n",
      "Epoch [280/1500], Training Loss: 42.84145800716078, Validation Loss: 93.27714538574219\n",
      "Epoch [281/1500], Training Loss: 42.091446137896156, Validation Loss: 91.82235717773438\n",
      "Epoch [282/1500], Training Loss: 41.35977591773917, Validation Loss: 90.40470123291016\n",
      "Epoch [283/1500], Training Loss: 40.64623543279025, Validation Loss: 89.02275848388672\n",
      "Epoch [284/1500], Training Loss: 39.95029861145231, Validation Loss: 87.67063903808594\n",
      "Epoch [285/1500], Training Loss: 39.27073100374899, Validation Loss: 86.3439712524414\n",
      "Epoch [286/1500], Training Loss: 38.60758180172625, Validation Loss: 85.04114532470703\n",
      "Epoch [287/1500], Training Loss: 37.95979787239228, Validation Loss: 83.75689697265625\n",
      "Epoch [288/1500], Training Loss: 37.32729677896416, Validation Loss: 82.48837280273438\n",
      "Epoch [289/1500], Training Loss: 36.70867099879526, Validation Loss: 81.23197174072266\n",
      "Epoch [290/1500], Training Loss: 36.10348465350948, Validation Loss: 79.98468017578125\n",
      "Epoch [291/1500], Training Loss: 35.51120682351232, Validation Loss: 78.74256134033203\n",
      "Epoch [292/1500], Training Loss: 34.93170340566063, Validation Loss: 77.50238800048828\n",
      "Epoch [293/1500], Training Loss: 34.363789266038175, Validation Loss: 76.26116180419922\n",
      "Epoch [294/1500], Training Loss: 33.807096448282834, Validation Loss: 75.01667022705078\n",
      "Epoch [295/1500], Training Loss: 33.26136630263839, Validation Loss: 73.76455688476562\n",
      "Epoch [296/1500], Training Loss: 32.725748062680246, Validation Loss: 72.50299835205078\n",
      "Epoch [297/1500], Training Loss: 32.199191694343725, Validation Loss: 71.22929382324219\n",
      "Epoch [298/1500], Training Loss: 31.68109398224412, Validation Loss: 69.9398193359375\n",
      "Epoch [299/1500], Training Loss: 31.17074583218941, Validation Loss: 68.63410949707031\n",
      "Epoch [300/1500], Training Loss: 30.667749671336914, Validation Loss: 67.31136322021484\n",
      "Epoch [301/1500], Training Loss: 30.171377678858686, Validation Loss: 65.97057342529297\n",
      "Epoch [302/1500], Training Loss: 29.681200906126524, Validation Loss: 64.6128921508789\n",
      "Epoch [303/1500], Training Loss: 29.196702453958043, Validation Loss: 63.239463806152344\n",
      "Epoch [304/1500], Training Loss: 28.718061556407616, Validation Loss: 61.85295104980469\n",
      "Epoch [305/1500], Training Loss: 28.244987117127234, Validation Loss: 60.457759857177734\n",
      "Epoch [306/1500], Training Loss: 27.77721031688018, Validation Loss: 59.05767822265625\n",
      "Epoch [307/1500], Training Loss: 27.3146407518493, Validation Loss: 57.658164978027344\n",
      "Epoch [308/1500], Training Loss: 26.857222403275617, Validation Loss: 56.265235900878906\n",
      "Epoch [309/1500], Training Loss: 26.405285053544933, Validation Loss: 54.88461685180664\n",
      "Epoch [310/1500], Training Loss: 25.9595784421168, Validation Loss: 53.52549743652344\n",
      "Epoch [311/1500], Training Loss: 25.520097285909284, Validation Loss: 52.19483947753906\n",
      "Epoch [312/1500], Training Loss: 25.088040033478308, Validation Loss: 50.90156173706055\n",
      "Epoch [313/1500], Training Loss: 24.663726081131937, Validation Loss: 49.65211868286133\n",
      "Epoch [314/1500], Training Loss: 24.247462333338685, Validation Loss: 48.452850341796875\n",
      "Epoch [315/1500], Training Loss: 23.84037206072273, Validation Loss: 47.310394287109375\n",
      "Epoch [316/1500], Training Loss: 23.442944948289448, Validation Loss: 46.22809982299805\n",
      "Epoch [317/1500], Training Loss: 23.055919928194314, Validation Loss: 45.20977020263672\n",
      "Epoch [318/1500], Training Loss: 22.679308410933785, Validation Loss: 44.25437927246094\n",
      "Epoch [319/1500], Training Loss: 22.31329101350044, Validation Loss: 43.361873626708984\n",
      "Epoch [320/1500], Training Loss: 21.95759162246311, Validation Loss: 42.53003692626953\n",
      "Epoch [321/1500], Training Loss: 21.612206203379543, Validation Loss: 41.75516128540039\n",
      "Epoch [322/1500], Training Loss: 21.277290876922603, Validation Loss: 41.03333282470703\n",
      "Epoch [323/1500], Training Loss: 20.952005053281006, Validation Loss: 40.35929870605469\n",
      "Epoch [324/1500], Training Loss: 20.63602616488163, Validation Loss: 39.72637176513672\n",
      "Epoch [325/1500], Training Loss: 20.328506200466435, Validation Loss: 39.12989044189453\n",
      "Epoch [326/1500], Training Loss: 20.028686276772454, Validation Loss: 38.56345748901367\n",
      "Epoch [327/1500], Training Loss: 19.73602888264035, Validation Loss: 38.022769927978516\n",
      "Epoch [328/1500], Training Loss: 19.450075098780253, Validation Loss: 37.50344467163086\n",
      "Epoch [329/1500], Training Loss: 19.170362817905, Validation Loss: 37.00331115722656\n",
      "Epoch [330/1500], Training Loss: 18.89654692830816, Validation Loss: 36.519290924072266\n",
      "Epoch [331/1500], Training Loss: 18.628564526247466, Validation Loss: 36.04941940307617\n",
      "Epoch [332/1500], Training Loss: 18.366153300723415, Validation Loss: 35.59223175048828\n",
      "Epoch [333/1500], Training Loss: 18.10920207034386, Validation Loss: 35.14670181274414\n",
      "Epoch [334/1500], Training Loss: 17.8574646370816, Validation Loss: 34.71110153198242\n",
      "Epoch [335/1500], Training Loss: 17.61082726246065, Validation Loss: 34.285667419433594\n",
      "Epoch [336/1500], Training Loss: 17.369188316510535, Validation Loss: 33.86960220336914\n",
      "Epoch [337/1500], Training Loss: 17.132267679856927, Validation Loss: 33.46330642700195\n",
      "Epoch [338/1500], Training Loss: 16.900106781003362, Validation Loss: 33.06538009643555\n",
      "Epoch [339/1500], Training Loss: 16.67278129023911, Validation Loss: 32.677181243896484\n",
      "Epoch [340/1500], Training Loss: 16.450216605181307, Validation Loss: 32.2977180480957\n",
      "Epoch [341/1500], Training Loss: 16.23260234544549, Validation Loss: 31.92729377746582\n",
      "Epoch [342/1500], Training Loss: 16.019863519276687, Validation Loss: 31.56580352783203\n",
      "Epoch [343/1500], Training Loss: 15.811848080799283, Validation Loss: 31.212770462036133\n",
      "Epoch [344/1500], Training Loss: 15.608899307337827, Validation Loss: 30.869125366210938\n",
      "Epoch [345/1500], Training Loss: 15.410517571576037, Validation Loss: 30.533084869384766\n",
      "Epoch [346/1500], Training Loss: 15.216822731500837, Validation Loss: 30.205074310302734\n",
      "Epoch [347/1500], Training Loss: 15.027703918487765, Validation Loss: 29.884952545166016\n",
      "Epoch [348/1500], Training Loss: 14.84302818197281, Validation Loss: 29.572813034057617\n",
      "Epoch [349/1500], Training Loss: 14.66276766120551, Validation Loss: 29.268245697021484\n",
      "Epoch [350/1500], Training Loss: 14.486791796176211, Validation Loss: 28.97092628479004\n",
      "Epoch [351/1500], Training Loss: 14.31511805535204, Validation Loss: 28.681392669677734\n",
      "Epoch [352/1500], Training Loss: 14.147601962503375, Validation Loss: 28.401229858398438\n",
      "Epoch [353/1500], Training Loss: 13.984302718902791, Validation Loss: 28.130090713500977\n",
      "Epoch [354/1500], Training Loss: 13.825546595857196, Validation Loss: 27.869291305541992\n",
      "Epoch [355/1500], Training Loss: 13.671164769471725, Validation Loss: 27.618743896484375\n",
      "Epoch [356/1500], Training Loss: 13.520968391611717, Validation Loss: 27.38172149658203\n",
      "Epoch [357/1500], Training Loss: 13.37522234728339, Validation Loss: 27.158660888671875\n",
      "Epoch [358/1500], Training Loss: 13.233609244758657, Validation Loss: 26.95224952697754\n",
      "Epoch [359/1500], Training Loss: 13.096111501305003, Validation Loss: 26.763221740722656\n",
      "Epoch [360/1500], Training Loss: 12.962808180346855, Validation Loss: 26.593189239501953\n",
      "Epoch [361/1500], Training Loss: 12.833361261878494, Validation Loss: 26.440418243408203\n",
      "Epoch [362/1500], Training Loss: 12.707673519576684, Validation Loss: 26.302688598632812\n",
      "Epoch [363/1500], Training Loss: 12.585364005861907, Validation Loss: 26.173738479614258\n",
      "Epoch [364/1500], Training Loss: 12.466188558453634, Validation Loss: 26.049707412719727\n",
      "Epoch [365/1500], Training Loss: 12.349897935499387, Validation Loss: 25.927021026611328\n",
      "Epoch [366/1500], Training Loss: 12.236203623939382, Validation Loss: 25.803834915161133\n",
      "Epoch [367/1500], Training Loss: 12.124649883977343, Validation Loss: 25.677553176879883\n",
      "Epoch [368/1500], Training Loss: 12.01509321676663, Validation Loss: 25.548643112182617\n",
      "Epoch [369/1500], Training Loss: 11.907500740895179, Validation Loss: 25.415546417236328\n",
      "Epoch [370/1500], Training Loss: 11.801802436684211, Validation Loss: 25.279399871826172\n",
      "Epoch [371/1500], Training Loss: 11.69773145863964, Validation Loss: 25.139875411987305\n",
      "Epoch [372/1500], Training Loss: 11.595107441181035, Validation Loss: 24.99860191345215\n",
      "Epoch [373/1500], Training Loss: 11.493952176500619, Validation Loss: 24.855697631835938\n",
      "Epoch [374/1500], Training Loss: 11.394295852712737, Validation Loss: 24.712526321411133\n",
      "Epoch [375/1500], Training Loss: 11.29620126072474, Validation Loss: 24.569934844970703\n",
      "Epoch [376/1500], Training Loss: 11.199417439146588, Validation Loss: 24.428014755249023\n",
      "Epoch [377/1500], Training Loss: 11.10403849223044, Validation Loss: 24.288188934326172\n",
      "Epoch [378/1500], Training Loss: 11.009992994498566, Validation Loss: 24.150033950805664\n",
      "Epoch [379/1500], Training Loss: 10.917435207847609, Validation Loss: 24.014253616333008\n",
      "Epoch [380/1500], Training Loss: 10.826418020122997, Validation Loss: 23.880794525146484\n",
      "Epoch [381/1500], Training Loss: 10.736647344239097, Validation Loss: 23.74924659729004\n",
      "Epoch [382/1500], Training Loss: 10.648100204363265, Validation Loss: 23.61996841430664\n",
      "Epoch [383/1500], Training Loss: 10.560669594233424, Validation Loss: 23.492345809936523\n",
      "Epoch [384/1500], Training Loss: 10.47448014985244, Validation Loss: 23.366798400878906\n",
      "Epoch [385/1500], Training Loss: 10.389573472571769, Validation Loss: 23.242876052856445\n",
      "Epoch [386/1500], Training Loss: 10.305724376325589, Validation Loss: 23.120723724365234\n",
      "Epoch [387/1500], Training Loss: 10.223063402089885, Validation Loss: 22.999374389648438\n",
      "Epoch [388/1500], Training Loss: 10.141557879023061, Validation Loss: 22.88035011291504\n",
      "Epoch [389/1500], Training Loss: 10.061099453244424, Validation Loss: 22.7628116607666\n",
      "Epoch [390/1500], Training Loss: 9.981554707065058, Validation Loss: 22.64577865600586\n",
      "Epoch [391/1500], Training Loss: 9.903008460531858, Validation Loss: 22.530536651611328\n",
      "Epoch [392/1500], Training Loss: 9.825499544347847, Validation Loss: 22.415681838989258\n",
      "Epoch [393/1500], Training Loss: 9.749154732033793, Validation Loss: 22.301799774169922\n",
      "Epoch [394/1500], Training Loss: 9.673735589144236, Validation Loss: 22.188823699951172\n",
      "Epoch [395/1500], Training Loss: 9.599167562029258, Validation Loss: 22.07639503479004\n",
      "Epoch [396/1500], Training Loss: 9.52552321771811, Validation Loss: 21.964445114135742\n",
      "Epoch [397/1500], Training Loss: 9.452817760804294, Validation Loss: 21.85295867919922\n",
      "Epoch [398/1500], Training Loss: 9.381057896298346, Validation Loss: 21.742765426635742\n",
      "Epoch [399/1500], Training Loss: 9.310033918691758, Validation Loss: 21.63184928894043\n",
      "Epoch [400/1500], Training Loss: 9.239831851411632, Validation Loss: 21.522491455078125\n",
      "Epoch [401/1500], Training Loss: 9.170502082833645, Validation Loss: 21.413000106811523\n",
      "Epoch [402/1500], Training Loss: 9.101911011769388, Validation Loss: 21.30397605895996\n",
      "Epoch [403/1500], Training Loss: 9.03414226060964, Validation Loss: 21.195125579833984\n",
      "Epoch [404/1500], Training Loss: 8.967283463528327, Validation Loss: 21.086397171020508\n",
      "Epoch [405/1500], Training Loss: 8.901252427792658, Validation Loss: 20.97817611694336\n",
      "Epoch [406/1500], Training Loss: 8.83585024057807, Validation Loss: 20.869142532348633\n",
      "Epoch [407/1500], Training Loss: 8.771109525395849, Validation Loss: 20.761188507080078\n",
      "Epoch [408/1500], Training Loss: 8.707162431577837, Validation Loss: 20.653167724609375\n",
      "Epoch [409/1500], Training Loss: 8.643908199277114, Validation Loss: 20.54545783996582\n",
      "Epoch [410/1500], Training Loss: 8.581357354418703, Validation Loss: 20.438228607177734\n",
      "Epoch [411/1500], Training Loss: 8.519444396447712, Validation Loss: 20.33042335510254\n",
      "Epoch [412/1500], Training Loss: 8.458197616494715, Validation Loss: 20.22332763671875\n",
      "Epoch [413/1500], Training Loss: 8.397571482457248, Validation Loss: 20.116661071777344\n",
      "Epoch [414/1500], Training Loss: 8.33761541320927, Validation Loss: 20.010013580322266\n",
      "Epoch [415/1500], Training Loss: 8.278341480740771, Validation Loss: 19.903369903564453\n",
      "Epoch [416/1500], Training Loss: 8.219594703847033, Validation Loss: 19.796749114990234\n",
      "Epoch [417/1500], Training Loss: 8.161525573910732, Validation Loss: 19.69091033935547\n",
      "Epoch [418/1500], Training Loss: 8.104122991440537, Validation Loss: 19.585289001464844\n",
      "Epoch [419/1500], Training Loss: 8.047227136738394, Validation Loss: 19.4796085357666\n",
      "Epoch [420/1500], Training Loss: 7.990803719515328, Validation Loss: 19.3745174407959\n",
      "Epoch [421/1500], Training Loss: 7.934943785965979, Validation Loss: 19.270164489746094\n",
      "Epoch [422/1500], Training Loss: 7.879651120525943, Validation Loss: 19.16605567932129\n",
      "Epoch [423/1500], Training Loss: 7.824950923470262, Validation Loss: 19.06231117248535\n",
      "Epoch [424/1500], Training Loss: 7.7707923895317315, Validation Loss: 18.959203720092773\n",
      "Epoch [425/1500], Training Loss: 7.717189607323926, Validation Loss: 18.85650634765625\n",
      "Epoch [426/1500], Training Loss: 7.66412829435772, Validation Loss: 18.755008697509766\n",
      "Epoch [427/1500], Training Loss: 7.611631557199712, Validation Loss: 18.653369903564453\n",
      "Epoch [428/1500], Training Loss: 7.559693584934392, Validation Loss: 18.552719116210938\n",
      "Epoch [429/1500], Training Loss: 7.508304926278543, Validation Loss: 18.452878952026367\n",
      "Epoch [430/1500], Training Loss: 7.457539182217477, Validation Loss: 18.354310989379883\n",
      "Epoch [431/1500], Training Loss: 7.407302752779245, Validation Loss: 18.255210876464844\n",
      "Epoch [432/1500], Training Loss: 7.357585521324285, Validation Loss: 18.15752410888672\n",
      "Epoch [433/1500], Training Loss: 7.308335993489235, Validation Loss: 18.059677124023438\n",
      "Epoch [434/1500], Training Loss: 7.2595557141067255, Validation Loss: 17.96336555480957\n",
      "Epoch [435/1500], Training Loss: 7.211226520223783, Validation Loss: 17.867046356201172\n",
      "Epoch [436/1500], Training Loss: 7.163372092847327, Validation Loss: 17.77179527282715\n",
      "Epoch [437/1500], Training Loss: 7.115991296680447, Validation Loss: 17.677907943725586\n",
      "Epoch [438/1500], Training Loss: 7.069123607574812, Validation Loss: 17.58416175842285\n",
      "Epoch [439/1500], Training Loss: 7.0227589599343885, Validation Loss: 17.491294860839844\n",
      "Epoch [440/1500], Training Loss: 6.976780763369376, Validation Loss: 17.398845672607422\n",
      "Epoch [441/1500], Training Loss: 6.931169185646279, Validation Loss: 17.3075008392334\n",
      "Epoch [442/1500], Training Loss: 6.886016982677789, Validation Loss: 17.21680450439453\n",
      "Epoch [443/1500], Training Loss: 6.841290214463996, Validation Loss: 17.12689781188965\n",
      "Epoch [444/1500], Training Loss: 6.796994140735108, Validation Loss: 17.038434982299805\n",
      "Epoch [445/1500], Training Loss: 6.753132666004167, Validation Loss: 16.949867248535156\n",
      "Epoch [446/1500], Training Loss: 6.709690227481705, Validation Loss: 16.862844467163086\n",
      "Epoch [447/1500], Training Loss: 6.666708021795582, Validation Loss: 16.776063919067383\n",
      "Epoch [448/1500], Training Loss: 6.624110616671952, Validation Loss: 16.690698623657227\n",
      "Epoch [449/1500], Training Loss: 6.581921581871687, Validation Loss: 16.60588264465332\n",
      "Epoch [450/1500], Training Loss: 6.54021858769187, Validation Loss: 16.52196502685547\n",
      "Epoch [451/1500], Training Loss: 6.498939095367384, Validation Loss: 16.438879013061523\n",
      "Epoch [452/1500], Training Loss: 6.458057205550724, Validation Loss: 16.357044219970703\n",
      "Epoch [453/1500], Training Loss: 6.417687654153866, Validation Loss: 16.27556037902832\n",
      "Epoch [454/1500], Training Loss: 6.3777104800926825, Validation Loss: 16.19577407836914\n",
      "Epoch [455/1500], Training Loss: 6.338179367809744, Validation Loss: 16.114700317382812\n",
      "Epoch [456/1500], Training Loss: 6.299107234056501, Validation Loss: 16.038715362548828\n",
      "Epoch [457/1500], Training Loss: 6.2604022414317235, Validation Loss: 15.95645523071289\n",
      "Epoch [458/1500], Training Loss: 6.222162488933418, Validation Loss: 15.887887001037598\n",
      "Epoch [459/1500], Training Loss: 6.184229452958863, Validation Loss: 15.797216415405273\n",
      "Epoch [460/1500], Training Loss: 6.14678956199429, Validation Loss: 15.747343063354492\n",
      "Epoch [461/1500], Training Loss: 6.109651962874555, Validation Loss: 15.629319190979004\n",
      "Epoch [462/1500], Training Loss: 6.07306247674806, Validation Loss: 15.632472038269043\n",
      "Epoch [463/1500], Training Loss: 6.036676645845903, Validation Loss: 15.435068130493164\n",
      "Epoch [464/1500], Training Loss: 6.001036615072465, Validation Loss: 15.551828384399414\n",
      "Epoch [465/1500], Training Loss: 5.9653344997801625, Validation Loss: 15.263427734375\n",
      "Epoch [466/1500], Training Loss: 5.93021464531546, Validation Loss: 15.435466766357422\n",
      "Epoch [467/1500], Training Loss: 5.895444618509135, Validation Loss: 15.146658897399902\n",
      "Epoch [468/1500], Training Loss: 5.860777270299869, Validation Loss: 15.273747444152832\n",
      "Epoch [469/1500], Training Loss: 5.826641876227103, Validation Loss: 15.006400108337402\n",
      "Epoch [470/1500], Training Loss: 5.792770155618881, Validation Loss: 15.134499549865723\n",
      "Epoch [471/1500], Training Loss: 5.7593119575804055, Validation Loss: 14.880603790283203\n",
      "Epoch [472/1500], Training Loss: 5.725920694614579, Validation Loss: 14.988574028015137\n",
      "Epoch [473/1500], Training Loss: 5.693129714619142, Validation Loss: 14.753059387207031\n",
      "Epoch [474/1500], Training Loss: 5.660402200793839, Validation Loss: 14.852618217468262\n",
      "Epoch [475/1500], Training Loss: 5.628299303171153, Validation Loss: 14.631463050842285\n",
      "Epoch [476/1500], Training Loss: 5.596157265125763, Validation Loss: 14.717869758605957\n",
      "Epoch [477/1500], Training Loss: 5.564659831830602, Validation Loss: 14.512219429016113\n",
      "Epoch [478/1500], Training Loss: 5.533199108856899, Validation Loss: 14.589910507202148\n",
      "Epoch [479/1500], Training Loss: 5.502388275025332, Validation Loss: 14.397428512573242\n",
      "Epoch [480/1500], Training Loss: 5.471573949078614, Validation Loss: 14.464407920837402\n",
      "Epoch [481/1500], Training Loss: 5.441373596792887, Validation Loss: 14.285261154174805\n",
      "Epoch [482/1500], Training Loss: 5.411156257108066, Validation Loss: 14.343802452087402\n",
      "Epoch [483/1500], Training Loss: 5.381574485555653, Validation Loss: 14.17642593383789\n",
      "Epoch [484/1500], Training Loss: 5.351949037121802, Validation Loss: 14.226524353027344\n",
      "Epoch [485/1500], Training Loss: 5.322948928883904, Validation Loss: 14.071410179138184\n",
      "Epoch [486/1500], Training Loss: 5.293938344881988, Validation Loss: 14.113064765930176\n",
      "Epoch [487/1500], Training Loss: 5.265537836494623, Validation Loss: 13.968889236450195\n",
      "Epoch [488/1500], Training Loss: 5.2370753668371, Validation Loss: 14.004861831665039\n",
      "Epoch [489/1500], Training Loss: 5.209234631191747, Validation Loss: 13.870311737060547\n",
      "Epoch [490/1500], Training Loss: 5.1813498835152, Validation Loss: 13.899900436401367\n",
      "Epoch [491/1500], Training Loss: 5.154031119640625, Validation Loss: 13.77425479888916\n",
      "Epoch [492/1500], Training Loss: 5.126748030748313, Validation Loss: 13.800016403198242\n",
      "Epoch [493/1500], Training Loss: 5.100067435935365, Validation Loss: 13.68185043334961\n",
      "Epoch [494/1500], Training Loss: 5.07336365803922, Validation Loss: 13.704540252685547\n",
      "Epoch [495/1500], Training Loss: 5.04731606575952, Validation Loss: 13.592921257019043\n",
      "Epoch [496/1500], Training Loss: 5.021190060856351, Validation Loss: 13.613755226135254\n",
      "Epoch [497/1500], Training Loss: 4.995579731781721, Validation Loss: 13.506771087646484\n",
      "Epoch [498/1500], Training Loss: 4.969905116680026, Validation Loss: 13.52739143371582\n",
      "Epoch [499/1500], Training Loss: 4.944777069801057, Validation Loss: 13.422612190246582\n",
      "Epoch [500/1500], Training Loss: 4.91956240519735, Validation Loss: 13.444438934326172\n",
      "Epoch [501/1500], Training Loss: 4.894993415789864, Validation Loss: 13.34200668334961\n",
      "Epoch [502/1500], Training Loss: 4.8703700395232215, Validation Loss: 13.366952896118164\n",
      "Epoch [503/1500], Training Loss: 4.846318170785396, Validation Loss: 13.2642822265625\n",
      "Epoch [504/1500], Training Loss: 4.822227663357269, Validation Loss: 13.293825149536133\n",
      "Epoch [505/1500], Training Loss: 4.79876980848147, Validation Loss: 13.188551902770996\n",
      "Epoch [506/1500], Training Loss: 4.7751697934067145, Validation Loss: 13.225537300109863\n",
      "Epoch [507/1500], Training Loss: 4.752215705864749, Validation Loss: 13.115503311157227\n",
      "Epoch [508/1500], Training Loss: 4.729159980369097, Validation Loss: 13.163394927978516\n",
      "Epoch [509/1500], Training Loss: 4.706665008263273, Validation Loss: 13.04567813873291\n",
      "Epoch [510/1500], Training Loss: 4.68404314306966, Validation Loss: 13.105843544006348\n",
      "Epoch [511/1500], Training Loss: 4.6620979561807, Validation Loss: 12.978697776794434\n",
      "Epoch [512/1500], Training Loss: 4.639925483525565, Validation Loss: 13.052892684936523\n",
      "Epoch [513/1500], Training Loss: 4.618552644573644, Validation Loss: 12.915387153625488\n",
      "Epoch [514/1500], Training Loss: 4.596821781798369, Validation Loss: 13.002866744995117\n",
      "Epoch [515/1500], Training Loss: 4.575886071406784, Validation Loss: 12.854704856872559\n",
      "Epoch [516/1500], Training Loss: 4.554555433670914, Validation Loss: 12.956075668334961\n",
      "Epoch [517/1500], Training Loss: 4.5341442089396935, Validation Loss: 12.797072410583496\n",
      "Epoch [518/1500], Training Loss: 4.513171691129786, Validation Loss: 12.912622451782227\n",
      "Epoch [519/1500], Training Loss: 4.493168433860374, Validation Loss: 12.742287635803223\n",
      "Epoch [520/1500], Training Loss: 4.472507219811891, Validation Loss: 12.870849609375\n",
      "Epoch [521/1500], Training Loss: 4.452934643547787, Validation Loss: 12.689179420471191\n",
      "Epoch [522/1500], Training Loss: 4.432694191002153, Validation Loss: 12.832537651062012\n",
      "Epoch [523/1500], Training Loss: 4.413585607326805, Validation Loss: 12.639020919799805\n",
      "Epoch [524/1500], Training Loss: 4.3936985396128785, Validation Loss: 12.795891761779785\n",
      "Epoch [525/1500], Training Loss: 4.374977417754921, Validation Loss: 12.590503692626953\n",
      "Epoch [526/1500], Training Loss: 4.355428620030512, Validation Loss: 12.761251449584961\n",
      "Epoch [527/1500], Training Loss: 4.337124895662096, Validation Loss: 12.543218612670898\n",
      "Epoch [528/1500], Training Loss: 4.31789449460607, Validation Loss: 12.727091789245605\n",
      "Epoch [529/1500], Training Loss: 4.299994952730102, Validation Loss: 12.497066497802734\n",
      "Epoch [530/1500], Training Loss: 4.281136091349242, Validation Loss: 12.693462371826172\n",
      "Epoch [531/1500], Training Loss: 4.263617616894028, Validation Loss: 12.450952529907227\n",
      "Epoch [532/1500], Training Loss: 4.245013061188737, Validation Loss: 12.660516738891602\n",
      "Epoch [533/1500], Training Loss: 4.227865959635354, Validation Loss: 12.404801368713379\n",
      "Epoch [534/1500], Training Loss: 4.209501493242545, Validation Loss: 12.625739097595215\n",
      "Epoch [535/1500], Training Loss: 4.192725795493709, Validation Loss: 12.35797119140625\n",
      "Epoch [536/1500], Training Loss: 4.174625805170605, Validation Loss: 12.58952808380127\n",
      "Epoch [537/1500], Training Loss: 4.158116146777118, Validation Loss: 12.309582710266113\n",
      "Epoch [538/1500], Training Loss: 4.140286590212159, Validation Loss: 12.551480293273926\n",
      "Epoch [539/1500], Training Loss: 4.124042070556492, Validation Loss: 12.260169982910156\n",
      "Epoch [540/1500], Training Loss: 4.106403901840353, Validation Loss: 12.509917259216309\n",
      "Epoch [541/1500], Training Loss: 4.090474517628676, Validation Loss: 12.209480285644531\n",
      "Epoch [542/1500], Training Loss: 4.073073716530988, Validation Loss: 12.466458320617676\n",
      "Epoch [543/1500], Training Loss: 4.057442303548978, Validation Loss: 12.15705394744873\n",
      "Epoch [544/1500], Training Loss: 4.040272646023167, Validation Loss: 12.419880867004395\n",
      "Epoch [545/1500], Training Loss: 4.024804400516783, Validation Loss: 12.103033065795898\n",
      "Epoch [546/1500], Training Loss: 4.007722521093065, Validation Loss: 12.368695259094238\n",
      "Epoch [547/1500], Training Loss: 3.992488047267255, Validation Loss: 12.047138214111328\n",
      "Epoch [548/1500], Training Loss: 3.975519806601252, Validation Loss: 12.31394100189209\n",
      "Epoch [549/1500], Training Loss: 3.9604651826634174, Validation Loss: 11.989437103271484\n",
      "Epoch [550/1500], Training Loss: 3.943739952848519, Validation Loss: 12.256596565246582\n",
      "Epoch [551/1500], Training Loss: 3.928848325988302, Validation Loss: 11.93097972869873\n",
      "Epoch [552/1500], Training Loss: 3.912346466622765, Validation Loss: 12.197237014770508\n",
      "Epoch [553/1500], Training Loss: 3.897680504954829, Validation Loss: 11.871850967407227\n",
      "Epoch [554/1500], Training Loss: 3.881337769268207, Validation Loss: 12.13392162322998\n",
      "Epoch [555/1500], Training Loss: 3.86682830486766, Validation Loss: 11.811601638793945\n",
      "Epoch [556/1500], Training Loss: 3.8506719651222086, Validation Loss: 12.068578720092773\n",
      "Epoch [557/1500], Training Loss: 3.836343667261299, Validation Loss: 11.751185417175293\n",
      "Epoch [558/1500], Training Loss: 3.8203902857791423, Validation Loss: 12.00255298614502\n",
      "Epoch [559/1500], Training Loss: 3.806244609160602, Validation Loss: 11.690563201904297\n",
      "Epoch [560/1500], Training Loss: 3.790507982122633, Validation Loss: 11.934895515441895\n",
      "Epoch [561/1500], Training Loss: 3.7764781765347104, Validation Loss: 11.62903118133545\n",
      "Epoch [562/1500], Training Loss: 3.7609830356409417, Validation Loss: 11.866742134094238\n",
      "Epoch [563/1500], Training Loss: 3.7471416021936412, Validation Loss: 11.567192077636719\n",
      "Epoch [564/1500], Training Loss: 3.731887970635637, Validation Loss: 11.798338890075684\n",
      "Epoch [565/1500], Training Loss: 3.7182210273858405, Validation Loss: 11.505111694335938\n",
      "Epoch [566/1500], Training Loss: 3.7031802271637866, Validation Loss: 11.728116989135742\n",
      "Epoch [567/1500], Training Loss: 3.689656091101729, Validation Loss: 11.442564964294434\n",
      "Epoch [568/1500], Training Loss: 3.6748160680896222, Validation Loss: 11.658235549926758\n",
      "Epoch [569/1500], Training Loss: 3.661451672911256, Validation Loss: 11.378690719604492\n",
      "Epoch [570/1500], Training Loss: 3.64681696949382, Validation Loss: 11.588547706604004\n",
      "Epoch [571/1500], Training Loss: 3.6336937521198274, Validation Loss: 11.316242218017578\n",
      "Epoch [572/1500], Training Loss: 3.619360343970299, Validation Loss: 11.520323753356934\n",
      "Epoch [573/1500], Training Loss: 3.6064642497300072, Validation Loss: 11.25252628326416\n",
      "Epoch [574/1500], Training Loss: 3.5923630948569727, Validation Loss: 11.452043533325195\n",
      "Epoch [575/1500], Training Loss: 3.5796994216643627, Validation Loss: 11.188772201538086\n",
      "Epoch [576/1500], Training Loss: 3.565894126952567, Validation Loss: 11.38478946685791\n",
      "Epoch [577/1500], Training Loss: 3.553477760994582, Validation Loss: 11.125631332397461\n",
      "Epoch [578/1500], Training Loss: 3.539945489902615, Validation Loss: 11.319116592407227\n",
      "Epoch [579/1500], Training Loss: 3.527702552699203, Validation Loss: 11.061907768249512\n",
      "Epoch [580/1500], Training Loss: 3.514392856347979, Validation Loss: 11.253561973571777\n",
      "Epoch [581/1500], Training Loss: 3.5023904667068555, Validation Loss: 10.999263763427734\n",
      "Epoch [582/1500], Training Loss: 3.4893272783641054, Validation Loss: 11.189114570617676\n",
      "Epoch [583/1500], Training Loss: 3.477538962945771, Validation Loss: 10.937596321105957\n",
      "Epoch [584/1500], Training Loss: 3.4646854684085686, Validation Loss: 11.126412391662598\n",
      "Epoch [585/1500], Training Loss: 3.4531166156688147, Validation Loss: 10.876140594482422\n",
      "Epoch [586/1500], Training Loss: 3.440526958607874, Validation Loss: 11.06383991241455\n",
      "Epoch [587/1500], Training Loss: 3.429156089928291, Validation Loss: 10.814557075500488\n",
      "Epoch [588/1500], Training Loss: 3.4167813841925048, Validation Loss: 11.002765655517578\n",
      "Epoch [589/1500], Training Loss: 3.405642813356915, Validation Loss: 10.75443172454834\n",
      "Epoch [590/1500], Training Loss: 3.3935026781393547, Validation Loss: 10.94256591796875\n",
      "Epoch [591/1500], Training Loss: 3.382584497608062, Validation Loss: 10.694995880126953\n",
      "Epoch [592/1500], Training Loss: 3.3706780393957514, Validation Loss: 10.883567810058594\n",
      "Epoch [593/1500], Training Loss: 3.359953459685169, Validation Loss: 10.63664436340332\n",
      "Epoch [594/1500], Training Loss: 3.348224794769847, Validation Loss: 10.825441360473633\n",
      "Epoch [595/1500], Training Loss: 3.337725270534339, Validation Loss: 10.57791519165039\n",
      "Epoch [596/1500], Training Loss: 3.326209268005044, Validation Loss: 10.76811408996582\n",
      "Epoch [597/1500], Training Loss: 3.3159242105227493, Validation Loss: 10.520889282226562\n",
      "Epoch [598/1500], Training Loss: 3.3046306292766228, Validation Loss: 10.711967468261719\n",
      "Epoch [599/1500], Training Loss: 3.2945867038696965, Validation Loss: 10.464780807495117\n",
      "Epoch [600/1500], Training Loss: 3.283542464123158, Validation Loss: 10.656983375549316\n",
      "Epoch [601/1500], Training Loss: 3.273737187913484, Validation Loss: 10.409514427185059\n",
      "Epoch [602/1500], Training Loss: 3.2629333323236347, Validation Loss: 10.603177070617676\n",
      "Epoch [603/1500], Training Loss: 3.2533431793550602, Validation Loss: 10.355231285095215\n",
      "Epoch [604/1500], Training Loss: 3.242748159443678, Validation Loss: 10.549516677856445\n",
      "Epoch [605/1500], Training Loss: 3.233362748659161, Validation Loss: 10.30182933807373\n",
      "Epoch [606/1500], Training Loss: 3.2229641615188003, Validation Loss: 10.496990203857422\n",
      "Epoch [607/1500], Training Loss: 3.2137876138669266, Validation Loss: 10.249367713928223\n",
      "Epoch [608/1500], Training Loss: 3.20356232902157, Validation Loss: 10.445871353149414\n",
      "Epoch [609/1500], Training Loss: 3.1946135701375713, Validation Loss: 10.19792652130127\n",
      "Epoch [610/1500], Training Loss: 3.1845422322422503, Validation Loss: 10.39555549621582\n",
      "Epoch [611/1500], Training Loss: 3.1758060018247933, Validation Loss: 10.147953033447266\n",
      "Epoch [612/1500], Training Loss: 3.165941151371526, Validation Loss: 10.346209526062012\n",
      "Epoch [613/1500], Training Loss: 3.157371931304612, Validation Loss: 10.097590446472168\n",
      "Epoch [614/1500], Training Loss: 3.1476912919443136, Validation Loss: 10.296992301940918\n",
      "Epoch [615/1500], Training Loss: 3.139332697746678, Validation Loss: 10.049397468566895\n",
      "Epoch [616/1500], Training Loss: 3.129836189658897, Validation Loss: 10.248844146728516\n",
      "Epoch [617/1500], Training Loss: 3.1216489434818664, Validation Loss: 10.001786231994629\n",
      "Epoch [618/1500], Training Loss: 3.1123154859252375, Validation Loss: 10.201549530029297\n",
      "Epoch [619/1500], Training Loss: 3.1042913094895006, Validation Loss: 9.954935073852539\n",
      "Epoch [620/1500], Training Loss: 3.0951077683204735, Validation Loss: 10.15452766418457\n",
      "Epoch [621/1500], Training Loss: 3.08728607865025, Validation Loss: 9.909214973449707\n",
      "Epoch [622/1500], Training Loss: 3.0782636049436465, Validation Loss: 10.109068870544434\n",
      "Epoch [623/1500], Training Loss: 3.070622913899372, Validation Loss: 9.86414623260498\n",
      "Epoch [624/1500], Training Loss: 3.0617432314862816, Validation Loss: 10.063444137573242\n",
      "Epoch [625/1500], Training Loss: 3.054284764290977, Validation Loss: 9.820341110229492\n",
      "Epoch [626/1500], Training Loss: 3.0455558340943205, Validation Loss: 10.018675804138184\n",
      "Epoch [627/1500], Training Loss: 3.038232563160756, Validation Loss: 9.776491165161133\n",
      "Epoch [628/1500], Training Loss: 3.029660056641958, Validation Loss: 9.9741849899292\n",
      "Epoch [629/1500], Training Loss: 3.022504887890042, Validation Loss: 9.734354972839355\n",
      "Epoch [630/1500], Training Loss: 3.014043453233662, Validation Loss: 9.930628776550293\n",
      "Epoch [631/1500], Training Loss: 3.007033106445121, Validation Loss: 9.692829132080078\n",
      "Epoch [632/1500], Training Loss: 2.9987168977689524, Validation Loss: 9.887606620788574\n",
      "Epoch [633/1500], Training Loss: 2.991807433322932, Validation Loss: 9.651867866516113\n",
      "Epoch [634/1500], Training Loss: 2.983666753149914, Validation Loss: 9.844907760620117\n",
      "Epoch [635/1500], Training Loss: 2.9768939505085035, Validation Loss: 9.612044334411621\n",
      "Epoch [636/1500], Training Loss: 2.968846819411447, Validation Loss: 9.802812576293945\n",
      "Epoch [637/1500], Training Loss: 2.9621964362656157, Validation Loss: 9.572067260742188\n",
      "Epoch [638/1500], Training Loss: 2.9542621003578478, Validation Loss: 9.761140823364258\n",
      "Epoch [639/1500], Training Loss: 2.9477572835186088, Validation Loss: 9.533863067626953\n",
      "Epoch [640/1500], Training Loss: 2.939952504771933, Validation Loss: 9.719547271728516\n",
      "Epoch [641/1500], Training Loss: 2.933571081050972, Validation Loss: 9.496102333068848\n",
      "Epoch [642/1500], Training Loss: 2.925894293971005, Validation Loss: 9.678826332092285\n",
      "Epoch [643/1500], Training Loss: 2.9196606147911774, Validation Loss: 9.45909309387207\n",
      "Epoch [644/1500], Training Loss: 2.9121266349588484, Validation Loss: 9.63919734954834\n",
      "Epoch [645/1500], Training Loss: 2.9059818007987688, Validation Loss: 9.421862602233887\n",
      "Epoch [646/1500], Training Loss: 2.898547918146237, Validation Loss: 9.598979949951172\n",
      "Epoch [647/1500], Training Loss: 2.892526383935669, Validation Loss: 9.3862886428833\n",
      "Epoch [648/1500], Training Loss: 2.8852171722854862, Validation Loss: 9.559625625610352\n",
      "Epoch [649/1500], Training Loss: 2.8793047345492453, Validation Loss: 9.350887298583984\n",
      "Epoch [650/1500], Training Loss: 2.8721172903738017, Validation Loss: 9.520500183105469\n",
      "Epoch [651/1500], Training Loss: 2.866278489001239, Validation Loss: 9.315951347351074\n",
      "Epoch [652/1500], Training Loss: 2.859188781849049, Validation Loss: 9.48076057434082\n",
      "Epoch [653/1500], Training Loss: 2.8534479274926055, Validation Loss: 9.28193473815918\n",
      "Epoch [654/1500], Training Loss: 2.846453777974997, Validation Loss: 9.443376541137695\n",
      "Epoch [655/1500], Training Loss: 2.840833215748194, Validation Loss: 9.248754501342773\n",
      "Epoch [656/1500], Training Loss: 2.8339625044319745, Validation Loss: 9.406328201293945\n",
      "Epoch [657/1500], Training Loss: 2.8284408356693755, Validation Loss: 9.216291427612305\n",
      "Epoch [658/1500], Training Loss: 2.8216741803831784, Validation Loss: 9.369399070739746\n",
      "Epoch [659/1500], Training Loss: 2.8162204964970114, Validation Loss: 9.183934211730957\n",
      "Epoch [660/1500], Training Loss: 2.8095749529643257, Validation Loss: 9.332767486572266\n",
      "Epoch [661/1500], Training Loss: 2.804190126095359, Validation Loss: 9.15241527557373\n",
      "Epoch [662/1500], Training Loss: 2.797645840022637, Validation Loss: 9.296217918395996\n",
      "Epoch [663/1500], Training Loss: 2.7923661274733274, Validation Loss: 9.120942115783691\n",
      "Epoch [664/1500], Training Loss: 2.7859556253573188, Validation Loss: 9.260993957519531\n",
      "Epoch [665/1500], Training Loss: 2.7807692972290297, Validation Loss: 9.091232299804688\n",
      "Epoch [666/1500], Training Loss: 2.7744626484123023, Validation Loss: 9.225878715515137\n",
      "Epoch [667/1500], Training Loss: 2.7693608600418824, Validation Loss: 9.06135368347168\n",
      "Epoch [668/1500], Training Loss: 2.7631851652083026, Validation Loss: 9.191617965698242\n",
      "Epoch [669/1500], Training Loss: 2.7581891106502043, Validation Loss: 9.03199291229248\n",
      "Epoch [670/1500], Training Loss: 2.7521219375049415, Validation Loss: 9.15658187866211\n",
      "Epoch [671/1500], Training Loss: 2.7471699312491595, Validation Loss: 9.003617286682129\n",
      "Epoch [672/1500], Training Loss: 2.7412246816116514, Validation Loss: 9.123682975769043\n",
      "Epoch [673/1500], Training Loss: 2.7363776587760227, Validation Loss: 8.975377082824707\n",
      "Epoch [674/1500], Training Loss: 2.7305319671188193, Validation Loss: 9.090919494628906\n",
      "Epoch [675/1500], Training Loss: 2.7257400519619766, Validation Loss: 8.94824504852295\n",
      "Epoch [676/1500], Training Loss: 2.7199983912110692, Validation Loss: 9.058854103088379\n",
      "Epoch [677/1500], Training Loss: 2.715277098163898, Validation Loss: 8.921273231506348\n",
      "Epoch [678/1500], Training Loss: 2.709647686841823, Validation Loss: 9.026566505432129\n",
      "Epoch [679/1500], Training Loss: 2.7049655915274387, Validation Loss: 8.894749641418457\n",
      "Epoch [680/1500], Training Loss: 2.6994531783513334, Validation Loss: 8.994908332824707\n",
      "Epoch [681/1500], Training Loss: 2.6948387437133015, Validation Loss: 8.868410110473633\n",
      "Epoch [682/1500], Training Loss: 2.68944662798355, Validation Loss: 8.963995933532715\n",
      "Epoch [683/1500], Training Loss: 2.6849001031825956, Validation Loss: 8.843446731567383\n",
      "Epoch [684/1500], Training Loss: 2.6796116358799047, Validation Loss: 8.934176445007324\n",
      "Epoch [685/1500], Training Loss: 2.6751229329116266, Validation Loss: 8.818077087402344\n",
      "Epoch [686/1500], Training Loss: 2.6699374506169784, Validation Loss: 8.904097557067871\n",
      "Epoch [687/1500], Training Loss: 2.665548949508574, Validation Loss: 8.79417610168457\n",
      "Epoch [688/1500], Training Loss: 2.660492698370039, Validation Loss: 8.874576568603516\n",
      "Epoch [689/1500], Training Loss: 2.656146252755346, Validation Loss: 8.769401550292969\n",
      "Epoch [690/1500], Training Loss: 2.6511928998034073, Validation Loss: 8.845880508422852\n",
      "Epoch [691/1500], Training Loss: 2.646918546179237, Validation Loss: 8.746908187866211\n",
      "Epoch [692/1500], Training Loss: 2.6420625277238914, Validation Loss: 8.818130493164062\n",
      "Epoch [693/1500], Training Loss: 2.6378432391378572, Validation Loss: 8.725281715393066\n",
      "Epoch [694/1500], Training Loss: 2.633096954805914, Validation Loss: 8.79080867767334\n",
      "Epoch [695/1500], Training Loss: 2.628933278822236, Validation Loss: 8.702290534973145\n",
      "Epoch [696/1500], Training Loss: 2.62427600408599, Validation Loss: 8.763123512268066\n",
      "Epoch [697/1500], Training Loss: 2.6201645376347678, Validation Loss: 8.681078910827637\n",
      "Epoch [698/1500], Training Loss: 2.6155882357762863, Validation Loss: 8.736156463623047\n",
      "Epoch [699/1500], Training Loss: 2.611510857866833, Validation Loss: 8.659131050109863\n",
      "Epoch [700/1500], Training Loss: 2.6070164098370863, Validation Loss: 8.709559440612793\n",
      "Epoch [701/1500], Training Loss: 2.6029915316683176, Validation Loss: 8.638875961303711\n",
      "Epoch [702/1500], Training Loss: 2.5986046505852682, Validation Loss: 8.68385124206543\n",
      "Epoch [703/1500], Training Loss: 2.594621806589862, Validation Loss: 8.61796760559082\n",
      "Epoch [704/1500], Training Loss: 2.5903192681024394, Validation Loss: 8.658303260803223\n",
      "Epoch [705/1500], Training Loss: 2.586390055748822, Validation Loss: 8.597736358642578\n",
      "Epoch [706/1500], Training Loss: 2.5821925749060557, Validation Loss: 8.632540702819824\n",
      "Epoch [707/1500], Training Loss: 2.5783101279531944, Validation Loss: 8.576944351196289\n",
      "Epoch [708/1500], Training Loss: 2.574174305011604, Validation Loss: 8.607224464416504\n",
      "Epoch [709/1500], Training Loss: 2.5703365271840886, Validation Loss: 8.558497428894043\n",
      "Epoch [710/1500], Training Loss: 2.5663097348482125, Validation Loss: 8.582634925842285\n",
      "Epoch [711/1500], Training Loss: 2.5625368821362273, Validation Loss: 8.539641380310059\n",
      "Epoch [712/1500], Training Loss: 2.5585696978662575, Validation Loss: 8.557999610900879\n",
      "Epoch [713/1500], Training Loss: 2.5548331586143984, Validation Loss: 8.521295547485352\n",
      "Epoch [714/1500], Training Loss: 2.5509567238947213, Validation Loss: 8.53413200378418\n",
      "Epoch [715/1500], Training Loss: 2.547281809633237, Validation Loss: 8.503355979919434\n",
      "Epoch [716/1500], Training Loss: 2.543502554590435, Validation Loss: 8.510224342346191\n",
      "Epoch [717/1500], Training Loss: 2.539875695013814, Validation Loss: 8.485392570495605\n",
      "Epoch [718/1500], Training Loss: 2.5361708699795176, Validation Loss: 8.486839294433594\n",
      "Epoch [719/1500], Training Loss: 2.532590663521852, Validation Loss: 8.467411994934082\n",
      "Epoch [720/1500], Training Loss: 2.5289821151990703, Validation Loss: 8.464507102966309\n",
      "Epoch [721/1500], Training Loss: 2.525464241415831, Validation Loss: 8.448676109313965\n",
      "Epoch [722/1500], Training Loss: 2.521930428390089, Validation Loss: 8.443260192871094\n",
      "Epoch [723/1500], Training Loss: 2.518451122739843, Validation Loss: 8.430403709411621\n",
      "Epoch [724/1500], Training Loss: 2.514961313115666, Validation Loss: 8.422383308410645\n",
      "Epoch [725/1500], Training Loss: 2.5114986473762735, Validation Loss: 8.41163444519043\n",
      "Epoch [726/1500], Training Loss: 2.5080714129690937, Validation Loss: 8.403116226196289\n",
      "Epoch [727/1500], Training Loss: 2.504677770376496, Validation Loss: 8.393653869628906\n",
      "Epoch [728/1500], Training Loss: 2.501316252116279, Validation Loss: 8.38483715057373\n",
      "Epoch [729/1500], Training Loss: 2.497996343675496, Validation Loss: 8.37596607208252\n",
      "Epoch [730/1500], Training Loss: 2.4946893185735726, Validation Loss: 8.366811752319336\n",
      "Epoch [731/1500], Training Loss: 2.491436700634254, Validation Loss: 8.358295440673828\n",
      "Epoch [732/1500], Training Loss: 2.4882345682387985, Validation Loss: 8.349479675292969\n",
      "Epoch [733/1500], Training Loss: 2.4850666089149134, Validation Loss: 8.340873718261719\n",
      "Epoch [734/1500], Training Loss: 2.481939555838131, Validation Loss: 8.332721710205078\n",
      "Epoch [735/1500], Training Loss: 2.478855957183053, Validation Loss: 8.324636459350586\n",
      "Epoch [736/1500], Training Loss: 2.4757928737165087, Validation Loss: 8.31641674041748\n",
      "Epoch [737/1500], Training Loss: 2.4727616469891984, Validation Loss: 8.308177947998047\n",
      "Epoch [738/1500], Training Loss: 2.4697548151332502, Validation Loss: 8.299922943115234\n",
      "Epoch [739/1500], Training Loss: 2.46677896654215, Validation Loss: 8.291866302490234\n",
      "Epoch [740/1500], Training Loss: 2.4638358259887587, Validation Loss: 8.283742904663086\n",
      "Epoch [741/1500], Training Loss: 2.4609324671889414, Validation Loss: 8.27490234375\n",
      "Epoch [742/1500], Training Loss: 2.4580609284424377, Validation Loss: 8.267099380493164\n",
      "Epoch [743/1500], Training Loss: 2.455236241346106, Validation Loss: 8.259004592895508\n",
      "Epoch [744/1500], Training Loss: 2.452431697359138, Validation Loss: 8.251425743103027\n",
      "Epoch [745/1500], Training Loss: 2.4496612890042537, Validation Loss: 8.243688583374023\n",
      "Epoch [746/1500], Training Loss: 2.4469387874033353, Validation Loss: 8.2358980178833\n",
      "Epoch [747/1500], Training Loss: 2.444248258745903, Validation Loss: 8.228078842163086\n",
      "Epoch [748/1500], Training Loss: 2.441596140011515, Validation Loss: 8.221166610717773\n",
      "Epoch [749/1500], Training Loss: 2.4389947121986677, Validation Loss: 8.213890075683594\n",
      "Epoch [750/1500], Training Loss: 2.4364219672367966, Validation Loss: 8.207416534423828\n",
      "Epoch [751/1500], Training Loss: 2.4338938617686012, Validation Loss: 8.201148986816406\n",
      "Epoch [752/1500], Training Loss: 2.4314038520015275, Validation Loss: 8.195374488830566\n",
      "Epoch [753/1500], Training Loss: 2.428937099488234, Validation Loss: 8.189935684204102\n",
      "Epoch [754/1500], Training Loss: 2.4265188439980254, Validation Loss: 8.1853609085083\n",
      "Epoch [755/1500], Training Loss: 2.4241434803308173, Validation Loss: 8.181281089782715\n",
      "Epoch [756/1500], Training Loss: 2.4218082361578017, Validation Loss: 8.17747688293457\n",
      "Epoch [757/1500], Training Loss: 2.4195049265158097, Validation Loss: 8.174221992492676\n",
      "Epoch [758/1500], Training Loss: 2.4172470091759677, Validation Loss: 8.171287536621094\n",
      "Epoch [759/1500], Training Loss: 2.4150234909722945, Validation Loss: 8.169197082519531\n",
      "Epoch [760/1500], Training Loss: 2.4128463719335937, Validation Loss: 8.167762756347656\n",
      "Epoch [761/1500], Training Loss: 2.4107129462468597, Validation Loss: 8.166908264160156\n",
      "Epoch [762/1500], Training Loss: 2.4086238770742234, Validation Loss: 8.16602611541748\n",
      "Epoch [763/1500], Training Loss: 2.406551063145351, Validation Loss: 8.166586875915527\n",
      "Epoch [764/1500], Training Loss: 2.404516585147873, Validation Loss: 8.167283058166504\n",
      "Epoch [765/1500], Training Loss: 2.4025320317864636, Validation Loss: 8.168335914611816\n",
      "Epoch [766/1500], Training Loss: 2.4005802321189016, Validation Loss: 8.169856071472168\n",
      "Epoch [767/1500], Training Loss: 2.398660000218258, Validation Loss: 8.17218017578125\n",
      "Epoch [768/1500], Training Loss: 2.3967904040705315, Validation Loss: 8.174150466918945\n",
      "Epoch [769/1500], Training Loss: 2.3949681876902393, Validation Loss: 8.177289962768555\n",
      "Epoch [770/1500], Training Loss: 2.3931926795214404, Validation Loss: 8.181317329406738\n",
      "Epoch [771/1500], Training Loss: 2.3914677877723314, Validation Loss: 8.185518264770508\n",
      "Early stopping at epoch 771\n",
      "Final Test Loss: 9.222787857055664\n",
      "Training model with target variable: volume\n",
      "\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0003, window_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Training Loss: 1081681903064891.8, Validation Loss: 1084111821209600.0\n",
      "Epoch [2/1500], Training Loss: 1081681628218269.5, Validation Loss: 1084111619883008.0\n",
      "Epoch [3/1500], Training Loss: 1081681329434942.2, Validation Loss: 1084111418556416.0\n",
      "Epoch [4/1500], Training Loss: 1081681058511754.9, Validation Loss: 1084111015903232.0\n",
      "Epoch [5/1500], Training Loss: 1081680750945815.5, Validation Loss: 1084110814576640.0\n",
      "Epoch [6/1500], Training Loss: 1081680475964818.8, Validation Loss: 1084110613250048.0\n",
      "Epoch [7/1500], Training Loss: 1081680169934761.4, Validation Loss: 1084110411923456.0\n",
      "Epoch [8/1500], Training Loss: 1081679912018406.1, Validation Loss: 1084110076379136.0\n",
      "Epoch [9/1500], Training Loss: 1081679623515738.6, Validation Loss: 1084109539508224.0\n",
      "Epoch [10/1500], Training Loss: 1081679371779612.6, Validation Loss: 1084109136855040.0\n",
      "Epoch [11/1500], Training Loss: 1081679074928910.8, Validation Loss: 1084108935528448.0\n",
      "Epoch [12/1500], Training Loss: 1081678806254693.9, Validation Loss: 1084108667092992.0\n",
      "Epoch [13/1500], Training Loss: 1081678498436134.9, Validation Loss: 1084108331548672.0\n",
      "Epoch [14/1500], Training Loss: 1081678229606173.2, Validation Loss: 1084107996004352.0\n",
      "Epoch [15/1500], Training Loss: 1081677921146216.6, Validation Loss: 1084107593351168.0\n",
      "Epoch [16/1500], Training Loss: 1081677654063137.2, Validation Loss: 1084107392024576.0\n",
      "Epoch [17/1500], Training Loss: 1081677348865043.4, Validation Loss: 1084107190697984.0\n",
      "Epoch [18/1500], Training Loss: 1081677074583740.4, Validation Loss: 1084106989371392.0\n",
      "Epoch [19/1500], Training Loss: 1081676765126039.6, Validation Loss: 1084106720935936.0\n",
      "Epoch [20/1500], Training Loss: 1081676491716090.2, Validation Loss: 1084106385391616.0\n",
      "Epoch [21/1500], Training Loss: 1081676210453975.5, Validation Loss: 1084106184065024.0\n",
      "Epoch [22/1500], Training Loss: 1081675921216950.9, Validation Loss: 1084105848520704.0\n",
      "Epoch [23/1500], Training Loss: 1081675650064601.9, Validation Loss: 1084105647194112.0\n",
      "Epoch [24/1500], Training Loss: 1081675342102282.8, Validation Loss: 1084105378758656.0\n",
      "Epoch [25/1500], Training Loss: 1081675073245445.8, Validation Loss: 1084104841887744.0\n",
      "Epoch [26/1500], Training Loss: 1081674762651440.9, Validation Loss: 1084104372125696.0\n",
      "Epoch [27/1500], Training Loss: 1081674494852643.0, Validation Loss: 1084104170799104.0\n",
      "Epoch [28/1500], Training Loss: 1081674192797541.6, Validation Loss: 1084103902363648.0\n",
      "Epoch [29/1500], Training Loss: 1081673929782206.8, Validation Loss: 1084103835254784.0\n",
      "Epoch [30/1500], Training Loss: 1081673619761954.9, Validation Loss: 1084103701037056.0\n",
      "Epoch [31/1500], Training Loss: 1081673348904462.5, Validation Loss: 1084103499710464.0\n",
      "Epoch [32/1500], Training Loss: 1081673038797127.6, Validation Loss: 1084103231275008.0\n",
      "Epoch [33/1500], Training Loss: 1081672771141736.4, Validation Loss: 1084103029948416.0\n",
      "Epoch [34/1500], Training Loss: 1081672474987804.2, Validation Loss: 1084102895730688.0\n",
      "Epoch [35/1500], Training Loss: 1081672222332821.5, Validation Loss: 1084102493077504.0\n",
      "Epoch [36/1500], Training Loss: 1081671937214190.0, Validation Loss: 1084102090424320.0\n",
      "Epoch [37/1500], Training Loss: 1081671679308645.0, Validation Loss: 1084101687771136.0\n",
      "Epoch [38/1500], Training Loss: 1081671370972296.4, Validation Loss: 1084101285117952.0\n",
      "Epoch [39/1500], Training Loss: 1081671098413741.9, Validation Loss: 1084101150900224.0\n",
      "Epoch [40/1500], Training Loss: 1081670792652911.4, Validation Loss: 1084100748247040.0\n",
      "Epoch [41/1500], Training Loss: 1081670524461528.6, Validation Loss: 1084100546920448.0\n",
      "Epoch [42/1500], Training Loss: 1081670221881182.4, Validation Loss: 1084100144267264.0\n",
      "Epoch [43/1500], Training Loss: 1081669947234052.2, Validation Loss: 1084099808722944.0\n",
      "Epoch [44/1500], Training Loss: 1081669641091918.6, Validation Loss: 1084099540287488.0\n",
      "Epoch [45/1500], Training Loss: 1081669367565479.0, Validation Loss: 1084099338960896.0\n",
      "Epoch [46/1500], Training Loss: 1081669068392235.8, Validation Loss: 1084098936307712.0\n",
      "Epoch [47/1500], Training Loss: 1081668789501282.9, Validation Loss: 1084098802089984.0\n",
      "Epoch [48/1500], Training Loss: 1081668509414476.0, Validation Loss: 1084098399436800.0\n",
      "Epoch [49/1500], Training Loss: 1081668213319574.9, Validation Loss: 1084098131001344.0\n",
      "Epoch [50/1500], Training Loss: 1081667941834923.0, Validation Loss: 1084097862565888.0\n",
      "Epoch [51/1500], Training Loss: 1081667635109167.1, Validation Loss: 1084097594130432.0\n",
      "Epoch [52/1500], Training Loss: 1081667363653750.4, Validation Loss: 1084097392803840.0\n",
      "Epoch [53/1500], Training Loss: 1081667059200513.6, Validation Loss: 1084097258586112.0\n",
      "Epoch [54/1500], Training Loss: 1081666793612022.9, Validation Loss: 1084097191477248.0\n",
      "Epoch [55/1500], Training Loss: 1081666491475871.2, Validation Loss: 1084096788824064.0\n",
      "Epoch [56/1500], Training Loss: 1081666218634848.9, Validation Loss: 1084096251953152.0\n",
      "Epoch [57/1500], Training Loss: 1081665912722956.5, Validation Loss: 1084095715082240.0\n",
      "Epoch [58/1500], Training Loss: 1081665640082610.0, Validation Loss: 1084095647973376.0\n",
      "Epoch [59/1500], Training Loss: 1081665333338044.4, Validation Loss: 1084095312429056.0\n",
      "Epoch [60/1500], Training Loss: 1081665075206525.5, Validation Loss: 1084095245320192.0\n",
      "Epoch [61/1500], Training Loss: 1081664783561975.5, Validation Loss: 1084095043993600.0\n",
      "Epoch [62/1500], Training Loss: 1081664536814098.6, Validation Loss: 1084094708449280.0\n",
      "Epoch [63/1500], Training Loss: 1081664243104710.5, Validation Loss: 1084094440013824.0\n",
      "Epoch [64/1500], Training Loss: 1081663969114702.8, Validation Loss: 1084094238687232.0\n",
      "Epoch [65/1500], Training Loss: 1081663659020012.5, Validation Loss: 1084093903142912.0\n",
      "Epoch [66/1500], Training Loss: 1081663391078899.2, Validation Loss: 1084093701816320.0\n",
      "Epoch [67/1500], Training Loss: 1081663086764995.8, Validation Loss: 1084093500489728.0\n",
      "Epoch [68/1500], Training Loss: 1081662818626536.6, Validation Loss: 1084093164945408.0\n",
      "Epoch [69/1500], Training Loss: 1081662510891834.9, Validation Loss: 1084092963618816.0\n",
      "Epoch [70/1500], Training Loss: 1081662239122437.6, Validation Loss: 1084092762292224.0\n",
      "Epoch [71/1500], Training Loss: 1081661929001243.5, Validation Loss: 1084092493856768.0\n",
      "Epoch [72/1500], Training Loss: 1081661656664802.1, Validation Loss: 1084091956985856.0\n",
      "Epoch [73/1500], Training Loss: 1081661372033207.0, Validation Loss: 1084091420114944.0\n",
      "Epoch [74/1500], Training Loss: 1081661083142997.5, Validation Loss: 1084091218788352.0\n",
      "Epoch [75/1500], Training Loss: 1081660813053686.9, Validation Loss: 1084090950352896.0\n",
      "Epoch [76/1500], Training Loss: 1081660504857175.1, Validation Loss: 1084090816135168.0\n",
      "Epoch [77/1500], Training Loss: 1081660232741961.8, Validation Loss: 1084090547699712.0\n",
      "Epoch [78/1500], Training Loss: 1081659925996513.5, Validation Loss: 1084090346373120.0\n",
      "Epoch [79/1500], Training Loss: 1081659659469090.9, Validation Loss: 1084090010828800.0\n",
      "Epoch [80/1500], Training Loss: 1081659354502183.9, Validation Loss: 1084089675284480.0\n",
      "Epoch [81/1500], Training Loss: 1081659088761978.8, Validation Loss: 1084089608175616.0\n",
      "Epoch [82/1500], Training Loss: 1081658781742035.0, Validation Loss: 1084089272631296.0\n",
      "Epoch [83/1500], Training Loss: 1081658512473828.1, Validation Loss: 1084089071304704.0\n",
      "Epoch [84/1500], Training Loss: 1081658201097881.8, Validation Loss: 1084088467324928.0\n",
      "Epoch [85/1500], Training Loss: 1081657930405509.5, Validation Loss: 1084088265998336.0\n",
      "Epoch [86/1500], Training Loss: 1081657639638622.6, Validation Loss: 1084088064671744.0\n",
      "Epoch [87/1500], Training Loss: 1081657389970385.2, Validation Loss: 1084087796236288.0\n",
      "Epoch [88/1500], Training Loss: 1081657104605725.9, Validation Loss: 1084087460691968.0\n",
      "Epoch [89/1500], Training Loss: 1081656840789583.4, Validation Loss: 1084087259365376.0\n",
      "Epoch [90/1500], Training Loss: 1081656531266828.6, Validation Loss: 1084086923821056.0\n",
      "Epoch [91/1500], Training Loss: 1081656262716916.1, Validation Loss: 1084086722494464.0\n",
      "Epoch [92/1500], Training Loss: 1081655958423833.5, Validation Loss: 1084086454059008.0\n",
      "Epoch [93/1500], Training Loss: 1081655690029832.1, Validation Loss: 1084086252732416.0\n",
      "Epoch [94/1500], Training Loss: 1081655384604815.9, Validation Loss: 1084085917188096.0\n",
      "Epoch [95/1500], Training Loss: 1081655115551161.0, Validation Loss: 1084085715861504.0\n",
      "Epoch [96/1500], Training Loss: 1081654806133724.1, Validation Loss: 1084085380317184.0\n",
      "Epoch [97/1500], Training Loss: 1081654528592310.5, Validation Loss: 1084084977664000.0\n",
      "Epoch [98/1500], Training Loss: 1081654240749975.1, Validation Loss: 1084084776337408.0\n",
      "Epoch [99/1500], Training Loss: 1081653959161274.9, Validation Loss: 1084084507901952.0\n",
      "Epoch [100/1500], Training Loss: 1081653689712848.4, Validation Loss: 1084084172357632.0\n",
      "Epoch [101/1500], Training Loss: 1081653383076829.9, Validation Loss: 1084083903922176.0\n",
      "Epoch [102/1500], Training Loss: 1081653110814140.2, Validation Loss: 1084083367051264.0\n",
      "Epoch [103/1500], Training Loss: 1081652803400630.2, Validation Loss: 1084083031506944.0\n",
      "Epoch [104/1500], Training Loss: 1081652537776304.2, Validation Loss: 1084082427527168.0\n",
      "Epoch [105/1500], Training Loss: 1081652234934635.2, Validation Loss: 1084082159091712.0\n",
      "Epoch [106/1500], Training Loss: 1081651969203462.0, Validation Loss: 1084081823547392.0\n",
      "Epoch [107/1500], Training Loss: 1081651661559784.2, Validation Loss: 1084081420894208.0\n",
      "Epoch [108/1500], Training Loss: 1081651392635184.9, Validation Loss: 1084081219567616.0\n",
      "Epoch [109/1500], Training Loss: 1081651082023898.6, Validation Loss: 1084080884023296.0\n",
      "Epoch [110/1500], Training Loss: 1081650814340325.0, Validation Loss: 1084080615587840.0\n",
      "Epoch [111/1500], Training Loss: 1081650522262886.6, Validation Loss: 1084080280043520.0\n",
      "Epoch [112/1500], Training Loss: 1081650273887213.0, Validation Loss: 1084080078716928.0\n",
      "Epoch [113/1500], Training Loss: 1081649989074498.6, Validation Loss: 1084079608954880.0\n",
      "Epoch [114/1500], Training Loss: 1081649722047675.8, Validation Loss: 1084079340519424.0\n",
      "Epoch [115/1500], Training Loss: 1081649413515052.8, Validation Loss: 1084079072083968.0\n",
      "Epoch [116/1500], Training Loss: 1081649144801795.4, Validation Loss: 1084078669430784.0\n",
      "Epoch [117/1500], Training Loss: 1081648840963470.1, Validation Loss: 1084078535213056.0\n",
      "Epoch [118/1500], Training Loss: 1081648571646029.9, Validation Loss: 1084078266777600.0\n",
      "Epoch [119/1500], Training Loss: 1081648265673569.8, Validation Loss: 1084077931233280.0\n",
      "Epoch [120/1500], Training Loss: 1081647993720154.4, Validation Loss: 1084077729906688.0\n",
      "Epoch [121/1500], Training Loss: 1081647686607059.1, Validation Loss: 1084077327253504.0\n",
      "Epoch [122/1500], Training Loss: 1081647408483374.2, Validation Loss: 1084077193035776.0\n",
      "Epoch [123/1500], Training Loss: 1081647127629060.5, Validation Loss: 1084076991709184.0\n",
      "Epoch [124/1500], Training Loss: 1081646838604550.1, Validation Loss: 1084076723273728.0\n",
      "Epoch [125/1500], Training Loss: 1081646569347188.4, Validation Loss: 1084076320620544.0\n",
      "Epoch [126/1500], Training Loss: 1081646262080830.1, Validation Loss: 1084075985076224.0\n",
      "Epoch [127/1500], Training Loss: 1081645990079899.6, Validation Loss: 1084075716640768.0\n",
      "Epoch [128/1500], Training Loss: 1081645684401559.1, Validation Loss: 1084075515314176.0\n",
      "Epoch [129/1500], Training Loss: 1081645416997095.2, Validation Loss: 1084075381096448.0\n",
      "Epoch [130/1500], Training Loss: 1081645114821319.1, Validation Loss: 1084075179769856.0\n",
      "Epoch [131/1500], Training Loss: 1081644846701613.5, Validation Loss: 1084074642898944.0\n",
      "Epoch [132/1500], Training Loss: 1081644544290593.1, Validation Loss: 1084074575790080.0\n",
      "Epoch [133/1500], Training Loss: 1081644272851870.4, Validation Loss: 1084074441572352.0\n",
      "Epoch [134/1500], Training Loss: 1081643962011500.4, Validation Loss: 1084074038919168.0\n",
      "Epoch [135/1500], Training Loss: 1081643695019399.0, Validation Loss: 1084073770483712.0\n",
      "Epoch [136/1500], Training Loss: 1081643405557107.6, Validation Loss: 1084073032286208.0\n",
      "Epoch [137/1500], Training Loss: 1081643158264878.2, Validation Loss: 1084072898068480.0\n",
      "Epoch [138/1500], Training Loss: 1081642871218334.6, Validation Loss: 1084072629633024.0\n",
      "Epoch [139/1500], Training Loss: 1081642601275834.9, Validation Loss: 1084072495415296.0\n",
      "Epoch [140/1500], Training Loss: 1081642294651948.4, Validation Loss: 1084072226979840.0\n",
      "Epoch [141/1500], Training Loss: 1081642025898201.4, Validation Loss: 1084072025653248.0\n",
      "Epoch [142/1500], Training Loss: 1081641721627200.6, Validation Loss: 1084071488782336.0\n",
      "Epoch [143/1500], Training Loss: 1081641452935374.1, Validation Loss: 1084071153238016.0\n",
      "Epoch [144/1500], Training Loss: 1081641144062657.0, Validation Loss: 1084070951911424.0\n",
      "Epoch [145/1500], Training Loss: 1081640872268927.4, Validation Loss: 1084070750584832.0\n",
      "Epoch [146/1500], Training Loss: 1081640566220058.9, Validation Loss: 1084070549258240.0\n",
      "Epoch [147/1500], Training Loss: 1081640292677822.9, Validation Loss: 1084070146605056.0\n",
      "Epoch [148/1500], Training Loss: 1081640012669904.0, Validation Loss: 1084069945278464.0\n",
      "Epoch [149/1500], Training Loss: 1081639720086712.8, Validation Loss: 1084069676843008.0\n",
      "Epoch [150/1500], Training Loss: 1081639453391866.5, Validation Loss: 1084069341298688.0\n",
      "Epoch [151/1500], Training Loss: 1081639145773140.6, Validation Loss: 1084069139972096.0\n",
      "Epoch [152/1500], Training Loss: 1081638872937412.0, Validation Loss: 1084068938645504.0\n",
      "Epoch [153/1500], Training Loss: 1081638565213939.9, Validation Loss: 1084068603101184.0\n",
      "Epoch [154/1500], Training Loss: 1081638298125892.8, Validation Loss: 1084068535992320.0\n",
      "Epoch [155/1500], Training Loss: 1081637997660268.8, Validation Loss: 1084068334665728.0\n",
      "Epoch [156/1500], Training Loss: 1081637729679325.8, Validation Loss: 1084067932012544.0\n",
      "Epoch [157/1500], Training Loss: 1081637423605243.5, Validation Loss: 1084067797794816.0\n",
      "Epoch [158/1500], Training Loss: 1081637151988252.1, Validation Loss: 1084067529359360.0\n",
      "Epoch [159/1500], Training Loss: 1081636844582572.8, Validation Loss: 1084067059597312.0\n",
      "Epoch [160/1500], Training Loss: 1081636581209858.6, Validation Loss: 1084066858270720.0\n",
      "Epoch [161/1500], Training Loss: 1081636291853580.0, Validation Loss: 1084066589835264.0\n",
      "Epoch [162/1500], Training Loss: 1081636044055506.0, Validation Loss: 1084066388508672.0\n",
      "Epoch [163/1500], Training Loss: 1081635753000042.9, Validation Loss: 1084066254290944.0\n",
      "Epoch [164/1500], Training Loss: 1081635482606866.0, Validation Loss: 1084066052964352.0\n",
      "Epoch [165/1500], Training Loss: 1081635174310462.9, Validation Loss: 1084065650311168.0\n",
      "Epoch [166/1500], Training Loss: 1081634904881600.0, Validation Loss: 1084065180549120.0\n",
      "Epoch [167/1500], Training Loss: 1081634603286262.0, Validation Loss: 1084064643678208.0\n",
      "Epoch [168/1500], Training Loss: 1081634330683549.6, Validation Loss: 1084064241025024.0\n",
      "Epoch [169/1500], Training Loss: 1081634023137451.2, Validation Loss: 1084063905480704.0\n",
      "Epoch [170/1500], Training Loss: 1081633752075652.9, Validation Loss: 1084063704154112.0\n",
      "Epoch [171/1500], Training Loss: 1081633442982741.2, Validation Loss: 1084063502827520.0\n",
      "Epoch [172/1500], Training Loss: 1081633171237318.2, Validation Loss: 1084063301500928.0\n",
      "Epoch [173/1500], Training Loss: 1081632891447344.5, Validation Loss: 1084063100174336.0\n",
      "Epoch [174/1500], Training Loss: 1081632601415680.5, Validation Loss: 1084062898847744.0\n",
      "Epoch [175/1500], Training Loss: 1081632331513877.4, Validation Loss: 1084062697521152.0\n",
      "Epoch [176/1500], Training Loss: 1081632022496632.4, Validation Loss: 1084062563303424.0\n",
      "Epoch [177/1500], Training Loss: 1081631748604099.4, Validation Loss: 1084062294867968.0\n",
      "Epoch [178/1500], Training Loss: 1081631443993195.2, Validation Loss: 1084062093541376.0\n",
      "Epoch [179/1500], Training Loss: 1081631176145218.2, Validation Loss: 1084061690888192.0\n",
      "Epoch [180/1500], Training Loss: 1081630876054797.1, Validation Loss: 1084061288235008.0\n",
      "Epoch [181/1500], Training Loss: 1081630607722436.1, Validation Loss: 1084061086908416.0\n",
      "Epoch [182/1500], Training Loss: 1081630303157415.2, Validation Loss: 1084060617146368.0\n",
      "Epoch [183/1500], Training Loss: 1081630031585170.0, Validation Loss: 1084060415819776.0\n",
      "Epoch [184/1500], Training Loss: 1081629721255424.9, Validation Loss: 1084060214493184.0\n",
      "Epoch [185/1500], Training Loss: 1081629455745155.6, Validation Loss: 1084059946057728.0\n",
      "Epoch [186/1500], Training Loss: 1081629167040416.9, Validation Loss: 1084059744731136.0\n",
      "Epoch [187/1500], Training Loss: 1081628919566672.1, Validation Loss: 1084059543404544.0\n",
      "Epoch [188/1500], Training Loss: 1081628630267542.6, Validation Loss: 1084059342077952.0\n",
      "Epoch [189/1500], Training Loss: 1081628359907626.1, Validation Loss: 1084059006533632.0\n",
      "Epoch [190/1500], Training Loss: 1081628051806085.8, Validation Loss: 1084058670989312.0\n",
      "Epoch [191/1500], Training Loss: 1081627783237180.9, Validation Loss: 1084058469662720.0\n",
      "Epoch [192/1500], Training Loss: 1081627478520130.5, Validation Loss: 1084058201227264.0\n",
      "Epoch [193/1500], Training Loss: 1081627206357625.9, Validation Loss: 1084057999900672.0\n",
      "Epoch [194/1500], Training Loss: 1081626900154506.6, Validation Loss: 1084057463029760.0\n",
      "Epoch [195/1500], Training Loss: 1081626629362231.4, Validation Loss: 1084057261703168.0\n",
      "Epoch [196/1500], Training Loss: 1081626323414031.5, Validation Loss: 1084056724832256.0\n",
      "Epoch [197/1500], Training Loss: 1081626050279894.2, Validation Loss: 1084056523505664.0\n",
      "Epoch [198/1500], Training Loss: 1081625765680571.5, Validation Loss: 1084056255070208.0\n",
      "Epoch [199/1500], Training Loss: 1081625475466798.9, Validation Loss: 1084055852417024.0\n",
      "Epoch [200/1500], Training Loss: 1081625206336700.4, Validation Loss: 1084055651090432.0\n",
      "Epoch [201/1500], Training Loss: 1081624899336209.1, Validation Loss: 1084055315546112.0\n",
      "Epoch [202/1500], Training Loss: 1081624627945074.4, Validation Loss: 1084055114219520.0\n",
      "Epoch [203/1500], Training Loss: 1081624319331966.6, Validation Loss: 1084054711566336.0\n",
      "Epoch [204/1500], Training Loss: 1081624053468278.0, Validation Loss: 1084054510239744.0\n",
      "Epoch [205/1500], Training Loss: 1081623751862291.0, Validation Loss: 1084054308913152.0\n",
      "Epoch [206/1500], Training Loss: 1081623485992369.9, Validation Loss: 1084053973368832.0\n",
      "Epoch [207/1500], Training Loss: 1081623178836374.6, Validation Loss: 1084053704933376.0\n",
      "Epoch [208/1500], Training Loss: 1081622907922745.2, Validation Loss: 1084053704933376.0\n",
      "Epoch [209/1500], Training Loss: 1081622599647704.0, Validation Loss: 1084053302280192.0\n",
      "Epoch [210/1500], Training Loss: 1081622330245382.1, Validation Loss: 1084052966735872.0\n",
      "Epoch [211/1500], Training Loss: 1081622040948024.6, Validation Loss: 1084052899627008.0\n",
      "Epoch [212/1500], Training Loss: 1081621791182392.1, Validation Loss: 1084052429864960.0\n",
      "Epoch [213/1500], Training Loss: 1081621502649191.1, Validation Loss: 1084052161429504.0\n",
      "Epoch [214/1500], Training Loss: 1081621236133391.4, Validation Loss: 1084051960102912.0\n",
      "Epoch [215/1500], Training Loss: 1081620926117103.1, Validation Loss: 1084051557449728.0\n",
      "Epoch [216/1500], Training Loss: 1081620656826241.6, Validation Loss: 1084051221905408.0\n",
      "Epoch [217/1500], Training Loss: 1081620353478992.5, Validation Loss: 1084050685034496.0\n",
      "Epoch [218/1500], Training Loss: 1081620082760795.2, Validation Loss: 1084050282381312.0\n",
      "Epoch [219/1500], Training Loss: 1081619779805265.6, Validation Loss: 1084049812619264.0\n",
      "Epoch [220/1500], Training Loss: 1081619507833142.4, Validation Loss: 1084049611292672.0\n",
      "Epoch [221/1500], Training Loss: 1081619199504497.0, Validation Loss: 1084049409966080.0\n",
      "Epoch [222/1500], Training Loss: 1081618923543384.4, Validation Loss: 1084049074421760.0\n",
      "Epoch [223/1500], Training Loss: 1081618638651740.5, Validation Loss: 1084048873095168.0\n",
      "Epoch [224/1500], Training Loss: 1081618352805184.0, Validation Loss: 1084048671768576.0\n",
      "Epoch [225/1500], Training Loss: 1081618083077258.2, Validation Loss: 1084048269115392.0\n",
      "Epoch [226/1500], Training Loss: 1081617776614487.1, Validation Loss: 1084047866462208.0\n",
      "Epoch [227/1500], Training Loss: 1081617504784233.8, Validation Loss: 1084047463809024.0\n",
      "Epoch [228/1500], Training Loss: 1081617198026277.6, Validation Loss: 1084047061155840.0\n",
      "Epoch [229/1500], Training Loss: 1081616931378845.4, Validation Loss: 1084046859829248.0\n",
      "Epoch [230/1500], Training Loss: 1081616625454128.5, Validation Loss: 1084046591393792.0\n",
      "Epoch [231/1500], Training Loss: 1081616362287714.4, Validation Loss: 1084046390067200.0\n",
      "Epoch [232/1500], Training Loss: 1081616053850528.2, Validation Loss: 1084045987414016.0\n",
      "Epoch [233/1500], Training Loss: 1081615786744610.0, Validation Loss: 1084045718978560.0\n",
      "Epoch [234/1500], Training Loss: 1081615474758843.9, Validation Loss: 1084045182107648.0\n",
      "Epoch [235/1500], Training Loss: 1081615207757079.1, Validation Loss: 1084044980781056.0\n",
      "Epoch [236/1500], Training Loss: 1081614914842007.2, Validation Loss: 1084044913672192.0\n",
      "Epoch [237/1500], Training Loss: 1081614665935880.4, Validation Loss: 1084044443910144.0\n",
      "Epoch [238/1500], Training Loss: 1081614379212512.0, Validation Loss: 1084044175474688.0\n",
      "Epoch [239/1500], Training Loss: 1081614114210263.0, Validation Loss: 1084043839930368.0\n",
      "Epoch [240/1500], Training Loss: 1081613805406397.1, Validation Loss: 1084043638603776.0\n",
      "Epoch [241/1500], Training Loss: 1081613536969905.1, Validation Loss: 1084043235950592.0\n",
      "Epoch [242/1500], Training Loss: 1081613232150221.0, Validation Loss: 1084043034624000.0\n",
      "Epoch [243/1500], Training Loss: 1081612963297162.6, Validation Loss: 1084042833297408.0\n",
      "Epoch [244/1500], Training Loss: 1081612659144595.6, Validation Loss: 1084042430644224.0\n",
      "Epoch [245/1500], Training Loss: 1081612387867119.9, Validation Loss: 1084042229317632.0\n",
      "Epoch [246/1500], Training Loss: 1081612077629038.6, Validation Loss: 1084041893773312.0\n",
      "Epoch [247/1500], Training Loss: 1081611799994339.2, Validation Loss: 1084041625337856.0\n",
      "Epoch [248/1500], Training Loss: 1081611511027214.6, Validation Loss: 1084041424011264.0\n",
      "Epoch [249/1500], Training Loss: 1081611227911697.8, Validation Loss: 1084041088466944.0\n",
      "Epoch [250/1500], Training Loss: 1081610958438301.6, Validation Loss: 1084040820031488.0\n",
      "Epoch [251/1500], Training Loss: 1081610655172463.8, Validation Loss: 1084040618704896.0\n",
      "Epoch [252/1500], Training Loss: 1081610382144179.2, Validation Loss: 1084040283160576.0\n",
      "Epoch [253/1500], Training Loss: 1081610074864890.9, Validation Loss: 1084039947616256.0\n",
      "Epoch [254/1500], Training Loss: 1081609808702666.8, Validation Loss: 1084039746289664.0\n",
      "Epoch [255/1500], Training Loss: 1081609504613757.6, Validation Loss: 1084039544963072.0\n",
      "Epoch [256/1500], Training Loss: 1081609239050539.9, Validation Loss: 1084039343636480.0\n",
      "Epoch [257/1500], Training Loss: 1081608933382473.4, Validation Loss: 1084038940983296.0\n",
      "Epoch [258/1500], Training Loss: 1081608662695591.1, Validation Loss: 1084038873874432.0\n",
      "Epoch [259/1500], Training Loss: 1081608352176601.8, Validation Loss: 1084038404112384.0\n",
      "Epoch [260/1500], Training Loss: 1081608081092490.6, Validation Loss: 1084038202785792.0\n",
      "Epoch [261/1500], Training Loss: 1081607786432764.8, Validation Loss: 1084038001459200.0\n",
      "Epoch [262/1500], Training Loss: 1081607537468143.4, Validation Loss: 1084037800132608.0\n",
      "Epoch [263/1500], Training Loss: 1081607253580643.1, Validation Loss: 1084037531697152.0\n",
      "Epoch [264/1500], Training Loss: 1081606991939142.1, Validation Loss: 1084037330370560.0\n",
      "Epoch [265/1500], Training Loss: 1081606682885272.8, Validation Loss: 1084036994826240.0\n",
      "Epoch [266/1500], Training Loss: 1081606413912974.4, Validation Loss: 1084036726390784.0\n",
      "Epoch [267/1500], Training Loss: 1081606107291769.4, Validation Loss: 1084036256628736.0\n",
      "Epoch [268/1500], Training Loss: 1081605838713243.2, Validation Loss: 1084036055302144.0\n",
      "Epoch [269/1500], Training Loss: 1081605533941659.6, Validation Loss: 1084035853975552.0\n",
      "Epoch [270/1500], Training Loss: 1081605263121212.5, Validation Loss: 1084035652648960.0\n",
      "Epoch [271/1500], Training Loss: 1081604956907221.4, Validation Loss: 1084035384213504.0\n",
      "Epoch [272/1500], Training Loss: 1081604679190732.9, Validation Loss: 1084035048669184.0\n",
      "Epoch [273/1500], Training Loss: 1081604388433596.5, Validation Loss: 1084034780233728.0\n",
      "Epoch [274/1500], Training Loss: 1081604107845103.9, Validation Loss: 1084034578907136.0\n",
      "Epoch [275/1500], Training Loss: 1081603836245337.6, Validation Loss: 1084034109145088.0\n",
      "Epoch [276/1500], Training Loss: 1081603533467344.5, Validation Loss: 1084033907818496.0\n",
      "Epoch [277/1500], Training Loss: 1081603259949134.2, Validation Loss: 1084033706491904.0\n",
      "Epoch [278/1500], Training Loss: 1081602952345386.4, Validation Loss: 1084033438056448.0\n",
      "Epoch [279/1500], Training Loss: 1081602684362604.9, Validation Loss: 1084033102512128.0\n",
      "Epoch [280/1500], Training Loss: 1081602381433780.8, Validation Loss: 1084032901185536.0\n",
      "Epoch [281/1500], Training Loss: 1081602114340775.4, Validation Loss: 1084032632750080.0\n",
      "Epoch [282/1500], Training Loss: 1081601810814237.4, Validation Loss: 1084032297205760.0\n",
      "Epoch [283/1500], Training Loss: 1081601541927773.0, Validation Loss: 1084031961661440.0\n",
      "Epoch [284/1500], Training Loss: 1081601231012439.5, Validation Loss: 1084031760334848.0\n",
      "Epoch [285/1500], Training Loss: 1081600959131429.9, Validation Loss: 1084031491899392.0\n",
      "Epoch [286/1500], Training Loss: 1081600663077825.2, Validation Loss: 1084031290572800.0\n",
      "Epoch [287/1500], Training Loss: 1081600412484819.5, Validation Loss: 1084030955028480.0\n",
      "Epoch [288/1500], Training Loss: 1081600127470553.4, Validation Loss: 1084030686593024.0\n",
      "Epoch [289/1500], Training Loss: 1081599869257208.4, Validation Loss: 1084030149722112.0\n",
      "Epoch [290/1500], Training Loss: 1081599559734146.0, Validation Loss: 1084029814177792.0\n",
      "Epoch [291/1500], Training Loss: 1081599289973638.5, Validation Loss: 1084029411524608.0\n",
      "Epoch [292/1500], Training Loss: 1081598985142511.9, Validation Loss: 1084029008871424.0\n",
      "Epoch [293/1500], Training Loss: 1081598714575701.6, Validation Loss: 1084028740435968.0\n",
      "Epoch [294/1500], Training Loss: 1081598412699897.9, Validation Loss: 1084028539109376.0\n",
      "Epoch [295/1500], Training Loss: 1081598138975003.0, Validation Loss: 1084028337782784.0\n",
      "Epoch [296/1500], Training Loss: 1081597833424774.0, Validation Loss: 1084028069347328.0\n",
      "Epoch [297/1500], Training Loss: 1081597555051664.8, Validation Loss: 1084027868020736.0\n",
      "Epoch [298/1500], Training Loss: 1081597262261614.8, Validation Loss: 1084027599585280.0\n",
      "Epoch [299/1500], Training Loss: 1081596983970571.2, Validation Loss: 1084027196932096.0\n",
      "Epoch [300/1500], Training Loss: 1081596710166301.2, Validation Loss: 1084026995605504.0\n",
      "Epoch [301/1500], Training Loss: 1081596411187261.8, Validation Loss: 1084026794278912.0\n",
      "Epoch [302/1500], Training Loss: 1081596136992937.2, Validation Loss: 1084026391625728.0\n",
      "Epoch [303/1500], Training Loss: 1081595828948699.6, Validation Loss: 1084026257408000.0\n",
      "Epoch [304/1500], Training Loss: 1081595559737382.1, Validation Loss: 1084026056081408.0\n",
      "Epoch [305/1500], Training Loss: 1081595256466125.5, Validation Loss: 1084025854754816.0\n",
      "Epoch [306/1500], Training Loss: 1081594991679323.6, Validation Loss: 1084025452101632.0\n",
      "Epoch [307/1500], Training Loss: 1081594688403256.2, Validation Loss: 1084024915230720.0\n",
      "Epoch [308/1500], Training Loss: 1081594419422866.9, Validation Loss: 1084024713904128.0\n",
      "Epoch [309/1500], Training Loss: 1081594108762498.8, Validation Loss: 1084024512577536.0\n",
      "Epoch [310/1500], Training Loss: 1081593836840982.6, Validation Loss: 1084024109924352.0\n",
      "Epoch [311/1500], Training Loss: 1081593538983602.8, Validation Loss: 1084023707271168.0\n",
      "Epoch [312/1500], Training Loss: 1081593287114969.4, Validation Loss: 1084023505944576.0\n",
      "Epoch [313/1500], Training Loss: 1081592999950450.1, Validation Loss: 1084023170400256.0\n",
      "Epoch [314/1500], Training Loss: 1081592745363908.0, Validation Loss: 1084022969073664.0\n",
      "Epoch [315/1500], Training Loss: 1081592438519850.8, Validation Loss: 1084022767747072.0\n",
      "Epoch [316/1500], Training Loss: 1081592168379685.5, Validation Loss: 1084022365093888.0\n",
      "Epoch [317/1500], Training Loss: 1081591861393978.5, Validation Loss: 1084022297985024.0\n",
      "Epoch [318/1500], Training Loss: 1081591592028797.6, Validation Loss: 1084021828222976.0\n",
      "Epoch [319/1500], Training Loss: 1081591289733321.8, Validation Loss: 1084021626896384.0\n",
      "Epoch [320/1500], Training Loss: 1081591015225197.4, Validation Loss: 1084021559787520.0\n",
      "Epoch [321/1500], Training Loss: 1081590713008825.6, Validation Loss: 1084020955807744.0\n",
      "Epoch [322/1500], Training Loss: 1081590434470500.1, Validation Loss: 1084020754481152.0\n",
      "Epoch [323/1500], Training Loss: 1081590137958996.2, Validation Loss: 1084020418936832.0\n",
      "Epoch [324/1500], Training Loss: 1081589861949589.8, Validation Loss: 1084020150501376.0\n",
      "Epoch [325/1500], Training Loss: 1081589586953715.0, Validation Loss: 1084019882065920.0\n",
      "Epoch [326/1500], Training Loss: 1081589287640287.2, Validation Loss: 1084019680739328.0\n",
      "Epoch [327/1500], Training Loss: 1081589013508291.6, Validation Loss: 1084019479412736.0\n",
      "Epoch [328/1500], Training Loss: 1081588707811349.6, Validation Loss: 1084019076759552.0\n",
      "Epoch [329/1500], Training Loss: 1081588438100552.4, Validation Loss: 1084018674106368.0\n",
      "Epoch [330/1500], Training Loss: 1081588133923551.4, Validation Loss: 1084018405670912.0\n",
      "Epoch [331/1500], Training Loss: 1081587868653934.8, Validation Loss: 1084018070126592.0\n",
      "Epoch [332/1500], Training Loss: 1081587566608085.4, Validation Loss: 1084017868800000.0\n",
      "Epoch [333/1500], Training Loss: 1081587299614268.5, Validation Loss: 1084017667473408.0\n",
      "Epoch [334/1500], Training Loss: 1081586989767677.8, Validation Loss: 1084017331929088.0\n",
      "Epoch [335/1500], Training Loss: 1081586719976258.9, Validation Loss: 1084016862167040.0\n",
      "Epoch [336/1500], Training Loss: 1081586431836923.9, Validation Loss: 1084016660840448.0\n",
      "Epoch [337/1500], Training Loss: 1081586189851533.1, Validation Loss: 1084016459513856.0\n",
      "Epoch [338/1500], Training Loss: 1081585905375631.1, Validation Loss: 1084016258187264.0\n",
      "Epoch [339/1500], Training Loss: 1081585635002771.1, Validation Loss: 1084015922642944.0\n",
      "Epoch [340/1500], Training Loss: 1081585328925752.0, Validation Loss: 1084015721316352.0\n",
      "Epoch [341/1500], Training Loss: 1081585062628407.1, Validation Loss: 1084015519989760.0\n",
      "Epoch [342/1500], Training Loss: 1081584762897460.1, Validation Loss: 1084015184445440.0\n",
      "Epoch [343/1500], Training Loss: 1081584491776514.8, Validation Loss: 1084014916009984.0\n",
      "Epoch [344/1500], Training Loss: 1081584184861331.4, Validation Loss: 1084014714683392.0\n",
      "Epoch [345/1500], Training Loss: 1081583911590797.4, Validation Loss: 1084014513356800.0\n",
      "Epoch [346/1500], Training Loss: 1081583633492860.4, Validation Loss: 1084014177812480.0\n",
      "Epoch [347/1500], Training Loss: 1081583345956716.9, Validation Loss: 1084013976485888.0\n",
      "Epoch [348/1500], Training Loss: 1081583075594728.4, Validation Loss: 1084013640941568.0\n",
      "Epoch [349/1500], Training Loss: 1081582769737180.4, Validation Loss: 1084013439614976.0\n",
      "Epoch [350/1500], Training Loss: 1081582502849217.9, Validation Loss: 1084013171179520.0\n",
      "Epoch [351/1500], Training Loss: 1081582202587175.1, Validation Loss: 1084012768526336.0\n",
      "Epoch [352/1500], Training Loss: 1081581939309939.4, Validation Loss: 1084012432982016.0\n",
      "Epoch [353/1500], Training Loss: 1081581634740642.9, Validation Loss: 1084012030328832.0\n",
      "Epoch [354/1500], Training Loss: 1081581367531846.9, Validation Loss: 1084011694784512.0\n",
      "Epoch [355/1500], Training Loss: 1081581060186784.5, Validation Loss: 1084011292131328.0\n",
      "Epoch [356/1500], Training Loss: 1081580799186803.1, Validation Loss: 1084011023695872.0\n",
      "Epoch [357/1500], Training Loss: 1081580516063762.8, Validation Loss: 1084010688151552.0\n",
      "Epoch [358/1500], Training Loss: 1081580275499619.6, Validation Loss: 1084010419716096.0\n",
      "Epoch [359/1500], Training Loss: 1081579976431672.8, Validation Loss: 1084010218389504.0\n",
      "Epoch [360/1500], Training Loss: 1081579705709605.6, Validation Loss: 1084009681518592.0\n",
      "Epoch [361/1500], Training Loss: 1081579402854217.5, Validation Loss: 1084009345974272.0\n",
      "Epoch [362/1500], Training Loss: 1081579137369720.6, Validation Loss: 1084009144647680.0\n",
      "Epoch [363/1500], Training Loss: 1081578834976356.4, Validation Loss: 1084008943321088.0\n",
      "Epoch [364/1500], Training Loss: 1081578562801565.9, Validation Loss: 1084008741994496.0\n",
      "Epoch [365/1500], Training Loss: 1081578253827061.9, Validation Loss: 1084008674885632.0\n",
      "Epoch [366/1500], Training Loss: 1081577984342958.1, Validation Loss: 1084008473559040.0\n",
      "Epoch [367/1500], Training Loss: 1081577708005841.8, Validation Loss: 1084008272232448.0\n",
      "Epoch [368/1500], Training Loss: 1081577420974827.4, Validation Loss: 1084007936688128.0\n",
      "Epoch [369/1500], Training Loss: 1081577147806796.4, Validation Loss: 1084007735361536.0\n",
      "Epoch [370/1500], Training Loss: 1081576839263875.4, Validation Loss: 1084007332708352.0\n",
      "Epoch [371/1500], Training Loss: 1081576574810637.0, Validation Loss: 1084007198490624.0\n",
      "Epoch [372/1500], Training Loss: 1081576275147962.0, Validation Loss: 1084006795837440.0\n",
      "Epoch [373/1500], Training Loss: 1081576011344843.9, Validation Loss: 1084006728728576.0\n",
      "Epoch [374/1500], Training Loss: 1081575711009125.6, Validation Loss: 1084006527401984.0\n",
      "Epoch [375/1500], Training Loss: 1081575438423692.4, Validation Loss: 1084006191857664.0\n",
      "Epoch [376/1500], Training Loss: 1081575132788142.5, Validation Loss: 1084005722095616.0\n",
      "Epoch [377/1500], Training Loss: 1081574882524036.8, Validation Loss: 1084005386551296.0\n",
      "Epoch [378/1500], Training Loss: 1081574602445575.0, Validation Loss: 1084005051006976.0\n",
      "Epoch [379/1500], Training Loss: 1081574353262180.2, Validation Loss: 1084004782571520.0\n",
      "Epoch [380/1500], Training Loss: 1081574048264551.4, Validation Loss: 1084004447027200.0\n",
      "Epoch [381/1500], Training Loss: 1081573778215808.4, Validation Loss: 1084004044374016.0\n",
      "Epoch [382/1500], Training Loss: 1081573476623285.5, Validation Loss: 1084003641720832.0\n",
      "Epoch [383/1500], Training Loss: 1081573210688487.9, Validation Loss: 1084003440394240.0\n",
      "Epoch [384/1500], Training Loss: 1081572906296444.5, Validation Loss: 1084003239067648.0\n",
      "Epoch [385/1500], Training Loss: 1081572629635004.4, Validation Loss: 1084002836414464.0\n",
      "Epoch [386/1500], Training Loss: 1081572326127636.1, Validation Loss: 1084002433761280.0\n",
      "Epoch [387/1500], Training Loss: 1081572056123716.9, Validation Loss: 1084002098216960.0\n",
      "Epoch [388/1500], Training Loss: 1081571788288823.0, Validation Loss: 1084001896890368.0\n",
      "Epoch [389/1500], Training Loss: 1081571488982456.6, Validation Loss: 1084001628454912.0\n",
      "Epoch [390/1500], Training Loss: 1081571217145408.8, Validation Loss: 1084001360019456.0\n",
      "Epoch [391/1500], Training Loss: 1081570911449833.0, Validation Loss: 1084001158692864.0\n",
      "Epoch [392/1500], Training Loss: 1081570646302144.0, Validation Loss: 1084000890257408.0\n",
      "Epoch [393/1500], Training Loss: 1081570352011453.6, Validation Loss: 1084000554713088.0\n",
      "Epoch [394/1500], Training Loss: 1081570085074768.9, Validation Loss: 1084000353386496.0\n",
      "Epoch [395/1500], Training Loss: 1081569775488722.5, Validation Loss: 1084000084951040.0\n",
      "Epoch [396/1500], Training Loss: 1081569505598034.6, Validation Loss: 1083999682297856.0\n",
      "Epoch [397/1500], Training Loss: 1081569212950858.4, Validation Loss: 1083999346753536.0\n",
      "Epoch [398/1500], Training Loss: 1081568965428182.4, Validation Loss: 1083999011209216.0\n",
      "Epoch [399/1500], Training Loss: 1081568684735385.1, Validation Loss: 1083998742773760.0\n",
      "Epoch [400/1500], Training Loss: 1081568422649233.6, Validation Loss: 1083998541447168.0\n",
      "Epoch [401/1500], Training Loss: 1081568119283810.1, Validation Loss: 1083998138793984.0\n",
      "Epoch [402/1500], Training Loss: 1081567849281514.0, Validation Loss: 1083997937467392.0\n",
      "Epoch [403/1500], Training Loss: 1081567550744865.8, Validation Loss: 1083997534814208.0\n",
      "Epoch [404/1500], Training Loss: 1081567279988245.5, Validation Loss: 1083997266378752.0\n",
      "Epoch [405/1500], Training Loss: 1081566975331844.2, Validation Loss: 1083996796616704.0\n",
      "Epoch [406/1500], Training Loss: 1081566699113721.0, Validation Loss: 1083996662398976.0\n",
      "Epoch [407/1500], Training Loss: 1081566413045132.5, Validation Loss: 1083996393963520.0\n",
      "Epoch [408/1500], Training Loss: 1081566130738692.5, Validation Loss: 1083996192636928.0\n",
      "Epoch [409/1500], Training Loss: 1081565865224215.6, Validation Loss: 1083995789983744.0\n",
      "Epoch [410/1500], Training Loss: 1081565561560847.4, Validation Loss: 1083995588657152.0\n",
      "Epoch [411/1500], Training Loss: 1081565288124443.4, Validation Loss: 1083995253112832.0\n",
      "Epoch [412/1500], Training Loss: 1081564986217454.8, Validation Loss: 1083995051786240.0\n",
      "Epoch [413/1500], Training Loss: 1081564723329877.4, Validation Loss: 1083994850459648.0\n",
      "Epoch [414/1500], Training Loss: 1081564424500169.4, Validation Loss: 1083994649133056.0\n",
      "Epoch [415/1500], Training Loss: 1081564154860822.8, Validation Loss: 1083994313588736.0\n",
      "Epoch [416/1500], Training Loss: 1081563845000353.4, Validation Loss: 1083994246479872.0\n",
      "Epoch [417/1500], Training Loss: 1081563581484043.2, Validation Loss: 1083994045153280.0\n",
      "Epoch [418/1500], Training Loss: 1081563296762335.5, Validation Loss: 1083993843826688.0\n",
      "Epoch [419/1500], Training Loss: 1081563053102641.1, Validation Loss: 1083993306955776.0\n",
      "Epoch [420/1500], Training Loss: 1081562762963180.9, Validation Loss: 1083993105629184.0\n",
      "Epoch [421/1500], Training Loss: 1081562494327283.6, Validation Loss: 1083992702976000.0\n",
      "Epoch [422/1500], Training Loss: 1081562190256390.2, Validation Loss: 1083992501649408.0\n",
      "Epoch [423/1500], Training Loss: 1081561920897823.2, Validation Loss: 1083992300322816.0\n",
      "Epoch [424/1500], Training Loss: 1081561619499048.9, Validation Loss: 1083991964778496.0\n",
      "Epoch [425/1500], Training Loss: 1081561352839632.8, Validation Loss: 1083991763451904.0\n",
      "Epoch [426/1500], Training Loss: 1081561046746403.2, Validation Loss: 1083991562125312.0\n",
      "Epoch [427/1500], Training Loss: 1081560774841398.6, Validation Loss: 1083991025254400.0\n",
      "Epoch [428/1500], Training Loss: 1081560494495328.6, Validation Loss: 1083990958145536.0\n",
      "Epoch [429/1500], Training Loss: 1081560206854483.0, Validation Loss: 1083990756818944.0\n",
      "Epoch [430/1500], Training Loss: 1081559938935491.6, Validation Loss: 1083990421274624.0\n",
      "Epoch [431/1500], Training Loss: 1081559629162252.0, Validation Loss: 1083990219948032.0\n",
      "Epoch [432/1500], Training Loss: 1081559358870227.9, Validation Loss: 1083989750185984.0\n",
      "Epoch [433/1500], Training Loss: 1081559059499067.8, Validation Loss: 1083989548859392.0\n",
      "Epoch [434/1500], Training Loss: 1081558799791094.2, Validation Loss: 1083989213315072.0\n",
      "Epoch [435/1500], Training Loss: 1081558497001550.2, Validation Loss: 1083989011988480.0\n",
      "Epoch [436/1500], Training Loss: 1081558224041328.8, Validation Loss: 1083988810661888.0\n",
      "Epoch [437/1500], Training Loss: 1081557916570490.0, Validation Loss: 1083988609335296.0\n",
      "Epoch [438/1500], Training Loss: 1081557662996097.0, Validation Loss: 1083988475117568.0\n",
      "Epoch [439/1500], Training Loss: 1081557380968395.1, Validation Loss: 1083988072464384.0\n",
      "Epoch [440/1500], Training Loss: 1081557136447412.2, Validation Loss: 1083987871137792.0\n",
      "Epoch [441/1500], Training Loss: 1081556833658721.5, Validation Loss: 1083987669811200.0\n",
      "Epoch [442/1500], Training Loss: 1081556566380526.6, Validation Loss: 1083987401375744.0\n",
      "Epoch [443/1500], Training Loss: 1081556263595617.1, Validation Loss: 1083987065831424.0\n",
      "Epoch [444/1500], Training Loss: 1081555995063808.4, Validation Loss: 1083986730287104.0\n",
      "Epoch [445/1500], Training Loss: 1081555691816819.2, Validation Loss: 1083986528960512.0\n",
      "Epoch [446/1500], Training Loss: 1081555420353760.9, Validation Loss: 1083986260525056.0\n",
      "Epoch [447/1500], Training Loss: 1081555114732722.4, Validation Loss: 1083986126307328.0\n",
      "Epoch [448/1500], Training Loss: 1081554846874876.1, Validation Loss: 1083985857871872.0\n",
      "Epoch [449/1500], Training Loss: 1081554568355651.4, Validation Loss: 1083985522327552.0\n",
      "Epoch [450/1500], Training Loss: 1081554276114079.0, Validation Loss: 1083985253892096.0\n",
      "Epoch [451/1500], Training Loss: 1081554007272558.1, Validation Loss: 1083984784130048.0\n",
      "Epoch [452/1500], Training Loss: 1081553701692779.6, Validation Loss: 1083984381476864.0\n",
      "Epoch [453/1500], Training Loss: 1081553434806702.8, Validation Loss: 1083983911714816.0\n",
      "Epoch [454/1500], Training Loss: 1081553137856935.0, Validation Loss: 1083983509061632.0\n",
      "Epoch [455/1500], Training Loss: 1081552874158178.8, Validation Loss: 1083982972190720.0\n",
      "Epoch [456/1500], Training Loss: 1081552568997994.8, Validation Loss: 1083982770864128.0\n",
      "Epoch [457/1500], Training Loss: 1081552294993027.9, Validation Loss: 1083982368210944.0\n",
      "Epoch [458/1500], Training Loss: 1081551995403116.6, Validation Loss: 1083982166884352.0\n",
      "Epoch [459/1500], Training Loss: 1081551748533647.8, Validation Loss: 1083981764231168.0\n",
      "Epoch [460/1500], Training Loss: 1081551467667043.1, Validation Loss: 1083981562904576.0\n",
      "Epoch [461/1500], Training Loss: 1081551212445437.8, Validation Loss: 1083981227360256.0\n",
      "Epoch [462/1500], Training Loss: 1081550907416232.0, Validation Loss: 1083981026033664.0\n",
      "Epoch [463/1500], Training Loss: 1081550639003641.5, Validation Loss: 1083980824707072.0\n",
      "Epoch [464/1500], Training Loss: 1081550337577457.4, Validation Loss: 1083980623380480.0\n",
      "Epoch [465/1500], Training Loss: 1081550068699012.6, Validation Loss: 1083980489162752.0\n",
      "Epoch [466/1500], Training Loss: 1081549765022933.2, Validation Loss: 1083980287836160.0\n",
      "Epoch [467/1500], Training Loss: 1081549491250630.1, Validation Loss: 1083980086509568.0\n",
      "Epoch [468/1500], Training Loss: 1081549195827775.9, Validation Loss: 1083979683856384.0\n",
      "Epoch [469/1500], Training Loss: 1081548919780606.9, Validation Loss: 1083979415420928.0\n",
      "Epoch [470/1500], Training Loss: 1081548649205758.8, Validation Loss: 1083979079876608.0\n",
      "Epoch [471/1500], Training Loss: 1081548345941607.4, Validation Loss: 1083978744332288.0\n",
      "Epoch [472/1500], Training Loss: 1081548078795326.2, Validation Loss: 1083978475896832.0\n",
      "Epoch [473/1500], Training Loss: 1081547773642802.9, Validation Loss: 1083978140352512.0\n",
      "Epoch [474/1500], Training Loss: 1081547507675378.2, Validation Loss: 1083977871917056.0\n",
      "Epoch [475/1500], Training Loss: 1081547211541331.4, Validation Loss: 1083977469263872.0\n",
      "Epoch [476/1500], Training Loss: 1081546946142370.1, Validation Loss: 1083977267937280.0\n",
      "Epoch [477/1500], Training Loss: 1081546635251685.9, Validation Loss: 1083976932392960.0\n",
      "Epoch [478/1500], Training Loss: 1081546366447124.2, Validation Loss: 1083976529739776.0\n",
      "Epoch [479/1500], Training Loss: 1081546076816563.9, Validation Loss: 1083976194195456.0\n",
      "Epoch [480/1500], Training Loss: 1081545832401037.6, Validation Loss: 1083976127086592.0\n",
      "Epoch [481/1500], Training Loss: 1081545551086011.1, Validation Loss: 1083975925760000.0\n",
      "Epoch [482/1500], Training Loss: 1081545284312087.1, Validation Loss: 1083975724433408.0\n",
      "Epoch [483/1500], Training Loss: 1081544977101557.8, Validation Loss: 1083975187562496.0\n",
      "Epoch [484/1500], Training Loss: 1081544711223213.4, Validation Loss: 1083974986235904.0\n",
      "Epoch [485/1500], Training Loss: 1081544410253863.6, Validation Loss: 1083974650691584.0\n",
      "Epoch [486/1500], Training Loss: 1081544138609190.8, Validation Loss: 1083974449364992.0\n",
      "Epoch [487/1500], Training Loss: 1081543833898222.2, Validation Loss: 1083974248038400.0\n",
      "Epoch [488/1500], Training Loss: 1081543560544276.6, Validation Loss: 1083974046711808.0\n",
      "Epoch [489/1500], Training Loss: 1081543279469306.8, Validation Loss: 1083973644058624.0\n",
      "Epoch [490/1500], Training Loss: 1081542994711125.2, Validation Loss: 1083973375623168.0\n",
      "Epoch [491/1500], Training Loss: 1081542724318170.4, Validation Loss: 1083972972969984.0\n",
      "Epoch [492/1500], Training Loss: 1081542416606046.4, Validation Loss: 1083972838752256.0\n",
      "Epoch [493/1500], Training Loss: 1081542148813293.6, Validation Loss: 1083972503207936.0\n",
      "Epoch [494/1500], Training Loss: 1081541845699180.9, Validation Loss: 1083972234772480.0\n",
      "Epoch [495/1500], Training Loss: 1081541585164324.1, Validation Loss: 1083972100554752.0\n",
      "Epoch [496/1500], Training Loss: 1081541283997079.1, Validation Loss: 1083971832119296.0\n",
      "Epoch [497/1500], Training Loss: 1081541017108339.5, Validation Loss: 1083971630792704.0\n",
      "Epoch [498/1500], Training Loss: 1081540707276714.9, Validation Loss: 1083971429466112.0\n",
      "Epoch [499/1500], Training Loss: 1081540443631513.5, Validation Loss: 1083971228139520.0\n",
      "Epoch [500/1500], Training Loss: 1081540160292313.5, Validation Loss: 1083970892595200.0\n",
      "Epoch [501/1500], Training Loss: 1081539918067100.0, Validation Loss: 1083970489942016.0\n",
      "Epoch [502/1500], Training Loss: 1081539625182656.4, Validation Loss: 1083970154397696.0\n",
      "Epoch [503/1500], Training Loss: 1081539353628380.1, Validation Loss: 1083969953071104.0\n",
      "Epoch [504/1500], Training Loss: 1081539052538032.6, Validation Loss: 1083969751744512.0\n",
      "Epoch [505/1500], Training Loss: 1081538784173133.8, Validation Loss: 1083969550417920.0\n",
      "Epoch [506/1500], Training Loss: 1081538483783260.2, Validation Loss: 1083969147764736.0\n",
      "Epoch [507/1500], Training Loss: 1081538211194050.6, Validation Loss: 1083968946438144.0\n",
      "Epoch [508/1500], Training Loss: 1081537902047418.8, Validation Loss: 1083968543784960.0\n",
      "Epoch [509/1500], Training Loss: 1081537630621954.1, Validation Loss: 1083968208240640.0\n",
      "Epoch [510/1500], Training Loss: 1081537357149996.4, Validation Loss: 1083967805587456.0\n",
      "Epoch [511/1500], Training Loss: 1081537066989681.5, Validation Loss: 1083967537152000.0\n",
      "Epoch [512/1500], Training Loss: 1081536795714996.4, Validation Loss: 1083967402934272.0\n",
      "Epoch [513/1500], Training Loss: 1081536489931302.8, Validation Loss: 1083967201607680.0\n",
      "Epoch [514/1500], Training Loss: 1081536223068316.4, Validation Loss: 1083966798954496.0\n",
      "Epoch [515/1500], Training Loss: 1081535922427960.0, Validation Loss: 1083966463410176.0\n",
      "Epoch [516/1500], Training Loss: 1081535660610006.9, Validation Loss: 1083966194974720.0\n",
      "Epoch [517/1500], Training Loss: 1081535353973771.8, Validation Loss: 1083965859430400.0\n",
      "Epoch [518/1500], Training Loss: 1081535083081214.9, Validation Loss: 1083965658103808.0\n",
      "Epoch [519/1500], Training Loss: 1081534779160178.5, Validation Loss: 1083965255450624.0\n",
      "Epoch [520/1500], Training Loss: 1081534527874384.8, Validation Loss: 1083965188341760.0\n",
      "Epoch [521/1500], Training Loss: 1081534245935916.8, Validation Loss: 1083964651470848.0\n",
      "Epoch [522/1500], Training Loss: 1081534001742305.6, Validation Loss: 1083964450144256.0\n",
      "Epoch [523/1500], Training Loss: 1081533697025797.4, Validation Loss: 1083964114599936.0\n",
      "Epoch [524/1500], Training Loss: 1081533426940451.6, Validation Loss: 1083963846164480.0\n",
      "Epoch [525/1500], Training Loss: 1081533122758128.1, Validation Loss: 1083963644837888.0\n",
      "Epoch [526/1500], Training Loss: 1081532857357571.2, Validation Loss: 1083963510620160.0\n",
      "Epoch [527/1500], Training Loss: 1081532554376800.6, Validation Loss: 1083963309293568.0\n",
      "Epoch [528/1500], Training Loss: 1081532280806937.2, Validation Loss: 1083963040858112.0\n",
      "Epoch [529/1500], Training Loss: 1081531972476210.5, Validation Loss: 1083962705313792.0\n",
      "Epoch [530/1500], Training Loss: 1081531708349536.2, Validation Loss: 1083962302660608.0\n",
      "Epoch [531/1500], Training Loss: 1081531436495350.8, Validation Loss: 1083962101334016.0\n",
      "Epoch [532/1500], Training Loss: 1081531139195062.8, Validation Loss: 1083961698680832.0\n",
      "Epoch [533/1500], Training Loss: 1081530867101411.8, Validation Loss: 1083961564463104.0\n",
      "Epoch [534/1500], Training Loss: 1081530561569370.1, Validation Loss: 1083961363136512.0\n",
      "Epoch [535/1500], Training Loss: 1081530297012602.8, Validation Loss: 1083961094701056.0\n",
      "Epoch [536/1500], Training Loss: 1081530000413983.4, Validation Loss: 1083960759156736.0\n",
      "Epoch [537/1500], Training Loss: 1081529730599494.5, Validation Loss: 1083959953850368.0\n",
      "Epoch [538/1500], Training Loss: 1081529425971347.8, Validation Loss: 1083959416979456.0\n",
      "Epoch [539/1500], Training Loss: 1081529155987832.4, Validation Loss: 1083958947217408.0\n",
      "Epoch [540/1500], Training Loss: 1081528861105396.5, Validation Loss: 1083958745890816.0\n",
      "Epoch [541/1500], Training Loss: 1081528613758628.6, Validation Loss: 1083958544564224.0\n",
      "Epoch [542/1500], Training Loss: 1081528332650500.4, Validation Loss: 1083958074802176.0\n",
      "Epoch [543/1500], Training Loss: 1081528073454647.9, Validation Loss: 1083957873475584.0\n",
      "Epoch [544/1500], Training Loss: 1081527766412336.9, Validation Loss: 1083957672148992.0\n",
      "Epoch [545/1500], Training Loss: 1081527497529122.2, Validation Loss: 1083957470822400.0\n",
      "Epoch [546/1500], Training Loss: 1081527198118455.8, Validation Loss: 1083957001060352.0\n",
      "Epoch [547/1500], Training Loss: 1081526930847753.1, Validation Loss: 1083956799733760.0\n",
      "Epoch [548/1500], Training Loss: 1081526626430451.0, Validation Loss: 1083956598407168.0\n",
      "Epoch [549/1500], Training Loss: 1081526348726787.4, Validation Loss: 1083956329971712.0\n",
      "Epoch [550/1500], Training Loss: 1081526060201438.1, Validation Loss: 1083956262862848.0\n",
      "Epoch [551/1500], Training Loss: 1081525779168942.5, Validation Loss: 1083956128645120.0\n",
      "Epoch [552/1500], Training Loss: 1081525512214405.8, Validation Loss: 1083955725991936.0\n",
      "Epoch [553/1500], Training Loss: 1081525211942623.8, Validation Loss: 1083955658883072.0\n",
      "Epoch [554/1500], Training Loss: 1081524939425509.6, Validation Loss: 1083955457556480.0\n",
      "Epoch [555/1500], Training Loss: 1081524634971508.8, Validation Loss: 1083955122012160.0\n",
      "Epoch [556/1500], Training Loss: 1081524371535084.4, Validation Loss: 1083954920685568.0\n",
      "Epoch [557/1500], Training Loss: 1081524074455864.5, Validation Loss: 1083954652250112.0\n",
      "Epoch [558/1500], Training Loss: 1081523804689389.2, Validation Loss: 1083954316705792.0\n",
      "Epoch [559/1500], Training Loss: 1081523495985490.9, Validation Loss: 1083953981161472.0\n",
      "Epoch [560/1500], Training Loss: 1081523229799793.6, Validation Loss: 1083953712726016.0\n",
      "Epoch [561/1500], Training Loss: 1081522944659742.2, Validation Loss: 1083953377181696.0\n",
      "Epoch [562/1500], Training Loss: 1081522698856161.6, Validation Loss: 1083952907419648.0\n",
      "Epoch [563/1500], Training Loss: 1081522412087582.4, Validation Loss: 1083952504766464.0\n",
      "Epoch [564/1500], Training Loss: 1081522144111339.0, Validation Loss: 1083952236331008.0\n",
      "Epoch [565/1500], Training Loss: 1081521839243809.0, Validation Loss: 1083952035004416.0\n",
      "Epoch [566/1500], Training Loss: 1081521571369888.8, Validation Loss: 1083951766568960.0\n",
      "Epoch [567/1500], Training Loss: 1081521272294198.8, Validation Loss: 1083951565242368.0\n",
      "Epoch [568/1500], Training Loss: 1081521003203676.2, Validation Loss: 1083951229698048.0\n",
      "Epoch [569/1500], Training Loss: 1081520695860673.1, Validation Loss: 1083951028371456.0\n",
      "Epoch [570/1500], Training Loss: 1081520419509502.0, Validation Loss: 1083950759936000.0\n",
      "Epoch [571/1500], Training Loss: 1081520141890479.6, Validation Loss: 1083950424391680.0\n",
      "Epoch [572/1500], Training Loss: 1081519854172999.8, Validation Loss: 1083950021738496.0\n",
      "Epoch [573/1500], Training Loss: 1081519586244982.1, Validation Loss: 1083949686194176.0\n",
      "Epoch [574/1500], Training Loss: 1081519281440175.9, Validation Loss: 1083949484867584.0\n",
      "Epoch [575/1500], Training Loss: 1081519009608217.5, Validation Loss: 1083949082214400.0\n",
      "Epoch [576/1500], Training Loss: 1081518708820335.8, Validation Loss: 1083948612452352.0\n",
      "Epoch [577/1500], Training Loss: 1081518448897183.4, Validation Loss: 1083948478234624.0\n",
      "Epoch [578/1500], Training Loss: 1081518144276564.0, Validation Loss: 1083948142690304.0\n",
      "Epoch [579/1500], Training Loss: 1081517874553887.5, Validation Loss: 1083948075581440.0\n",
      "Epoch [580/1500], Training Loss: 1081517565828033.5, Validation Loss: 1083947672928256.0\n",
      "Epoch [581/1500], Training Loss: 1081517310499565.5, Validation Loss: 1083947337383936.0\n",
      "Epoch [582/1500], Training Loss: 1081517026162664.2, Validation Loss: 1083947270275072.0\n",
      "Epoch [583/1500], Training Loss: 1081516784352931.8, Validation Loss: 1083947068948480.0\n",
      "Epoch [584/1500], Training Loss: 1081516482520116.9, Validation Loss: 1083946867621888.0\n",
      "Epoch [585/1500], Training Loss: 1081516214820229.6, Validation Loss: 1083946532077568.0\n",
      "Epoch [586/1500], Training Loss: 1081515912547270.5, Validation Loss: 1083946330750976.0\n",
      "Epoch [587/1500], Training Loss: 1081515644296369.5, Validation Loss: 1083946129424384.0\n",
      "Epoch [588/1500], Training Loss: 1081515342487991.9, Validation Loss: 1083945928097792.0\n",
      "Epoch [589/1500], Training Loss: 1081515072860692.8, Validation Loss: 1083945793880064.0\n",
      "Epoch [590/1500], Training Loss: 1081514764595708.4, Validation Loss: 1083945525444608.0\n",
      "Epoch [591/1500], Training Loss: 1081514495507336.0, Validation Loss: 1083945189900288.0\n",
      "Epoch [592/1500], Training Loss: 1081514219507480.0, Validation Loss: 1083944921464832.0\n",
      "Epoch [593/1500], Training Loss: 1081513929631103.2, Validation Loss: 1083944720138240.0\n",
      "Epoch [594/1500], Training Loss: 1081513659226450.4, Validation Loss: 1083944518811648.0\n",
      "Epoch [595/1500], Training Loss: 1081513351138924.4, Validation Loss: 1083944183267328.0\n",
      "Epoch [596/1500], Training Loss: 1081513084167376.5, Validation Loss: 1083943847723008.0\n",
      "Epoch [597/1500], Training Loss: 1081512782595303.5, Validation Loss: 1083943780614144.0\n",
      "Epoch [598/1500], Training Loss: 1081512521449242.0, Validation Loss: 1083943579287552.0\n",
      "Epoch [599/1500], Training Loss: 1081512218884137.6, Validation Loss: 1083943377960960.0\n",
      "Epoch [600/1500], Training Loss: 1081511945624185.9, Validation Loss: 1083943042416640.0\n",
      "Epoch [601/1500], Training Loss: 1081511640532620.4, Validation Loss: 1083942773981184.0\n",
      "Epoch [602/1500], Training Loss: 1081511394212065.1, Validation Loss: 1083942438436864.0\n",
      "Epoch [603/1500], Training Loss: 1081511116238567.8, Validation Loss: 1083942035783680.0\n",
      "Epoch [604/1500], Training Loss: 1081510862807212.6, Validation Loss: 1083941700239360.0\n",
      "Epoch [605/1500], Training Loss: 1081510555014775.1, Validation Loss: 1083941431803904.0\n",
      "Epoch [606/1500], Training Loss: 1081510289049916.1, Validation Loss: 1083941029150720.0\n",
      "Epoch [607/1500], Training Loss: 1081509986446646.0, Validation Loss: 1083940894932992.0\n",
      "Epoch [608/1500], Training Loss: 1081509717777354.1, Validation Loss: 1083940626497536.0\n",
      "Epoch [609/1500], Training Loss: 1081509414158409.4, Validation Loss: 1083940223844352.0\n",
      "Epoch [610/1500], Training Loss: 1081509141619946.6, Validation Loss: 1083939754082304.0\n",
      "Epoch [611/1500], Training Loss: 1081508841859390.6, Validation Loss: 1083939485646848.0\n",
      "Epoch [612/1500], Training Loss: 1081508570110801.6, Validation Loss: 1083939284320256.0\n",
      "Epoch [613/1500], Training Loss: 1081508299616063.8, Validation Loss: 1083939082993664.0\n",
      "Epoch [614/1500], Training Loss: 1081507999222754.4, Validation Loss: 1083938680340480.0\n",
      "Epoch [615/1500], Training Loss: 1081507728590783.9, Validation Loss: 1083938546122752.0\n",
      "Epoch [616/1500], Training Loss: 1081507425095464.8, Validation Loss: 1083937942142976.0\n",
      "Epoch [617/1500], Training Loss: 1081507158934528.5, Validation Loss: 1083937539489792.0\n",
      "Epoch [618/1500], Training Loss: 1081506859085258.1, Validation Loss: 1083936935510016.0\n",
      "Epoch [619/1500], Training Loss: 1081506591494477.9, Validation Loss: 1083936734183424.0\n",
      "Epoch [620/1500], Training Loss: 1081506289293992.0, Validation Loss: 1083936532856832.0\n",
      "Epoch [621/1500], Training Loss: 1081506017128126.2, Validation Loss: 1083936331530240.0\n",
      "Epoch [622/1500], Training Loss: 1081505723659920.2, Validation Loss: 1083935861768192.0\n",
      "Epoch [623/1500], Training Loss: 1081505477367376.1, Validation Loss: 1083935392006144.0\n",
      "Epoch [624/1500], Training Loss: 1081505200075580.1, Validation Loss: 1083935257788416.0\n",
      "Epoch [625/1500], Training Loss: 1081504933296255.9, Validation Loss: 1083934989352960.0\n",
      "Epoch [626/1500], Training Loss: 1081504625731276.1, Validation Loss: 1083934788026368.0\n",
      "Epoch [627/1500], Training Loss: 1081504360043834.2, Validation Loss: 1083934586699776.0\n",
      "Epoch [628/1500], Training Loss: 1081504062155118.0, Validation Loss: 1083934251155456.0\n",
      "Epoch [629/1500], Training Loss: 1081503789828496.6, Validation Loss: 1083934049828864.0\n",
      "Epoch [630/1500], Training Loss: 1081503484306279.2, Validation Loss: 1083933848502272.0\n",
      "Epoch [631/1500], Training Loss: 1081503210177816.5, Validation Loss: 1083933647175680.0\n",
      "Epoch [632/1500], Training Loss: 1081502925874693.4, Validation Loss: 1083933311631360.0\n",
      "Epoch [633/1500], Training Loss: 1081502642905882.0, Validation Loss: 1083933043195904.0\n",
      "Epoch [634/1500], Training Loss: 1081502373968088.4, Validation Loss: 1083932841869312.0\n",
      "Epoch [635/1500], Training Loss: 1081502070015121.8, Validation Loss: 1083932506324992.0\n",
      "Epoch [636/1500], Training Loss: 1081501802268212.5, Validation Loss: 1083932304998400.0\n",
      "Epoch [637/1500], Training Loss: 1081501497433840.1, Validation Loss: 1083932036562944.0\n",
      "Epoch [638/1500], Training Loss: 1081501234601398.5, Validation Loss: 1083931835236352.0\n",
      "Epoch [639/1500], Training Loss: 1081500934688332.9, Validation Loss: 1083931566800896.0\n",
      "Epoch [640/1500], Training Loss: 1081500666687713.5, Validation Loss: 1083931298365440.0\n",
      "Epoch [641/1500], Training Loss: 1081500357498827.6, Validation Loss: 1083930895712256.0\n",
      "Epoch [642/1500], Training Loss: 1081500092187483.0, Validation Loss: 1083930560167936.0\n",
      "Epoch [643/1500], Training Loss: 1081499809320909.0, Validation Loss: 1083930157514752.0\n",
      "Epoch [644/1500], Training Loss: 1081499567330277.4, Validation Loss: 1083929956188160.0\n",
      "Epoch [645/1500], Training Loss: 1081499275838054.1, Validation Loss: 1083929889079296.0\n",
      "Epoch [646/1500], Training Loss: 1081499005468639.8, Validation Loss: 1083929620643840.0\n",
      "Epoch [647/1500], Training Loss: 1081498701031424.2, Validation Loss: 1083929352208384.0\n",
      "Epoch [648/1500], Training Loss: 1081498435174974.0, Validation Loss: 1083929016664064.0\n",
      "Epoch [649/1500], Training Loss: 1081498133343208.4, Validation Loss: 1083928815337472.0\n",
      "Epoch [650/1500], Training Loss: 1081497863353734.1, Validation Loss: 1083928614010880.0\n",
      "Epoch [651/1500], Training Loss: 1081497553924035.8, Validation Loss: 1083928345575424.0\n",
      "Epoch [652/1500], Training Loss: 1081497281730888.6, Validation Loss: 1083927942922240.0\n",
      "Epoch [653/1500], Training Loss: 1081497007102678.6, Validation Loss: 1083927808704512.0\n",
      "Epoch [654/1500], Training Loss: 1081496717687382.0, Validation Loss: 1083927473160192.0\n",
      "Epoch [655/1500], Training Loss: 1081496446833814.2, Validation Loss: 1083927204724736.0\n",
      "Epoch [656/1500], Training Loss: 1081496139073032.2, Validation Loss: 1083926802071552.0\n",
      "Epoch [657/1500], Training Loss: 1081495874556048.4, Validation Loss: 1083926600744960.0\n",
      "Epoch [658/1500], Training Loss: 1081495573480649.1, Validation Loss: 1083926265200640.0\n",
      "Epoch [659/1500], Training Loss: 1081495310566974.4, Validation Loss: 1083926063874048.0\n",
      "Epoch [660/1500], Training Loss: 1081495005483439.5, Validation Loss: 1083925661220864.0\n",
      "Epoch [661/1500], Training Loss: 1081494737937501.5, Validation Loss: 1083925325676544.0\n",
      "Epoch [662/1500], Training Loss: 1081494428168698.0, Validation Loss: 1083925124349952.0\n",
      "Epoch [663/1500], Training Loss: 1081494173781348.8, Validation Loss: 1083924855914496.0\n",
      "Epoch [664/1500], Training Loss: 1081493892945947.8, Validation Loss: 1083924520370176.0\n",
      "Epoch [665/1500], Training Loss: 1081493651271529.2, Validation Loss: 1083924319043584.0\n",
      "Epoch [666/1500], Training Loss: 1081493346939777.4, Validation Loss: 1083924050608128.0\n",
      "Epoch [667/1500], Training Loss: 1081493077846978.4, Validation Loss: 1083923849281536.0\n",
      "Epoch [668/1500], Training Loss: 1081492772792589.4, Validation Loss: 1083923513737216.0\n",
      "Epoch [669/1500], Training Loss: 1081492509765186.6, Validation Loss: 1083923312410624.0\n",
      "Epoch [670/1500], Training Loss: 1081492211483399.5, Validation Loss: 1083923111084032.0\n",
      "Epoch [671/1500], Training Loss: 1081491937824012.6, Validation Loss: 1083922909757440.0\n",
      "Epoch [672/1500], Training Loss: 1081491667321759.9, Validation Loss: 1083922708430848.0\n",
      "Epoch [673/1500], Training Loss: 1081491381153210.6, Validation Loss: 1083922372886528.0\n",
      "Epoch [674/1500], Training Loss: 1081491116291756.5, Validation Loss: 1083921970233344.0\n",
      "Epoch [675/1500], Training Loss: 1081490819532317.5, Validation Loss: 1083921768906752.0\n",
      "Epoch [676/1500], Training Loss: 1081490566001146.2, Validation Loss: 1083921366253568.0\n",
      "Epoch [677/1500], Training Loss: 1081490265833892.9, Validation Loss: 1083920963600384.0\n",
      "Epoch [678/1500], Training Loss: 1081490002801925.0, Validation Loss: 1083920628056064.0\n",
      "Epoch [679/1500], Training Loss: 1081489733713662.2, Validation Loss: 1083920158294016.0\n",
      "Epoch [680/1500], Training Loss: 1081489502499783.4, Validation Loss: 1083919956967424.0\n",
      "Epoch [681/1500], Training Loss: 1081489199206546.9, Validation Loss: 1083919621423104.0\n",
      "Epoch [682/1500], Training Loss: 1081488938374341.0, Validation Loss: 1083919554314240.0\n",
      "Epoch [683/1500], Training Loss: 1081488644814636.2, Validation Loss: 1083919218769920.0\n",
      "Epoch [684/1500], Training Loss: 1081488372949685.0, Validation Loss: 1083919084552192.0\n",
      "Epoch [685/1500], Training Loss: 1081488089280469.4, Validation Loss: 1083918883225600.0\n",
      "Epoch [686/1500], Training Loss: 1081487815307106.4, Validation Loss: 1083918681899008.0\n",
      "Epoch [687/1500], Training Loss: 1081487550784466.5, Validation Loss: 1083918480572416.0\n",
      "Epoch [688/1500], Training Loss: 1081487251623646.2, Validation Loss: 1083918077919232.0\n",
      "Epoch [689/1500], Training Loss: 1081486996622575.5, Validation Loss: 1083917608157184.0\n",
      "Epoch [690/1500], Training Loss: 1081486702823083.6, Validation Loss: 1083917272612864.0\n",
      "Epoch [691/1500], Training Loss: 1081486435358045.1, Validation Loss: 1083917071286272.0\n",
      "Epoch [692/1500], Training Loss: 1081486151498706.4, Validation Loss: 1083916735741952.0\n",
      "Epoch [693/1500], Training Loss: 1081485924935421.9, Validation Loss: 1083916333088768.0\n",
      "Epoch [694/1500], Training Loss: 1081485634489191.9, Validation Loss: 1083916265979904.0\n",
      "Epoch [695/1500], Training Loss: 1081485371613284.4, Validation Loss: 1083915863326720.0\n",
      "Epoch [696/1500], Training Loss: 1081485076703905.0, Validation Loss: 1083915527782400.0\n",
      "Epoch [697/1500], Training Loss: 1081484809864913.5, Validation Loss: 1083915326455808.0\n",
      "Epoch [698/1500], Training Loss: 1081484504205069.2, Validation Loss: 1083915125129216.0\n",
      "Epoch [699/1500], Training Loss: 1081484245999785.5, Validation Loss: 1083914722476032.0\n",
      "Epoch [700/1500], Training Loss: 1081483986712676.4, Validation Loss: 1083914386931712.0\n",
      "Epoch [701/1500], Training Loss: 1081483684701462.4, Validation Loss: 1083913984278528.0\n",
      "Epoch [702/1500], Training Loss: 1081483422417644.4, Validation Loss: 1083913782951936.0\n",
      "Epoch [703/1500], Training Loss: 1081483131973569.2, Validation Loss: 1083913715843072.0\n",
      "Epoch [704/1500], Training Loss: 1081482868909292.4, Validation Loss: 1083913514516480.0\n",
      "Epoch [705/1500], Training Loss: 1081482571057852.1, Validation Loss: 1083913178972160.0\n",
      "Epoch [706/1500], Training Loss: 1081482340448915.8, Validation Loss: 1083912776318976.0\n",
      "Epoch [707/1500], Training Loss: 1081482068219879.4, Validation Loss: 1083912440774656.0\n",
      "Epoch [708/1500], Training Loss: 1081481803716059.1, Validation Loss: 1083912373665792.0\n",
      "Epoch [709/1500], Training Loss: 1081481506579835.8, Validation Loss: 1083911836794880.0\n",
      "Epoch [710/1500], Training Loss: 1081481241968383.8, Validation Loss: 1083911568359424.0\n",
      "Epoch [711/1500], Training Loss: 1081480940994638.8, Validation Loss: 1083911232815104.0\n",
      "Epoch [712/1500], Training Loss: 1081480677312561.8, Validation Loss: 1083911031488512.0\n",
      "Epoch [713/1500], Training Loss: 1081480417977018.1, Validation Loss: 1083910695944192.0\n",
      "Epoch [714/1500], Training Loss: 1081480117573851.6, Validation Loss: 1083910494617600.0\n",
      "Epoch [715/1500], Training Loss: 1081479855397102.1, Validation Loss: 1083910091964416.0\n",
      "Epoch [716/1500], Training Loss: 1081479566208852.6, Validation Loss: 1083909823528960.0\n",
      "Epoch [717/1500], Training Loss: 1081479307396053.5, Validation Loss: 1083909689311232.0\n",
      "Epoch [718/1500], Training Loss: 1081479000841797.1, Validation Loss: 1083909420875776.0\n",
      "Epoch [719/1500], Training Loss: 1081478760060481.9, Validation Loss: 1083909152440320.0\n",
      "Epoch [720/1500], Training Loss: 1081478497192346.4, Validation Loss: 1083909085331456.0\n",
      "Epoch [721/1500], Training Loss: 1081478238228385.0, Validation Loss: 1083908749787136.0\n",
      "Epoch [722/1500], Training Loss: 1081477939458830.6, Validation Loss: 1083908481351680.0\n",
      "Epoch [723/1500], Training Loss: 1081477677766186.9, Validation Loss: 1083908280025088.0\n",
      "Epoch [724/1500], Training Loss: 1081477377516757.5, Validation Loss: 1083907944480768.0\n",
      "Epoch [725/1500], Training Loss: 1081477108810840.4, Validation Loss: 1083907676045312.0\n",
      "Epoch [726/1500], Training Loss: 1081476841098859.8, Validation Loss: 1083907340500992.0\n",
      "Epoch [727/1500], Training Loss: 1081476553334716.9, Validation Loss: 1083907139174400.0\n",
      "Epoch [728/1500], Training Loss: 1081476287424105.0, Validation Loss: 1083906803630080.0\n",
      "Epoch [729/1500], Training Loss: 1081475992628849.5, Validation Loss: 1083906535194624.0\n",
      "Epoch [730/1500], Training Loss: 1081475739858004.9, Validation Loss: 1083906333868032.0\n",
      "Epoch [731/1500], Training Loss: 1081475436287301.2, Validation Loss: 1083906132541440.0\n",
      "Epoch [732/1500], Training Loss: 1081475177894978.9, Validation Loss: 1083905796997120.0\n",
      "Epoch [733/1500], Training Loss: 1081474911518822.6, Validation Loss: 1083905729888256.0\n",
      "Epoch [734/1500], Training Loss: 1081474671885283.4, Validation Loss: 1083905528561664.0\n",
      "Epoch [735/1500], Training Loss: 1081474371928797.1, Validation Loss: 1083905058799616.0\n",
      "Epoch [736/1500], Training Loss: 1081474109862308.4, Validation Loss: 1083904790364160.0\n",
      "Epoch [737/1500], Training Loss: 1081473813017133.8, Validation Loss: 1083904589037568.0\n",
      "Epoch [738/1500], Training Loss: 1081473544468179.0, Validation Loss: 1083904186384384.0\n",
      "Epoch [739/1500], Training Loss: 1081473271048401.6, Validation Loss: 1083903985057792.0\n",
      "Epoch [740/1500], Training Loss: 1081472987770838.8, Validation Loss: 1083903783731200.0\n",
      "Epoch [741/1500], Training Loss: 1081472721055482.5, Validation Loss: 1083903448186880.0\n",
      "Epoch [742/1500], Training Loss: 1081472423483522.5, Validation Loss: 1083903045533696.0\n",
      "Epoch [743/1500], Training Loss: 1081472170958054.5, Validation Loss: 1083902642880512.0\n",
      "Epoch [744/1500], Training Loss: 1081471870462859.4, Validation Loss: 1083902441553920.0\n",
      "Epoch [745/1500], Training Loss: 1081471601112606.4, Validation Loss: 1083902106009600.0\n",
      "Epoch [746/1500], Training Loss: 1081471330093501.8, Validation Loss: 1083901703356416.0\n",
      "Epoch [747/1500], Training Loss: 1081471105387979.2, Validation Loss: 1083901502029824.0\n",
      "Epoch [748/1500], Training Loss: 1081470805888512.1, Validation Loss: 1083901233594368.0\n",
      "Epoch [749/1500], Training Loss: 1081470540141875.2, Validation Loss: 1083900898050048.0\n",
      "Epoch [750/1500], Training Loss: 1081470247440083.1, Validation Loss: 1083900696723456.0\n",
      "Epoch [751/1500], Training Loss: 1081469981084453.0, Validation Loss: 1083900495396864.0\n",
      "Epoch [752/1500], Training Loss: 1081469690128948.2, Validation Loss: 1083899958525952.0\n",
      "Epoch [753/1500], Training Loss: 1081469419731756.2, Validation Loss: 1083899757199360.0\n",
      "Epoch [754/1500], Training Loss: 1081469156660396.6, Validation Loss: 1083899287437312.0\n",
      "Epoch [755/1500], Training Loss: 1081468853533432.2, Validation Loss: 1083898817675264.0\n",
      "Epoch [756/1500], Training Loss: 1081468598156493.9, Validation Loss: 1083898817675264.0\n",
      "Epoch [757/1500], Training Loss: 1081468306016872.9, Validation Loss: 1083898415022080.0\n",
      "Epoch [758/1500], Training Loss: 1081468037799310.0, Validation Loss: 1083898347913216.0\n",
      "Epoch [759/1500], Training Loss: 1081467746871339.8, Validation Loss: 1083898012368896.0\n",
      "Epoch [760/1500], Training Loss: 1081467522119035.8, Validation Loss: 1083897811042304.0\n",
      "Epoch [761/1500], Training Loss: 1081467239970112.4, Validation Loss: 1083897609715712.0\n",
      "Epoch [762/1500], Training Loss: 1081466973611729.8, Validation Loss: 1083897139953664.0\n",
      "Epoch [763/1500], Training Loss: 1081466679854407.9, Validation Loss: 1083896804409344.0\n",
      "Epoch [764/1500], Training Loss: 1081466417479694.5, Validation Loss: 1083896603082752.0\n",
      "Epoch [765/1500], Training Loss: 1081466112814641.4, Validation Loss: 1083896267538432.0\n",
      "Epoch [766/1500], Training Loss: 1081465849820635.2, Validation Loss: 1083896066211840.0\n",
      "Epoch [767/1500], Training Loss: 1081465588095119.5, Validation Loss: 1083895864885248.0\n",
      "Epoch [768/1500], Training Loss: 1081465286939880.8, Validation Loss: 1083895663558656.0\n",
      "Epoch [769/1500], Training Loss: 1081465028077440.2, Validation Loss: 1083895596449792.0\n",
      "Epoch [770/1500], Training Loss: 1081464739153303.2, Validation Loss: 1083895260905472.0\n",
      "Epoch [771/1500], Training Loss: 1081464473546609.4, Validation Loss: 1083894992470016.0\n",
      "Epoch [772/1500], Training Loss: 1081464169156890.6, Validation Loss: 1083894656925696.0\n",
      "Epoch [773/1500], Training Loss: 1081463936380697.2, Validation Loss: 1083894455599104.0\n",
      "Epoch [774/1500], Training Loss: 1081463673695934.6, Validation Loss: 1083894254272512.0\n",
      "Epoch [775/1500], Training Loss: 1081463407191158.9, Validation Loss: 1083894120054784.0\n",
      "Epoch [776/1500], Training Loss: 1081463108909397.9, Validation Loss: 1083893717401600.0\n",
      "Epoch [777/1500], Training Loss: 1081462849708151.4, Validation Loss: 1083893448966144.0\n",
      "Epoch [778/1500], Training Loss: 1081462547220939.8, Validation Loss: 1083893247639552.0\n",
      "Epoch [779/1500], Training Loss: 1081462281528265.4, Validation Loss: 1083893113421824.0\n",
      "Epoch [780/1500], Training Loss: 1081462018913511.8, Validation Loss: 1083892912095232.0\n",
      "Epoch [781/1500], Training Loss: 1081461720874773.6, Validation Loss: 1083892576550912.0\n",
      "Epoch [782/1500], Training Loss: 1081461457615752.5, Validation Loss: 1083892173897728.0\n",
      "Epoch [783/1500], Training Loss: 1081461168437371.6, Validation Loss: 1083891771244544.0\n",
      "Epoch [784/1500], Training Loss: 1081460907357065.9, Validation Loss: 1083891569917952.0\n",
      "Epoch [785/1500], Training Loss: 1081460603724309.9, Validation Loss: 1083891301482496.0\n",
      "Epoch [786/1500], Training Loss: 1081460354433142.6, Validation Loss: 1083891167264768.0\n",
      "Epoch [787/1500], Training Loss: 1081460092782722.1, Validation Loss: 1083890764611584.0\n",
      "Epoch [788/1500], Training Loss: 1081459841251781.9, Validation Loss: 1083890563284992.0\n",
      "Epoch [789/1500], Training Loss: 1081459542661529.5, Validation Loss: 1083890227740672.0\n",
      "Epoch [790/1500], Training Loss: 1081459282899923.0, Validation Loss: 1083890026414080.0\n",
      "Epoch [791/1500], Training Loss: 1081458985045175.9, Validation Loss: 1083889825087488.0\n",
      "Epoch [792/1500], Training Loss: 1081458709307225.8, Validation Loss: 1083889422434304.0\n",
      "Epoch [793/1500], Training Loss: 1081458442658877.8, Validation Loss: 1083889153998848.0\n",
      "Epoch [794/1500], Training Loss: 1081458156627715.9, Validation Loss: 1083888751345664.0\n",
      "Epoch [795/1500], Training Loss: 1081457892628773.2, Validation Loss: 1083888415801344.0\n",
      "Epoch [796/1500], Training Loss: 1081457594400015.8, Validation Loss: 1083888080257024.0\n",
      "Epoch [797/1500], Training Loss: 1081457340806591.1, Validation Loss: 1083887811821568.0\n",
      "Epoch [798/1500], Training Loss: 1081457040413685.2, Validation Loss: 1083887610494976.0\n",
      "Epoch [799/1500], Training Loss: 1081456777883738.5, Validation Loss: 1083887274950656.0\n",
      "Epoch [800/1500], Training Loss: 1081456509304422.1, Validation Loss: 1083887073624064.0\n",
      "Epoch [801/1500], Training Loss: 1081456275587930.5, Validation Loss: 1083886670970880.0\n",
      "Epoch [802/1500], Training Loss: 1081455974093065.6, Validation Loss: 1083886335426560.0\n",
      "Epoch [803/1500], Training Loss: 1081455713944349.8, Validation Loss: 1083886268317696.0\n",
      "Epoch [804/1500], Training Loss: 1081455418369826.0, Validation Loss: 1083885932773376.0\n",
      "Epoch [805/1500], Training Loss: 1081455148768493.2, Validation Loss: 1083885731446784.0\n",
      "Epoch [806/1500], Training Loss: 1081454866848173.2, Validation Loss: 1083885463011328.0\n",
      "Epoch [807/1500], Training Loss: 1081454591750049.8, Validation Loss: 1083885328793600.0\n",
      "Epoch [808/1500], Training Loss: 1081454325324414.8, Validation Loss: 1083885127467008.0\n",
      "Epoch [809/1500], Training Loss: 1081454027198922.0, Validation Loss: 1083884926140416.0\n",
      "Epoch [810/1500], Training Loss: 1081453773152447.1, Validation Loss: 1083884657704960.0\n",
      "Epoch [811/1500], Training Loss: 1081453475435970.6, Validation Loss: 1083884322160640.0\n",
      "Epoch [812/1500], Training Loss: 1081453209106071.4, Validation Loss: 1083883986616320.0\n",
      "Epoch [813/1500], Training Loss: 1081452927733922.2, Validation Loss: 1083883785289728.0\n",
      "Epoch [814/1500], Training Loss: 1081452700968241.8, Validation Loss: 1083883718180864.0\n",
      "Epoch [815/1500], Training Loss: 1081452409689466.8, Validation Loss: 1083883382636544.0\n",
      "Epoch [816/1500], Training Loss: 1081452147253511.8, Validation Loss: 1083883181309952.0\n",
      "Epoch [817/1500], Training Loss: 1081451852555160.2, Validation Loss: 1083882979983360.0\n",
      "Epoch [818/1500], Training Loss: 1081451585825945.5, Validation Loss: 1083882778656768.0\n",
      "Epoch [819/1500], Training Loss: 1081451283895668.6, Validation Loss: 1083882376003584.0\n",
      "Epoch [820/1500], Training Loss: 1081451021905258.6, Validation Loss: 1083882040459264.0\n",
      "Epoch [821/1500], Training Loss: 1081450760564403.4, Validation Loss: 1083881772023808.0\n",
      "Epoch [822/1500], Training Loss: 1081450461320289.0, Validation Loss: 1083881436479488.0\n",
      "Epoch [823/1500], Training Loss: 1081450200000185.0, Validation Loss: 1083881168044032.0\n",
      "Epoch [824/1500], Training Loss: 1081449909577923.4, Validation Loss: 1083880765390848.0\n",
      "Epoch [825/1500], Training Loss: 1081449645415830.1, Validation Loss: 1083880429846528.0\n",
      "Epoch [826/1500], Training Loss: 1081449348138720.8, Validation Loss: 1083880295628800.0\n",
      "Epoch [827/1500], Training Loss: 1081449118020777.5, Validation Loss: 1083880094302208.0\n",
      "Epoch [828/1500], Training Loss: 1081448841548890.2, Validation Loss: 1083880027193344.0\n",
      "Epoch [829/1500], Training Loss: 1081448579576427.1, Validation Loss: 1083879624540160.0\n",
      "Epoch [830/1500], Training Loss: 1081448282015500.2, Validation Loss: 1083879423213568.0\n",
      "Epoch [831/1500], Training Loss: 1081448019654217.1, Validation Loss: 1083878886342656.0\n",
      "Epoch [832/1500], Training Loss: 1081447716783666.2, Validation Loss: 1083878685016064.0\n",
      "Epoch [833/1500], Training Loss: 1081447453442371.5, Validation Loss: 1083878483689472.0\n",
      "Epoch [834/1500], Training Loss: 1081447192272195.0, Validation Loss: 1083878081036288.0\n",
      "Epoch [835/1500], Training Loss: 1081446895247932.2, Validation Loss: 1083877678383104.0\n",
      "Epoch [836/1500], Training Loss: 1081446629989922.9, Validation Loss: 1083877477056512.0\n",
      "Epoch [837/1500], Training Loss: 1081446340156631.0, Validation Loss: 1083877074403328.0\n",
      "Epoch [838/1500], Training Loss: 1081446080925386.1, Validation Loss: 1083876940185600.0\n",
      "Epoch [839/1500], Training Loss: 1081445775585566.1, Validation Loss: 1083876738859008.0\n",
      "Epoch [840/1500], Training Loss: 1081445534592216.2, Validation Loss: 1083876201988096.0\n",
      "Epoch [841/1500], Training Loss: 1081445274441643.1, Validation Loss: 1083876000661504.0\n",
      "Epoch [842/1500], Training Loss: 1081445014848051.0, Validation Loss: 1083875799334912.0\n",
      "Epoch [843/1500], Training Loss: 1081444713633768.4, Validation Loss: 1083875598008320.0\n",
      "Epoch [844/1500], Training Loss: 1081444452276400.1, Validation Loss: 1083875195355136.0\n",
      "Epoch [845/1500], Training Loss: 1081444152930540.5, Validation Loss: 1083874926919680.0\n",
      "Epoch [846/1500], Training Loss: 1081443884104961.4, Validation Loss: 1083874591375360.0\n",
      "Epoch [847/1500], Training Loss: 1081443615282793.4, Validation Loss: 1083874390048768.0\n",
      "Epoch [848/1500], Training Loss: 1081443328925432.9, Validation Loss: 1083874188722176.0\n",
      "Epoch [849/1500], Training Loss: 1081443061888936.0, Validation Loss: 1083873786068992.0\n",
      "Epoch [850/1500], Training Loss: 1081442768702735.5, Validation Loss: 1083873584742400.0\n",
      "Epoch [851/1500], Training Loss: 1081442515512532.2, Validation Loss: 1083873182089216.0\n",
      "Epoch [852/1500], Training Loss: 1081442210877590.5, Validation Loss: 1083872980762624.0\n",
      "Epoch [853/1500], Training Loss: 1081441953347692.0, Validation Loss: 1083872779436032.0\n",
      "Epoch [854/1500], Training Loss: 1081441688787074.1, Validation Loss: 1083872645218304.0\n",
      "Epoch [855/1500], Training Loss: 1081441447999425.5, Validation Loss: 1083872443891712.0\n",
      "Epoch [856/1500], Training Loss: 1081441148325268.8, Validation Loss: 1083871907020800.0\n",
      "Epoch [857/1500], Training Loss: 1081440886137553.5, Validation Loss: 1083871705694208.0\n",
      "Epoch [858/1500], Training Loss: 1081440588232882.6, Validation Loss: 1083871504367616.0\n",
      "Epoch [859/1500], Training Loss: 1081440317473276.9, Validation Loss: 1083871303041024.0\n",
      "Epoch [860/1500], Training Loss: 1081440046083941.8, Validation Loss: 1083871101714432.0\n",
      "Epoch [861/1500], Training Loss: 1081439764360733.9, Validation Loss: 1083870699061248.0\n",
      "Epoch [862/1500], Training Loss: 1081439497436145.8, Validation Loss: 1083870430625792.0\n",
      "Epoch [863/1500], Training Loss: 1081439197526199.4, Validation Loss: 1083869960863744.0\n",
      "Epoch [864/1500], Training Loss: 1081438947020912.6, Validation Loss: 1083869558210560.0\n",
      "Epoch [865/1500], Training Loss: 1081438646483509.5, Validation Loss: 1083869155557376.0\n",
      "Epoch [866/1500], Training Loss: 1081438377585466.0, Validation Loss: 1083868752904192.0\n",
      "Epoch [867/1500], Training Loss: 1081438103766338.5, Validation Loss: 1083868551577600.0\n",
      "Epoch [868/1500], Training Loss: 1081437879082405.6, Validation Loss: 1083868283142144.0\n",
      "Epoch [869/1500], Training Loss: 1081437580326187.6, Validation Loss: 1083868014706688.0\n",
      "Epoch [870/1500], Training Loss: 1081437317094038.2, Validation Loss: 1083867746271232.0\n",
      "Epoch [871/1500], Training Loss: 1081437022391716.1, Validation Loss: 1083867612053504.0\n",
      "Epoch [872/1500], Training Loss: 1081436755633126.4, Validation Loss: 1083867209400320.0\n",
      "Epoch [873/1500], Training Loss: 1081436467653441.2, Validation Loss: 1083867008073728.0\n",
      "Epoch [874/1500], Training Loss: 1081436194776066.2, Validation Loss: 1083866605420544.0\n",
      "Epoch [875/1500], Training Loss: 1081435930820142.5, Validation Loss: 1083866538311680.0\n",
      "Epoch [876/1500], Training Loss: 1081435629155129.8, Validation Loss: 1083866202767360.0\n",
      "Epoch [877/1500], Training Loss: 1081435373603617.1, Validation Loss: 1083865867223040.0\n",
      "Epoch [878/1500], Training Loss: 1081435081704462.9, Validation Loss: 1083865598787584.0\n",
      "Epoch [879/1500], Training Loss: 1081434813835590.6, Validation Loss: 1083865263243264.0\n",
      "Epoch [880/1500], Training Loss: 1081434522767114.0, Validation Loss: 1083865061916672.0\n",
      "Epoch [881/1500], Training Loss: 1081434298675467.1, Validation Loss: 1083864860590080.0\n",
      "Epoch [882/1500], Training Loss: 1081434016276925.8, Validation Loss: 1083864592154624.0\n",
      "Epoch [883/1500], Training Loss: 1081433749707706.8, Validation Loss: 1083864390828032.0\n",
      "Epoch [884/1500], Training Loss: 1081433456342320.1, Validation Loss: 1083863921065984.0\n",
      "Epoch [885/1500], Training Loss: 1081433193438946.0, Validation Loss: 1083863518412800.0\n",
      "Epoch [886/1500], Training Loss: 1081432886258069.0, Validation Loss: 1083863115759616.0\n",
      "Epoch [887/1500], Training Loss: 1081432625437350.8, Validation Loss: 1083862847324160.0\n",
      "Epoch [888/1500], Training Loss: 1081432365008477.9, Validation Loss: 1083862645997568.0\n",
      "Epoch [889/1500], Training Loss: 1081432064398989.2, Validation Loss: 1083862310453248.0\n",
      "Epoch [890/1500], Training Loss: 1081431803243510.6, Validation Loss: 1083862109126656.0\n",
      "Epoch [891/1500], Training Loss: 1081431514730616.6, Validation Loss: 1083861974908928.0\n",
      "Epoch [892/1500], Training Loss: 1081431249751492.8, Validation Loss: 1083861706473472.0\n",
      "Epoch [893/1500], Training Loss: 1081430946082781.1, Validation Loss: 1083861505146880.0\n",
      "Epoch [894/1500], Training Loss: 1081430714077626.2, Validation Loss: 1083860968275968.0\n",
      "Epoch [895/1500], Training Loss: 1081430448723624.6, Validation Loss: 1083860699840512.0\n",
      "Epoch [896/1500], Training Loss: 1081430183605920.0, Validation Loss: 1083860565622784.0\n",
      "Epoch [897/1500], Training Loss: 1081429886213214.6, Validation Loss: 1083860162969600.0\n",
      "Epoch [898/1500], Training Loss: 1081429626370113.6, Validation Loss: 1083859961643008.0\n",
      "Epoch [899/1500], Training Loss: 1081429323978894.8, Validation Loss: 1083859626098688.0\n",
      "Epoch [900/1500], Training Loss: 1081429055094746.9, Validation Loss: 1083859558989824.0\n",
      "Epoch [901/1500], Training Loss: 1081428793853893.0, Validation Loss: 1083859223445504.0\n",
      "Epoch [902/1500], Training Loss: 1081428497904578.9, Validation Loss: 1083859156336640.0\n",
      "Epoch [903/1500], Training Loss: 1081428235160508.1, Validation Loss: 1083859022118912.0\n",
      "Epoch [904/1500], Training Loss: 1081427944565906.5, Validation Loss: 1083858753683456.0\n",
      "Epoch [905/1500], Training Loss: 1081427683021900.8, Validation Loss: 1083858216812544.0\n",
      "Epoch [906/1500], Training Loss: 1081427379828864.4, Validation Loss: 1083858149703680.0\n",
      "Epoch [907/1500], Training Loss: 1081427131637036.4, Validation Loss: 1083857881268224.0\n",
      "Epoch [908/1500], Training Loss: 1081426871756531.8, Validation Loss: 1083857612832768.0\n",
      "Epoch [909/1500], Training Loss: 1081426619382882.8, Validation Loss: 1083857277288448.0\n",
      "Epoch [910/1500], Training Loss: 1081426315045300.8, Validation Loss: 1083857008852992.0\n",
      "Epoch [911/1500], Training Loss: 1081426059155476.1, Validation Loss: 1083856807526400.0\n",
      "Epoch [912/1500], Training Loss: 1081425761993748.0, Validation Loss: 1083856673308672.0\n",
      "Epoch [913/1500], Training Loss: 1081425488503492.5, Validation Loss: 1083856404873216.0\n",
      "Epoch [914/1500], Training Loss: 1081425219926246.0, Validation Loss: 1083856069328896.0\n",
      "Epoch [915/1500], Training Loss: 1081424933158645.1, Validation Loss: 1083855666675712.0\n",
      "Epoch [916/1500], Training Loss: 1081424668784268.1, Validation Loss: 1083855264022528.0\n",
      "Epoch [917/1500], Training Loss: 1081424373032699.5, Validation Loss: 1083854861369344.0\n",
      "Epoch [918/1500], Training Loss: 1081424118131407.0, Validation Loss: 1083854525825024.0\n",
      "Epoch [919/1500], Training Loss: 1081423815657433.9, Validation Loss: 1083854458716160.0\n",
      "Epoch [920/1500], Training Loss: 1081423553787273.1, Validation Loss: 1083854257389568.0\n",
      "Epoch [921/1500], Training Loss: 1081423285864989.0, Validation Loss: 1083854056062976.0\n",
      "Epoch [922/1500], Training Loss: 1081423050994589.8, Validation Loss: 1083853787627520.0\n",
      "Epoch [923/1500], Training Loss: 1081422750079181.0, Validation Loss: 1083853586300928.0\n",
      "Epoch [924/1500], Training Loss: 1081422490573478.9, Validation Loss: 1083853116538880.0\n",
      "Epoch [925/1500], Training Loss: 1081422194017377.2, Validation Loss: 1083852915212288.0\n",
      "Epoch [926/1500], Training Loss: 1081421925125540.5, Validation Loss: 1083852780994560.0\n",
      "Epoch [927/1500], Training Loss: 1081421643612346.8, Validation Loss: 1083852512559104.0\n",
      "Epoch [928/1500], Training Loss: 1081421366554528.8, Validation Loss: 1083852177014784.0\n",
      "Epoch [929/1500], Training Loss: 1081421102204542.6, Validation Loss: 1083851774361600.0\n",
      "Epoch [930/1500], Training Loss: 1081420803388266.8, Validation Loss: 1083851573035008.0\n",
      "Epoch [931/1500], Training Loss: 1081420548245099.1, Validation Loss: 1083851371708416.0\n",
      "Epoch [932/1500], Training Loss: 1081420253582562.8, Validation Loss: 1083851036164096.0\n",
      "Epoch [933/1500], Training Loss: 1081419987293243.5, Validation Loss: 1083850633510912.0\n",
      "Epoch [934/1500], Training Loss: 1081419703872354.9, Validation Loss: 1083850365075456.0\n",
      "Epoch [935/1500], Training Loss: 1081419478261850.2, Validation Loss: 1083849962422272.0\n",
      "Epoch [936/1500], Training Loss: 1081419186261707.0, Validation Loss: 1083849626877952.0\n",
      "Epoch [937/1500], Training Loss: 1081418922744853.9, Validation Loss: 1083849291333632.0\n",
      "Epoch [938/1500], Training Loss: 1081418628310634.6, Validation Loss: 1083849090007040.0\n",
      "Epoch [939/1500], Training Loss: 1081418362766040.4, Validation Loss: 1083848687353856.0\n",
      "Epoch [940/1500], Training Loss: 1081418063401243.9, Validation Loss: 1083848620244992.0\n",
      "Epoch [941/1500], Training Loss: 1081417798004626.2, Validation Loss: 1083848486027264.0\n",
      "Epoch [942/1500], Training Loss: 1081417537004084.8, Validation Loss: 1083848284700672.0\n",
      "Epoch [943/1500], Training Loss: 1081417232960578.0, Validation Loss: 1083848083374080.0\n",
      "Epoch [944/1500], Training Loss: 1081416975248962.4, Validation Loss: 1083847814938624.0\n",
      "Epoch [945/1500], Training Loss: 1081416687414228.5, Validation Loss: 1083847345176576.0\n",
      "Epoch [946/1500], Training Loss: 1081416424036656.4, Validation Loss: 1083847278067712.0\n",
      "Epoch [947/1500], Training Loss: 1081416123911421.9, Validation Loss: 1083846875414528.0\n",
      "Epoch [948/1500], Training Loss: 1081415892729786.6, Validation Loss: 1083846539870208.0\n",
      "Epoch [949/1500], Training Loss: 1081415619096943.9, Validation Loss: 1083846338543616.0\n",
      "Epoch [950/1500], Training Loss: 1081415357183928.9, Validation Loss: 1083846070108160.0\n",
      "Epoch [951/1500], Training Loss: 1081415059738506.0, Validation Loss: 1083845734563840.0\n",
      "Epoch [952/1500], Training Loss: 1081414795494037.6, Validation Loss: 1083845399019520.0\n",
      "Epoch [953/1500], Training Loss: 1081414493505503.9, Validation Loss: 1083845130584064.0\n",
      "Epoch [954/1500], Training Loss: 1081414231478865.8, Validation Loss: 1083844929257472.0\n",
      "Epoch [955/1500], Training Loss: 1081413971060373.6, Validation Loss: 1083844795039744.0\n",
      "Epoch [956/1500], Training Loss: 1081413670407104.5, Validation Loss: 1083844392386560.0\n",
      "Epoch [957/1500], Training Loss: 1081413405804325.1, Validation Loss: 1083844123951104.0\n",
      "Epoch [958/1500], Training Loss: 1081413115988048.0, Validation Loss: 1083843922624512.0\n",
      "Epoch [959/1500], Training Loss: 1081412858781838.0, Validation Loss: 1083843721297920.0\n",
      "Epoch [960/1500], Training Loss: 1081412553698463.1, Validation Loss: 1083843385753600.0\n",
      "Epoch [961/1500], Training Loss: 1081412313049480.5, Validation Loss: 1083843050209280.0\n",
      "Epoch [962/1500], Training Loss: 1081412048668697.1, Validation Loss: 1083842848882688.0\n",
      "Epoch [963/1500], Training Loss: 1081411790152662.1, Validation Loss: 1083842647556096.0\n",
      "Epoch [964/1500], Training Loss: 1081411491674041.6, Validation Loss: 1083842446229504.0\n",
      "Epoch [965/1500], Training Loss: 1081411230755140.0, Validation Loss: 1083842043576320.0\n",
      "Epoch [966/1500], Training Loss: 1081410928469383.9, Validation Loss: 1083841775140864.0\n",
      "Epoch [967/1500], Training Loss: 1081410661911943.8, Validation Loss: 1083841439596544.0\n",
      "Epoch [968/1500], Training Loss: 1081410395563972.5, Validation Loss: 1083841305378816.0\n",
      "Epoch [969/1500], Training Loss: 1081410104667029.0, Validation Loss: 1083841036943360.0\n",
      "Epoch [970/1500], Training Loss: 1081409839089013.6, Validation Loss: 1083840500072448.0\n",
      "Epoch [971/1500], Training Loss: 1081409544445701.8, Validation Loss: 1083840298745856.0\n",
      "Epoch [972/1500], Training Loss: 1081409290084976.1, Validation Loss: 1083840097419264.0\n",
      "Epoch [973/1500], Training Loss: 1081408986949767.0, Validation Loss: 1083839828983808.0\n",
      "Epoch [974/1500], Training Loss: 1081408730808677.4, Validation Loss: 1083839627657216.0\n",
      "Epoch [975/1500], Training Loss: 1081408467408141.9, Validation Loss: 1083839493439488.0\n",
      "Epoch [976/1500], Training Loss: 1081408224986593.9, Validation Loss: 1083839292112896.0\n",
      "Epoch [977/1500], Training Loss: 1081407924494597.8, Validation Loss: 1083838889459712.0\n",
      "Epoch [978/1500], Training Loss: 1081407663589704.6, Validation Loss: 1083838755241984.0\n",
      "Epoch [979/1500], Training Loss: 1081407366666874.2, Validation Loss: 1083838352588800.0\n",
      "Epoch [980/1500], Training Loss: 1081407094721445.1, Validation Loss: 1083838151262208.0\n",
      "Epoch [981/1500], Training Loss: 1081406823052963.2, Validation Loss: 1083837882826752.0\n",
      "Epoch [982/1500], Training Loss: 1081406542313304.5, Validation Loss: 1083837681500160.0\n",
      "Epoch [983/1500], Training Loss: 1081406274030567.9, Validation Loss: 1083837211738112.0\n",
      "Epoch [984/1500], Training Loss: 1081405975629342.9, Validation Loss: 1083836809084928.0\n",
      "Epoch [985/1500], Training Loss: 1081405725098421.4, Validation Loss: 1083836540649472.0\n",
      "Epoch [986/1500], Training Loss: 1081405425143350.5, Validation Loss: 1083836205105152.0\n",
      "Epoch [987/1500], Training Loss: 1081405156126724.1, Validation Loss: 1083836003778560.0\n",
      "Epoch [988/1500], Training Loss: 1081404883968788.0, Validation Loss: 1083835802451968.0\n",
      "Epoch [989/1500], Training Loss: 1081404658757010.4, Validation Loss: 1083835534016512.0\n",
      "Epoch [990/1500], Training Loss: 1081404356843334.8, Validation Loss: 1083835198472192.0\n",
      "Epoch [991/1500], Training Loss: 1081404093212007.1, Validation Loss: 1083834997145600.0\n",
      "Epoch [992/1500], Training Loss: 1081403800224153.8, Validation Loss: 1083834661601280.0\n",
      "Epoch [993/1500], Training Loss: 1081403532762142.2, Validation Loss: 1083834393165824.0\n",
      "Epoch [994/1500], Training Loss: 1081403248375251.4, Validation Loss: 1083834258948096.0\n",
      "Epoch [995/1500], Training Loss: 1081402972959505.1, Validation Loss: 1083833856294912.0\n",
      "Epoch [996/1500], Training Loss: 1081402708605005.2, Validation Loss: 1083833654968320.0\n",
      "Epoch [997/1500], Training Loss: 1081402408471156.2, Validation Loss: 1083833252315136.0\n",
      "Epoch [998/1500], Training Loss: 1081402154089308.9, Validation Loss: 1083833050988544.0\n",
      "Epoch [999/1500], Training Loss: 1081401856535595.2, Validation Loss: 1083832849661952.0\n",
      "Epoch [1000/1500], Training Loss: 1081401588728065.1, Validation Loss: 1083832312791040.0\n",
      "Epoch [1001/1500], Training Loss: 1081401302997301.4, Validation Loss: 1083832111464448.0\n",
      "Epoch [1002/1500], Training Loss: 1081401080836921.4, Validation Loss: 1083831910137856.0\n",
      "Epoch [1003/1500], Training Loss: 1081400792772272.5, Validation Loss: 1083831708811264.0\n",
      "Epoch [1004/1500], Training Loss: 1081400527121749.1, Validation Loss: 1083831440375808.0\n",
      "Epoch [1005/1500], Training Loss: 1081400235917089.8, Validation Loss: 1083831104831488.0\n",
      "Epoch [1006/1500], Training Loss: 1081399968914685.0, Validation Loss: 1083830903504896.0\n",
      "Epoch [1007/1500], Training Loss: 1081399666237978.9, Validation Loss: 1083830567960576.0\n",
      "Epoch [1008/1500], Training Loss: 1081399406211387.0, Validation Loss: 1083830366633984.0\n",
      "Epoch [1009/1500], Training Loss: 1081399144026375.9, Validation Loss: 1083830165307392.0\n",
      "Epoch [1010/1500], Training Loss: 1081398841976180.5, Validation Loss: 1083829762654208.0\n",
      "Epoch [1011/1500], Training Loss: 1081398582337754.9, Validation Loss: 1083829695545344.0\n",
      "Epoch [1012/1500], Training Loss: 1081398294892090.9, Validation Loss: 1083829158674432.0\n",
      "Epoch [1013/1500], Training Loss: 1081398028695190.4, Validation Loss: 1083829024456704.0\n",
      "Epoch [1014/1500], Training Loss: 1081397725679814.5, Validation Loss: 1083828756021248.0\n",
      "Epoch [1015/1500], Training Loss: 1081397494463131.2, Validation Loss: 1083828420476928.0\n",
      "Epoch [1016/1500], Training Loss: 1081397227774002.6, Validation Loss: 1083828152041472.0\n",
      "Epoch [1017/1500], Training Loss: 1081396961531101.9, Validation Loss: 1083827950714880.0\n",
      "Epoch [1018/1500], Training Loss: 1081396665172083.4, Validation Loss: 1083827749388288.0\n",
      "Epoch [1019/1500], Training Loss: 1081396403629033.1, Validation Loss: 1083827413843968.0\n",
      "Epoch [1020/1500], Training Loss: 1081396104503534.1, Validation Loss: 1083826809864192.0\n",
      "Epoch [1021/1500], Training Loss: 1081395837896307.9, Validation Loss: 1083826474319872.0\n",
      "Epoch [1022/1500], Training Loss: 1081395574442268.8, Validation Loss: 1083826205884416.0\n",
      "Epoch [1023/1500], Training Loss: 1081395275463070.1, Validation Loss: 1083826004557824.0\n",
      "Epoch [1024/1500], Training Loss: 1081395014521917.0, Validation Loss: 1083825669013504.0\n",
      "Epoch [1025/1500], Training Loss: 1081394724044125.4, Validation Loss: 1083825400578048.0\n",
      "Epoch [1026/1500], Training Loss: 1081394463397057.1, Validation Loss: 1083825065033728.0\n",
      "Epoch [1027/1500], Training Loss: 1081394159031866.6, Validation Loss: 1083824662380544.0\n",
      "Epoch [1028/1500], Training Loss: 1081393915346514.2, Validation Loss: 1083824326836224.0\n",
      "Epoch [1029/1500], Training Loss: 1081393652697094.5, Validation Loss: 1083824125509632.0\n",
      "Epoch [1030/1500], Training Loss: 1081393397361819.1, Validation Loss: 1083823924183040.0\n",
      "Epoch [1031/1500], Training Loss: 1081393096957426.9, Validation Loss: 1083823655747584.0\n",
      "Epoch [1032/1500], Training Loss: 1081392838428050.4, Validation Loss: 1083823320203264.0\n",
      "Epoch [1033/1500], Training Loss: 1081392538101027.5, Validation Loss: 1083823118876672.0\n",
      "Epoch [1034/1500], Training Loss: 1081392267282358.6, Validation Loss: 1083822917550080.0\n",
      "Epoch [1035/1500], Training Loss: 1081391998370263.4, Validation Loss: 1083822716223488.0\n",
      "Epoch [1036/1500], Training Loss: 1081391711999512.8, Validation Loss: 1083822514896896.0\n",
      "Epoch [1037/1500], Training Loss: 1081391446144387.9, Validation Loss: 1083822179352576.0\n",
      "Epoch [1038/1500], Training Loss: 1081391152858303.6, Validation Loss: 1083821978025984.0\n",
      "Epoch [1039/1500], Training Loss: 1081390897476702.0, Validation Loss: 1083821910917120.0\n",
      "Epoch [1040/1500], Training Loss: 1081390593835733.5, Validation Loss: 1083821709590528.0\n",
      "Epoch [1041/1500], Training Loss: 1081390334822050.0, Validation Loss: 1083821172719616.0\n",
      "Epoch [1042/1500], Training Loss: 1081390068756091.2, Validation Loss: 1083820837175296.0\n",
      "Epoch [1043/1500], Training Loss: 1081389829685670.6, Validation Loss: 1083820635848704.0\n",
      "Epoch [1044/1500], Training Loss: 1081389532145580.0, Validation Loss: 1083820233195520.0\n",
      "Epoch [1045/1500], Training Loss: 1081389271775347.5, Validation Loss: 1083820031868928.0\n",
      "Epoch [1046/1500], Training Loss: 1081388974517339.2, Validation Loss: 1083819629215744.0\n",
      "Epoch [1047/1500], Training Loss: 1081388704351436.1, Validation Loss: 1083819360780288.0\n",
      "Epoch [1048/1500], Training Loss: 1081388424818494.4, Validation Loss: 1083819025235968.0\n",
      "Epoch [1049/1500], Training Loss: 1081388146452007.2, Validation Loss: 1083818823909376.0\n",
      "Epoch [1050/1500], Training Loss: 1081387880585108.2, Validation Loss: 1083818622582784.0\n",
      "Epoch [1051/1500], Training Loss: 1081387583168346.8, Validation Loss: 1083818287038464.0\n",
      "Epoch [1052/1500], Training Loss: 1081387328626780.8, Validation Loss: 1083817817276416.0\n",
      "Epoch [1053/1500], Training Loss: 1081387031777035.5, Validation Loss: 1083817615949824.0\n",
      "Epoch [1054/1500], Training Loss: 1081386765166977.2, Validation Loss: 1083817213296640.0\n",
      "Epoch [1055/1500], Training Loss: 1081386487202364.6, Validation Loss: 1083816877752320.0\n",
      "Epoch [1056/1500], Training Loss: 1081386259707771.6, Validation Loss: 1083816676425728.0\n",
      "Epoch [1057/1500], Training Loss: 1081385963250223.5, Validation Loss: 1083816340881408.0\n",
      "Epoch [1058/1500], Training Loss: 1081385700421485.9, Validation Loss: 1083816139554816.0\n",
      "Epoch [1059/1500], Training Loss: 1081385407327824.1, Validation Loss: 1083815938228224.0\n",
      "Epoch [1060/1500], Training Loss: 1081385140505872.4, Validation Loss: 1083815736901632.0\n",
      "Epoch [1061/1500], Training Loss: 1081384847960034.2, Validation Loss: 1083815535575040.0\n",
      "Epoch [1062/1500], Training Loss: 1081384579523392.1, Validation Loss: 1083815334248448.0\n",
      "Epoch [1063/1500], Training Loss: 1081384318984338.9, Validation Loss: 1083815132921856.0\n",
      "Epoch [1064/1500], Training Loss: 1081384016394999.1, Validation Loss: 1083814797377536.0\n",
      "Epoch [1065/1500], Training Loss: 1081383757882114.0, Validation Loss: 1083814528942080.0\n",
      "Epoch [1066/1500], Training Loss: 1081383463702745.0, Validation Loss: 1083814394724352.0\n",
      "Epoch [1067/1500], Training Loss: 1081383199264074.1, Validation Loss: 1083814327615488.0\n",
      "Epoch [1068/1500], Training Loss: 1081382909700997.2, Validation Loss: 1083813992071168.0\n",
      "Epoch [1069/1500], Training Loss: 1081382681081245.2, Validation Loss: 1083813723635712.0\n",
      "Epoch [1070/1500], Training Loss: 1081382399039280.0, Validation Loss: 1083813522309120.0\n",
      "Epoch [1071/1500], Training Loss: 1081382138810572.5, Validation Loss: 1083813186764800.0\n",
      "Epoch [1072/1500], Training Loss: 1081381843538799.9, Validation Loss: 1083812649893888.0\n",
      "Epoch [1073/1500], Training Loss: 1081381577621992.8, Validation Loss: 1083812448567296.0\n",
      "Epoch [1074/1500], Training Loss: 1081381271393838.1, Validation Loss: 1083812247240704.0\n",
      "Epoch [1075/1500], Training Loss: 1081381009340058.1, Validation Loss: 1083812045914112.0\n",
      "Epoch [1076/1500], Training Loss: 1081380751154068.2, Validation Loss: 1083811844587520.0\n",
      "Epoch [1077/1500], Training Loss: 1081380449386291.8, Validation Loss: 1083811441934336.0\n",
      "Epoch [1078/1500], Training Loss: 1081380188801578.5, Validation Loss: 1083810972172288.0\n",
      "Epoch [1079/1500], Training Loss: 1081379899285947.5, Validation Loss: 1083810837954560.0\n",
      "Epoch [1080/1500], Training Loss: 1081379636558571.2, Validation Loss: 1083810435301376.0\n",
      "Epoch [1081/1500], Training Loss: 1081379333047705.5, Validation Loss: 1083810032648192.0\n",
      "Epoch [1082/1500], Training Loss: 1081379096108966.2, Validation Loss: 1083809831321600.0\n",
      "Epoch [1083/1500], Training Loss: 1081378833505558.8, Validation Loss: 1083809697103872.0\n",
      "Epoch [1084/1500], Training Loss: 1081378568773972.5, Validation Loss: 1083809495777280.0\n",
      "Epoch [1085/1500], Training Loss: 1081378270558981.6, Validation Loss: 1083809227341824.0\n",
      "Epoch [1086/1500], Training Loss: 1081378009451735.8, Validation Loss: 1083808891797504.0\n",
      "Epoch [1087/1500], Training Loss: 1081377709451163.4, Validation Loss: 1083808824688640.0\n",
      "Epoch [1088/1500], Training Loss: 1081377442098021.8, Validation Loss: 1083808556253184.0\n",
      "Epoch [1089/1500], Training Loss: 1081377180889230.5, Validation Loss: 1083808354926592.0\n",
      "Epoch [1090/1500], Training Loss: 1081376885250817.6, Validation Loss: 1083807952273408.0\n",
      "Epoch [1091/1500], Training Loss: 1081376621043386.4, Validation Loss: 1083807683837952.0\n",
      "Epoch [1092/1500], Training Loss: 1081376327040175.0, Validation Loss: 1083807281184768.0\n",
      "Epoch [1093/1500], Training Loss: 1081376071002812.5, Validation Loss: 1083806878531584.0\n",
      "Epoch [1094/1500], Training Loss: 1081375767444499.5, Validation Loss: 1083806542987264.0\n",
      "Epoch [1095/1500], Training Loss: 1081375516753633.8, Validation Loss: 1083806341660672.0\n",
      "Epoch [1096/1500], Training Loss: 1081375254751507.8, Validation Loss: 1083805804789760.0\n",
      "Epoch [1097/1500], Training Loss: 1081375007709492.0, Validation Loss: 1083805603463168.0\n",
      "Epoch [1098/1500], Training Loss: 1081374705845056.6, Validation Loss: 1083805200809984.0\n",
      "Epoch [1099/1500], Training Loss: 1081374443947871.8, Validation Loss: 1083805133701120.0\n",
      "Epoch [1100/1500], Training Loss: 1081374146491464.0, Validation Loss: 1083804731047936.0\n",
      "Epoch [1101/1500], Training Loss: 1081373875137167.8, Validation Loss: 1083804395503616.0\n",
      "Epoch [1102/1500], Training Loss: 1081373607067346.5, Validation Loss: 1083804059959296.0\n",
      "Epoch [1103/1500], Training Loss: 1081373319488467.8, Validation Loss: 1083803791523840.0\n",
      "Epoch [1104/1500], Training Loss: 1081373053184395.0, Validation Loss: 1083803657306112.0\n",
      "Epoch [1105/1500], Training Loss: 1081372755231324.9, Validation Loss: 1083803388870656.0\n",
      "Epoch [1106/1500], Training Loss: 1081372504471684.4, Validation Loss: 1083803187544064.0\n",
      "Epoch [1107/1500], Training Loss: 1081372206202222.8, Validation Loss: 1083802851999744.0\n",
      "Epoch [1108/1500], Training Loss: 1081371940059255.0, Validation Loss: 1083802516455424.0\n",
      "Epoch [1109/1500], Training Loss: 1081371670010751.9, Validation Loss: 1083802449346560.0\n",
      "Epoch [1110/1500], Training Loss: 1081371441591268.2, Validation Loss: 1083802248019968.0\n",
      "Epoch [1111/1500], Training Loss: 1081371137576166.8, Validation Loss: 1083801711149056.0\n",
      "Epoch [1112/1500], Training Loss: 1081370876238800.4, Validation Loss: 1083801442713600.0\n",
      "Epoch [1113/1500], Training Loss: 1081370580109769.8, Validation Loss: 1083801308495872.0\n",
      "Epoch [1114/1500], Training Loss: 1081370311744705.4, Validation Loss: 1083801040060416.0\n",
      "Epoch [1115/1500], Training Loss: 1081370031505556.4, Validation Loss: 1083800838733824.0\n",
      "Epoch [1116/1500], Training Loss: 1081369757607778.9, Validation Loss: 1083800503189504.0\n",
      "Epoch [1117/1500], Training Loss: 1081369489558058.5, Validation Loss: 1083800100536320.0\n",
      "Epoch [1118/1500], Training Loss: 1081369188405961.2, Validation Loss: 1083799899209728.0\n",
      "Epoch [1119/1500], Training Loss: 1081368937215102.0, Validation Loss: 1083799697883136.0\n",
      "Epoch [1120/1500], Training Loss: 1081368640832024.5, Validation Loss: 1083799362338816.0\n",
      "Epoch [1121/1500], Training Loss: 1081368371049246.2, Validation Loss: 1083799161012224.0\n",
      "Epoch [1122/1500], Training Loss: 1081368088455648.1, Validation Loss: 1083798959685632.0\n",
      "Epoch [1123/1500], Training Loss: 1081367865174010.4, Validation Loss: 1083798758359040.0\n",
      "Epoch [1124/1500], Training Loss: 1081367572638825.5, Validation Loss: 1083798557032448.0\n",
      "Epoch [1125/1500], Training Loss: 1081367307983040.6, Validation Loss: 1083798355705856.0\n",
      "Epoch [1126/1500], Training Loss: 1081367015492496.6, Validation Loss: 1083798221488128.0\n",
      "Epoch [1127/1500], Training Loss: 1081366749602270.8, Validation Loss: 1083797953052672.0\n",
      "Epoch [1128/1500], Training Loss: 1081366450038810.2, Validation Loss: 1083797550399488.0\n",
      "Epoch [1129/1500], Training Loss: 1081366185194068.5, Validation Loss: 1083797147746304.0\n",
      "Epoch [1130/1500], Training Loss: 1081365925112431.5, Validation Loss: 1083796946419712.0\n",
      "Epoch [1131/1500], Training Loss: 1081365621730148.8, Validation Loss: 1083796610875392.0\n",
      "Epoch [1132/1500], Training Loss: 1081365364735387.5, Validation Loss: 1083796543766528.0\n",
      "Epoch [1133/1500], Training Loss: 1081365076510646.8, Validation Loss: 1083796074004480.0\n",
      "Epoch [1134/1500], Training Loss: 1081364811281062.0, Validation Loss: 1083795604242432.0\n",
      "Epoch [1135/1500], Training Loss: 1081364510469243.0, Validation Loss: 1083795402915840.0\n",
      "Epoch [1136/1500], Training Loss: 1081364282686492.9, Validation Loss: 1083795067371520.0\n",
      "Epoch [1137/1500], Training Loss: 1081364008031039.8, Validation Loss: 1083794866044928.0\n",
      "Epoch [1138/1500], Training Loss: 1081363741419244.8, Validation Loss: 1083794463391744.0\n",
      "Epoch [1139/1500], Training Loss: 1081363447581445.5, Validation Loss: 1083794396282880.0\n",
      "Epoch [1140/1500], Training Loss: 1081363183825513.9, Validation Loss: 1083794127847424.0\n",
      "Epoch [1141/1500], Training Loss: 1081362881684631.1, Validation Loss: 1083793926520832.0\n",
      "Epoch [1142/1500], Training Loss: 1081362618987398.5, Validation Loss: 1083793859411968.0\n",
      "Epoch [1143/1500], Training Loss: 1081362356907355.5, Validation Loss: 1083793456758784.0\n",
      "Epoch [1144/1500], Training Loss: 1081362058483613.2, Validation Loss: 1083793121214464.0\n",
      "Epoch [1145/1500], Training Loss: 1081361796885125.0, Validation Loss: 1083792852779008.0\n",
      "Epoch [1146/1500], Training Loss: 1081361507071151.0, Validation Loss: 1083792517234688.0\n",
      "Epoch [1147/1500], Training Loss: 1081361244135380.2, Validation Loss: 1083792315908096.0\n",
      "Epoch [1148/1500], Training Loss: 1081360938018089.1, Validation Loss: 1083791980363776.0\n",
      "Epoch [1149/1500], Training Loss: 1081360701598919.5, Validation Loss: 1083791779037184.0\n",
      "Epoch [1150/1500], Training Loss: 1081360438140756.8, Validation Loss: 1083791376384000.0\n",
      "Epoch [1151/1500], Training Loss: 1081360177571816.2, Validation Loss: 1083791107948544.0\n",
      "Epoch [1152/1500], Training Loss: 1081359879204516.6, Validation Loss: 1083790973730816.0\n",
      "Epoch [1153/1500], Training Loss: 1081359619070547.0, Validation Loss: 1083790772404224.0\n",
      "Epoch [1154/1500], Training Loss: 1081359321076326.8, Validation Loss: 1083790571077632.0\n",
      "Epoch [1155/1500], Training Loss: 1081359053122187.4, Validation Loss: 1083790168424448.0\n",
      "Epoch [1156/1500], Training Loss: 1081358781872875.8, Validation Loss: 1083789631553536.0\n",
      "Epoch [1157/1500], Training Loss: 1081358492437270.4, Validation Loss: 1083789363118080.0\n",
      "Epoch [1158/1500], Training Loss: 1081358227795850.0, Validation Loss: 1083789228900352.0\n",
      "Epoch [1159/1500], Training Loss: 1081357935026748.1, Validation Loss: 1083788759138304.0\n",
      "Epoch [1160/1500], Training Loss: 1081357676657640.0, Validation Loss: 1083788423593984.0\n",
      "Epoch [1161/1500], Training Loss: 1081357374388928.0, Validation Loss: 1083788020940800.0\n",
      "Epoch [1162/1500], Training Loss: 1081357121631606.9, Validation Loss: 1083787819614208.0\n",
      "Epoch [1163/1500], Training Loss: 1081356857924169.9, Validation Loss: 1083787618287616.0\n",
      "Epoch [1164/1500], Training Loss: 1081356612502374.8, Validation Loss: 1083787416961024.0\n",
      "Epoch [1165/1500], Training Loss: 1081356311982298.5, Validation Loss: 1083787215634432.0\n",
      "Epoch [1166/1500], Training Loss: 1081356051542340.2, Validation Loss: 1083787215634432.0\n",
      "Epoch [1167/1500], Training Loss: 1081355756258867.2, Validation Loss: 1083786812981248.0\n",
      "Epoch [1168/1500], Training Loss: 1081355484339480.2, Validation Loss: 1083786678763520.0\n",
      "Epoch [1169/1500], Training Loss: 1081355212351870.2, Validation Loss: 1083786276110336.0\n",
      "Epoch [1170/1500], Training Loss: 1081354930217661.2, Validation Loss: 1083785739239424.0\n",
      "Epoch [1171/1500], Training Loss: 1081354664473725.8, Validation Loss: 1083785537912832.0\n",
      "Epoch [1172/1500], Training Loss: 1081354366227992.2, Validation Loss: 1083785068150784.0\n",
      "Epoch [1173/1500], Training Loss: 1081354110901762.1, Validation Loss: 1083784866824192.0\n",
      "Epoch [1174/1500], Training Loss: 1081353812464609.0, Validation Loss: 1083784665497600.0\n",
      "Epoch [1175/1500], Training Loss: 1081353546711500.0, Validation Loss: 1083784329953280.0\n",
      "Epoch [1176/1500], Training Loss: 1081353277020236.1, Validation Loss: 1083784128626688.0\n",
      "Epoch [1177/1500], Training Loss: 1081353047629902.0, Validation Loss: 1083783591755776.0\n",
      "Epoch [1178/1500], Training Loss: 1081352744704361.1, Validation Loss: 1083783390429184.0\n",
      "Epoch [1179/1500], Training Loss: 1081352484501874.2, Validation Loss: 1083783121993728.0\n",
      "Epoch [1180/1500], Training Loss: 1081352190384032.9, Validation Loss: 1083782920667136.0\n",
      "Epoch [1181/1500], Training Loss: 1081351920607089.4, Validation Loss: 1083782719340544.0\n",
      "Epoch [1182/1500], Training Loss: 1081351635872691.5, Validation Loss: 1083782383796224.0\n",
      "Epoch [1183/1500], Training Loss: 1081351362263164.0, Validation Loss: 1083782182469632.0\n",
      "Epoch [1184/1500], Training Loss: 1081351099543553.2, Validation Loss: 1083782115360768.0\n",
      "Epoch [1185/1500], Training Loss: 1081350797331319.9, Validation Loss: 1083781846925312.0\n",
      "Epoch [1186/1500], Training Loss: 1081350540258141.8, Validation Loss: 1083781578489856.0\n",
      "Epoch [1187/1500], Training Loss: 1081350246871221.6, Validation Loss: 1083781377163264.0\n",
      "Epoch [1188/1500], Training Loss: 1081349980629434.9, Validation Loss: 1083781175836672.0\n",
      "Epoch [1189/1500], Training Loss: 1081349693245371.9, Validation Loss: 1083780840292352.0\n",
      "Epoch [1190/1500], Training Loss: 1081349465305766.8, Validation Loss: 1083780638965760.0\n",
      "Epoch [1191/1500], Training Loss: 1081349180656328.5, Validation Loss: 1083780236312576.0\n",
      "Epoch [1192/1500], Training Loss: 1081348917912958.5, Validation Loss: 1083780034985984.0\n",
      "Epoch [1193/1500], Training Loss: 1081348622247554.9, Validation Loss: 1083779833659392.0\n",
      "Epoch [1194/1500], Training Loss: 1081348357531788.1, Validation Loss: 1083779498115072.0\n",
      "Epoch [1195/1500], Training Loss: 1081348052877896.5, Validation Loss: 1083779095461888.0\n",
      "Epoch [1196/1500], Training Loss: 1081347794275963.8, Validation Loss: 1083778491482112.0\n",
      "Epoch [1197/1500], Training Loss: 1081347534782005.6, Validation Loss: 1083778290155520.0\n",
      "Epoch [1198/1500], Training Loss: 1081347230788088.0, Validation Loss: 1083778088828928.0\n",
      "Epoch [1199/1500], Training Loss: 1081346969177577.6, Validation Loss: 1083777686175744.0\n",
      "Epoch [1200/1500], Training Loss: 1081346679645336.8, Validation Loss: 1083777484849152.0\n",
      "Epoch [1201/1500], Training Loss: 1081346415840758.8, Validation Loss: 1083777283522560.0\n",
      "Epoch [1202/1500], Training Loss: 1081346117045877.0, Validation Loss: 1083776947978240.0\n",
      "Epoch [1203/1500], Training Loss: 1081345883705843.8, Validation Loss: 1083776679542784.0\n",
      "Epoch [1204/1500], Training Loss: 1081345617064611.2, Validation Loss: 1083776679542784.0\n",
      "Epoch [1205/1500], Training Loss: 1081345353350920.0, Validation Loss: 1083776276889600.0\n",
      "Epoch [1206/1500], Training Loss: 1081345055673318.1, Validation Loss: 1083775941345280.0\n",
      "Epoch [1207/1500], Training Loss: 1081344792502453.6, Validation Loss: 1083775605800960.0\n",
      "Epoch [1208/1500], Training Loss: 1081344490113022.8, Validation Loss: 1083775538692096.0\n",
      "Epoch [1209/1500], Training Loss: 1081344224684771.1, Validation Loss: 1083775001821184.0\n",
      "Epoch [1210/1500], Training Loss: 1081343965148952.6, Validation Loss: 1083774733385728.0\n",
      "Epoch [1211/1500], Training Loss: 1081343667877732.8, Validation Loss: 1083774532059136.0\n",
      "Epoch [1212/1500], Training Loss: 1081343403353239.0, Validation Loss: 1083774330732544.0\n",
      "Epoch [1213/1500], Training Loss: 1081343112111121.5, Validation Loss: 1083774129405952.0\n",
      "Epoch [1214/1500], Training Loss: 1081342855988755.6, Validation Loss: 1083773928079360.0\n",
      "Epoch [1215/1500], Training Loss: 1081342549537471.0, Validation Loss: 1083773793861632.0\n",
      "Epoch [1216/1500], Training Loss: 1081342303637387.6, Validation Loss: 1083773458317312.0\n",
      "Epoch [1217/1500], Training Loss: 1081342041491117.8, Validation Loss: 1083773189881856.0\n",
      "Epoch [1218/1500], Training Loss: 1081341787124194.6, Validation Loss: 1083772988555264.0\n",
      "Epoch [1219/1500], Training Loss: 1081341488948719.5, Validation Loss: 1083772653010944.0\n",
      "Epoch [1220/1500], Training Loss: 1081341227593744.5, Validation Loss: 1083772384575488.0\n",
      "Epoch [1221/1500], Training Loss: 1081340927340604.0, Validation Loss: 1083772183248896.0\n",
      "Epoch [1222/1500], Training Loss: 1081340657288348.0, Validation Loss: 1083771981922304.0\n",
      "Epoch [1223/1500], Training Loss: 1081340390169182.2, Validation Loss: 1083771847704576.0\n",
      "Epoch [1224/1500], Training Loss: 1081340101777832.5, Validation Loss: 1083771512160256.0\n",
      "Epoch [1225/1500], Training Loss: 1081339835696794.1, Validation Loss: 1083771042398208.0\n",
      "Epoch [1226/1500], Training Loss: 1081339539586584.0, Validation Loss: 1083770639745024.0\n",
      "Epoch [1227/1500], Training Loss: 1081339287737788.0, Validation Loss: 1083770237091840.0\n",
      "Epoch [1228/1500], Training Loss: 1081338986335928.6, Validation Loss: 1083769834438656.0\n",
      "Epoch [1229/1500], Training Loss: 1081338724698106.2, Validation Loss: 1083769566003200.0\n",
      "Epoch [1230/1500], Training Loss: 1081338458761491.1, Validation Loss: 1083768894914560.0\n",
      "Epoch [1231/1500], Training Loss: 1081338222773183.6, Validation Loss: 1083768760696832.0\n",
      "Epoch [1232/1500], Training Loss: 1081337921218162.8, Validation Loss: 1083768492261376.0\n",
      "Epoch [1233/1500], Training Loss: 1081337659447282.1, Validation Loss: 1083768156717056.0\n",
      "Epoch [1234/1500], Training Loss: 1081337362902946.9, Validation Loss: 1083767955390464.0\n",
      "Epoch [1235/1500], Training Loss: 1081337095552387.4, Validation Loss: 1083767552737280.0\n",
      "Epoch [1236/1500], Training Loss: 1081336818460540.2, Validation Loss: 1083767351410688.0\n",
      "Epoch [1237/1500], Training Loss: 1081336539595874.5, Validation Loss: 1083767150084096.0\n",
      "Epoch [1238/1500], Training Loss: 1081336271822727.5, Validation Loss: 1083767015866368.0\n",
      "Epoch [1239/1500], Training Loss: 1081335972753332.5, Validation Loss: 1083766814539776.0\n",
      "Epoch [1240/1500], Training Loss: 1081335719742972.6, Validation Loss: 1083766747430912.0\n",
      "Epoch [1241/1500], Training Loss: 1081335421806316.5, Validation Loss: 1083766411886592.0\n",
      "Epoch [1242/1500], Training Loss: 1081335152123444.9, Validation Loss: 1083766210560000.0\n",
      "Epoch [1243/1500], Training Loss: 1081334876896690.4, Validation Loss: 1083765942124544.0\n",
      "Epoch [1244/1500], Training Loss: 1081334653443459.1, Validation Loss: 1083765606580224.0\n",
      "Epoch [1245/1500], Training Loss: 1081334355630891.0, Validation Loss: 1083765271035904.0\n",
      "Epoch [1246/1500], Training Loss: 1081334090799187.8, Validation Loss: 1083765069709312.0\n",
      "Epoch [1247/1500], Training Loss: 1081333796821325.2, Validation Loss: 1083764667056128.0\n",
      "Epoch [1248/1500], Training Loss: 1081333529744485.6, Validation Loss: 1083764465729536.0\n",
      "Epoch [1249/1500], Training Loss: 1081333240735210.9, Validation Loss: 1083764264402944.0\n",
      "Epoch [1250/1500], Training Loss: 1081332969362690.2, Validation Loss: 1083763794640896.0\n",
      "Epoch [1251/1500], Training Loss: 1081332707865291.5, Validation Loss: 1083763459096576.0\n",
      "Epoch [1252/1500], Training Loss: 1081332406101822.1, Validation Loss: 1083763056443392.0\n",
      "Epoch [1253/1500], Training Loss: 1081332150138802.5, Validation Loss: 1083762720899072.0\n",
      "Epoch [1254/1500], Training Loss: 1081331857472733.6, Validation Loss: 1083762452463616.0\n",
      "Epoch [1255/1500], Training Loss: 1081331589351989.5, Validation Loss: 1083762251137024.0\n",
      "Epoch [1256/1500], Training Loss: 1081331297352250.8, Validation Loss: 1083761915592704.0\n",
      "Epoch [1257/1500], Training Loss: 1081331071146892.9, Validation Loss: 1083761714266112.0\n",
      "Epoch [1258/1500], Training Loss: 1081330792042819.5, Validation Loss: 1083761512939520.0\n",
      "Epoch [1259/1500], Training Loss: 1081330527358537.1, Validation Loss: 1083761177395200.0\n",
      "Epoch [1260/1500], Training Loss: 1081330230868556.0, Validation Loss: 1083760707633152.0\n",
      "Epoch [1261/1500], Training Loss: 1081329968411406.2, Validation Loss: 1083760372088832.0\n",
      "Epoch [1262/1500], Training Loss: 1081329664482176.5, Validation Loss: 1083760170762240.0\n",
      "Epoch [1263/1500], Training Loss: 1081329402879689.5, Validation Loss: 1083759902326784.0\n",
      "Epoch [1264/1500], Training Loss: 1081329139980024.8, Validation Loss: 1083759566782464.0\n",
      "Epoch [1265/1500], Training Loss: 1081328839670815.8, Validation Loss: 1083759365455872.0\n",
      "Epoch [1266/1500], Training Loss: 1081328578851400.5, Validation Loss: 1083759164129280.0\n",
      "Epoch [1267/1500], Training Loss: 1081328289794471.8, Validation Loss: 1083758962802688.0\n",
      "Epoch [1268/1500], Training Loss: 1081328026386946.6, Validation Loss: 1083758425931776.0\n",
      "Epoch [1269/1500], Training Loss: 1081327722181233.2, Validation Loss: 1083758224605184.0\n",
      "Epoch [1270/1500], Training Loss: 1081327487647439.9, Validation Loss: 1083758023278592.0\n",
      "Epoch [1271/1500], Training Loss: 1081327226198189.4, Validation Loss: 1083757821952000.0\n",
      "Epoch [1272/1500], Training Loss: 1081326959701020.6, Validation Loss: 1083757620625408.0\n",
      "Epoch [1273/1500], Training Loss: 1081326661442543.5, Validation Loss: 1083757419298816.0\n",
      "Epoch [1274/1500], Training Loss: 1081326402620467.0, Validation Loss: 1083757285081088.0\n",
      "Epoch [1275/1500], Training Loss: 1081326100906337.8, Validation Loss: 1083757083754496.0\n",
      "Epoch [1276/1500], Training Loss: 1081325834153318.4, Validation Loss: 1083756815319040.0\n",
      "Epoch [1277/1500], Training Loss: 1081325571056029.4, Validation Loss: 1083756412665856.0\n",
      "Epoch [1278/1500], Training Loss: 1081325274679944.1, Validation Loss: 1083756278448128.0\n",
      "Epoch [1279/1500], Training Loss: 1081325011590394.1, Validation Loss: 1083756077121536.0\n",
      "Epoch [1280/1500], Training Loss: 1081324719283713.8, Validation Loss: 1083755875794944.0\n",
      "Epoch [1281/1500], Training Loss: 1081324460822408.5, Validation Loss: 1083755674468352.0\n",
      "Epoch [1282/1500], Training Loss: 1081324156636569.5, Validation Loss: 1083755473141760.0\n",
      "Epoch [1283/1500], Training Loss: 1081323906324943.0, Validation Loss: 1083755271815168.0\n",
      "Epoch [1284/1500], Training Loss: 1081323646211353.1, Validation Loss: 1083755070488576.0\n",
      "Epoch [1285/1500], Training Loss: 1081323395884501.4, Validation Loss: 1083754869161984.0\n",
      "Epoch [1286/1500], Training Loss: 1081323095638453.6, Validation Loss: 1083754466508800.0\n",
      "Epoch [1287/1500], Training Loss: 1081322836708360.9, Validation Loss: 1083754265182208.0\n",
      "Epoch [1288/1500], Training Loss: 1081322541634471.8, Validation Loss: 1083754130964480.0\n",
      "Epoch [1289/1500], Training Loss: 1081322265068712.2, Validation Loss: 1083753661202432.0\n",
      "Epoch [1290/1500], Training Loss: 1081321997137290.9, Validation Loss: 1083753459875840.0\n",
      "Epoch [1291/1500], Training Loss: 1081321709896525.1, Validation Loss: 1083753191440384.0\n",
      "Epoch [1292/1500], Training Loss: 1081321445746136.1, Validation Loss: 1083752788787200.0\n",
      "Epoch [1293/1500], Training Loss: 1081321148324076.9, Validation Loss: 1083752386134016.0\n",
      "Epoch [1294/1500], Training Loss: 1081320895138963.6, Validation Loss: 1083752117698560.0\n",
      "Epoch [1295/1500], Training Loss: 1081320595027855.1, Validation Loss: 1083751983480832.0\n",
      "Epoch [1296/1500], Training Loss: 1081320331914880.8, Validation Loss: 1083751580827648.0\n",
      "Epoch [1297/1500], Training Loss: 1081320063844081.1, Validation Loss: 1083751379501056.0\n",
      "Epoch [1298/1500], Training Loss: 1081319830809350.8, Validation Loss: 1083751178174464.0\n",
      "Epoch [1299/1500], Training Loss: 1081319528533627.8, Validation Loss: 1083750439976960.0\n",
      "Epoch [1300/1500], Training Loss: 1081319268285501.8, Validation Loss: 1083750238650368.0\n",
      "Epoch [1301/1500], Training Loss: 1081318971889981.2, Validation Loss: 1083749970214912.0\n",
      "Epoch [1302/1500], Training Loss: 1081318702011617.1, Validation Loss: 1083749768888320.0\n",
      "Epoch [1303/1500], Training Loss: 1081318422251994.5, Validation Loss: 1083749567561728.0\n",
      "Epoch [1304/1500], Training Loss: 1081318147402722.4, Validation Loss: 1083749232017408.0\n",
      "Epoch [1305/1500], Training Loss: 1081317881343270.4, Validation Loss: 1083748829364224.0\n",
      "Epoch [1306/1500], Training Loss: 1081317583073172.9, Validation Loss: 1083748628037632.0\n",
      "Epoch [1307/1500], Training Loss: 1081317328507909.6, Validation Loss: 1083748225384448.0\n",
      "Epoch [1308/1500], Training Loss: 1081317031975448.5, Validation Loss: 1083748024057856.0\n",
      "Epoch [1309/1500], Training Loss: 1081316764398411.2, Validation Loss: 1083747822731264.0\n",
      "Epoch [1310/1500], Training Loss: 1081316483659119.9, Validation Loss: 1083747487186944.0\n",
      "Epoch [1311/1500], Training Loss: 1081316256828800.5, Validation Loss: 1083747285860352.0\n",
      "Epoch [1312/1500], Training Loss: 1081315962329829.8, Validation Loss: 1083746883207168.0\n",
      "Epoch [1313/1500], Training Loss: 1081315701778399.8, Validation Loss: 1083746748989440.0\n",
      "Epoch [1314/1500], Training Loss: 1081315406582833.9, Validation Loss: 1083746547662848.0\n",
      "Epoch [1315/1500], Training Loss: 1081315141465317.8, Validation Loss: 1083746480553984.0\n",
      "Epoch [1316/1500], Training Loss: 1081314839978624.2, Validation Loss: 1083746279227392.0\n",
      "Epoch [1317/1500], Training Loss: 1081314578247690.4, Validation Loss: 1083745742356480.0\n",
      "Epoch [1318/1500], Training Loss: 1081314318280316.1, Validation Loss: 1083745541029888.0\n",
      "Epoch [1319/1500], Training Loss: 1081314015239394.5, Validation Loss: 1083745272594432.0\n",
      "Epoch [1320/1500], Training Loss: 1081313754065135.6, Validation Loss: 1083744937050112.0\n",
      "Epoch [1321/1500], Training Loss: 1081313464928228.2, Validation Loss: 1083744735723520.0\n",
      "Epoch [1322/1500], Training Loss: 1081313200528619.1, Validation Loss: 1083744400179200.0\n",
      "Epoch [1323/1500], Training Loss: 1081312904697583.9, Validation Loss: 1083744198852608.0\n",
      "Epoch [1324/1500], Training Loss: 1081312674819916.9, Validation Loss: 1083743930417152.0\n",
      "Epoch [1325/1500], Training Loss: 1081312399006887.8, Validation Loss: 1083743594872832.0\n",
      "Epoch [1326/1500], Training Loss: 1081312136438369.4, Validation Loss: 1083743125110784.0\n",
      "Epoch [1327/1500], Training Loss: 1081311839244096.8, Validation Loss: 1083742990893056.0\n",
      "Epoch [1328/1500], Training Loss: 1081311575679965.8, Validation Loss: 1083742655348736.0\n",
      "Epoch [1329/1500], Training Loss: 1081311273149383.6, Validation Loss: 1083742454022144.0\n",
      "Epoch [1330/1500], Training Loss: 1081311009178556.4, Validation Loss: 1083742051368960.0\n",
      "Epoch [1331/1500], Training Loss: 1081310749777340.0, Validation Loss: 1083741782933504.0\n",
      "Epoch [1332/1500], Training Loss: 1081310451869122.1, Validation Loss: 1083741447389184.0\n",
      "Epoch [1333/1500], Training Loss: 1081310188312077.2, Validation Loss: 1083741246062592.0\n",
      "Epoch [1334/1500], Training Loss: 1081309895705099.6, Validation Loss: 1083740910518272.0\n",
      "Epoch [1335/1500], Training Loss: 1081309638783092.8, Validation Loss: 1083740709191680.0\n",
      "Epoch [1336/1500], Training Loss: 1081309333389567.9, Validation Loss: 1083740507865088.0\n",
      "Epoch [1337/1500], Training Loss: 1081309091913953.0, Validation Loss: 1083740306538496.0\n",
      "Epoch [1338/1500], Training Loss: 1081308827890397.5, Validation Loss: 1083740038103040.0\n",
      "Epoch [1339/1500], Training Loss: 1081308569053203.4, Validation Loss: 1083739836776448.0\n",
      "Epoch [1340/1500], Training Loss: 1081308273380675.4, Validation Loss: 1083739635449856.0\n",
      "Epoch [1341/1500], Training Loss: 1081308010805857.9, Validation Loss: 1083739434123264.0\n",
      "Epoch [1342/1500], Training Loss: 1081307710498264.8, Validation Loss: 1083739031470080.0\n",
      "Epoch [1343/1500], Training Loss: 1081307441727098.1, Validation Loss: 1083738695925760.0\n",
      "Epoch [1344/1500], Training Loss: 1081307175383596.9, Validation Loss: 1083738159054848.0\n",
      "Epoch [1345/1500], Training Loss: 1081306884710575.4, Validation Loss: 1083737957728256.0\n",
      "Epoch [1346/1500], Training Loss: 1081306619292811.5, Validation Loss: 1083737555075072.0\n",
      "Epoch [1347/1500], Training Loss: 1081306325698330.6, Validation Loss: 1083737152421888.0\n",
      "Epoch [1348/1500], Training Loss: 1081306071973447.0, Validation Loss: 1083736951095296.0\n",
      "Epoch [1349/1500], Training Loss: 1081305768249860.0, Validation Loss: 1083736615550976.0\n",
      "Epoch [1350/1500], Training Loss: 1081305513168511.2, Validation Loss: 1083736414224384.0\n",
      "Epoch [1351/1500], Training Loss: 1081305247717637.0, Validation Loss: 1083736011571200.0\n",
      "Epoch [1352/1500], Training Loss: 1081305006247343.9, Validation Loss: 1083735944462336.0\n",
      "Epoch [1353/1500], Training Loss: 1081304705157705.1, Validation Loss: 1083735810244608.0\n",
      "Epoch [1354/1500], Training Loss: 1081304443137891.0, Validation Loss: 1083735541809152.0\n",
      "Epoch [1355/1500], Training Loss: 1081304147928788.1, Validation Loss: 1083735004938240.0\n",
      "Epoch [1356/1500], Training Loss: 1081303877118254.9, Validation Loss: 1083734803611648.0\n",
      "Epoch [1357/1500], Training Loss: 1081303606452774.0, Validation Loss: 1083734468067328.0\n",
      "Epoch [1358/1500], Training Loss: 1081303324186341.2, Validation Loss: 1083734266740736.0\n",
      "Epoch [1359/1500], Training Loss: 1081303056588996.9, Validation Loss: 1083733864087552.0\n",
      "Epoch [1360/1500], Training Loss: 1081302755509645.8, Validation Loss: 1083733796978688.0\n",
      "Epoch [1361/1500], Training Loss: 1081302501293531.0, Validation Loss: 1083733192998912.0\n",
      "Epoch [1362/1500], Training Loss: 1081302199591405.9, Validation Loss: 1083732723236864.0\n",
      "Epoch [1363/1500], Training Loss: 1081301927836704.1, Validation Loss: 1083732454801408.0\n",
      "Epoch [1364/1500], Training Loss: 1081301621973842.5, Validation Loss: 1083732253474816.0\n",
      "Epoch [1365/1500], Training Loss: 1081301365616832.0, Validation Loss: 1083732052148224.0\n",
      "Epoch [1366/1500], Training Loss: 1081301081897595.0, Validation Loss: 1083731850821632.0\n",
      "Epoch [1367/1500], Training Loss: 1081300838653538.2, Validation Loss: 1083731515277312.0\n",
      "Epoch [1368/1500], Training Loss: 1081300539360284.9, Validation Loss: 1083731246841856.0\n",
      "Epoch [1369/1500], Training Loss: 1081300273457262.9, Validation Loss: 1083730911297536.0\n",
      "Epoch [1370/1500], Training Loss: 1081299969487450.9, Validation Loss: 1083730844188672.0\n",
      "Epoch [1371/1500], Training Loss: 1081299703741686.8, Validation Loss: 1083730575753216.0\n",
      "Epoch [1372/1500], Training Loss: 1081299399833653.4, Validation Loss: 1083730307317760.0\n",
      "Epoch [1373/1500], Training Loss: 1081299129895220.5, Validation Loss: 1083729971773440.0\n",
      "Epoch [1374/1500], Training Loss: 1081298820709117.5, Validation Loss: 1083729770446848.0\n",
      "Epoch [1375/1500], Training Loss: 1081298554694090.9, Validation Loss: 1083729569120256.0\n",
      "Epoch [1376/1500], Training Loss: 1081298283665812.9, Validation Loss: 1083729300684800.0\n",
      "Epoch [1377/1500], Training Loss: 1081297986535381.1, Validation Loss: 1083729166467072.0\n",
      "Epoch [1378/1500], Training Loss: 1081297720265942.0, Validation Loss: 1083728898031616.0\n",
      "Epoch [1379/1500], Training Loss: 1081297412685997.6, Validation Loss: 1083728361160704.0\n",
      "Epoch [1380/1500], Training Loss: 1081297147882765.9, Validation Loss: 1083728159834112.0\n",
      "Epoch [1381/1500], Training Loss: 1081296850458110.5, Validation Loss: 1083727824289792.0\n",
      "Epoch [1382/1500], Training Loss: 1081296582301897.5, Validation Loss: 1083727622963200.0\n",
      "Epoch [1383/1500], Training Loss: 1081296277901180.4, Validation Loss: 1083727421636608.0\n",
      "Epoch [1384/1500], Training Loss: 1081296006713222.0, Validation Loss: 1083727153201152.0\n",
      "Epoch [1385/1500], Training Loss: 1081295719445726.9, Validation Loss: 1083726817656832.0\n",
      "Epoch [1386/1500], Training Loss: 1081295475731612.2, Validation Loss: 1083726280785920.0\n",
      "Epoch [1387/1500], Training Loss: 1081295195410191.5, Validation Loss: 1083725878132736.0\n",
      "Epoch [1388/1500], Training Loss: 1081294927019427.6, Validation Loss: 1083725878132736.0\n",
      "Epoch [1389/1500], Training Loss: 1081294621133626.0, Validation Loss: 1083725475479552.0\n",
      "Epoch [1390/1500], Training Loss: 1081294355667684.0, Validation Loss: 1083725207044096.0\n",
      "Epoch [1391/1500], Training Loss: 1081294057783562.9, Validation Loss: 1083725005717504.0\n",
      "Epoch [1392/1500], Training Loss: 1081293785335925.2, Validation Loss: 1083724804390912.0\n",
      "Epoch [1393/1500], Training Loss: 1081293477681465.6, Validation Loss: 1083724535955456.0\n",
      "Epoch [1394/1500], Training Loss: 1081293206502375.1, Validation Loss: 1083724334628864.0\n",
      "Epoch [1395/1500], Training Loss: 1081292931497016.1, Validation Loss: 1083724066193408.0\n",
      "Epoch [1396/1500], Training Loss: 1081292641622776.2, Validation Loss: 1083723663540224.0\n",
      "Epoch [1397/1500], Training Loss: 1081292370752000.0, Validation Loss: 1083723327995904.0\n",
      "Epoch [1398/1500], Training Loss: 1081292066029432.8, Validation Loss: 1083723059560448.0\n",
      "Epoch [1399/1500], Training Loss: 1081291801177869.0, Validation Loss: 1083722724016128.0\n",
      "Epoch [1400/1500], Training Loss: 1081291501488161.2, Validation Loss: 1083722321362944.0\n",
      "Epoch [1401/1500], Training Loss: 1081291237954689.8, Validation Loss: 1083722120036352.0\n",
      "Epoch [1402/1500], Training Loss: 1081290934263776.8, Validation Loss: 1083721918709760.0\n",
      "Epoch [1403/1500], Training Loss: 1081290663947425.9, Validation Loss: 1083721516056576.0\n",
      "Epoch [1404/1500], Training Loss: 1081290361420308.5, Validation Loss: 1083721180512256.0\n",
      "Epoch [1405/1500], Training Loss: 1081290113902922.2, Validation Loss: 1083720912076800.0\n",
      "Epoch [1406/1500], Training Loss: 1081289833773072.6, Validation Loss: 1083720442314752.0\n",
      "Epoch [1407/1500], Training Loss: 1081289582812364.1, Validation Loss: 1083720240988160.0\n",
      "Epoch [1408/1500], Training Loss: 1081289278445560.0, Validation Loss: 1083720039661568.0\n",
      "Epoch [1409/1500], Training Loss: 1081289010264915.1, Validation Loss: 1083719637008384.0\n",
      "Epoch [1410/1500], Training Loss: 1081288707771631.0, Validation Loss: 1083719435681792.0\n",
      "Epoch [1411/1500], Training Loss: 1081288440359497.4, Validation Loss: 1083719167246336.0\n",
      "Epoch [1412/1500], Training Loss: 1081288138704810.9, Validation Loss: 1083719033028608.0\n",
      "Epoch [1413/1500], Training Loss: 1081287862656986.4, Validation Loss: 1083718965919744.0\n",
      "Epoch [1414/1500], Training Loss: 1081287574893671.2, Validation Loss: 1083718764593152.0\n",
      "Epoch [1415/1500], Training Loss: 1081287293842835.6, Validation Loss: 1083718429048832.0\n",
      "Epoch [1416/1500], Training Loss: 1081287028058629.5, Validation Loss: 1083718093504512.0\n",
      "Epoch [1417/1500], Training Loss: 1081286723533794.5, Validation Loss: 1083717825069056.0\n",
      "Epoch [1418/1500], Training Loss: 1081286452377507.6, Validation Loss: 1083717623742464.0\n",
      "Epoch [1419/1500], Training Loss: 1081286152091238.8, Validation Loss: 1083717288198144.0\n",
      "Epoch [1420/1500], Training Loss: 1081285891838652.5, Validation Loss: 1083716885544960.0\n",
      "Epoch [1421/1500], Training Loss: 1081285590209578.9, Validation Loss: 1083716482891776.0\n",
      "Epoch [1422/1500], Training Loss: 1081285320457472.5, Validation Loss: 1083716281565184.0\n",
      "Epoch [1423/1500], Training Loss: 1081285011884084.4, Validation Loss: 1083716147347456.0\n",
      "Epoch [1424/1500], Training Loss: 1081284755496076.2, Validation Loss: 1083715543367680.0\n",
      "Epoch [1425/1500], Training Loss: 1081284471953989.8, Validation Loss: 1083715140714496.0\n",
      "Epoch [1426/1500], Training Loss: 1081284228258783.6, Validation Loss: 1083714872279040.0\n",
      "Epoch [1427/1500], Training Loss: 1081283929796322.8, Validation Loss: 1083714738061312.0\n",
      "Epoch [1428/1500], Training Loss: 1081283662978466.6, Validation Loss: 1083714670952448.0\n",
      "Epoch [1429/1500], Training Loss: 1081283362337947.8, Validation Loss: 1083714335408128.0\n",
      "Epoch [1430/1500], Training Loss: 1081283094170258.5, Validation Loss: 1083714201190400.0\n",
      "Epoch [1431/1500], Training Loss: 1081282790112324.4, Validation Loss: 1083713999863808.0\n",
      "Epoch [1432/1500], Training Loss: 1081282522532748.4, Validation Loss: 1083713530101760.0\n",
      "Epoch [1433/1500], Training Loss: 1081282215389674.0, Validation Loss: 1083713328775168.0\n",
      "Epoch [1434/1500], Training Loss: 1081281948780835.2, Validation Loss: 1083712926121984.0\n",
      "Epoch [1435/1500], Training Loss: 1081281676689631.1, Validation Loss: 1083712724795392.0\n",
      "Epoch [1436/1500], Training Loss: 1081281378164025.4, Validation Loss: 1083712389251072.0\n",
      "Epoch [1437/1500], Training Loss: 1081281109047605.2, Validation Loss: 1083712255033344.0\n",
      "Epoch [1438/1500], Training Loss: 1081280806176628.8, Validation Loss: 1083711852380160.0\n",
      "Epoch [1439/1500], Training Loss: 1081280539699280.0, Validation Loss: 1083711651053568.0\n",
      "Epoch [1440/1500], Training Loss: 1081280245477798.1, Validation Loss: 1083711248400384.0\n",
      "Epoch [1441/1500], Training Loss: 1081279979199886.2, Validation Loss: 1083711047073792.0\n",
      "Epoch [1442/1500], Training Loss: 1081279669665111.4, Validation Loss: 1083710644420608.0\n",
      "Epoch [1443/1500], Training Loss: 1081279397944306.6, Validation Loss: 1083710443094016.0\n",
      "Epoch [1444/1500], Training Loss: 1081279109319162.5, Validation Loss: 1083710107549696.0\n",
      "Epoch [1445/1500], Training Loss: 1081278866300016.1, Validation Loss: 1083709839114240.0\n",
      "Epoch [1446/1500], Training Loss: 1081278587080544.6, Validation Loss: 1083709637787648.0\n",
      "Epoch [1447/1500], Training Loss: 1081278319491666.8, Validation Loss: 1083709436461056.0\n",
      "Epoch [1448/1500], Training Loss: 1081278015097498.5, Validation Loss: 1083709235134464.0\n",
      "Epoch [1449/1500], Training Loss: 1081277750367970.1, Validation Loss: 1083709033807872.0\n",
      "Epoch [1450/1500], Training Loss: 1081277447201456.5, Validation Loss: 1083708832481280.0\n",
      "Epoch [1451/1500], Training Loss: 1081277174687859.0, Validation Loss: 1083708631154688.0\n",
      "Epoch [1452/1500], Training Loss: 1081276869186732.2, Validation Loss: 1083708295610368.0\n",
      "Epoch [1453/1500], Training Loss: 1081276598689442.2, Validation Loss: 1083707960066048.0\n",
      "Epoch [1454/1500], Training Loss: 1081276325225124.8, Validation Loss: 1083707892957184.0\n",
      "Epoch [1455/1500], Training Loss: 1081276033132012.1, Validation Loss: 1083707356086272.0\n",
      "Epoch [1456/1500], Training Loss: 1081275763618897.6, Validation Loss: 1083707154759680.0\n",
      "Epoch [1457/1500], Training Loss: 1081275458626843.0, Validation Loss: 1083707087650816.0\n",
      "Epoch [1458/1500], Training Loss: 1081275194455953.8, Validation Loss: 1083706752106496.0\n",
      "Epoch [1459/1500], Training Loss: 1081274893663552.2, Validation Loss: 1083706550779904.0\n",
      "Epoch [1460/1500], Training Loss: 1081274630330202.5, Validation Loss: 1083706349453312.0\n",
      "Epoch [1461/1500], Training Loss: 1081274325456250.6, Validation Loss: 1083706148126720.0\n",
      "Epoch [1462/1500], Training Loss: 1081274054603439.8, Validation Loss: 1083705611255808.0\n",
      "Epoch [1463/1500], Training Loss: 1081273754164120.2, Validation Loss: 1083705342820352.0\n",
      "Epoch [1464/1500], Training Loss: 1081273503889539.4, Validation Loss: 1083705007276032.0\n",
      "Epoch [1465/1500], Training Loss: 1081273224686761.4, Validation Loss: 1083704604622848.0\n",
      "Epoch [1466/1500], Training Loss: 1081272975105409.1, Validation Loss: 1083704537513984.0\n",
      "Epoch [1467/1500], Training Loss: 1081272668065752.1, Validation Loss: 1083704336187392.0\n",
      "Epoch [1468/1500], Training Loss: 1081272401602216.0, Validation Loss: 1083703799316480.0\n",
      "Epoch [1469/1500], Training Loss: 1081272099350879.5, Validation Loss: 1083703665098752.0\n",
      "Epoch [1470/1500], Training Loss: 1081271832810869.9, Validation Loss: 1083703195336704.0\n",
      "Epoch [1471/1500], Training Loss: 1081271527402589.5, Validation Loss: 1083703061118976.0\n",
      "Epoch [1472/1500], Training Loss: 1081271253349742.8, Validation Loss: 1083702792683520.0\n",
      "Epoch [1473/1500], Training Loss: 1081270965797276.1, Validation Loss: 1083702255812608.0\n",
      "Epoch [1474/1500], Training Loss: 1081270685888311.2, Validation Loss: 1083702054486016.0\n",
      "Epoch [1475/1500], Training Loss: 1081270419785651.0, Validation Loss: 1083701718941696.0\n",
      "Epoch [1476/1500], Training Loss: 1081270113784542.0, Validation Loss: 1083701517615104.0\n",
      "Epoch [1477/1500], Training Loss: 1081269842932485.8, Validation Loss: 1083700913635328.0\n",
      "Epoch [1478/1500], Training Loss: 1081269540648883.5, Validation Loss: 1083700443873280.0\n",
      "Epoch [1479/1500], Training Loss: 1081269282113162.5, Validation Loss: 1083700242546688.0\n",
      "Epoch [1480/1500], Training Loss: 1081268980779556.6, Validation Loss: 1083699772784640.0\n",
      "Epoch [1481/1500], Training Loss: 1081268709787672.0, Validation Loss: 1083699571458048.0\n",
      "Epoch [1482/1500], Training Loss: 1081268403553345.5, Validation Loss: 1083699370131456.0\n",
      "Epoch [1483/1500], Training Loss: 1081268147328159.5, Validation Loss: 1083699168804864.0\n",
      "Epoch [1484/1500], Training Loss: 1081267863113928.6, Validation Loss: 1083698900369408.0\n",
      "Epoch [1485/1500], Training Loss: 1081267619945573.9, Validation Loss: 1083698766151680.0\n",
      "Epoch [1486/1500], Training Loss: 1081267322509752.6, Validation Loss: 1083698497716224.0\n",
      "Epoch [1487/1500], Training Loss: 1081267055169004.0, Validation Loss: 1083698162171904.0\n",
      "Epoch [1488/1500], Training Loss: 1081266751864401.5, Validation Loss: 1083697960845312.0\n",
      "Epoch [1489/1500], Training Loss: 1081266486597486.9, Validation Loss: 1083697625300992.0\n",
      "Epoch [1490/1500], Training Loss: 1081266184549957.0, Validation Loss: 1083697423974400.0\n",
      "Epoch [1491/1500], Training Loss: 1081265912761412.1, Validation Loss: 1083697222647808.0\n",
      "Epoch [1492/1500], Training Loss: 1081265605529142.0, Validation Loss: 1083696752885760.0\n",
      "Epoch [1493/1500], Training Loss: 1081265339142715.4, Validation Loss: 1083696417341440.0\n",
      "Epoch [1494/1500], Training Loss: 1081265064413942.6, Validation Loss: 1083696148905984.0\n",
      "Epoch [1495/1500], Training Loss: 1081264768368867.2, Validation Loss: 1083695612035072.0\n",
      "Epoch [1496/1500], Training Loss: 1081264500279907.6, Validation Loss: 1083695477817344.0\n",
      "Epoch [1497/1500], Training Loss: 1081264195540494.9, Validation Loss: 1083695209381888.0\n",
      "Epoch [1498/1500], Training Loss: 1081263930967941.5, Validation Loss: 1083695075164160.0\n",
      "Epoch [1499/1500], Training Loss: 1081263634863442.0, Validation Loss: 1083694806728704.0\n",
      "Epoch [1500/1500], Training Loss: 1081263368591260.9, Validation Loss: 1083694404075520.0\n",
      "Final Test Loss: 1014949493080064.0\n",
      "Training model with target variable: adjusted_close\n",
      "\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0003, window_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Training Loss: 1477.0827132002764, Validation Loss: 1601.29443359375\n",
      "Epoch [2/1500], Training Loss: 1375.9381709412535, Validation Loss: 1556.0877685546875\n",
      "Epoch [3/1500], Training Loss: 1227.1264117063918, Validation Loss: 1549.349365234375\n",
      "Epoch [4/1500], Training Loss: 1176.0651796448917, Validation Loss: 1585.2747802734375\n",
      "Epoch [5/1500], Training Loss: 1108.0236272536438, Validation Loss: 1603.22509765625\n",
      "Epoch [6/1500], Training Loss: 1129.3897837722923, Validation Loss: 1469.6185302734375\n",
      "Epoch [7/1500], Training Loss: 975.8385512424835, Validation Loss: 1558.1126708984375\n",
      "Epoch [8/1500], Training Loss: 840.1363723915558, Validation Loss: 2129.787841796875\n",
      "Epoch [9/1500], Training Loss: 779.0273987774591, Validation Loss: 2287.599853515625\n",
      "Epoch [10/1500], Training Loss: 719.0314337880462, Validation Loss: 2354.13916015625\n",
      "Epoch [11/1500], Training Loss: 674.8785216691358, Validation Loss: 2372.530029296875\n",
      "Epoch [12/1500], Training Loss: 638.2442117021714, Validation Loss: 2808.49755859375\n",
      "Epoch [13/1500], Training Loss: 624.0471675952425, Validation Loss: 2340.5458984375\n",
      "Epoch [14/1500], Training Loss: 568.4390392050582, Validation Loss: 2253.505859375\n",
      "Epoch [15/1500], Training Loss: 525.1468933769111, Validation Loss: 2422.10498046875\n",
      "Early stopping at epoch 15\n",
      "Final Test Loss: 2286.759033203125\n",
      "Training model with target variable: change_percent\n",
      "\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0003, window_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Training Loss: 7.71623811945319, Validation Loss: 8.313543319702148\n",
      "Epoch [2/1500], Training Loss: 7.693008788534319, Validation Loss: 8.313610076904297\n",
      "Epoch [3/1500], Training Loss: 7.691792592723835, Validation Loss: 8.313133239746094\n",
      "Epoch [4/1500], Training Loss: 7.6907254041559145, Validation Loss: 8.312504768371582\n",
      "Epoch [5/1500], Training Loss: 7.689439067895689, Validation Loss: 8.311439514160156\n",
      "Epoch [6/1500], Training Loss: 7.687862586589357, Validation Loss: 8.310015678405762\n",
      "Epoch [7/1500], Training Loss: 7.686076526201698, Validation Loss: 8.308466911315918\n",
      "Epoch [8/1500], Training Loss: 7.683973968357831, Validation Loss: 8.306602478027344\n",
      "Epoch [9/1500], Training Loss: 7.681275566763107, Validation Loss: 8.304149627685547\n",
      "Epoch [10/1500], Training Loss: 7.677921568677332, Validation Loss: 8.301270484924316\n",
      "Epoch [11/1500], Training Loss: 7.674313079771361, Validation Loss: 8.298552513122559\n",
      "Epoch [12/1500], Training Loss: 7.670632182302673, Validation Loss: 8.295977592468262\n",
      "Epoch [13/1500], Training Loss: 7.666622139787541, Validation Loss: 8.293282508850098\n",
      "Epoch [14/1500], Training Loss: 7.661997090029334, Validation Loss: 8.290326118469238\n",
      "Epoch [15/1500], Training Loss: 7.656486319222631, Validation Loss: 8.28701114654541\n",
      "Epoch [16/1500], Training Loss: 7.6498401945661625, Validation Loss: 8.283310890197754\n",
      "Epoch [17/1500], Training Loss: 7.641900387426204, Validation Loss: 8.279377937316895\n",
      "Epoch [18/1500], Training Loss: 7.632660457037893, Validation Loss: 8.275450706481934\n",
      "Epoch [19/1500], Training Loss: 7.622278439247394, Validation Loss: 8.271706581115723\n",
      "Epoch [20/1500], Training Loss: 7.610943226901109, Validation Loss: 8.268288612365723\n",
      "Epoch [21/1500], Training Loss: 7.5987564111124595, Validation Loss: 8.265222549438477\n",
      "Epoch [22/1500], Training Loss: 7.585727737187101, Validation Loss: 8.262516021728516\n",
      "Epoch [23/1500], Training Loss: 7.57180826029397, Validation Loss: 8.260110855102539\n",
      "Epoch [24/1500], Training Loss: 7.556935375396311, Validation Loss: 8.258055686950684\n",
      "Epoch [25/1500], Training Loss: 7.541063361213027, Validation Loss: 8.25632095336914\n",
      "Epoch [26/1500], Training Loss: 7.524192090933255, Validation Loss: 8.255030632019043\n",
      "Epoch [27/1500], Training Loss: 7.5064088699507545, Validation Loss: 8.254324913024902\n",
      "Epoch [28/1500], Training Loss: 7.487857090201517, Validation Loss: 8.2542724609375\n",
      "Epoch [29/1500], Training Loss: 7.468685538149079, Validation Loss: 8.254938125610352\n",
      "Epoch [30/1500], Training Loss: 7.448976403569795, Validation Loss: 8.256305694580078\n",
      "Epoch [31/1500], Training Loss: 7.428758389904185, Validation Loss: 8.258411407470703\n",
      "Epoch [32/1500], Training Loss: 7.408033322486851, Validation Loss: 8.261362075805664\n",
      "Epoch [33/1500], Training Loss: 7.386777851570711, Validation Loss: 8.26535701751709\n",
      "Epoch [34/1500], Training Loss: 7.364948237387315, Validation Loss: 8.270541191101074\n",
      "Epoch [35/1500], Training Loss: 7.342493710076642, Validation Loss: 8.276955604553223\n",
      "Epoch [36/1500], Training Loss: 7.319377751613252, Validation Loss: 8.28466796875\n",
      "Epoch [37/1500], Training Loss: 7.295589977752376, Validation Loss: 8.293848991394043\n",
      "Early stopping at epoch 37\n",
      "Final Test Loss: 8.675812721252441\n",
      "Training model with target variable: avg_vol_20d\n",
      "\n",
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0003, window_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Training Loss: 1.7908883906995037e+17, Validation Loss: 1.781535426310308e+17\n",
      "Epoch [2/1500], Training Loss: 1.7908883174111565e+17, Validation Loss: 1.781535082712924e+17\n",
      "Epoch [3/1500], Training Loss: 1.7908882816462256e+17, Validation Loss: 1.7815349109142323e+17\n",
      "Epoch [4/1500], Training Loss: 1.7908881441549414e+17, Validation Loss: 1.7815349109142323e+17\n",
      "Epoch [5/1500], Training Loss: 1.790888077089176e+17, Validation Loss: 1.7815349109142323e+17\n",
      "Epoch [6/1500], Training Loss: 1.7908880426669635e+17, Validation Loss: 1.7815345673168486e+17\n",
      "Epoch [7/1500], Training Loss: 1.7908879354706304e+17, Validation Loss: 1.7815345673168486e+17\n",
      "Epoch [8/1500], Training Loss: 1.7908878871883514e+17, Validation Loss: 1.7815345673168486e+17\n",
      "Epoch [9/1500], Training Loss: 1.7908878380299632e+17, Validation Loss: 1.7815345673168486e+17\n",
      "Epoch [10/1500], Training Loss: 1.7908876871913194e+17, Validation Loss: 1.7815343955181568e+17\n",
      "Epoch [11/1500], Training Loss: 1.790887641009086e+17, Validation Loss: 1.7815343955181568e+17\n",
      "Epoch [12/1500], Training Loss: 1.790887587556476e+17, Validation Loss: 1.7815343955181568e+17\n",
      "Epoch [13/1500], Training Loss: 1.7908875688080934e+17, Validation Loss: 1.7815343955181568e+17\n",
      "Epoch [14/1500], Training Loss: 1.790887537893273e+17, Validation Loss: 1.781534051920773e+17\n",
      "Epoch [15/1500], Training Loss: 1.7908874730242026e+17, Validation Loss: 1.7815338801220813e+17\n",
      "Epoch [16/1500], Training Loss: 1.7908873269159923e+17, Validation Loss: 1.7815337083233894e+17\n",
      "Epoch [17/1500], Training Loss: 1.790887302887159e+17, Validation Loss: 1.7815337083233894e+17\n",
      "Epoch [18/1500], Training Loss: 1.7908872220058205e+17, Validation Loss: 1.7815337083233894e+17\n",
      "Epoch [19/1500], Training Loss: 1.7908871157461427e+17, Validation Loss: 1.7815333647260058e+17\n",
      "Epoch [20/1500], Training Loss: 1.7908870987493738e+17, Validation Loss: 1.7815333647260058e+17\n",
      "Epoch [21/1500], Training Loss: 1.7908870218618998e+17, Validation Loss: 1.781533192927314e+17\n",
      "Epoch [22/1500], Training Loss: 1.7908869299558192e+17, Validation Loss: 1.781533021128622e+17\n",
      "Epoch [23/1500], Training Loss: 1.7908868499169226e+17, Validation Loss: 1.781533021128622e+17\n",
      "Epoch [24/1500], Training Loss: 1.7908867759893008e+17, Validation Loss: 1.7815328493299302e+17\n",
      "Epoch [25/1500], Training Loss: 1.7908867466632144e+17, Validation Loss: 1.7815328493299302e+17\n",
      "Epoch [26/1500], Training Loss: 1.7908867346851818e+17, Validation Loss: 1.7815328493299302e+17\n",
      "Epoch [27/1500], Training Loss: 1.790886667297843e+17, Validation Loss: 1.7815328493299302e+17\n",
      "Epoch [28/1500], Training Loss: 1.790886632762512e+17, Validation Loss: 1.7815328493299302e+17\n",
      "Epoch [29/1500], Training Loss: 1.790886484625754e+17, Validation Loss: 1.7815325057325466e+17\n",
      "Epoch [30/1500], Training Loss: 1.7908864344422234e+17, Validation Loss: 1.7815323339338547e+17\n",
      "Epoch [31/1500], Training Loss: 1.790886386578604e+17, Validation Loss: 1.7815323339338547e+17\n",
      "Epoch [32/1500], Training Loss: 1.7908862794751933e+17, Validation Loss: 1.781532162135163e+17\n",
      "Epoch [33/1500], Training Loss: 1.79088623382757e+17, Validation Loss: 1.781532162135163e+17\n",
      "Epoch [34/1500], Training Loss: 1.790886180350184e+17, Validation Loss: 1.781532162135163e+17\n",
      "Epoch [35/1500], Training Loss: 1.7908860299195606e+17, Validation Loss: 1.781531990336471e+17\n",
      "Epoch [36/1500], Training Loss: 1.7908859963697718e+17, Validation Loss: 1.781531990336471e+17\n",
      "Epoch [37/1500], Training Loss: 1.790885933713262e+17, Validation Loss: 1.781531990336471e+17\n",
      "Epoch [38/1500], Training Loss: 1.790885911032956e+17, Validation Loss: 1.781531990336471e+17\n",
      "Epoch [39/1500], Training Loss: 1.790885894223427e+17, Validation Loss: 1.7815318185377792e+17\n",
      "Epoch [40/1500], Training Loss: 1.790885814121819e+17, Validation Loss: 1.7815318185377792e+17\n",
      "Epoch [41/1500], Training Loss: 1.7908856837142346e+17, Validation Loss: 1.7815318185377792e+17\n",
      "Epoch [42/1500], Training Loss: 1.7908856492130915e+17, Validation Loss: 1.7815318185377792e+17\n",
      "Epoch [43/1500], Training Loss: 1.7908855722593872e+17, Validation Loss: 1.7815316467390874e+17\n",
      "Epoch [44/1500], Training Loss: 1.7908854834405808e+17, Validation Loss: 1.7815316467390874e+17\n",
      "Epoch [45/1500], Training Loss: 1.79088544434592e+17, Validation Loss: 1.7815316467390874e+17\n",
      "Epoch [46/1500], Training Loss: 1.7908853697981578e+17, Validation Loss: 1.7815316467390874e+17\n",
      "Epoch [47/1500], Training Loss: 1.790885326162301e+17, Validation Loss: 1.7815316467390874e+17\n",
      "Epoch [48/1500], Training Loss: 1.790885196436112e+17, Validation Loss: 1.7815316467390874e+17\n",
      "Epoch [49/1500], Training Loss: 1.7908851287753363e+17, Validation Loss: 1.7815313031417037e+17\n",
      "Epoch [50/1500], Training Loss: 1.79088509449462e+17, Validation Loss: 1.7815313031417037e+17\n",
      "Epoch [51/1500], Training Loss: 1.79088507772794e+17, Validation Loss: 1.7815313031417037e+17\n",
      "Epoch [52/1500], Training Loss: 1.7908850265896618e+17, Validation Loss: 1.7815313031417037e+17\n",
      "Epoch [53/1500], Training Loss: 1.790884979454096e+17, Validation Loss: 1.7815313031417037e+17\n",
      "Epoch [54/1500], Training Loss: 1.7908848296053453e+17, Validation Loss: 1.781531131343012e+17\n",
      "Epoch [55/1500], Training Loss: 1.7908847846689114e+17, Validation Loss: 1.781531131343012e+17\n",
      "Epoch [56/1500], Training Loss: 1.790884730974304e+17, Validation Loss: 1.78153095954432e+17\n",
      "Epoch [57/1500], Training Loss: 1.7908846208913034e+17, Validation Loss: 1.781530787745628e+17\n",
      "Epoch [58/1500], Training Loss: 1.790884586154138e+17, Validation Loss: 1.781530787745628e+17\n",
      "Epoch [59/1500], Training Loss: 1.790884524612216e+17, Validation Loss: 1.781530787745628e+17\n",
      "Epoch [60/1500], Training Loss: 1.7908843745603814e+17, Validation Loss: 1.781530787745628e+17\n",
      "Epoch [61/1500], Training Loss: 1.7908843537332314e+17, Validation Loss: 1.7815306159469363e+17\n",
      "Epoch [62/1500], Training Loss: 1.7908842740736234e+17, Validation Loss: 1.7815306159469363e+17\n",
      "Epoch [63/1500], Training Loss: 1.7908842526631437e+17, Validation Loss: 1.7815306159469363e+17\n",
      "Epoch [64/1500], Training Loss: 1.7908842387859418e+17, Validation Loss: 1.7815306159469363e+17\n",
      "Epoch [65/1500], Training Loss: 1.790884160727373e+17, Validation Loss: 1.7815306159469363e+17\n",
      "Epoch [66/1500], Training Loss: 1.7908840622709744e+17, Validation Loss: 1.7815306159469363e+17\n",
      "Epoch [67/1500], Training Loss: 1.790883989041404e+17, Validation Loss: 1.7815306159469363e+17\n",
      "Epoch [68/1500], Training Loss: 1.7908839154028554e+17, Validation Loss: 1.7815306159469363e+17\n",
      "Epoch [69/1500], Training Loss: 1.7908838852994122e+17, Validation Loss: 1.7815302723495526e+17\n",
      "Epoch [70/1500], Training Loss: 1.7908837861883318e+17, Validation Loss: 1.7815302723495526e+17\n",
      "Epoch [71/1500], Training Loss: 1.79088371714278e+17, Validation Loss: 1.7815301005508608e+17\n",
      "Epoch [72/1500], Training Loss: 1.790883683230547e+17, Validation Loss: 1.7815301005508608e+17\n",
      "Epoch [73/1500], Training Loss: 1.7908835356646054e+17, Validation Loss: 1.781529928752169e+17\n",
      "Epoch [74/1500], Training Loss: 1.7908834837562576e+17, Validation Loss: 1.781529928752169e+17\n",
      "Epoch [75/1500], Training Loss: 1.7908834372934672e+17, Validation Loss: 1.781529928752169e+17\n",
      "Epoch [76/1500], Training Loss: 1.790883417670329e+17, Validation Loss: 1.781529928752169e+17\n",
      "Epoch [77/1500], Training Loss: 1.7908833738736547e+17, Validation Loss: 1.781529756953477e+17\n",
      "Epoch [78/1500], Training Loss: 1.7908833201764243e+17, Validation Loss: 1.781529756953477e+17\n",
      "Epoch [79/1500], Training Loss: 1.790883170055132e+17, Validation Loss: 1.781529756953477e+17\n",
      "Epoch [80/1500], Training Loss: 1.7908831321912592e+17, Validation Loss: 1.781529756953477e+17\n",
      "Epoch [81/1500], Training Loss: 1.7908830738956803e+17, Validation Loss: 1.781529756953477e+17\n",
      "Epoch [82/1500], Training Loss: 1.7908829599805702e+17, Validation Loss: 1.7815294133560934e+17\n",
      "Epoch [83/1500], Training Loss: 1.7908829428915248e+17, Validation Loss: 1.7815292415574016e+17\n",
      "Epoch [84/1500], Training Loss: 1.790882863098311e+17, Validation Loss: 1.7815292415574016e+17\n",
      "Epoch [85/1500], Training Loss: 1.790882732197113e+17, Validation Loss: 1.7815290697587098e+17\n",
      "Epoch [86/1500], Training Loss: 1.7908826961272022e+17, Validation Loss: 1.7815290697587098e+17\n",
      "Epoch [87/1500], Training Loss: 1.7908826174180323e+17, Validation Loss: 1.781528897960018e+17\n",
      "Epoch [88/1500], Training Loss: 1.7908825921961805e+17, Validation Loss: 1.781528897960018e+17\n",
      "Epoch [89/1500], Training Loss: 1.790882579325112e+17, Validation Loss: 1.781528897960018e+17\n",
      "Epoch [90/1500], Training Loss: 1.7908825054855616e+17, Validation Loss: 1.781528897960018e+17\n",
      "Epoch [91/1500], Training Loss: 1.7908824582572618e+17, Validation Loss: 1.781528726161326e+17\n",
      "Epoch [92/1500], Training Loss: 1.7908823350226554e+17, Validation Loss: 1.781528726161326e+17\n",
      "Epoch [93/1500], Training Loss: 1.7908822650525248e+17, Validation Loss: 1.781528726161326e+17\n",
      "Epoch [94/1500], Training Loss: 1.7908822315766077e+17, Validation Loss: 1.7815283825639424e+17\n",
      "Epoch [95/1500], Training Loss: 1.7908821278255795e+17, Validation Loss: 1.7815283825639424e+17\n",
      "Epoch [96/1500], Training Loss: 1.7908820743219597e+17, Validation Loss: 1.7815283825639424e+17\n",
      "Epoch [97/1500], Training Loss: 1.7908820292811933e+17, Validation Loss: 1.7815282107652506e+17\n",
      "Epoch [98/1500], Training Loss: 1.7908818764990534e+17, Validation Loss: 1.7815280389665587e+17\n",
      "Epoch [99/1500], Training Loss: 1.7908818336426154e+17, Validation Loss: 1.7815280389665587e+17\n",
      "Epoch [100/1500], Training Loss: 1.790881780351636e+17, Validation Loss: 1.7815280389665587e+17\n",
      "Epoch [101/1500], Training Loss: 1.790881761817643e+17, Validation Loss: 1.7815280389665587e+17\n",
      "Epoch [102/1500], Training Loss: 1.7908817226984554e+17, Validation Loss: 1.7815280389665587e+17\n",
      "Epoch [103/1500], Training Loss: 1.7908816653093216e+17, Validation Loss: 1.7815280389665587e+17\n",
      "Epoch [104/1500], Training Loss: 1.7908815140705894e+17, Validation Loss: 1.781527867167867e+17\n",
      "Epoch [105/1500], Training Loss: 1.7908814938626435e+17, Validation Loss: 1.781527867167867e+17\n",
      "Epoch [106/1500], Training Loss: 1.790881414732669e+17, Validation Loss: 1.781527695369175e+17\n",
      "Epoch [107/1500], Training Loss: 1.7908813087197568e+17, Validation Loss: 1.7815273517717914e+17\n",
      "Epoch [108/1500], Training Loss: 1.7908812913720576e+17, Validation Loss: 1.7815273517717914e+17\n",
      "Epoch [109/1500], Training Loss: 1.7908812115873805e+17, Validation Loss: 1.7815273517717914e+17\n",
      "Epoch [110/1500], Training Loss: 1.790881106934301e+17, Validation Loss: 1.7815273517717914e+17\n",
      "Epoch [111/1500], Training Loss: 1.7908810436466787e+17, Validation Loss: 1.7815273517717914e+17\n",
      "Epoch [112/1500], Training Loss: 1.7908809685109018e+17, Validation Loss: 1.7815271799730995e+17\n",
      "Epoch [113/1500], Training Loss: 1.7908809388270144e+17, Validation Loss: 1.7815271799730995e+17\n",
      "Epoch [114/1500], Training Loss: 1.7908809282001952e+17, Validation Loss: 1.7815270081744077e+17\n",
      "Epoch [115/1500], Training Loss: 1.790880857856523e+17, Validation Loss: 1.7815270081744077e+17\n",
      "Epoch [116/1500], Training Loss: 1.7908808253790352e+17, Validation Loss: 1.781526836375716e+17\n",
      "Epoch [117/1500], Training Loss: 1.7908806773028858e+17, Validation Loss: 1.781526836375716e+17\n",
      "Epoch [118/1500], Training Loss: 1.7908806221063866e+17, Validation Loss: 1.781526836375716e+17\n",
      "Epoch [119/1500], Training Loss: 1.790880578906305e+17, Validation Loss: 1.781526836375716e+17\n",
      "Epoch [120/1500], Training Loss: 1.7908804690145203e+17, Validation Loss: 1.781526492778332e+17\n",
      "Epoch [121/1500], Training Loss: 1.7908804268217795e+17, Validation Loss: 1.7815263209796403e+17\n",
      "Epoch [122/1500], Training Loss: 1.7908803728586912e+17, Validation Loss: 1.7815263209796403e+17\n",
      "Epoch [123/1500], Training Loss: 1.790880223658106e+17, Validation Loss: 1.7815261491809485e+17\n",
      "Epoch [124/1500], Training Loss: 1.7908801838248563e+17, Validation Loss: 1.7815261491809485e+17\n",
      "Epoch [125/1500], Training Loss: 1.790880127185575e+17, Validation Loss: 1.7815259773822566e+17\n",
      "Epoch [126/1500], Training Loss: 1.7908801043197584e+17, Validation Loss: 1.7815259773822566e+17\n",
      "Epoch [127/1500], Training Loss: 1.790880086679884e+17, Validation Loss: 1.7815259773822566e+17\n",
      "Epoch [128/1500], Training Loss: 1.790880008012855e+17, Validation Loss: 1.7815259773822566e+17\n",
      "Epoch [129/1500], Training Loss: 1.7908798757531152e+17, Validation Loss: 1.781525633784873e+17\n",
      "Epoch [130/1500], Training Loss: 1.7908798406021827e+17, Validation Loss: 1.781525633784873e+17\n",
      "Epoch [131/1500], Training Loss: 1.7908797602035696e+17, Validation Loss: 1.781525461986181e+17\n",
      "Epoch [132/1500], Training Loss: 1.7908796653490883e+17, Validation Loss: 1.781525461986181e+17\n",
      "Epoch [133/1500], Training Loss: 1.7908796312583395e+17, Validation Loss: 1.781525461986181e+17\n",
      "Epoch [134/1500], Training Loss: 1.7908795561840042e+17, Validation Loss: 1.7815252901874893e+17\n",
      "Epoch [135/1500], Training Loss: 1.7908795005856266e+17, Validation Loss: 1.7815251183887974e+17\n",
      "Epoch [136/1500], Training Loss: 1.790879385737443e+17, Validation Loss: 1.7815251183887974e+17\n",
      "Epoch [137/1500], Training Loss: 1.7908793140790346e+17, Validation Loss: 1.7815251183887974e+17\n",
      "Epoch [138/1500], Training Loss: 1.7908792819085254e+17, Validation Loss: 1.7815251183887974e+17\n",
      "Epoch [139/1500], Training Loss: 1.7908792660441462e+17, Validation Loss: 1.7815249465901056e+17\n",
      "Epoch [140/1500], Training Loss: 1.790879209566245e+17, Validation Loss: 1.7815249465901056e+17\n",
      "Epoch [141/1500], Training Loss: 1.7908791678247696e+17, Validation Loss: 1.7815249465901056e+17\n",
      "Epoch [142/1500], Training Loss: 1.790879015951406e+17, Validation Loss: 1.7815247747914138e+17\n",
      "Epoch [143/1500], Training Loss: 1.7908789736816083e+17, Validation Loss: 1.7815247747914138e+17\n",
      "Epoch [144/1500], Training Loss: 1.7908789196622634e+17, Validation Loss: 1.7815247747914138e+17\n",
      "Epoch [145/1500], Training Loss: 1.7908788153290294e+17, Validation Loss: 1.781524602992722e+17\n",
      "Epoch [146/1500], Training Loss: 1.7908787737684013e+17, Validation Loss: 1.781524602992722e+17\n",
      "Epoch [147/1500], Training Loss: 1.7908787185000714e+17, Validation Loss: 1.781524602992722e+17\n",
      "Epoch [148/1500], Training Loss: 1.7908785668400147e+17, Validation Loss: 1.781524602992722e+17\n",
      "Epoch [149/1500], Training Loss: 1.790878546936674e+17, Validation Loss: 1.78152443119403e+17\n",
      "Epoch [150/1500], Training Loss: 1.7908784689118138e+17, Validation Loss: 1.78152443119403e+17\n",
      "Epoch [151/1500], Training Loss: 1.7908784465766717e+17, Validation Loss: 1.78152443119403e+17\n",
      "Epoch [152/1500], Training Loss: 1.7908784331837568e+17, Validation Loss: 1.78152443119403e+17\n",
      "Epoch [153/1500], Training Loss: 1.7908783526899312e+17, Validation Loss: 1.7815242593953382e+17\n",
      "Epoch [154/1500], Training Loss: 1.7908782420994915e+17, Validation Loss: 1.7815239157979546e+17\n",
      "Epoch [155/1500], Training Loss: 1.7908781844914086e+17, Validation Loss: 1.7815237439992627e+17\n",
      "Epoch [156/1500], Training Loss: 1.790878109395066e+17, Validation Loss: 1.7815237439992627e+17\n",
      "Epoch [157/1500], Training Loss: 1.7908780795263958e+17, Validation Loss: 1.781523572200571e+17\n",
      "Epoch [158/1500], Training Loss: 1.790877979760946e+17, Validation Loss: 1.781523572200571e+17\n",
      "Epoch [159/1500], Training Loss: 1.7908779074401312e+17, Validation Loss: 1.781523572200571e+17\n",
      "Epoch [160/1500], Training Loss: 1.7908778759317162e+17, Validation Loss: 1.781523400401879e+17\n",
      "Epoch [161/1500], Training Loss: 1.7908777288357162e+17, Validation Loss: 1.781523400401879e+17\n",
      "Epoch [162/1500], Training Loss: 1.7908776703207098e+17, Validation Loss: 1.781523400401879e+17\n",
      "Epoch [163/1500], Training Loss: 1.7908776298912006e+17, Validation Loss: 1.781523400401879e+17\n",
      "Epoch [164/1500], Training Loss: 1.7908776093782544e+17, Validation Loss: 1.781523400401879e+17\n",
      "Epoch [165/1500], Training Loss: 1.7908775673904925e+17, Validation Loss: 1.781523400401879e+17\n",
      "Epoch [166/1500], Training Loss: 1.7908775136535363e+17, Validation Loss: 1.781523400401879e+17\n",
      "Epoch [167/1500], Training Loss: 1.7908773637674755e+17, Validation Loss: 1.7815232286031872e+17\n",
      "Epoch [168/1500], Training Loss: 1.7908773192785802e+17, Validation Loss: 1.7815228850058035e+17\n",
      "Epoch [169/1500], Training Loss: 1.7908772671329194e+17, Validation Loss: 1.7815228850058035e+17\n",
      "Epoch [170/1500], Training Loss: 1.7908771542212778e+17, Validation Loss: 1.7815228850058035e+17\n",
      "Epoch [171/1500], Training Loss: 1.79087713580896e+17, Validation Loss: 1.7815228850058035e+17\n",
      "Epoch [172/1500], Training Loss: 1.7908770585200035e+17, Validation Loss: 1.7815228850058035e+17\n",
      "Epoch [173/1500], Training Loss: 1.7908769260856826e+17, Validation Loss: 1.7815228850058035e+17\n",
      "Epoch [174/1500], Training Loss: 1.7908768917903098e+17, Validation Loss: 1.7815228850058035e+17\n",
      "Epoch [175/1500], Training Loss: 1.7908768105176698e+17, Validation Loss: 1.7815227132071117e+17\n",
      "Epoch [176/1500], Training Loss: 1.790876787567509e+17, Validation Loss: 1.7815227132071117e+17\n",
      "Epoch [177/1500], Training Loss: 1.7908767746311056e+17, Validation Loss: 1.7815227132071117e+17\n",
      "Epoch [178/1500], Training Loss: 1.7908766991996147e+17, Validation Loss: 1.78152254140842e+17\n",
      "Epoch [179/1500], Training Loss: 1.7908766377211747e+17, Validation Loss: 1.781522369609728e+17\n",
      "Epoch [180/1500], Training Loss: 1.7908765284562144e+17, Validation Loss: 1.781522369609728e+17\n",
      "Epoch [181/1500], Training Loss: 1.7908764562390448e+17, Validation Loss: 1.781522197811036e+17\n",
      "Epoch [182/1500], Training Loss: 1.790876424661027e+17, Validation Loss: 1.781522197811036e+17\n",
      "Epoch [183/1500], Training Loss: 1.7908763226047517e+17, Validation Loss: 1.781522197811036e+17\n",
      "Epoch [184/1500], Training Loss: 1.7908762626664512e+17, Validation Loss: 1.7815218542136525e+17\n",
      "Epoch [185/1500], Training Loss: 1.7908762226192083e+17, Validation Loss: 1.7815218542136525e+17\n",
      "Epoch [186/1500], Training Loss: 1.7908760705949584e+17, Validation Loss: 1.7815218542136525e+17\n",
      "Epoch [187/1500], Training Loss: 1.7908760284826266e+17, Validation Loss: 1.7815218542136525e+17\n",
      "Epoch [188/1500], Training Loss: 1.7908759748360733e+17, Validation Loss: 1.7815218542136525e+17\n",
      "Epoch [189/1500], Training Loss: 1.7908759565314816e+17, Validation Loss: 1.7815218542136525e+17\n",
      "Epoch [190/1500], Training Loss: 1.7908759105846467e+17, Validation Loss: 1.7815216824149606e+17\n",
      "Epoch [191/1500], Training Loss: 1.79087585979199e+17, Validation Loss: 1.7815216824149606e+17\n",
      "Epoch [192/1500], Training Loss: 1.7908757054795382e+17, Validation Loss: 1.781521338817577e+17\n",
      "Epoch [193/1500], Training Loss: 1.7908756869020515e+17, Validation Loss: 1.781521338817577e+17\n",
      "Epoch [194/1500], Training Loss: 1.790875609683635e+17, Validation Loss: 1.781521338817577e+17\n",
      "Epoch [195/1500], Training Loss: 1.7908755009026486e+17, Validation Loss: 1.781521167018885e+17\n",
      "Epoch [196/1500], Training Loss: 1.790875483593447e+17, Validation Loss: 1.781521167018885e+17\n",
      "Epoch [197/1500], Training Loss: 1.7908754018153046e+17, Validation Loss: 1.781521167018885e+17\n",
      "Epoch [198/1500], Training Loss: 1.7908752858376464e+17, Validation Loss: 1.7815208234215014e+17\n",
      "Epoch [199/1500], Training Loss: 1.790875236485461e+17, Validation Loss: 1.7815208234215014e+17\n",
      "Epoch [200/1500], Training Loss: 1.7908751610533245e+17, Validation Loss: 1.7815208234215014e+17\n",
      "Epoch [201/1500], Training Loss: 1.7908751307508995e+17, Validation Loss: 1.7815208234215014e+17\n",
      "Epoch [202/1500], Training Loss: 1.7908751210213226e+17, Validation Loss: 1.7815208234215014e+17\n",
      "Epoch [203/1500], Training Loss: 1.7908750490234768e+17, Validation Loss: 1.7815206516228096e+17\n",
      "Epoch [204/1500], Training Loss: 1.7908750173874736e+17, Validation Loss: 1.7815206516228096e+17\n",
      "Epoch [205/1500], Training Loss: 1.790874869783556e+17, Validation Loss: 1.7815206516228096e+17\n",
      "Epoch [206/1500], Training Loss: 1.7908748095288035e+17, Validation Loss: 1.7815206516228096e+17\n",
      "Epoch [207/1500], Training Loss: 1.7908747706050173e+17, Validation Loss: 1.7815204798241178e+17\n",
      "Epoch [208/1500], Training Loss: 1.790874661307119e+17, Validation Loss: 1.7815204798241178e+17\n",
      "Epoch [209/1500], Training Loss: 1.7908746183281312e+17, Validation Loss: 1.7815204798241178e+17\n",
      "Epoch [210/1500], Training Loss: 1.7908745652887267e+17, Validation Loss: 1.7815204798241178e+17\n",
      "Epoch [211/1500], Training Loss: 1.790874416643739e+17, Validation Loss: 1.781520308025426e+17\n",
      "Epoch [212/1500], Training Loss: 1.790874369627351e+17, Validation Loss: 1.781520136226734e+17\n",
      "Epoch [213/1500], Training Loss: 1.790874318342288e+17, Validation Loss: 1.781520136226734e+17\n",
      "Epoch [214/1500], Training Loss: 1.7908742965362176e+17, Validation Loss: 1.781520136226734e+17\n",
      "Epoch [215/1500], Training Loss: 1.7908742774856458e+17, Validation Loss: 1.7815199644280422e+17\n",
      "Epoch [216/1500], Training Loss: 1.7908742006153286e+17, Validation Loss: 1.7815199644280422e+17\n",
      "Epoch [217/1500], Training Loss: 1.7908740634358125e+17, Validation Loss: 1.7815196208306586e+17\n",
      "Epoch [218/1500], Training Loss: 1.790874031848155e+17, Validation Loss: 1.7815196208306586e+17\n",
      "Epoch [219/1500], Training Loss: 1.790873950592942e+17, Validation Loss: 1.7815196208306586e+17\n",
      "Epoch [220/1500], Training Loss: 1.7908738525781894e+17, Validation Loss: 1.7815196208306586e+17\n",
      "Epoch [221/1500], Training Loss: 1.7908738262199302e+17, Validation Loss: 1.7815194490319667e+17\n",
      "Epoch [222/1500], Training Loss: 1.790873750006273e+17, Validation Loss: 1.781519277233275e+17\n",
      "Epoch [223/1500], Training Loss: 1.790873679671616e+17, Validation Loss: 1.781519105434583e+17\n",
      "Epoch [224/1500], Training Loss: 1.790873579270098e+17, Validation Loss: 1.781519105434583e+17\n",
      "Epoch [225/1500], Training Loss: 1.7908735070133693e+17, Validation Loss: 1.781519105434583e+17\n",
      "Epoch [226/1500], Training Loss: 1.7908734757363958e+17, Validation Loss: 1.781519105434583e+17\n",
      "Epoch [227/1500], Training Loss: 1.7908734615317286e+17, Validation Loss: 1.781519105434583e+17\n",
      "Epoch [228/1500], Training Loss: 1.7908734009474077e+17, Validation Loss: 1.781519105434583e+17\n",
      "Epoch [229/1500], Training Loss: 1.790873362536516e+17, Validation Loss: 1.781519105434583e+17\n",
      "Epoch [230/1500], Training Loss: 1.7908732093550794e+17, Validation Loss: 1.781519105434583e+17\n",
      "Epoch [231/1500], Training Loss: 1.7908731652585683e+17, Validation Loss: 1.781519105434583e+17\n",
      "Epoch [232/1500], Training Loss: 1.7908731130854038e+17, Validation Loss: 1.7815189336358912e+17\n",
      "Epoch [233/1500], Training Loss: 1.7908730058239654e+17, Validation Loss: 1.7815189336358912e+17\n",
      "Epoch [234/1500], Training Loss: 1.7908729591558435e+17, Validation Loss: 1.7815189336358912e+17\n",
      "Epoch [235/1500], Training Loss: 1.790872907753727e+17, Validation Loss: 1.7815189336358912e+17\n",
      "Epoch [236/1500], Training Loss: 1.7908727571879165e+17, Validation Loss: 1.7815187618371994e+17\n",
      "Epoch [237/1500], Training Loss: 1.7908727376573683e+17, Validation Loss: 1.7815187618371994e+17\n",
      "Epoch [238/1500], Training Loss: 1.7908726605582326e+17, Validation Loss: 1.7815185900385075e+17\n",
      "Epoch [239/1500], Training Loss: 1.790872637926856e+17, Validation Loss: 1.7815185900385075e+17\n",
      "Epoch [240/1500], Training Loss: 1.790872623914425e+17, Validation Loss: 1.7815185900385075e+17\n",
      "Epoch [241/1500], Training Loss: 1.7908725427782016e+17, Validation Loss: 1.7815185900385075e+17\n",
      "Epoch [242/1500], Training Loss: 1.7908724225636826e+17, Validation Loss: 1.7815185900385075e+17\n",
      "Epoch [243/1500], Training Loss: 1.790872376877188e+17, Validation Loss: 1.7815185900385075e+17\n",
      "Epoch [244/1500], Training Loss: 1.7908723016837386e+17, Validation Loss: 1.7815184182398157e+17\n",
      "Epoch [245/1500], Training Loss: 1.7908722496682067e+17, Validation Loss: 1.7815184182398157e+17\n",
      "Epoch [246/1500], Training Loss: 1.7908721696463837e+17, Validation Loss: 1.7815184182398157e+17\n",
      "Epoch [247/1500], Training Loss: 1.790872097980418e+17, Validation Loss: 1.7815184182398157e+17\n",
      "Epoch [248/1500], Training Loss: 1.7908720659740378e+17, Validation Loss: 1.78151790284374e+17\n",
      "Epoch [249/1500], Training Loss: 1.790871920141534e+17, Validation Loss: 1.78151790284374e+17\n",
      "Epoch [250/1500], Training Loss: 1.7908718580503034e+17, Validation Loss: 1.78151790284374e+17\n",
      "Epoch [251/1500], Training Loss: 1.7908718205543296e+17, Validation Loss: 1.78151790284374e+17\n",
      "Epoch [252/1500], Training Loss: 1.7908718010923853e+17, Validation Loss: 1.7815177310450483e+17\n",
      "Epoch [253/1500], Training Loss: 1.7908717568734704e+17, Validation Loss: 1.7815177310450483e+17\n",
      "Epoch [254/1500], Training Loss: 1.790871704427244e+17, Validation Loss: 1.7815175592463565e+17\n",
      "Epoch [255/1500], Training Loss: 1.7908715552893712e+17, Validation Loss: 1.7815173874476646e+17\n",
      "Epoch [256/1500], Training Loss: 1.7908715094219456e+17, Validation Loss: 1.7815173874476646e+17\n",
      "Epoch [257/1500], Training Loss: 1.7908714585044906e+17, Validation Loss: 1.7815173874476646e+17\n",
      "Epoch [258/1500], Training Loss: 1.7908713493403437e+17, Validation Loss: 1.7815173874476646e+17\n",
      "Epoch [259/1500], Training Loss: 1.790871330642743e+17, Validation Loss: 1.7815173874476646e+17\n",
      "Epoch [260/1500], Training Loss: 1.7908712531820166e+17, Validation Loss: 1.7815173874476646e+17\n",
      "Epoch [261/1500], Training Loss: 1.790871119742063e+17, Validation Loss: 1.7815172156489728e+17\n",
      "Epoch [262/1500], Training Loss: 1.7908710859156733e+17, Validation Loss: 1.7815172156489728e+17\n",
      "Epoch [263/1500], Training Loss: 1.790871005090655e+17, Validation Loss: 1.7815172156489728e+17\n",
      "Epoch [264/1500], Training Loss: 1.7908709820382864e+17, Validation Loss: 1.7815172156489728e+17\n",
      "Epoch [265/1500], Training Loss: 1.7908709691835626e+17, Validation Loss: 1.7815172156489728e+17\n",
      "Epoch [266/1500], Training Loss: 1.790870894122303e+17, Validation Loss: 1.781517043850281e+17\n",
      "Epoch [267/1500], Training Loss: 1.7908708390675507e+17, Validation Loss: 1.781517043850281e+17\n",
      "Epoch [268/1500], Training Loss: 1.790870722522824e+17, Validation Loss: 1.781516872051589e+17\n",
      "Epoch [269/1500], Training Loss: 1.7908706521182726e+17, Validation Loss: 1.781516872051589e+17\n",
      "Epoch [270/1500], Training Loss: 1.790870619523066e+17, Validation Loss: 1.781516872051589e+17\n",
      "Epoch [271/1500], Training Loss: 1.7908705133623072e+17, Validation Loss: 1.781516872051589e+17\n",
      "Epoch [272/1500], Training Loss: 1.7908704603719562e+17, Validation Loss: 1.781516872051589e+17\n",
      "Epoch [273/1500], Training Loss: 1.7908704149837162e+17, Validation Loss: 1.781516872051589e+17\n",
      "Epoch [274/1500], Training Loss: 1.7908702643489274e+17, Validation Loss: 1.781516872051589e+17\n",
      "Epoch [275/1500], Training Loss: 1.7908702204501286e+17, Validation Loss: 1.7815167002528973e+17\n",
      "Epoch [276/1500], Training Loss: 1.7908701670912538e+17, Validation Loss: 1.7815167002528973e+17\n",
      "Epoch [277/1500], Training Loss: 1.7908701484922592e+17, Validation Loss: 1.7815167002528973e+17\n",
      "Epoch [278/1500], Training Loss: 1.7908701147954778e+17, Validation Loss: 1.7815167002528973e+17\n",
      "Epoch [279/1500], Training Loss: 1.7908700518462525e+17, Validation Loss: 1.7815163566555136e+17\n",
      "Epoch [280/1500], Training Loss: 1.7908699032991837e+17, Validation Loss: 1.7815163566555136e+17\n",
      "Epoch [281/1500], Training Loss: 1.790869880760521e+17, Validation Loss: 1.7815163566555136e+17\n",
      "Epoch [282/1500], Training Loss: 1.790869801024377e+17, Validation Loss: 1.7815163566555136e+17\n",
      "Epoch [283/1500], Training Loss: 1.7908696965280147e+17, Validation Loss: 1.7815163566555136e+17\n",
      "Epoch [284/1500], Training Loss: 1.7908696789388806e+17, Validation Loss: 1.7815163566555136e+17\n",
      "Epoch [285/1500], Training Loss: 1.7908696023347546e+17, Validation Loss: 1.7815163566555136e+17\n",
      "Epoch [286/1500], Training Loss: 1.7908695154892186e+17, Validation Loss: 1.7815161848568218e+17\n",
      "Epoch [287/1500], Training Loss: 1.7908694306084573e+17, Validation Loss: 1.7815161848568218e+17\n",
      "Epoch [288/1500], Training Loss: 1.790869358600118e+17, Validation Loss: 1.781515841259438e+17\n",
      "Epoch [289/1500], Training Loss: 1.7908693279463885e+17, Validation Loss: 1.781515841259438e+17\n",
      "Epoch [290/1500], Training Loss: 1.7908693155107408e+17, Validation Loss: 1.781515841259438e+17\n",
      "Epoch [291/1500], Training Loss: 1.7908692510630797e+17, Validation Loss: 1.781515841259438e+17\n",
      "Epoch [292/1500], Training Loss: 1.7908692148537136e+17, Validation Loss: 1.781515841259438e+17\n",
      "Epoch [293/1500], Training Loss: 1.7908690655609354e+17, Validation Loss: 1.7815156694607462e+17\n",
      "Epoch [294/1500], Training Loss: 1.7908690215989878e+17, Validation Loss: 1.7815156694607462e+17\n",
      "Epoch [295/1500], Training Loss: 1.7908689686037232e+17, Validation Loss: 1.7815156694607462e+17\n",
      "Epoch [296/1500], Training Loss: 1.790868860710818e+17, Validation Loss: 1.7815154976620544e+17\n",
      "Epoch [297/1500], Training Loss: 1.7908688140119027e+17, Validation Loss: 1.7815154976620544e+17\n",
      "Epoch [298/1500], Training Loss: 1.7908687624471776e+17, Validation Loss: 1.7815151540646707e+17\n",
      "Epoch [299/1500], Training Loss: 1.7908686098133642e+17, Validation Loss: 1.7815151540646707e+17\n",
      "Epoch [300/1500], Training Loss: 1.7908685902151702e+17, Validation Loss: 1.7815151540646707e+17\n",
      "Epoch [301/1500], Training Loss: 1.7908685133276342e+17, Validation Loss: 1.7815151540646707e+17\n",
      "Epoch [302/1500], Training Loss: 1.790868490667067e+17, Validation Loss: 1.7815151540646707e+17\n",
      "Epoch [303/1500], Training Loss: 1.7908684769494973e+17, Validation Loss: 1.7815151540646707e+17\n",
      "Epoch [304/1500], Training Loss: 1.790868395451392e+17, Validation Loss: 1.781514982265979e+17\n",
      "Epoch [305/1500], Training Loss: 1.7908682772806406e+17, Validation Loss: 1.781514982265979e+17\n",
      "Epoch [306/1500], Training Loss: 1.7908682285164704e+17, Validation Loss: 1.781514982265979e+17\n",
      "Epoch [307/1500], Training Loss: 1.790868153330704e+17, Validation Loss: 1.781514982265979e+17\n",
      "Epoch [308/1500], Training Loss: 1.7908681235430886e+17, Validation Loss: 1.781514810467287e+17\n",
      "Epoch [309/1500], Training Loss: 1.7908680219645213e+17, Validation Loss: 1.781514810467287e+17\n",
      "Epoch [310/1500], Training Loss: 1.7908679489785734e+17, Validation Loss: 1.781514810467287e+17\n",
      "Epoch [311/1500], Training Loss: 1.7908679174583744e+17, Validation Loss: 1.7815146386685952e+17\n",
      "Epoch [312/1500], Training Loss: 1.790867772109866e+17, Validation Loss: 1.7815146386685952e+17\n",
      "Epoch [313/1500], Training Loss: 1.7908677153373962e+17, Validation Loss: 1.7815146386685952e+17\n",
      "Epoch [314/1500], Training Loss: 1.7908676740502256e+17, Validation Loss: 1.7815146386685952e+17\n",
      "Epoch [315/1500], Training Loss: 1.7908676531501632e+17, Validation Loss: 1.7815146386685952e+17\n",
      "Epoch [316/1500], Training Loss: 1.7908676108745984e+17, Validation Loss: 1.7815146386685952e+17\n",
      "Epoch [317/1500], Training Loss: 1.790867557238123e+17, Validation Loss: 1.7815146386685952e+17\n",
      "Epoch [318/1500], Training Loss: 1.790867407627269e+17, Validation Loss: 1.7815144668699034e+17\n",
      "Epoch [319/1500], Training Loss: 1.7908673689161443e+17, Validation Loss: 1.7815144668699034e+17\n",
      "Epoch [320/1500], Training Loss: 1.7908673110501782e+17, Validation Loss: 1.7815144668699034e+17\n",
      "Epoch [321/1500], Training Loss: 1.7908672014561075e+17, Validation Loss: 1.7815144668699034e+17\n",
      "Epoch [322/1500], Training Loss: 1.7908671843385594e+17, Validation Loss: 1.7815142950712115e+17\n",
      "Epoch [323/1500], Training Loss: 1.7908671053592227e+17, Validation Loss: 1.7815141232725197e+17\n",
      "Epoch [324/1500], Training Loss: 1.7908669742829866e+17, Validation Loss: 1.7815141232725197e+17\n",
      "Epoch [325/1500], Training Loss: 1.7908669390123366e+17, Validation Loss: 1.7815141232725197e+17\n",
      "Epoch [326/1500], Training Loss: 1.7908668618150742e+17, Validation Loss: 1.781513951473828e+17\n",
      "Epoch [327/1500], Training Loss: 1.790866833789275e+17, Validation Loss: 1.781513951473828e+17\n",
      "Epoch [328/1500], Training Loss: 1.7908668211445686e+17, Validation Loss: 1.781513951473828e+17\n",
      "Epoch [329/1500], Training Loss: 1.7908667478894726e+17, Validation Loss: 1.781513951473828e+17\n",
      "Epoch [330/1500], Training Loss: 1.7908667125101395e+17, Validation Loss: 1.781513607876444e+17\n",
      "Epoch [331/1500], Training Loss: 1.7908665748183754e+17, Validation Loss: 1.781513607876444e+17\n",
      "Epoch [332/1500], Training Loss: 1.7908665082001907e+17, Validation Loss: 1.781513607876444e+17\n",
      "Epoch [333/1500], Training Loss: 1.7908664732692077e+17, Validation Loss: 1.781513607876444e+17\n",
      "Epoch [334/1500], Training Loss: 1.7908663644708992e+17, Validation Loss: 1.7815129206816768e+17\n",
      "Epoch [335/1500], Training Loss: 1.790866318495311e+17, Validation Loss: 1.7815129206816768e+17\n",
      "Epoch [336/1500], Training Loss: 1.7908662677170624e+17, Validation Loss: 1.7815129206816768e+17\n",
      "Epoch [337/1500], Training Loss: 1.790866118251453e+17, Validation Loss: 1.781512577084293e+17\n",
      "Epoch [338/1500], Training Loss: 1.7908660715619693e+17, Validation Loss: 1.781512577084293e+17\n",
      "Epoch [339/1500], Training Loss: 1.790866018712074e+17, Validation Loss: 1.781512577084293e+17\n",
      "Epoch [340/1500], Training Loss: 1.79086599928363e+17, Validation Loss: 1.781512577084293e+17\n",
      "Epoch [341/1500], Training Loss: 1.7908659774878662e+17, Validation Loss: 1.781512577084293e+17\n",
      "Epoch [342/1500], Training Loss: 1.7908659030365664e+17, Validation Loss: 1.781512577084293e+17\n",
      "Epoch [343/1500], Training Loss: 1.790865766086681e+17, Validation Loss: 1.7815124052856013e+17\n",
      "Epoch [344/1500], Training Loss: 1.7908657352566838e+17, Validation Loss: 1.7815122334869094e+17\n",
      "Epoch [345/1500], Training Loss: 1.790865654262957e+17, Validation Loss: 1.7815118898895258e+17\n",
      "Epoch [346/1500], Training Loss: 1.7908655573523616e+17, Validation Loss: 1.7815118898895258e+17\n",
      "Epoch [347/1500], Training Loss: 1.7908655319512246e+17, Validation Loss: 1.7815118898895258e+17\n",
      "Epoch [348/1500], Training Loss: 1.7908654567752646e+17, Validation Loss: 1.7815118898895258e+17\n",
      "Epoch [349/1500], Training Loss: 1.790865386042873e+17, Validation Loss: 1.781511718090834e+17\n",
      "Epoch [350/1500], Training Loss: 1.7908652850018093e+17, Validation Loss: 1.7815113744934502e+17\n",
      "Epoch [351/1500], Training Loss: 1.79086521282701e+17, Validation Loss: 1.7815113744934502e+17\n",
      "Epoch [352/1500], Training Loss: 1.79086518142197e+17, Validation Loss: 1.7815113744934502e+17\n",
      "Epoch [353/1500], Training Loss: 1.7908651669012467e+17, Validation Loss: 1.7815113744934502e+17\n",
      "Epoch [354/1500], Training Loss: 1.7908651063641885e+17, Validation Loss: 1.7815113744934502e+17\n",
      "Epoch [355/1500], Training Loss: 1.790865067791313e+17, Validation Loss: 1.7815113744934502e+17\n",
      "Epoch [356/1500], Training Loss: 1.7908649144518902e+17, Validation Loss: 1.7815113744934502e+17\n",
      "Epoch [357/1500], Training Loss: 1.7908648724247574e+17, Validation Loss: 1.7815112026947584e+17\n",
      "Epoch [358/1500], Training Loss: 1.790864818473662e+17, Validation Loss: 1.7815112026947584e+17\n",
      "Epoch [359/1500], Training Loss: 1.7908647103002208e+17, Validation Loss: 1.7815110308960666e+17\n",
      "Epoch [360/1500], Training Loss: 1.7908646679602166e+17, Validation Loss: 1.7815110308960666e+17\n",
      "Epoch [361/1500], Training Loss: 1.7908646140742483e+17, Validation Loss: 1.7815110308960666e+17\n",
      "Epoch [362/1500], Training Loss: 1.7908644634520352e+17, Validation Loss: 1.7815110308960666e+17\n",
      "Epoch [363/1500], Training Loss: 1.7908644435956867e+17, Validation Loss: 1.7815110308960666e+17\n",
      "Epoch [364/1500], Training Loss: 1.7908643651422778e+17, Validation Loss: 1.7815110308960666e+17\n",
      "Epoch [365/1500], Training Loss: 1.79086434321803e+17, Validation Loss: 1.7815110308960666e+17\n",
      "Epoch [366/1500], Training Loss: 1.7908643296333376e+17, Validation Loss: 1.7815108590973747e+17\n",
      "Epoch [367/1500], Training Loss: 1.7908642495957747e+17, Validation Loss: 1.7815108590973747e+17\n",
      "Epoch [368/1500], Training Loss: 1.7908641500739882e+17, Validation Loss: 1.7815108590973747e+17\n",
      "Epoch [369/1500], Training Loss: 1.790864080433906e+17, Validation Loss: 1.7815108590973747e+17\n",
      "Epoch [370/1500], Training Loss: 1.79086400703646e+17, Validation Loss: 1.7815108590973747e+17\n",
      "Epoch [371/1500], Training Loss: 1.7908639769366618e+17, Validation Loss: 1.781510687298683e+17\n",
      "Epoch [372/1500], Training Loss: 1.7908638772444384e+17, Validation Loss: 1.781510687298683e+17\n",
      "Epoch [373/1500], Training Loss: 1.790863808890068e+17, Validation Loss: 1.7815101719026074e+17\n",
      "Epoch [374/1500], Training Loss: 1.7908637749694864e+17, Validation Loss: 1.7815101719026074e+17\n",
      "Epoch [375/1500], Training Loss: 1.790863626325477e+17, Validation Loss: 1.7815101719026074e+17\n",
      "Epoch [376/1500], Training Loss: 1.7908635770534102e+17, Validation Loss: 1.7815101719026074e+17\n",
      "Epoch [377/1500], Training Loss: 1.7908635286558694e+17, Validation Loss: 1.7815101719026074e+17\n",
      "Epoch [378/1500], Training Loss: 1.790863510103409e+17, Validation Loss: 1.7815101719026074e+17\n",
      "Epoch [379/1500], Training Loss: 1.7908634643882035e+17, Validation Loss: 1.7815100001039155e+17\n",
      "Epoch [380/1500], Training Loss: 1.7908634110821088e+17, Validation Loss: 1.7815100001039155e+17\n",
      "Epoch [381/1500], Training Loss: 1.7908632605762618e+17, Validation Loss: 1.7815100001039155e+17\n",
      "Epoch [382/1500], Training Loss: 1.790863232516942e+17, Validation Loss: 1.7815100001039155e+17\n",
      "Epoch [383/1500], Training Loss: 1.7908631640178893e+17, Validation Loss: 1.7815100001039155e+17\n",
      "Epoch [384/1500], Training Loss: 1.7908630516909315e+17, Validation Loss: 1.7815098283052237e+17\n",
      "Epoch [385/1500], Training Loss: 1.790863036470554e+17, Validation Loss: 1.7815098283052237e+17\n",
      "Epoch [386/1500], Training Loss: 1.790862955794736e+17, Validation Loss: 1.781509656506532e+17\n",
      "Epoch [387/1500], Training Loss: 1.7908628275032934e+17, Validation Loss: 1.781509656506532e+17\n",
      "Epoch [388/1500], Training Loss: 1.790862789383161e+17, Validation Loss: 1.78150948470784e+17\n",
      "Epoch [389/1500], Training Loss: 1.7908627130627155e+17, Validation Loss: 1.78150948470784e+17\n",
      "Epoch [390/1500], Training Loss: 1.7908626831340413e+17, Validation Loss: 1.78150948470784e+17\n",
      "Epoch [391/1500], Training Loss: 1.790862671785041e+17, Validation Loss: 1.78150948470784e+17\n",
      "Epoch [392/1500], Training Loss: 1.7908626013913373e+17, Validation Loss: 1.78150948470784e+17\n",
      "Epoch [393/1500], Training Loss: 1.790862569535615e+17, Validation Loss: 1.78150948470784e+17\n",
      "Epoch [394/1500], Training Loss: 1.790862427216604e+17, Validation Loss: 1.78150948470784e+17\n",
      "Epoch [395/1500], Training Loss: 1.7908623649106928e+17, Validation Loss: 1.781509312909148e+17\n",
      "Epoch [396/1500], Training Loss: 1.7908623272808016e+17, Validation Loss: 1.781509312909148e+17\n",
      "Epoch [397/1500], Training Loss: 1.7908622186185142e+17, Validation Loss: 1.781509312909148e+17\n",
      "Epoch [398/1500], Training Loss: 1.7908621743213766e+17, Validation Loss: 1.7815091411104563e+17\n",
      "Epoch [399/1500], Training Loss: 1.790862122223271e+17, Validation Loss: 1.7815091411104563e+17\n",
      "Epoch [400/1500], Training Loss: 1.7908619721434755e+17, Validation Loss: 1.7815091411104563e+17\n",
      "Epoch [401/1500], Training Loss: 1.790861926135928e+17, Validation Loss: 1.7815089693117645e+17\n",
      "Epoch [402/1500], Training Loss: 1.7908618759315782e+17, Validation Loss: 1.7815089693117645e+17\n",
      "Epoch [403/1500], Training Loss: 1.7908618531212278e+17, Validation Loss: 1.7815089693117645e+17\n",
      "Epoch [404/1500], Training Loss: 1.790861834676327e+17, Validation Loss: 1.7815089693117645e+17\n",
      "Epoch [405/1500], Training Loss: 1.7908617564245654e+17, Validation Loss: 1.7815087975130726e+17\n",
      "Epoch [406/1500], Training Loss: 1.790861622460225e+17, Validation Loss: 1.7815087975130726e+17\n",
      "Epoch [407/1500], Training Loss: 1.7908615884277536e+17, Validation Loss: 1.7815087975130726e+17\n",
      "Epoch [408/1500], Training Loss: 1.7908615089393334e+17, Validation Loss: 1.7815086257143808e+17\n",
      "Epoch [409/1500], Training Loss: 1.790861413248699e+17, Validation Loss: 1.7815086257143808e+17\n",
      "Epoch [410/1500], Training Loss: 1.7908613824266554e+17, Validation Loss: 1.7815086257143808e+17\n",
      "Epoch [411/1500], Training Loss: 1.79086130714499e+17, Validation Loss: 1.7815086257143808e+17\n",
      "Epoch [412/1500], Training Loss: 1.790861254852399e+17, Validation Loss: 1.7815086257143808e+17\n",
      "Epoch [413/1500], Training Loss: 1.790861137279886e+17, Validation Loss: 1.7815086257143808e+17\n",
      "Epoch [414/1500], Training Loss: 1.7908610675054282e+17, Validation Loss: 1.781508282116997e+17\n",
      "Epoch [415/1500], Training Loss: 1.790861034354583e+17, Validation Loss: 1.781508282116997e+17\n",
      "Epoch [416/1500], Training Loss: 1.7908610180488502e+17, Validation Loss: 1.7815081103183053e+17\n",
      "Epoch [417/1500], Training Loss: 1.7908609665994093e+17, Validation Loss: 1.7815079385196134e+17\n",
      "Epoch [418/1500], Training Loss: 1.7908609199965808e+17, Validation Loss: 1.7815079385196134e+17\n",
      "Epoch [419/1500], Training Loss: 1.7908607683019005e+17, Validation Loss: 1.7815079385196134e+17\n",
      "Epoch [420/1500], Training Loss: 1.7908607236961187e+17, Validation Loss: 1.7815079385196134e+17\n",
      "Epoch [421/1500], Training Loss: 1.7908606702875248e+17, Validation Loss: 1.7815079385196134e+17\n",
      "Epoch [422/1500], Training Loss: 1.7908605644186496e+17, Validation Loss: 1.7815075949222298e+17\n",
      "Epoch [423/1500], Training Loss: 1.7908605328739248e+17, Validation Loss: 1.781507423123538e+17\n",
      "Epoch [424/1500], Training Loss: 1.7908604681499542e+17, Validation Loss: 1.781507423123538e+17\n",
      "Epoch [425/1500], Training Loss: 1.790860322715832e+17, Validation Loss: 1.781507423123538e+17\n",
      "Epoch [426/1500], Training Loss: 1.790860298223013e+17, Validation Loss: 1.781507423123538e+17\n",
      "Epoch [427/1500], Training Loss: 1.7908602184326947e+17, Validation Loss: 1.781507251324846e+17\n",
      "Epoch [428/1500], Training Loss: 1.7908601973342742e+17, Validation Loss: 1.781507251324846e+17\n",
      "Epoch [429/1500], Training Loss: 1.7908601835783117e+17, Validation Loss: 1.781507251324846e+17\n",
      "Epoch [430/1500], Training Loss: 1.790860106567977e+17, Validation Loss: 1.7815070795261542e+17\n",
      "Epoch [431/1500], Training Loss: 1.790860021188522e+17, Validation Loss: 1.7815069077274624e+17\n",
      "Epoch [432/1500], Training Loss: 1.7908599338760435e+17, Validation Loss: 1.7815069077274624e+17\n",
      "Epoch [433/1500], Training Loss: 1.7908598626432083e+17, Validation Loss: 1.7815069077274624e+17\n",
      "Epoch [434/1500], Training Loss: 1.790859831688809e+17, Validation Loss: 1.7815067359287706e+17\n",
      "Epoch [435/1500], Training Loss: 1.790859729050183e+17, Validation Loss: 1.7815067359287706e+17\n",
      "Epoch [436/1500], Training Loss: 1.7908596640996848e+17, Validation Loss: 1.7815065641300787e+17\n",
      "Epoch [437/1500], Training Loss: 1.7908596283788112e+17, Validation Loss: 1.7815065641300787e+17\n",
      "Epoch [438/1500], Training Loss: 1.7908594782617264e+17, Validation Loss: 1.781506392331387e+17\n",
      "Epoch [439/1500], Training Loss: 1.790859434160115e+17, Validation Loss: 1.781506392331387e+17\n",
      "Epoch [440/1500], Training Loss: 1.7908593822452512e+17, Validation Loss: 1.781506392331387e+17\n",
      "Epoch [441/1500], Training Loss: 1.7908593641244224e+17, Validation Loss: 1.781506392331387e+17\n",
      "Epoch [442/1500], Training Loss: 1.7908593170032867e+17, Validation Loss: 1.781506220532695e+17\n",
      "Epoch [443/1500], Training Loss: 1.790859265450139e+17, Validation Loss: 1.781506220532695e+17\n",
      "Epoch [444/1500], Training Loss: 1.790859113157554e+17, Validation Loss: 1.781506220532695e+17\n",
      "Epoch [445/1500], Training Loss: 1.7908590937208237e+17, Validation Loss: 1.781506220532695e+17\n",
      "Epoch [446/1500], Training Loss: 1.79085901704368e+17, Validation Loss: 1.781506220532695e+17\n",
      "Epoch [447/1500], Training Loss: 1.7908589082104557e+17, Validation Loss: 1.781506220532695e+17\n",
      "Epoch [448/1500], Training Loss: 1.7908588927782915e+17, Validation Loss: 1.7815058769353114e+17\n",
      "Epoch [449/1500], Training Loss: 1.7908588109020432e+17, Validation Loss: 1.7815058769353114e+17\n",
      "Epoch [450/1500], Training Loss: 1.7908586951427104e+17, Validation Loss: 1.7815057051366195e+17\n",
      "Epoch [451/1500], Training Loss: 1.790858643657146e+17, Validation Loss: 1.7815057051366195e+17\n",
      "Epoch [452/1500], Training Loss: 1.7908585683945315e+17, Validation Loss: 1.7815057051366195e+17\n",
      "Epoch [453/1500], Training Loss: 1.7908585389240765e+17, Validation Loss: 1.7815055333379277e+17\n",
      "Epoch [454/1500], Training Loss: 1.7908585291311638e+17, Validation Loss: 1.7815055333379277e+17\n",
      "Epoch [455/1500], Training Loss: 1.790858456974916e+17, Validation Loss: 1.781505361539236e+17\n",
      "Epoch [456/1500], Training Loss: 1.7908584254099533e+17, Validation Loss: 1.781505189740544e+17\n",
      "Epoch [457/1500], Training Loss: 1.790858279834418e+17, Validation Loss: 1.781505189740544e+17\n",
      "Epoch [458/1500], Training Loss: 1.7908582237113837e+17, Validation Loss: 1.781505189740544e+17\n",
      "Epoch [459/1500], Training Loss: 1.7908581809763302e+17, Validation Loss: 1.781505189740544e+17\n",
      "Epoch [460/1500], Training Loss: 1.7908580725746333e+17, Validation Loss: 1.781505017941852e+17\n",
      "Epoch [461/1500], Training Loss: 1.7908580300856374e+17, Validation Loss: 1.7815048461431603e+17\n",
      "Epoch [462/1500], Training Loss: 1.790857976153094e+17, Validation Loss: 1.7815048461431603e+17\n",
      "Epoch [463/1500], Training Loss: 1.7908578263395936e+17, Validation Loss: 1.7815046743444685e+17\n",
      "Epoch [464/1500], Training Loss: 1.7908577889365222e+17, Validation Loss: 1.7815046743444685e+17\n",
      "Epoch [465/1500], Training Loss: 1.7908577305939318e+17, Validation Loss: 1.7815046743444685e+17\n",
      "Epoch [466/1500], Training Loss: 1.79085770784417e+17, Validation Loss: 1.7815046743444685e+17\n",
      "Epoch [467/1500], Training Loss: 1.7908576908511914e+17, Validation Loss: 1.7815046743444685e+17\n",
      "Epoch [468/1500], Training Loss: 1.7908576112547334e+17, Validation Loss: 1.7815045025457766e+17\n",
      "Epoch [469/1500], Training Loss: 1.790857480479688e+17, Validation Loss: 1.7815043307470848e+17\n",
      "Epoch [470/1500], Training Loss: 1.7908574455721283e+17, Validation Loss: 1.7815043307470848e+17\n",
      "Epoch [471/1500], Training Loss: 1.7908573682284976e+17, Validation Loss: 1.7815043307470848e+17\n",
      "Epoch [472/1500], Training Loss: 1.790857278526755e+17, Validation Loss: 1.7815043307470848e+17\n",
      "Epoch [473/1500], Training Loss: 1.790857238273347e+17, Validation Loss: 1.7815043307470848e+17\n",
      "Epoch [474/1500], Training Loss: 1.7908571650874803e+17, Validation Loss: 1.781504158948393e+17\n",
      "Epoch [475/1500], Training Loss: 1.790857129525863e+17, Validation Loss: 1.781504158948393e+17\n",
      "Epoch [476/1500], Training Loss: 1.7908569901021194e+17, Validation Loss: 1.781504158948393e+17\n",
      "Epoch [477/1500], Training Loss: 1.7908569243403683e+17, Validation Loss: 1.781504158948393e+17\n",
      "Epoch [478/1500], Training Loss: 1.7908568893204192e+17, Validation Loss: 1.781504158948393e+17\n",
      "Epoch [479/1500], Training Loss: 1.790856871063799e+17, Validation Loss: 1.781504158948393e+17\n",
      "Epoch [480/1500], Training Loss: 1.790856825966902e+17, Validation Loss: 1.781503987149701e+17\n",
      "Epoch [481/1500], Training Loss: 1.7908567742584534e+17, Validation Loss: 1.781503987149701e+17\n",
      "Epoch [482/1500], Training Loss: 1.7908566266736912e+17, Validation Loss: 1.7815038153510093e+17\n",
      "Epoch [483/1500], Training Loss: 1.7908565797679446e+17, Validation Loss: 1.7815038153510093e+17\n",
      "Epoch [484/1500], Training Loss: 1.7908565265592285e+17, Validation Loss: 1.7815038153510093e+17\n",
      "Epoch [485/1500], Training Loss: 1.790856415853277e+17, Validation Loss: 1.7815036435523174e+17\n",
      "Epoch [486/1500], Training Loss: 1.790856395115761e+17, Validation Loss: 1.7815036435523174e+17\n",
      "Epoch [487/1500], Training Loss: 1.790856320050421e+17, Validation Loss: 1.7815032999549338e+17\n",
      "Epoch [488/1500], Training Loss: 1.790856184030799e+17, Validation Loss: 1.7815032999549338e+17\n",
      "Epoch [489/1500], Training Loss: 1.790856152549993e+17, Validation Loss: 1.7815032999549338e+17\n",
      "Epoch [490/1500], Training Loss: 1.7908560714692573e+17, Validation Loss: 1.781503128156242e+17\n",
      "Epoch [491/1500], Training Loss: 1.790856049775598e+17, Validation Loss: 1.781503128156242e+17\n",
      "Epoch [492/1500], Training Loss: 1.7908560368729667e+17, Validation Loss: 1.781503128156242e+17\n",
      "Epoch [493/1500], Training Loss: 1.7908559617375245e+17, Validation Loss: 1.781503128156242e+17\n",
      "Epoch [494/1500], Training Loss: 1.7908558905583443e+17, Validation Loss: 1.781503128156242e+17\n",
      "Epoch [495/1500], Training Loss: 1.790855787661419e+17, Validation Loss: 1.781503128156242e+17\n",
      "Epoch [496/1500], Training Loss: 1.790855715517684e+17, Validation Loss: 1.78150295635755e+17\n",
      "Epoch [497/1500], Training Loss: 1.790855684461222e+17, Validation Loss: 1.78150295635755e+17\n",
      "Epoch [498/1500], Training Loss: 1.7908555810848826e+17, Validation Loss: 1.78150295635755e+17\n",
      "Epoch [499/1500], Training Loss: 1.7908555215259958e+17, Validation Loss: 1.78150295635755e+17\n",
      "Epoch [500/1500], Training Loss: 1.790855481861186e+17, Validation Loss: 1.78150295635755e+17\n",
      "Epoch [501/1500], Training Loss: 1.790855330721499e+17, Validation Loss: 1.7815024409614746e+17\n",
      "Epoch [502/1500], Training Loss: 1.790855288537462e+17, Validation Loss: 1.7815024409614746e+17\n",
      "Epoch [503/1500], Training Loss: 1.7908552344705216e+17, Validation Loss: 1.7815024409614746e+17\n",
      "Epoch [504/1500], Training Loss: 1.79085521583276e+17, Validation Loss: 1.7815024409614746e+17\n",
      "Epoch [505/1500], Training Loss: 1.790855169871289e+17, Validation Loss: 1.7815024409614746e+17\n",
      "Epoch [506/1500], Training Loss: 1.790855116819414e+17, Validation Loss: 1.7815024409614746e+17\n",
      "Epoch [507/1500], Training Loss: 1.790854964741864e+17, Validation Loss: 1.7815022691627827e+17\n",
      "Epoch [508/1500], Training Loss: 1.7908549335408662e+17, Validation Loss: 1.781502097364091e+17\n",
      "Epoch [509/1500], Training Loss: 1.790854868111349e+17, Validation Loss: 1.781502097364091e+17\n",
      "Epoch [510/1500], Training Loss: 1.79085475661253e+17, Validation Loss: 1.781502097364091e+17\n",
      "Epoch [511/1500], Training Loss: 1.790854737496832e+17, Validation Loss: 1.781502097364091e+17\n",
      "Epoch [512/1500], Training Loss: 1.7908546601691085e+17, Validation Loss: 1.781501925565399e+17\n",
      "Epoch [513/1500], Training Loss: 1.7908545216420237e+17, Validation Loss: 1.781501925565399e+17\n",
      "Epoch [514/1500], Training Loss: 1.7908544922730067e+17, Validation Loss: 1.781501925565399e+17\n",
      "Epoch [515/1500], Training Loss: 1.790854411723716e+17, Validation Loss: 1.781501925565399e+17\n",
      "Epoch [516/1500], Training Loss: 1.790854389472003e+17, Validation Loss: 1.781501925565399e+17\n",
      "Epoch [517/1500], Training Loss: 1.7908543757306154e+17, Validation Loss: 1.781501925565399e+17\n",
      "Epoch [518/1500], Training Loss: 1.7908542939740438e+17, Validation Loss: 1.781501925565399e+17\n",
      "Epoch [519/1500], Training Loss: 1.790854165501254e+17, Validation Loss: 1.7815017537667072e+17\n",
      "Epoch [520/1500], Training Loss: 1.7908541254932173e+17, Validation Loss: 1.7815017537667072e+17\n",
      "Epoch [521/1500], Training Loss: 1.790854046706242e+17, Validation Loss: 1.7815017537667072e+17\n",
      "Epoch [522/1500], Training Loss: 1.7908539519490774e+17, Validation Loss: 1.7815014101693235e+17\n",
      "Epoch [523/1500], Training Loss: 1.7908539175277408e+17, Validation Loss: 1.7815014101693235e+17\n",
      "Epoch [524/1500], Training Loss: 1.7908538422097226e+17, Validation Loss: 1.7815014101693235e+17\n",
      "Epoch [525/1500], Training Loss: 1.7908537587210048e+17, Validation Loss: 1.7815014101693235e+17\n",
      "Epoch [526/1500], Training Loss: 1.790853668307636e+17, Validation Loss: 1.7815014101693235e+17\n",
      "Epoch [527/1500], Training Loss: 1.79085359390083e+17, Validation Loss: 1.7815014101693235e+17\n",
      "Epoch [528/1500], Training Loss: 1.790853563491347e+17, Validation Loss: 1.7815012383706317e+17\n",
      "Epoch [529/1500], Training Loss: 1.7908535522245258e+17, Validation Loss: 1.7815012383706317e+17\n",
      "Epoch [530/1500], Training Loss: 1.790853479398251e+17, Validation Loss: 1.78150106657194e+17\n",
      "Epoch [531/1500], Training Loss: 1.7908534469743776e+17, Validation Loss: 1.78150106657194e+17\n",
      "Epoch [532/1500], Training Loss: 1.7908533020285187e+17, Validation Loss: 1.78150106657194e+17\n",
      "Epoch [533/1500], Training Loss: 1.7908532344836944e+17, Validation Loss: 1.78150106657194e+17\n",
      "Epoch [534/1500], Training Loss: 1.790853198739523e+17, Validation Loss: 1.781500894773248e+17\n",
      "Epoch [535/1500], Training Loss: 1.7908530931454384e+17, Validation Loss: 1.781500894773248e+17\n",
      "Epoch [536/1500], Training Loss: 1.7908530326685936e+17, Validation Loss: 1.781500894773248e+17\n",
      "Epoch [537/1500], Training Loss: 1.7908529934094314e+17, Validation Loss: 1.781500894773248e+17\n",
      "Epoch [538/1500], Training Loss: 1.790852842014484e+17, Validation Loss: 1.781500894773248e+17\n",
      "Epoch [539/1500], Training Loss: 1.790852792775751e+17, Validation Loss: 1.781500894773248e+17\n",
      "Epoch [540/1500], Training Loss: 1.7908527438544275e+17, Validation Loss: 1.781500894773248e+17\n",
      "Epoch [541/1500], Training Loss: 1.7908527226072237e+17, Validation Loss: 1.781500894773248e+17\n",
      "Epoch [542/1500], Training Loss: 1.790852680538596e+17, Validation Loss: 1.781500722974556e+17\n",
      "Epoch [543/1500], Training Loss: 1.7908526263637008e+17, Validation Loss: 1.781500722974556e+17\n",
      "Epoch [544/1500], Training Loss: 1.790852477226349e+17, Validation Loss: 1.7815005511758643e+17\n",
      "Epoch [545/1500], Training Loss: 1.7908524309924202e+17, Validation Loss: 1.7815005511758643e+17\n",
      "Epoch [546/1500], Training Loss: 1.7908523773112643e+17, Validation Loss: 1.7815005511758643e+17\n",
      "Epoch [547/1500], Training Loss: 1.790852270377876e+17, Validation Loss: 1.7815003793771725e+17\n",
      "Epoch [548/1500], Training Loss: 1.7908522247829715e+17, Validation Loss: 1.7815003793771725e+17\n",
      "Epoch [549/1500], Training Loss: 1.7908521719121942e+17, Validation Loss: 1.7815003793771725e+17\n",
      "Epoch [550/1500], Training Loss: 1.7908520194455286e+17, Validation Loss: 1.7815003793771725e+17\n",
      "Epoch [551/1500], Training Loss: 1.7908519905650877e+17, Validation Loss: 1.7815002075784806e+17\n",
      "Epoch [552/1500], Training Loss: 1.7908519223145085e+17, Validation Loss: 1.7815002075784806e+17\n",
      "Epoch [553/1500], Training Loss: 1.7908518994447994e+17, Validation Loss: 1.7815002075784806e+17\n",
      "Epoch [554/1500], Training Loss: 1.7908518808303987e+17, Validation Loss: 1.7815002075784806e+17\n",
      "Epoch [555/1500], Training Loss: 1.790851803556517e+17, Validation Loss: 1.7815000357797888e+17\n",
      "Epoch [556/1500], Training Loss: 1.7908516660140787e+17, Validation Loss: 1.7815000357797888e+17\n",
      "Epoch [557/1500], Training Loss: 1.7908516345416845e+17, Validation Loss: 1.7815000357797888e+17\n",
      "Epoch [558/1500], Training Loss: 1.7908515534937206e+17, Validation Loss: 1.7815000357797888e+17\n",
      "Epoch [559/1500], Training Loss: 1.7908514451493427e+17, Validation Loss: 1.781499863981097e+17\n",
      "Epoch [560/1500], Training Loss: 1.790851427719487e+17, Validation Loss: 1.781499692182405e+17\n",
      "Epoch [561/1500], Training Loss: 1.790851345913779e+17, Validation Loss: 1.781499692182405e+17\n",
      "Epoch [562/1500], Training Loss: 1.7908512212575043e+17, Validation Loss: 1.781499692182405e+17\n",
      "Epoch [563/1500], Training Loss: 1.7908511789950365e+17, Validation Loss: 1.781499692182405e+17\n",
      "Epoch [564/1500], Training Loss: 1.7908511008511888e+17, Validation Loss: 1.7814995203837133e+17\n",
      "Epoch [565/1500], Training Loss: 1.7908510729352397e+17, Validation Loss: 1.7814995203837133e+17\n",
      "Epoch [566/1500], Training Loss: 1.7908510599607776e+17, Validation Loss: 1.7814995203837133e+17\n",
      "Epoch [567/1500], Training Loss: 1.7908509844537296e+17, Validation Loss: 1.7814993485850214e+17\n",
      "Epoch [568/1500], Training Loss: 1.7908509042565776e+17, Validation Loss: 1.7814991767863296e+17\n",
      "Epoch [569/1500], Training Loss: 1.790850810023779e+17, Validation Loss: 1.7814991767863296e+17\n",
      "Epoch [570/1500], Training Loss: 1.7908507365054288e+17, Validation Loss: 1.7814991767863296e+17\n",
      "Epoch [571/1500], Training Loss: 1.790850706025135e+17, Validation Loss: 1.7814991767863296e+17\n",
      "Epoch [572/1500], Training Loss: 1.790850604231117e+17, Validation Loss: 1.7814991767863296e+17\n",
      "Epoch [573/1500], Training Loss: 1.7908505312256608e+17, Validation Loss: 1.7814991767863296e+17\n",
      "Epoch [574/1500], Training Loss: 1.7908504984977654e+17, Validation Loss: 1.7814991767863296e+17\n",
      "Epoch [575/1500], Training Loss: 1.7908503525866445e+17, Validation Loss: 1.7814991767863296e+17\n",
      "Epoch [576/1500], Training Loss: 1.7908502855190275e+17, Validation Loss: 1.7814990049876378e+17\n",
      "Epoch [577/1500], Training Loss: 1.79085024978941e+17, Validation Loss: 1.7814990049876378e+17\n",
      "Epoch [578/1500], Training Loss: 1.7908502331386595e+17, Validation Loss: 1.7814990049876378e+17\n",
      "Epoch [579/1500], Training Loss: 1.790850173333695e+17, Validation Loss: 1.7814990049876378e+17\n",
      "Epoch [580/1500], Training Loss: 1.790850133402257e+17, Validation Loss: 1.7814990049876378e+17\n",
      "Epoch [581/1500], Training Loss: 1.7908499824794995e+17, Validation Loss: 1.781498833188946e+17\n",
      "Epoch [582/1500], Training Loss: 1.790849934404366e+17, Validation Loss: 1.7814984895915622e+17\n",
      "Epoch [583/1500], Training Loss: 1.790849884790967e+17, Validation Loss: 1.7814984895915622e+17\n",
      "Epoch [584/1500], Training Loss: 1.7908497764302243e+17, Validation Loss: 1.7814984895915622e+17\n",
      "Epoch [585/1500], Training Loss: 1.7908497341030256e+17, Validation Loss: 1.7814984895915622e+17\n",
      "Epoch [586/1500], Training Loss: 1.790849680085326e+17, Validation Loss: 1.7814984895915622e+17\n",
      "Epoch [587/1500], Training Loss: 1.7908495309542e+17, Validation Loss: 1.7814983177928704e+17\n",
      "Epoch [588/1500], Training Loss: 1.790849484531137e+17, Validation Loss: 1.7814981459941786e+17\n",
      "Epoch [589/1500], Training Loss: 1.790849430598344e+17, Validation Loss: 1.7814981459941786e+17\n",
      "Epoch [590/1500], Training Loss: 1.79084941201253e+17, Validation Loss: 1.7814981459941786e+17\n",
      "Epoch [591/1500], Training Loss: 1.790849367759928e+17, Validation Loss: 1.7814981459941786e+17\n",
      "Epoch [592/1500], Training Loss: 1.7908493136634022e+17, Validation Loss: 1.7814981459941786e+17\n",
      "Epoch [593/1500], Training Loss: 1.790849160834188e+17, Validation Loss: 1.7814979741954867e+17\n",
      "Epoch [594/1500], Training Loss: 1.7908491329504694e+17, Validation Loss: 1.7814979741954867e+17\n",
      "Epoch [595/1500], Training Loss: 1.790849064605511e+17, Validation Loss: 1.781497630598103e+17\n",
      "Epoch [596/1500], Training Loss: 1.7908489521450106e+17, Validation Loss: 1.781497630598103e+17\n",
      "Epoch [597/1500], Training Loss: 1.7908489338150394e+17, Validation Loss: 1.7814974587994112e+17\n",
      "Epoch [598/1500], Training Loss: 1.7908488554574685e+17, Validation Loss: 1.7814974587994112e+17\n",
      "Epoch [599/1500], Training Loss: 1.790848717331367e+17, Validation Loss: 1.7814974587994112e+17\n",
      "Epoch [600/1500], Training Loss: 1.7908486851128886e+17, Validation Loss: 1.7814974587994112e+17\n",
      "Epoch [601/1500], Training Loss: 1.790848604674572e+17, Validation Loss: 1.7814974587994112e+17\n",
      "Epoch [602/1500], Training Loss: 1.790848582371016e+17, Validation Loss: 1.7814974587994112e+17\n",
      "Epoch [603/1500], Training Loss: 1.7908485683443658e+17, Validation Loss: 1.7814971152020275e+17\n",
      "Epoch [604/1500], Training Loss: 1.7908484862479616e+17, Validation Loss: 1.7814971152020275e+17\n",
      "Epoch [605/1500], Training Loss: 1.7908483625337968e+17, Validation Loss: 1.7814969434033357e+17\n",
      "Epoch [606/1500], Training Loss: 1.7908483199634554e+17, Validation Loss: 1.7814969434033357e+17\n",
      "Epoch [607/1500], Training Loss: 1.7908482419490698e+17, Validation Loss: 1.781496771604644e+17\n",
      "Epoch [608/1500], Training Loss: 1.7908481458052554e+17, Validation Loss: 1.781496771604644e+17\n",
      "Epoch [609/1500], Training Loss: 1.7908481113579555e+17, Validation Loss: 1.781496771604644e+17\n",
      "Epoch [610/1500], Training Loss: 1.79084803531534e+17, Validation Loss: 1.781496599805952e+17\n",
      "Epoch [611/1500], Training Loss: 1.790847961342519e+17, Validation Loss: 1.78149642800726e+17\n",
      "Epoch [612/1500], Training Loss: 1.7908478605370826e+17, Validation Loss: 1.78149642800726e+17\n",
      "Epoch [613/1500], Training Loss: 1.7908477882220573e+17, Validation Loss: 1.78149642800726e+17\n",
      "Epoch [614/1500], Training Loss: 1.7908477574648083e+17, Validation Loss: 1.78149642800726e+17\n",
      "Epoch [615/1500], Training Loss: 1.790847745945663e+17, Validation Loss: 1.78149642800726e+17\n",
      "Epoch [616/1500], Training Loss: 1.7908476728297952e+17, Validation Loss: 1.7814962562085683e+17\n",
      "Epoch [617/1500], Training Loss: 1.7908476399991088e+17, Validation Loss: 1.7814960844098765e+17\n",
      "Epoch [618/1500], Training Loss: 1.7908474950792138e+17, Validation Loss: 1.7814960844098765e+17\n",
      "Epoch [619/1500], Training Loss: 1.7908474287304275e+17, Validation Loss: 1.7814960844098765e+17\n",
      "Epoch [620/1500], Training Loss: 1.7908473930454282e+17, Validation Loss: 1.7814959126111846e+17\n",
      "Epoch [621/1500], Training Loss: 1.790847286215747e+17, Validation Loss: 1.7814959126111846e+17\n",
      "Epoch [622/1500], Training Loss: 1.7908472275025718e+17, Validation Loss: 1.7814959126111846e+17\n",
      "Epoch [623/1500], Training Loss: 1.7908471865934624e+17, Validation Loss: 1.7814959126111846e+17\n",
      "Epoch [624/1500], Training Loss: 1.7908470345394192e+17, Validation Loss: 1.781495569013801e+17\n",
      "Epoch [625/1500], Training Loss: 1.7908469884086774e+17, Validation Loss: 1.781495569013801e+17\n",
      "Epoch [626/1500], Training Loss: 1.7908469375681555e+17, Validation Loss: 1.781495569013801e+17\n",
      "Epoch [627/1500], Training Loss: 1.790846916377938e+17, Validation Loss: 1.781495569013801e+17\n",
      "Epoch [628/1500], Training Loss: 1.7908468737047226e+17, Validation Loss: 1.781495569013801e+17\n",
      "Epoch [629/1500], Training Loss: 1.790846819269425e+17, Validation Loss: 1.781495569013801e+17\n",
      "Epoch [630/1500], Training Loss: 1.7908466701318237e+17, Validation Loss: 1.781495569013801e+17\n",
      "Epoch [631/1500], Training Loss: 1.7908466226842426e+17, Validation Loss: 1.781495569013801e+17\n",
      "Epoch [632/1500], Training Loss: 1.7908465693172906e+17, Validation Loss: 1.781495569013801e+17\n",
      "Epoch [633/1500], Training Loss: 1.7908464618686765e+17, Validation Loss: 1.781495397215109e+17\n",
      "Epoch [634/1500], Training Loss: 1.790846420593021e+17, Validation Loss: 1.781495397215109e+17\n",
      "Epoch [635/1500], Training Loss: 1.7908463640500362e+17, Validation Loss: 1.781495397215109e+17\n",
      "Epoch [636/1500], Training Loss: 1.7908462099401494e+17, Validation Loss: 1.7814952254164173e+17\n",
      "Epoch [637/1500], Training Loss: 1.7908461844061152e+17, Validation Loss: 1.7814950536177254e+17\n",
      "Epoch [638/1500], Training Loss: 1.7908461140928842e+17, Validation Loss: 1.7814950536177254e+17\n",
      "Epoch [639/1500], Training Loss: 1.7908460911252563e+17, Validation Loss: 1.7814950536177254e+17\n",
      "Epoch [640/1500], Training Loss: 1.7908460730798403e+17, Validation Loss: 1.7814948818190336e+17\n",
      "Epoch [641/1500], Training Loss: 1.7908459943725875e+17, Validation Loss: 1.7814948818190336e+17\n",
      "Epoch [642/1500], Training Loss: 1.7908458580350774e+17, Validation Loss: 1.7814947100203418e+17\n",
      "Epoch [643/1500], Training Loss: 1.7908458251437408e+17, Validation Loss: 1.7814947100203418e+17\n",
      "Epoch [644/1500], Training Loss: 1.7908457439474307e+17, Validation Loss: 1.78149453822165e+17\n",
      "Epoch [645/1500], Training Loss: 1.790845637166095e+17, Validation Loss: 1.7814941946242662e+17\n",
      "Epoch [646/1500], Training Loss: 1.7908456196001978e+17, Validation Loss: 1.7814941946242662e+17\n",
      "Epoch [647/1500], Training Loss: 1.7908455384113622e+17, Validation Loss: 1.7814941946242662e+17\n",
      "Epoch [648/1500], Training Loss: 1.7908454174032058e+17, Validation Loss: 1.7814940228255744e+17\n",
      "Epoch [649/1500], Training Loss: 1.7908453716180432e+17, Validation Loss: 1.7814940228255744e+17\n",
      "Epoch [650/1500], Training Loss: 1.7908452942876966e+17, Validation Loss: 1.7814938510268826e+17\n",
      "Epoch [651/1500], Training Loss: 1.790845265911884e+17, Validation Loss: 1.7814938510268826e+17\n",
      "Epoch [652/1500], Training Loss: 1.7908452530030275e+17, Validation Loss: 1.7814936792281907e+17\n",
      "Epoch [653/1500], Training Loss: 1.7908451774066182e+17, Validation Loss: 1.7814936792281907e+17\n",
      "Epoch [654/1500], Training Loss: 1.7908451075816918e+17, Validation Loss: 1.7814936792281907e+17\n",
      "Epoch [655/1500], Training Loss: 1.7908450044542624e+17, Validation Loss: 1.7814936792281907e+17\n",
      "Epoch [656/1500], Training Loss: 1.7908449313665235e+17, Validation Loss: 1.781493507429499e+17\n",
      "Epoch [657/1500], Training Loss: 1.7908449002115187e+17, Validation Loss: 1.781493335630807e+17\n",
      "Epoch [658/1500], Training Loss: 1.7908447980899725e+17, Validation Loss: 1.781493335630807e+17\n",
      "Epoch [659/1500], Training Loss: 1.7908447261226928e+17, Validation Loss: 1.781493335630807e+17\n",
      "Epoch [660/1500], Training Loss: 1.7908446933153466e+17, Validation Loss: 1.7814929920334234e+17\n",
      "Epoch [661/1500], Training Loss: 1.7908445470592704e+17, Validation Loss: 1.7814929920334234e+17\n",
      "Epoch [662/1500], Training Loss: 1.7908444816032077e+17, Validation Loss: 1.7814926484360397e+17\n",
      "Epoch [663/1500], Training Loss: 1.7908444452172422e+17, Validation Loss: 1.7814926484360397e+17\n",
      "Epoch [664/1500], Training Loss: 1.7908444277317747e+17, Validation Loss: 1.7814926484360397e+17\n",
      "Epoch [665/1500], Training Loss: 1.7908443709646778e+17, Validation Loss: 1.7814926484360397e+17\n",
      "Epoch [666/1500], Training Loss: 1.790844328992574e+17, Validation Loss: 1.7814926484360397e+17\n",
      "Epoch [667/1500], Training Loss: 1.7908441767505008e+17, Validation Loss: 1.781492476637348e+17\n",
      "Epoch [668/1500], Training Loss: 1.7908441316207494e+17, Validation Loss: 1.781492304838656e+17\n",
      "Epoch [669/1500], Training Loss: 1.790844078515512e+17, Validation Loss: 1.781492304838656e+17\n",
      "Epoch [670/1500], Training Loss: 1.7908439702854394e+17, Validation Loss: 1.781492304838656e+17\n",
      "Epoch [671/1500], Training Loss: 1.790843926343793e+17, Validation Loss: 1.781492304838656e+17\n",
      "Epoch [672/1500], Training Loss: 1.7908438729781526e+17, Validation Loss: 1.781492133039964e+17\n",
      "Epoch [673/1500], Training Loss: 1.7908437241301446e+17, Validation Loss: 1.7814919612412723e+17\n",
      "Epoch [674/1500], Training Loss: 1.7908436769734483e+17, Validation Loss: 1.7814919612412723e+17\n",
      "Epoch [675/1500], Training Loss: 1.7908436231050534e+17, Validation Loss: 1.7814919612412723e+17\n",
      "Epoch [676/1500], Training Loss: 1.7908436044907987e+17, Validation Loss: 1.7814919612412723e+17\n",
      "Epoch [677/1500], Training Loss: 1.7908435648499328e+17, Validation Loss: 1.7814919612412723e+17\n",
      "Epoch [678/1500], Training Loss: 1.7908435074873466e+17, Validation Loss: 1.7814919612412723e+17\n",
      "Epoch [679/1500], Training Loss: 1.7908433533311757e+17, Validation Loss: 1.7814919612412723e+17\n",
      "Epoch [680/1500], Training Loss: 1.7908433312798403e+17, Validation Loss: 1.7814914458451968e+17\n",
      "Epoch [681/1500], Training Loss: 1.7908432571712272e+17, Validation Loss: 1.7814914458451968e+17\n",
      "Epoch [682/1500], Training Loss: 1.7908431443396426e+17, Validation Loss: 1.7814914458451968e+17\n",
      "Epoch [683/1500], Training Loss: 1.7908431268936093e+17, Validation Loss: 1.7814914458451968e+17\n",
      "Epoch [684/1500], Training Loss: 1.7908430473613622e+17, Validation Loss: 1.7814914458451968e+17\n",
      "Epoch [685/1500], Training Loss: 1.7908429111446323e+17, Validation Loss: 1.781491274046505e+17\n",
      "Epoch [686/1500], Training Loss: 1.790842879659767e+17, Validation Loss: 1.781491274046505e+17\n",
      "Epoch [687/1500], Training Loss: 1.7908427987222566e+17, Validation Loss: 1.781491274046505e+17\n",
      "Epoch [688/1500], Training Loss: 1.790842776941339e+17, Validation Loss: 1.781491274046505e+17\n",
      "Epoch [689/1500], Training Loss: 1.7908427626622186e+17, Validation Loss: 1.781491274046505e+17\n",
      "Epoch [690/1500], Training Loss: 1.7908426816571245e+17, Validation Loss: 1.7814909304491213e+17\n",
      "Epoch [691/1500], Training Loss: 1.7908425614031517e+17, Validation Loss: 1.7814909304491213e+17\n",
      "Epoch [692/1500], Training Loss: 1.7908425124625283e+17, Validation Loss: 1.7814909304491213e+17\n",
      "Epoch [693/1500], Training Loss: 1.7908424350859184e+17, Validation Loss: 1.7814907586504294e+17\n",
      "Epoch [694/1500], Training Loss: 1.7908423463044662e+17, Validation Loss: 1.7814904150530458e+17\n",
      "Epoch [695/1500], Training Loss: 1.7908423051655814e+17, Validation Loss: 1.7814904150530458e+17\n",
      "Epoch [696/1500], Training Loss: 1.7908422299749648e+17, Validation Loss: 1.7814904150530458e+17\n",
      "Epoch [697/1500], Training Loss: 1.790842159302579e+17, Validation Loss: 1.781490243254354e+17\n",
      "Epoch [698/1500], Training Loss: 1.790842055412971e+17, Validation Loss: 1.781490243254354e+17\n",
      "Epoch [699/1500], Training Loss: 1.7908419831349648e+17, Validation Loss: 1.781490071455662e+17\n",
      "Epoch [700/1500], Training Loss: 1.7908419518598666e+17, Validation Loss: 1.781490071455662e+17\n",
      "Epoch [701/1500], Training Loss: 1.7908419395356317e+17, Validation Loss: 1.7814898996569702e+17\n",
      "Epoch [702/1500], Training Loss: 1.790841868071627e+17, Validation Loss: 1.7814898996569702e+17\n",
      "Epoch [703/1500], Training Loss: 1.7908418347169498e+17, Validation Loss: 1.7814898996569702e+17\n",
      "Epoch [704/1500], Training Loss: 1.7908416893425414e+17, Validation Loss: 1.7814898996569702e+17\n",
      "Epoch [705/1500], Training Loss: 1.7908416245628768e+17, Validation Loss: 1.7814897278582784e+17\n",
      "Epoch [706/1500], Training Loss: 1.790841587568326e+17, Validation Loss: 1.7814897278582784e+17\n",
      "Epoch [707/1500], Training Loss: 1.7908414816418208e+17, Validation Loss: 1.7814897278582784e+17\n",
      "Epoch [708/1500], Training Loss: 1.7908414260391562e+17, Validation Loss: 1.7814897278582784e+17\n",
      "Epoch [709/1500], Training Loss: 1.7908413824678038e+17, Validation Loss: 1.7814897278582784e+17\n",
      "Epoch [710/1500], Training Loss: 1.7908412293664864e+17, Validation Loss: 1.781489212462203e+17\n",
      "Epoch [711/1500], Training Loss: 1.7908411856902816e+17, Validation Loss: 1.781489212462203e+17\n",
      "Epoch [712/1500], Training Loss: 1.7908411327121536e+17, Validation Loss: 1.781489212462203e+17\n",
      "Epoch [713/1500], Training Loss: 1.790841113025286e+17, Validation Loss: 1.781489212462203e+17\n",
      "Epoch [714/1500], Training Loss: 1.7908410679988048e+17, Validation Loss: 1.781489212462203e+17\n",
      "Epoch [715/1500], Training Loss: 1.7908410139720074e+17, Validation Loss: 1.781489212462203e+17\n",
      "Epoch [716/1500], Training Loss: 1.7908408650261424e+17, Validation Loss: 1.781489040663511e+17\n",
      "Epoch [717/1500], Training Loss: 1.790840817904487e+17, Validation Loss: 1.781489040663511e+17\n",
      "Epoch [718/1500], Training Loss: 1.7908407646566707e+17, Validation Loss: 1.781489040663511e+17\n",
      "Epoch [719/1500], Training Loss: 1.79084065526125e+17, Validation Loss: 1.7814888688648192e+17\n",
      "Epoch [720/1500], Training Loss: 1.7908406157897805e+17, Validation Loss: 1.7814886970661274e+17\n",
      "Epoch [721/1500], Training Loss: 1.7908405583521155e+17, Validation Loss: 1.7814886970661274e+17\n",
      "Epoch [722/1500], Training Loss: 1.7908404027480195e+17, Validation Loss: 1.7814886970661274e+17\n",
      "Epoch [723/1500], Training Loss: 1.7908403822140877e+17, Validation Loss: 1.7814886970661274e+17\n",
      "Epoch [724/1500], Training Loss: 1.790840306232269e+17, Validation Loss: 1.7814886970661274e+17\n",
      "Epoch [725/1500], Training Loss: 1.79084028348942e+17, Validation Loss: 1.7814886970661274e+17\n",
      "Epoch [726/1500], Training Loss: 1.7908402661735155e+17, Validation Loss: 1.7814886970661274e+17\n",
      "Epoch [727/1500], Training Loss: 1.7908401864252138e+17, Validation Loss: 1.7814886970661274e+17\n",
      "Epoch [728/1500], Training Loss: 1.7908400512704374e+17, Validation Loss: 1.7814883534687437e+17\n",
      "Epoch [729/1500], Training Loss: 1.7908400192107786e+17, Validation Loss: 1.7814883534687437e+17\n",
      "Epoch [730/1500], Training Loss: 1.7908399385533258e+17, Validation Loss: 1.781488181670052e+17\n",
      "Epoch [731/1500], Training Loss: 1.7908398334533142e+17, Validation Loss: 1.781488181670052e+17\n",
      "Epoch [732/1500], Training Loss: 1.7908398156245366e+17, Validation Loss: 1.781488181670052e+17\n",
      "Epoch [733/1500], Training Loss: 1.7908397345124458e+17, Validation Loss: 1.781488181670052e+17\n",
      "Epoch [734/1500], Training Loss: 1.790839617561781e+17, Validation Loss: 1.78148800987136e+17\n",
      "Epoch [735/1500], Training Loss: 1.790839567643861e+17, Validation Loss: 1.78148800987136e+17\n",
      "Epoch [736/1500], Training Loss: 1.7908394900600867e+17, Validation Loss: 1.78148800987136e+17\n",
      "Epoch [737/1500], Training Loss: 1.7908394611495395e+17, Validation Loss: 1.78148800987136e+17\n",
      "Epoch [738/1500], Training Loss: 1.7908394482751e+17, Validation Loss: 1.78148800987136e+17\n",
      "Epoch [739/1500], Training Loss: 1.790839373184048e+17, Validation Loss: 1.7814876662739763e+17\n",
      "Epoch [740/1500], Training Loss: 1.7908393077640256e+17, Validation Loss: 1.7814874944752845e+17\n",
      "Epoch [741/1500], Training Loss: 1.7908391979120726e+17, Validation Loss: 1.7814874944752845e+17\n",
      "Epoch [742/1500], Training Loss: 1.790839125553887e+17, Validation Loss: 1.7814873226765926e+17\n",
      "Epoch [743/1500], Training Loss: 1.7908390932776733e+17, Validation Loss: 1.7814873226765926e+17\n",
      "Epoch [744/1500], Training Loss: 1.7908389891464637e+17, Validation Loss: 1.7814873226765926e+17\n",
      "Epoch [745/1500], Training Loss: 1.7908389190614678e+17, Validation Loss: 1.7814873226765926e+17\n",
      "Epoch [746/1500], Training Loss: 1.7908388853272314e+17, Validation Loss: 1.781486979079209e+17\n",
      "Epoch [747/1500], Training Loss: 1.790838740333132e+17, Validation Loss: 1.781486807280517e+17\n",
      "Epoch [748/1500], Training Loss: 1.7908386752300618e+17, Validation Loss: 1.781486807280517e+17\n",
      "Epoch [749/1500], Training Loss: 1.7908386387467187e+17, Validation Loss: 1.781486807280517e+17\n",
      "Epoch [750/1500], Training Loss: 1.7908386205767546e+17, Validation Loss: 1.781486807280517e+17\n",
      "Epoch [751/1500], Training Loss: 1.7908385668383222e+17, Validation Loss: 1.7814866354818253e+17\n",
      "Epoch [752/1500], Training Loss: 1.7908385215951197e+17, Validation Loss: 1.7814866354818253e+17\n",
      "Epoch [753/1500], Training Loss: 1.790838368577168e+17, Validation Loss: 1.7814866354818253e+17\n",
      "Epoch [754/1500], Training Loss: 1.7908383246187197e+17, Validation Loss: 1.7814864636831334e+17\n",
      "Epoch [755/1500], Training Loss: 1.7908382717111734e+17, Validation Loss: 1.7814864636831334e+17\n",
      "Epoch [756/1500], Training Loss: 1.790838164810932e+17, Validation Loss: 1.7814862918844416e+17\n",
      "Epoch [757/1500], Training Loss: 1.7908381190533366e+17, Validation Loss: 1.781485948287058e+17\n",
      "Epoch [758/1500], Training Loss: 1.7908380653301658e+17, Validation Loss: 1.781485948287058e+17\n",
      "Epoch [759/1500], Training Loss: 1.7908379161505696e+17, Validation Loss: 1.781485948287058e+17\n",
      "Epoch [760/1500], Training Loss: 1.7908378691739296e+17, Validation Loss: 1.781485776488366e+17\n",
      "Epoch [761/1500], Training Loss: 1.7908378157928822e+17, Validation Loss: 1.7814856046896742e+17\n",
      "Epoch [762/1500], Training Loss: 1.7908377969511005e+17, Validation Loss: 1.7814856046896742e+17\n",
      "Epoch [763/1500], Training Loss: 1.7908377591965162e+17, Validation Loss: 1.7814856046896742e+17\n",
      "Epoch [764/1500], Training Loss: 1.7908377001196896e+17, Validation Loss: 1.7814856046896742e+17\n",
      "Epoch [765/1500], Training Loss: 1.7908375460528595e+17, Validation Loss: 1.7814854328909824e+17\n",
      "Epoch [766/1500], Training Loss: 1.7908375258179533e+17, Validation Loss: 1.7814852610922906e+17\n",
      "Epoch [767/1500], Training Loss: 1.790837450141218e+17, Validation Loss: 1.7814850892935987e+17\n",
      "Epoch [768/1500], Training Loss: 1.790837336672232e+17, Validation Loss: 1.781484917494907e+17\n",
      "Epoch [769/1500], Training Loss: 1.7908373195417766e+17, Validation Loss: 1.781484917494907e+17\n",
      "Epoch [770/1500], Training Loss: 1.7908372397734874e+17, Validation Loss: 1.781484917494907e+17\n",
      "Epoch [771/1500], Training Loss: 1.7908371056478925e+17, Validation Loss: 1.781484917494907e+17\n",
      "Epoch [772/1500], Training Loss: 1.790837071042024e+17, Validation Loss: 1.781484745696215e+17\n",
      "Epoch [773/1500], Training Loss: 1.7908369901374938e+17, Validation Loss: 1.7814845738975232e+17\n",
      "Epoch [774/1500], Training Loss: 1.7908369682379613e+17, Validation Loss: 1.7814845738975232e+17\n",
      "Epoch [775/1500], Training Loss: 1.7908369543531414e+17, Validation Loss: 1.7814845738975232e+17\n",
      "Epoch [776/1500], Training Loss: 1.790836873132159e+17, Validation Loss: 1.7814845738975232e+17\n",
      "Epoch [777/1500], Training Loss: 1.7908367587512534e+17, Validation Loss: 1.7814842303001395e+17\n",
      "Epoch [778/1500], Training Loss: 1.7908367045237453e+17, Validation Loss: 1.7814842303001395e+17\n",
      "Epoch [779/1500], Training Loss: 1.7908366275844096e+17, Validation Loss: 1.7814840585014477e+17\n",
      "Epoch [780/1500], Training Loss: 1.7908365479339437e+17, Validation Loss: 1.781483714904064e+17\n",
      "Epoch [781/1500], Training Loss: 1.790836497172874e+17, Validation Loss: 1.781483714904064e+17\n",
      "Epoch [782/1500], Training Loss: 1.790836423041858e+17, Validation Loss: 1.781483714904064e+17\n",
      "Epoch [783/1500], Training Loss: 1.790836361825947e+17, Validation Loss: 1.781483543105372e+17\n",
      "Epoch [784/1500], Training Loss: 1.7908362490632362e+17, Validation Loss: 1.781483543105372e+17\n",
      "Epoch [785/1500], Training Loss: 1.790836176701095e+17, Validation Loss: 1.7814833713066803e+17\n",
      "Epoch [786/1500], Training Loss: 1.7908361437892502e+17, Validation Loss: 1.7814833713066803e+17\n",
      "Epoch [787/1500], Training Loss: 1.7908361313907693e+17, Validation Loss: 1.7814831995079885e+17\n",
      "Epoch [788/1500], Training Loss: 1.7908360617447542e+17, Validation Loss: 1.7814831995079885e+17\n",
      "Epoch [789/1500], Training Loss: 1.790836027093372e+17, Validation Loss: 1.7814830277092966e+17\n",
      "Epoch [790/1500], Training Loss: 1.790835882084074e+17, Validation Loss: 1.7814830277092966e+17\n",
      "Epoch [791/1500], Training Loss: 1.7908358184602218e+17, Validation Loss: 1.7814830277092966e+17\n",
      "Epoch [792/1500], Training Loss: 1.7908357811277747e+17, Validation Loss: 1.7814828559106048e+17\n",
      "Epoch [793/1500], Training Loss: 1.790835673939064e+17, Validation Loss: 1.7814828559106048e+17\n",
      "Epoch [794/1500], Training Loss: 1.7908356218834544e+17, Validation Loss: 1.7814828559106048e+17\n",
      "Epoch [795/1500], Training Loss: 1.7908355754771315e+17, Validation Loss: 1.7814828559106048e+17\n",
      "Epoch [796/1500], Training Loss: 1.7908354229671606e+17, Validation Loss: 1.781482684111913e+17\n",
      "Epoch [797/1500], Training Loss: 1.790835378958118e+17, Validation Loss: 1.781482684111913e+17\n",
      "Epoch [798/1500], Training Loss: 1.7908353259193405e+17, Validation Loss: 1.781482684111913e+17\n",
      "Epoch [799/1500], Training Loss: 1.7908353064197747e+17, Validation Loss: 1.781482684111913e+17\n",
      "Epoch [800/1500], Training Loss: 1.7908352608409654e+17, Validation Loss: 1.7814823405145293e+17\n",
      "Epoch [801/1500], Training Loss: 1.7908352071618717e+17, Validation Loss: 1.7814823405145293e+17\n",
      "Epoch [802/1500], Training Loss: 1.7908350580387408e+17, Validation Loss: 1.7814821687158374e+17\n",
      "Epoch [803/1500], Training Loss: 1.7908350110727616e+17, Validation Loss: 1.7814821687158374e+17\n",
      "Epoch [804/1500], Training Loss: 1.790834957473806e+17, Validation Loss: 1.7814821687158374e+17\n",
      "Epoch [805/1500], Training Loss: 1.7908348483007286e+17, Validation Loss: 1.7814821687158374e+17\n",
      "Epoch [806/1500], Training Loss: 1.790834812976134e+17, Validation Loss: 1.7814818251184538e+17\n",
      "Epoch [807/1500], Training Loss: 1.790834751718436e+17, Validation Loss: 1.7814818251184538e+17\n",
      "Epoch [808/1500], Training Loss: 1.7908345982488653e+17, Validation Loss: 1.781481653319762e+17\n",
      "Epoch [809/1500], Training Loss: 1.7908345776051258e+17, Validation Loss: 1.78148148152107e+17\n",
      "Epoch [810/1500], Training Loss: 1.7908345016492288e+17, Validation Loss: 1.78148148152107e+17\n",
      "Epoch [811/1500], Training Loss: 1.790834478991995e+17, Validation Loss: 1.78148148152107e+17\n",
      "Epoch [812/1500], Training Loss: 1.790834462035828e+17, Validation Loss: 1.78148148152107e+17\n",
      "Epoch [813/1500], Training Loss: 1.7908343811621747e+17, Validation Loss: 1.78148148152107e+17\n",
      "Epoch [814/1500], Training Loss: 1.7908342481086106e+17, Validation Loss: 1.7814813097223782e+17\n",
      "Epoch [815/1500], Training Loss: 1.790834213309777e+17, Validation Loss: 1.7814813097223782e+17\n",
      "Epoch [816/1500], Training Loss: 1.790834132068703e+17, Validation Loss: 1.7814811379236864e+17\n",
      "Epoch [817/1500], Training Loss: 1.7908340258633274e+17, Validation Loss: 1.7814809661249946e+17\n",
      "Epoch [818/1500], Training Loss: 1.7908340085071146e+17, Validation Loss: 1.7814809661249946e+17\n",
      "Epoch [819/1500], Training Loss: 1.7908339280730253e+17, Validation Loss: 1.7814809661249946e+17\n",
      "Epoch [820/1500], Training Loss: 1.790833815498012e+17, Validation Loss: 1.7814807943263027e+17\n",
      "Epoch [821/1500], Training Loss: 1.7908337587565456e+17, Validation Loss: 1.7814807943263027e+17\n",
      "Epoch [822/1500], Training Loss: 1.7908336826205718e+17, Validation Loss: 1.781480622527611e+17\n",
      "Epoch [823/1500], Training Loss: 1.79083365211038e+17, Validation Loss: 1.781480622527611e+17\n",
      "Epoch [824/1500], Training Loss: 1.7908336393646954e+17, Validation Loss: 1.781480450728919e+17\n",
      "Epoch [825/1500], Training Loss: 1.7908335654878992e+17, Validation Loss: 1.781480450728919e+17\n",
      "Epoch [826/1500], Training Loss: 1.7908335052962144e+17, Validation Loss: 1.781480450728919e+17\n",
      "Epoch [827/1500], Training Loss: 1.7908333912468342e+17, Validation Loss: 1.781480450728919e+17\n",
      "Epoch [828/1500], Training Loss: 1.7908333185133786e+17, Validation Loss: 1.781480450728919e+17\n",
      "Epoch [829/1500], Training Loss: 1.79083328582648e+17, Validation Loss: 1.781480450728919e+17\n",
      "Epoch [830/1500], Training Loss: 1.790833182059443e+17, Validation Loss: 1.7814802789302272e+17\n",
      "Epoch [831/1500], Training Loss: 1.7908331130241994e+17, Validation Loss: 1.7814802789302272e+17\n",
      "Epoch [832/1500], Training Loss: 1.7908330783752118e+17, Validation Loss: 1.7814802789302272e+17\n",
      "Epoch [833/1500], Training Loss: 1.790832931832104e+17, Validation Loss: 1.7814801071315354e+17\n",
      "Epoch [834/1500], Training Loss: 1.7908328698367744e+17, Validation Loss: 1.7814801071315354e+17\n",
      "Epoch [835/1500], Training Loss: 1.7908328316719e+17, Validation Loss: 1.7814801071315354e+17\n",
      "Epoch [836/1500], Training Loss: 1.7908328129612467e+17, Validation Loss: 1.7814801071315354e+17\n",
      "Epoch [837/1500], Training Loss: 1.7908327614116605e+17, Validation Loss: 1.7814799353328435e+17\n",
      "Epoch [838/1500], Training Loss: 1.7908327143095766e+17, Validation Loss: 1.7814799353328435e+17\n",
      "Epoch [839/1500], Training Loss: 1.790832562859914e+17, Validation Loss: 1.7814799353328435e+17\n",
      "Epoch [840/1500], Training Loss: 1.790832518939151e+17, Validation Loss: 1.7814799353328435e+17\n",
      "Epoch [841/1500], Training Loss: 1.790832466017427e+17, Validation Loss: 1.7814799353328435e+17\n",
      "Epoch [842/1500], Training Loss: 1.790832360370209e+17, Validation Loss: 1.7814799353328435e+17\n",
      "Epoch [843/1500], Training Loss: 1.7908323142250803e+17, Validation Loss: 1.7814799353328435e+17\n",
      "Epoch [844/1500], Training Loss: 1.790832260475655e+17, Validation Loss: 1.7814799353328435e+17\n",
      "Epoch [845/1500], Training Loss: 1.790832109918341e+17, Validation Loss: 1.78147959173546e+17\n",
      "Epoch [846/1500], Training Loss: 1.790832063346891e+17, Validation Loss: 1.78147959173546e+17\n",
      "Epoch [847/1500], Training Loss: 1.790832010358311e+17, Validation Loss: 1.781479419936768e+17\n",
      "Epoch [848/1500], Training Loss: 1.7908319907332806e+17, Validation Loss: 1.781479419936768e+17\n",
      "Epoch [849/1500], Training Loss: 1.790831957596845e+17, Validation Loss: 1.781479419936768e+17\n",
      "Epoch [850/1500], Training Loss: 1.790831894475685e+17, Validation Loss: 1.781479419936768e+17\n",
      "Epoch [851/1500], Training Loss: 1.7908317421954483e+17, Validation Loss: 1.781479419936768e+17\n",
      "Epoch [852/1500], Training Loss: 1.7908317207064352e+17, Validation Loss: 1.781479419936768e+17\n",
      "Epoch [853/1500], Training Loss: 1.7908316438531302e+17, Validation Loss: 1.7814790763393843e+17\n",
      "Epoch [854/1500], Training Loss: 1.7908315292180467e+17, Validation Loss: 1.7814790763393843e+17\n",
      "Epoch [855/1500], Training Loss: 1.7908315126616147e+17, Validation Loss: 1.7814790763393843e+17\n",
      "Epoch [856/1500], Training Loss: 1.7908314323380618e+17, Validation Loss: 1.7814789045406925e+17\n",
      "Epoch [857/1500], Training Loss: 1.7908312995649718e+17, Validation Loss: 1.7814787327420006e+17\n",
      "Epoch [858/1500], Training Loss: 1.790831264097777e+17, Validation Loss: 1.7814787327420006e+17\n",
      "Epoch [859/1500], Training Loss: 1.7908311820293978e+17, Validation Loss: 1.7814787327420006e+17\n",
      "Epoch [860/1500], Training Loss: 1.7908311601432742e+17, Validation Loss: 1.7814787327420006e+17\n",
      "Epoch [861/1500], Training Loss: 1.7908311463302022e+17, Validation Loss: 1.7814787327420006e+17\n",
      "Epoch [862/1500], Training Loss: 1.7908310672999606e+17, Validation Loss: 1.781478217345925e+17\n",
      "Epoch [863/1500], Training Loss: 1.7908309570102557e+17, Validation Loss: 1.781478217345925e+17\n",
      "Epoch [864/1500], Training Loss: 1.7908308971164298e+17, Validation Loss: 1.781478217345925e+17\n",
      "Epoch [865/1500], Training Loss: 1.79083082172933e+17, Validation Loss: 1.7814780455472333e+17\n",
      "Epoch [866/1500], Training Loss: 1.7908307526679987e+17, Validation Loss: 1.7814780455472333e+17\n",
      "Epoch [867/1500], Training Loss: 1.790830690356426e+17, Validation Loss: 1.7814780455472333e+17\n",
      "Epoch [868/1500], Training Loss: 1.790830616043834e+17, Validation Loss: 1.7814777019498496e+17\n",
      "Epoch [869/1500], Training Loss: 1.790830559710534e+17, Validation Loss: 1.7814777019498496e+17\n",
      "Epoch [870/1500], Training Loss: 1.7908304434805744e+17, Validation Loss: 1.7814777019498496e+17\n",
      "Epoch [871/1500], Training Loss: 1.7908303703085754e+17, Validation Loss: 1.7814775301511578e+17\n",
      "Epoch [872/1500], Training Loss: 1.790830337850474e+17, Validation Loss: 1.7814775301511578e+17\n",
      "Epoch [873/1500], Training Loss: 1.7908303248543594e+17, Validation Loss: 1.781477358352466e+17\n",
      "Epoch [874/1500], Training Loss: 1.7908302567118608e+17, Validation Loss: 1.781477358352466e+17\n",
      "Epoch [875/1500], Training Loss: 1.7908302218679507e+17, Validation Loss: 1.781477358352466e+17\n",
      "Epoch [876/1500], Training Loss: 1.790830073240912e+17, Validation Loss: 1.781477358352466e+17\n",
      "Epoch [877/1500], Training Loss: 1.7908300126137024e+17, Validation Loss: 1.781477358352466e+17\n",
      "Epoch [878/1500], Training Loss: 1.790829972799277e+17, Validation Loss: 1.781477358352466e+17\n",
      "Epoch [879/1500], Training Loss: 1.7908298657751536e+17, Validation Loss: 1.781477186553774e+17\n",
      "Epoch [880/1500], Training Loss: 1.7908298151475226e+17, Validation Loss: 1.781477186553774e+17\n",
      "Epoch [881/1500], Training Loss: 1.790829766904826e+17, Validation Loss: 1.781477186553774e+17\n",
      "Epoch [882/1500], Training Loss: 1.7908296159415942e+17, Validation Loss: 1.7814768429563904e+17\n",
      "Epoch [883/1500], Training Loss: 1.7908295723891683e+17, Validation Loss: 1.7814768429563904e+17\n",
      "Epoch [884/1500], Training Loss: 1.7908295200594976e+17, Validation Loss: 1.7814768429563904e+17\n",
      "Epoch [885/1500], Training Loss: 1.7908295011298326e+17, Validation Loss: 1.7814768429563904e+17\n",
      "Epoch [886/1500], Training Loss: 1.7908294547979648e+17, Validation Loss: 1.7814768429563904e+17\n",
      "Epoch [887/1500], Training Loss: 1.790829401123889e+17, Validation Loss: 1.7814768429563904e+17\n",
      "Epoch [888/1500], Training Loss: 1.7908292524495043e+17, Validation Loss: 1.7814764993590067e+17\n",
      "Epoch [889/1500], Training Loss: 1.7908292058282304e+17, Validation Loss: 1.7814764993590067e+17\n",
      "Epoch [890/1500], Training Loss: 1.7908291523305258e+17, Validation Loss: 1.7814764993590067e+17\n",
      "Epoch [891/1500], Training Loss: 1.790829043496387e+17, Validation Loss: 1.781476327560315e+17\n",
      "Epoch [892/1500], Training Loss: 1.7908290123177302e+17, Validation Loss: 1.781476155761623e+17\n",
      "Epoch [893/1500], Training Loss: 1.7908289473217613e+17, Validation Loss: 1.781476155761623e+17\n",
      "Epoch [894/1500], Training Loss: 1.7908287947216995e+17, Validation Loss: 1.781476155761623e+17\n",
      "Epoch [895/1500], Training Loss: 1.7908287733818454e+17, Validation Loss: 1.7814759839629312e+17\n",
      "Epoch [896/1500], Training Loss: 1.7908286959665098e+17, Validation Loss: 1.7814759839629312e+17\n",
      "Epoch [897/1500], Training Loss: 1.790828673361223e+17, Validation Loss: 1.7814759839629312e+17\n",
      "Epoch [898/1500], Training Loss: 1.7908286571403354e+17, Validation Loss: 1.7814759839629312e+17\n",
      "Epoch [899/1500], Training Loss: 1.7908285767421616e+17, Validation Loss: 1.7814758121642394e+17\n",
      "Epoch [900/1500], Training Loss: 1.7908284430499475e+17, Validation Loss: 1.7814758121642394e+17\n",
      "Epoch [901/1500], Training Loss: 1.7908284077945398e+17, Validation Loss: 1.7814758121642394e+17\n",
      "Epoch [902/1500], Training Loss: 1.790828325812962e+17, Validation Loss: 1.7814754685668557e+17\n",
      "Epoch [903/1500], Training Loss: 1.790828218432578e+17, Validation Loss: 1.7814754685668557e+17\n",
      "Epoch [904/1500], Training Loss: 1.7908282004551626e+17, Validation Loss: 1.7814754685668557e+17\n",
      "Epoch [905/1500], Training Loss: 1.790828121551427e+17, Validation Loss: 1.7814754685668557e+17\n",
      "Epoch [906/1500], Training Loss: 1.790828013771163e+17, Validation Loss: 1.7814754685668557e+17\n",
      "Epoch [907/1500], Training Loss: 1.7908279515209274e+17, Validation Loss: 1.7814754685668557e+17\n",
      "Epoch [908/1500], Training Loss: 1.7908278756421085e+17, Validation Loss: 1.781475296768164e+17\n",
      "Epoch [909/1500], Training Loss: 1.7908278444562483e+17, Validation Loss: 1.781475296768164e+17\n",
      "Epoch [910/1500], Training Loss: 1.7908278317498938e+17, Validation Loss: 1.781475296768164e+17\n",
      "Epoch [911/1500], Training Loss: 1.7908277579598986e+17, Validation Loss: 1.781475296768164e+17\n",
      "Epoch [912/1500], Training Loss: 1.79082770745513e+17, Validation Loss: 1.781475124969472e+17\n",
      "Epoch [913/1500], Training Loss: 1.790827584732192e+17, Validation Loss: 1.781475124969472e+17\n",
      "Epoch [914/1500], Training Loss: 1.7908275114488445e+17, Validation Loss: 1.78147495317078e+17\n",
      "Epoch [915/1500], Training Loss: 1.7908274790343622e+17, Validation Loss: 1.78147495317078e+17\n",
      "Epoch [916/1500], Training Loss: 1.7908273775447632e+17, Validation Loss: 1.78147495317078e+17\n",
      "Epoch [917/1500], Training Loss: 1.7908273100559472e+17, Validation Loss: 1.7814747813720883e+17\n",
      "Epoch [918/1500], Training Loss: 1.790827274539741e+17, Validation Loss: 1.7814746095733965e+17\n",
      "Epoch [919/1500], Training Loss: 1.790827127036722e+17, Validation Loss: 1.7814746095733965e+17\n",
      "Epoch [920/1500], Training Loss: 1.7908270669082486e+17, Validation Loss: 1.7814746095733965e+17\n",
      "Epoch [921/1500], Training Loss: 1.790827027215332e+17, Validation Loss: 1.7814746095733965e+17\n",
      "Epoch [922/1500], Training Loss: 1.7908270079451882e+17, Validation Loss: 1.7814746095733965e+17\n",
      "Epoch [923/1500], Training Loss: 1.7908269582274166e+17, Validation Loss: 1.7814744377747046e+17\n",
      "Epoch [924/1500], Training Loss: 1.790826909258686e+17, Validation Loss: 1.7814744377747046e+17\n",
      "Epoch [925/1500], Training Loss: 1.7908267569314573e+17, Validation Loss: 1.7814742659760128e+17\n",
      "Epoch [926/1500], Training Loss: 1.7908267146628486e+17, Validation Loss: 1.7814742659760128e+17\n",
      "Epoch [927/1500], Training Loss: 1.790826660580773e+17, Validation Loss: 1.7814742659760128e+17\n",
      "Epoch [928/1500], Training Loss: 1.7908265521826378e+17, Validation Loss: 1.781473922378629e+17\n",
      "Epoch [929/1500], Training Loss: 1.7908265063149642e+17, Validation Loss: 1.781473922378629e+17\n",
      "Epoch [930/1500], Training Loss: 1.7908264525299568e+17, Validation Loss: 1.781473922378629e+17\n",
      "Epoch [931/1500], Training Loss: 1.790826303996215e+17, Validation Loss: 1.7814737505799373e+17\n",
      "Epoch [932/1500], Training Loss: 1.7908262581119066e+17, Validation Loss: 1.7814737505799373e+17\n",
      "Epoch [933/1500], Training Loss: 1.7908262051180387e+17, Validation Loss: 1.7814737505799373e+17\n",
      "Epoch [934/1500], Training Loss: 1.7908261845273712e+17, Validation Loss: 1.7814737505799373e+17\n",
      "Epoch [935/1500], Training Loss: 1.7908261548608925e+17, Validation Loss: 1.7814737505799373e+17\n",
      "Epoch [936/1500], Training Loss: 1.7908260880394797e+17, Validation Loss: 1.7814735787812454e+17\n",
      "Epoch [937/1500], Training Loss: 1.7908259358650534e+17, Validation Loss: 1.78147306338517e+17\n",
      "Epoch [938/1500], Training Loss: 1.790825914134168e+17, Validation Loss: 1.78147306338517e+17\n",
      "Epoch [939/1500], Training Loss: 1.790825836325428e+17, Validation Loss: 1.781472891586478e+17\n",
      "Epoch [940/1500], Training Loss: 1.7908257249895312e+17, Validation Loss: 1.781472891586478e+17\n",
      "Epoch [941/1500], Training Loss: 1.7908257091854086e+17, Validation Loss: 1.781472891586478e+17\n",
      "Epoch [942/1500], Training Loss: 1.7908256285930627e+17, Validation Loss: 1.781472891586478e+17\n",
      "Epoch [943/1500], Training Loss: 1.7908254966193782e+17, Validation Loss: 1.7814723761904026e+17\n",
      "Epoch [944/1500], Training Loss: 1.7908254615254762e+17, Validation Loss: 1.7814723761904026e+17\n",
      "Epoch [945/1500], Training Loss: 1.7908253794043587e+17, Validation Loss: 1.7814723761904026e+17\n",
      "Epoch [946/1500], Training Loss: 1.7908253576791786e+17, Validation Loss: 1.7814723761904026e+17\n",
      "Epoch [947/1500], Training Loss: 1.7908253440150778e+17, Validation Loss: 1.7814723761904026e+17\n",
      "Epoch [948/1500], Training Loss: 1.790825265930632e+17, Validation Loss: 1.7814723761904026e+17\n",
      "Epoch [949/1500], Training Loss: 1.7908251608863782e+17, Validation Loss: 1.7814722043917107e+17\n",
      "Epoch [950/1500], Training Loss: 1.7908250937412685e+17, Validation Loss: 1.7814722043917107e+17\n",
      "Epoch [951/1500], Training Loss: 1.7908250185262925e+17, Validation Loss: 1.7814722043917107e+17\n",
      "Epoch [952/1500], Training Loss: 1.7908249669664627e+17, Validation Loss: 1.781472032593019e+17\n",
      "Epoch [953/1500], Training Loss: 1.790824883921412e+17, Validation Loss: 1.781472032593019e+17\n",
      "Epoch [954/1500], Training Loss: 1.7908248101058906e+17, Validation Loss: 1.781472032593019e+17\n",
      "Epoch [955/1500], Training Loss: 1.7908247627954976e+17, Validation Loss: 1.781472032593019e+17\n",
      "Epoch [956/1500], Training Loss: 1.7908246365198624e+17, Validation Loss: 1.781472032593019e+17\n",
      "Epoch [957/1500], Training Loss: 1.7908245639763328e+17, Validation Loss: 1.781471860794327e+17\n",
      "Epoch [958/1500], Training Loss: 1.7908245310501642e+17, Validation Loss: 1.781471860794327e+17\n",
      "Epoch [959/1500], Training Loss: 1.7908245174424458e+17, Validation Loss: 1.7814716889956352e+17\n",
      "Epoch [960/1500], Training Loss: 1.7908244496713238e+17, Validation Loss: 1.7814716889956352e+17\n",
      "Epoch [961/1500], Training Loss: 1.7908244143525386e+17, Validation Loss: 1.7814716889956352e+17\n",
      "Epoch [962/1500], Training Loss: 1.790824268548603e+17, Validation Loss: 1.7814715171969434e+17\n",
      "Epoch [963/1500], Training Loss: 1.7908242090405203e+17, Validation Loss: 1.7814715171969434e+17\n",
      "Epoch [964/1500], Training Loss: 1.7908241687992106e+17, Validation Loss: 1.7814715171969434e+17\n",
      "Epoch [965/1500], Training Loss: 1.79082406102563e+17, Validation Loss: 1.7814715171969434e+17\n",
      "Epoch [966/1500], Training Loss: 1.7908240129323005e+17, Validation Loss: 1.7814715171969434e+17\n",
      "Epoch [967/1500], Training Loss: 1.790823963732753e+17, Validation Loss: 1.7814713453982515e+17\n",
      "Epoch [968/1500], Training Loss: 1.7908238088667056e+17, Validation Loss: 1.781471001800868e+17\n",
      "Epoch [969/1500], Training Loss: 1.790823766258472e+17, Validation Loss: 1.781471001800868e+17\n",
      "Epoch [970/1500], Training Loss: 1.7908237122325078e+17, Validation Loss: 1.781471001800868e+17\n",
      "Epoch [971/1500], Training Loss: 1.7908236935832547e+17, Validation Loss: 1.781471001800868e+17\n",
      "Epoch [972/1500], Training Loss: 1.790823646763124e+17, Validation Loss: 1.781471001800868e+17\n",
      "Epoch [973/1500], Training Loss: 1.7908235926216054e+17, Validation Loss: 1.781471001800868e+17\n",
      "Epoch [974/1500], Training Loss: 1.7908234442634646e+17, Validation Loss: 1.781471001800868e+17\n",
      "Epoch [975/1500], Training Loss: 1.7908233994340096e+17, Validation Loss: 1.781470658203484e+17\n",
      "Epoch [976/1500], Training Loss: 1.7908233465123062e+17, Validation Loss: 1.781470658203484e+17\n",
      "Epoch [977/1500], Training Loss: 1.7908232360397344e+17, Validation Loss: 1.781470658203484e+17\n",
      "Epoch [978/1500], Training Loss: 1.790823207649431e+17, Validation Loss: 1.7814704864047923e+17\n",
      "Epoch [979/1500], Training Loss: 1.790823139793091e+17, Validation Loss: 1.7814704864047923e+17\n",
      "Epoch [980/1500], Training Loss: 1.7908229873839955e+17, Validation Loss: 1.7814703146061005e+17\n",
      "Epoch [981/1500], Training Loss: 1.7908229661353568e+17, Validation Loss: 1.7814703146061005e+17\n",
      "Epoch [982/1500], Training Loss: 1.7908228884906205e+17, Validation Loss: 1.7814703146061005e+17\n",
      "Epoch [983/1500], Training Loss: 1.7908228657966173e+17, Validation Loss: 1.7814703146061005e+17\n",
      "Epoch [984/1500], Training Loss: 1.7908228502551885e+17, Validation Loss: 1.7814703146061005e+17\n",
      "Epoch [985/1500], Training Loss: 1.790822769058088e+17, Validation Loss: 1.7814699710087168e+17\n",
      "Epoch [986/1500], Training Loss: 1.7908226365943504e+17, Validation Loss: 1.7814699710087168e+17\n",
      "Epoch [987/1500], Training Loss: 1.790822601232654e+17, Validation Loss: 1.7814699710087168e+17\n",
      "Epoch [988/1500], Training Loss: 1.7908225199657837e+17, Validation Loss: 1.7814699710087168e+17\n",
      "Epoch [989/1500], Training Loss: 1.7908224201114534e+17, Validation Loss: 1.7814699710087168e+17\n",
      "Epoch [990/1500], Training Loss: 1.790822395925011e+17, Validation Loss: 1.781469799210025e+17\n",
      "Epoch [991/1500], Training Loss: 1.7908223171203818e+17, Validation Loss: 1.781469799210025e+17\n",
      "Epoch [992/1500], Training Loss: 1.790822216814432e+17, Validation Loss: 1.781469627411333e+17\n",
      "Epoch [993/1500], Training Loss: 1.790822144863518e+17, Validation Loss: 1.781469627411333e+17\n",
      "Epoch [994/1500], Training Loss: 1.7908220695744003e+17, Validation Loss: 1.7814694556126413e+17\n",
      "Epoch [995/1500], Training Loss: 1.7908220387592714e+17, Validation Loss: 1.7814694556126413e+17\n",
      "Epoch [996/1500], Training Loss: 1.7908220300787203e+17, Validation Loss: 1.7814694556126413e+17\n",
      "Epoch [997/1500], Training Loss: 1.7908219623350813e+17, Validation Loss: 1.7814694556126413e+17\n",
      "Epoch [998/1500], Training Loss: 1.7908219296270288e+17, Validation Loss: 1.7814694556126413e+17\n",
      "Epoch [999/1500], Training Loss: 1.7908217802846998e+17, Validation Loss: 1.7814692838139494e+17\n",
      "Epoch [1000/1500], Training Loss: 1.7908217383780362e+17, Validation Loss: 1.7814692838139494e+17\n",
      "Epoch [1001/1500], Training Loss: 1.7908216846561773e+17, Validation Loss: 1.7814692838139494e+17\n",
      "Epoch [1002/1500], Training Loss: 1.7908215758100042e+17, Validation Loss: 1.7814691120152576e+17\n",
      "Epoch [1003/1500], Training Loss: 1.7908215574289613e+17, Validation Loss: 1.7814691120152576e+17\n",
      "Epoch [1004/1500], Training Loss: 1.7908214796447898e+17, Validation Loss: 1.7814689402165658e+17\n",
      "Epoch [1005/1500], Training Loss: 1.790821350501933e+17, Validation Loss: 1.7814689402165658e+17\n",
      "Epoch [1006/1500], Training Loss: 1.7908213151836266e+17, Validation Loss: 1.7814689402165658e+17\n",
      "Epoch [1007/1500], Training Loss: 1.790821240009084e+17, Validation Loss: 1.781468768417874e+17\n",
      "Epoch [1008/1500], Training Loss: 1.790821212156452e+17, Validation Loss: 1.781468596619182e+17\n",
      "Epoch [1009/1500], Training Loss: 1.7908212004222307e+17, Validation Loss: 1.781468596619182e+17\n",
      "Epoch [1010/1500], Training Loss: 1.7908211399223184e+17, Validation Loss: 1.781468596619182e+17\n",
      "Epoch [1011/1500], Training Loss: 1.790821102221972e+17, Validation Loss: 1.781468596619182e+17\n",
      "Epoch [1012/1500], Training Loss: 1.7908209544736032e+17, Validation Loss: 1.7814684248204902e+17\n",
      "Epoch [1013/1500], Training Loss: 1.790820907448952e+17, Validation Loss: 1.7814684248204902e+17\n",
      "Epoch [1014/1500], Training Loss: 1.79082085762283e+17, Validation Loss: 1.7814684248204902e+17\n",
      "Epoch [1015/1500], Training Loss: 1.7908207461266147e+17, Validation Loss: 1.7814684248204902e+17\n",
      "Epoch [1016/1500], Training Loss: 1.7908207306502912e+17, Validation Loss: 1.7814682530217984e+17\n",
      "Epoch [1017/1500], Training Loss: 1.7908206517445155e+17, Validation Loss: 1.7814682530217984e+17\n",
      "Epoch [1018/1500], Training Loss: 1.7908205464078995e+17, Validation Loss: 1.7814679094244147e+17\n",
      "Epoch [1019/1500], Training Loss: 1.790820486587779e+17, Validation Loss: 1.781467737625723e+17\n",
      "Epoch [1020/1500], Training Loss: 1.7908204157966125e+17, Validation Loss: 1.781467737625723e+17\n",
      "Epoch [1021/1500], Training Loss: 1.79082038595041e+17, Validation Loss: 1.781467737625723e+17\n",
      "Epoch [1022/1500], Training Loss: 1.790820371233559e+17, Validation Loss: 1.781467737625723e+17\n",
      "Epoch [1023/1500], Training Loss: 1.79082032343001e+17, Validation Loss: 1.781467737625723e+17\n",
      "Epoch [1024/1500], Training Loss: 1.790820275456669e+17, Validation Loss: 1.781467565827031e+17\n",
      "Epoch [1025/1500], Training Loss: 1.790820124556045e+17, Validation Loss: 1.7814673940283392e+17\n",
      "Epoch [1026/1500], Training Loss: 1.7908200891552685e+17, Validation Loss: 1.7814672222296474e+17\n",
      "Epoch [1027/1500], Training Loss: 1.79082002792876e+17, Validation Loss: 1.7814672222296474e+17\n",
      "Epoch [1028/1500], Training Loss: 1.790819916074493e+17, Validation Loss: 1.7814672222296474e+17\n",
      "Epoch [1029/1500], Training Loss: 1.7908199029256806e+17, Validation Loss: 1.7814672222296474e+17\n",
      "Epoch [1030/1500], Training Loss: 1.7908198279111034e+17, Validation Loss: 1.7814672222296474e+17\n",
      "Epoch [1031/1500], Training Loss: 1.7908197460164928e+17, Validation Loss: 1.7814670504309555e+17\n",
      "Epoch [1032/1500], Training Loss: 1.7908196593802266e+17, Validation Loss: 1.7814670504309555e+17\n",
      "Epoch [1033/1500], Training Loss: 1.790819590513651e+17, Validation Loss: 1.7814670504309555e+17\n",
      "Epoch [1034/1500], Training Loss: 1.7908195584872435e+17, Validation Loss: 1.781466706833572e+17\n",
      "Epoch [1035/1500], Training Loss: 1.7908195405822845e+17, Validation Loss: 1.781466706833572e+17\n",
      "Epoch [1036/1500], Training Loss: 1.7908194987027082e+17, Validation Loss: 1.78146653503488e+17\n",
      "Epoch [1037/1500], Training Loss: 1.7908194445893395e+17, Validation Loss: 1.781466363236188e+17\n",
      "Epoch [1038/1500], Training Loss: 1.7908192955143258e+17, Validation Loss: 1.781466363236188e+17\n",
      "Epoch [1039/1500], Training Loss: 1.7908192761435562e+17, Validation Loss: 1.7814660196388045e+17\n",
      "Epoch [1040/1500], Training Loss: 1.7908191991167744e+17, Validation Loss: 1.7814660196388045e+17\n",
      "Epoch [1041/1500], Training Loss: 1.7908190948050512e+17, Validation Loss: 1.7814660196388045e+17\n",
      "Epoch [1042/1500], Training Loss: 1.7908190777862154e+17, Validation Loss: 1.7814660196388045e+17\n",
      "Epoch [1043/1500], Training Loss: 1.790819003370125e+17, Validation Loss: 1.7814656760414208e+17\n",
      "Epoch [1044/1500], Training Loss: 1.7908189576350995e+17, Validation Loss: 1.7814656760414208e+17\n",
      "Epoch [1045/1500], Training Loss: 1.790818832698477e+17, Validation Loss: 1.7814656760414208e+17\n",
      "Epoch [1046/1500], Training Loss: 1.7908187696830237e+17, Validation Loss: 1.7814656760414208e+17\n",
      "Epoch [1047/1500], Training Loss: 1.7908187331231034e+17, Validation Loss: 1.7814656760414208e+17\n",
      "Epoch [1048/1500], Training Loss: 1.790818714646765e+17, Validation Loss: 1.7814656760414208e+17\n",
      "Epoch [1049/1500], Training Loss: 1.7908186703254774e+17, Validation Loss: 1.7814656760414208e+17\n",
      "Epoch [1050/1500], Training Loss: 1.7908186206141187e+17, Validation Loss: 1.7814656760414208e+17\n",
      "Epoch [1051/1500], Training Loss: 1.7908184700543277e+17, Validation Loss: 1.781465504242729e+17\n",
      "Epoch [1052/1500], Training Loss: 1.7908184513212093e+17, Validation Loss: 1.781465504242729e+17\n",
      "Epoch [1053/1500], Training Loss: 1.790818371992776e+17, Validation Loss: 1.781465332444037e+17\n",
      "Epoch [1054/1500], Training Loss: 1.7908182742393862e+17, Validation Loss: 1.7814651606453453e+17\n",
      "Epoch [1055/1500], Training Loss: 1.7908182484768272e+17, Validation Loss: 1.7814649888466534e+17\n",
      "Epoch [1056/1500], Training Loss: 1.7908181773183856e+17, Validation Loss: 1.7814649888466534e+17\n",
      "Epoch [1057/1500], Training Loss: 1.790818148059865e+17, Validation Loss: 1.7814648170479616e+17\n",
      "Epoch [1058/1500], Training Loss: 1.790818002442794e+17, Validation Loss: 1.7814648170479616e+17\n",
      "Epoch [1059/1500], Training Loss: 1.790817952859378e+17, Validation Loss: 1.7814646452492698e+17\n",
      "Epoch [1060/1500], Training Loss: 1.7908179056997258e+17, Validation Loss: 1.781464473450578e+17\n",
      "Epoch [1061/1500], Training Loss: 1.790817887666866e+17, Validation Loss: 1.781464473450578e+17\n",
      "Epoch [1062/1500], Training Loss: 1.7908178486418525e+17, Validation Loss: 1.781464301651886e+17\n",
      "Epoch [1063/1500], Training Loss: 1.7908177914425632e+17, Validation Loss: 1.781464301651886e+17\n",
      "Epoch [1064/1500], Training Loss: 1.7908176538762656e+17, Validation Loss: 1.781464301651886e+17\n",
      "Epoch [1065/1500], Training Loss: 1.7908176236451744e+17, Validation Loss: 1.781464301651886e+17\n",
      "Epoch [1066/1500], Training Loss: 1.790817544657886e+17, Validation Loss: 1.7814641298531942e+17\n",
      "Epoch [1067/1500], Training Loss: 1.7908174766546982e+17, Validation Loss: 1.7814641298531942e+17\n",
      "Epoch [1068/1500], Training Loss: 1.790817421160905e+17, Validation Loss: 1.7814641298531942e+17\n",
      "Epoch [1069/1500], Training Loss: 1.7908173507046787e+17, Validation Loss: 1.7814641298531942e+17\n",
      "Epoch [1070/1500], Training Loss: 1.790817319681948e+17, Validation Loss: 1.7814641298531942e+17\n",
      "Epoch [1071/1500], Training Loss: 1.7908171736979978e+17, Validation Loss: 1.7814639580545024e+17\n",
      "Epoch [1072/1500], Training Loss: 1.7908171314660963e+17, Validation Loss: 1.7814639580545024e+17\n",
      "Epoch [1073/1500], Training Loss: 1.790817077963291e+17, Validation Loss: 1.7814639580545024e+17\n",
      "Epoch [1074/1500], Training Loss: 1.7908170594335686e+17, Validation Loss: 1.7814639580545024e+17\n",
      "Epoch [1075/1500], Training Loss: 1.790817038456784e+17, Validation Loss: 1.7814639580545024e+17\n",
      "Epoch [1076/1500], Training Loss: 1.7908169635059382e+17, Validation Loss: 1.7814639580545024e+17\n",
      "Epoch [1077/1500], Training Loss: 1.7908168316808925e+17, Validation Loss: 1.7814639580545024e+17\n",
      "Epoch [1078/1500], Training Loss: 1.790816796713163e+17, Validation Loss: 1.7814639580545024e+17\n",
      "Epoch [1079/1500], Training Loss: 1.7908167212382218e+17, Validation Loss: 1.7814639580545024e+17\n",
      "Epoch [1080/1500], Training Loss: 1.790816692368421e+17, Validation Loss: 1.7814639580545024e+17\n",
      "Epoch [1081/1500], Training Loss: 1.7908165950534922e+17, Validation Loss: 1.7814637862558106e+17\n",
      "Epoch [1082/1500], Training Loss: 1.7908165310763155e+17, Validation Loss: 1.7814637862558106e+17\n",
      "Epoch [1083/1500], Training Loss: 1.7908164970335405e+17, Validation Loss: 1.7814637862558106e+17\n",
      "Epoch [1084/1500], Training Loss: 1.7908163453876646e+17, Validation Loss: 1.781463442658427e+17\n",
      "Epoch [1085/1500], Training Loss: 1.7908163028940486e+17, Validation Loss: 1.781463442658427e+17\n",
      "Epoch [1086/1500], Training Loss: 1.7908162518311853e+17, Validation Loss: 1.781463442658427e+17\n",
      "Epoch [1087/1500], Training Loss: 1.7908162304286406e+17, Validation Loss: 1.781463442658427e+17\n",
      "Epoch [1088/1500], Training Loss: 1.7908162137207184e+17, Validation Loss: 1.781463442658427e+17\n",
      "Epoch [1089/1500], Training Loss: 1.7908161347065302e+17, Validation Loss: 1.781463442658427e+17\n",
      "Epoch [1090/1500], Training Loss: 1.7908160179949283e+17, Validation Loss: 1.781463270859735e+17\n",
      "Epoch [1091/1500], Training Loss: 1.790815969503364e+17, Validation Loss: 1.781463270859735e+17\n",
      "Epoch [1092/1500], Training Loss: 1.7908158966252685e+17, Validation Loss: 1.7814630990610432e+17\n",
      "Epoch [1093/1500], Training Loss: 1.7908158683496662e+17, Validation Loss: 1.7814629272623514e+17\n",
      "Epoch [1094/1500], Training Loss: 1.7908157623052758e+17, Validation Loss: 1.7814629272623514e+17\n",
      "Epoch [1095/1500], Training Loss: 1.790815711277495e+17, Validation Loss: 1.7814627554636595e+17\n",
      "Epoch [1096/1500], Training Loss: 1.7908156654681194e+17, Validation Loss: 1.7814627554636595e+17\n",
      "Epoch [1097/1500], Training Loss: 1.790815516922011e+17, Validation Loss: 1.7814627554636595e+17\n",
      "Epoch [1098/1500], Training Loss: 1.790815476124626e+17, Validation Loss: 1.7814627554636595e+17\n",
      "Epoch [1099/1500], Training Loss: 1.7908154211691485e+17, Validation Loss: 1.7814627554636595e+17\n",
      "Epoch [1100/1500], Training Loss: 1.7908153985843702e+17, Validation Loss: 1.7814627554636595e+17\n",
      "Epoch [1101/1500], Training Loss: 1.7908153851039488e+17, Validation Loss: 1.7814627554636595e+17\n",
      "Epoch [1102/1500], Training Loss: 1.7908153054778858e+17, Validation Loss: 1.7814625836649677e+17\n",
      "Epoch [1103/1500], Training Loss: 1.7908152214553715e+17, Validation Loss: 1.781462240067584e+17\n",
      "Epoch [1104/1500], Training Loss: 1.7908151398421302e+17, Validation Loss: 1.781462240067584e+17\n",
      "Epoch [1105/1500], Training Loss: 1.7908150690535875e+17, Validation Loss: 1.781462240067584e+17\n",
      "Epoch [1106/1500], Training Loss: 1.7908150389242883e+17, Validation Loss: 1.781462240067584e+17\n",
      "Epoch [1107/1500], Training Loss: 1.7908149353515494e+17, Validation Loss: 1.7814618964702003e+17\n",
      "Epoch [1108/1500], Training Loss: 1.790814891262349e+17, Validation Loss: 1.7814618964702003e+17\n",
      "Epoch [1109/1500], Training Loss: 1.790814839298038e+17, Validation Loss: 1.7814618964702003e+17\n",
      "Epoch [1110/1500], Training Loss: 1.7908146901638938e+17, Validation Loss: 1.7814617246715085e+17\n",
      "Epoch [1111/1500], Training Loss: 1.7908146656114678e+17, Validation Loss: 1.7814617246715085e+17\n",
      "Epoch [1112/1500], Training Loss: 1.7908145936156026e+17, Validation Loss: 1.7814617246715085e+17\n",
      "Epoch [1113/1500], Training Loss: 1.790814572391511e+17, Validation Loss: 1.7814617246715085e+17\n",
      "Epoch [1114/1500], Training Loss: 1.790814559588425e+17, Validation Loss: 1.7814617246715085e+17\n",
      "Epoch [1115/1500], Training Loss: 1.7908144854864963e+17, Validation Loss: 1.7814617246715085e+17\n",
      "Epoch [1116/1500], Training Loss: 1.7908144258677104e+17, Validation Loss: 1.7814615528728166e+17\n",
      "Epoch [1117/1500], Training Loss: 1.790814314743809e+17, Validation Loss: 1.7814613810741248e+17\n",
      "Epoch [1118/1500], Training Loss: 1.790814249856128e+17, Validation Loss: 1.7814613810741248e+17\n",
      "Epoch [1119/1500], Training Loss: 1.7908142157403763e+17, Validation Loss: 1.7814613810741248e+17\n",
      "Epoch [1120/1500], Training Loss: 1.7908141053038662e+17, Validation Loss: 1.781461209275433e+17\n",
      "Epoch [1121/1500], Training Loss: 1.7908140630660096e+17, Validation Loss: 1.781461209275433e+17\n",
      "Epoch [1122/1500], Training Loss: 1.7908140108158115e+17, Validation Loss: 1.781461209275433e+17\n",
      "Epoch [1123/1500], Training Loss: 1.7908138586477357e+17, Validation Loss: 1.781461037476741e+17\n",
      "Epoch [1124/1500], Training Loss: 1.7908138417142022e+17, Validation Loss: 1.781461037476741e+17\n",
      "Epoch [1125/1500], Training Loss: 1.7908137627847117e+17, Validation Loss: 1.781461037476741e+17\n",
      "Epoch [1126/1500], Training Loss: 1.7908137419240227e+17, Validation Loss: 1.781461037476741e+17\n",
      "Epoch [1127/1500], Training Loss: 1.7908137290818976e+17, Validation Loss: 1.781461037476741e+17\n",
      "Epoch [1128/1500], Training Loss: 1.7908136559258685e+17, Validation Loss: 1.781461037476741e+17\n",
      "Epoch [1129/1500], Training Loss: 1.7908136271491984e+17, Validation Loss: 1.7814606938793574e+17\n",
      "Epoch [1130/1500], Training Loss: 1.790813484644928e+17, Validation Loss: 1.7814606938793574e+17\n",
      "Epoch [1131/1500], Training Loss: 1.7908134303285382e+17, Validation Loss: 1.7814606938793574e+17\n",
      "Epoch [1132/1500], Training Loss: 1.7908133874864896e+17, Validation Loss: 1.7814606938793574e+17\n",
      "Epoch [1133/1500], Training Loss: 1.790813281937775e+17, Validation Loss: 1.7814603502819738e+17\n",
      "Epoch [1134/1500], Training Loss: 1.7908132364550534e+17, Validation Loss: 1.7814603502819738e+17\n",
      "Epoch [1135/1500], Training Loss: 1.790813186087389e+17, Validation Loss: 1.7814603502819738e+17\n",
      "Epoch [1136/1500], Training Loss: 1.7908130419312768e+17, Validation Loss: 1.7814603502819738e+17\n",
      "Epoch [1137/1500], Training Loss: 1.7908130168720365e+17, Validation Loss: 1.7814603502819738e+17\n",
      "Epoch [1138/1500], Training Loss: 1.790812937176162e+17, Validation Loss: 1.78146000668459e+17\n",
      "Epoch [1139/1500], Training Loss: 1.7908129134525405e+17, Validation Loss: 1.78146000668459e+17\n",
      "Epoch [1140/1500], Training Loss: 1.790812901781552e+17, Validation Loss: 1.78146000668459e+17\n",
      "Epoch [1141/1500], Training Loss: 1.7908128319368262e+17, Validation Loss: 1.78146000668459e+17\n",
      "Epoch [1142/1500], Training Loss: 1.7908128020982237e+17, Validation Loss: 1.78146000668459e+17\n",
      "Epoch [1143/1500], Training Loss: 1.7908126541199546e+17, Validation Loss: 1.7814598348858982e+17\n",
      "Epoch [1144/1500], Training Loss: 1.7908126104704637e+17, Validation Loss: 1.7814594912885146e+17\n",
      "Epoch [1145/1500], Training Loss: 1.79081255839164e+17, Validation Loss: 1.7814594912885146e+17\n",
      "Epoch [1146/1500], Training Loss: 1.790812450143038e+17, Validation Loss: 1.7814594912885146e+17\n",
      "Epoch [1147/1500], Training Loss: 1.7908124217339552e+17, Validation Loss: 1.7814593194898227e+17\n",
      "Epoch [1148/1500], Training Loss: 1.790812354268896e+17, Validation Loss: 1.7814593194898227e+17\n",
      "Epoch [1149/1500], Training Loss: 1.7908122227862448e+17, Validation Loss: 1.7814593194898227e+17\n",
      "Epoch [1150/1500], Training Loss: 1.7908121889309795e+17, Validation Loss: 1.781459147691131e+17\n",
      "Epoch [1151/1500], Training Loss: 1.790812113153557e+17, Validation Loss: 1.781459147691131e+17\n",
      "Epoch [1152/1500], Training Loss: 1.7908120844356845e+17, Validation Loss: 1.781458975892439e+17\n",
      "Epoch [1153/1500], Training Loss: 1.7908120752709094e+17, Validation Loss: 1.781458975892439e+17\n",
      "Epoch [1154/1500], Training Loss: 1.790812009188482e+17, Validation Loss: 1.781458975892439e+17\n",
      "Epoch [1155/1500], Training Loss: 1.7908119753250966e+17, Validation Loss: 1.781458975892439e+17\n",
      "Epoch [1156/1500], Training Loss: 1.7908118259822058e+17, Validation Loss: 1.781458975892439e+17\n",
      "Epoch [1157/1500], Training Loss: 1.7908117838144525e+17, Validation Loss: 1.781458975892439e+17\n",
      "Epoch [1158/1500], Training Loss: 1.790811730046331e+17, Validation Loss: 1.781458975892439e+17\n",
      "Epoch [1159/1500], Training Loss: 1.790811623376907e+17, Validation Loss: 1.7814588040937472e+17\n",
      "Epoch [1160/1500], Training Loss: 1.7908116061020826e+17, Validation Loss: 1.7814584604963635e+17\n",
      "Epoch [1161/1500], Training Loss: 1.7908115279871347e+17, Validation Loss: 1.7814584604963635e+17\n",
      "Epoch [1162/1500], Training Loss: 1.7908114027005187e+17, Validation Loss: 1.7814584604963635e+17\n",
      "Epoch [1163/1500], Training Loss: 1.790811361924246e+17, Validation Loss: 1.7814584604963635e+17\n",
      "Epoch [1164/1500], Training Loss: 1.7908112881570704e+17, Validation Loss: 1.7814584604963635e+17\n",
      "Epoch [1165/1500], Training Loss: 1.790811259851008e+17, Validation Loss: 1.7814584604963635e+17\n",
      "Epoch [1166/1500], Training Loss: 1.7908112474894845e+17, Validation Loss: 1.7814584604963635e+17\n",
      "Epoch [1167/1500], Training Loss: 1.7908111893797738e+17, Validation Loss: 1.7814584604963635e+17\n",
      "Epoch [1168/1500], Training Loss: 1.7908111494784438e+17, Validation Loss: 1.7814584604963635e+17\n",
      "Epoch [1169/1500], Training Loss: 1.7908110003953312e+17, Validation Loss: 1.7814582886976717e+17\n",
      "Epoch [1170/1500], Training Loss: 1.7908109539582157e+17, Validation Loss: 1.78145811689898e+17\n",
      "Epoch [1171/1500], Training Loss: 1.790810904472865e+17, Validation Loss: 1.78145811689898e+17\n",
      "Epoch [1172/1500], Training Loss: 1.7908107909157674e+17, Validation Loss: 1.78145811689898e+17\n",
      "Epoch [1173/1500], Training Loss: 1.79081077619621e+17, Validation Loss: 1.78145811689898e+17\n",
      "Epoch [1174/1500], Training Loss: 1.7908106963602752e+17, Validation Loss: 1.781457945100288e+17\n",
      "Epoch [1175/1500], Training Loss: 1.7908106015551206e+17, Validation Loss: 1.781457773301596e+17\n",
      "Epoch [1176/1500], Training Loss: 1.790810530518022e+17, Validation Loss: 1.781457773301596e+17\n",
      "Epoch [1177/1500], Training Loss: 1.7908104612109462e+17, Validation Loss: 1.781457773301596e+17\n",
      "Epoch [1178/1500], Training Loss: 1.790810431299951e+17, Validation Loss: 1.7814576015029043e+17\n",
      "Epoch [1179/1500], Training Loss: 1.7908104156346813e+17, Validation Loss: 1.7814576015029043e+17\n",
      "Epoch [1180/1500], Training Loss: 1.7908103716256816e+17, Validation Loss: 1.7814576015029043e+17\n",
      "Epoch [1181/1500], Training Loss: 1.7908103193988237e+17, Validation Loss: 1.7814576015029043e+17\n",
      "Epoch [1182/1500], Training Loss: 1.790810170074921e+17, Validation Loss: 1.7814574297042125e+17\n",
      "Epoch [1183/1500], Training Loss: 1.790810139394543e+17, Validation Loss: 1.7814572579055206e+17\n",
      "Epoch [1184/1500], Training Loss: 1.7908100747427597e+17, Validation Loss: 1.7814572579055206e+17\n",
      "Epoch [1185/1500], Training Loss: 1.790809967509598e+17, Validation Loss: 1.7814572579055206e+17\n",
      "Epoch [1186/1500], Training Loss: 1.790809951293729e+17, Validation Loss: 1.7814572579055206e+17\n",
      "Epoch [1187/1500], Training Loss: 1.7908098755858266e+17, Validation Loss: 1.7814572579055206e+17\n",
      "Epoch [1188/1500], Training Loss: 1.79080980649897e+17, Validation Loss: 1.7814572579055206e+17\n",
      "Epoch [1189/1500], Training Loss: 1.7908097053443533e+17, Validation Loss: 1.7814572579055206e+17\n",
      "Epoch [1190/1500], Training Loss: 1.790809638267806e+17, Validation Loss: 1.7814572579055206e+17\n",
      "Epoch [1191/1500], Training Loss: 1.7908096051733248e+17, Validation Loss: 1.7814570861068288e+17\n",
      "Epoch [1192/1500], Training Loss: 1.7908095863509488e+17, Validation Loss: 1.781456914308137e+17\n",
      "Epoch [1193/1500], Training Loss: 1.790809544480222e+17, Validation Loss: 1.781456914308137e+17\n",
      "Epoch [1194/1500], Training Loss: 1.7908094903743075e+17, Validation Loss: 1.781456914308137e+17\n",
      "Epoch [1195/1500], Training Loss: 1.7908093408695574e+17, Validation Loss: 1.781456742509445e+17\n",
      "Epoch [1196/1500], Training Loss: 1.7908093228360314e+17, Validation Loss: 1.781456742509445e+17\n",
      "Epoch [1197/1500], Training Loss: 1.7908092449903146e+17, Validation Loss: 1.781456742509445e+17\n",
      "Epoch [1198/1500], Training Loss: 1.7908091394744547e+17, Validation Loss: 1.7814565707107533e+17\n",
      "Epoch [1199/1500], Training Loss: 1.7908091232211917e+17, Validation Loss: 1.7814565707107533e+17\n",
      "Epoch [1200/1500], Training Loss: 1.7908090486568384e+17, Validation Loss: 1.7814565707107533e+17\n",
      "Epoch [1201/1500], Training Loss: 1.7908090103673328e+17, Validation Loss: 1.7814565707107533e+17\n",
      "Epoch [1202/1500], Training Loss: 1.7908088792633245e+17, Validation Loss: 1.7814563989120614e+17\n",
      "Epoch [1203/1500], Training Loss: 1.790808819064624e+17, Validation Loss: 1.7814563989120614e+17\n",
      "Epoch [1204/1500], Training Loss: 1.790808780802414e+17, Validation Loss: 1.7814563989120614e+17\n",
      "Epoch [1205/1500], Training Loss: 1.7908087645833178e+17, Validation Loss: 1.7814563989120614e+17\n",
      "Epoch [1206/1500], Training Loss: 1.790808717860586e+17, Validation Loss: 1.7814563989120614e+17\n",
      "Epoch [1207/1500], Training Loss: 1.7908086683912666e+17, Validation Loss: 1.7814563989120614e+17\n",
      "Epoch [1208/1500], Training Loss: 1.7908085177020134e+17, Validation Loss: 1.7814563989120614e+17\n",
      "Epoch [1209/1500], Training Loss: 1.7908084976230538e+17, Validation Loss: 1.7814562271133696e+17\n",
      "Epoch [1210/1500], Training Loss: 1.79080841748291e+17, Validation Loss: 1.7814562271133696e+17\n",
      "Epoch [1211/1500], Training Loss: 1.790808328792924e+17, Validation Loss: 1.7814562271133696e+17\n",
      "Epoch [1212/1500], Training Loss: 1.790808294623664e+17, Validation Loss: 1.7814562271133696e+17\n",
      "Epoch [1213/1500], Training Loss: 1.790808225009339e+17, Validation Loss: 1.7814560553146778e+17\n",
      "Epoch [1214/1500], Training Loss: 1.7908081947592182e+17, Validation Loss: 1.7814560553146778e+17\n",
      "Epoch [1215/1500], Training Loss: 1.7908080483781578e+17, Validation Loss: 1.781455883515986e+17\n",
      "Epoch [1216/1500], Training Loss: 1.7908080014524877e+17, Validation Loss: 1.781455883515986e+17\n",
      "Epoch [1217/1500], Training Loss: 1.790807951976839e+17, Validation Loss: 1.781455883515986e+17\n",
      "Epoch [1218/1500], Training Loss: 1.7908079336542403e+17, Validation Loss: 1.781455883515986e+17\n",
      "Epoch [1219/1500], Training Loss: 1.790807900221383e+17, Validation Loss: 1.7814555399186022e+17\n",
      "Epoch [1220/1500], Training Loss: 1.790807837617636e+17, Validation Loss: 1.7814555399186022e+17\n",
      "Epoch [1221/1500], Training Loss: 1.7908077025087904e+17, Validation Loss: 1.7814553681199104e+17\n",
      "Epoch [1222/1500], Training Loss: 1.7908076713856842e+17, Validation Loss: 1.7814553681199104e+17\n",
      "Epoch [1223/1500], Training Loss: 1.7908075965384003e+17, Validation Loss: 1.7814551963212186e+17\n",
      "Epoch [1224/1500], Training Loss: 1.7908075464234976e+17, Validation Loss: 1.7814551963212186e+17\n",
      "Epoch [1225/1500], Training Loss: 1.7908074674500557e+17, Validation Loss: 1.7814551963212186e+17\n",
      "Epoch [1226/1500], Training Loss: 1.7908073986774234e+17, Validation Loss: 1.7814551963212186e+17\n",
      "Epoch [1227/1500], Training Loss: 1.790807366589637e+17, Validation Loss: 1.7814551963212186e+17\n",
      "Epoch [1228/1500], Training Loss: 1.79080721769316e+17, Validation Loss: 1.7814551963212186e+17\n",
      "Epoch [1229/1500], Training Loss: 1.7908071754887626e+17, Validation Loss: 1.7814550245225267e+17\n",
      "Epoch [1230/1500], Training Loss: 1.7908071216955734e+17, Validation Loss: 1.7814550245225267e+17\n",
      "Epoch [1231/1500], Training Loss: 1.7908071029136522e+17, Validation Loss: 1.7814550245225267e+17\n",
      "Epoch [1232/1500], Training Loss: 1.790807084422865e+17, Validation Loss: 1.7814550245225267e+17\n",
      "Epoch [1233/1500], Training Loss: 1.7908070076091405e+17, Validation Loss: 1.7814550245225267e+17\n",
      "Epoch [1234/1500], Training Loss: 1.7908068779848502e+17, Validation Loss: 1.7814550245225267e+17\n",
      "Epoch [1235/1500], Training Loss: 1.790806842242325e+17, Validation Loss: 1.781454852723835e+17\n",
      "Epoch [1236/1500], Training Loss: 1.7908067667894125e+17, Validation Loss: 1.781454852723835e+17\n",
      "Epoch [1237/1500], Training Loss: 1.790806738312309e+17, Validation Loss: 1.781454680925143e+17\n",
      "Epoch [1238/1500], Training Loss: 1.7908066398935574e+17, Validation Loss: 1.781454680925143e+17\n",
      "Epoch [1239/1500], Training Loss: 1.7908065796440755e+17, Validation Loss: 1.7814545091264512e+17\n",
      "Epoch [1240/1500], Training Loss: 1.790806542037256e+17, Validation Loss: 1.7814543373277594e+17\n",
      "Epoch [1241/1500], Training Loss: 1.7908063940733738e+17, Validation Loss: 1.7814541655290675e+17\n",
      "Epoch [1242/1500], Training Loss: 1.7908063474519968e+17, Validation Loss: 1.7814541655290675e+17\n",
      "Epoch [1243/1500], Training Loss: 1.7908062974673686e+17, Validation Loss: 1.7814541655290675e+17\n",
      "Epoch [1244/1500], Training Loss: 1.790806275156423e+17, Validation Loss: 1.7814541655290675e+17\n",
      "Epoch [1245/1500], Training Loss: 1.7908062593901533e+17, Validation Loss: 1.7814541655290675e+17\n",
      "Epoch [1246/1500], Training Loss: 1.7908061800763987e+17, Validation Loss: 1.7814539937303757e+17\n",
      "Epoch [1247/1500], Training Loss: 1.790806071262977e+17, Validation Loss: 1.7814539937303757e+17\n",
      "Epoch [1248/1500], Training Loss: 1.7908060130750022e+17, Validation Loss: 1.7814539937303757e+17\n",
      "Epoch [1249/1500], Training Loss: 1.790805942263807e+17, Validation Loss: 1.781453821931684e+17\n",
      "Epoch [1250/1500], Training Loss: 1.790805912715048e+17, Validation Loss: 1.781453821931684e+17\n",
      "Epoch [1251/1500], Training Loss: 1.7908058083586234e+17, Validation Loss: 1.781453821931684e+17\n",
      "Epoch [1252/1500], Training Loss: 1.7908057595635786e+17, Validation Loss: 1.781453821931684e+17\n",
      "Epoch [1253/1500], Training Loss: 1.790805711504416e+17, Validation Loss: 1.781453821931684e+17\n",
      "Epoch [1254/1500], Training Loss: 1.7908055620249632e+17, Validation Loss: 1.781453650132992e+17\n",
      "Epoch [1255/1500], Training Loss: 1.7908055243300723e+17, Validation Loss: 1.7814534783343e+17\n",
      "Epoch [1256/1500], Training Loss: 1.7908054660690592e+17, Validation Loss: 1.7814534783343e+17\n",
      "Epoch [1257/1500], Training Loss: 1.790805443537374e+17, Validation Loss: 1.7814534783343e+17\n",
      "Epoch [1258/1500], Training Loss: 1.7908054304095494e+17, Validation Loss: 1.7814534783343e+17\n",
      "Epoch [1259/1500], Training Loss: 1.7908053530286518e+17, Validation Loss: 1.7814534783343e+17\n",
      "Epoch [1260/1500], Training Loss: 1.7908052733961555e+17, Validation Loss: 1.7814531347369165e+17\n",
      "Epoch [1261/1500], Training Loss: 1.790805187505433e+17, Validation Loss: 1.7814531347369165e+17\n",
      "Epoch [1262/1500], Training Loss: 1.7908051181575696e+17, Validation Loss: 1.7814531347369165e+17\n",
      "Epoch [1263/1500], Training Loss: 1.7908050866003126e+17, Validation Loss: 1.7814531347369165e+17\n",
      "Epoch [1264/1500], Training Loss: 1.7908049818213347e+17, Validation Loss: 1.7814527911395328e+17\n",
      "Epoch [1265/1500], Training Loss: 1.7908049391966742e+17, Validation Loss: 1.7814527911395328e+17\n",
      "Epoch [1266/1500], Training Loss: 1.790804885754603e+17, Validation Loss: 1.7814527911395328e+17\n",
      "Epoch [1267/1500], Training Loss: 1.790804734899202e+17, Validation Loss: 1.7814522757434573e+17\n",
      "Epoch [1268/1500], Training Loss: 1.7908047149002768e+17, Validation Loss: 1.7814521039447654e+17\n",
      "Epoch [1269/1500], Training Loss: 1.7908046388267654e+17, Validation Loss: 1.7814521039447654e+17\n",
      "Epoch [1270/1500], Training Loss: 1.7908046185011654e+17, Validation Loss: 1.7814521039447654e+17\n",
      "Epoch [1271/1500], Training Loss: 1.7908046053404026e+17, Validation Loss: 1.7814521039447654e+17\n",
      "Epoch [1272/1500], Training Loss: 1.790804530091407e+17, Validation Loss: 1.7814521039447654e+17\n",
      "Epoch [1273/1500], Training Loss: 1.790804482188794e+17, Validation Loss: 1.7814521039447654e+17\n",
      "Epoch [1274/1500], Training Loss: 1.790804360759939e+17, Validation Loss: 1.7814517603473818e+17\n",
      "Epoch [1275/1500], Training Loss: 1.790804297210251e+17, Validation Loss: 1.7814517603473818e+17\n",
      "Epoch [1276/1500], Training Loss: 1.7908042618185107e+17, Validation Loss: 1.7814517603473818e+17\n",
      "Epoch [1277/1500], Training Loss: 1.7908041513588483e+17, Validation Loss: 1.78145158854869e+17\n",
      "Epoch [1278/1500], Training Loss: 1.7908041089055622e+17, Validation Loss: 1.78145158854869e+17\n",
      "Epoch [1279/1500], Training Loss: 1.7908040587934288e+17, Validation Loss: 1.78145158854869e+17\n",
      "Epoch [1280/1500], Training Loss: 1.7908039083525235e+17, Validation Loss: 1.7814512449513062e+17\n",
      "Epoch [1281/1500], Training Loss: 1.790803889608725e+17, Validation Loss: 1.7814512449513062e+17\n",
      "Epoch [1282/1500], Training Loss: 1.7908038100116858e+17, Validation Loss: 1.7814512449513062e+17\n",
      "Epoch [1283/1500], Training Loss: 1.7908037885887795e+17, Validation Loss: 1.7814512449513062e+17\n",
      "Epoch [1284/1500], Training Loss: 1.7908037757800096e+17, Validation Loss: 1.7814510731526144e+17\n",
      "Epoch [1285/1500], Training Loss: 1.7908037044407005e+17, Validation Loss: 1.7814510731526144e+17\n",
      "Epoch [1286/1500], Training Loss: 1.7908036754750218e+17, Validation Loss: 1.7814507295552307e+17\n",
      "Epoch [1287/1500], Training Loss: 1.7908035298049837e+17, Validation Loss: 1.781450557756539e+17\n",
      "Epoch [1288/1500], Training Loss: 1.7908034792483315e+17, Validation Loss: 1.781450557756539e+17\n",
      "Epoch [1289/1500], Training Loss: 1.790803432753564e+17, Validation Loss: 1.781450557756539e+17\n",
      "Epoch [1290/1500], Training Loss: 1.7908033250566662e+17, Validation Loss: 1.781450557756539e+17\n",
      "Epoch [1291/1500], Training Loss: 1.7908032852479674e+17, Validation Loss: 1.781450557756539e+17\n",
      "Epoch [1292/1500], Training Loss: 1.7908032292008682e+17, Validation Loss: 1.781450557756539e+17\n",
      "Epoch [1293/1500], Training Loss: 1.7908030917395277e+17, Validation Loss: 1.781450385957847e+17\n",
      "Epoch [1294/1500], Training Loss: 1.7908030628389965e+17, Validation Loss: 1.781450385957847e+17\n",
      "Epoch [1295/1500], Training Loss: 1.7908029827351846e+17, Validation Loss: 1.781450385957847e+17\n",
      "Epoch [1296/1500], Training Loss: 1.79080295854993e+17, Validation Loss: 1.781450385957847e+17\n",
      "Epoch [1297/1500], Training Loss: 1.7908029486936835e+17, Validation Loss: 1.781450385957847e+17\n",
      "Epoch [1298/1500], Training Loss: 1.7908028783753946e+17, Validation Loss: 1.7814502141591552e+17\n",
      "Epoch [1299/1500], Training Loss: 1.7908028482484067e+17, Validation Loss: 1.7814502141591552e+17\n",
      "Epoch [1300/1500], Training Loss: 1.7908026990759946e+17, Validation Loss: 1.7814502141591552e+17\n",
      "Epoch [1301/1500], Training Loss: 1.790802656106921e+17, Validation Loss: 1.7814502141591552e+17\n",
      "Epoch [1302/1500], Training Loss: 1.7908026026303706e+17, Validation Loss: 1.7814500423604634e+17\n",
      "Epoch [1303/1500], Training Loss: 1.7908024969404925e+17, Validation Loss: 1.7814500423604634e+17\n",
      "Epoch [1304/1500], Training Loss: 1.7908024747508272e+17, Validation Loss: 1.7814498705617715e+17\n",
      "Epoch [1305/1500], Training Loss: 1.7908024008011184e+17, Validation Loss: 1.7814498705617715e+17\n",
      "Epoch [1306/1500], Training Loss: 1.7908022710742867e+17, Validation Loss: 1.7814496987630797e+17\n",
      "Epoch [1307/1500], Training Loss: 1.79080223628143e+17, Validation Loss: 1.7814496987630797e+17\n",
      "Epoch [1308/1500], Training Loss: 1.7908021616708973e+17, Validation Loss: 1.7814496987630797e+17\n",
      "Epoch [1309/1500], Training Loss: 1.7908021324028202e+17, Validation Loss: 1.781449526964388e+17\n",
      "Epoch [1310/1500], Training Loss: 1.790802122329228e+17, Validation Loss: 1.781449526964388e+17\n",
      "Epoch [1311/1500], Training Loss: 1.7908020576493574e+17, Validation Loss: 1.781449526964388e+17\n",
      "Epoch [1312/1500], Training Loss: 1.7908020235222176e+17, Validation Loss: 1.781449526964388e+17\n",
      "Epoch [1313/1500], Training Loss: 1.79080187250298e+17, Validation Loss: 1.781449355165696e+17\n",
      "Epoch [1314/1500], Training Loss: 1.790801829851107e+17, Validation Loss: 1.781449355165696e+17\n",
      "Epoch [1315/1500], Training Loss: 1.7908017786236157e+17, Validation Loss: 1.781449355165696e+17\n",
      "Epoch [1316/1500], Training Loss: 1.7908016667289162e+17, Validation Loss: 1.7814490115683123e+17\n",
      "Epoch [1317/1500], Training Loss: 1.7908016497682746e+17, Validation Loss: 1.7814490115683123e+17\n",
      "Epoch [1318/1500], Training Loss: 1.7908015713132858e+17, Validation Loss: 1.7814490115683123e+17\n",
      "Epoch [1319/1500], Training Loss: 1.7908014521959302e+17, Validation Loss: 1.7814486679709286e+17\n",
      "Epoch [1320/1500], Training Loss: 1.7908014046537427e+17, Validation Loss: 1.7814486679709286e+17\n",
      "Epoch [1321/1500], Training Loss: 1.790801331329775e+17, Validation Loss: 1.7814486679709286e+17\n",
      "Epoch [1322/1500], Training Loss: 1.7908013029873395e+17, Validation Loss: 1.7814486679709286e+17\n",
      "Epoch [1323/1500], Training Loss: 1.7908012898396938e+17, Validation Loss: 1.7814486679709286e+17\n",
      "Epoch [1324/1500], Training Loss: 1.7908012371527222e+17, Validation Loss: 1.7814484961722368e+17\n",
      "Epoch [1325/1500], Training Loss: 1.7908011930885488e+17, Validation Loss: 1.7814484961722368e+17\n",
      "Epoch [1326/1500], Training Loss: 1.79080104554899e+17, Validation Loss: 1.7814484961722368e+17\n",
      "Epoch [1327/1500], Training Loss: 1.7908010022842848e+17, Validation Loss: 1.7814484961722368e+17\n",
      "Epoch [1328/1500], Training Loss: 1.7908009492271446e+17, Validation Loss: 1.7814484961722368e+17\n",
      "Epoch [1329/1500], Training Loss: 1.7908008393023414e+17, Validation Loss: 1.7814484961722368e+17\n",
      "Epoch [1330/1500], Training Loss: 1.7908008257480496e+17, Validation Loss: 1.7814484961722368e+17\n",
      "Epoch [1331/1500], Training Loss: 1.790800746536483e+17, Validation Loss: 1.781448324373545e+17\n",
      "Epoch [1332/1500], Training Loss: 1.790800660488544e+17, Validation Loss: 1.7814479807761613e+17\n",
      "Epoch [1333/1500], Training Loss: 1.7908005810079338e+17, Validation Loss: 1.7814479807761613e+17\n",
      "Epoch [1334/1500], Training Loss: 1.7908005106679082e+17, Validation Loss: 1.7814479807761613e+17\n",
      "Epoch [1335/1500], Training Loss: 1.7908004803813933e+17, Validation Loss: 1.7814479807761613e+17\n",
      "Epoch [1336/1500], Training Loss: 1.7908004640804723e+17, Validation Loss: 1.7814479807761613e+17\n",
      "Epoch [1337/1500], Training Loss: 1.790800419928915e+17, Validation Loss: 1.7814478089774694e+17\n",
      "Epoch [1338/1500], Training Loss: 1.7908003678559418e+17, Validation Loss: 1.7814478089774694e+17\n",
      "Epoch [1339/1500], Training Loss: 1.7908002182730736e+17, Validation Loss: 1.7814476371787776e+17\n",
      "Epoch [1340/1500], Training Loss: 1.790800192012216e+17, Validation Loss: 1.781447293581394e+17\n",
      "Epoch [1341/1500], Training Loss: 1.79080012234761e+17, Validation Loss: 1.781447293581394e+17\n",
      "Epoch [1342/1500], Training Loss: 1.790800015319344e+17, Validation Loss: 1.781447293581394e+17\n",
      "Epoch [1343/1500], Training Loss: 1.7907999989815082e+17, Validation Loss: 1.781447293581394e+17\n",
      "Epoch [1344/1500], Training Loss: 1.790799924105888e+17, Validation Loss: 1.781447293581394e+17\n",
      "Epoch [1345/1500], Training Loss: 1.7907998640550346e+17, Validation Loss: 1.7814469499840102e+17\n",
      "Epoch [1346/1500], Training Loss: 1.7907997536686605e+17, Validation Loss: 1.7814467781853184e+17\n",
      "Epoch [1347/1500], Training Loss: 1.7907996875361597e+17, Validation Loss: 1.7814467781853184e+17\n",
      "Epoch [1348/1500], Training Loss: 1.790799653985396e+17, Validation Loss: 1.7814467781853184e+17\n",
      "Epoch [1349/1500], Training Loss: 1.7907996345096918e+17, Validation Loss: 1.7814467781853184e+17\n",
      "Epoch [1350/1500], Training Loss: 1.7907995925242858e+17, Validation Loss: 1.7814466063866266e+17\n",
      "Epoch [1351/1500], Training Loss: 1.790799539633252e+17, Validation Loss: 1.7814466063866266e+17\n",
      "Epoch [1352/1500], Training Loss: 1.7907993884868835e+17, Validation Loss: 1.7814466063866266e+17\n",
      "Epoch [1353/1500], Training Loss: 1.790799371390388e+17, Validation Loss: 1.7814466063866266e+17\n",
      "Epoch [1354/1500], Training Loss: 1.7907992930026493e+17, Validation Loss: 1.781446262789243e+17\n",
      "Epoch [1355/1500], Training Loss: 1.7907991924258877e+17, Validation Loss: 1.781446262789243e+17\n",
      "Epoch [1356/1500], Training Loss: 1.7907991698047574e+17, Validation Loss: 1.781446262789243e+17\n",
      "Epoch [1357/1500], Training Loss: 1.7907990951636806e+17, Validation Loss: 1.781446262789243e+17\n",
      "Epoch [1358/1500], Training Loss: 1.7907990638700538e+17, Validation Loss: 1.7814459191918592e+17\n",
      "Epoch [1359/1500], Training Loss: 1.7907989234707645e+17, Validation Loss: 1.7814459191918592e+17\n",
      "Epoch [1360/1500], Training Loss: 1.7907988670820208e+17, Validation Loss: 1.7814459191918592e+17\n",
      "Epoch [1361/1500], Training Loss: 1.790798826760532e+17, Validation Loss: 1.7814459191918592e+17\n",
      "Epoch [1362/1500], Training Loss: 1.7907988095604954e+17, Validation Loss: 1.7814459191918592e+17\n",
      "Epoch [1363/1500], Training Loss: 1.790798763607459e+17, Validation Loss: 1.7814457473931674e+17\n",
      "Epoch [1364/1500], Training Loss: 1.7907987133561613e+17, Validation Loss: 1.7814457473931674e+17\n",
      "Epoch [1365/1500], Training Loss: 1.7907985665873597e+17, Validation Loss: 1.7814457473931674e+17\n",
      "Epoch [1366/1500], Training Loss: 1.7907985432703226e+17, Validation Loss: 1.7814457473931674e+17\n",
      "Epoch [1367/1500], Training Loss: 1.7907984643943837e+17, Validation Loss: 1.7814457473931674e+17\n",
      "Epoch [1368/1500], Training Loss: 1.7907983715371814e+17, Validation Loss: 1.7814457473931674e+17\n",
      "Epoch [1369/1500], Training Loss: 1.7907983381504835e+17, Validation Loss: 1.7814457473931674e+17\n",
      "Epoch [1370/1500], Training Loss: 1.7907982683847104e+17, Validation Loss: 1.7814457473931674e+17\n",
      "Epoch [1371/1500], Training Loss: 1.7907982389752205e+17, Validation Loss: 1.7814455755944755e+17\n",
      "Epoch [1372/1500], Training Loss: 1.790798092016535e+17, Validation Loss: 1.7814454037957837e+17\n",
      "Epoch [1373/1500], Training Loss: 1.7907980479437002e+17, Validation Loss: 1.7814454037957837e+17\n",
      "Epoch [1374/1500], Training Loss: 1.7907979961928845e+17, Validation Loss: 1.7814454037957837e+17\n",
      "Epoch [1375/1500], Training Loss: 1.7907979778184637e+17, Validation Loss: 1.7814454037957837e+17\n",
      "Epoch [1376/1500], Training Loss: 1.7907979484372272e+17, Validation Loss: 1.781445231997092e+17\n",
      "Epoch [1377/1500], Training Loss: 1.7907978813714448e+17, Validation Loss: 1.781445231997092e+17\n",
      "Epoch [1378/1500], Training Loss: 1.7907977495067155e+17, Validation Loss: 1.7814450601984e+17\n",
      "Epoch [1379/1500], Training Loss: 1.790797716135217e+17, Validation Loss: 1.7814450601984e+17\n",
      "Epoch [1380/1500], Training Loss: 1.7907976413441696e+17, Validation Loss: 1.7814450601984e+17\n",
      "Epoch [1381/1500], Training Loss: 1.7907976058419357e+17, Validation Loss: 1.7814450601984e+17\n",
      "Epoch [1382/1500], Training Loss: 1.7907975161324704e+17, Validation Loss: 1.7814450601984e+17\n",
      "Epoch [1383/1500], Training Loss: 1.7907974480002374e+17, Validation Loss: 1.781444888399708e+17\n",
      "Epoch [1384/1500], Training Loss: 1.7907974150163968e+17, Validation Loss: 1.781444888399708e+17\n",
      "Epoch [1385/1500], Training Loss: 1.790797264353982e+17, Validation Loss: 1.7814447166010163e+17\n",
      "Epoch [1386/1500], Training Loss: 1.7907972224105293e+17, Validation Loss: 1.7814445448023245e+17\n",
      "Epoch [1387/1500], Training Loss: 1.790797169267317e+17, Validation Loss: 1.7814445448023245e+17\n",
      "Epoch [1388/1500], Training Loss: 1.790797150289162e+17, Validation Loss: 1.7814445448023245e+17\n",
      "Epoch [1389/1500], Training Loss: 1.7907971327357174e+17, Validation Loss: 1.7814445448023245e+17\n",
      "Epoch [1390/1500], Training Loss: 1.7907970542402954e+17, Validation Loss: 1.7814445448023245e+17\n",
      "Epoch [1391/1500], Training Loss: 1.7907969273628422e+17, Validation Loss: 1.7814443730036326e+17\n",
      "Epoch [1392/1500], Training Loss: 1.7907968897766976e+17, Validation Loss: 1.7814443730036326e+17\n",
      "Epoch [1393/1500], Training Loss: 1.7907968153537584e+17, Validation Loss: 1.7814442012049408e+17\n",
      "Epoch [1394/1500], Training Loss: 1.7907967873478877e+17, Validation Loss: 1.7814442012049408e+17\n",
      "Epoch [1395/1500], Training Loss: 1.7907966848705824e+17, Validation Loss: 1.781444029406249e+17\n",
      "Epoch [1396/1500], Training Loss: 1.7907966258123498e+17, Validation Loss: 1.781443857607557e+17\n",
      "Epoch [1397/1500], Training Loss: 1.790796587296567e+17, Validation Loss: 1.781443857607557e+17\n",
      "Epoch [1398/1500], Training Loss: 1.7907964404238496e+17, Validation Loss: 1.7814436858088653e+17\n",
      "Epoch [1399/1500], Training Loss: 1.7907963936994112e+17, Validation Loss: 1.7814435140101734e+17\n",
      "Epoch [1400/1500], Training Loss: 1.790796344253495e+17, Validation Loss: 1.7814435140101734e+17\n",
      "Epoch [1401/1500], Training Loss: 1.790796321773924e+17, Validation Loss: 1.7814435140101734e+17\n",
      "Epoch [1402/1500], Training Loss: 1.7907963068433296e+17, Validation Loss: 1.7814435140101734e+17\n",
      "Epoch [1403/1500], Training Loss: 1.790796226500477e+17, Validation Loss: 1.7814435140101734e+17\n",
      "Epoch [1404/1500], Training Loss: 1.790796126854166e+17, Validation Loss: 1.7814433422114816e+17\n",
      "Epoch [1405/1500], Training Loss: 1.7907960593757408e+17, Validation Loss: 1.7814433422114816e+17\n",
      "Epoch [1406/1500], Training Loss: 1.7907959901901536e+17, Validation Loss: 1.7814431704127898e+17\n",
      "Epoch [1407/1500], Training Loss: 1.7907959599239392e+17, Validation Loss: 1.7814431704127898e+17\n",
      "Epoch [1408/1500], Training Loss: 1.7907958567239933e+17, Validation Loss: 1.7814431704127898e+17\n",
      "Epoch [1409/1500], Training Loss: 1.790795811597034e+17, Validation Loss: 1.7814431704127898e+17\n",
      "Epoch [1410/1500], Training Loss: 1.7907957603739565e+17, Validation Loss: 1.7814431704127898e+17\n",
      "Epoch [1411/1500], Training Loss: 1.79079561109865e+17, Validation Loss: 1.781442998614098e+17\n",
      "Epoch [1412/1500], Training Loss: 1.7907955791828426e+17, Validation Loss: 1.781442826815406e+17\n",
      "Epoch [1413/1500], Training Loss: 1.7907955149383725e+17, Validation Loss: 1.781442826815406e+17\n",
      "Epoch [1414/1500], Training Loss: 1.7907954923391456e+17, Validation Loss: 1.781442826815406e+17\n",
      "Epoch [1415/1500], Training Loss: 1.79079547953556e+17, Validation Loss: 1.781442826815406e+17\n",
      "Epoch [1416/1500], Training Loss: 1.7907954053712746e+17, Validation Loss: 1.781442826815406e+17\n",
      "Epoch [1417/1500], Training Loss: 1.7907953306276362e+17, Validation Loss: 1.7814423114193306e+17\n",
      "Epoch [1418/1500], Training Loss: 1.790795236071288e+17, Validation Loss: 1.7814423114193306e+17\n",
      "Epoch [1419/1500], Training Loss: 1.7907951677393024e+17, Validation Loss: 1.7814423114193306e+17\n",
      "Epoch [1420/1500], Training Loss: 1.790795135359592e+17, Validation Loss: 1.7814423114193306e+17\n",
      "Epoch [1421/1500], Training Loss: 1.790795026605176e+17, Validation Loss: 1.7814421396206387e+17\n",
      "Epoch [1422/1500], Training Loss: 1.7907949845205184e+17, Validation Loss: 1.7814421396206387e+17\n",
      "Epoch [1423/1500], Training Loss: 1.790794931037327e+17, Validation Loss: 1.7814421396206387e+17\n",
      "Epoch [1424/1500], Training Loss: 1.7907947826119786e+17, Validation Loss: 1.781441796023255e+17\n",
      "Epoch [1425/1500], Training Loss: 1.7907947642987494e+17, Validation Loss: 1.781441796023255e+17\n",
      "Epoch [1426/1500], Training Loss: 1.790794686230253e+17, Validation Loss: 1.781441796023255e+17\n",
      "Epoch [1427/1500], Training Loss: 1.7907946657376307e+17, Validation Loss: 1.781441796023255e+17\n",
      "Epoch [1428/1500], Training Loss: 1.7907946528918422e+17, Validation Loss: 1.7814416242245632e+17\n",
      "Epoch [1429/1500], Training Loss: 1.7907945776672278e+17, Validation Loss: 1.7814416242245632e+17\n",
      "Epoch [1430/1500], Training Loss: 1.7907945357819478e+17, Validation Loss: 1.7814414524258714e+17\n",
      "Epoch [1431/1500], Training Loss: 1.7907944059708976e+17, Validation Loss: 1.7814414524258714e+17\n",
      "Epoch [1432/1500], Training Loss: 1.7907943457017613e+17, Validation Loss: 1.7814414524258714e+17\n",
      "Epoch [1433/1500], Training Loss: 1.7907943077052435e+17, Validation Loss: 1.7814414524258714e+17\n",
      "Epoch [1434/1500], Training Loss: 1.790794202734321e+17, Validation Loss: 1.7814414524258714e+17\n",
      "Epoch [1435/1500], Training Loss: 1.7907941559593094e+17, Validation Loss: 1.7814412806271795e+17\n",
      "Epoch [1436/1500], Training Loss: 1.7907941058090125e+17, Validation Loss: 1.7814412806271795e+17\n",
      "Epoch [1437/1500], Training Loss: 1.790793953944501e+17, Validation Loss: 1.7814412806271795e+17\n",
      "Epoch [1438/1500], Training Loss: 1.7907939351275395e+17, Validation Loss: 1.7814412806271795e+17\n",
      "Epoch [1439/1500], Training Loss: 1.790793855626463e+17, Validation Loss: 1.7814411088284877e+17\n",
      "Epoch [1440/1500], Training Loss: 1.790793834040615e+17, Validation Loss: 1.7814411088284877e+17\n",
      "Epoch [1441/1500], Training Loss: 1.7907938213657008e+17, Validation Loss: 1.7814411088284877e+17\n",
      "Epoch [1442/1500], Training Loss: 1.7907937510227606e+17, Validation Loss: 1.781440765231104e+17\n",
      "Epoch [1443/1500], Training Loss: 1.7907937212033555e+17, Validation Loss: 1.781440765231104e+17\n",
      "Epoch [1444/1500], Training Loss: 1.7907935751485955e+17, Validation Loss: 1.781440765231104e+17\n",
      "Epoch [1445/1500], Training Loss: 1.7907935268929498e+17, Validation Loss: 1.7814402498350285e+17\n",
      "Epoch [1446/1500], Training Loss: 1.790793478943075e+17, Validation Loss: 1.7814402498350285e+17\n",
      "Epoch [1447/1500], Training Loss: 1.7907933704480835e+17, Validation Loss: 1.7814402498350285e+17\n",
      "Epoch [1448/1500], Training Loss: 1.7907933347451392e+17, Validation Loss: 1.7814402498350285e+17\n",
      "Epoch [1449/1500], Training Loss: 1.790793274442586e+17, Validation Loss: 1.7814402498350285e+17\n",
      "Epoch [1450/1500], Training Loss: 1.7907931383887792e+17, Validation Loss: 1.7814399062376448e+17\n",
      "Epoch [1451/1500], Training Loss: 1.7907931081779888e+17, Validation Loss: 1.7814399062376448e+17\n",
      "Epoch [1452/1500], Training Loss: 1.7907930331404682e+17, Validation Loss: 1.781439734438953e+17\n",
      "Epoch [1453/1500], Training Loss: 1.7907930042912598e+17, Validation Loss: 1.781439734438953e+17\n",
      "Epoch [1454/1500], Training Loss: 1.7907929954945936e+17, Validation Loss: 1.781439734438953e+17\n",
      "Epoch [1455/1500], Training Loss: 1.7907929263542288e+17, Validation Loss: 1.781439734438953e+17\n",
      "Epoch [1456/1500], Training Loss: 1.7907928944723798e+17, Validation Loss: 1.781439734438953e+17\n",
      "Epoch [1457/1500], Training Loss: 1.7907927445772448e+17, Validation Loss: 1.781439562640261e+17\n",
      "Epoch [1458/1500], Training Loss: 1.790792702375076e+17, Validation Loss: 1.781439562640261e+17\n",
      "Epoch [1459/1500], Training Loss: 1.7907926483544013e+17, Validation Loss: 1.781439562640261e+17\n",
      "Epoch [1460/1500], Training Loss: 1.7907925402759254e+17, Validation Loss: 1.781439562640261e+17\n",
      "Epoch [1461/1500], Training Loss: 1.790792520845881e+17, Validation Loss: 1.7814393908415693e+17\n",
      "Epoch [1462/1500], Training Loss: 1.790792444087436e+17, Validation Loss: 1.7814393908415693e+17\n",
      "Epoch [1463/1500], Training Loss: 1.790792315238484e+17, Validation Loss: 1.7814393908415693e+17\n",
      "Epoch [1464/1500], Training Loss: 1.7907922796751827e+17, Validation Loss: 1.7814393908415693e+17\n",
      "Epoch [1465/1500], Training Loss: 1.7907922046949603e+17, Validation Loss: 1.7814392190428774e+17\n",
      "Epoch [1466/1500], Training Loss: 1.790792176095599e+17, Validation Loss: 1.7814392190428774e+17\n",
      "Epoch [1467/1500], Training Loss: 1.7907921653535414e+17, Validation Loss: 1.7814392190428774e+17\n",
      "Epoch [1468/1500], Training Loss: 1.7907921028399274e+17, Validation Loss: 1.7814392190428774e+17\n",
      "Epoch [1469/1500], Training Loss: 1.790792066757091e+17, Validation Loss: 1.7814392190428774e+17\n",
      "Epoch [1470/1500], Training Loss: 1.7907919173543206e+17, Validation Loss: 1.7814388754454938e+17\n",
      "Epoch [1471/1500], Training Loss: 1.7907918732886275e+17, Validation Loss: 1.7814388754454938e+17\n",
      "Epoch [1472/1500], Training Loss: 1.7907918234211782e+17, Validation Loss: 1.7814388754454938e+17\n",
      "Epoch [1473/1500], Training Loss: 1.790791715465419e+17, Validation Loss: 1.7814388754454938e+17\n",
      "Epoch [1474/1500], Training Loss: 1.7907916994869462e+17, Validation Loss: 1.7814388754454938e+17\n",
      "Epoch [1475/1500], Training Loss: 1.7907916200366512e+17, Validation Loss: 1.781438703646802e+17\n",
      "Epoch [1476/1500], Training Loss: 1.7907915087063142e+17, Validation Loss: 1.78143853184811e+17\n",
      "Epoch [1477/1500], Training Loss: 1.7907914537194822e+17, Validation Loss: 1.78143853184811e+17\n",
      "Epoch [1478/1500], Training Loss: 1.7907913821048218e+17, Validation Loss: 1.7814383600494182e+17\n",
      "Epoch [1479/1500], Training Loss: 1.7907913527278538e+17, Validation Loss: 1.7814383600494182e+17\n",
      "Epoch [1480/1500], Training Loss: 1.7907913387121162e+17, Validation Loss: 1.7814383600494182e+17\n",
      "Epoch [1481/1500], Training Loss: 1.7907912889617197e+17, Validation Loss: 1.7814381882507264e+17\n",
      "Epoch [1482/1500], Training Loss: 1.7907912420271184e+17, Validation Loss: 1.7814381882507264e+17\n",
      "Epoch [1483/1500], Training Loss: 1.790791094208856e+17, Validation Loss: 1.7814380164520346e+17\n",
      "Epoch [1484/1500], Training Loss: 1.7907910552169696e+17, Validation Loss: 1.7814380164520346e+17\n",
      "Epoch [1485/1500], Training Loss: 1.790790998281915e+17, Validation Loss: 1.781437672854651e+17\n",
      "Epoch [1486/1500], Training Loss: 1.7907908844527142e+17, Validation Loss: 1.781437672854651e+17\n",
      "Epoch [1487/1500], Training Loss: 1.7907908711222227e+17, Validation Loss: 1.781437672854651e+17\n",
      "Epoch [1488/1500], Training Loss: 1.7907907915452346e+17, Validation Loss: 1.781437501055959e+17\n",
      "Epoch [1489/1500], Training Loss: 1.7907907116904166e+17, Validation Loss: 1.7814373292572672e+17\n",
      "Epoch [1490/1500], Training Loss: 1.7907906255485984e+17, Validation Loss: 1.7814373292572672e+17\n",
      "Epoch [1491/1500], Training Loss: 1.790790556449042e+17, Validation Loss: 1.7814373292572672e+17\n",
      "Epoch [1492/1500], Training Loss: 1.79079052576321e+17, Validation Loss: 1.7814373292572672e+17\n",
      "Epoch [1493/1500], Training Loss: 1.7907905083478058e+17, Validation Loss: 1.7814373292572672e+17\n",
      "Epoch [1494/1500], Training Loss: 1.7907904662642307e+17, Validation Loss: 1.7814371574585754e+17\n",
      "Epoch [1495/1500], Training Loss: 1.790790412072016e+17, Validation Loss: 1.7814371574585754e+17\n",
      "Epoch [1496/1500], Training Loss: 1.7907902624486518e+17, Validation Loss: 1.7814368138611917e+17\n",
      "Epoch [1497/1500], Training Loss: 1.7907902413017645e+17, Validation Loss: 1.7814368138611917e+17\n",
      "Epoch [1498/1500], Training Loss: 1.7907901663312026e+17, Validation Loss: 1.7814368138611917e+17\n",
      "Epoch [1499/1500], Training Loss: 1.7907900610719776e+17, Validation Loss: 1.7814368138611917e+17\n",
      "Epoch [1500/1500], Training Loss: 1.7907900441374454e+17, Validation Loss: 1.7814368138611917e+17\n",
      "Final Test Loss: 1.8028611415257907e+17\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define a function to split the data\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i + split_window_size + 1]\n",
    "        y_out1_out2 = y_in.iloc[i:i + split_window_size + 1]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data = data.drop(\"date\", axis=1)\n",
    "data = data.fillna(0)  # Filling null values with zero\n",
    "data = data.astype('float32')\n",
    "\n",
    "# Keep data until 31.07.2023\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Ensuring deterministic behavior in cuDNN\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.0003]\n",
    "window_sizes = [5]\n",
    "\n",
    "num_epochs = 1500\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Loop through each column to use it as the target variable\n",
    "for target_column in data.columns:\n",
    "    print(f\"Training model with target variable: {target_column}\")\n",
    "    print()\n",
    "\n",
    "    # Set the target column as y_data and the rest as x_data\n",
    "    y_data = data[target_column]\n",
    "    x_data = data.drop(columns=[target_column])\n",
    "\n",
    "    # Split Data to train and temp\n",
    "    split_window_size = 3\n",
    "    x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "    # Split temp into val and test\n",
    "    split_window_size = 1\n",
    "    x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_normalized = scaler.fit_transform(x_train)\n",
    "    x_val_normalized = scaler.transform(x_val)\n",
    "    x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "    for hyperparams in hyperparameter_combinations:\n",
    "        input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "        # Print hyperparameters\n",
    "        print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "        # Initialize the model\n",
    "        model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        counter = 0\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i in range(len(x_train_tensor)):\n",
    "                window_end = min(i + window_size, len(x_train_tensor))\n",
    "                inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(x_val_tensor)):\n",
    "                    window_end = min(i + window_size, len(x_val_tensor))\n",
    "                    inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                    labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss += criterion(outputs, labels)\n",
    "\n",
    "            # Early stopping based on validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch}')\n",
    "                    break\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "        # Calculate test loss after training is complete\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_test_tensor)):\n",
    "                window_end = min(i + window_size, len(x_test_tensor))\n",
    "                inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                test_loss += criterion(outputs, labels)\n",
    "\n",
    "        print(f'Final Test Loss: {test_loss / len(x_test_tensor)}')\n",
    "\n",
    "        for _ in range(4):\n",
    "            print()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with target variable: open\n",
      "Training model with target variable: high\n",
      "Training model with target variable: low\n",
      "Training model with target variable: close\n",
      "Training model with target variable: volume\n",
      "Training model with target variable: adjusted_close\n",
      "Training model with target variable: change_percent\n",
      "Training model with target variable: avg_vol_20d\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define your LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define a function to split the data\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i + split_window_size + 1]\n",
    "        y_out1_out2 = y_in.iloc[i:i + split_window_size + 1]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data = data.drop(\"date\", axis=1)\n",
    "data = data.fillna(0)  # Filling null values with zero\n",
    "data = data.astype('float32')\n",
    "\n",
    "# Keep data until 31.07.2023\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "# Loop through each column to use it as the target variable\n",
    "for target_column in data.columns:\n",
    "    print(f\"Training model with target variable: {target_column}\")\n",
    "\n",
    "    # Set the target column as y_data and the rest as x_data\n",
    "    y_data = data[target_column]\n",
    "    x_data = data.drop(columns=[target_column])\n",
    "\n",
    "    # Split Data to train and temp\n",
    "    split_window_size = 3\n",
    "    x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "    # Split temp into val and test\n",
    "    split_window_size = 1\n",
    "    x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_normalized = scaler.fit_transform(x_train)\n",
    "    x_val_normalized = scaler.transform(x_val)\n",
    "    x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # At this point, you can train your LSTM model with the `x_train_tensor` and `y_train_tensor`.\n",
    "    # And you can evaluate it using `x_val_tensor`, `y_val_tensor`, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
