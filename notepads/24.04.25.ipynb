{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.0002, window_size=6\n",
    "Early stopping at epoch 410\n",
    "Final Test Loss: 9.392582893371582\n",
    "\n",
    "Hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.0002, window_size=7\n",
    "Final Test Loss: 12.175009727478027\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.0002, window_size=5\n",
      "Epoch [1/1200], Training Loss: 27106.72337908144, Validation Loss: 26555.455078125\n",
      "Epoch [2/1200], Training Loss: 25188.89507490649, Validation Loss: 24831.21484375\n",
      "Epoch [3/1200], Training Loss: 23704.539963062944, Validation Loss: 23454.642578125\n",
      "Epoch [4/1200], Training Loss: 22552.00051886824, Validation Loss: 22383.990234375\n",
      "Epoch [5/1200], Training Loss: 21673.043056203314, Validation Loss: 21562.671875\n",
      "Epoch [6/1200], Training Loss: 21004.204643214613, Validation Loss: 20932.716796875\n",
      "Epoch [7/1200], Training Loss: 19875.877832368238, Validation Loss: 19775.63671875\n",
      "Epoch [8/1200], Training Loss: 18845.673225793264, Validation Loss: 18777.5625\n",
      "Epoch [9/1200], Training Loss: 17961.15567246013, Validation Loss: 17899.828125\n",
      "Epoch [10/1200], Training Loss: 17142.866305356172, Validation Loss: 17102.705078125\n",
      "Epoch [11/1200], Training Loss: 16381.01566835584, Validation Loss: 16398.43359375\n",
      "Epoch [12/1200], Training Loss: 15673.672454281783, Validation Loss: 15700.9619140625\n",
      "Epoch [13/1200], Training Loss: 15016.304280789414, Validation Loss: 15015.830078125\n",
      "Epoch [14/1200], Training Loss: 14403.211264825142, Validation Loss: 14365.0615234375\n",
      "Epoch [15/1200], Training Loss: 13828.62627131239, Validation Loss: 13815.5166015625\n",
      "Epoch [16/1200], Training Loss: 13290.387015814009, Validation Loss: 13249.2021484375\n",
      "Epoch [17/1200], Training Loss: 12784.099177269807, Validation Loss: 12747.287109375\n",
      "Epoch [18/1200], Training Loss: 12308.167381935695, Validation Loss: 12278.5849609375\n",
      "Epoch [19/1200], Training Loss: 11861.242184811199, Validation Loss: 11844.31640625\n",
      "Epoch [20/1200], Training Loss: 11440.837593756227, Validation Loss: 11441.607421875\n",
      "Epoch [21/1200], Training Loss: 11045.206398230488, Validation Loss: 11068.083984375\n",
      "Epoch [22/1200], Training Loss: 10673.215485145083, Validation Loss: 10719.15625\n",
      "Epoch [23/1200], Training Loss: 10323.591681685622, Validation Loss: 10385.5830078125\n",
      "Epoch [24/1200], Training Loss: 9995.445566824877, Validation Loss: 10062.75\n",
      "Epoch [25/1200], Training Loss: 9688.229385016908, Validation Loss: 9753.1123046875\n",
      "Epoch [26/1200], Training Loss: 9401.047384108939, Validation Loss: 9460.6005859375\n",
      "Epoch [27/1200], Training Loss: 9132.555586821682, Validation Loss: 9186.9599609375\n",
      "Epoch [28/1200], Training Loss: 8881.326137565678, Validation Loss: 8944.05859375\n",
      "Epoch [29/1200], Training Loss: 8567.811590204423, Validation Loss: 9169.5185546875\n",
      "Epoch [30/1200], Training Loss: 8287.47484987114, Validation Loss: 9087.6728515625\n",
      "Epoch [31/1200], Training Loss: 8029.957927592331, Validation Loss: 8511.6201171875\n",
      "Epoch [32/1200], Training Loss: 7781.159109483679, Validation Loss: 8094.27587890625\n",
      "Epoch [33/1200], Training Loss: 7543.565186895843, Validation Loss: 7747.69970703125\n",
      "Epoch [34/1200], Training Loss: 7315.386230034087, Validation Loss: 7458.21240234375\n",
      "Epoch [35/1200], Training Loss: 7096.15709617705, Validation Loss: 7209.76904296875\n",
      "Epoch [36/1200], Training Loss: 6884.35561841315, Validation Loss: 6982.009765625\n",
      "Epoch [37/1200], Training Loss: 6678.6627948537725, Validation Loss: 6763.44091796875\n",
      "Epoch [38/1200], Training Loss: 6478.430610972124, Validation Loss: 6553.0478515625\n",
      "Epoch [39/1200], Training Loss: 6283.648743289705, Validation Loss: 6353.40576171875\n",
      "Epoch [40/1200], Training Loss: 6094.401807530713, Validation Loss: 6166.0341796875\n",
      "Epoch [41/1200], Training Loss: 5910.76811133894, Validation Loss: 5979.7470703125\n",
      "Epoch [42/1200], Training Loss: 5732.9066032017245, Validation Loss: 5798.48876953125\n",
      "Epoch [43/1200], Training Loss: 5560.988908462795, Validation Loss: 5626.4140625\n",
      "Epoch [44/1200], Training Loss: 5394.997568118752, Validation Loss: 5460.9111328125\n",
      "Epoch [45/1200], Training Loss: 5234.8711917461105, Validation Loss: 5299.75146484375\n",
      "Epoch [46/1200], Training Loss: 5080.239891945439, Validation Loss: 5143.125\n",
      "Epoch [47/1200], Training Loss: 4929.9888476142, Validation Loss: 4987.58154296875\n",
      "Epoch [48/1200], Training Loss: 4783.494910265841, Validation Loss: 4837.6689453125\n",
      "Epoch [49/1200], Training Loss: 4641.207998608235, Validation Loss: 4695.857421875\n",
      "Epoch [50/1200], Training Loss: 4503.015568005538, Validation Loss: 4559.298828125\n",
      "Epoch [51/1200], Training Loss: 4368.661044426559, Validation Loss: 4428.65185546875\n",
      "Epoch [52/1200], Training Loss: 4237.842520138144, Validation Loss: 4300.705078125\n",
      "Epoch [53/1200], Training Loss: 4110.342964201244, Validation Loss: 4173.216796875\n",
      "Epoch [54/1200], Training Loss: 3986.0198470223495, Validation Loss: 4045.512939453125\n",
      "Epoch [55/1200], Training Loss: 3864.741093553593, Validation Loss: 3919.84814453125\n",
      "Epoch [56/1200], Training Loss: 3746.419115486926, Validation Loss: 3798.400390625\n",
      "Epoch [57/1200], Training Loss: 3631.0558149877306, Validation Loss: 3681.994384765625\n",
      "Epoch [58/1200], Training Loss: 3518.771840496592, Validation Loss: 3567.966552734375\n",
      "Epoch [59/1200], Training Loss: 3409.859087651522, Validation Loss: 3455.26611328125\n",
      "Epoch [60/1200], Training Loss: 3304.3322676506946, Validation Loss: 3348.101318359375\n",
      "Epoch [61/1200], Training Loss: 3202.021475462363, Validation Loss: 3245.488525390625\n",
      "Epoch [62/1200], Training Loss: 3102.897722574682, Validation Loss: 3142.67822265625\n",
      "Epoch [63/1200], Training Loss: 3007.2741132213187, Validation Loss: 3043.5185546875\n",
      "Epoch [64/1200], Training Loss: 2915.090110457868, Validation Loss: 2949.721923828125\n",
      "Epoch [65/1200], Training Loss: 2826.0699005377014, Validation Loss: 2860.784423828125\n",
      "Epoch [66/1200], Training Loss: 2740.266797488643, Validation Loss: 2775.99951171875\n",
      "Epoch [67/1200], Training Loss: 2657.7155775567758, Validation Loss: 2694.69287109375\n",
      "Epoch [68/1200], Training Loss: 2578.3580885458246, Validation Loss: 2616.017822265625\n",
      "Epoch [69/1200], Training Loss: 2502.2717812637006, Validation Loss: 2539.941650390625\n",
      "Epoch [70/1200], Training Loss: 2429.62511379216, Validation Loss: 2466.616943359375\n",
      "Epoch [71/1200], Training Loss: 2360.474367160741, Validation Loss: 2396.61572265625\n",
      "Epoch [72/1200], Training Loss: 2294.483164372227, Validation Loss: 2330.283203125\n",
      "Epoch [73/1200], Training Loss: 2230.5038559491536, Validation Loss: 2266.209716796875\n",
      "Epoch [74/1200], Training Loss: 2168.262151954882, Validation Loss: 2203.35400390625\n",
      "Epoch [75/1200], Training Loss: 2107.883856838966, Validation Loss: 2141.70703125\n",
      "Epoch [76/1200], Training Loss: 2049.2888251622962, Validation Loss: 2082.19677734375\n",
      "Epoch [77/1200], Training Loss: 1992.3641030338833, Validation Loss: 2024.729736328125\n",
      "Epoch [78/1200], Training Loss: 1936.929458593787, Validation Loss: 1968.9320068359375\n",
      "Epoch [79/1200], Training Loss: 1883.0365778780501, Validation Loss: 1914.56884765625\n",
      "Epoch [80/1200], Training Loss: 1830.6706829959796, Validation Loss: 1861.4664306640625\n",
      "Epoch [81/1200], Training Loss: 1779.794792308142, Validation Loss: 1809.342529296875\n",
      "Epoch [82/1200], Training Loss: 1730.4211722549617, Validation Loss: 1758.1715087890625\n",
      "Epoch [83/1200], Training Loss: 1682.5389645188925, Validation Loss: 1708.0909423828125\n",
      "Epoch [84/1200], Training Loss: 1636.1880046563585, Validation Loss: 1659.8695068359375\n",
      "Epoch [85/1200], Training Loss: 1591.4877488622185, Validation Loss: 1614.15283203125\n",
      "Epoch [86/1200], Training Loss: 1548.4945618380204, Validation Loss: 1570.7672119140625\n",
      "Epoch [87/1200], Training Loss: 1507.1844261271438, Validation Loss: 1529.4083251953125\n",
      "Epoch [88/1200], Training Loss: 1467.5031769672905, Validation Loss: 1489.7479248046875\n",
      "Epoch [89/1200], Training Loss: 1429.3712344566216, Validation Loss: 1451.4891357421875\n",
      "Epoch [90/1200], Training Loss: 1392.692481595559, Validation Loss: 1414.5394287109375\n",
      "Epoch [91/1200], Training Loss: 1357.309464056871, Validation Loss: 1378.75927734375\n",
      "Epoch [92/1200], Training Loss: 1323.1018872603183, Validation Loss: 1344.1434326171875\n",
      "Epoch [93/1200], Training Loss: 1289.9496692517475, Validation Loss: 1310.80078125\n",
      "Epoch [94/1200], Training Loss: 1257.781888523705, Validation Loss: 1278.7005615234375\n",
      "Epoch [95/1200], Training Loss: 1226.5345598897886, Validation Loss: 1247.7186279296875\n",
      "Epoch [96/1200], Training Loss: 1196.1624278928023, Validation Loss: 1217.695556640625\n",
      "Epoch [97/1200], Training Loss: 1166.6092266814292, Validation Loss: 1188.5096435546875\n",
      "Epoch [98/1200], Training Loss: 1137.8184940738347, Validation Loss: 1160.09130859375\n",
      "Epoch [99/1200], Training Loss: 1109.7415690457024, Validation Loss: 1132.4012451171875\n",
      "Epoch [100/1200], Training Loss: 1082.337000818339, Validation Loss: 1105.4027099609375\n",
      "Epoch [101/1200], Training Loss: 1055.5718381789693, Validation Loss: 1079.0855712890625\n",
      "Epoch [102/1200], Training Loss: 1029.4354724067196, Validation Loss: 1053.4462890625\n",
      "Epoch [103/1200], Training Loss: 1003.9161116797986, Validation Loss: 1028.4544677734375\n",
      "Epoch [104/1200], Training Loss: 979.0161477401016, Validation Loss: 1004.0848999023438\n",
      "Epoch [105/1200], Training Loss: 954.7221140617232, Validation Loss: 980.2772216796875\n",
      "Epoch [106/1200], Training Loss: 931.0350650851724, Validation Loss: 956.9570922851562\n",
      "Epoch [107/1200], Training Loss: 907.9411431925735, Validation Loss: 934.0966796875\n",
      "Epoch [108/1200], Training Loss: 885.4313272665288, Validation Loss: 911.7047729492188\n",
      "Epoch [109/1200], Training Loss: 863.487685700101, Validation Loss: 889.8173828125\n",
      "Epoch [110/1200], Training Loss: 842.0869364519136, Validation Loss: 868.4464111328125\n",
      "Epoch [111/1200], Training Loss: 821.2130874875468, Validation Loss: 847.52294921875\n",
      "Epoch [112/1200], Training Loss: 800.8441451835671, Validation Loss: 826.9598999023438\n",
      "Epoch [113/1200], Training Loss: 780.9710683988883, Validation Loss: 806.6795043945312\n",
      "Epoch [114/1200], Training Loss: 761.5707388093468, Validation Loss: 786.6865844726562\n",
      "Epoch [115/1200], Training Loss: 742.6224158725487, Validation Loss: 767.1528930664062\n",
      "Epoch [116/1200], Training Loss: 724.1077104639246, Validation Loss: 748.2096557617188\n",
      "Epoch [117/1200], Training Loss: 706.0019704867319, Validation Loss: 729.8054809570312\n",
      "Epoch [118/1200], Training Loss: 688.3047679069973, Validation Loss: 711.9102783203125\n",
      "Epoch [119/1200], Training Loss: 671.0146968365913, Validation Loss: 694.5051879882812\n",
      "Epoch [120/1200], Training Loss: 654.124591634398, Validation Loss: 677.5394287109375\n",
      "Epoch [121/1200], Training Loss: 637.616194590885, Validation Loss: 661.0214233398438\n",
      "Epoch [122/1200], Training Loss: 621.4728401716729, Validation Loss: 644.9442138671875\n",
      "Epoch [123/1200], Training Loss: 605.6833421506113, Validation Loss: 629.2529907226562\n",
      "Epoch [124/1200], Training Loss: 590.2335932657115, Validation Loss: 613.889892578125\n",
      "Epoch [125/1200], Training Loss: 575.1140563749666, Validation Loss: 598.870361328125\n",
      "Epoch [126/1200], Training Loss: 560.3202637561877, Validation Loss: 584.23681640625\n",
      "Epoch [127/1200], Training Loss: 545.8554801294508, Validation Loss: 570.014892578125\n",
      "Epoch [128/1200], Training Loss: 531.7105937434483, Validation Loss: 556.2073364257812\n",
      "Epoch [129/1200], Training Loss: 517.8842550007004, Validation Loss: 542.7926025390625\n",
      "Epoch [130/1200], Training Loss: 504.36750045242235, Validation Loss: 529.727294921875\n",
      "Epoch [131/1200], Training Loss: 491.15541452314454, Validation Loss: 516.9769287109375\n",
      "Epoch [132/1200], Training Loss: 478.24274967524417, Validation Loss: 504.5111389160156\n",
      "Epoch [133/1200], Training Loss: 465.6218023497722, Validation Loss: 492.31365966796875\n",
      "Epoch [134/1200], Training Loss: 453.29255657949955, Validation Loss: 480.38311767578125\n",
      "Epoch [135/1200], Training Loss: 441.2485628108326, Validation Loss: 468.7042236328125\n",
      "Epoch [136/1200], Training Loss: 429.4848747018557, Validation Loss: 457.2729797363281\n",
      "Epoch [137/1200], Training Loss: 417.99883816999045, Validation Loss: 446.0774230957031\n",
      "Epoch [138/1200], Training Loss: 406.78475500660676, Validation Loss: 435.1028137207031\n",
      "Epoch [139/1200], Training Loss: 395.83730253028705, Validation Loss: 424.34368896484375\n",
      "Epoch [140/1200], Training Loss: 385.15626477420847, Validation Loss: 413.7940979003906\n",
      "Epoch [141/1200], Training Loss: 374.740755296064, Validation Loss: 403.45367431640625\n",
      "Epoch [142/1200], Training Loss: 364.5854010454547, Validation Loss: 393.322998046875\n",
      "Epoch [143/1200], Training Loss: 354.6857445402986, Validation Loss: 383.4160461425781\n",
      "Epoch [144/1200], Training Loss: 345.0394991508934, Validation Loss: 373.7445373535156\n",
      "Epoch [145/1200], Training Loss: 335.64352254869436, Validation Loss: 364.3277893066406\n",
      "Epoch [146/1200], Training Loss: 326.4954289272155, Validation Loss: 355.1748352050781\n",
      "Epoch [147/1200], Training Loss: 317.588898178342, Validation Loss: 346.2940368652344\n",
      "Epoch [148/1200], Training Loss: 308.9223303095891, Validation Loss: 337.6843566894531\n",
      "Epoch [149/1200], Training Loss: 300.4897567109052, Validation Loss: 329.34710693359375\n",
      "Epoch [150/1200], Training Loss: 292.2907601563565, Validation Loss: 321.2833557128906\n",
      "Epoch [151/1200], Training Loss: 284.3246041028823, Validation Loss: 313.48870849609375\n",
      "Epoch [152/1200], Training Loss: 276.5862200363667, Validation Loss: 305.9521484375\n",
      "Epoch [153/1200], Training Loss: 269.07163964856284, Validation Loss: 298.66680908203125\n",
      "Epoch [154/1200], Training Loss: 261.7832748342743, Validation Loss: 291.5999755859375\n",
      "Epoch [155/1200], Training Loss: 254.71490051910976, Validation Loss: 284.73443603515625\n",
      "Epoch [156/1200], Training Loss: 247.859304909554, Validation Loss: 278.2684020996094\n",
      "Epoch [157/1200], Training Loss: 241.2181685693259, Validation Loss: 271.42047119140625\n",
      "Epoch [158/1200], Training Loss: 234.77523297674043, Validation Loss: 265.5108337402344\n",
      "Epoch [159/1200], Training Loss: 228.53124882406206, Validation Loss: 259.67730712890625\n",
      "Epoch [160/1200], Training Loss: 222.47106354033684, Validation Loss: 253.34568786621094\n",
      "Epoch [161/1200], Training Loss: 216.58947344365023, Validation Loss: 248.1474151611328\n",
      "Epoch [162/1200], Training Loss: 210.87875331016588, Validation Loss: 242.58412170410156\n",
      "Epoch [163/1200], Training Loss: 205.33686428528117, Validation Loss: 237.58609008789062\n",
      "Epoch [164/1200], Training Loss: 199.95579545843623, Validation Loss: 232.7545166015625\n",
      "Epoch [165/1200], Training Loss: 194.7351267018733, Validation Loss: 228.01776123046875\n",
      "Epoch [166/1200], Training Loss: 189.66812163457755, Validation Loss: 223.46397399902344\n",
      "Epoch [167/1200], Training Loss: 184.75074788454683, Validation Loss: 219.02032470703125\n",
      "Epoch [168/1200], Training Loss: 179.97922298294964, Validation Loss: 214.66903686523438\n",
      "Epoch [169/1200], Training Loss: 175.35014163441315, Validation Loss: 210.4226531982422\n",
      "Epoch [170/1200], Training Loss: 170.85867734385175, Validation Loss: 206.2791748046875\n",
      "Epoch [171/1200], Training Loss: 166.50071237486458, Validation Loss: 202.2490234375\n",
      "Epoch [172/1200], Training Loss: 162.27340151659445, Validation Loss: 198.3522491455078\n",
      "Epoch [173/1200], Training Loss: 158.1727583198576, Validation Loss: 194.5790252685547\n",
      "Epoch [174/1200], Training Loss: 154.1925708931162, Validation Loss: 190.8758087158203\n",
      "Epoch [175/1200], Training Loss: 150.32837920190372, Validation Loss: 187.15187072753906\n",
      "Epoch [176/1200], Training Loss: 146.5714502472097, Validation Loss: 183.3479461669922\n",
      "Epoch [177/1200], Training Loss: 142.91250424437175, Validation Loss: 179.47784423828125\n",
      "Epoch [178/1200], Training Loss: 139.34603691960294, Validation Loss: 175.6251220703125\n",
      "Epoch [179/1200], Training Loss: 135.865405036671, Validation Loss: 171.88433837890625\n",
      "Epoch [180/1200], Training Loss: 132.46887231374865, Validation Loss: 168.31642150878906\n",
      "Epoch [181/1200], Training Loss: 129.15623469829103, Validation Loss: 164.945068359375\n",
      "Epoch [182/1200], Training Loss: 125.929083174215, Validation Loss: 161.7686309814453\n",
      "Epoch [183/1200], Training Loss: 122.78538272801089, Validation Loss: 158.72300720214844\n",
      "Epoch [184/1200], Training Loss: 119.71646049008474, Validation Loss: 155.71633911132812\n",
      "Epoch [185/1200], Training Loss: 116.7139389417485, Validation Loss: 152.7286834716797\n",
      "Epoch [186/1200], Training Loss: 113.78146915572972, Validation Loss: 149.8068084716797\n",
      "Epoch [187/1200], Training Loss: 110.927118683078, Validation Loss: 146.9975128173828\n",
      "Epoch [188/1200], Training Loss: 108.1541077485243, Validation Loss: 144.289306640625\n",
      "Epoch [189/1200], Training Loss: 105.45863927608124, Validation Loss: 141.638916015625\n",
      "Epoch [190/1200], Training Loss: 102.83729908734122, Validation Loss: 139.02235412597656\n",
      "Epoch [191/1200], Training Loss: 100.28432343652224, Validation Loss: 136.42225646972656\n",
      "Epoch [192/1200], Training Loss: 97.79447344932542, Validation Loss: 133.85247802734375\n",
      "Epoch [193/1200], Training Loss: 95.36536548557967, Validation Loss: 131.324951171875\n",
      "Epoch [194/1200], Training Loss: 92.99389899021249, Validation Loss: 128.8548126220703\n",
      "Epoch [195/1200], Training Loss: 90.67964650363372, Validation Loss: 126.47297668457031\n",
      "Epoch [196/1200], Training Loss: 88.4244698331545, Validation Loss: 124.14529418945312\n",
      "Epoch [197/1200], Training Loss: 86.23365939424303, Validation Loss: 121.93916320800781\n",
      "Epoch [198/1200], Training Loss: 84.11090358595297, Validation Loss: 119.79244995117188\n",
      "Epoch [199/1200], Training Loss: 82.06085685423369, Validation Loss: 117.88060760498047\n",
      "Epoch [200/1200], Training Loss: 80.0812086232979, Validation Loss: 116.04134368896484\n",
      "Epoch [201/1200], Training Loss: 78.17179472451586, Validation Loss: 114.326904296875\n",
      "Epoch [202/1200], Training Loss: 76.32502400786628, Validation Loss: 112.4580307006836\n",
      "Epoch [203/1200], Training Loss: 74.53852703996401, Validation Loss: 110.57722473144531\n",
      "Epoch [204/1200], Training Loss: 72.80644487139207, Validation Loss: 108.54651641845703\n",
      "Epoch [205/1200], Training Loss: 71.12755184581067, Validation Loss: 106.57563018798828\n",
      "Epoch [206/1200], Training Loss: 69.4975708885929, Validation Loss: 104.57012176513672\n",
      "Epoch [207/1200], Training Loss: 67.91496419362416, Validation Loss: 102.64811706542969\n",
      "Epoch [208/1200], Training Loss: 66.37819796708176, Validation Loss: 100.77654266357422\n",
      "Epoch [209/1200], Training Loss: 64.88622443938544, Validation Loss: 98.97315216064453\n",
      "Epoch [210/1200], Training Loss: 63.43586585885171, Validation Loss: 97.2420883178711\n",
      "Epoch [211/1200], Training Loss: 62.0273681293464, Validation Loss: 95.57353973388672\n",
      "Epoch [212/1200], Training Loss: 60.658584061252824, Validation Loss: 93.96736907958984\n",
      "Epoch [213/1200], Training Loss: 59.32852503458252, Validation Loss: 92.4203872680664\n",
      "Epoch [214/1200], Training Loss: 58.03612110299, Validation Loss: 90.9260025024414\n",
      "Epoch [215/1200], Training Loss: 56.780098441656584, Validation Loss: 89.48001098632812\n",
      "Epoch [216/1200], Training Loss: 55.55970253228153, Validation Loss: 88.07551574707031\n",
      "Epoch [217/1200], Training Loss: 54.37271608356185, Validation Loss: 86.70578002929688\n",
      "Epoch [218/1200], Training Loss: 53.21776586848001, Validation Loss: 85.3619613647461\n",
      "Epoch [219/1200], Training Loss: 52.093190866367344, Validation Loss: 84.03648376464844\n",
      "Epoch [220/1200], Training Loss: 50.99723883616473, Validation Loss: 82.72352600097656\n",
      "Epoch [221/1200], Training Loss: 49.92745501521328, Validation Loss: 81.4176025390625\n",
      "Epoch [222/1200], Training Loss: 48.88357956518853, Validation Loss: 80.11434173583984\n",
      "Epoch [223/1200], Training Loss: 47.86257622614368, Validation Loss: 78.81205749511719\n",
      "Epoch [224/1200], Training Loss: 46.86402287743444, Validation Loss: 77.50695037841797\n",
      "Epoch [225/1200], Training Loss: 45.88598080062994, Validation Loss: 76.19921875\n",
      "Epoch [226/1200], Training Loss: 44.92782297469653, Validation Loss: 74.88825988769531\n",
      "Epoch [227/1200], Training Loss: 43.98807496486926, Validation Loss: 73.57274627685547\n",
      "Epoch [228/1200], Training Loss: 43.06592700593017, Validation Loss: 72.25498962402344\n",
      "Epoch [229/1200], Training Loss: 42.160599573716596, Validation Loss: 70.93592834472656\n",
      "Epoch [230/1200], Training Loss: 41.273103565196436, Validation Loss: 69.61942291259766\n",
      "Epoch [231/1200], Training Loss: 40.40186577058386, Validation Loss: 68.30752563476562\n",
      "Epoch [232/1200], Training Loss: 39.547303810701315, Validation Loss: 67.00231170654297\n",
      "Epoch [233/1200], Training Loss: 38.7088985594241, Validation Loss: 65.70323944091797\n",
      "Epoch [234/1200], Training Loss: 37.88641323339775, Validation Loss: 64.40614318847656\n",
      "Epoch [235/1200], Training Loss: 37.07982091704765, Validation Loss: 63.108001708984375\n",
      "Epoch [236/1200], Training Loss: 36.28886973853273, Validation Loss: 61.80389404296875\n",
      "Epoch [237/1200], Training Loss: 35.51390220366573, Validation Loss: 60.4871826171875\n",
      "Epoch [238/1200], Training Loss: 34.75335481342164, Validation Loss: 59.15447998046875\n",
      "Epoch [239/1200], Training Loss: 34.00704200753292, Validation Loss: 57.803829193115234\n",
      "Epoch [240/1200], Training Loss: 33.273669762212776, Validation Loss: 56.43315887451172\n",
      "Epoch [241/1200], Training Loss: 32.55170806018157, Validation Loss: 55.04499816894531\n",
      "Epoch [242/1200], Training Loss: 31.842192017451893, Validation Loss: 53.64031219482422\n",
      "Epoch [243/1200], Training Loss: 31.144586577336135, Validation Loss: 52.22050857543945\n",
      "Epoch [244/1200], Training Loss: 30.45885148722186, Validation Loss: 50.78738021850586\n",
      "Epoch [245/1200], Training Loss: 29.78471795841867, Validation Loss: 49.344688415527344\n",
      "Epoch [246/1200], Training Loss: 29.122851001297597, Validation Loss: 47.896934509277344\n",
      "Epoch [247/1200], Training Loss: 28.473746879374886, Validation Loss: 46.45086669921875\n",
      "Epoch [248/1200], Training Loss: 27.83723459564692, Validation Loss: 45.016273498535156\n",
      "Epoch [249/1200], Training Loss: 27.213965182278873, Validation Loss: 43.6034049987793\n",
      "Epoch [250/1200], Training Loss: 26.604643011164864, Validation Loss: 42.22279357910156\n",
      "Epoch [251/1200], Training Loss: 26.010108471174103, Validation Loss: 40.883827209472656\n",
      "Epoch [252/1200], Training Loss: 25.43060649548423, Validation Loss: 39.593238830566406\n",
      "Epoch [253/1200], Training Loss: 24.8669323764373, Validation Loss: 38.35494613647461\n",
      "Epoch [254/1200], Training Loss: 24.319108562781558, Validation Loss: 37.17129135131836\n",
      "Epoch [255/1200], Training Loss: 23.787714103928227, Validation Loss: 36.04011535644531\n",
      "Epoch [256/1200], Training Loss: 23.272185867814297, Validation Loss: 34.952186584472656\n",
      "Epoch [257/1200], Training Loss: 22.772395174402348, Validation Loss: 33.9043083190918\n",
      "Epoch [258/1200], Training Loss: 22.28812005295757, Validation Loss: 32.895606994628906\n",
      "Epoch [259/1200], Training Loss: 21.819780148675882, Validation Loss: 31.924951553344727\n",
      "Epoch [260/1200], Training Loss: 21.366535169123225, Validation Loss: 30.990646362304688\n",
      "Epoch [261/1200], Training Loss: 20.92792897744137, Validation Loss: 30.091323852539062\n",
      "Epoch [262/1200], Training Loss: 20.50422417906175, Validation Loss: 29.22626495361328\n",
      "Epoch [263/1200], Training Loss: 20.094730325996558, Validation Loss: 28.39333724975586\n",
      "Epoch [264/1200], Training Loss: 19.698198505173718, Validation Loss: 27.592145919799805\n",
      "Epoch [265/1200], Training Loss: 19.313827334395793, Validation Loss: 26.821760177612305\n",
      "Epoch [266/1200], Training Loss: 18.941550830128847, Validation Loss: 26.08295249938965\n",
      "Epoch [267/1200], Training Loss: 18.580803160447992, Validation Loss: 25.374879837036133\n",
      "Epoch [268/1200], Training Loss: 18.23053325639192, Validation Loss: 24.697715759277344\n",
      "Epoch [269/1200], Training Loss: 17.89052838648016, Validation Loss: 24.05147933959961\n",
      "Epoch [270/1200], Training Loss: 17.56035103161898, Validation Loss: 23.435941696166992\n",
      "Epoch [271/1200], Training Loss: 17.23935874940955, Validation Loss: 22.85125732421875\n",
      "Epoch [272/1200], Training Loss: 16.927488307250623, Validation Loss: 22.29676055908203\n",
      "Epoch [273/1200], Training Loss: 16.624141030143527, Validation Loss: 21.771968841552734\n",
      "Epoch [274/1200], Training Loss: 16.32920590971695, Validation Loss: 21.27608871459961\n",
      "Epoch [275/1200], Training Loss: 16.042138824368433, Validation Loss: 20.807687759399414\n",
      "Epoch [276/1200], Training Loss: 15.76283660860275, Validation Loss: 20.365873336791992\n",
      "Epoch [277/1200], Training Loss: 15.490967237276214, Validation Loss: 19.948652267456055\n",
      "Epoch [278/1200], Training Loss: 15.226005907811635, Validation Loss: 19.55449867248535\n",
      "Epoch [279/1200], Training Loss: 14.968201954982902, Validation Loss: 19.182180404663086\n",
      "Epoch [280/1200], Training Loss: 14.717188152494806, Validation Loss: 18.83040428161621\n",
      "Epoch [281/1200], Training Loss: 14.472705059216938, Validation Loss: 18.49802589416504\n",
      "Epoch [282/1200], Training Loss: 14.234076513597604, Validation Loss: 18.183849334716797\n",
      "Epoch [283/1200], Training Loss: 14.001442967957178, Validation Loss: 17.88780403137207\n",
      "Epoch [284/1200], Training Loss: 13.77442512233743, Validation Loss: 17.608570098876953\n",
      "Epoch [285/1200], Training Loss: 13.552350697016362, Validation Loss: 17.343706130981445\n",
      "Epoch [286/1200], Training Loss: 13.33970147633509, Validation Loss: 17.10735511779785\n",
      "Epoch [287/1200], Training Loss: 13.116668086615151, Validation Loss: 16.923892974853516\n",
      "Epoch [288/1200], Training Loss: 12.956998826263783, Validation Loss: 16.942171096801758\n",
      "Epoch [289/1200], Training Loss: 12.698414710716069, Validation Loss: 16.713558197021484\n",
      "Epoch [290/1200], Training Loss: 12.58229754558708, Validation Loss: 17.17317771911621\n",
      "Epoch [291/1200], Training Loss: 12.343289938873896, Validation Loss: 15.856222152709961\n",
      "Epoch [292/1200], Training Loss: 12.107139574176378, Validation Loss: 15.610905647277832\n",
      "Epoch [293/1200], Training Loss: 11.948009321462822, Validation Loss: 15.658536911010742\n",
      "Epoch [294/1200], Training Loss: 11.820945425621238, Validation Loss: 16.20604705810547\n",
      "Epoch [295/1200], Training Loss: 11.597125277634863, Validation Loss: 14.950698852539062\n",
      "Epoch [296/1200], Training Loss: 11.388934756437143, Validation Loss: 15.053792953491211\n",
      "Epoch [297/1200], Training Loss: 11.288309968524171, Validation Loss: 15.563240051269531\n",
      "Epoch [298/1200], Training Loss: 11.124385678479443, Validation Loss: 14.493247985839844\n",
      "Epoch [299/1200], Training Loss: 10.88051946840034, Validation Loss: 14.601166725158691\n",
      "Epoch [300/1200], Training Loss: 10.791606550284206, Validation Loss: 14.898843765258789\n",
      "Epoch [301/1200], Training Loss: 10.617246531627805, Validation Loss: 13.795858383178711\n",
      "Epoch [302/1200], Training Loss: 10.401919617449257, Validation Loss: 14.223211288452148\n",
      "Epoch [303/1200], Training Loss: 10.310498992476164, Validation Loss: 14.291836738586426\n",
      "Epoch [304/1200], Training Loss: 10.16413957238492, Validation Loss: 13.372077941894531\n",
      "Epoch [305/1200], Training Loss: 9.95266427830231, Validation Loss: 13.925748825073242\n",
      "Epoch [306/1200], Training Loss: 9.851681634251879, Validation Loss: 13.66993236541748\n",
      "Epoch [307/1200], Training Loss: 9.728621513686896, Validation Loss: 13.020026206970215\n",
      "Epoch [308/1200], Training Loss: 9.525906706541601, Validation Loss: 13.663887977600098\n",
      "Epoch [309/1200], Training Loss: 9.418413748965113, Validation Loss: 13.187166213989258\n",
      "Epoch [310/1200], Training Loss: 9.30040494444652, Validation Loss: 12.79143238067627\n",
      "Epoch [311/1200], Training Loss: 9.10223638487903, Validation Loss: 13.33901309967041\n",
      "Epoch [312/1200], Training Loss: 9.02139600123175, Validation Loss: 12.892862319946289\n",
      "Epoch [313/1200], Training Loss: 8.864387894683215, Validation Loss: 12.68272876739502\n",
      "Epoch [314/1200], Training Loss: 8.716212254637771, Validation Loss: 12.294952392578125\n",
      "Epoch [315/1200], Training Loss: 8.63681583310984, Validation Loss: 13.092257499694824\n",
      "Epoch [316/1200], Training Loss: 8.50286404443172, Validation Loss: 12.377279281616211\n",
      "Epoch [317/1200], Training Loss: 8.383082649354622, Validation Loss: 12.539562225341797\n",
      "Epoch [318/1200], Training Loss: 8.254108781747714, Validation Loss: 11.948497772216797\n",
      "Epoch [319/1200], Training Loss: 8.156774933406911, Validation Loss: 12.549090385437012\n",
      "Epoch [320/1200], Training Loss: 8.042841090873148, Validation Loss: 12.111762046813965\n",
      "Epoch [321/1200], Training Loss: 7.944561435393115, Validation Loss: 12.250897407531738\n",
      "Epoch [322/1200], Training Loss: 7.825487585110515, Validation Loss: 11.63879680633545\n",
      "Epoch [323/1200], Training Loss: 7.744950718114308, Validation Loss: 12.115626335144043\n",
      "Epoch [324/1200], Training Loss: 7.630442487443347, Validation Loss: 11.858695983886719\n",
      "Epoch [325/1200], Training Loss: 7.561869852346088, Validation Loss: 12.032140731811523\n",
      "Epoch [326/1200], Training Loss: 7.444691332774982, Validation Loss: 11.428518295288086\n",
      "Epoch [327/1200], Training Loss: 7.380759712861636, Validation Loss: 11.734613418579102\n",
      "Epoch [328/1200], Training Loss: 7.257728920429483, Validation Loss: 11.763554573059082\n",
      "Epoch [329/1200], Training Loss: 7.204713633077894, Validation Loss: 11.978416442871094\n",
      "Epoch [330/1200], Training Loss: 7.098221195712172, Validation Loss: 11.295428276062012\n",
      "Epoch [331/1200], Training Loss: 7.034208780691593, Validation Loss: 11.374717712402344\n",
      "Epoch [332/1200], Training Loss: 6.905345813430531, Validation Loss: 11.809651374816895\n",
      "Epoch [333/1200], Training Loss: 6.842861960154372, Validation Loss: 12.09296989440918\n",
      "Epoch [334/1200], Training Loss: 6.779802082155977, Validation Loss: 11.196856498718262\n",
      "Epoch [335/1200], Training Loss: 6.6640964417097885, Validation Loss: 11.115979194641113\n",
      "Epoch [336/1200], Training Loss: 6.621191565721727, Validation Loss: 11.8908052444458\n",
      "Epoch [337/1200], Training Loss: 6.484219886857928, Validation Loss: 12.079291343688965\n",
      "Epoch [338/1200], Training Loss: 6.5315002238165425, Validation Loss: 11.374403953552246\n",
      "Epoch [339/1200], Training Loss: 6.3325595541777275, Validation Loss: 10.738349914550781\n",
      "Epoch [340/1200], Training Loss: 6.289693851141047, Validation Loss: 10.782028198242188\n",
      "Epoch [341/1200], Training Loss: 6.169600034636418, Validation Loss: 11.668811798095703\n",
      "Epoch [342/1200], Training Loss: 6.105763325976738, Validation Loss: 11.741305351257324\n",
      "Epoch [343/1200], Training Loss: 6.085519048905704, Validation Loss: 10.8887357711792\n",
      "Epoch [344/1200], Training Loss: 5.955133006778362, Validation Loss: 10.438470840454102\n",
      "Epoch [345/1200], Training Loss: 5.936716165285701, Validation Loss: 10.388981819152832\n",
      "Epoch [346/1200], Training Loss: 5.803036297552793, Validation Loss: 11.091622352600098\n",
      "Epoch [347/1200], Training Loss: 5.7361350635657855, Validation Loss: 11.602823257446289\n",
      "Epoch [348/1200], Training Loss: 5.807636834384327, Validation Loss: 11.053780555725098\n",
      "Epoch [349/1200], Training Loss: 5.61174036812707, Validation Loss: 9.685794830322266\n",
      "Epoch [350/1200], Training Loss: 5.6018468263859535, Validation Loss: 10.35020637512207\n",
      "Epoch [351/1200], Training Loss: 5.4823069695207645, Validation Loss: 11.126094818115234\n",
      "Epoch [352/1200], Training Loss: 5.406801576509105, Validation Loss: 11.13998794555664\n",
      "Epoch [353/1200], Training Loss: 5.445406254245046, Validation Loss: 10.605923652648926\n",
      "Epoch [354/1200], Training Loss: 5.29512197202561, Validation Loss: 9.789009094238281\n",
      "Epoch [355/1200], Training Loss: 5.258123618976602, Validation Loss: 9.86983585357666\n",
      "Epoch [356/1200], Training Loss: 5.152412073293777, Validation Loss: 10.182549476623535\n",
      "Epoch [357/1200], Training Loss: 5.090343709266202, Validation Loss: 10.377252578735352\n",
      "Epoch [358/1200], Training Loss: 5.0517466977182215, Validation Loss: 10.198360443115234\n",
      "Epoch [359/1200], Training Loss: 5.029911374365088, Validation Loss: 9.529708862304688\n",
      "Epoch [360/1200], Training Loss: 4.92056212034575, Validation Loss: 9.074267387390137\n",
      "Epoch [361/1200], Training Loss: 4.895969061151796, Validation Loss: 9.469476699829102\n",
      "Epoch [362/1200], Training Loss: 4.814769171965188, Validation Loss: 9.70540714263916\n",
      "Epoch [363/1200], Training Loss: 4.761390146553617, Validation Loss: 9.774291038513184\n",
      "Epoch [364/1200], Training Loss: 4.713087817172209, Validation Loss: 9.69314193725586\n",
      "Epoch [365/1200], Training Loss: 4.735731445920074, Validation Loss: 9.44264030456543\n",
      "Epoch [366/1200], Training Loss: 4.615901767238408, Validation Loss: 8.706660270690918\n",
      "Epoch [367/1200], Training Loss: 4.579729379568022, Validation Loss: 9.325403213500977\n",
      "Epoch [368/1200], Training Loss: 4.518399435367473, Validation Loss: 9.344971656799316\n",
      "Epoch [369/1200], Training Loss: 4.468527089984173, Validation Loss: 9.42920207977295\n",
      "Epoch [370/1200], Training Loss: 4.426464655053142, Validation Loss: 9.454940795898438\n",
      "Epoch [371/1200], Training Loss: 4.449365093272311, Validation Loss: 9.291863441467285\n",
      "Epoch [372/1200], Training Loss: 4.351470085382657, Validation Loss: 8.671302795410156\n",
      "Epoch [373/1200], Training Loss: 4.282679297623346, Validation Loss: 8.751551628112793\n",
      "Epoch [374/1200], Training Loss: 4.288636173679013, Validation Loss: 9.562897682189941\n",
      "Epoch [375/1200], Training Loss: 4.237823505750192, Validation Loss: 9.563252449035645\n",
      "Epoch [376/1200], Training Loss: 4.195609781071724, Validation Loss: 9.648130416870117\n",
      "Epoch [377/1200], Training Loss: 4.155616703512207, Validation Loss: 9.819111824035645\n",
      "Epoch [378/1200], Training Loss: 4.194011558956748, Validation Loss: 9.830469131469727\n",
      "Epoch [379/1200], Training Loss: 4.107337778394462, Validation Loss: 9.102239608764648\n",
      "Epoch [380/1200], Training Loss: 4.053691301211881, Validation Loss: 9.21870231628418\n",
      "Epoch [381/1200], Training Loss: 4.06533820996926, Validation Loss: 10.033418655395508\n",
      "Early stopping at epoch 381\n",
      "Final Test Loss: 14.415648460388184\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [8]\n",
    "num_layers_list = [2]\n",
    "learning_rates = [0.0002]\n",
    "window_sizes = [5]\n",
    "\n",
    "num_epochs = 1200\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "       # Calculate test loss after training is complete\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Final Test Loss: {test_loss / len(x_test_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mk\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.0002, window_size=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1200], Training Loss: 27117.52164265969, Validation Loss: 26584.203125\n",
      "Epoch [2/1200], Training Loss: 25198.483440674136, Validation Loss: 24856.16015625\n",
      "Epoch [3/1200], Training Loss: 23712.270647069785, Validation Loss: 23476.134765625\n",
      "Epoch [4/1200], Training Loss: 22558.14364037406, Validation Loss: 22402.552734375\n",
      "Epoch [5/1200], Training Loss: 21677.92769077198, Validation Loss: 21578.845703125\n",
      "Epoch [6/1200], Training Loss: 21008.06265529373, Validation Loss: 20946.91015625\n",
      "Epoch [7/1200], Training Loss: 20493.798455898712, Validation Loss: 20457.845703125\n",
      "Epoch [8/1200], Training Loss: 20093.617437314868, Validation Loss: 20074.599609375\n",
      "Epoch [9/1200], Training Loss: 18910.328940366795, Validation Loss: 18835.66796875\n",
      "Epoch [10/1200], Training Loss: 17919.38735957018, Validation Loss: 17976.1015625\n",
      "Epoch [11/1200], Training Loss: 17109.105262208832, Validation Loss: 17194.298828125\n",
      "Epoch [12/1200], Training Loss: 16353.566402327893, Validation Loss: 16522.1796875\n",
      "Epoch [13/1200], Training Loss: 15649.297335462406, Validation Loss: 15849.3544921875\n",
      "Epoch [14/1200], Training Loss: 14993.230572155886, Validation Loss: 15201.8671875\n",
      "Epoch [15/1200], Training Loss: 14381.770605229416, Validation Loss: 14592.107421875\n",
      "Epoch [16/1200], Training Loss: 13810.413175442063, Validation Loss: 14009.849609375\n",
      "Epoch [17/1200], Training Loss: 13274.09286530211, Validation Loss: 13387.37109375\n",
      "Epoch [18/1200], Training Loss: 12769.977913032517, Validation Loss: 12799.4091796875\n",
      "Epoch [19/1200], Training Loss: 12296.591823969944, Validation Loss: 12302.5625\n",
      "Epoch [20/1200], Training Loss: 11851.533101759293, Validation Loss: 11855.8486328125\n",
      "Epoch [21/1200], Training Loss: 11432.873478328249, Validation Loss: 11442.048828125\n",
      "Epoch [22/1200], Training Loss: 11038.870606137705, Validation Loss: 11057.138671875\n",
      "Epoch [23/1200], Training Loss: 10668.305298597445, Validation Loss: 10698.5283203125\n",
      "Epoch [24/1200], Training Loss: 10320.433373264874, Validation Loss: 10368.7197265625\n",
      "Epoch [25/1200], Training Loss: 9994.59586807412, Validation Loss: 10065.20703125\n",
      "Epoch [26/1200], Training Loss: 9689.866283120296, Validation Loss: 9780.2255859375\n",
      "Epoch [27/1200], Training Loss: 9405.070876438196, Validation Loss: 9510.8701171875\n",
      "Epoch [28/1200], Training Loss: 9138.81018253763, Validation Loss: 9245.982421875\n",
      "Epoch [29/1200], Training Loss: 8889.070764067023, Validation Loss: 9004.4541015625\n",
      "Epoch [30/1200], Training Loss: 8654.096271523864, Validation Loss: 8789.7822265625\n",
      "Epoch [31/1200], Training Loss: 8331.80984750818, Validation Loss: 9405.28515625\n",
      "Epoch [32/1200], Training Loss: 8077.116016664745, Validation Loss: 8841.916015625\n",
      "Epoch [33/1200], Training Loss: 7821.653260999524, Validation Loss: 8539.3798828125\n",
      "Epoch [34/1200], Training Loss: 7582.1756333536005, Validation Loss: 8157.0009765625\n",
      "Epoch [35/1200], Training Loss: 7351.941305288972, Validation Loss: 7819.2861328125\n",
      "Epoch [36/1200], Training Loss: 7129.9071625250035, Validation Loss: 7448.55615234375\n",
      "Epoch [37/1200], Training Loss: 6915.469110423887, Validation Loss: 7135.69580078125\n",
      "Epoch [38/1200], Training Loss: 6708.250944355424, Validation Loss: 6874.44580078125\n",
      "Epoch [39/1200], Training Loss: 6507.164163996543, Validation Loss: 6641.60009765625\n",
      "Epoch [40/1200], Training Loss: 6311.7200997486925, Validation Loss: 6427.73583984375\n",
      "Epoch [41/1200], Training Loss: 6121.80532110399, Validation Loss: 6229.0546875\n",
      "Epoch [42/1200], Training Loss: 5937.570609104961, Validation Loss: 6042.68408203125\n",
      "Epoch [43/1200], Training Loss: 5759.129074085519, Validation Loss: 5865.42431640625\n",
      "Epoch [44/1200], Training Loss: 5586.582481130876, Validation Loss: 5694.92431640625\n",
      "Epoch [45/1200], Training Loss: 5419.973592006787, Validation Loss: 5530.6142578125\n",
      "Epoch [46/1200], Training Loss: 5259.324875966277, Validation Loss: 5372.58740234375\n",
      "Epoch [47/1200], Training Loss: 5104.600923127956, Validation Loss: 5220.1005859375\n",
      "Epoch [48/1200], Training Loss: 4955.33916907549, Validation Loss: 5072.0546875\n",
      "Epoch [49/1200], Training Loss: 4810.5374880371355, Validation Loss: 4927.3349609375\n",
      "Epoch [50/1200], Training Loss: 4669.625882760164, Validation Loss: 4786.30078125\n",
      "Epoch [51/1200], Training Loss: 4532.147954929392, Validation Loss: 4649.50439453125\n",
      "Epoch [52/1200], Training Loss: 4397.819147338403, Validation Loss: 4515.41455078125\n",
      "Epoch [53/1200], Training Loss: 4266.803083649602, Validation Loss: 4383.94482421875\n",
      "Epoch [54/1200], Training Loss: 4139.024106517751, Validation Loss: 4255.89501953125\n",
      "Epoch [55/1200], Training Loss: 4014.3625445893213, Validation Loss: 4130.1552734375\n",
      "Epoch [56/1200], Training Loss: 3892.7815405422334, Validation Loss: 4007.43017578125\n",
      "Epoch [57/1200], Training Loss: 3774.2214928634608, Validation Loss: 3888.892578125\n",
      "Epoch [58/1200], Training Loss: 3658.5466773226954, Validation Loss: 3774.58544921875\n",
      "Epoch [59/1200], Training Loss: 3545.7439905566125, Validation Loss: 3663.0927734375\n",
      "Epoch [60/1200], Training Loss: 3436.4287062781036, Validation Loss: 3553.519775390625\n",
      "Epoch [61/1200], Training Loss: 3330.500341423178, Validation Loss: 3446.380615234375\n",
      "Epoch [62/1200], Training Loss: 3227.819145683774, Validation Loss: 3342.568115234375\n",
      "Epoch [63/1200], Training Loss: 3128.0069570430774, Validation Loss: 3242.11474609375\n",
      "Epoch [64/1200], Training Loss: 3031.368969348356, Validation Loss: 3142.466064453125\n",
      "Epoch [65/1200], Training Loss: 2938.3292280323317, Validation Loss: 3048.02001953125\n",
      "Epoch [66/1200], Training Loss: 2848.630301469439, Validation Loss: 2956.725830078125\n",
      "Epoch [67/1200], Training Loss: 2762.1182335246426, Validation Loss: 2869.312255859375\n",
      "Epoch [68/1200], Training Loss: 2678.8231608018004, Validation Loss: 2785.194091796875\n",
      "Epoch [69/1200], Training Loss: 2598.810610316158, Validation Loss: 2703.723876953125\n",
      "Epoch [70/1200], Training Loss: 2522.1565510315704, Validation Loss: 2624.74951171875\n",
      "Epoch [71/1200], Training Loss: 2448.899588537004, Validation Loss: 2548.5634765625\n",
      "Epoch [72/1200], Training Loss: 2378.866700646108, Validation Loss: 2475.55126953125\n",
      "Epoch [73/1200], Training Loss: 2311.529201004713, Validation Loss: 2405.265380859375\n",
      "Epoch [74/1200], Training Loss: 2246.3039651017575, Validation Loss: 2336.499267578125\n",
      "Epoch [75/1200], Training Loss: 2182.9927882839406, Validation Loss: 2270.32763671875\n",
      "Epoch [76/1200], Training Loss: 2121.5504691329656, Validation Loss: 2206.628662109375\n",
      "Epoch [77/1200], Training Loss: 2061.90366373595, Validation Loss: 2144.837646484375\n",
      "Epoch [78/1200], Training Loss: 2003.9489240132443, Validation Loss: 2084.396240234375\n",
      "Epoch [79/1200], Training Loss: 1947.6529124845113, Validation Loss: 2025.70751953125\n",
      "Epoch [80/1200], Training Loss: 1893.0441334354407, Validation Loss: 1969.676025390625\n",
      "Epoch [81/1200], Training Loss: 1840.1203077735631, Validation Loss: 1915.7635498046875\n",
      "Epoch [82/1200], Training Loss: 1788.855241255801, Validation Loss: 1863.6051025390625\n",
      "Epoch [83/1200], Training Loss: 1739.2166791240818, Validation Loss: 1813.2216796875\n",
      "Epoch [84/1200], Training Loss: 1691.1859083552524, Validation Loss: 1764.662353515625\n",
      "Epoch [85/1200], Training Loss: 1644.7462567884502, Validation Loss: 1717.8388671875\n",
      "Epoch [86/1200], Training Loss: 1599.8893303802931, Validation Loss: 1672.5980224609375\n",
      "Epoch [87/1200], Training Loss: 1556.6346280690395, Validation Loss: 1628.803466796875\n",
      "Epoch [88/1200], Training Loss: 1514.9679943791457, Validation Loss: 1586.5400390625\n",
      "Epoch [89/1200], Training Loss: 1474.8453909720552, Validation Loss: 1545.849609375\n",
      "Epoch [90/1200], Training Loss: 1436.209281067145, Validation Loss: 1506.7310791015625\n",
      "Epoch [91/1200], Training Loss: 1398.9907476941673, Validation Loss: 1469.1561279296875\n",
      "Epoch [92/1200], Training Loss: 1363.1061705070626, Validation Loss: 1433.0445556640625\n",
      "Epoch [93/1200], Training Loss: 1328.4870756538962, Validation Loss: 1398.2252197265625\n",
      "Epoch [94/1200], Training Loss: 1295.0404944442805, Validation Loss: 1364.5106201171875\n",
      "Epoch [95/1200], Training Loss: 1262.688939299451, Validation Loss: 1331.7615966796875\n",
      "Epoch [96/1200], Training Loss: 1231.3443200898407, Validation Loss: 1299.91650390625\n",
      "Epoch [97/1200], Training Loss: 1200.9384674142138, Validation Loss: 1268.9366455078125\n",
      "Epoch [98/1200], Training Loss: 1171.4017791190606, Validation Loss: 1238.78173828125\n",
      "Epoch [99/1200], Training Loss: 1142.6709106484423, Validation Loss: 1209.417724609375\n",
      "Epoch [100/1200], Training Loss: 1114.6992470285522, Validation Loss: 1180.807861328125\n",
      "Epoch [101/1200], Training Loss: 1087.437971127306, Validation Loss: 1152.9185791015625\n",
      "Epoch [102/1200], Training Loss: 1060.8553245515538, Validation Loss: 1125.722412109375\n",
      "Epoch [103/1200], Training Loss: 1034.9273370044034, Validation Loss: 1099.2135009765625\n",
      "Epoch [104/1200], Training Loss: 1009.638804921469, Validation Loss: 1073.3802490234375\n",
      "Epoch [105/1200], Training Loss: 984.967775343406, Validation Loss: 1048.210205078125\n",
      "Epoch [106/1200], Training Loss: 960.8968670173541, Validation Loss: 1023.6641235351562\n",
      "Epoch [107/1200], Training Loss: 937.4148677465191, Validation Loss: 999.6417846679688\n",
      "Epoch [108/1200], Training Loss: 914.5021705332755, Validation Loss: 976.0541381835938\n",
      "Epoch [109/1200], Training Loss: 892.1425586894975, Validation Loss: 952.8417358398438\n",
      "Epoch [110/1200], Training Loss: 870.3208176287901, Validation Loss: 929.9971313476562\n",
      "Epoch [111/1200], Training Loss: 849.0114089993658, Validation Loss: 907.5565795898438\n",
      "Epoch [112/1200], Training Loss: 828.2031318913358, Validation Loss: 885.5703125\n",
      "Epoch [113/1200], Training Loss: 807.8753479772506, Validation Loss: 864.0848999023438\n",
      "Epoch [114/1200], Training Loss: 788.014767163161, Validation Loss: 843.1246337890625\n",
      "Epoch [115/1200], Training Loss: 768.597911707456, Validation Loss: 822.701904296875\n",
      "Epoch [116/1200], Training Loss: 749.6134104511943, Validation Loss: 802.8090209960938\n",
      "Epoch [117/1200], Training Loss: 731.0456012250178, Validation Loss: 783.424560546875\n",
      "Epoch [118/1200], Training Loss: 712.8803865549543, Validation Loss: 764.5338745117188\n",
      "Epoch [119/1200], Training Loss: 695.1127785829764, Validation Loss: 746.1224975585938\n",
      "Epoch [120/1200], Training Loss: 677.7314113145671, Validation Loss: 728.1577758789062\n",
      "Epoch [121/1200], Training Loss: 660.7252755416796, Validation Loss: 710.59375\n",
      "Epoch [122/1200], Training Loss: 644.0871103307418, Validation Loss: 693.3935546875\n",
      "Epoch [123/1200], Training Loss: 627.8079435553575, Validation Loss: 676.5287475585938\n",
      "Epoch [124/1200], Training Loss: 611.8826508875981, Validation Loss: 659.981201171875\n",
      "Epoch [125/1200], Training Loss: 596.3002755653712, Validation Loss: 643.7412109375\n",
      "Epoch [126/1200], Training Loss: 581.0531818304527, Validation Loss: 627.811767578125\n",
      "Epoch [127/1200], Training Loss: 566.1412921232384, Validation Loss: 612.1890869140625\n",
      "Epoch [128/1200], Training Loss: 551.5601133640099, Validation Loss: 596.8502807617188\n",
      "Epoch [129/1200], Training Loss: 537.2994256974284, Validation Loss: 581.7745361328125\n",
      "Epoch [130/1200], Training Loss: 523.3552536021595, Validation Loss: 566.952880859375\n",
      "Epoch [131/1200], Training Loss: 509.72377449841827, Validation Loss: 552.4025268554688\n",
      "Epoch [132/1200], Training Loss: 496.4033820536704, Validation Loss: 538.149169921875\n",
      "Epoch [133/1200], Training Loss: 483.38105876737484, Validation Loss: 524.2213134765625\n",
      "Epoch [134/1200], Training Loss: 470.6535459054652, Validation Loss: 510.628173828125\n",
      "Epoch [135/1200], Training Loss: 458.213976051183, Validation Loss: 497.3670654296875\n",
      "Epoch [136/1200], Training Loss: 446.0540109710575, Validation Loss: 484.4249267578125\n",
      "Epoch [137/1200], Training Loss: 434.1639542718657, Validation Loss: 471.7977600097656\n",
      "Epoch [138/1200], Training Loss: 422.54223393510125, Validation Loss: 459.4665222167969\n",
      "Epoch [139/1200], Training Loss: 411.1809028207824, Validation Loss: 447.40557861328125\n",
      "Epoch [140/1200], Training Loss: 400.07519819840496, Validation Loss: 435.6019287109375\n",
      "Epoch [141/1200], Training Loss: 389.2202267967985, Validation Loss: 424.0513000488281\n",
      "Epoch [142/1200], Training Loss: 378.615517182213, Validation Loss: 412.762451171875\n",
      "Epoch [143/1200], Training Loss: 368.2580133498242, Validation Loss: 401.74346923828125\n",
      "Epoch [144/1200], Training Loss: 358.1479175535068, Validation Loss: 391.010498046875\n",
      "Epoch [145/1200], Training Loss: 348.28605451521383, Validation Loss: 380.5747375488281\n",
      "Epoch [146/1200], Training Loss: 338.67292280942377, Validation Loss: 370.4394226074219\n",
      "Epoch [147/1200], Training Loss: 329.3100257992729, Validation Loss: 360.59991455078125\n",
      "Epoch [148/1200], Training Loss: 320.1962882277661, Validation Loss: 351.0393371582031\n",
      "Epoch [149/1200], Training Loss: 311.3291450203557, Validation Loss: 341.7442321777344\n",
      "Epoch [150/1200], Training Loss: 302.7102109805718, Validation Loss: 332.6994323730469\n",
      "Epoch [151/1200], Training Loss: 294.3389826820772, Validation Loss: 323.8936767578125\n",
      "Epoch [152/1200], Training Loss: 286.20974683629316, Validation Loss: 315.33148193359375\n",
      "Epoch [153/1200], Training Loss: 278.3220669928073, Validation Loss: 307.02978515625\n",
      "Epoch [154/1200], Training Loss: 270.67645621162154, Validation Loss: 299.00616455078125\n",
      "Epoch [155/1200], Training Loss: 263.2679145319541, Validation Loss: 291.2696533203125\n",
      "Epoch [156/1200], Training Loss: 256.08967821866815, Validation Loss: 283.81072998046875\n",
      "Epoch [157/1200], Training Loss: 249.13580942502486, Validation Loss: 276.60992431640625\n",
      "Epoch [158/1200], Training Loss: 242.3975475822263, Validation Loss: 269.627685546875\n",
      "Epoch [159/1200], Training Loss: 235.865348922743, Validation Loss: 262.8503112792969\n",
      "Epoch [160/1200], Training Loss: 229.52980130676895, Validation Loss: 256.33380126953125\n",
      "Epoch [161/1200], Training Loss: 223.3837185409339, Validation Loss: 250.1533203125\n",
      "Epoch [162/1200], Training Loss: 217.42254639787635, Validation Loss: 244.30917358398438\n",
      "Epoch [163/1200], Training Loss: 211.64325929183804, Validation Loss: 238.68170166015625\n",
      "Epoch [164/1200], Training Loss: 206.04155140152693, Validation Loss: 233.11654663085938\n",
      "Epoch [165/1200], Training Loss: 200.60952950955632, Validation Loss: 227.5567626953125\n",
      "Epoch [166/1200], Training Loss: 195.33834680102726, Validation Loss: 221.99380493164062\n",
      "Epoch [167/1200], Training Loss: 190.21746168757272, Validation Loss: 216.44334411621094\n",
      "Epoch [168/1200], Training Loss: 185.23914986510593, Validation Loss: 210.93161010742188\n",
      "Epoch [169/1200], Training Loss: 180.39587924074021, Validation Loss: 205.50282287597656\n",
      "Epoch [170/1200], Training Loss: 175.68133871358177, Validation Loss: 200.1934356689453\n",
      "Epoch [171/1200], Training Loss: 171.0871806276319, Validation Loss: 195.03594970703125\n",
      "Epoch [172/1200], Training Loss: 166.61014804910928, Validation Loss: 190.05166625976562\n",
      "Epoch [173/1200], Training Loss: 162.2448227299218, Validation Loss: 185.24331665039062\n",
      "Epoch [174/1200], Training Loss: 157.98784060704094, Validation Loss: 180.59820556640625\n",
      "Epoch [175/1200], Training Loss: 153.83683608602175, Validation Loss: 176.09629821777344\n",
      "Epoch [176/1200], Training Loss: 149.78864123726416, Validation Loss: 171.721923828125\n",
      "Epoch [177/1200], Training Loss: 145.8419477264953, Validation Loss: 167.46681213378906\n",
      "Epoch [178/1200], Training Loss: 141.9937664556125, Validation Loss: 163.3260955810547\n",
      "Epoch [179/1200], Training Loss: 138.23955487596814, Validation Loss: 159.3025360107422\n",
      "Epoch [180/1200], Training Loss: 134.5778534827252, Validation Loss: 155.397216796875\n",
      "Epoch [181/1200], Training Loss: 131.00726646258454, Validation Loss: 151.60604858398438\n",
      "Epoch [182/1200], Training Loss: 127.52654965225253, Validation Loss: 147.92105102539062\n",
      "Epoch [183/1200], Training Loss: 124.13470593078479, Validation Loss: 144.33529663085938\n",
      "Epoch [184/1200], Training Loss: 120.8304581891859, Validation Loss: 140.8428955078125\n",
      "Epoch [185/1200], Training Loss: 117.61345802661805, Validation Loss: 137.4406280517578\n",
      "Epoch [186/1200], Training Loss: 114.48159981477728, Validation Loss: 134.12884521484375\n",
      "Epoch [187/1200], Training Loss: 111.43348253548177, Validation Loss: 130.90969848632812\n",
      "Epoch [188/1200], Training Loss: 108.46873336899397, Validation Loss: 127.78397369384766\n",
      "Epoch [189/1200], Training Loss: 105.5858562931531, Validation Loss: 124.75179290771484\n",
      "Epoch [190/1200], Training Loss: 102.78301300293103, Validation Loss: 121.81364440917969\n",
      "Epoch [191/1200], Training Loss: 100.05742703752894, Validation Loss: 118.96737670898438\n",
      "Epoch [192/1200], Training Loss: 97.40639732262204, Validation Loss: 116.20940399169922\n",
      "Epoch [193/1200], Training Loss: 94.82711351852574, Validation Loss: 113.53715515136719\n",
      "Epoch [194/1200], Training Loss: 92.31667870575427, Validation Loss: 110.94556427001953\n",
      "Epoch [195/1200], Training Loss: 89.87311150177806, Validation Loss: 108.43009948730469\n",
      "Epoch [196/1200], Training Loss: 87.49424319351077, Validation Loss: 105.98599243164062\n",
      "Epoch [197/1200], Training Loss: 85.17791509775923, Validation Loss: 103.60980987548828\n",
      "Epoch [198/1200], Training Loss: 82.92206564672591, Validation Loss: 101.29503631591797\n",
      "Epoch [199/1200], Training Loss: 80.72443973450434, Validation Loss: 99.0378189086914\n",
      "Epoch [200/1200], Training Loss: 78.5840506194608, Validation Loss: 96.83193969726562\n",
      "Epoch [201/1200], Training Loss: 76.499292455793, Validation Loss: 94.67245483398438\n",
      "Epoch [202/1200], Training Loss: 74.46974741687221, Validation Loss: 92.55187225341797\n",
      "Epoch [203/1200], Training Loss: 72.49394291684887, Validation Loss: 90.46681213378906\n",
      "Epoch [204/1200], Training Loss: 70.57100406875202, Validation Loss: 88.41340637207031\n",
      "Epoch [205/1200], Training Loss: 68.700794714956, Validation Loss: 86.38936614990234\n",
      "Epoch [206/1200], Training Loss: 66.8822084419215, Validation Loss: 84.3948974609375\n",
      "Epoch [207/1200], Training Loss: 65.11477156278575, Validation Loss: 82.42996215820312\n",
      "Epoch [208/1200], Training Loss: 63.39757450645712, Validation Loss: 80.49662017822266\n",
      "Epoch [209/1200], Training Loss: 61.72929754093264, Validation Loss: 78.5948715209961\n",
      "Epoch [210/1200], Training Loss: 60.10953957173246, Validation Loss: 76.72667694091797\n",
      "Epoch [211/1200], Training Loss: 58.537012640019555, Validation Loss: 74.89347076416016\n",
      "Epoch [212/1200], Training Loss: 57.010555320913326, Validation Loss: 73.09730529785156\n",
      "Epoch [213/1200], Training Loss: 55.529156925497034, Validation Loss: 71.33897399902344\n",
      "Epoch [214/1200], Training Loss: 54.092007111754356, Validation Loss: 69.61985778808594\n",
      "Epoch [215/1200], Training Loss: 52.69803439951391, Validation Loss: 67.94214630126953\n",
      "Epoch [216/1200], Training Loss: 51.3468340648866, Validation Loss: 66.30598449707031\n",
      "Epoch [217/1200], Training Loss: 50.03659729808689, Validation Loss: 64.71185302734375\n",
      "Epoch [218/1200], Training Loss: 48.76616603053869, Validation Loss: 63.161109924316406\n",
      "Epoch [219/1200], Training Loss: 47.53447207903926, Validation Loss: 61.65327835083008\n",
      "Epoch [220/1200], Training Loss: 46.34042660886437, Validation Loss: 60.18815994262695\n",
      "Epoch [221/1200], Training Loss: 45.18278743807379, Validation Loss: 58.765968322753906\n",
      "Epoch [222/1200], Training Loss: 44.06117211881474, Validation Loss: 57.384864807128906\n",
      "Epoch [223/1200], Training Loss: 42.97396129423007, Validation Loss: 56.04217529296875\n",
      "Epoch [224/1200], Training Loss: 41.919379391259625, Validation Loss: 54.73685836791992\n",
      "Epoch [225/1200], Training Loss: 40.89699845680273, Validation Loss: 53.46674728393555\n",
      "Epoch [226/1200], Training Loss: 39.90609096847427, Validation Loss: 52.23067092895508\n",
      "Epoch [227/1200], Training Loss: 38.946201124035426, Validation Loss: 51.026981353759766\n",
      "Epoch [228/1200], Training Loss: 38.01522856272372, Validation Loss: 49.854270935058594\n",
      "Epoch [229/1200], Training Loss: 37.11252482091747, Validation Loss: 48.7135009765625\n",
      "Epoch [230/1200], Training Loss: 36.23665822028241, Validation Loss: 47.603092193603516\n",
      "Epoch [231/1200], Training Loss: 35.38622199520492, Validation Loss: 46.5258674621582\n",
      "Epoch [232/1200], Training Loss: 34.55898520360171, Validation Loss: 45.48119354248047\n",
      "Epoch [233/1200], Training Loss: 33.75370546028526, Validation Loss: 44.47183609008789\n",
      "Epoch [234/1200], Training Loss: 32.969284919791114, Validation Loss: 43.49843978881836\n",
      "Epoch [235/1200], Training Loss: 32.20413044035035, Validation Loss: 42.56280517578125\n",
      "Epoch [236/1200], Training Loss: 31.457848909647513, Validation Loss: 41.663936614990234\n",
      "Epoch [237/1200], Training Loss: 30.73006799121004, Validation Loss: 40.79889678955078\n",
      "Epoch [238/1200], Training Loss: 30.0198530454807, Validation Loss: 39.96585464477539\n",
      "Epoch [239/1200], Training Loss: 29.327158547882064, Validation Loss: 39.159732818603516\n",
      "Epoch [240/1200], Training Loss: 28.651402591038817, Validation Loss: 38.37799072265625\n",
      "Epoch [241/1200], Training Loss: 27.992572542825883, Validation Loss: 37.61687088012695\n",
      "Epoch [242/1200], Training Loss: 27.3503304706726, Validation Loss: 36.874691009521484\n",
      "Epoch [243/1200], Training Loss: 26.724150552888464, Validation Loss: 36.1480712890625\n",
      "Epoch [244/1200], Training Loss: 26.114005712096834, Validation Loss: 35.43399429321289\n",
      "Epoch [245/1200], Training Loss: 25.519307766542706, Validation Loss: 34.72709274291992\n",
      "Epoch [246/1200], Training Loss: 24.939359218317108, Validation Loss: 34.02326965332031\n",
      "Epoch [247/1200], Training Loss: 24.37378930369736, Validation Loss: 33.320594787597656\n",
      "Epoch [248/1200], Training Loss: 23.82209560879653, Validation Loss: 32.61857223510742\n",
      "Epoch [249/1200], Training Loss: 23.284710253844878, Validation Loss: 31.921566009521484\n",
      "Epoch [250/1200], Training Loss: 22.76191076884869, Validation Loss: 31.235166549682617\n",
      "Epoch [251/1200], Training Loss: 22.254175360692223, Validation Loss: 30.56353759765625\n",
      "Epoch [252/1200], Training Loss: 21.76150385541541, Validation Loss: 29.911361694335938\n",
      "Epoch [253/1200], Training Loss: 21.284089129083572, Validation Loss: 29.27984619140625\n",
      "Epoch [254/1200], Training Loss: 20.822425647323573, Validation Loss: 28.66819953918457\n",
      "Epoch [255/1200], Training Loss: 20.376097937167227, Validation Loss: 28.07448959350586\n",
      "Epoch [256/1200], Training Loss: 19.944467110123963, Validation Loss: 27.49496841430664\n",
      "Epoch [257/1200], Training Loss: 19.52664139345615, Validation Loss: 26.92823028564453\n",
      "Epoch [258/1200], Training Loss: 19.122502672528736, Validation Loss: 26.374034881591797\n",
      "Epoch [259/1200], Training Loss: 18.731554184720707, Validation Loss: 25.83293914794922\n",
      "Epoch [260/1200], Training Loss: 18.353529692532028, Validation Loss: 25.30629539489746\n",
      "Epoch [261/1200], Training Loss: 17.98749826170137, Validation Loss: 24.795188903808594\n",
      "Epoch [262/1200], Training Loss: 17.63280335096366, Validation Loss: 24.300670623779297\n",
      "Epoch [263/1200], Training Loss: 17.288730377938418, Validation Loss: 23.824050903320312\n",
      "Epoch [264/1200], Training Loss: 16.954760550223117, Validation Loss: 23.365875244140625\n",
      "Epoch [265/1200], Training Loss: 16.63026924883672, Validation Loss: 22.928255081176758\n",
      "Epoch [266/1200], Training Loss: 16.31469384473332, Validation Loss: 22.50956153869629\n",
      "Epoch [267/1200], Training Loss: 16.007722893923212, Validation Loss: 22.106969833374023\n",
      "Epoch [268/1200], Training Loss: 15.708970500725654, Validation Loss: 21.717180252075195\n",
      "Epoch [269/1200], Training Loss: 15.418174443298444, Validation Loss: 21.340469360351562\n",
      "Epoch [270/1200], Training Loss: 15.134928861510375, Validation Loss: 20.97467041015625\n",
      "Epoch [271/1200], Training Loss: 14.858827403376008, Validation Loss: 20.62779426574707\n",
      "Epoch [272/1200], Training Loss: 14.591644798808668, Validation Loss: 20.291515350341797\n",
      "Epoch [273/1200], Training Loss: 14.328448016702287, Validation Loss: 19.954647064208984\n",
      "Epoch [274/1200], Training Loss: 14.071330678865502, Validation Loss: 19.711950302124023\n",
      "Epoch [275/1200], Training Loss: 13.822823887961322, Validation Loss: 19.32344627380371\n",
      "Epoch [276/1200], Training Loss: 13.57721334691307, Validation Loss: 19.091413497924805\n",
      "Epoch [277/1200], Training Loss: 13.340309705447886, Validation Loss: 18.7347469329834\n",
      "Epoch [278/1200], Training Loss: 13.105534657175246, Validation Loss: 18.51080322265625\n",
      "Epoch [279/1200], Training Loss: 12.879175364581313, Validation Loss: 18.180898666381836\n",
      "Epoch [280/1200], Training Loss: 12.655060190572467, Validation Loss: 17.964630126953125\n",
      "Epoch [281/1200], Training Loss: 12.438872445944535, Validation Loss: 17.65656852722168\n",
      "Epoch [282/1200], Training Loss: 12.22462161220461, Validation Loss: 17.448110580444336\n",
      "Epoch [283/1200], Training Loss: 12.018197628825565, Validation Loss: 17.1580867767334\n",
      "Epoch [284/1200], Training Loss: 11.81322885929372, Validation Loss: 16.95781707763672\n",
      "Epoch [285/1200], Training Loss: 11.61621247406009, Validation Loss: 16.681556701660156\n",
      "Epoch [286/1200], Training Loss: 11.419938722026387, Validation Loss: 16.48977279663086\n",
      "Epoch [287/1200], Training Loss: 11.23167436095353, Validation Loss: 16.223453521728516\n",
      "Epoch [288/1200], Training Loss: 11.043924709767754, Validation Loss: 16.03950309753418\n",
      "Epoch [289/1200], Training Loss: 10.86435674954725, Validation Loss: 15.778623580932617\n",
      "Epoch [290/1200], Training Loss: 10.684675658409477, Validation Loss: 15.601579666137695\n",
      "Epoch [291/1200], Training Loss: 10.512589701069732, Validation Loss: 15.342254638671875\n",
      "Epoch [292/1200], Training Loss: 10.340211611332306, Validation Loss: 15.171382904052734\n",
      "Epoch [293/1200], Training Loss: 10.174901793353051, Validation Loss: 14.910416603088379\n",
      "Epoch [294/1200], Training Loss: 10.00955363255263, Validation Loss: 14.746394157409668\n",
      "Epoch [295/1200], Training Loss: 9.850578645943566, Validation Loss: 14.482040405273438\n",
      "Epoch [296/1200], Training Loss: 9.691756329132954, Validation Loss: 14.32572078704834\n",
      "Epoch [297/1200], Training Loss: 9.538362418396174, Validation Loss: 14.057297706604004\n",
      "Epoch [298/1200], Training Loss: 9.385808037931776, Validation Loss: 13.909111976623535\n",
      "Epoch [299/1200], Training Loss: 9.23786456883686, Validation Loss: 13.637826919555664\n",
      "Epoch [300/1200], Training Loss: 9.091052351442823, Validation Loss: 13.498226165771484\n",
      "Epoch [301/1200], Training Loss: 8.948060270980207, Validation Loss: 13.225420951843262\n",
      "Epoch [302/1200], Training Loss: 8.806727543164754, Validation Loss: 13.094361305236816\n",
      "Epoch [303/1200], Training Loss: 8.668327954748268, Validation Loss: 12.821537017822266\n",
      "Epoch [304/1200], Training Loss: 8.53220246285036, Validation Loss: 12.697796821594238\n",
      "Epoch [305/1200], Training Loss: 8.398337389937856, Validation Loss: 12.425385475158691\n",
      "Epoch [306/1200], Training Loss: 8.267159380038704, Validation Loss: 12.303095817565918\n",
      "Epoch [307/1200], Training Loss: 8.138181523768608, Validation Loss: 12.028641700744629\n",
      "Epoch [308/1200], Training Loss: 8.01166740626454, Validation Loss: 11.89147663116455\n",
      "Epoch [309/1200], Training Loss: 7.887245080938059, Validation Loss: 11.605643272399902\n",
      "Epoch [310/1200], Training Loss: 7.764285434428375, Validation Loss: 11.428884506225586\n",
      "Epoch [311/1200], Training Loss: 7.642693107123943, Validation Loss: 11.182888984680176\n",
      "Epoch [312/1200], Training Loss: 7.523213047871558, Validation Loss: 11.022711753845215\n",
      "Epoch [313/1200], Training Loss: 7.404982794611265, Validation Loss: 10.879237174987793\n",
      "Epoch [314/1200], Training Loss: 7.288310137045484, Validation Loss: 10.743856430053711\n",
      "Epoch [315/1200], Training Loss: 7.173623635352298, Validation Loss: 10.62148380279541\n",
      "Epoch [316/1200], Training Loss: 7.0611997173048175, Validation Loss: 10.51136302947998\n",
      "Epoch [317/1200], Training Loss: 6.951167099571504, Validation Loss: 10.411195755004883\n",
      "Epoch [318/1200], Training Loss: 6.843700231472344, Validation Loss: 10.319485664367676\n",
      "Epoch [319/1200], Training Loss: 6.738667066670929, Validation Loss: 10.234795570373535\n",
      "Epoch [320/1200], Training Loss: 6.636213305114538, Validation Loss: 10.156241416931152\n",
      "Epoch [321/1200], Training Loss: 6.536230940926589, Validation Loss: 10.083067893981934\n",
      "Epoch [322/1200], Training Loss: 6.438501843514076, Validation Loss: 10.013801574707031\n",
      "Epoch [323/1200], Training Loss: 6.343081415514996, Validation Loss: 9.948585510253906\n",
      "Epoch [324/1200], Training Loss: 6.249893504186366, Validation Loss: 9.887958526611328\n",
      "Epoch [325/1200], Training Loss: 6.158784083118372, Validation Loss: 9.835143089294434\n",
      "Epoch [326/1200], Training Loss: 6.06957505944203, Validation Loss: 9.78238582611084\n",
      "Epoch [327/1200], Training Loss: 5.982467943153523, Validation Loss: 9.738212585449219\n",
      "Epoch [328/1200], Training Loss: 5.89736534094288, Validation Loss: 9.691600799560547\n",
      "Epoch [329/1200], Training Loss: 5.814139511161216, Validation Loss: 9.654242515563965\n",
      "Epoch [330/1200], Training Loss: 5.732672219228242, Validation Loss: 9.612370491027832\n",
      "Epoch [331/1200], Training Loss: 5.652953052885951, Validation Loss: 9.578238487243652\n",
      "Epoch [332/1200], Training Loss: 5.574823257389356, Validation Loss: 9.54041862487793\n",
      "Epoch [333/1200], Training Loss: 5.498222292203601, Validation Loss: 9.506110191345215\n",
      "Epoch [334/1200], Training Loss: 5.423181890685185, Validation Loss: 9.470690727233887\n",
      "Epoch [335/1200], Training Loss: 5.349589957376306, Validation Loss: 9.433947563171387\n",
      "Epoch [336/1200], Training Loss: 5.2776557046983, Validation Loss: 9.399942398071289\n",
      "Epoch [337/1200], Training Loss: 5.206884902322583, Validation Loss: 9.360344886779785\n",
      "Epoch [338/1200], Training Loss: 5.13780669521915, Validation Loss: 9.327125549316406\n",
      "Epoch [339/1200], Training Loss: 5.069644742606093, Validation Loss: 9.283173561096191\n",
      "Epoch [340/1200], Training Loss: 5.003175616123789, Validation Loss: 9.250990867614746\n",
      "Epoch [341/1200], Training Loss: 4.9375329531345225, Validation Loss: 9.2025146484375\n",
      "Epoch [342/1200], Training Loss: 4.873538190898808, Validation Loss: 9.172059059143066\n",
      "Epoch [343/1200], Training Loss: 4.810451541847635, Validation Loss: 9.119688987731934\n",
      "Epoch [344/1200], Training Loss: 4.748955367126002, Validation Loss: 9.091611862182617\n",
      "Epoch [345/1200], Training Loss: 4.6882897157213606, Validation Loss: 9.036291122436523\n",
      "Epoch [346/1200], Training Loss: 4.629185290606135, Validation Loss: 9.011302947998047\n",
      "Epoch [347/1200], Training Loss: 4.570914550000053, Validation Loss: 8.952744483947754\n",
      "Epoch [348/1200], Training Loss: 4.514168872690795, Validation Loss: 8.931781768798828\n",
      "Epoch [349/1200], Training Loss: 4.4580499710813095, Validation Loss: 8.869994163513184\n",
      "Epoch [350/1200], Training Loss: 4.4032723726912595, Validation Loss: 8.85315227508545\n",
      "Epoch [351/1200], Training Loss: 4.349199220079757, Validation Loss: 8.790820121765137\n",
      "Epoch [352/1200], Training Loss: 4.29634761521886, Validation Loss: 8.778122901916504\n",
      "Epoch [353/1200], Training Loss: 4.244237365119185, Validation Loss: 8.710465431213379\n",
      "Epoch [354/1200], Training Loss: 4.193389050464306, Validation Loss: 8.704035758972168\n",
      "Epoch [355/1200], Training Loss: 4.143641872756831, Validation Loss: 8.650971412658691\n",
      "Epoch [356/1200], Training Loss: 4.094251879279801, Validation Loss: 8.646899223327637\n",
      "Epoch [357/1200], Training Loss: 4.04559551702611, Validation Loss: 8.555320739746094\n",
      "Epoch [358/1200], Training Loss: 3.9984793457181693, Validation Loss: 8.56009292602539\n",
      "Epoch [359/1200], Training Loss: 3.9526776575810865, Validation Loss: 8.536749839782715\n",
      "Epoch [360/1200], Training Loss: 3.906109398928438, Validation Loss: 8.534550666809082\n",
      "Epoch [361/1200], Training Loss: 3.861363026518988, Validation Loss: 8.430427551269531\n",
      "Epoch [362/1200], Training Loss: 3.817593987105338, Validation Loss: 8.437742233276367\n",
      "Epoch [363/1200], Training Loss: 3.7754071502413766, Validation Loss: 8.436945915222168\n",
      "Epoch [364/1200], Training Loss: 3.731456395542501, Validation Loss: 8.439277648925781\n",
      "Epoch [365/1200], Training Loss: 3.69016819396876, Validation Loss: 8.328280448913574\n",
      "Epoch [366/1200], Training Loss: 3.6494364430319783, Validation Loss: 8.335776329040527\n",
      "Epoch [367/1200], Training Loss: 3.610406927007781, Validation Loss: 8.337924003601074\n",
      "Epoch [368/1200], Training Loss: 3.569166640235355, Validation Loss: 8.350531578063965\n",
      "Epoch [369/1200], Training Loss: 3.5309719591124544, Validation Loss: 8.238138198852539\n",
      "Epoch [370/1200], Training Loss: 3.4930466124078983, Validation Loss: 8.250842094421387\n",
      "Epoch [371/1200], Training Loss: 3.4568636843194587, Validation Loss: 8.249317169189453\n",
      "Epoch [372/1200], Training Loss: 3.418200146978356, Validation Loss: 8.273663520812988\n",
      "Epoch [373/1200], Training Loss: 3.38296422093484, Validation Loss: 8.161846160888672\n",
      "Epoch [374/1200], Training Loss: 3.347643584478711, Validation Loss: 8.17978286743164\n",
      "Epoch [375/1200], Training Loss: 3.314077616858154, Validation Loss: 8.171174049377441\n",
      "Epoch [376/1200], Training Loss: 3.2779023238601215, Validation Loss: 8.20655345916748\n",
      "Epoch [377/1200], Training Loss: 3.2454855832399665, Validation Loss: 8.097293853759766\n",
      "Epoch [378/1200], Training Loss: 3.21266361217054, Validation Loss: 8.12166690826416\n",
      "Epoch [379/1200], Training Loss: 3.1816822669672593, Validation Loss: 8.101222038269043\n",
      "Epoch [380/1200], Training Loss: 3.1479990410878242, Validation Loss: 8.144551277160645\n",
      "Epoch [381/1200], Training Loss: 3.1182525253559894, Validation Loss: 8.0396728515625\n",
      "Epoch [382/1200], Training Loss: 3.0876108376008338, Validation Loss: 8.068897247314453\n",
      "Epoch [383/1200], Training Loss: 3.0589929416882065, Validation Loss: 8.036538124084473\n",
      "Epoch [384/1200], Training Loss: 3.0277686072796, Validation Loss: 8.0829439163208\n",
      "Epoch [385/1200], Training Loss: 3.0004381031254956, Validation Loss: 7.982351779937744\n",
      "Epoch [386/1200], Training Loss: 2.971913239890777, Validation Loss: 8.013084411621094\n",
      "Epoch [387/1200], Training Loss: 2.9455848434620107, Validation Loss: 7.967924118041992\n",
      "Epoch [388/1200], Training Loss: 2.916926293380028, Validation Loss: 8.01237964630127\n",
      "Epoch [389/1200], Training Loss: 2.8918347521179375, Validation Loss: 7.917456150054932\n",
      "Epoch [390/1200], Training Loss: 2.8654230680704003, Validation Loss: 7.948885440826416\n",
      "Epoch [391/1200], Training Loss: 2.841144472316633, Validation Loss: 7.897897243499756\n",
      "Epoch [392/1200], Training Loss: 2.814962602355554, Validation Loss: 7.939198970794678\n",
      "Epoch [393/1200], Training Loss: 2.7919067245270996, Validation Loss: 7.857913494110107\n",
      "Epoch [394/1200], Training Loss: 2.7675425339592152, Validation Loss: 7.89001989364624\n",
      "Epoch [395/1200], Training Loss: 2.745087983137802, Validation Loss: 7.841846942901611\n",
      "Epoch [396/1200], Training Loss: 2.721433674864701, Validation Loss: 7.879447937011719\n",
      "Epoch [397/1200], Training Loss: 2.7000251504389934, Validation Loss: 7.817663192749023\n",
      "Epoch [398/1200], Training Loss: 2.677827155492873, Validation Loss: 7.849122047424316\n",
      "Epoch [399/1200], Training Loss: 2.656982552716853, Validation Loss: 7.810176849365234\n",
      "Epoch [400/1200], Training Loss: 2.6357282041250296, Validation Loss: 7.842335224151611\n",
      "Epoch [401/1200], Training Loss: 2.6156470850107647, Validation Loss: 7.804742336273193\n",
      "Epoch [402/1200], Training Loss: 2.5956076003599193, Validation Loss: 7.832275390625\n",
      "Epoch [403/1200], Training Loss: 2.5761576545235743, Validation Loss: 7.8100457191467285\n",
      "Epoch [404/1200], Training Loss: 2.557136187334454, Validation Loss: 7.835149765014648\n",
      "Epoch [405/1200], Training Loss: 2.5384377173806434, Validation Loss: 7.822381973266602\n",
      "Epoch [406/1200], Training Loss: 2.5203275919379493, Validation Loss: 7.843717098236084\n",
      "Epoch [407/1200], Training Loss: 2.5023278173285424, Validation Loss: 7.843559741973877\n",
      "Epoch [408/1200], Training Loss: 2.485044747110761, Validation Loss: 7.861175060272217\n",
      "Epoch [409/1200], Training Loss: 2.467809074341232, Validation Loss: 7.871438980102539\n",
      "Epoch [410/1200], Training Loss: 2.4513198087965473, Validation Loss: 7.885215759277344\n",
      "Early stopping at epoch 410\n",
      "Final Test Loss: 9.392582893371582\n",
      "Hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.0002, window_size=7\n",
      "Epoch [1/1200], Training Loss: 27270.187893055816, Validation Loss: 26742.42578125\n",
      "Epoch [2/1200], Training Loss: 25321.005203042718, Validation Loss: 24986.400390625\n",
      "Epoch [3/1200], Training Loss: 23809.0891711589, Validation Loss: 23580.728515625\n",
      "Epoch [4/1200], Training Loss: 22632.774039663014, Validation Loss: 22485.353515625\n",
      "Epoch [5/1200], Training Loss: 21734.911743524783, Validation Loss: 21644.310546875\n",
      "Epoch [6/1200], Training Loss: 21051.829602969232, Validation Loss: 20999.201171875\n",
      "Epoch [7/1200], Training Loss: 20527.811700871356, Validation Loss: 20500.22265625\n",
      "Epoch [8/1200], Training Loss: 20120.41137208431, Validation Loss: 20109.484375\n",
      "Epoch [9/1200], Training Loss: 19799.161687812106, Validation Loss: 19799.498046875\n",
      "Epoch [10/1200], Training Loss: 19542.386218903826, Validation Loss: 19550.451171875\n",
      "Epoch [11/1200], Training Loss: 19334.739333606176, Validation Loss: 19348.1796875\n",
      "Epoch [12/1200], Training Loss: 19164.99892062129, Validation Loss: 19182.212890625\n",
      "Epoch [13/1200], Training Loss: 17967.829923944086, Validation Loss: 18429.1796875\n",
      "Epoch [14/1200], Training Loss: 16906.598026782984, Validation Loss: 17469.55078125\n",
      "Epoch [15/1200], Training Loss: 16164.799821630542, Validation Loss: 16877.421875\n",
      "Epoch [16/1200], Training Loss: 15473.306487259479, Validation Loss: 16241.341796875\n",
      "Epoch [17/1200], Training Loss: 14827.748107485257, Validation Loss: 15644.2294921875\n",
      "Epoch [18/1200], Training Loss: 14226.743618997374, Validation Loss: 14985.3232421875\n",
      "Epoch [19/1200], Training Loss: 13666.302161482321, Validation Loss: 14191.98828125\n",
      "Epoch [20/1200], Training Loss: 13139.724787622747, Validation Loss: 13525.453125\n",
      "Epoch [21/1200], Training Loss: 12645.098391474454, Validation Loss: 12972.9130859375\n",
      "Epoch [22/1200], Training Loss: 12179.98208528889, Validation Loss: 12459.125\n",
      "Epoch [23/1200], Training Loss: 11741.107752469483, Validation Loss: 11935.9521484375\n",
      "Epoch [24/1200], Training Loss: 11327.267273985044, Validation Loss: 11410.146484375\n",
      "Epoch [25/1200], Training Loss: 10938.820874948211, Validation Loss: 10994.30859375\n",
      "Epoch [26/1200], Training Loss: 10574.544902401176, Validation Loss: 10605.96875\n",
      "Epoch [27/1200], Training Loss: 10233.42596128102, Validation Loss: 10261.8125\n",
      "Epoch [28/1200], Training Loss: 9914.564118965163, Validation Loss: 9964.517578125\n",
      "Epoch [29/1200], Training Loss: 9618.137916548825, Validation Loss: 9693.0224609375\n",
      "Epoch [30/1200], Training Loss: 9338.49679573389, Validation Loss: 9411.8203125\n",
      "Epoch [31/1200], Training Loss: 9076.511835400679, Validation Loss: 9137.505859375\n",
      "Epoch [32/1200], Training Loss: 8830.276600584404, Validation Loss: 8902.6728515625\n",
      "Epoch [33/1200], Training Loss: 8596.326402898427, Validation Loss: 8692.78125\n",
      "Epoch [34/1200], Training Loss: 8375.261646283565, Validation Loss: 8669.771484375\n",
      "Epoch [35/1200], Training Loss: 8040.056168717258, Validation Loss: 9177.8681640625\n",
      "Epoch [36/1200], Training Loss: 7780.99466966182, Validation Loss: 8247.0009765625\n",
      "Epoch [37/1200], Training Loss: 7536.822418851086, Validation Loss: 7859.2021484375\n",
      "Epoch [38/1200], Training Loss: 7306.47677321689, Validation Loss: 7516.2802734375\n",
      "Epoch [39/1200], Training Loss: 7085.5634767706815, Validation Loss: 7224.07763671875\n",
      "Epoch [40/1200], Training Loss: 6872.589499141611, Validation Loss: 6988.03125\n",
      "Epoch [41/1200], Training Loss: 6666.560824410834, Validation Loss: 6754.5556640625\n",
      "Epoch [42/1200], Training Loss: 6466.2038640185765, Validation Loss: 6537.3564453125\n",
      "Epoch [43/1200], Training Loss: 6271.2934676700215, Validation Loss: 6335.1396484375\n",
      "Epoch [44/1200], Training Loss: 6081.757217910756, Validation Loss: 6146.6123046875\n",
      "Epoch [45/1200], Training Loss: 5897.820398728902, Validation Loss: 5964.640625\n",
      "Epoch [46/1200], Training Loss: 5719.663834565295, Validation Loss: 5787.47314453125\n",
      "Epoch [47/1200], Training Loss: 5547.480873502367, Validation Loss: 5614.94873046875\n",
      "Epoch [48/1200], Training Loss: 5381.371440017696, Validation Loss: 5447.974609375\n",
      "Epoch [49/1200], Training Loss: 5221.43530630895, Validation Loss: 5290.193359375\n",
      "Epoch [50/1200], Training Loss: 5067.581939920951, Validation Loss: 5143.91015625\n",
      "Epoch [51/1200], Training Loss: 4918.993317948412, Validation Loss: 5002.01171875\n",
      "Epoch [52/1200], Training Loss: 4774.447797006209, Validation Loss: 4862.9189453125\n",
      "Epoch [53/1200], Training Loss: 4633.520182474595, Validation Loss: 4723.7705078125\n",
      "Epoch [54/1200], Training Loss: 4495.949984680255, Validation Loss: 4585.80322265625\n",
      "Epoch [55/1200], Training Loss: 4361.563398970432, Validation Loss: 4448.67626953125\n",
      "Epoch [56/1200], Training Loss: 4229.978378176455, Validation Loss: 4306.15185546875\n",
      "Epoch [57/1200], Training Loss: 4101.118167077956, Validation Loss: 4157.62939453125\n",
      "Epoch [58/1200], Training Loss: 3975.1883193921735, Validation Loss: 4025.64208984375\n",
      "Epoch [59/1200], Training Loss: 3852.7066481802526, Validation Loss: 3901.960205078125\n",
      "Epoch [60/1200], Training Loss: 3733.7529251932424, Validation Loss: 3782.91748046875\n",
      "Epoch [61/1200], Training Loss: 3618.290691448654, Validation Loss: 3666.968017578125\n",
      "Epoch [62/1200], Training Loss: 3506.264808975267, Validation Loss: 3553.671630859375\n",
      "Epoch [63/1200], Training Loss: 3397.6297463326, Validation Loss: 3443.0224609375\n",
      "Epoch [64/1200], Training Loss: 3292.3254416911454, Validation Loss: 3335.451171875\n",
      "Epoch [65/1200], Training Loss: 3190.337840634247, Validation Loss: 3231.61181640625\n",
      "Epoch [66/1200], Training Loss: 3091.6924761818113, Validation Loss: 3131.828857421875\n",
      "Epoch [67/1200], Training Loss: 2996.3601792079044, Validation Loss: 3035.8740234375\n",
      "Epoch [68/1200], Training Loss: 2904.203582143313, Validation Loss: 2943.34619140625\n",
      "Epoch [69/1200], Training Loss: 2815.1337281530514, Validation Loss: 2854.011962890625\n",
      "Epoch [70/1200], Training Loss: 2729.2037897149767, Validation Loss: 2767.875\n",
      "Epoch [71/1200], Training Loss: 2646.521971283642, Validation Loss: 2685.018310546875\n",
      "Epoch [72/1200], Training Loss: 2567.1811592680733, Validation Loss: 2605.49072265625\n",
      "Epoch [73/1200], Training Loss: 2491.2660683397535, Validation Loss: 2529.41357421875\n",
      "Epoch [74/1200], Training Loss: 2418.888892237835, Validation Loss: 2457.087646484375\n",
      "Epoch [75/1200], Training Loss: 2350.0995512469995, Validation Loss: 2388.266845703125\n",
      "Epoch [76/1200], Training Loss: 2284.1816914771784, Validation Loss: 2321.29541015625\n",
      "Epoch [77/1200], Training Loss: 2220.04513658423, Validation Loss: 2256.76123046875\n",
      "Epoch [78/1200], Training Loss: 2157.725279762013, Validation Loss: 2194.832763671875\n",
      "Epoch [79/1200], Training Loss: 2097.2403945239384, Validation Loss: 2134.7236328125\n",
      "Epoch [80/1200], Training Loss: 2038.409395492337, Validation Loss: 2076.127197265625\n",
      "Epoch [81/1200], Training Loss: 1981.1413341772802, Validation Loss: 2019.159423828125\n",
      "Epoch [82/1200], Training Loss: 1925.4194123942218, Validation Loss: 1963.667236328125\n",
      "Epoch [83/1200], Training Loss: 1871.2300547637515, Validation Loss: 1909.360595703125\n",
      "Epoch [84/1200], Training Loss: 1818.5505567010202, Validation Loss: 1855.780029296875\n",
      "Epoch [85/1200], Training Loss: 1767.354705467372, Validation Loss: 1802.4859619140625\n",
      "Epoch [86/1200], Training Loss: 1717.6421886682094, Validation Loss: 1749.9681396484375\n",
      "Epoch [87/1200], Training Loss: 1669.5267740659217, Validation Loss: 1699.53955078125\n",
      "Epoch [88/1200], Training Loss: 1623.21534157074, Validation Loss: 1651.84765625\n",
      "Epoch [89/1200], Training Loss: 1578.8218000538927, Validation Loss: 1606.8016357421875\n",
      "Epoch [90/1200], Training Loss: 1536.2722975425645, Validation Loss: 1564.0794677734375\n",
      "Epoch [91/1200], Training Loss: 1495.4264470689122, Validation Loss: 1523.28369140625\n",
      "Epoch [92/1200], Training Loss: 1456.1442123565712, Validation Loss: 1484.072509765625\n",
      "Epoch [93/1200], Training Loss: 1418.3336778555442, Validation Loss: 1446.2615966796875\n",
      "Epoch [94/1200], Training Loss: 1381.8901690224375, Validation Loss: 1409.5333251953125\n",
      "Epoch [95/1200], Training Loss: 1346.7036031870882, Validation Loss: 1373.75927734375\n",
      "Epoch [96/1200], Training Loss: 1312.684272204531, Validation Loss: 1338.9549560546875\n",
      "Epoch [97/1200], Training Loss: 1279.7298154544496, Validation Loss: 1305.094482421875\n",
      "Epoch [98/1200], Training Loss: 1247.7566527159127, Validation Loss: 1272.378173828125\n",
      "Epoch [99/1200], Training Loss: 1216.6672142381506, Validation Loss: 1240.885498046875\n",
      "Epoch [100/1200], Training Loss: 1186.3937050176291, Validation Loss: 1210.6531982421875\n",
      "Epoch [101/1200], Training Loss: 1156.884294824852, Validation Loss: 1181.5506591796875\n",
      "Epoch [102/1200], Training Loss: 1128.0970072185646, Validation Loss: 1153.3758544921875\n",
      "Epoch [103/1200], Training Loss: 1100.009264451446, Validation Loss: 1125.9248046875\n",
      "Epoch [104/1200], Training Loss: 1072.5966234874686, Validation Loss: 1099.055419921875\n",
      "Epoch [105/1200], Training Loss: 1045.8370656673135, Validation Loss: 1072.6982421875\n",
      "Epoch [106/1200], Training Loss: 1019.7214961279991, Validation Loss: 1046.8057861328125\n",
      "Epoch [107/1200], Training Loss: 994.2455578058507, Validation Loss: 1021.2998046875\n",
      "Epoch [108/1200], Training Loss: 969.385536396196, Validation Loss: 996.1492309570312\n",
      "Epoch [109/1200], Training Loss: 945.1297349111172, Validation Loss: 971.4137573242188\n",
      "Epoch [110/1200], Training Loss: 921.4706601054396, Validation Loss: 947.2116088867188\n",
      "Epoch [111/1200], Training Loss: 898.3923124612079, Validation Loss: 923.6342163085938\n",
      "Epoch [112/1200], Training Loss: 875.879166106879, Validation Loss: 900.7083129882812\n",
      "Epoch [113/1200], Training Loss: 853.9189537176541, Validation Loss: 878.4296264648438\n",
      "Epoch [114/1200], Training Loss: 832.4999395392842, Validation Loss: 856.7757568359375\n",
      "Epoch [115/1200], Training Loss: 811.6007411717106, Validation Loss: 835.7371215820312\n",
      "Epoch [116/1200], Training Loss: 791.2094219974665, Validation Loss: 815.2943725585938\n",
      "Epoch [117/1200], Training Loss: 771.311226589276, Validation Loss: 795.3841552734375\n",
      "Epoch [118/1200], Training Loss: 751.8844026847385, Validation Loss: 775.9552001953125\n",
      "Epoch [119/1200], Training Loss: 732.909996757696, Validation Loss: 756.9721069335938\n",
      "Epoch [120/1200], Training Loss: 714.3652130164891, Validation Loss: 738.4024047851562\n",
      "Epoch [121/1200], Training Loss: 696.2375010902816, Validation Loss: 720.1918334960938\n",
      "Epoch [122/1200], Training Loss: 678.518335652615, Validation Loss: 702.2828369140625\n",
      "Epoch [123/1200], Training Loss: 661.1942027666896, Validation Loss: 684.6683959960938\n",
      "Epoch [124/1200], Training Loss: 644.2537397443267, Validation Loss: 667.3672485351562\n",
      "Epoch [125/1200], Training Loss: 627.6868455090074, Validation Loss: 650.424560546875\n",
      "Epoch [126/1200], Training Loss: 611.4853723084516, Validation Loss: 633.8663330078125\n",
      "Epoch [127/1200], Training Loss: 595.6373090175289, Validation Loss: 617.7117919921875\n",
      "Epoch [128/1200], Training Loss: 580.135866038628, Validation Loss: 601.9684448242188\n",
      "Epoch [129/1200], Training Loss: 564.9749191068248, Validation Loss: 586.6282958984375\n",
      "Epoch [130/1200], Training Loss: 550.151397206829, Validation Loss: 571.6569213867188\n",
      "Epoch [131/1200], Training Loss: 535.6626511607243, Validation Loss: 557.0055541992188\n",
      "Epoch [132/1200], Training Loss: 521.5054797958227, Validation Loss: 542.6537475585938\n",
      "Epoch [133/1200], Training Loss: 507.67884763401474, Validation Loss: 528.5965576171875\n",
      "Epoch [134/1200], Training Loss: 494.1721827085523, Validation Loss: 514.8457641601562\n",
      "Epoch [135/1200], Training Loss: 480.9823940667705, Validation Loss: 501.4192199707031\n",
      "Epoch [136/1200], Training Loss: 468.09951549327496, Validation Loss: 488.3027648925781\n",
      "Epoch [137/1200], Training Loss: 455.5166291377405, Validation Loss: 475.51300048828125\n",
      "Epoch [138/1200], Training Loss: 443.22810277275386, Validation Loss: 463.03070068359375\n",
      "Epoch [139/1200], Training Loss: 431.2262784265759, Validation Loss: 450.85601806640625\n",
      "Epoch [140/1200], Training Loss: 419.51053561044836, Validation Loss: 438.98199462890625\n",
      "Epoch [141/1200], Training Loss: 408.07356989975204, Validation Loss: 427.39453125\n",
      "Epoch [142/1200], Training Loss: 396.90768310961096, Validation Loss: 416.0914306640625\n",
      "Epoch [143/1200], Training Loss: 386.0093232605052, Validation Loss: 405.06134033203125\n",
      "Epoch [144/1200], Training Loss: 375.3730559008581, Validation Loss: 394.3096618652344\n",
      "Epoch [145/1200], Training Loss: 364.9980413240738, Validation Loss: 383.8311767578125\n",
      "Epoch [146/1200], Training Loss: 354.87697297529746, Validation Loss: 373.6183776855469\n",
      "Epoch [147/1200], Training Loss: 345.00779054864716, Validation Loss: 363.6690979003906\n",
      "Epoch [148/1200], Training Loss: 335.38587562966416, Validation Loss: 353.96826171875\n",
      "Epoch [149/1200], Training Loss: 326.0091867632714, Validation Loss: 344.4963684082031\n",
      "Epoch [150/1200], Training Loss: 316.87354959428876, Validation Loss: 335.2384948730469\n",
      "Epoch [151/1200], Training Loss: 307.9802566666142, Validation Loss: 326.1798095703125\n",
      "Epoch [152/1200], Training Loss: 299.3268414058364, Validation Loss: 317.3067321777344\n",
      "Epoch [153/1200], Training Loss: 290.91045459217617, Validation Loss: 308.61614990234375\n",
      "Epoch [154/1200], Training Loss: 282.7291499568953, Validation Loss: 300.1004638671875\n",
      "Epoch [155/1200], Training Loss: 274.7815122389065, Validation Loss: 291.74798583984375\n",
      "Epoch [156/1200], Training Loss: 267.063381942697, Validation Loss: 283.55291748046875\n",
      "Epoch [157/1200], Training Loss: 259.5742655496009, Validation Loss: 275.5471496582031\n",
      "Epoch [158/1200], Training Loss: 252.30543008733267, Validation Loss: 267.7976379394531\n",
      "Epoch [159/1200], Training Loss: 245.2518790808147, Validation Loss: 260.34503173828125\n",
      "Epoch [160/1200], Training Loss: 238.40740711951025, Validation Loss: 253.1741943359375\n",
      "Epoch [161/1200], Training Loss: 231.7651930838612, Validation Loss: 246.2421875\n",
      "Epoch [162/1200], Training Loss: 225.3163261634556, Validation Loss: 239.5093231201172\n",
      "Epoch [163/1200], Training Loss: 219.0567264082966, Validation Loss: 232.95556640625\n",
      "Epoch [164/1200], Training Loss: 212.98599943984468, Validation Loss: 226.6141815185547\n",
      "Epoch [165/1200], Training Loss: 207.1055823216591, Validation Loss: 220.5451202392578\n",
      "Epoch [166/1200], Training Loss: 201.4151457059728, Validation Loss: 214.77037048339844\n",
      "Epoch [167/1200], Training Loss: 195.90246777250405, Validation Loss: 209.18324279785156\n",
      "Epoch [168/1200], Training Loss: 190.56027964991398, Validation Loss: 203.79029846191406\n",
      "Epoch [169/1200], Training Loss: 185.3793052292687, Validation Loss: 198.5849151611328\n",
      "Epoch [170/1200], Training Loss: 180.34913702525108, Validation Loss: 193.5629119873047\n",
      "Epoch [171/1200], Training Loss: 175.4633012633324, Validation Loss: 188.7086944580078\n",
      "Epoch [172/1200], Training Loss: 170.7134606784504, Validation Loss: 184.02723693847656\n",
      "Epoch [173/1200], Training Loss: 166.09355285206635, Validation Loss: 179.5630645751953\n",
      "Epoch [174/1200], Training Loss: 161.60040912643856, Validation Loss: 175.30355834960938\n",
      "Epoch [175/1200], Training Loss: 157.2291898046146, Validation Loss: 171.19874572753906\n",
      "Epoch [176/1200], Training Loss: 152.9774011567287, Validation Loss: 167.1867218017578\n",
      "Epoch [177/1200], Training Loss: 148.83724606501832, Validation Loss: 163.16322326660156\n",
      "Epoch [178/1200], Training Loss: 144.8079402224415, Validation Loss: 159.07395935058594\n",
      "Epoch [179/1200], Training Loss: 140.8776668339682, Validation Loss: 154.97164916992188\n",
      "Epoch [180/1200], Training Loss: 137.03645431018748, Validation Loss: 151.01841735839844\n",
      "Epoch [181/1200], Training Loss: 133.28929854805907, Validation Loss: 147.22813415527344\n",
      "Epoch [182/1200], Training Loss: 129.64031319544662, Validation Loss: 143.63052368164062\n",
      "Epoch [183/1200], Training Loss: 126.08676844132665, Validation Loss: 140.28453063964844\n",
      "Epoch [184/1200], Training Loss: 122.6276677664855, Validation Loss: 137.15394592285156\n",
      "Epoch [185/1200], Training Loss: 119.25640552662013, Validation Loss: 134.13922119140625\n",
      "Epoch [186/1200], Training Loss: 115.9648794722162, Validation Loss: 131.1243133544922\n",
      "Epoch [187/1200], Training Loss: 112.75054533240352, Validation Loss: 128.05731201171875\n",
      "Epoch [188/1200], Training Loss: 109.61311241298402, Validation Loss: 124.91658782958984\n",
      "Epoch [189/1200], Training Loss: 106.55557756324832, Validation Loss: 121.66323852539062\n",
      "Epoch [190/1200], Training Loss: 103.57715724597622, Validation Loss: 118.24144744873047\n",
      "Epoch [191/1200], Training Loss: 100.67863252638331, Validation Loss: 114.63752746582031\n",
      "Epoch [192/1200], Training Loss: 97.8610153552269, Validation Loss: 110.89895629882812\n",
      "Epoch [193/1200], Training Loss: 95.1242533783294, Validation Loss: 107.04349517822266\n",
      "Epoch [194/1200], Training Loss: 92.46561664101365, Validation Loss: 103.10826873779297\n",
      "Epoch [195/1200], Training Loss: 89.8841670450782, Validation Loss: 99.36682891845703\n",
      "Epoch [196/1200], Training Loss: 87.37655327658689, Validation Loss: 96.01480102539062\n",
      "Epoch [197/1200], Training Loss: 84.93961338623042, Validation Loss: 92.95096588134766\n",
      "Epoch [198/1200], Training Loss: 82.57081395495129, Validation Loss: 90.15047454833984\n",
      "Epoch [199/1200], Training Loss: 80.27114775820336, Validation Loss: 87.5013198852539\n",
      "Epoch [200/1200], Training Loss: 78.04059077511077, Validation Loss: 84.95821380615234\n",
      "Epoch [201/1200], Training Loss: 75.87708203038666, Validation Loss: 82.53302001953125\n",
      "Epoch [202/1200], Training Loss: 73.77855441073905, Validation Loss: 80.1474609375\n",
      "Epoch [203/1200], Training Loss: 71.74520826829085, Validation Loss: 78.02796936035156\n",
      "Epoch [204/1200], Training Loss: 69.77404661467567, Validation Loss: 75.50399017333984\n",
      "Epoch [205/1200], Training Loss: 67.86335480281093, Validation Loss: 74.00557708740234\n",
      "Epoch [206/1200], Training Loss: 66.01913633824088, Validation Loss: 71.47329711914062\n",
      "Epoch [207/1200], Training Loss: 64.22529738951131, Validation Loss: 69.73381805419922\n",
      "Epoch [208/1200], Training Loss: 62.49856447878608, Validation Loss: 68.4245376586914\n",
      "Epoch [209/1200], Training Loss: 60.815352972595264, Validation Loss: 65.72030639648438\n",
      "Epoch [210/1200], Training Loss: 59.19081235318352, Validation Loss: 65.10948944091797\n",
      "Epoch [211/1200], Training Loss: 57.63263604964839, Validation Loss: 63.118560791015625\n",
      "Epoch [212/1200], Training Loss: 56.10436693235549, Validation Loss: 61.06844711303711\n",
      "Epoch [213/1200], Training Loss: 54.64266331986268, Validation Loss: 60.52653503417969\n",
      "Epoch [214/1200], Training Loss: 53.22773521094819, Validation Loss: 58.20152282714844\n",
      "Epoch [215/1200], Training Loss: 51.85148827212509, Validation Loss: 57.398193359375\n",
      "Epoch [216/1200], Training Loss: 50.54498887913882, Validation Loss: 56.19676971435547\n",
      "Epoch [217/1200], Training Loss: 49.24297440228244, Validation Loss: 53.98504638671875\n",
      "Epoch [218/1200], Training Loss: 48.004012493990274, Validation Loss: 53.56232452392578\n",
      "Epoch [219/1200], Training Loss: 46.81465269973729, Validation Loss: 52.110137939453125\n",
      "Epoch [220/1200], Training Loss: 45.627241208937555, Validation Loss: 50.421226501464844\n",
      "Epoch [221/1200], Training Loss: 44.504634678385614, Validation Loss: 50.0855712890625\n",
      "Epoch [222/1200], Training Loss: 43.40765758617079, Validation Loss: 48.27262878417969\n",
      "Epoch [223/1200], Training Loss: 42.33097655370107, Validation Loss: 47.562522888183594\n",
      "Epoch [224/1200], Training Loss: 41.31586255428347, Validation Loss: 46.92939758300781\n",
      "Epoch [225/1200], Training Loss: 40.29077371845866, Validation Loss: 45.09632873535156\n",
      "Epoch [226/1200], Training Loss: 39.31870479553919, Validation Loss: 45.07508850097656\n",
      "Epoch [227/1200], Training Loss: 38.38008348090817, Validation Loss: 43.63951873779297\n",
      "Epoch [228/1200], Training Loss: 37.43994490548808, Validation Loss: 42.48338317871094\n",
      "Epoch [229/1200], Training Loss: 36.55734108147768, Validation Loss: 42.32745361328125\n",
      "Epoch [230/1200], Training Loss: 35.67541772867234, Validation Loss: 40.7023811340332\n",
      "Epoch [231/1200], Training Loss: 34.83144314941845, Validation Loss: 40.818519592285156\n",
      "Epoch [232/1200], Training Loss: 34.01640269810852, Validation Loss: 39.512481689453125\n",
      "Epoch [233/1200], Training Loss: 33.200198384341846, Validation Loss: 38.41761779785156\n",
      "Epoch [234/1200], Training Loss: 32.433329361136465, Validation Loss: 38.239654541015625\n",
      "Epoch [235/1200], Training Loss: 31.665006067972854, Validation Loss: 36.87944030761719\n",
      "Epoch [236/1200], Training Loss: 30.93393709757119, Validation Loss: 36.803192138671875\n",
      "Epoch [237/1200], Training Loss: 30.21592768608376, Validation Loss: 35.46978759765625\n",
      "Epoch [238/1200], Training Loss: 29.51688835097468, Validation Loss: 35.13899612426758\n",
      "Epoch [239/1200], Training Loss: 28.85532315198615, Validation Loss: 34.402767181396484\n",
      "Epoch [240/1200], Training Loss: 28.182997067492263, Validation Loss: 33.32175064086914\n",
      "Epoch [241/1200], Training Loss: 27.55532775591391, Validation Loss: 33.12123107910156\n",
      "Epoch [242/1200], Training Loss: 26.92899934548369, Validation Loss: 32.07265853881836\n",
      "Epoch [243/1200], Training Loss: 26.333344266404268, Validation Loss: 32.008426666259766\n",
      "Epoch [244/1200], Training Loss: 25.74977676140751, Validation Loss: 31.009294509887695\n",
      "Epoch [245/1200], Training Loss: 25.183181823162712, Validation Loss: 30.805736541748047\n",
      "Epoch [246/1200], Training Loss: 24.646526070955, Validation Loss: 30.317710876464844\n",
      "Epoch [247/1200], Training Loss: 24.103958374617626, Validation Loss: 29.522146224975586\n",
      "Epoch [248/1200], Training Loss: 23.595872344339057, Validation Loss: 29.476160049438477\n",
      "Epoch [249/1200], Training Loss: 23.09242085187924, Validation Loss: 28.706783294677734\n",
      "Epoch [250/1200], Training Loss: 22.611049841056076, Validation Loss: 28.748083114624023\n",
      "Epoch [251/1200], Training Loss: 22.14522984159142, Validation Loss: 28.089113235473633\n",
      "Epoch [252/1200], Training Loss: 21.687425185711863, Validation Loss: 27.765750885009766\n",
      "Epoch [253/1200], Training Loss: 21.25396966475637, Validation Loss: 27.574186325073242\n",
      "Epoch [254/1200], Training Loss: 20.82253106244191, Validation Loss: 26.954261779785156\n",
      "Epoch [255/1200], Training Loss: 20.41179130088969, Validation Loss: 26.965770721435547\n",
      "Epoch [256/1200], Training Loss: 20.010976999043063, Validation Loss: 26.295392990112305\n",
      "Epoch [257/1200], Training Loss: 19.618253264794028, Validation Loss: 26.20414161682129\n",
      "Epoch [258/1200], Training Loss: 19.245577295717197, Validation Loss: 25.745214462280273\n",
      "Epoch [259/1200], Training Loss: 18.869025184707677, Validation Loss: 25.287080764770508\n",
      "Epoch [260/1200], Training Loss: 18.5130344842556, Validation Loss: 25.173084259033203\n",
      "Epoch [261/1200], Training Loss: 18.158478437442426, Validation Loss: 24.561521530151367\n",
      "Epoch [262/1200], Training Loss: 17.81593911878327, Validation Loss: 24.544099807739258\n",
      "Epoch [263/1200], Training Loss: 17.483754755841264, Validation Loss: 23.984228134155273\n",
      "Epoch [264/1200], Training Loss: 17.152044553252875, Validation Loss: 23.728422164916992\n",
      "Epoch [265/1200], Training Loss: 16.836845180827293, Validation Loss: 23.561748504638672\n",
      "Epoch [266/1200], Training Loss: 16.520804565348833, Validation Loss: 23.052597045898438\n",
      "Epoch [267/1200], Training Loss: 16.216385571503437, Validation Loss: 23.08047866821289\n",
      "Epoch [268/1200], Training Loss: 15.918624242017597, Validation Loss: 22.590972900390625\n",
      "Epoch [269/1200], Training Loss: 15.623427560120145, Validation Loss: 22.47882080078125\n",
      "Epoch [270/1200], Training Loss: 15.341187161177821, Validation Loss: 22.268766403198242\n",
      "Epoch [271/1200], Training Loss: 15.057016317407513, Validation Loss: 21.830646514892578\n",
      "Epoch [272/1200], Training Loss: 14.783774230092536, Validation Loss: 21.92844009399414\n",
      "Epoch [273/1200], Training Loss: 14.515634554806068, Validation Loss: 21.47933578491211\n",
      "Epoch [274/1200], Training Loss: 14.249842779153832, Validation Loss: 21.402896881103516\n",
      "Epoch [275/1200], Training Loss: 13.994853538138393, Validation Loss: 21.2752685546875\n",
      "Epoch [276/1200], Training Loss: 13.739344906829977, Validation Loss: 20.828454971313477\n",
      "Epoch [277/1200], Training Loss: 13.492136923312943, Validation Loss: 21.050813674926758\n",
      "Epoch [278/1200], Training Loss: 13.252433977428566, Validation Loss: 20.805604934692383\n",
      "Epoch [279/1200], Training Loss: 13.012971055968114, Validation Loss: 20.377891540527344\n",
      "Epoch [280/1200], Training Loss: 12.781432714299942, Validation Loss: 20.62331771850586\n",
      "Epoch [281/1200], Training Loss: 12.557796207109368, Validation Loss: 20.483802795410156\n",
      "Epoch [282/1200], Training Loss: 12.334337432010509, Validation Loss: 19.895687103271484\n",
      "Epoch [283/1200], Training Loss: 12.117057542237944, Validation Loss: 20.23623275756836\n",
      "Epoch [284/1200], Training Loss: 11.909341391052328, Validation Loss: 20.234094619750977\n",
      "Epoch [285/1200], Training Loss: 11.700112034292388, Validation Loss: 19.353952407836914\n",
      "Epoch [286/1200], Training Loss: 11.495816545273717, Validation Loss: 19.759750366210938\n",
      "Epoch [287/1200], Training Loss: 11.30152128314292, Validation Loss: 19.850221633911133\n",
      "Epoch [288/1200], Training Loss: 11.106152858324052, Validation Loss: 18.835630416870117\n",
      "Epoch [289/1200], Training Loss: 10.914500110899393, Validation Loss: 19.163652420043945\n",
      "Epoch [290/1200], Training Loss: 10.730984581916683, Validation Loss: 19.27816390991211\n",
      "Epoch [291/1200], Training Loss: 10.548180095031855, Validation Loss: 18.28087615966797\n",
      "Epoch [292/1200], Training Loss: 10.368443270450477, Validation Loss: 18.477760314941406\n",
      "Epoch [293/1200], Training Loss: 10.195525177414963, Validation Loss: 18.532373428344727\n",
      "Epoch [294/1200], Training Loss: 10.024221468278412, Validation Loss: 17.626848220825195\n",
      "Epoch [295/1200], Training Loss: 9.855996676789532, Validation Loss: 17.740758895874023\n",
      "Epoch [296/1200], Training Loss: 9.693132730297963, Validation Loss: 17.713626861572266\n",
      "Epoch [297/1200], Training Loss: 9.532319085388133, Validation Loss: 16.891220092773438\n",
      "Epoch [298/1200], Training Loss: 9.374995613903632, Validation Loss: 16.971052169799805\n",
      "Epoch [299/1200], Training Loss: 9.22225998263746, Validation Loss: 16.91962242126465\n",
      "Epoch [300/1200], Training Loss: 9.071947812255871, Validation Loss: 16.179550170898438\n",
      "Epoch [301/1200], Training Loss: 8.9248048388638, Validation Loss: 16.242473602294922\n",
      "Epoch [302/1200], Training Loss: 8.7817351140208, Validation Loss: 16.20639991760254\n",
      "Epoch [303/1200], Training Loss: 8.641046762761233, Validation Loss: 15.536954879760742\n",
      "Epoch [304/1200], Training Loss: 8.503576826982497, Validation Loss: 15.592047691345215\n",
      "Epoch [305/1200], Training Loss: 8.369611261198841, Validation Loss: 15.589521408081055\n",
      "Epoch [306/1200], Training Loss: 8.237819115453128, Validation Loss: 14.992500305175781\n",
      "Epoch [307/1200], Training Loss: 8.109200616832652, Validation Loss: 15.027031898498535\n",
      "Epoch [308/1200], Training Loss: 7.983753080617771, Validation Loss: 15.058248519897461\n",
      "Epoch [309/1200], Training Loss: 7.860236065997785, Validation Loss: 14.532266616821289\n",
      "Epoch [310/1200], Training Loss: 7.739654997733298, Validation Loss: 14.541629791259766\n",
      "Epoch [311/1200], Training Loss: 7.6222281806873315, Validation Loss: 14.604413986206055\n",
      "Epoch [312/1200], Training Loss: 7.506260549204707, Validation Loss: 14.143840789794922\n",
      "Epoch [313/1200], Training Loss: 7.392915434597558, Validation Loss: 14.12344741821289\n",
      "Epoch [314/1200], Training Loss: 7.282704518274745, Validation Loss: 14.213457107543945\n",
      "Epoch [315/1200], Training Loss: 7.173823978322827, Validation Loss: 13.812917709350586\n",
      "Epoch [316/1200], Training Loss: 7.067205394479241, Validation Loss: 13.771997451782227\n",
      "Epoch [317/1200], Training Loss: 6.963681576835461, Validation Loss: 13.87817096710205\n",
      "Epoch [318/1200], Training Loss: 6.861177424170063, Validation Loss: 13.527215957641602\n",
      "Epoch [319/1200], Training Loss: 6.760819411450742, Validation Loss: 13.469928741455078\n",
      "Epoch [320/1200], Training Loss: 6.6633011221736975, Validation Loss: 13.57275104522705\n",
      "Epoch [321/1200], Training Loss: 6.566551408788988, Validation Loss: 13.264669418334961\n",
      "Epoch [322/1200], Training Loss: 6.471681368028515, Validation Loss: 13.215188980102539\n",
      "Epoch [323/1200], Training Loss: 6.379601697516809, Validation Loss: 13.298349380493164\n",
      "Epoch [324/1200], Training Loss: 6.28802282772497, Validation Loss: 13.01857852935791\n",
      "Epoch [325/1200], Training Loss: 6.1981725205366915, Validation Loss: 12.967184066772461\n",
      "Epoch [326/1200], Training Loss: 6.1111437701944435, Validation Loss: 13.019957542419434\n",
      "Epoch [327/1200], Training Loss: 6.024531780860344, Validation Loss: 12.768387794494629\n",
      "Epoch [328/1200], Training Loss: 5.939477560506279, Validation Loss: 12.735769271850586\n",
      "Epoch [329/1200], Training Loss: 5.85728103964738, Validation Loss: 12.748527526855469\n",
      "Epoch [330/1200], Training Loss: 5.775220335144074, Validation Loss: 12.5398530960083\n",
      "Epoch [331/1200], Training Loss: 5.694911595085629, Validation Loss: 12.479391098022461\n",
      "Epoch [332/1200], Training Loss: 5.6170375569601525, Validation Loss: 12.41116714477539\n",
      "Epoch [333/1200], Training Loss: 5.539274134391261, Validation Loss: 12.251554489135742\n",
      "Epoch [334/1200], Training Loss: 5.463531587700109, Validation Loss: 12.231121063232422\n",
      "Epoch [335/1200], Training Loss: 5.389885492957013, Validation Loss: 12.082452774047852\n",
      "Epoch [336/1200], Training Loss: 5.315818720284258, Validation Loss: 12.038115501403809\n",
      "Epoch [337/1200], Training Loss: 5.244793639859962, Validation Loss: 11.966699600219727\n",
      "Epoch [338/1200], Training Loss: 5.174986666487183, Validation Loss: 11.66307544708252\n",
      "Epoch [339/1200], Training Loss: 5.1047568439683895, Validation Loss: 11.785359382629395\n",
      "Epoch [340/1200], Training Loss: 5.038840718471723, Validation Loss: 11.737746238708496\n",
      "Epoch [341/1200], Training Loss: 4.971788468758876, Validation Loss: 11.952765464782715\n",
      "Epoch [342/1200], Training Loss: 4.907590448002364, Validation Loss: 12.138422012329102\n",
      "Epoch [343/1200], Training Loss: 4.8454360373882945, Validation Loss: 11.441766738891602\n",
      "Epoch [344/1200], Training Loss: 4.776379446607407, Validation Loss: 10.627388954162598\n",
      "Epoch [345/1200], Training Loss: 4.717619716536263, Validation Loss: 10.414166450500488\n",
      "Epoch [346/1200], Training Loss: 4.660896660488937, Validation Loss: 11.089593887329102\n",
      "Epoch [347/1200], Training Loss: 4.602836366772342, Validation Loss: 11.578932762145996\n",
      "Epoch [348/1200], Training Loss: 4.54413186371245, Validation Loss: 12.010494232177734\n",
      "Epoch [349/1200], Training Loss: 4.489559068904982, Validation Loss: 12.914573669433594\n",
      "Epoch [350/1200], Training Loss: 4.438917352015307, Validation Loss: 11.393959999084473\n",
      "Epoch [351/1200], Training Loss: 4.367409768240754, Validation Loss: 10.116074562072754\n",
      "Epoch [352/1200], Training Loss: 4.309354792288538, Validation Loss: 9.754436492919922\n",
      "Epoch [353/1200], Training Loss: 4.25887637259966, Validation Loss: 9.576010704040527\n",
      "Epoch [354/1200], Training Loss: 4.212488457564514, Validation Loss: 10.29991340637207\n",
      "Epoch [355/1200], Training Loss: 4.162254621043111, Validation Loss: 11.013606071472168\n",
      "Epoch [356/1200], Training Loss: 4.110944618317714, Validation Loss: 11.541753768920898\n",
      "Epoch [357/1200], Training Loss: 4.064236319192825, Validation Loss: 12.78574275970459\n",
      "Epoch [358/1200], Training Loss: 4.022820589940219, Validation Loss: 11.264178276062012\n",
      "Epoch [359/1200], Training Loss: 3.955268904212698, Validation Loss: 9.79426097869873\n",
      "Epoch [360/1200], Training Loss: 3.9006860205617206, Validation Loss: 9.421465873718262\n",
      "Epoch [361/1200], Training Loss: 3.8526851288521318, Validation Loss: 9.037370681762695\n",
      "Epoch [362/1200], Training Loss: 3.8134426860803874, Validation Loss: 9.333352088928223\n",
      "Epoch [363/1200], Training Loss: 3.772749540017044, Validation Loss: 10.423555374145508\n",
      "Epoch [364/1200], Training Loss: 3.7284316689338994, Validation Loss: 11.225029945373535\n",
      "Epoch [365/1200], Training Loss: 3.683971972738066, Validation Loss: 12.072343826293945\n",
      "Epoch [366/1200], Training Loss: 3.648572815566413, Validation Loss: 12.660504341125488\n",
      "Epoch [367/1200], Training Loss: 3.6016118463588866, Validation Loss: 10.25166130065918\n",
      "Epoch [368/1200], Training Loss: 3.541703168377923, Validation Loss: 9.178773880004883\n",
      "Epoch [369/1200], Training Loss: 3.4968776537151554, Validation Loss: 9.266616821289062\n",
      "Epoch [370/1200], Training Loss: 3.4614418312575337, Validation Loss: 9.824694633483887\n",
      "Early stopping at epoch 370\n",
      "Final Test Loss: 12.175009727478027\n",
      "Hyperparameters: input_size=7, hidden_size=8, num_layers=2, learning_rate=0.0002, window_size=8\n",
      "Epoch [1/1200], Training Loss: 27210.47555843503, Validation Loss: 26703.421875\n",
      "Epoch [2/1200], Training Loss: 25271.3054634928, Validation Loss: 24956.578125\n",
      "Epoch [3/1200], Training Loss: 23770.232971451383, Validation Loss: 23559.359375\n",
      "Epoch [4/1200], Training Loss: 22603.25372561285, Validation Loss: 22471.083984375\n",
      "Epoch [5/1200], Training Loss: 21712.776925467544, Validation Loss: 21635.529296875\n",
      "Epoch [6/1200], Training Loss: 21035.16480492619, Validation Loss: 20994.365234375\n",
      "Epoch [7/1200], Training Loss: 20515.116288140347, Validation Loss: 20498.16796875\n",
      "Epoch [8/1200], Training Loss: 20110.627765666308, Validation Loss: 20109.3671875\n",
      "Epoch [9/1200], Training Loss: 19791.49989019708, Validation Loss: 19800.736328125\n",
      "Epoch [10/1200], Training Loss: 19536.37807374594, Validation Loss: 19552.697265625\n",
      "Epoch [11/1200], Training Loss: 19329.92360182121, Validation Loss: 19351.08203125\n",
      "Epoch [12/1200], Training Loss: 19161.108185730212, Validation Loss: 19185.580078125\n",
      "Epoch [13/1200], Training Loss: 19021.813302042567, Validation Loss: 19048.576171875\n",
      "Epoch [14/1200], Training Loss: 18905.938450209447, Validation Loss: 18934.228515625\n",
      "Epoch [15/1200], Training Loss: 18808.821523866976, Validation Loss: 18838.1484375\n",
      "Epoch [16/1200], Training Loss: 18726.92885683337, Validation Loss: 18756.94140625\n",
      "Epoch [17/1200], Training Loss: 18657.50653159342, Validation Loss: 18687.947265625\n",
      "Epoch [18/1200], Training Loss: 18598.342790265313, Validation Loss: 18629.052734375\n",
      "Epoch [19/1200], Training Loss: 18547.684432951293, Validation Loss: 18578.501953125\n",
      "Epoch [20/1200], Training Loss: 18504.192438087266, Validation Loss: 18535.0234375\n",
      "Epoch [21/1200], Training Loss: 18466.657134108544, Validation Loss: 18497.455078125\n",
      "Epoch [22/1200], Training Loss: 18434.203710957485, Validation Loss: 18464.935546875\n",
      "Epoch [23/1200], Training Loss: 18406.030931867277, Validation Loss: 18436.6796875\n",
      "Epoch [24/1200], Training Loss: 18381.544847803827, Validation Loss: 18412.044921875\n",
      "Epoch [25/1200], Training Loss: 18360.173447082103, Validation Loss: 18390.580078125\n",
      "Epoch [26/1200], Training Loss: 18341.51389724992, Validation Loss: 18371.8203125\n",
      "Epoch [27/1200], Training Loss: 18325.17617563011, Validation Loss: 18355.3515625\n",
      "Epoch [28/1200], Training Loss: 18310.88819149217, Validation Loss: 18340.9296875\n",
      "Epoch [29/1200], Training Loss: 18298.320590381863, Validation Loss: 18328.263671875\n",
      "Epoch [30/1200], Training Loss: 18287.294593264705, Validation Loss: 18317.13671875\n",
      "Epoch [31/1200], Training Loss: 18277.591306071703, Validation Loss: 18307.3359375\n",
      "Epoch [32/1200], Training Loss: 18269.02955009226, Validation Loss: 18298.708984375\n",
      "Epoch [33/1200], Training Loss: 18261.5261020728, Validation Loss: 18291.10546875\n",
      "Epoch [34/1200], Training Loss: 18254.86595196219, Validation Loss: 18284.369140625\n",
      "Epoch [35/1200], Training Loss: 18249.00104395873, Validation Loss: 18278.431640625\n",
      "Epoch [36/1200], Training Loss: 18243.82152414795, Validation Loss: 18273.1953125\n",
      "Epoch [37/1200], Training Loss: 18239.236804035158, Validation Loss: 18268.564453125\n",
      "Epoch [38/1200], Training Loss: 18235.169623166996, Validation Loss: 18264.451171875\n",
      "Epoch [39/1200], Training Loss: 18231.596846224118, Validation Loss: 18260.82421875\n",
      "Epoch [40/1200], Training Loss: 18228.408454698136, Validation Loss: 18257.599609375\n",
      "Epoch [41/1200], Training Loss: 18225.597271404327, Validation Loss: 18254.74609375\n",
      "Epoch [42/1200], Training Loss: 18223.113943423297, Validation Loss: 18252.232421875\n",
      "Epoch [43/1200], Training Loss: 18220.900185009064, Validation Loss: 18249.99609375\n",
      "Epoch [44/1200], Training Loss: 18218.935590038454, Validation Loss: 18248.03125\n",
      "Epoch [45/1200], Training Loss: 18217.233595585247, Validation Loss: 18246.275390625\n",
      "Epoch [46/1200], Training Loss: 18215.710699795927, Validation Loss: 18244.716796875\n",
      "Epoch [47/1200], Training Loss: 18214.35913605563, Validation Loss: 18243.3359375\n",
      "Epoch [48/1200], Training Loss: 18213.15573264698, Validation Loss: 18242.123046875\n",
      "Epoch [49/1200], Training Loss: 18212.079415565888, Validation Loss: 18241.013671875\n",
      "Epoch [50/1200], Training Loss: 18211.13684992593, Validation Loss: 18240.0625\n",
      "Epoch [51/1200], Training Loss: 18210.280260304768, Validation Loss: 18239.2109375\n",
      "Epoch [52/1200], Training Loss: 18209.50718088608, Validation Loss: 18238.42578125\n",
      "Epoch [53/1200], Training Loss: 18208.826231527066, Validation Loss: 18237.748046875\n",
      "Epoch [54/1200], Training Loss: 18208.22679931417, Validation Loss: 18237.13671875\n",
      "Epoch [55/1200], Training Loss: 18207.697859399086, Validation Loss: 18236.58984375\n",
      "Epoch [56/1200], Training Loss: 18207.22391145725, Validation Loss: 18236.119140625\n",
      "Epoch [57/1200], Training Loss: 18206.804290834116, Validation Loss: 18235.6875\n",
      "Epoch [58/1200], Training Loss: 18206.428330057883, Validation Loss: 18235.296875\n",
      "Epoch [59/1200], Training Loss: 18206.096965264416, Validation Loss: 18234.98046875\n",
      "Epoch [60/1200], Training Loss: 18205.793697576057, Validation Loss: 18234.6640625\n",
      "Epoch [61/1200], Training Loss: 18205.526537432732, Validation Loss: 18234.396484375\n",
      "Epoch [62/1200], Training Loss: 18205.2956686983, Validation Loss: 18234.142578125\n",
      "Epoch [63/1200], Training Loss: 18205.091781811665, Validation Loss: 18233.947265625\n",
      "Epoch [64/1200], Training Loss: 18204.90534123681, Validation Loss: 18233.763671875\n",
      "Epoch [65/1200], Training Loss: 18204.73904870489, Validation Loss: 18233.583984375\n",
      "Epoch [66/1200], Training Loss: 18203.911520093414, Validation Loss: 18232.32421875\n",
      "Epoch [67/1200], Training Loss: 16245.158953793361, Validation Loss: 17098.017578125\n",
      "Epoch [68/1200], Training Loss: 15372.75171689927, Validation Loss: 16286.599609375\n",
      "Epoch [69/1200], Training Loss: 14738.18887445444, Validation Loss: 15337.181640625\n",
      "Epoch [70/1200], Training Loss: 14145.700316283053, Validation Loss: 14761.61328125\n",
      "Epoch [71/1200], Training Loss: 13594.488441717926, Validation Loss: 14221.203125\n",
      "Epoch [72/1200], Training Loss: 13082.042669304292, Validation Loss: 13680.736328125\n",
      "Epoch [73/1200], Training Loss: 12600.86503095002, Validation Loss: 12898.6728515625\n",
      "Epoch [74/1200], Training Loss: 12139.487268357636, Validation Loss: 12598.11328125\n",
      "Epoch [75/1200], Training Loss: 11698.695853613393, Validation Loss: 11931.4521484375\n",
      "Epoch [76/1200], Training Loss: 11289.490242266998, Validation Loss: 11418.6669921875\n",
      "Epoch [77/1200], Training Loss: 10905.256330881039, Validation Loss: 10990.2177734375\n",
      "Epoch [78/1200], Training Loss: 10543.678368441435, Validation Loss: 10635.146484375\n",
      "Epoch [79/1200], Training Loss: 10203.903021379174, Validation Loss: 10300.509765625\n",
      "Epoch [80/1200], Training Loss: 9884.449950011754, Validation Loss: 9998.6181640625\n",
      "Epoch [81/1200], Training Loss: 9585.259838018856, Validation Loss: 9704.8935546875\n",
      "Epoch [82/1200], Training Loss: 9305.814556977319, Validation Loss: 9427.1943359375\n",
      "Epoch [83/1200], Training Loss: 9043.980050483488, Validation Loss: 9179.1923828125\n",
      "Epoch [84/1200], Training Loss: 8798.73468171439, Validation Loss: 8937.4775390625\n",
      "Epoch [85/1200], Training Loss: 8568.26010133725, Validation Loss: 8701.533203125\n",
      "Epoch [86/1200], Training Loss: 8350.31520921802, Validation Loss: 8484.6865234375\n",
      "Epoch [87/1200], Training Loss: 8145.3482573975725, Validation Loss: 8286.140625\n",
      "Epoch [88/1200], Training Loss: 7952.373084700057, Validation Loss: 8099.39306640625\n",
      "Epoch [89/1200], Training Loss: 7580.585588606197, Validation Loss: 9292.91015625\n",
      "Epoch [90/1200], Training Loss: 7352.680510769073, Validation Loss: 8038.8740234375\n",
      "Epoch [91/1200], Training Loss: 7112.180896424545, Validation Loss: 7413.54296875\n",
      "Epoch [92/1200], Training Loss: 6891.974796072618, Validation Loss: 7063.0244140625\n",
      "Epoch [93/1200], Training Loss: 6683.489364952305, Validation Loss: 6795.63330078125\n",
      "Epoch [94/1200], Training Loss: 6482.459690963669, Validation Loss: 6589.888671875\n",
      "Epoch [95/1200], Training Loss: 6287.610963748507, Validation Loss: 6417.0830078125\n",
      "Epoch [96/1200], Training Loss: 6098.713573136325, Validation Loss: 6250.8330078125\n",
      "Epoch [97/1200], Training Loss: 5915.399098055073, Validation Loss: 6079.81396484375\n",
      "Epoch [98/1200], Training Loss: 5737.636198291652, Validation Loss: 5908.818359375\n",
      "Epoch [99/1200], Training Loss: 5565.599998567047, Validation Loss: 5740.52294921875\n",
      "Epoch [100/1200], Training Loss: 5399.573595104764, Validation Loss: 5573.8525390625\n",
      "Epoch [101/1200], Training Loss: 5239.864448323808, Validation Loss: 5412.6728515625\n",
      "Epoch [102/1200], Training Loss: 5086.140652766929, Validation Loss: 5265.388671875\n",
      "Epoch [103/1200], Training Loss: 4937.01129482435, Validation Loss: 5116.7041015625\n",
      "Epoch [104/1200], Training Loss: 4791.453175092045, Validation Loss: 4958.7314453125\n",
      "Epoch [105/1200], Training Loss: 4649.459240390708, Validation Loss: 4804.93408203125\n",
      "Epoch [106/1200], Training Loss: 4511.129121314358, Validation Loss: 4653.7705078125\n",
      "Epoch [107/1200], Training Loss: 4376.488936854598, Validation Loss: 4507.82568359375\n",
      "Epoch [108/1200], Training Loss: 4245.399149545497, Validation Loss: 4369.345703125\n",
      "Epoch [109/1200], Training Loss: 4117.66247840049, Validation Loss: 4236.47412109375\n",
      "Epoch [110/1200], Training Loss: 3993.080668785939, Validation Loss: 4108.7109375\n",
      "Epoch [111/1200], Training Loss: 3871.5282902260296, Validation Loss: 3984.840087890625\n",
      "Epoch [112/1200], Training Loss: 3753.1534042584626, Validation Loss: 3865.128173828125\n",
      "Epoch [113/1200], Training Loss: 3638.183947336113, Validation Loss: 3749.82666015625\n",
      "Epoch [114/1200], Training Loss: 3526.539925059025, Validation Loss: 3638.80908203125\n",
      "Epoch [115/1200], Training Loss: 3418.1419055190167, Validation Loss: 3532.3515625\n",
      "Epoch [116/1200], Training Loss: 3313.0652655054973, Validation Loss: 3430.701416015625\n",
      "Epoch [117/1200], Training Loss: 3211.323114933953, Validation Loss: 3335.523193359375\n",
      "Epoch [118/1200], Training Loss: 3112.7733378932353, Validation Loss: 3248.802001953125\n",
      "Epoch [119/1200], Training Loss: 3017.254975034754, Validation Loss: 3165.866943359375\n",
      "Epoch [120/1200], Training Loss: 2924.831642950032, Validation Loss: 3087.12890625\n",
      "Epoch [121/1200], Training Loss: 2835.606896798295, Validation Loss: 3012.071044921875\n",
      "Epoch [122/1200], Training Loss: 2749.9046812425327, Validation Loss: 2939.3388671875\n",
      "Epoch [123/1200], Training Loss: 2667.6934316442876, Validation Loss: 2869.333984375\n",
      "Epoch [124/1200], Training Loss: 2588.7670545738706, Validation Loss: 2801.75048828125\n",
      "Epoch [125/1200], Training Loss: 2513.060216786094, Validation Loss: 2737.1787109375\n",
      "Epoch [126/1200], Training Loss: 2440.6260658216393, Validation Loss: 2686.6123046875\n",
      "Epoch [127/1200], Training Loss: 2371.713813913946, Validation Loss: 2631.266357421875\n",
      "Epoch [128/1200], Training Loss: 2305.9378595412763, Validation Loss: 2572.206787109375\n",
      "Epoch [129/1200], Training Loss: 2241.8688485332377, Validation Loss: 2511.456298828125\n",
      "Epoch [130/1200], Training Loss: 2179.6511372537784, Validation Loss: 2468.44140625\n",
      "Epoch [131/1200], Training Loss: 2119.2182339514493, Validation Loss: 2419.59033203125\n",
      "Epoch [132/1200], Training Loss: 2060.5378078819494, Validation Loss: 2361.154296875\n",
      "Epoch [133/1200], Training Loss: 2003.5269752032625, Validation Loss: 2297.740966796875\n",
      "Epoch [134/1200], Training Loss: 1948.143470879308, Validation Loss: 2232.970458984375\n",
      "Epoch [135/1200], Training Loss: 1894.3758320626487, Validation Loss: 2168.2392578125\n",
      "Epoch [136/1200], Training Loss: 1842.2207701403242, Validation Loss: 2104.293701171875\n",
      "Epoch [137/1200], Training Loss: 1791.7073205642043, Validation Loss: 2042.0965576171875\n",
      "Epoch [138/1200], Training Loss: 1742.8681726795521, Validation Loss: 1982.6820068359375\n",
      "Epoch [139/1200], Training Loss: 1695.7291329498753, Validation Loss: 1926.4989013671875\n",
      "Epoch [140/1200], Training Loss: 1650.3152106003074, Validation Loss: 1873.266357421875\n",
      "Epoch [141/1200], Training Loss: 1606.6151532055467, Validation Loss: 1822.3538818359375\n",
      "Epoch [142/1200], Training Loss: 1564.5953492642825, Validation Loss: 1772.9595947265625\n",
      "Epoch [143/1200], Training Loss: 1524.1655010159582, Validation Loss: 1724.4765625\n",
      "Epoch [144/1200], Training Loss: 1485.192564375653, Validation Loss: 1676.7159423828125\n",
      "Epoch [145/1200], Training Loss: 1447.5603385534218, Validation Loss: 1630.1842041015625\n",
      "Epoch [146/1200], Training Loss: 1411.1723929177904, Validation Loss: 1585.936279296875\n",
      "Epoch [147/1200], Training Loss: 1375.8838283680066, Validation Loss: 1544.7327880859375\n",
      "Epoch [148/1200], Training Loss: 1341.5615861833098, Validation Loss: 1506.4102783203125\n",
      "Epoch [149/1200], Training Loss: 1308.0999352854838, Validation Loss: 1470.4056396484375\n",
      "Epoch [150/1200], Training Loss: 1275.4367041907137, Validation Loss: 1436.4013671875\n",
      "Epoch [151/1200], Training Loss: 1243.5347743955094, Validation Loss: 1402.9993896484375\n",
      "Epoch [152/1200], Training Loss: 1212.4001109567819, Validation Loss: 1368.66455078125\n",
      "Epoch [153/1200], Training Loss: 1182.0579993744134, Validation Loss: 1332.83447265625\n",
      "Epoch [154/1200], Training Loss: 1152.4854076945476, Validation Loss: 1297.59716796875\n",
      "Epoch [155/1200], Training Loss: 1123.6684289456869, Validation Loss: 1265.0418701171875\n",
      "Epoch [156/1200], Training Loss: 1095.5902057774504, Validation Loss: 1235.295654296875\n",
      "Epoch [157/1200], Training Loss: 1068.2413219371288, Validation Loss: 1207.656005859375\n",
      "Epoch [158/1200], Training Loss: 1041.622480443601, Validation Loss: 1182.06591796875\n",
      "Epoch [159/1200], Training Loss: 1015.7280585721703, Validation Loss: 1154.2208251953125\n",
      "Epoch [160/1200], Training Loss: 990.5002674206975, Validation Loss: 1123.265380859375\n",
      "Epoch [161/1200], Training Loss: 965.8650749728981, Validation Loss: 1098.060546875\n",
      "Epoch [162/1200], Training Loss: 941.9880759284688, Validation Loss: 1069.3880615234375\n",
      "Epoch [163/1200], Training Loss: 918.7264513642403, Validation Loss: 1043.169677734375\n",
      "Epoch [164/1200], Training Loss: 896.1161816280512, Validation Loss: 1016.7613525390625\n",
      "Epoch [165/1200], Training Loss: 874.0208113955383, Validation Loss: 991.1832885742188\n",
      "Epoch [166/1200], Training Loss: 852.4661005150026, Validation Loss: 965.2147827148438\n",
      "Epoch [167/1200], Training Loss: 831.4606865038907, Validation Loss: 939.0496826171875\n",
      "Epoch [168/1200], Training Loss: 810.9934307111339, Validation Loss: 913.5724487304688\n",
      "Epoch [169/1200], Training Loss: 791.0303818864337, Validation Loss: 889.512939453125\n",
      "Epoch [170/1200], Training Loss: 771.5335595599103, Validation Loss: 867.1005859375\n",
      "Epoch [171/1200], Training Loss: 752.4856196939413, Validation Loss: 846.2339477539062\n",
      "Epoch [172/1200], Training Loss: 733.8701830461944, Validation Loss: 826.7608032226562\n",
      "Epoch [173/1200], Training Loss: 715.6714740287593, Validation Loss: 808.4774169921875\n",
      "Epoch [174/1200], Training Loss: 697.8768507698246, Validation Loss: 790.943115234375\n",
      "Epoch [175/1200], Training Loss: 680.4854599515157, Validation Loss: 773.6071166992188\n",
      "Epoch [176/1200], Training Loss: 663.4981989460262, Validation Loss: 756.2237548828125\n",
      "Epoch [177/1200], Training Loss: 646.913153323555, Validation Loss: 738.8929443359375\n",
      "Epoch [178/1200], Training Loss: 630.726161961575, Validation Loss: 721.73095703125\n",
      "Epoch [179/1200], Training Loss: 614.927134177984, Validation Loss: 704.8140869140625\n",
      "Epoch [180/1200], Training Loss: 599.5013279127654, Validation Loss: 688.219970703125\n",
      "Epoch [181/1200], Training Loss: 584.4270568680718, Validation Loss: 671.9841918945312\n",
      "Epoch [182/1200], Training Loss: 569.6889341354766, Validation Loss: 656.1329956054688\n",
      "Epoch [183/1200], Training Loss: 555.2679652053309, Validation Loss: 640.66943359375\n",
      "Epoch [184/1200], Training Loss: 541.1491466782683, Validation Loss: 625.606689453125\n",
      "Epoch [185/1200], Training Loss: 527.3272209021704, Validation Loss: 610.9462890625\n",
      "Epoch [186/1200], Training Loss: 513.7970354838342, Validation Loss: 596.6787109375\n",
      "Epoch [187/1200], Training Loss: 500.55591197656594, Validation Loss: 582.7745971679688\n",
      "Epoch [188/1200], Training Loss: 487.6031316983769, Validation Loss: 569.1893920898438\n",
      "Epoch [189/1200], Training Loss: 474.9317722084376, Validation Loss: 555.88037109375\n",
      "Epoch [190/1200], Training Loss: 462.5408150034332, Validation Loss: 542.8329467773438\n",
      "Epoch [191/1200], Training Loss: 450.4261951175357, Validation Loss: 530.0571899414062\n",
      "Epoch [192/1200], Training Loss: 438.58782154428695, Validation Loss: 517.5675048828125\n",
      "Epoch [193/1200], Training Loss: 427.02629522554633, Validation Loss: 505.3502502441406\n",
      "Epoch [194/1200], Training Loss: 415.7399542091216, Validation Loss: 493.3714599609375\n",
      "Epoch [195/1200], Training Loss: 404.7237701008058, Validation Loss: 481.6007995605469\n",
      "Epoch [196/1200], Training Loss: 393.9740530472382, Validation Loss: 470.0018615722656\n",
      "Epoch [197/1200], Training Loss: 383.4813804343697, Validation Loss: 458.5488586425781\n",
      "Epoch [198/1200], Training Loss: 373.23679887170846, Validation Loss: 447.21685791015625\n",
      "Epoch [199/1200], Training Loss: 363.2299998155347, Validation Loss: 435.9803771972656\n",
      "Epoch [200/1200], Training Loss: 353.4496350719804, Validation Loss: 424.8246765136719\n",
      "Epoch [201/1200], Training Loss: 343.88781403899776, Validation Loss: 413.7431335449219\n",
      "Epoch [202/1200], Training Loss: 334.53669682868184, Validation Loss: 402.74078369140625\n",
      "Epoch [203/1200], Training Loss: 325.3888960342001, Validation Loss: 391.8395690917969\n",
      "Epoch [204/1200], Training Loss: 316.4419738434611, Validation Loss: 381.0866394042969\n",
      "Epoch [205/1200], Training Loss: 307.6964061032158, Validation Loss: 370.53656005859375\n",
      "Epoch [206/1200], Training Loss: 299.15100621903804, Validation Loss: 360.2560729980469\n",
      "Epoch [207/1200], Training Loss: 290.8103303499034, Validation Loss: 350.3020935058594\n",
      "Epoch [208/1200], Training Loss: 282.67668866011627, Validation Loss: 340.7088317871094\n",
      "Epoch [209/1200], Training Loss: 274.75226593607823, Validation Loss: 331.49462890625\n",
      "Epoch [210/1200], Training Loss: 267.035083084107, Validation Loss: 322.6493835449219\n",
      "Epoch [211/1200], Training Loss: 259.52279834197583, Validation Loss: 314.15020751953125\n",
      "Epoch [212/1200], Training Loss: 252.2111931129844, Validation Loss: 305.96881103515625\n",
      "Epoch [213/1200], Training Loss: 245.094725269179, Validation Loss: 298.0621643066406\n",
      "Epoch [214/1200], Training Loss: 238.17700359460946, Validation Loss: 290.3572082519531\n",
      "Epoch [215/1200], Training Loss: 231.45135259730964, Validation Loss: 282.7890319824219\n",
      "Epoch [216/1200], Training Loss: 224.91792360565813, Validation Loss: 275.3299865722656\n",
      "Epoch [217/1200], Training Loss: 218.57598598278327, Validation Loss: 267.99127197265625\n",
      "Epoch [218/1200], Training Loss: 212.42464951237125, Validation Loss: 260.8034362792969\n",
      "Epoch [219/1200], Training Loss: 206.46174316815717, Validation Loss: 253.80967712402344\n",
      "Epoch [220/1200], Training Loss: 200.6834438387748, Validation Loss: 247.0500030517578\n",
      "Epoch [221/1200], Training Loss: 195.08472655716136, Validation Loss: 240.55665588378906\n",
      "Epoch [222/1200], Training Loss: 189.66134255809152, Validation Loss: 234.3406982421875\n",
      "Epoch [223/1200], Training Loss: 184.40846543018932, Validation Loss: 228.39891052246094\n",
      "Epoch [224/1200], Training Loss: 179.32117791997817, Validation Loss: 222.70721435546875\n",
      "Epoch [225/1200], Training Loss: 174.39213155197368, Validation Loss: 217.2218780517578\n",
      "Epoch [226/1200], Training Loss: 169.61787300467185, Validation Loss: 211.89468383789062\n",
      "Epoch [227/1200], Training Loss: 164.99564634472617, Validation Loss: 206.68563842773438\n",
      "Epoch [228/1200], Training Loss: 160.5192311519843, Validation Loss: 201.56182861328125\n",
      "Epoch [229/1200], Training Loss: 156.18458195810098, Validation Loss: 196.50477600097656\n",
      "Epoch [230/1200], Training Loss: 151.9854635454686, Validation Loss: 191.497802734375\n",
      "Epoch [231/1200], Training Loss: 147.9104354415608, Validation Loss: 186.53994750976562\n",
      "Epoch [232/1200], Training Loss: 143.9473772067484, Validation Loss: 181.65786743164062\n",
      "Epoch [233/1200], Training Loss: 140.0839028535595, Validation Loss: 176.89266967773438\n",
      "Epoch [234/1200], Training Loss: 136.30731400227734, Validation Loss: 172.27294921875\n",
      "Epoch [235/1200], Training Loss: 132.60891046014385, Validation Loss: 167.81503295898438\n",
      "Epoch [236/1200], Training Loss: 128.9891900214759, Validation Loss: 163.54339599609375\n",
      "Epoch [237/1200], Training Loss: 125.45422823081061, Validation Loss: 159.48904418945312\n",
      "Epoch [238/1200], Training Loss: 122.01054146186758, Validation Loss: 155.69398498535156\n",
      "Epoch [239/1200], Training Loss: 118.6653283177587, Validation Loss: 152.20510864257812\n",
      "Epoch [240/1200], Training Loss: 115.42097474127907, Validation Loss: 149.01345825195312\n",
      "Epoch [241/1200], Training Loss: 112.27381113202308, Validation Loss: 145.98460388183594\n",
      "Epoch [242/1200], Training Loss: 109.21409503994907, Validation Loss: 143.01751708984375\n",
      "Epoch [243/1200], Training Loss: 106.2401161671191, Validation Loss: 140.11883544921875\n",
      "Epoch [244/1200], Training Loss: 103.35758225656926, Validation Loss: 137.30792236328125\n",
      "Epoch [245/1200], Training Loss: 100.56431986784798, Validation Loss: 134.5837860107422\n",
      "Epoch [246/1200], Training Loss: 97.85424463062479, Validation Loss: 131.93853759765625\n",
      "Epoch [247/1200], Training Loss: 95.22404020242806, Validation Loss: 129.3458251953125\n",
      "Epoch [248/1200], Training Loss: 92.67054946202855, Validation Loss: 126.76494598388672\n",
      "Epoch [249/1200], Training Loss: 90.19058474042522, Validation Loss: 124.159912109375\n",
      "Epoch [250/1200], Training Loss: 87.78252886269588, Validation Loss: 121.49102020263672\n",
      "Epoch [251/1200], Training Loss: 85.44376859801844, Validation Loss: 118.74124145507812\n",
      "Epoch [252/1200], Training Loss: 83.17343779154444, Validation Loss: 115.91210174560547\n",
      "Epoch [253/1200], Training Loss: 80.97098036276739, Validation Loss: 113.00640869140625\n",
      "Epoch [254/1200], Training Loss: 78.83662166353399, Validation Loss: 110.0154037475586\n",
      "Epoch [255/1200], Training Loss: 76.7697983975002, Validation Loss: 106.9132080078125\n",
      "Epoch [256/1200], Training Loss: 74.7680697405219, Validation Loss: 103.69852447509766\n",
      "Epoch [257/1200], Training Loss: 72.8299643640859, Validation Loss: 100.43179321289062\n",
      "Epoch [258/1200], Training Loss: 70.95400452528858, Validation Loss: 97.22914123535156\n",
      "Epoch [259/1200], Training Loss: 69.13897465406028, Validation Loss: 94.2061767578125\n",
      "Epoch [260/1200], Training Loss: 67.38412956397265, Validation Loss: 91.4311752319336\n",
      "Epoch [261/1200], Training Loss: 65.68921279515891, Validation Loss: 88.90579986572266\n",
      "Epoch [262/1200], Training Loss: 64.05115120331017, Validation Loss: 86.59015655517578\n",
      "Epoch [263/1200], Training Loss: 62.4676773290573, Validation Loss: 84.45606231689453\n",
      "Epoch [264/1200], Training Loss: 60.93759184420349, Validation Loss: 82.49740600585938\n",
      "Epoch [265/1200], Training Loss: 59.45927392686421, Validation Loss: 80.69694519042969\n",
      "Epoch [266/1200], Training Loss: 58.03067479377881, Validation Loss: 79.01805877685547\n",
      "Epoch [267/1200], Training Loss: 56.65020452685963, Validation Loss: 77.4085464477539\n",
      "Epoch [268/1200], Training Loss: 55.315495456754526, Validation Loss: 75.83358764648438\n",
      "Epoch [269/1200], Training Loss: 54.024673813307864, Validation Loss: 74.2735595703125\n",
      "Epoch [270/1200], Training Loss: 52.776710355653556, Validation Loss: 72.73199462890625\n",
      "Epoch [271/1200], Training Loss: 51.57045596929307, Validation Loss: 71.22030639648438\n",
      "Epoch [272/1200], Training Loss: 50.403542184077274, Validation Loss: 69.75517272949219\n",
      "Epoch [273/1200], Training Loss: 49.273405920739236, Validation Loss: 68.33656311035156\n",
      "Epoch [274/1200], Training Loss: 48.17808890472297, Validation Loss: 66.99828338623047\n",
      "Epoch [275/1200], Training Loss: 47.11469346698203, Validation Loss: 65.68035125732422\n",
      "Epoch [276/1200], Training Loss: 46.08230354832465, Validation Loss: 64.51237487792969\n",
      "Epoch [277/1200], Training Loss: 45.07748708931556, Validation Loss: 63.217742919921875\n",
      "Epoch [278/1200], Training Loss: 44.1067483167603, Validation Loss: 62.372196197509766\n",
      "Epoch [279/1200], Training Loss: 43.14986232702298, Validation Loss: 60.75474166870117\n",
      "Epoch [280/1200], Training Loss: 42.29316662747139, Validation Loss: 60.83974075317383\n",
      "Epoch [281/1200], Training Loss: 41.32111480868219, Validation Loss: 58.51511764526367\n",
      "Epoch [282/1200], Training Loss: 40.4451966313255, Validation Loss: 58.572017669677734\n",
      "Epoch [283/1200], Training Loss: 39.57707035351021, Validation Loss: 57.3851203918457\n",
      "Epoch [284/1200], Training Loss: 38.77111080006994, Validation Loss: 57.53835678100586\n",
      "Epoch [285/1200], Training Loss: 37.91698877977166, Validation Loss: 56.1585578918457\n",
      "Epoch [286/1200], Training Loss: 37.12845727571105, Validation Loss: 55.98032760620117\n",
      "Epoch [287/1200], Training Loss: 36.32947205604056, Validation Loss: 55.270023345947266\n",
      "Epoch [288/1200], Training Loss: 35.56941306949885, Validation Loss: 54.81483459472656\n",
      "Epoch [289/1200], Training Loss: 34.80919881427266, Validation Loss: 54.368526458740234\n",
      "Epoch [290/1200], Training Loss: 34.07435848786654, Validation Loss: 53.50389862060547\n",
      "Epoch [291/1200], Training Loss: 33.34920943532742, Validation Loss: 53.25347900390625\n",
      "Epoch [292/1200], Training Loss: 32.64648093934262, Validation Loss: 52.10536575317383\n",
      "Epoch [293/1200], Training Loss: 31.95588835401988, Validation Loss: 51.80965805053711\n",
      "Epoch [294/1200], Training Loss: 31.292746536119285, Validation Loss: 51.47478103637695\n",
      "Epoch [295/1200], Training Loss: 30.638996307249737, Validation Loss: 50.54910659790039\n",
      "Epoch [296/1200], Training Loss: 29.992036782714, Validation Loss: 50.501670837402344\n",
      "Epoch [297/1200], Training Loss: 29.373612195013518, Validation Loss: 49.444271087646484\n",
      "Epoch [298/1200], Training Loss: 28.760637701425043, Validation Loss: 49.9183464050293\n",
      "Epoch [299/1200], Training Loss: 28.17590359665381, Validation Loss: 48.57676696777344\n",
      "Epoch [300/1200], Training Loss: 27.598932825989092, Validation Loss: 49.214969635009766\n",
      "Epoch [301/1200], Training Loss: 27.042291464523252, Validation Loss: 47.72312927246094\n",
      "Epoch [302/1200], Training Loss: 26.501735337376815, Validation Loss: 48.343971252441406\n",
      "Epoch [303/1200], Training Loss: 25.970526830664586, Validation Loss: 46.774993896484375\n",
      "Epoch [304/1200], Training Loss: 25.465770424850096, Validation Loss: 47.28281021118164\n",
      "Epoch [305/1200], Training Loss: 24.958336030940636, Validation Loss: 45.707496643066406\n",
      "Epoch [306/1200], Training Loss: 24.484734674327857, Validation Loss: 46.06925582885742\n",
      "Epoch [307/1200], Training Loss: 24.00042752117644, Validation Loss: 44.543052673339844\n",
      "Epoch [308/1200], Training Loss: 23.554291317831183, Validation Loss: 44.777252197265625\n",
      "Epoch [309/1200], Training Loss: 23.09282982753471, Validation Loss: 43.319881439208984\n",
      "Epoch [310/1200], Training Loss: 22.671190925271237, Validation Loss: 43.46168518066406\n",
      "Epoch [311/1200], Training Loss: 22.231736958330224, Validation Loss: 42.07134246826172\n",
      "Epoch [312/1200], Training Loss: 21.83199381157486, Validation Loss: 42.17284393310547\n",
      "Epoch [313/1200], Training Loss: 21.414752374092863, Validation Loss: 40.83757781982422\n",
      "Epoch [314/1200], Training Loss: 21.034307438977393, Validation Loss: 40.93941116333008\n",
      "Epoch [315/1200], Training Loss: 20.63902119407854, Validation Loss: 39.64397430419922\n",
      "Epoch [316/1200], Training Loss: 20.275817812892694, Validation Loss: 39.744876861572266\n",
      "Epoch [317/1200], Training Loss: 19.901972076733877, Validation Loss: 38.46171951293945\n",
      "Epoch [318/1200], Training Loss: 19.55571568524175, Validation Loss: 38.62537384033203\n",
      "Epoch [319/1200], Training Loss: 19.20464589950314, Validation Loss: 37.29084777832031\n",
      "Epoch [320/1200], Training Loss: 18.878430509428657, Validation Loss: 37.84747314453125\n",
      "Epoch [321/1200], Training Loss: 18.544631923960498, Validation Loss: 36.469261169433594\n",
      "Epoch [322/1200], Training Loss: 18.240929459758846, Validation Loss: 37.08003616333008\n",
      "Epoch [323/1200], Training Loss: 17.915829153232412, Validation Loss: 35.26409149169922\n",
      "Epoch [324/1200], Training Loss: 17.632989113617597, Validation Loss: 35.76157760620117\n",
      "Epoch [325/1200], Training Loss: 17.317063579306154, Validation Loss: 34.17646408081055\n",
      "Epoch [326/1200], Training Loss: 17.05569129341141, Validation Loss: 34.376792907714844\n",
      "Epoch [327/1200], Training Loss: 16.745746668878155, Validation Loss: 33.12877655029297\n",
      "Epoch [328/1200], Training Loss: 16.504480608207704, Validation Loss: 33.04104232788086\n",
      "Epoch [329/1200], Training Loss: 16.199694298989456, Validation Loss: 32.15769958496094\n",
      "Epoch [330/1200], Training Loss: 15.976883962720837, Validation Loss: 31.79180908203125\n",
      "Epoch [331/1200], Training Loss: 15.677234719496273, Validation Loss: 31.25527000427246\n",
      "Epoch [332/1200], Training Loss: 15.471368533364062, Validation Loss: 30.640132904052734\n",
      "Epoch [333/1200], Training Loss: 15.17771681013363, Validation Loss: 30.399599075317383\n",
      "Epoch [334/1200], Training Loss: 14.98622891248147, Validation Loss: 29.573476791381836\n",
      "Epoch [335/1200], Training Loss: 14.698762790672955, Validation Loss: 29.576799392700195\n",
      "Epoch [336/1200], Training Loss: 14.519750548656438, Validation Loss: 28.578439712524414\n",
      "Epoch [337/1200], Training Loss: 14.239334760129474, Validation Loss: 28.801034927368164\n",
      "Epoch [338/1200], Training Loss: 14.071156119247085, Validation Loss: 27.643850326538086\n",
      "Epoch [339/1200], Training Loss: 13.798400535224369, Validation Loss: 28.076391220092773\n",
      "Epoch [340/1200], Training Loss: 13.639467012856107, Validation Loss: 26.76947593688965\n",
      "Epoch [341/1200], Training Loss: 13.37563930898271, Validation Loss: 27.41604995727539\n",
      "Epoch [342/1200], Training Loss: 13.22472181502171, Validation Loss: 25.96481704711914\n",
      "Epoch [343/1200], Training Loss: 12.970942704844685, Validation Loss: 26.818744659423828\n",
      "Epoch [344/1200], Training Loss: 12.82655464020834, Validation Loss: 25.223562240600586\n",
      "Epoch [345/1200], Training Loss: 12.583521164178975, Validation Loss: 26.283702850341797\n",
      "Epoch [346/1200], Training Loss: 12.445462532703143, Validation Loss: 24.541370391845703\n",
      "Epoch [347/1200], Training Loss: 12.213339581420936, Validation Loss: 25.789791107177734\n",
      "Epoch [348/1200], Training Loss: 12.080858986809604, Validation Loss: 23.899927139282227\n",
      "Epoch [349/1200], Training Loss: 11.859007420377251, Validation Loss: 25.32588768005371\n",
      "Epoch [350/1200], Training Loss: 11.732174454834277, Validation Loss: 23.286285400390625\n",
      "Epoch [351/1200], Training Loss: 11.520320414806886, Validation Loss: 24.8851261138916\n",
      "Epoch [352/1200], Training Loss: 11.39874939136119, Validation Loss: 22.685678482055664\n",
      "Epoch [353/1200], Training Loss: 11.195897374734727, Validation Loss: 24.473562240600586\n",
      "Epoch [354/1200], Training Loss: 11.079814141323755, Validation Loss: 22.099334716796875\n",
      "Epoch [355/1200], Training Loss: 10.885212026909675, Validation Loss: 24.092687606811523\n",
      "Epoch [356/1200], Training Loss: 10.774540721407924, Validation Loss: 21.526063919067383\n",
      "Epoch [357/1200], Training Loss: 10.587549712446116, Validation Loss: 23.740205764770508\n",
      "Epoch [358/1200], Training Loss: 10.482503060491412, Validation Loss: 20.970277786254883\n",
      "Epoch [359/1200], Training Loss: 10.302588206523227, Validation Loss: 23.3961181640625\n",
      "Epoch [360/1200], Training Loss: 10.202608226932355, Validation Loss: 20.447938919067383\n",
      "Epoch [361/1200], Training Loss: 10.028446609529253, Validation Loss: 23.008615493774414\n",
      "Epoch [362/1200], Training Loss: 9.932668811085945, Validation Loss: 19.95079803466797\n",
      "Epoch [363/1200], Training Loss: 9.76687390223106, Validation Loss: 22.7802791595459\n",
      "Epoch [364/1200], Training Loss: 9.676059698136028, Validation Loss: 19.474748611450195\n",
      "Epoch [365/1200], Training Loss: 9.514453563148505, Validation Loss: 22.409530639648438\n",
      "Epoch [366/1200], Training Loss: 9.42677365169299, Validation Loss: 19.046735763549805\n",
      "Epoch [367/1200], Training Loss: 9.274153498344347, Validation Loss: 22.220417022705078\n",
      "Epoch [368/1200], Training Loss: 9.189902998694613, Validation Loss: 18.62517547607422\n",
      "Epoch [369/1200], Training Loss: 9.040761088373886, Validation Loss: 21.83092498779297\n",
      "Epoch [370/1200], Training Loss: 8.95889228002527, Validation Loss: 18.247039794921875\n",
      "Epoch [371/1200], Training Loss: 8.817880657725176, Validation Loss: 21.579687118530273\n",
      "Epoch [372/1200], Training Loss: 8.738685486705219, Validation Loss: 17.864309310913086\n",
      "Epoch [373/1200], Training Loss: 8.601212771301807, Validation Loss: 21.18921661376953\n",
      "Epoch [374/1200], Training Loss: 8.52391963260606, Validation Loss: 17.512540817260742\n",
      "Epoch [375/1200], Training Loss: 8.394032069266633, Validation Loss: 20.89082145690918\n",
      "Epoch [376/1200], Training Loss: 8.319218998099085, Validation Loss: 17.162296295166016\n",
      "Epoch [377/1200], Training Loss: 8.193077589457367, Validation Loss: 20.476978302001953\n",
      "Epoch [378/1200], Training Loss: 8.11985512016893, Validation Loss: 16.835784912109375\n",
      "Epoch [379/1200], Training Loss: 8.000666291484281, Validation Loss: 20.129852294921875\n",
      "Epoch [380/1200], Training Loss: 7.929722911109894, Validation Loss: 16.51445770263672\n",
      "Epoch [381/1200], Training Loss: 7.8139354310917986, Validation Loss: 19.68947982788086\n",
      "Epoch [382/1200], Training Loss: 7.744313051491824, Validation Loss: 16.21198272705078\n",
      "Epoch [383/1200], Training Loss: 7.635496130037611, Validation Loss: 19.305355072021484\n",
      "Epoch [384/1200], Training Loss: 7.567670559997005, Validation Loss: 15.920915603637695\n",
      "Epoch [385/1200], Training Loss: 7.46289494330537, Validation Loss: 18.83502769470215\n",
      "Epoch [386/1200], Training Loss: 7.3959042149281355, Validation Loss: 15.64891529083252\n",
      "Epoch [387/1200], Training Loss: 7.299209388459454, Validation Loss: 18.42594337463379\n",
      "Epoch [388/1200], Training Loss: 7.233280184613197, Validation Loss: 15.395119667053223\n",
      "Epoch [389/1200], Training Loss: 7.142204085702754, Validation Loss: 17.943315505981445\n",
      "Epoch [390/1200], Training Loss: 7.076879349631559, Validation Loss: 15.159523010253906\n",
      "Epoch [391/1200], Training Loss: 6.99606054150932, Validation Loss: 17.533920288085938\n",
      "Epoch [392/1200], Training Loss: 6.931085348327764, Validation Loss: 14.93213176727295\n",
      "Epoch [393/1200], Training Loss: 6.857579022902593, Validation Loss: 17.037677764892578\n",
      "Epoch [394/1200], Training Loss: 6.793911191748092, Validation Loss: 14.71241569519043\n",
      "Epoch [395/1200], Training Loss: 6.7300407003015374, Validation Loss: 16.52823257446289\n",
      "Epoch [396/1200], Training Loss: 6.667452419452702, Validation Loss: 14.464170455932617\n",
      "Epoch [397/1200], Training Loss: 6.610583252268696, Validation Loss: 15.913226127624512\n",
      "Epoch [398/1200], Training Loss: 6.551852547749908, Validation Loss: 14.208362579345703\n",
      "Epoch [399/1200], Training Loss: 6.501731550092672, Validation Loss: 15.319229125976562\n",
      "Epoch [400/1200], Training Loss: 6.448973600153498, Validation Loss: 13.940381050109863\n",
      "Epoch [401/1200], Training Loss: 6.404872145789471, Validation Loss: 14.78466796875\n",
      "Epoch [402/1200], Training Loss: 6.358568806526086, Validation Loss: 13.673388481140137\n",
      "Epoch [403/1200], Training Loss: 6.319146487171399, Validation Loss: 14.29914379119873\n",
      "Epoch [404/1200], Training Loss: 6.279915392914082, Validation Loss: 13.408025741577148\n",
      "Epoch [405/1200], Training Loss: 6.244048508713633, Validation Loss: 13.858270645141602\n",
      "Epoch [406/1200], Training Loss: 6.211775407239061, Validation Loss: 13.14667797088623\n",
      "Epoch [407/1200], Training Loss: 6.1787758976796425, Validation Loss: 13.454143524169922\n",
      "Epoch [408/1200], Training Loss: 6.152257765732413, Validation Loss: 12.8914155960083\n",
      "Epoch [409/1200], Training Loss: 6.122201267641213, Validation Loss: 13.083847045898438\n",
      "Epoch [410/1200], Training Loss: 6.099321831026474, Validation Loss: 12.650636672973633\n",
      "Epoch [411/1200], Training Loss: 6.072808198600001, Validation Loss: 12.733159065246582\n",
      "Epoch [412/1200], Training Loss: 6.051219330662796, Validation Loss: 12.432597160339355\n",
      "Epoch [413/1200], Training Loss: 6.029569685547833, Validation Loss: 12.401551246643066\n",
      "Epoch [414/1200], Training Loss: 6.006554978719685, Validation Loss: 12.265434265136719\n",
      "Epoch [415/1200], Training Loss: 5.992253543489443, Validation Loss: 12.098752975463867\n",
      "Epoch [416/1200], Training Loss: 5.96572838445936, Validation Loss: 12.245489120483398\n",
      "Epoch [417/1200], Training Loss: 5.962412494391511, Validation Loss: 11.88895034790039\n",
      "Epoch [418/1200], Training Loss: 5.925360726179596, Validation Loss: 12.645856857299805\n",
      "Epoch [419/1200], Training Loss: 5.930821411305571, Validation Loss: 12.092642784118652\n",
      "Epoch [420/1200], Training Loss: 5.869745780703804, Validation Loss: 13.359245300292969\n",
      "Epoch [421/1200], Training Loss: 5.869523027384653, Validation Loss: 12.818286895751953\n",
      "Epoch [422/1200], Training Loss: 5.80184175592431, Validation Loss: 13.67724323272705\n",
      "Epoch [423/1200], Training Loss: 5.7892555665287695, Validation Loss: 13.303900718688965\n",
      "Epoch [424/1200], Training Loss: 5.73577066303029, Validation Loss: 13.754890441894531\n",
      "Epoch [425/1200], Training Loss: 5.713052952237911, Validation Loss: 13.440206527709961\n",
      "Epoch [426/1200], Training Loss: 5.676295713451863, Validation Loss: 13.765913009643555\n",
      "Early stopping at epoch 426\n",
      "Final Test Loss: 13.497543334960938\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [8]\n",
    "num_layers_list = [2]\n",
    "learning_rates = [0.0002]\n",
    "window_sizes = [6, 7, 8]\n",
    "\n",
    "num_epochs = 1200\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "       # Calculate test loss after training is complete\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Final Test Loss: {test_loss / len(x_test_tensor)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
