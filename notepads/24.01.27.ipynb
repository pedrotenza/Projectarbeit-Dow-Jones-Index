{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "data.shape\n",
    "data.tail(1)\n",
    "\n",
    "data = data.iloc[:10747]\n",
    "\n",
    "data.tail(1)\n",
    "\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "#data[\"date\"] = data[\"date\"].astype(float)\n",
    "data[\"open\"] = data[\"open\"].astype(float)\n",
    "data[\"high\"] = data[\"high\"].astype(float)\n",
    "data[\"low\"] = data[\"low\"].astype(float)\n",
    "data[\"volume\"] = data[\"volume\"].astype(float)\n",
    "data[\"adjusted_close\"] = data[\"adjusted_close\"].astype(float)\n",
    "data[\"change_percent\"] = data[\"change_percent\"].astype(float)\n",
    "data[\"avg_vol_20d\"] = data[\"avg_vol_20d\"].astype(float)\n",
    "\n",
    "data[\"close\"] = data[\"close\"].astype(float)\n",
    "\n",
    "#print(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "x_data = data[[ 'open', 'high', 'low', 'volume','adjusted_close', 'change_percent', 'avg_vol_20d']].values\n",
    "y_data = data[\"close\"].values.reshape(-1, 1)  # Reshape y_data if it's 1D\n",
    "\n",
    "\n",
    "\n",
    "# Convert data to Tensors\n",
    "x_feature_tensors = torch.tensor(x_data, dtype=torch.float32)\n",
    "y_feature_tensors = torch.tensor(y_data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the training data into training and temporary sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_feature_tensors, y_feature_tensors, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both training and test data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Make sure to reshape y_train and y_test if they are 1D arrays\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Fit the scaler to the training labels and transform both training and test labels\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n",
    "y_val_scaled = scaler.transform(y_val)\n",
    "y_test_scaled = scaler.transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "window_size = 50\n",
    "test_window_size = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 4\n",
    "batch_evaluation_frequency = 10\n",
    "epochs = len(x_train_scaled) - window_size\n",
    "batch_size = 1\n",
    "input_size = x_train_scaled.shape[1]  # Input size based on your dataset\n",
    "output_size = 1  # Output size (for regression task)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'รถ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mรถ\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'รถ' is not defined"
     ]
    }
   ],
   "source": [
    "รถ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_2660\\1547132306.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_2660\\1547132306.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_window = torch.tensor(y_train[start_idx:end_idx], dtype=torch.float32)\n",
      "c:\\Projectarbeit-Dow-Jones-Index\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([6, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_2660\\1547132306.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_2660\\1547132306.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_window = torch.tensor(y_val[:params['window_size']], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000, Train Loss: 32128.59375, Validation Loss: 45835.42578125\n",
      "Iteration 2000, Train Loss: 49744.14453125, Validation Loss: 44519.609375\n",
      "Iteration 3000, Train Loss: 4220.86767578125, Validation Loss: 43346.54296875\n",
      "Iteration 4000, Train Loss: 5180.62353515625, Validation Loss: 42222.5390625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled, x_test_scaled, y_test_scaled,\n",
    "# input_size, output_size, test_window_size, scaler are available\n",
    "\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'learning_rate': [0.00001, 0.00005, 0.0001],\n",
    "    'window_size': [3, 5, 6],\n",
    "    'hidden_dim': [128, 256, 512],\n",
    "    'n_layers': [10, 11, 12],\n",
    "    'batch_evaluation_frequency': [2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "#Best Parameters: {'learning_rate': 5e-05, 'window_size': 5, 'hidden_dim': 256,\n",
    "#                  'n_layers': 11, 'batch_evaluation_frequency': 4, 'test_loss': 6463.20556640625}\n",
    "\n",
    "\n",
    "# Number of random search iterations\n",
    "num_iterations = 10\n",
    "\n",
    "best_params = None\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    params = {\n",
    "        'learning_rate': random.choice(search_space['learning_rate']),\n",
    "        'window_size': random.choice(search_space['window_size']),\n",
    "        'hidden_dim': random.choice(search_space['hidden_dim']),\n",
    "        'n_layers': random.choice(search_space['n_layers']),\n",
    "        'batch_evaluation_frequency': random.choice(search_space['batch_evaluation_frequency'])\n",
    "    }\n",
    "\n",
    "    model = LSTMModel(input_size, params['hidden_dim'], params['n_layers'], output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Training using Walk-Forward Validation\n",
    "    for i in range(params['window_size'], len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_idx = i - params['window_size']\n",
    "        end_idx = i\n",
    "        x_window = torch.tensor(x_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        y_window = torch.tensor(y_train[start_idx:end_idx], dtype=torch.float32)\n",
    "        x_window = x_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        outputs, hidden = model(x_window, hidden)\n",
    "        loss = criterion(outputs, y_window)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % params['batch_evaluation_frequency'] == 0:\n",
    "            with torch.no_grad():\n",
    "                x_val_window = torch.tensor(x_val[:params['window_size']], dtype=torch.float32)\n",
    "                y_val_window = torch.tensor(y_val[:params['window_size']], dtype=torch.float32)\n",
    "                x_val_window = x_val_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "                hidden = model.init_hidden(1)\n",
    "                val_outputs, _ = model(x_val_window, hidden)\n",
    "                val_loss = criterion(val_outputs, y_val_window)\n",
    "\n",
    "                if i % 1000 == 0:  # Print every 1000 iterations\n",
    "                    print(f\"Iteration {i}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # Test set evaluation\n",
    "    with torch.no_grad():\n",
    "        x_test_window = torch.tensor(x_test[:params['window_size']], dtype=torch.float32)\n",
    "        y_test_window = torch.tensor(y_test[:params['window_size']], dtype=torch.float32)\n",
    "        x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "        test_outputs, _ = model(x_test_window, hidden)\n",
    "        test_loss = criterion(test_outputs, y_test_window)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "    # Update the best_params if the current model performs better on the test set\n",
    "    if best_params is None:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "    elif test_loss < best_params['test_loss']:\n",
    "        best_params = params\n",
    "        best_params['test_loss'] = test_loss.item()\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last entries of the last window in the test set:\n",
      "Input values:\n",
      "[[ 6.0618042e+02  6.1628003e+02  6.0606000e+02  2.1683200e+07\n",
      "   1.8627701e+01  1.2400000e+00  7.2708064e+08]\n",
      " [ 3.1875200e+01  3.1875200e+01  3.1628799e+01  1.0580000e+05\n",
      "   1.0940000e-01 -8.1999999e-01  7.9405760e+07]\n",
      " [ 3.9244801e+01  3.9995201e+01  3.8998402e+01  7.4960000e+05\n",
      "   2.7750000e-01  9.4999999e-01  1.2576536e+08]\n",
      " [ 3.4742401e+01  3.5369598e+01  3.4630402e+01  9.3110000e+05\n",
      "   1.2240000e-01  2.1700001e+00  1.8372256e+08]\n",
      " [ 5.6810040e+02  5.6950043e+02  5.6337958e+02  7.2860000e+06\n",
      "   1.7679199e+01 -6.6000003e-01  3.5181875e+08]]\n",
      "Actual output values:\n",
      "[614.4796  31.6288  39.7488  35.3696 563.9004]\n",
      "Predicted output values:\n",
      "83.52607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_1384\\2117283741.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_window = torch.tensor(x_test[-params['window_size']:], dtype=torch.float32)\n",
      "C:\\Users\\pitr7\\AppData\\Local\\Temp\\ipykernel_1384\\2117283741.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_window = torch.tensor(y_test[-params['window_size']:], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# ...\n",
    "\n",
    "# Test set evaluation\n",
    "with torch.no_grad():\n",
    "    x_test_window = torch.tensor(x_test[-params['window_size']:], dtype=torch.float32)\n",
    "    y_test_window = torch.tensor(y_test[-params['window_size']:], dtype=torch.float32)\n",
    "    x_test_window = x_test_window.view(1, params['window_size'], input_size)\n",
    "\n",
    "    hidden = model.init_hidden(1)\n",
    "    test_outputs, _ = model(x_test_window, hidden)\n",
    "    test_loss = criterion(test_outputs, y_test_window)\n",
    "\n",
    "# Print the last entries of the last window\n",
    "print(f\"Last entries of the last window in the test set:\")\n",
    "print(\"Input values:\")\n",
    "print(x_test_window.squeeze().numpy())\n",
    "print(\"Actual output values:\")\n",
    "print(y_test_window.squeeze().numpy())\n",
    "print(\"Predicted output values:\")\n",
    "print(test_outputs.squeeze().numpy())\n",
    "\n",
    "# ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
