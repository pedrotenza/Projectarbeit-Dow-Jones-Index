{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0003, window_size=5\n",
    "Epoch [1052/1500], Training Loss: 1.4935169181005978, Validation Loss: 4.272566318511963\n",
    "Early stopping at epoch 1052\n",
    "Final Test Loss: 4.418914318084717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def split_data_with_window(x_in, y_in, split_window_size):\n",
    "    # Initialize lists to store training and temporary sets\n",
    "    x_out1_list, y_out1_list, x_out2_list, y_out2_list = [], [], [], []\n",
    "\n",
    "    # Iterate through the data with the specified window size\n",
    "    for i in range(0, len(x_in) - split_window_size, split_window_size + 1):\n",
    "        x_out1_out2 = x_in.iloc[i:i+split_window_size+1]\n",
    "        y_out1_out2 = y_in.iloc[i:i+split_window_size+1]\n",
    "\n",
    "        # Separate the last row for the temporary set\n",
    "        # [ :-1]: all elements except the last one\n",
    "        # [-1:]:  selects only the last element\n",
    "        # (:) is used to indicate slicing of a sequence\n",
    "        # sequence[start : end : step]\n",
    "\n",
    "        x_out1 = x_out1_out2.iloc[:-1]\n",
    "        y_out1 = y_out1_out2.iloc[:-1]\n",
    "\n",
    "        x_out2 = x_out1_out2.iloc[-1:]\n",
    "        y_out2 = y_out1_out2.iloc[-1:]\n",
    "\n",
    "        x_out1_list.append(x_out1)\n",
    "        y_out1_list.append(y_out1)\n",
    "        x_out2_list.append(x_out2)\n",
    "        y_out2_list.append(y_out2)\n",
    "\n",
    "    # Concatenate the lists into pandas DataFrames\n",
    "    x_out1 = pd.concat(x_out1_list)\n",
    "    y_out1 = pd.concat(y_out1_list)\n",
    "    x_out2 = pd.concat(x_out2_list)\n",
    "    y_out2 = pd.concat(y_out2_list)\n",
    "\n",
    "    return x_out1, y_out1, x_out2, y_out2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "# Split Data to train and temp\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 3\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_train, y_train, x_temp, y_temp = split_data_with_window(x_data, y_data, split_window_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split temp into val and test\n",
    "\n",
    "# Define your split_window_size\n",
    "split_window_size = 1\n",
    "\n",
    "# Call the split_data_with_window function\n",
    "x_val, y_val, x_test, y_test = split_data_with_window(x_temp, y_temp, split_window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, learning_rate, window_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0004, window_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Training Loss: 27156.800065458516, Validation Loss: 26517.798828125\n",
      "Epoch [2/1500], Training Loss: 25032.448395751853, Validation Loss: 24625.51171875\n",
      "Epoch [3/1500], Training Loss: 23433.21921403852, Validation Loss: 23156.517578125\n",
      "Epoch [4/1500], Training Loss: 22027.5540707363, Validation Loss: 21823.076171875\n",
      "Epoch [5/1500], Training Loss: 20739.042967078512, Validation Loss: 20551.982421875\n",
      "Epoch [6/1500], Training Loss: 19590.729650406956, Validation Loss: 19423.87109375\n",
      "Epoch [7/1500], Training Loss: 18543.016668294258, Validation Loss: 18398.404296875\n",
      "Epoch [8/1500], Training Loss: 17584.287421004166, Validation Loss: 17468.224609375\n",
      "Epoch [9/1500], Training Loss: 16702.483996535906, Validation Loss: 16617.76953125\n",
      "Epoch [10/1500], Training Loss: 15891.403394818359, Validation Loss: 15820.7783203125\n",
      "Epoch [11/1500], Training Loss: 15145.29850130117, Validation Loss: 15061.609375\n",
      "Epoch [12/1500], Training Loss: 14455.926694789987, Validation Loss: 14368.7294921875\n",
      "Epoch [13/1500], Training Loss: 13816.528626668958, Validation Loss: 13747.5615234375\n",
      "Epoch [14/1500], Training Loss: 13219.12518389506, Validation Loss: 13163.830078125\n",
      "Epoch [15/1500], Training Loss: 12662.687093736287, Validation Loss: 12673.919921875\n",
      "Epoch [16/1500], Training Loss: 12143.535705090599, Validation Loss: 12170.06640625\n",
      "Epoch [17/1500], Training Loss: 11659.0050875171, Validation Loss: 11681.763671875\n",
      "Epoch [18/1500], Training Loss: 11205.839856906725, Validation Loss: 11241.92578125\n",
      "Epoch [19/1500], Training Loss: 10781.993986177184, Validation Loss: 10833.9599609375\n",
      "Epoch [20/1500], Training Loss: 10385.922502522364, Validation Loss: 10448.05859375\n",
      "Epoch [21/1500], Training Loss: 10016.491757601996, Validation Loss: 10090.1630859375\n",
      "Epoch [22/1500], Training Loss: 9673.15913834666, Validation Loss: 9771.8310546875\n",
      "Epoch [23/1500], Training Loss: 9353.357398831748, Validation Loss: 9647.912109375\n",
      "Epoch [24/1500], Training Loss: 9021.559425092044, Validation Loss: 9723.87890625\n",
      "Epoch [25/1500], Training Loss: 8709.228050048843, Validation Loss: 9362.8349609375\n",
      "Epoch [26/1500], Training Loss: 8457.87107061942, Validation Loss: 8863.765625\n",
      "Epoch [27/1500], Training Loss: 8100.14648227439, Validation Loss: 8725.638671875\n",
      "Epoch [28/1500], Training Loss: 8216.953876136915, Validation Loss: 8135.49609375\n",
      "Epoch [29/1500], Training Loss: 7719.303923438735, Validation Loss: 7764.7138671875\n",
      "Epoch [30/1500], Training Loss: 7402.002034845066, Validation Loss: 7518.74658203125\n",
      "Epoch [31/1500], Training Loss: 7106.179897105261, Validation Loss: 7373.9267578125\n",
      "Epoch [32/1500], Training Loss: 6833.292535614489, Validation Loss: 7122.1455078125\n",
      "Epoch [33/1500], Training Loss: 6581.479189418436, Validation Loss: 6807.04833984375\n",
      "Epoch [34/1500], Training Loss: 6348.114258632921, Validation Loss: 6489.93359375\n",
      "Epoch [35/1500], Training Loss: 6125.121095460967, Validation Loss: 6228.6474609375\n",
      "Epoch [36/1500], Training Loss: 5910.335242976509, Validation Loss: 5997.98828125\n",
      "Epoch [37/1500], Training Loss: 5702.166076440679, Validation Loss: 5771.73095703125\n",
      "Epoch [38/1500], Training Loss: 5499.870475809766, Validation Loss: 5555.7568359375\n",
      "Epoch [39/1500], Training Loss: 5304.547554170192, Validation Loss: 5358.365234375\n",
      "Epoch [40/1500], Training Loss: 5117.535302950946, Validation Loss: 5182.646484375\n",
      "Epoch [41/1500], Training Loss: 4948.631133827433, Validation Loss: 5019.646484375\n",
      "Epoch [42/1500], Training Loss: 4769.364286097906, Validation Loss: 4866.58203125\n",
      "Epoch [43/1500], Training Loss: 4599.467011119939, Validation Loss: 4705.35107421875\n",
      "Epoch [44/1500], Training Loss: 4436.2600140423065, Validation Loss: 4544.236328125\n",
      "Epoch [45/1500], Training Loss: 4279.161237995248, Validation Loss: 4389.7548828125\n",
      "Epoch [46/1500], Training Loss: 4127.652669739013, Validation Loss: 4239.05029296875\n",
      "Epoch [47/1500], Training Loss: 3981.186676999986, Validation Loss: 4091.9970703125\n",
      "Epoch [48/1500], Training Loss: 3839.2192763711364, Validation Loss: 3950.248046875\n",
      "Epoch [49/1500], Training Loss: 3701.9306242132066, Validation Loss: 3814.453369140625\n",
      "Epoch [50/1500], Training Loss: 3569.9449637301304, Validation Loss: 3684.02392578125\n",
      "Epoch [51/1500], Training Loss: 3443.6158478833604, Validation Loss: 3564.2802734375\n",
      "Epoch [52/1500], Training Loss: 3323.453870088352, Validation Loss: 3456.08837890625\n",
      "Epoch [53/1500], Training Loss: 3209.1110129087824, Validation Loss: 3350.961669921875\n",
      "Epoch [54/1500], Training Loss: 3099.9303990121725, Validation Loss: 3253.200927734375\n",
      "Epoch [55/1500], Training Loss: 2995.5773431393823, Validation Loss: 3157.857177734375\n",
      "Epoch [56/1500], Training Loss: 2895.6301644262, Validation Loss: 3065.943603515625\n",
      "Epoch [57/1500], Training Loss: 2799.487144939788, Validation Loss: 2980.28271484375\n",
      "Epoch [58/1500], Training Loss: 2706.900578147167, Validation Loss: 2895.470458984375\n",
      "Epoch [59/1500], Training Loss: 2617.9813879743124, Validation Loss: 2808.671142578125\n",
      "Epoch [60/1500], Training Loss: 2532.9640272564156, Validation Loss: 2720.923828125\n",
      "Epoch [61/1500], Training Loss: 2451.9651516385748, Validation Loss: 2636.140869140625\n",
      "Epoch [62/1500], Training Loss: 2374.9270356780617, Validation Loss: 2554.170654296875\n",
      "Epoch [63/1500], Training Loss: 2301.4707433008302, Validation Loss: 2473.89208984375\n",
      "Epoch [64/1500], Training Loss: 2231.0072496111566, Validation Loss: 2396.0224609375\n",
      "Epoch [65/1500], Training Loss: 2163.097525334478, Validation Loss: 2322.173583984375\n",
      "Epoch [66/1500], Training Loss: 2097.4635510640856, Validation Loss: 2252.33837890625\n",
      "Epoch [67/1500], Training Loss: 2033.922996176373, Validation Loss: 2186.016845703125\n",
      "Epoch [68/1500], Training Loss: 1972.3954167587647, Validation Loss: 2122.95703125\n",
      "Epoch [69/1500], Training Loss: 1912.8513152725725, Validation Loss: 2063.0849609375\n",
      "Epoch [70/1500], Training Loss: 1855.3504240304715, Validation Loss: 2006.3680419921875\n",
      "Epoch [71/1500], Training Loss: 1799.893241344643, Validation Loss: 1952.5078125\n",
      "Epoch [72/1500], Training Loss: 1746.519782009722, Validation Loss: 1901.17822265625\n",
      "Epoch [73/1500], Training Loss: 1695.2500987312483, Validation Loss: 1852.1864013671875\n",
      "Epoch [74/1500], Training Loss: 1646.0959906268138, Validation Loss: 1805.3936767578125\n",
      "Epoch [75/1500], Training Loss: 1599.0249668551407, Validation Loss: 1760.590576171875\n",
      "Epoch [76/1500], Training Loss: 1553.9671381168298, Validation Loss: 1717.576416015625\n",
      "Epoch [77/1500], Training Loss: 1510.7931948327778, Validation Loss: 1676.220947265625\n",
      "Epoch [78/1500], Training Loss: 1469.3763711530207, Validation Loss: 1636.4444580078125\n",
      "Epoch [79/1500], Training Loss: 1429.5522766032966, Validation Loss: 1598.0640869140625\n",
      "Epoch [80/1500], Training Loss: 1391.1638646996134, Validation Loss: 1560.8302001953125\n",
      "Epoch [81/1500], Training Loss: 1354.0806928586844, Validation Loss: 1524.422119140625\n",
      "Epoch [82/1500], Training Loss: 1318.1796282779112, Validation Loss: 1488.5089111328125\n",
      "Epoch [83/1500], Training Loss: 1283.3852360538724, Validation Loss: 1452.8487548828125\n",
      "Epoch [84/1500], Training Loss: 1249.623538627463, Validation Loss: 1417.357666015625\n",
      "Epoch [85/1500], Training Loss: 1216.8561001840362, Validation Loss: 1382.1099853515625\n",
      "Epoch [86/1500], Training Loss: 1185.0352747500697, Validation Loss: 1347.24462890625\n",
      "Epoch [87/1500], Training Loss: 1154.0634358706225, Validation Loss: 1312.9161376953125\n",
      "Epoch [88/1500], Training Loss: 1123.8629671025642, Validation Loss: 1279.1861572265625\n",
      "Epoch [89/1500], Training Loss: 1094.379620953524, Validation Loss: 1246.10791015625\n",
      "Epoch [90/1500], Training Loss: 1065.5753944112905, Validation Loss: 1213.807861328125\n",
      "Epoch [91/1500], Training Loss: 1037.4636544671896, Validation Loss: 1182.4034423828125\n",
      "Epoch [92/1500], Training Loss: 1010.069320970263, Validation Loss: 1151.9686279296875\n",
      "Epoch [93/1500], Training Loss: 983.4105137287373, Validation Loss: 1122.5615234375\n",
      "Epoch [94/1500], Training Loss: 957.505441972748, Validation Loss: 1094.1448974609375\n",
      "Epoch [95/1500], Training Loss: 932.2884561176098, Validation Loss: 1066.5345458984375\n",
      "Epoch [96/1500], Training Loss: 907.6814648189157, Validation Loss: 1039.3853759765625\n",
      "Epoch [97/1500], Training Loss: 883.6061677896212, Validation Loss: 1012.18798828125\n",
      "Epoch [98/1500], Training Loss: 859.9638501756771, Validation Loss: 984.4852905273438\n",
      "Epoch [99/1500], Training Loss: 836.6627564966639, Validation Loss: 955.8657836914062\n",
      "Epoch [100/1500], Training Loss: 813.6318052785113, Validation Loss: 926.0949096679688\n",
      "Epoch [101/1500], Training Loss: 790.911204293467, Validation Loss: 895.7283935546875\n",
      "Epoch [102/1500], Training Loss: 768.6424182132112, Validation Loss: 865.8054809570312\n",
      "Epoch [103/1500], Training Loss: 746.9820872526909, Validation Loss: 837.199462890625\n",
      "Epoch [104/1500], Training Loss: 725.9953714023112, Validation Loss: 810.3015747070312\n",
      "Epoch [105/1500], Training Loss: 705.6842856941801, Validation Loss: 785.1345825195312\n",
      "Epoch [106/1500], Training Loss: 686.0221577609202, Validation Loss: 761.5440673828125\n",
      "Epoch [107/1500], Training Loss: 666.9695545197577, Validation Loss: 739.3240966796875\n",
      "Epoch [108/1500], Training Loss: 648.5015600605346, Validation Loss: 718.2791748046875\n",
      "Epoch [109/1500], Training Loss: 630.5891388606616, Validation Loss: 698.2047119140625\n",
      "Epoch [110/1500], Training Loss: 613.2016873066657, Validation Loss: 678.9241333007812\n",
      "Epoch [111/1500], Training Loss: 596.3101048245866, Validation Loss: 660.2872924804688\n",
      "Epoch [112/1500], Training Loss: 579.8940882288684, Validation Loss: 642.1965942382812\n",
      "Epoch [113/1500], Training Loss: 563.9317076333069, Validation Loss: 624.5823364257812\n",
      "Epoch [114/1500], Training Loss: 548.4015854698271, Validation Loss: 607.4254760742188\n",
      "Epoch [115/1500], Training Loss: 533.2809995809243, Validation Loss: 590.6915893554688\n",
      "Epoch [116/1500], Training Loss: 518.5468737780752, Validation Loss: 574.3058471679688\n",
      "Epoch [117/1500], Training Loss: 504.1760192996563, Validation Loss: 558.2145385742188\n",
      "Epoch [118/1500], Training Loss: 490.1313216922647, Validation Loss: 542.3311157226562\n",
      "Epoch [119/1500], Training Loss: 476.38749072118816, Validation Loss: 526.5421752929688\n",
      "Epoch [120/1500], Training Loss: 462.91001854146333, Validation Loss: 510.7039489746094\n",
      "Epoch [121/1500], Training Loss: 449.65919696108324, Validation Loss: 494.6291809082031\n",
      "Epoch [122/1500], Training Loss: 436.58782073487504, Validation Loss: 478.09307861328125\n",
      "Epoch [123/1500], Training Loss: 423.63827803399585, Validation Loss: 460.89544677734375\n",
      "Epoch [124/1500], Training Loss: 410.7697802177711, Validation Loss: 443.0164489746094\n",
      "Epoch [125/1500], Training Loss: 397.9944838764097, Validation Loss: 424.861572265625\n",
      "Epoch [126/1500], Training Loss: 385.43573118292323, Validation Loss: 407.3741760253906\n",
      "Epoch [127/1500], Training Loss: 373.2320879498413, Validation Loss: 390.96649169921875\n",
      "Epoch [128/1500], Training Loss: 361.3506164018492, Validation Loss: 375.64300537109375\n",
      "Epoch [129/1500], Training Loss: 349.8466996400791, Validation Loss: 361.45013427734375\n",
      "Epoch [130/1500], Training Loss: 338.7370549918007, Validation Loss: 348.4792175292969\n",
      "Epoch [131/1500], Training Loss: 328.0369183526119, Validation Loss: 336.7251892089844\n",
      "Epoch [132/1500], Training Loss: 317.7398956312948, Validation Loss: 325.9802551269531\n",
      "Epoch [133/1500], Training Loss: 307.82962826077323, Validation Loss: 315.9612121582031\n",
      "Epoch [134/1500], Training Loss: 298.2788227101944, Validation Loss: 306.447021484375\n",
      "Epoch [135/1500], Training Loss: 289.06370378720516, Validation Loss: 297.1959228515625\n",
      "Epoch [136/1500], Training Loss: 280.1649658933603, Validation Loss: 288.15057373046875\n",
      "Epoch [137/1500], Training Loss: 271.56719221518375, Validation Loss: 279.4206848144531\n",
      "Epoch [138/1500], Training Loss: 263.25488166856456, Validation Loss: 271.01861572265625\n",
      "Epoch [139/1500], Training Loss: 255.22062225274013, Validation Loss: 262.91644287109375\n",
      "Epoch [140/1500], Training Loss: 247.45800394970456, Validation Loss: 255.0840606689453\n",
      "Epoch [141/1500], Training Loss: 239.95523138507062, Validation Loss: 247.5035400390625\n",
      "Epoch [142/1500], Training Loss: 232.70502238171525, Validation Loss: 240.18470764160156\n",
      "Epoch [143/1500], Training Loss: 225.6945297183255, Validation Loss: 233.12469482421875\n",
      "Epoch [144/1500], Training Loss: 218.9138873368811, Validation Loss: 226.30865478515625\n",
      "Epoch [145/1500], Training Loss: 212.35451134224434, Validation Loss: 219.71774291992188\n",
      "Epoch [146/1500], Training Loss: 206.00942846729077, Validation Loss: 213.33836364746094\n",
      "Epoch [147/1500], Training Loss: 199.869293227166, Validation Loss: 207.16061401367188\n",
      "Epoch [148/1500], Training Loss: 193.93267806405905, Validation Loss: 201.18768310546875\n",
      "Epoch [149/1500], Training Loss: 188.19273344578954, Validation Loss: 195.4219512939453\n",
      "Epoch [150/1500], Training Loss: 182.6498054231151, Validation Loss: 189.86627197265625\n",
      "Epoch [151/1500], Training Loss: 177.3058006882002, Validation Loss: 184.37701416015625\n",
      "Epoch [152/1500], Training Loss: 172.15392300880484, Validation Loss: 179.38241577148438\n",
      "Epoch [153/1500], Training Loss: 167.16793594519595, Validation Loss: 174.64724731445312\n",
      "Epoch [154/1500], Training Loss: 162.35986477963658, Validation Loss: 170.1847686767578\n",
      "Epoch [155/1500], Training Loss: 157.7060101068014, Validation Loss: 165.92919921875\n",
      "Epoch [156/1500], Training Loss: 153.20877760448164, Validation Loss: 161.85633850097656\n",
      "Epoch [157/1500], Training Loss: 148.84958657876393, Validation Loss: 157.9548797607422\n",
      "Epoch [158/1500], Training Loss: 144.62041821760678, Validation Loss: 154.16751098632812\n",
      "Epoch [159/1500], Training Loss: 140.5129739790816, Validation Loss: 150.46824645996094\n",
      "Epoch [160/1500], Training Loss: 136.52081995331002, Validation Loss: 146.83885192871094\n",
      "Epoch [161/1500], Training Loss: 132.63689479179286, Validation Loss: 143.26263427734375\n",
      "Epoch [162/1500], Training Loss: 128.85550444641063, Validation Loss: 139.73062133789062\n",
      "Epoch [163/1500], Training Loss: 125.17664359914797, Validation Loss: 136.24261474609375\n",
      "Epoch [164/1500], Training Loss: 121.59637048216884, Validation Loss: 132.7911834716797\n",
      "Epoch [165/1500], Training Loss: 118.11094499439966, Validation Loss: 129.37869262695312\n",
      "Epoch [166/1500], Training Loss: 114.72193004169213, Validation Loss: 126.0077133178711\n",
      "Epoch [167/1500], Training Loss: 111.42553621036035, Validation Loss: 122.68604278564453\n",
      "Epoch [168/1500], Training Loss: 108.22134224242308, Validation Loss: 119.4216537475586\n",
      "Epoch [169/1500], Training Loss: 105.10818489140334, Validation Loss: 116.22783660888672\n",
      "Epoch [170/1500], Training Loss: 102.08489518242847, Validation Loss: 113.11001586914062\n",
      "Epoch [171/1500], Training Loss: 99.14921284968193, Validation Loss: 110.07366180419922\n",
      "Epoch [172/1500], Training Loss: 96.29923381205134, Validation Loss: 107.12130737304688\n",
      "Epoch [173/1500], Training Loss: 93.53257871812697, Validation Loss: 104.25784301757812\n",
      "Epoch [174/1500], Training Loss: 90.8471127814742, Validation Loss: 101.48479461669922\n",
      "Epoch [175/1500], Training Loss: 88.24089470697307, Validation Loss: 98.8055191040039\n",
      "Epoch [176/1500], Training Loss: 85.71120787673976, Validation Loss: 96.22024536132812\n",
      "Epoch [177/1500], Training Loss: 83.25794899181433, Validation Loss: 93.73017120361328\n",
      "Epoch [178/1500], Training Loss: 80.87829973284447, Validation Loss: 91.3347396850586\n",
      "Epoch [179/1500], Training Loss: 78.5708995495376, Validation Loss: 89.03250885009766\n",
      "Epoch [180/1500], Training Loss: 76.33337807573344, Validation Loss: 86.821533203125\n",
      "Epoch [181/1500], Training Loss: 74.16674442745683, Validation Loss: 84.6977767944336\n",
      "Epoch [182/1500], Training Loss: 72.06727099458837, Validation Loss: 82.65911102294922\n",
      "Epoch [183/1500], Training Loss: 70.0349344512146, Validation Loss: 80.69867706298828\n",
      "Epoch [184/1500], Training Loss: 68.06751083499024, Validation Loss: 78.81383514404297\n",
      "Epoch [185/1500], Training Loss: 66.16409490714742, Validation Loss: 76.99952697753906\n",
      "Epoch [186/1500], Training Loss: 64.32234692965584, Validation Loss: 75.25041198730469\n",
      "Epoch [187/1500], Training Loss: 62.539595646953785, Validation Loss: 73.56066131591797\n",
      "Epoch [188/1500], Training Loss: 60.815627194704746, Validation Loss: 71.92732238769531\n",
      "Epoch [189/1500], Training Loss: 59.14888293801634, Validation Loss: 70.34516143798828\n",
      "Epoch [190/1500], Training Loss: 57.537313904388256, Validation Loss: 68.81079864501953\n",
      "Epoch [191/1500], Training Loss: 55.97897502492941, Validation Loss: 67.32495880126953\n",
      "Epoch [192/1500], Training Loss: 54.47230922702019, Validation Loss: 65.88627624511719\n",
      "Epoch [193/1500], Training Loss: 53.015336402562596, Validation Loss: 64.50563049316406\n",
      "Epoch [194/1500], Training Loss: 51.60680779561888, Validation Loss: 63.18143081665039\n",
      "Epoch [195/1500], Training Loss: 50.24396016784784, Validation Loss: 61.868534088134766\n",
      "Epoch [196/1500], Training Loss: 48.92516877019693, Validation Loss: 60.61326599121094\n",
      "Epoch [197/1500], Training Loss: 47.64877118384411, Validation Loss: 59.38642501831055\n",
      "Epoch [198/1500], Training Loss: 46.413127292339134, Validation Loss: 58.22056198120117\n",
      "Epoch [199/1500], Training Loss: 45.21685797714357, Validation Loss: 57.07261276245117\n",
      "Epoch [200/1500], Training Loss: 44.058424184032084, Validation Loss: 55.995174407958984\n",
      "Epoch [201/1500], Training Loss: 42.93667480471149, Validation Loss: 54.915584564208984\n",
      "Epoch [202/1500], Training Loss: 41.85102680373538, Validation Loss: 53.934505462646484\n",
      "Epoch [203/1500], Training Loss: 40.79902043526046, Validation Loss: 52.89768600463867\n",
      "Epoch [204/1500], Training Loss: 39.781823326123195, Validation Loss: 52.042911529541016\n",
      "Epoch [205/1500], Training Loss: 38.7951254936155, Validation Loss: 50.97102737426758\n",
      "Epoch [206/1500], Training Loss: 37.84201546288518, Validation Loss: 50.3646125793457\n",
      "Epoch [207/1500], Training Loss: 36.915484532021146, Validation Loss: 49.007835388183594\n",
      "Epoch [208/1500], Training Loss: 36.024708590735365, Validation Loss: 48.9511604309082\n",
      "Epoch [209/1500], Training Loss: 35.152803401395026, Validation Loss: 47.242679595947266\n",
      "Epoch [210/1500], Training Loss: 34.32176018874005, Validation Loss: 47.44396209716797\n",
      "Epoch [211/1500], Training Loss: 33.50120220683812, Validation Loss: 45.762081146240234\n",
      "Epoch [212/1500], Training Loss: 32.724045777331234, Validation Loss: 45.974342346191406\n",
      "Epoch [213/1500], Training Loss: 31.955613784860432, Validation Loss: 44.41001510620117\n",
      "Epoch [214/1500], Training Loss: 31.22742516593503, Validation Loss: 44.610408782958984\n",
      "Epoch [215/1500], Training Loss: 30.508478927272417, Validation Loss: 43.17564010620117\n",
      "Epoch [216/1500], Training Loss: 29.8260405479886, Validation Loss: 43.35405731201172\n",
      "Epoch [217/1500], Training Loss: 29.151932321225683, Validation Loss: 42.048667907714844\n",
      "Epoch [218/1500], Training Loss: 28.511946678888037, Validation Loss: 42.20587921142578\n",
      "Epoch [219/1500], Training Loss: 27.880402530726887, Validation Loss: 41.02141571044922\n",
      "Epoch [220/1500], Training Loss: 27.280263195660105, Validation Loss: 41.16770553588867\n",
      "Epoch [221/1500], Training Loss: 26.689618067479604, Validation Loss: 40.08327102661133\n",
      "Epoch [222/1500], Training Loss: 26.12735794363488, Validation Loss: 40.239383697509766\n",
      "Epoch [223/1500], Training Loss: 25.574518391579193, Validation Loss: 39.215232849121094\n",
      "Epoch [224/1500], Training Loss: 25.046872089266145, Validation Loss: 39.41829299926758\n",
      "Epoch [225/1500], Training Loss: 24.529655267601715, Validation Loss: 38.401573181152344\n",
      "Epoch [226/1500], Training Loss: 24.0352324487708, Validation Loss: 38.702022552490234\n",
      "Epoch [227/1500], Training Loss: 23.551128610074866, Validation Loss: 37.62208938598633\n",
      "Epoch [228/1500], Training Loss: 23.08762987705772, Validation Loss: 38.07369613647461\n",
      "Epoch [229/1500], Training Loss: 22.633399229272978, Validation Loss: 37.0021858215332\n",
      "Epoch [230/1500], Training Loss: 22.199002723479342, Validation Loss: 37.38315963745117\n",
      "Epoch [231/1500], Training Loss: 21.77068501956623, Validation Loss: 36.45028305053711\n",
      "Epoch [232/1500], Training Loss: 21.361649749518133, Validation Loss: 36.74669647216797\n",
      "Epoch [233/1500], Training Loss: 20.960133090957925, Validation Loss: 35.83300018310547\n",
      "Epoch [234/1500], Training Loss: 20.57457485803246, Validation Loss: 36.30550003051758\n",
      "Epoch [235/1500], Training Loss: 20.198052523392597, Validation Loss: 35.35927200317383\n",
      "Epoch [236/1500], Training Loss: 19.833645636409283, Validation Loss: 35.69820785522461\n",
      "Epoch [237/1500], Training Loss: 19.471394584001512, Validation Loss: 34.895103454589844\n",
      "Epoch [238/1500], Training Loss: 19.127446608116752, Validation Loss: 35.21289825439453\n",
      "Epoch [239/1500], Training Loss: 18.78904710050465, Validation Loss: 34.41949462890625\n",
      "Epoch [240/1500], Training Loss: 18.462541185206582, Validation Loss: 34.78964614868164\n",
      "Epoch [241/1500], Training Loss: 18.13987039461433, Validation Loss: 34.00922393798828\n",
      "Epoch [242/1500], Training Loss: 17.830663534059926, Validation Loss: 34.29647445678711\n",
      "Epoch [243/1500], Training Loss: 17.522973441949368, Validation Loss: 33.59931564331055\n",
      "Epoch [244/1500], Training Loss: 17.228710987522806, Validation Loss: 33.80137634277344\n",
      "Epoch [245/1500], Training Loss: 16.936965779284446, Validation Loss: 33.19237518310547\n",
      "Epoch [246/1500], Training Loss: 16.65691606871062, Validation Loss: 33.30610275268555\n",
      "Epoch [247/1500], Training Loss: 16.379218785433046, Validation Loss: 32.79640579223633\n",
      "Epoch [248/1500], Training Loss: 16.112267890083416, Validation Loss: 32.802616119384766\n",
      "Epoch [249/1500], Training Loss: 15.848416853269415, Validation Loss: 32.39963912963867\n",
      "Epoch [250/1500], Training Loss: 15.593263768169209, Validation Loss: 32.30228042602539\n",
      "Epoch [251/1500], Training Loss: 15.341881185652671, Validation Loss: 31.987003326416016\n",
      "Epoch [252/1500], Training Loss: 15.097482506734394, Validation Loss: 31.810501098632812\n",
      "Epoch [253/1500], Training Loss: 14.85746454179812, Validation Loss: 31.54326629638672\n",
      "Epoch [254/1500], Training Loss: 14.623084538078718, Validation Loss: 31.31964874267578\n",
      "Epoch [255/1500], Training Loss: 14.39371232793259, Validation Loss: 31.065139770507812\n",
      "Epoch [256/1500], Training Loss: 14.169269761528588, Validation Loss: 30.81805992126465\n",
      "Epoch [257/1500], Training Loss: 13.949506210648662, Validation Loss: 30.562633514404297\n",
      "Epoch [258/1500], Training Loss: 13.734785937857557, Validation Loss: 30.303207397460938\n",
      "Epoch [259/1500], Training Loss: 13.524361670103852, Validation Loss: 30.039859771728516\n",
      "Epoch [260/1500], Training Loss: 13.318140329035224, Validation Loss: 29.77192497253418\n",
      "Epoch [261/1500], Training Loss: 13.116109978004891, Validation Loss: 29.500965118408203\n",
      "Epoch [262/1500], Training Loss: 12.91826189198081, Validation Loss: 29.227712631225586\n",
      "Epoch [263/1500], Training Loss: 12.724438085082559, Validation Loss: 28.953710556030273\n",
      "Epoch [264/1500], Training Loss: 12.534553954919371, Validation Loss: 28.678674697875977\n",
      "Epoch [265/1500], Training Loss: 12.348348571075785, Validation Loss: 28.402416229248047\n",
      "Epoch [266/1500], Training Loss: 12.166083527398456, Validation Loss: 28.127248764038086\n",
      "Epoch [267/1500], Training Loss: 11.987430475858606, Validation Loss: 27.8543758392334\n",
      "Epoch [268/1500], Training Loss: 11.812213097146854, Validation Loss: 27.582218170166016\n",
      "Epoch [269/1500], Training Loss: 11.64053630838612, Validation Loss: 27.311931610107422\n",
      "Epoch [270/1500], Training Loss: 11.47213827682252, Validation Loss: 27.045934677124023\n",
      "Epoch [271/1500], Training Loss: 11.306995453986188, Validation Loss: 26.783710479736328\n",
      "Epoch [272/1500], Training Loss: 11.145049314275626, Validation Loss: 26.526052474975586\n",
      "Epoch [273/1500], Training Loss: 10.98617078161259, Validation Loss: 26.272985458374023\n",
      "Epoch [274/1500], Training Loss: 10.83037296563219, Validation Loss: 26.02424430847168\n",
      "Epoch [275/1500], Training Loss: 10.67709703767147, Validation Loss: 25.77960968017578\n",
      "Epoch [276/1500], Training Loss: 10.526595763709194, Validation Loss: 25.541141510009766\n",
      "Epoch [277/1500], Training Loss: 10.378656740560915, Validation Loss: 25.3043270111084\n",
      "Epoch [278/1500], Training Loss: 10.233285737294946, Validation Loss: 25.07101821899414\n",
      "Epoch [279/1500], Training Loss: 10.090500089154599, Validation Loss: 24.841463088989258\n",
      "Epoch [280/1500], Training Loss: 9.95032155869823, Validation Loss: 24.613142013549805\n",
      "Epoch [281/1500], Training Loss: 9.812275112608448, Validation Loss: 24.386524200439453\n",
      "Epoch [282/1500], Training Loss: 9.676770921865222, Validation Loss: 24.160118103027344\n",
      "Epoch [283/1500], Training Loss: 9.543647461325664, Validation Loss: 23.935562133789062\n",
      "Epoch [284/1500], Training Loss: 9.41264990205422, Validation Loss: 23.71029281616211\n",
      "Epoch [285/1500], Training Loss: 9.284255639716749, Validation Loss: 23.486083984375\n",
      "Epoch [286/1500], Training Loss: 9.157817387834356, Validation Loss: 23.26236915588379\n",
      "Epoch [287/1500], Training Loss: 9.033497863416496, Validation Loss: 23.041046142578125\n",
      "Epoch [288/1500], Training Loss: 8.911213742263255, Validation Loss: 22.822275161743164\n",
      "Epoch [289/1500], Training Loss: 8.791141426499248, Validation Loss: 22.60486602783203\n",
      "Epoch [290/1500], Training Loss: 8.672865945052656, Validation Loss: 22.390750885009766\n",
      "Epoch [291/1500], Training Loss: 8.556444489306683, Validation Loss: 22.180782318115234\n",
      "Epoch [292/1500], Training Loss: 8.441819882818235, Validation Loss: 21.9755859375\n",
      "Epoch [293/1500], Training Loss: 8.329083022063381, Validation Loss: 21.776281356811523\n",
      "Epoch [294/1500], Training Loss: 8.217815520394229, Validation Loss: 21.582853317260742\n",
      "Epoch [295/1500], Training Loss: 8.108287756220788, Validation Loss: 21.39851188659668\n",
      "Epoch [296/1500], Training Loss: 8.000720736461552, Validation Loss: 21.222694396972656\n",
      "Epoch [297/1500], Training Loss: 7.894781539284179, Validation Loss: 21.06169319152832\n",
      "Epoch [298/1500], Training Loss: 7.790465888525892, Validation Loss: 20.921058654785156\n",
      "Epoch [299/1500], Training Loss: 7.68822804705485, Validation Loss: 20.81991195678711\n",
      "Epoch [300/1500], Training Loss: 7.588304825330778, Validation Loss: 20.82486343383789\n",
      "Epoch [301/1500], Training Loss: 7.492070839425981, Validation Loss: 21.26560401916504\n",
      "Epoch [302/1500], Training Loss: 7.406937104398843, Validation Loss: 21.310075759887695\n",
      "Epoch [303/1500], Training Loss: 7.318062900080486, Validation Loss: 21.145957946777344\n",
      "Epoch [304/1500], Training Loss: 7.2202956758294015, Validation Loss: 21.19086456298828\n",
      "Epoch [305/1500], Training Loss: 7.13899124858703, Validation Loss: 20.636173248291016\n",
      "Epoch [306/1500], Training Loss: 7.056493009039133, Validation Loss: 20.51941680908203\n",
      "Epoch [307/1500], Training Loss: 6.961081628761253, Validation Loss: 20.5250301361084\n",
      "Epoch [308/1500], Training Loss: 6.885164073275601, Validation Loss: 20.088199615478516\n",
      "Epoch [309/1500], Training Loss: 6.803468048647508, Validation Loss: 19.739990234375\n",
      "Epoch [310/1500], Training Loss: 6.718629406979022, Validation Loss: 20.161304473876953\n",
      "Epoch [311/1500], Training Loss: 6.650814039734208, Validation Loss: 19.66032600402832\n",
      "Epoch [312/1500], Training Loss: 6.563717501873438, Validation Loss: 19.358959197998047\n",
      "Epoch [313/1500], Training Loss: 6.499940673828873, Validation Loss: 19.974889755249023\n",
      "Epoch [314/1500], Training Loss: 6.425162679342905, Validation Loss: 19.32282257080078\n",
      "Epoch [315/1500], Training Loss: 6.344570573865014, Validation Loss: 19.4775447845459\n",
      "Epoch [316/1500], Training Loss: 6.279208773441824, Validation Loss: 19.38424301147461\n",
      "Epoch [317/1500], Training Loss: 6.207591602893478, Validation Loss: 18.5423641204834\n",
      "Epoch [318/1500], Training Loss: 6.149854843871451, Validation Loss: 19.338768005371094\n",
      "Epoch [319/1500], Training Loss: 6.083066339180446, Validation Loss: 18.80945587158203\n",
      "Epoch [320/1500], Training Loss: 6.008020924809342, Validation Loss: 19.02860450744629\n",
      "Epoch [321/1500], Training Loss: 5.946864512585813, Validation Loss: 18.851816177368164\n",
      "Epoch [322/1500], Training Loss: 5.882471595187342, Validation Loss: 18.114273071289062\n",
      "Epoch [323/1500], Training Loss: 5.825773363473656, Validation Loss: 18.751251220703125\n",
      "Epoch [324/1500], Training Loss: 5.76696826432172, Validation Loss: 18.34976577758789\n",
      "Epoch [325/1500], Training Loss: 5.699062628952341, Validation Loss: 18.19050407409668\n",
      "Epoch [326/1500], Training Loss: 5.646016356736174, Validation Loss: 18.481821060180664\n",
      "Epoch [327/1500], Training Loss: 5.58740967881199, Validation Loss: 18.10529136657715\n",
      "Epoch [328/1500], Training Loss: 5.526679966607967, Validation Loss: 17.67344856262207\n",
      "Epoch [329/1500], Training Loss: 5.47572632961848, Validation Loss: 18.19383430480957\n",
      "Epoch [330/1500], Training Loss: 5.4215486313185535, Validation Loss: 17.787521362304688\n",
      "Epoch [331/1500], Training Loss: 5.36188908440476, Validation Loss: 17.411157608032227\n",
      "Epoch [332/1500], Training Loss: 5.313973996471735, Validation Loss: 17.929964065551758\n",
      "Epoch [333/1500], Training Loss: 5.261706661580276, Validation Loss: 17.501827239990234\n",
      "Epoch [334/1500], Training Loss: 5.205265512405737, Validation Loss: 17.080039978027344\n",
      "Epoch [335/1500], Training Loss: 5.159318144086504, Validation Loss: 17.643468856811523\n",
      "Epoch [336/1500], Training Loss: 5.110236809763432, Validation Loss: 17.215272903442383\n",
      "Epoch [337/1500], Training Loss: 5.056591659116568, Validation Loss: 16.707317352294922\n",
      "Epoch [338/1500], Training Loss: 5.0142371235029115, Validation Loss: 17.398698806762695\n",
      "Epoch [339/1500], Training Loss: 4.966708226308487, Validation Loss: 16.94521141052246\n",
      "Epoch [340/1500], Training Loss: 4.91570689115983, Validation Loss: 16.425901412963867\n",
      "Epoch [341/1500], Training Loss: 4.874914925187389, Validation Loss: 17.1383056640625\n",
      "Epoch [342/1500], Training Loss: 4.830572177914243, Validation Loss: 16.67171287536621\n",
      "Epoch [343/1500], Training Loss: 4.78234680552385, Validation Loss: 16.054454803466797\n",
      "Epoch [344/1500], Training Loss: 4.744783322457929, Validation Loss: 16.928245544433594\n",
      "Epoch [345/1500], Training Loss: 4.701738401437667, Validation Loss: 16.40049171447754\n",
      "Epoch [346/1500], Training Loss: 4.656292458844852, Validation Loss: 15.744295120239258\n",
      "Epoch [347/1500], Training Loss: 4.62059686194357, Validation Loss: 16.71439552307129\n",
      "Epoch [348/1500], Training Loss: 4.580122298304846, Validation Loss: 16.099042892456055\n",
      "Epoch [349/1500], Training Loss: 4.537706354489047, Validation Loss: 15.405938148498535\n",
      "Epoch [350/1500], Training Loss: 4.502446705608611, Validation Loss: 16.446434020996094\n",
      "Epoch [351/1500], Training Loss: 4.4653849521461995, Validation Loss: 15.679560661315918\n",
      "Epoch [352/1500], Training Loss: 4.423987913622675, Validation Loss: 15.096951484680176\n",
      "Epoch [353/1500], Training Loss: 4.393690916834244, Validation Loss: 16.23515510559082\n",
      "Epoch [354/1500], Training Loss: 4.354811566128729, Validation Loss: 15.577276229858398\n",
      "Epoch [355/1500], Training Loss: 4.318675221223304, Validation Loss: 14.8248291015625\n",
      "Epoch [356/1500], Training Loss: 4.285556637788866, Validation Loss: 15.958218574523926\n",
      "Epoch [357/1500], Training Loss: 4.255402813886612, Validation Loss: 15.031120300292969\n",
      "Epoch [358/1500], Training Loss: 4.21517182028373, Validation Loss: 14.616788864135742\n",
      "Epoch [359/1500], Training Loss: 4.193051982626088, Validation Loss: 15.864957809448242\n",
      "Epoch [360/1500], Training Loss: 4.154460371748535, Validation Loss: 15.237447738647461\n",
      "Epoch [361/1500], Training Loss: 4.122662252554774, Validation Loss: 14.396998405456543\n",
      "Epoch [362/1500], Training Loss: 4.092210019186639, Validation Loss: 15.529912948608398\n",
      "Epoch [363/1500], Training Loss: 4.068280676687062, Validation Loss: 14.615944862365723\n",
      "Epoch [364/1500], Training Loss: 4.030322756574039, Validation Loss: 14.21873950958252\n",
      "Epoch [365/1500], Training Loss: 4.010939623453689, Validation Loss: 15.469734191894531\n",
      "Epoch [366/1500], Training Loss: 3.9771458134717443, Validation Loss: 14.873140335083008\n",
      "Epoch [367/1500], Training Loss: 3.950280558486684, Validation Loss: 14.061110496520996\n",
      "Epoch [368/1500], Training Loss: 3.921056852533848, Validation Loss: 14.87290096282959\n",
      "Epoch [369/1500], Training Loss: 3.899302105118759, Validation Loss: 14.915236473083496\n",
      "Epoch [370/1500], Training Loss: 3.870580829046693, Validation Loss: 13.894994735717773\n",
      "Epoch [371/1500], Training Loss: 3.8413198919507328, Validation Loss: 14.508386611938477\n",
      "Epoch [372/1500], Training Loss: 3.8222481079045107, Validation Loss: 14.92065715789795\n",
      "Epoch [373/1500], Training Loss: 3.788264511255045, Validation Loss: 13.669990539550781\n",
      "Epoch [374/1500], Training Loss: 3.7656015574584196, Validation Loss: 15.021525382995605\n",
      "Epoch [375/1500], Training Loss: 3.747285652600549, Validation Loss: 14.267995834350586\n",
      "Epoch [376/1500], Training Loss: 3.717225233186266, Validation Loss: 13.653084754943848\n",
      "Epoch [377/1500], Training Loss: 3.694724208002731, Validation Loss: 14.673287391662598\n",
      "Epoch [378/1500], Training Loss: 3.676377174262834, Validation Loss: 13.955246925354004\n",
      "Epoch [379/1500], Training Loss: 3.6460930481146288, Validation Loss: 13.370287895202637\n",
      "Epoch [380/1500], Training Loss: 3.626637586175211, Validation Loss: 14.624711990356445\n",
      "Epoch [381/1500], Training Loss: 3.6091770502792455, Validation Loss: 13.836617469787598\n",
      "Epoch [382/1500], Training Loss: 3.5791393230409607, Validation Loss: 13.292802810668945\n",
      "Epoch [383/1500], Training Loss: 3.5607216466159537, Validation Loss: 14.505556106567383\n",
      "Epoch [384/1500], Training Loss: 3.544706792912583, Validation Loss: 13.705514907836914\n",
      "Epoch [385/1500], Training Loss: 3.5152825677605857, Validation Loss: 13.162463188171387\n",
      "Epoch [386/1500], Training Loss: 3.4977192149311787, Validation Loss: 14.333011627197266\n",
      "Epoch [387/1500], Training Loss: 3.483162443101119, Validation Loss: 13.701042175292969\n",
      "Epoch [388/1500], Training Loss: 3.455918471810966, Validation Loss: 13.062365531921387\n",
      "Epoch [389/1500], Training Loss: 3.438248061195385, Validation Loss: 13.813359260559082\n",
      "Epoch [390/1500], Training Loss: 3.4248409434013096, Validation Loss: 14.284993171691895\n",
      "Epoch [391/1500], Training Loss: 3.3994383804950608, Validation Loss: 12.91850471496582\n",
      "Epoch [392/1500], Training Loss: 3.379133784220288, Validation Loss: 13.889520645141602\n",
      "Epoch [393/1500], Training Loss: 3.367826899933036, Validation Loss: 14.171263694763184\n",
      "Epoch [394/1500], Training Loss: 3.345344089338755, Validation Loss: 12.886951446533203\n",
      "Epoch [395/1500], Training Loss: 3.325035850249538, Validation Loss: 13.478021621704102\n",
      "Epoch [396/1500], Training Loss: 3.3151713320312046, Validation Loss: 14.186470031738281\n",
      "Epoch [397/1500], Training Loss: 3.2899857745943937, Validation Loss: 12.955175399780273\n",
      "Epoch [398/1500], Training Loss: 3.271274360502352, Validation Loss: 13.169134140014648\n",
      "Epoch [399/1500], Training Loss: 3.2597413958457744, Validation Loss: 14.024429321289062\n",
      "Epoch [400/1500], Training Loss: 3.242629243278617, Validation Loss: 13.699804306030273\n",
      "Epoch [401/1500], Training Loss: 3.234265069000429, Validation Loss: 13.224924087524414\n",
      "Epoch [402/1500], Training Loss: 3.211054736575907, Validation Loss: 12.614389419555664\n",
      "Epoch [403/1500], Training Loss: 3.193099892915693, Validation Loss: 12.886220932006836\n",
      "Epoch [404/1500], Training Loss: 3.1849134750658448, Validation Loss: 13.907835960388184\n",
      "Epoch [405/1500], Training Loss: 3.1618256428761815, Validation Loss: 13.142715454101562\n",
      "Epoch [406/1500], Training Loss: 3.146400550332858, Validation Loss: 12.749094009399414\n",
      "Epoch [407/1500], Training Loss: 3.1286718734914603, Validation Loss: 12.862990379333496\n",
      "Epoch [408/1500], Training Loss: 3.1160724738929297, Validation Loss: 13.569208145141602\n",
      "Epoch [409/1500], Training Loss: 3.1021686797558394, Validation Loss: 13.392302513122559\n",
      "Epoch [410/1500], Training Loss: 3.0936491069453855, Validation Loss: 12.904882431030273\n",
      "Epoch [411/1500], Training Loss: 3.072502907154631, Validation Loss: 12.408214569091797\n",
      "Epoch [412/1500], Training Loss: 3.0562720622562924, Validation Loss: 12.5866117477417\n",
      "Epoch [413/1500], Training Loss: 3.0457437831488403, Validation Loss: 13.51063060760498\n",
      "Epoch [414/1500], Training Loss: 3.0305202348521267, Validation Loss: 13.369851112365723\n",
      "Epoch [415/1500], Training Loss: 3.024611859029923, Validation Loss: 12.946409225463867\n",
      "Epoch [416/1500], Training Loss: 3.0034646004803487, Validation Loss: 12.424544334411621\n",
      "Epoch [417/1500], Training Loss: 2.9864773110359946, Validation Loss: 12.279006958007812\n",
      "Epoch [418/1500], Training Loss: 2.9751673926984092, Validation Loss: 13.313011169433594\n",
      "Epoch [419/1500], Training Loss: 2.9641384177481775, Validation Loss: 13.370715141296387\n",
      "Epoch [420/1500], Training Loss: 2.9592396413325797, Validation Loss: 12.907868385314941\n",
      "Epoch [421/1500], Training Loss: 2.938418212495248, Validation Loss: 12.428253173828125\n",
      "Epoch [422/1500], Training Loss: 2.9207878426901157, Validation Loss: 12.101078033447266\n",
      "Epoch [423/1500], Training Loss: 2.9093534475813425, Validation Loss: 13.166333198547363\n",
      "Epoch [424/1500], Training Loss: 2.8999104375147393, Validation Loss: 13.059287071228027\n",
      "Epoch [425/1500], Training Loss: 2.8896499668046025, Validation Loss: 12.47979736328125\n",
      "Epoch [426/1500], Training Loss: 2.8710811227648647, Validation Loss: 12.307579040527344\n",
      "Epoch [427/1500], Training Loss: 2.856670087199283, Validation Loss: 12.34003734588623\n",
      "Epoch [428/1500], Training Loss: 2.8473269596388793, Validation Loss: 13.070603370666504\n",
      "Epoch [429/1500], Training Loss: 2.8354904755405523, Validation Loss: 12.117826461791992\n",
      "Epoch [430/1500], Training Loss: 2.8138686631356937, Validation Loss: 13.280638694763184\n",
      "Epoch [431/1500], Training Loss: 2.8159598817955604, Validation Loss: 12.177135467529297\n",
      "Early stopping at epoch 431\n",
      "Final Test Loss: 11.985057830810547\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.0004]\n",
    "window_sizes = [5]\n",
    "\n",
    "num_epochs = 1500\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "       # Calculate test loss after training is complete\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Final Test Loss: {test_loss / len(x_test_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0007, window_size=5\n",
      "Epoch [1/1500], Training Loss: 25411.24711494686, Validation Loss: 24401.83984375\n",
      "Epoch [2/1500], Training Loss: 22565.914460869244, Validation Loss: 22063.27734375\n",
      "Epoch [3/1500], Training Loss: 20315.269413412538, Validation Loss: 19922.8046875\n",
      "Epoch [4/1500], Training Loss: 18425.450800612343, Validation Loss: 18147.189453125\n",
      "Epoch [5/1500], Training Loss: 16816.49951734048, Validation Loss: 16608.201171875\n",
      "Epoch [6/1500], Training Loss: 15425.173289274886, Validation Loss: 15265.5595703125\n",
      "Epoch [7/1500], Training Loss: 14219.047912826038, Validation Loss: 14024.4755859375\n",
      "Epoch [8/1500], Training Loss: 13160.80661821665, Validation Loss: 12980.7783203125\n",
      "Epoch [9/1500], Training Loss: 12220.211974309908, Validation Loss: 12113.783203125\n",
      "Epoch [10/1500], Training Loss: 11386.808369850125, Validation Loss: 11349.4111328125\n",
      "Epoch [11/1500], Training Loss: 10645.47638226688, Validation Loss: 10642.748046875\n",
      "Epoch [12/1500], Training Loss: 9987.876616181333, Validation Loss: 10030.107421875\n",
      "Epoch [13/1500], Training Loss: 9409.259115154337, Validation Loss: 9515.2412109375\n",
      "Epoch [14/1500], Training Loss: 8901.218694316187, Validation Loss: 9035.3515625\n",
      "Epoch [15/1500], Training Loss: 8390.691476206543, Validation Loss: 9481.08984375\n",
      "Epoch [16/1500], Training Loss: 8349.685485005493, Validation Loss: 10005.71484375\n",
      "Epoch [17/1500], Training Loss: 7829.9976784662595, Validation Loss: 8372.330078125\n",
      "Epoch [18/1500], Training Loss: 7243.237450209426, Validation Loss: 7716.90771484375\n",
      "Epoch [19/1500], Training Loss: 6751.593138199569, Validation Loss: 7106.03369140625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[5], line 69\u001b[0m\n",
      "\u001b[0;32m     66\u001b[0m labels \u001b[38;5;241m=\u001b[39m y_train_tensor[window_end \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;32m     68\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;32m---> 69\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     70\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "\u001b[0;32m     71\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "Cell \u001b[1;32mIn[3], line 183\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m    179\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;32m    181\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, (h0, c0))\n",
      "\u001b[1;32m--> 183\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n",
      "\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.0004]\n",
    "window_sizes = [5]\n",
    "\n",
    "num_epochs = 1500\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "       # Calculate test loss after training is complete\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Final Test Loss: {test_loss / len(x_test_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0001, window_size=5\n",
      "Epoch [1/1500], Training Loss: 29180.910660816964, Validation Loss: 29115.841796875\n",
      "Epoch [2/1500], Training Loss: 28490.621125741116, Validation Loss: 28476.634765625\n",
      "Epoch [3/1500], Training Loss: 27875.5025118435, Validation Loss: 27871.70703125\n",
      "Epoch [4/1500], Training Loss: 27291.745596651857, Validation Loss: 27296.8671875\n",
      "Epoch [5/1500], Training Loss: 26737.937201601657, Validation Loss: 26751.525390625\n",
      "Epoch [6/1500], Training Loss: 26213.70474501326, Validation Loss: 26235.421875\n",
      "Epoch [7/1500], Training Loss: 25718.683968091846, Validation Loss: 25748.134765625\n",
      "Epoch [8/1500], Training Loss: 25252.437326470175, Validation Loss: 25289.18359375\n",
      "Epoch [9/1500], Training Loss: 24814.36263721805, Validation Loss: 24857.95703125\n",
      "Epoch [10/1500], Training Loss: 24403.736375581386, Validation Loss: 24453.6875\n",
      "Epoch [11/1500], Training Loss: 24019.70402562207, Validation Loss: 24075.513671875\n",
      "Epoch [12/1500], Training Loss: 23661.25877564121, Validation Loss: 23722.40625\n",
      "Epoch [13/1500], Training Loss: 23327.24147916101, Validation Loss: 23393.22265625\n",
      "Epoch [14/1500], Training Loss: 23016.425024952354, Validation Loss: 23086.736328125\n",
      "Epoch [15/1500], Training Loss: 22727.49254975946, Validation Loss: 22801.66796875\n",
      "Epoch [16/1500], Training Loss: 22459.077957117977, Validation Loss: 22536.6484375\n",
      "Epoch [17/1500], Training Loss: 22209.83447117018, Validation Loss: 22290.427734375\n",
      "Epoch [18/1500], Training Loss: 21978.379569960118, Validation Loss: 22061.57421875\n",
      "Epoch [19/1500], Training Loss: 21763.464209251742, Validation Loss: 21848.978515625\n",
      "Epoch [20/1500], Training Loss: 21563.740355708884, Validation Loss: 21651.185546875\n",
      "Epoch [21/1500], Training Loss: 21177.163896991642, Validation Loss: 21188.615234375\n",
      "Epoch [22/1500], Training Loss: 20808.2559255426, Validation Loss: 20890.640625\n",
      "Epoch [23/1500], Training Loss: 20511.06756519889, Validation Loss: 20593.048828125\n",
      "Epoch [24/1500], Training Loss: 20220.27347787476, Validation Loss: 20302.00390625\n",
      "Epoch [25/1500], Training Loss: 19936.19753491822, Validation Loss: 20017.96484375\n",
      "Epoch [26/1500], Training Loss: 19658.64808508066, Validation Loss: 19740.330078125\n",
      "Epoch [27/1500], Training Loss: 19387.2895108537, Validation Loss: 19468.125\n",
      "Epoch [28/1500], Training Loss: 19121.908420385655, Validation Loss: 19201.068359375\n",
      "Epoch [29/1500], Training Loss: 18862.420667121474, Validation Loss: 18939.30078125\n",
      "Epoch [30/1500], Training Loss: 18608.698853001377, Validation Loss: 18683.109375\n",
      "Epoch [31/1500], Training Loss: 18360.570775855645, Validation Loss: 18432.759765625\n",
      "Epoch [32/1500], Training Loss: 18117.843039001014, Validation Loss: 18188.376953125\n",
      "Epoch [33/1500], Training Loss: 17880.200304944687, Validation Loss: 17949.9140625\n",
      "Epoch [34/1500], Training Loss: 17647.410690090583, Validation Loss: 17717.404296875\n",
      "Epoch [35/1500], Training Loss: 17419.30048044928, Validation Loss: 17490.59375\n",
      "Epoch [36/1500], Training Loss: 17195.79790624085, Validation Loss: 17268.935546875\n",
      "Epoch [37/1500], Training Loss: 16976.82993367456, Validation Loss: 17051.6328125\n",
      "Epoch [38/1500], Training Loss: 16762.34301502448, Validation Loss: 16838.130859375\n",
      "Epoch [39/1500], Training Loss: 16552.238307328877, Validation Loss: 16628.30859375\n",
      "Epoch [40/1500], Training Loss: 16346.402281857914, Validation Loss: 16422.306640625\n",
      "Epoch [41/1500], Training Loss: 16144.847260157316, Validation Loss: 16220.0205078125\n",
      "Epoch [42/1500], Training Loss: 15947.320268181695, Validation Loss: 16021.40234375\n",
      "Epoch [43/1500], Training Loss: 15753.811745154788, Validation Loss: 15826.46875\n",
      "Epoch [44/1500], Training Loss: 15564.21489868677, Validation Loss: 15635.21875\n",
      "Epoch [45/1500], Training Loss: 15378.407126288941, Validation Loss: 15447.720703125\n",
      "Epoch [46/1500], Training Loss: 15196.253881464705, Validation Loss: 15263.8525390625\n",
      "Epoch [47/1500], Training Loss: 15017.63286856866, Validation Loss: 15083.7177734375\n",
      "Epoch [48/1500], Training Loss: 14842.484550782061, Validation Loss: 14907.3271484375\n",
      "Epoch [49/1500], Training Loss: 14670.709779784182, Validation Loss: 14734.5\n",
      "Epoch [50/1500], Training Loss: 14502.174678084823, Validation Loss: 14565.34375\n",
      "Epoch [51/1500], Training Loss: 14336.807824063033, Validation Loss: 14400.2998046875\n",
      "Epoch [52/1500], Training Loss: 14174.487118664258, Validation Loss: 14239.9609375\n",
      "Epoch [53/1500], Training Loss: 14014.991801467258, Validation Loss: 14083.8212890625\n",
      "Epoch [54/1500], Training Loss: 13858.185000499841, Validation Loss: 13929.7734375\n",
      "Epoch [55/1500], Training Loss: 13704.049766133676, Validation Loss: 13777.595703125\n",
      "Epoch [56/1500], Training Loss: 13552.535060801238, Validation Loss: 13627.8671875\n",
      "Epoch [57/1500], Training Loss: 13403.616565736536, Validation Loss: 13480.8994140625\n",
      "Epoch [58/1500], Training Loss: 13257.29227715555, Validation Loss: 13336.8232421875\n",
      "Epoch [59/1500], Training Loss: 13113.493001264598, Validation Loss: 13195.6630859375\n",
      "Epoch [60/1500], Training Loss: 12972.145138228507, Validation Loss: 13057.5380859375\n",
      "Epoch [61/1500], Training Loss: 12833.218891993523, Validation Loss: 12922.2880859375\n",
      "Epoch [62/1500], Training Loss: 12696.651687707033, Validation Loss: 12789.6689453125\n",
      "Epoch [63/1500], Training Loss: 12562.382330450699, Validation Loss: 12659.09765625\n",
      "Epoch [64/1500], Training Loss: 12430.405027886416, Validation Loss: 12529.81640625\n",
      "Epoch [65/1500], Training Loss: 12300.655347145463, Validation Loss: 12401.3359375\n",
      "Epoch [66/1500], Training Loss: 12173.091165619671, Validation Loss: 12273.7177734375\n",
      "Epoch [67/1500], Training Loss: 12047.673375350805, Validation Loss: 12147.392578125\n",
      "Epoch [68/1500], Training Loss: 11924.351185132347, Validation Loss: 12022.7734375\n",
      "Epoch [69/1500], Training Loss: 11803.090536445132, Validation Loss: 11900.11328125\n",
      "Epoch [70/1500], Training Loss: 11683.880336008937, Validation Loss: 11779.5205078125\n",
      "Epoch [71/1500], Training Loss: 11566.665123407805, Validation Loss: 11660.90625\n",
      "Epoch [72/1500], Training Loss: 11451.388874400474, Validation Loss: 11544.2197265625\n",
      "Epoch [73/1500], Training Loss: 11338.035480937911, Validation Loss: 11429.3798828125\n",
      "Epoch [74/1500], Training Loss: 11226.553505332307, Validation Loss: 11316.341796875\n",
      "Epoch [75/1500], Training Loss: 11116.923297475354, Validation Loss: 11205.0595703125\n",
      "Epoch [76/1500], Training Loss: 11009.122160656678, Validation Loss: 11095.5078125\n",
      "Epoch [77/1500], Training Loss: 10903.11233739735, Validation Loss: 10987.6259765625\n",
      "Epoch [78/1500], Training Loss: 10798.852109034042, Validation Loss: 10881.5576171875\n",
      "Epoch [79/1500], Training Loss: 10696.33935989265, Validation Loss: 10777.9052734375\n",
      "Epoch [80/1500], Training Loss: 10595.542793806639, Validation Loss: 10677.1318359375\n",
      "Epoch [81/1500], Training Loss: 10496.453222588534, Validation Loss: 10579.3994140625\n",
      "Epoch [82/1500], Training Loss: 10399.030806724875, Validation Loss: 10484.8671875\n",
      "Epoch [83/1500], Training Loss: 10303.163462063001, Validation Loss: 10393.736328125\n",
      "Epoch [84/1500], Training Loss: 10208.64640441034, Validation Loss: 10305.658203125\n",
      "Epoch [85/1500], Training Loss: 10115.31980010266, Validation Loss: 10219.87890625\n",
      "Epoch [86/1500], Training Loss: 10022.90629958214, Validation Loss: 10136.0\n",
      "Epoch [87/1500], Training Loss: 9931.075085337636, Validation Loss: 10052.248046875\n",
      "Epoch [88/1500], Training Loss: 9839.513426626216, Validation Loss: 9967.2177734375\n",
      "Epoch [89/1500], Training Loss: 9747.99999814745, Validation Loss: 9883.1689453125\n",
      "Epoch [90/1500], Training Loss: 9656.440178434354, Validation Loss: 9800.0634765625\n",
      "Epoch [91/1500], Training Loss: 9564.810772372111, Validation Loss: 9716.5673828125\n",
      "Epoch [92/1500], Training Loss: 9473.41714771204, Validation Loss: 9631.7021484375\n",
      "Epoch [93/1500], Training Loss: 9382.676997981298, Validation Loss: 9544.505859375\n",
      "Epoch [94/1500], Training Loss: 9293.009911619347, Validation Loss: 9454.0830078125\n",
      "Epoch [95/1500], Training Loss: 9204.776547371204, Validation Loss: 9360.8916015625\n",
      "Epoch [96/1500], Training Loss: 9118.330148781966, Validation Loss: 9266.44921875\n",
      "Epoch [97/1500], Training Loss: 9033.768112054075, Validation Loss: 9172.3642578125\n",
      "Epoch [98/1500], Training Loss: 8951.003558875149, Validation Loss: 9079.9228515625\n",
      "Epoch [99/1500], Training Loss: 8869.914700031459, Validation Loss: 8989.9423828125\n",
      "Epoch [100/1500], Training Loss: 8790.308829718722, Validation Loss: 8902.6806640625\n",
      "Epoch [101/1500], Training Loss: 8711.981342244095, Validation Loss: 8818.0234375\n",
      "Epoch [102/1500], Training Loss: 8634.876311772587, Validation Loss: 8735.6611328125\n",
      "Epoch [103/1500], Training Loss: 8558.892854946293, Validation Loss: 8655.2607421875\n",
      "Epoch [104/1500], Training Loss: 8483.915289077016, Validation Loss: 8576.5615234375\n",
      "Epoch [105/1500], Training Loss: 8409.935665717021, Validation Loss: 8499.423828125\n",
      "Epoch [106/1500], Training Loss: 8336.845384694412, Validation Loss: 8423.638671875\n",
      "Epoch [107/1500], Training Loss: 8264.64347389373, Validation Loss: 8349.1396484375\n",
      "Epoch [108/1500], Training Loss: 8193.295938843123, Validation Loss: 8275.8125\n",
      "Epoch [109/1500], Training Loss: 8122.795322200993, Validation Loss: 8203.5634765625\n",
      "Epoch [110/1500], Training Loss: 8053.06511637382, Validation Loss: 8132.34228515625\n",
      "Epoch [111/1500], Training Loss: 7984.133858450919, Validation Loss: 8062.1328125\n",
      "Epoch [112/1500], Training Loss: 7915.986145211936, Validation Loss: 7992.91357421875\n",
      "Epoch [113/1500], Training Loss: 7848.6089789961525, Validation Loss: 7924.65087890625\n",
      "Epoch [114/1500], Training Loss: 7781.993480511982, Validation Loss: 7857.2939453125\n",
      "Epoch [115/1500], Training Loss: 7716.115219888695, Validation Loss: 7790.81201171875\n",
      "Epoch [116/1500], Training Loss: 7650.9662733519035, Validation Loss: 7725.16015625\n",
      "Epoch [117/1500], Training Loss: 7586.494483148286, Validation Loss: 7660.275390625\n",
      "Epoch [118/1500], Training Loss: 7522.664192749521, Validation Loss: 7596.123046875\n",
      "Epoch [119/1500], Training Loss: 7459.5051936464115, Validation Loss: 7532.69482421875\n",
      "Epoch [120/1500], Training Loss: 7396.971748579953, Validation Loss: 7469.92919921875\n",
      "Epoch [121/1500], Training Loss: 7335.0220688922955, Validation Loss: 7407.78515625\n",
      "Epoch [122/1500], Training Loss: 7273.637674388165, Validation Loss: 7346.2685546875\n",
      "Epoch [123/1500], Training Loss: 7212.802215705483, Validation Loss: 7285.3662109375\n",
      "Epoch [124/1500], Training Loss: 7152.491529187589, Validation Loss: 7225.04541015625\n",
      "Epoch [125/1500], Training Loss: 7092.693471711634, Validation Loss: 7165.322265625\n",
      "Epoch [126/1500], Training Loss: 7033.41419694396, Validation Loss: 7106.16796875\n",
      "Epoch [127/1500], Training Loss: 6974.610436473827, Validation Loss: 7047.53857421875\n",
      "Epoch [128/1500], Training Loss: 6916.272521671311, Validation Loss: 6989.45556640625\n",
      "Epoch [129/1500], Training Loss: 6858.425086992235, Validation Loss: 6931.89404296875\n",
      "Epoch [130/1500], Training Loss: 6801.060235867849, Validation Loss: 6874.8671875\n",
      "Epoch [131/1500], Training Loss: 6744.184292486648, Validation Loss: 6818.3701171875\n",
      "Epoch [132/1500], Training Loss: 6687.774650403776, Validation Loss: 6762.3564453125\n",
      "Epoch [133/1500], Training Loss: 6631.816275533052, Validation Loss: 6706.82275390625\n",
      "Epoch [134/1500], Training Loss: 6576.2968786426545, Validation Loss: 6651.7724609375\n",
      "Epoch [135/1500], Training Loss: 6521.2342789563745, Validation Loss: 6597.20361328125\n",
      "Epoch [136/1500], Training Loss: 6466.6310193793315, Validation Loss: 6543.111328125\n",
      "Epoch [137/1500], Training Loss: 6412.473911976067, Validation Loss: 6489.49462890625\n",
      "Epoch [138/1500], Training Loss: 6358.794332343127, Validation Loss: 6436.373046875\n",
      "Epoch [139/1500], Training Loss: 6305.580557919482, Validation Loss: 6383.72802734375\n",
      "Epoch [140/1500], Training Loss: 6252.810007002795, Validation Loss: 6331.53759765625\n",
      "Epoch [141/1500], Training Loss: 6200.503492960722, Validation Loss: 6279.8349609375\n",
      "Epoch [142/1500], Training Loss: 6148.662046997083, Validation Loss: 6228.58935546875\n",
      "Epoch [143/1500], Training Loss: 6097.269854651426, Validation Loss: 6177.7861328125\n",
      "Epoch [144/1500], Training Loss: 6046.3167773347295, Validation Loss: 6127.44970703125\n",
      "Epoch [145/1500], Training Loss: 5995.820247585724, Validation Loss: 6077.568359375\n",
      "Epoch [146/1500], Training Loss: 5945.778214099004, Validation Loss: 6028.14404296875\n",
      "Epoch [147/1500], Training Loss: 5896.210880617793, Validation Loss: 5979.19775390625\n",
      "Epoch [148/1500], Training Loss: 5847.10545181647, Validation Loss: 5930.7216796875\n",
      "Epoch [149/1500], Training Loss: 5798.456909937694, Validation Loss: 5882.69287109375\n",
      "Epoch [150/1500], Training Loss: 5750.278284960651, Validation Loss: 5835.1298828125\n",
      "Epoch [151/1500], Training Loss: 5702.5904371424585, Validation Loss: 5788.0419921875\n",
      "Epoch [152/1500], Training Loss: 5655.39371831281, Validation Loss: 5741.44091796875\n",
      "Epoch [153/1500], Training Loss: 5608.68379275895, Validation Loss: 5695.30126953125\n",
      "Epoch [154/1500], Training Loss: 5562.455470894523, Validation Loss: 5649.64208984375\n",
      "Epoch [155/1500], Training Loss: 5516.725887380462, Validation Loss: 5604.4599609375\n",
      "Epoch [156/1500], Training Loss: 5471.4900525229605, Validation Loss: 5559.75537109375\n",
      "Epoch [157/1500], Training Loss: 5426.762513731078, Validation Loss: 5515.5283203125\n",
      "Epoch [158/1500], Training Loss: 5382.53113122467, Validation Loss: 5471.76220703125\n",
      "Epoch [159/1500], Training Loss: 5338.7667564997455, Validation Loss: 5428.45361328125\n",
      "Epoch [160/1500], Training Loss: 5295.4902984560285, Validation Loss: 5385.62451171875\n",
      "Epoch [161/1500], Training Loss: 5252.712574769055, Validation Loss: 5343.283203125\n",
      "Epoch [162/1500], Training Loss: 5210.434472374541, Validation Loss: 5301.43310546875\n",
      "Epoch [163/1500], Training Loss: 5168.637465238065, Validation Loss: 5260.041015625\n",
      "Epoch [164/1500], Training Loss: 5127.303881126636, Validation Loss: 5219.12841796875\n",
      "Epoch [165/1500], Training Loss: 5086.421455876848, Validation Loss: 5178.69580078125\n",
      "Epoch [166/1500], Training Loss: 5045.98440340264, Validation Loss: 5138.72802734375\n",
      "Epoch [167/1500], Training Loss: 5005.967927524793, Validation Loss: 5099.234375\n",
      "Epoch [168/1500], Training Loss: 4966.353087762634, Validation Loss: 5060.21826171875\n",
      "Epoch [169/1500], Training Loss: 4927.084338110255, Validation Loss: 5021.6591796875\n",
      "Epoch [170/1500], Training Loss: 4888.075321885844, Validation Loss: 4983.54248046875\n",
      "Epoch [171/1500], Training Loss: 4849.145474086591, Validation Loss: 4945.53564453125\n",
      "Epoch [172/1500], Training Loss: 4809.803714302813, Validation Loss: 4906.70458984375\n",
      "Epoch [173/1500], Training Loss: 4768.828413316199, Validation Loss: 4865.87890625\n",
      "Epoch [174/1500], Training Loss: 4723.867663654635, Validation Loss: 4822.31884765625\n",
      "Epoch [175/1500], Training Loss: 4675.243948096352, Validation Loss: 4777.67333984375\n",
      "Epoch [176/1500], Training Loss: 4630.194715219175, Validation Loss: 4734.37451171875\n",
      "Epoch [177/1500], Training Loss: 4589.3316019386975, Validation Loss: 4692.67724609375\n",
      "Epoch [178/1500], Training Loss: 4549.655177415804, Validation Loss: 4652.75634765625\n",
      "Epoch [179/1500], Training Loss: 4510.4843632739385, Validation Loss: 4613.9248046875\n",
      "Epoch [180/1500], Training Loss: 4471.717307086011, Validation Loss: 4575.734375\n",
      "Epoch [181/1500], Training Loss: 4433.360229775398, Validation Loss: 4538.0009765625\n",
      "Epoch [182/1500], Training Loss: 4395.372540757574, Validation Loss: 4500.6357421875\n",
      "Epoch [183/1500], Training Loss: 4357.745466434537, Validation Loss: 4463.6103515625\n",
      "Epoch [184/1500], Training Loss: 4320.4620102536355, Validation Loss: 4426.91748046875\n",
      "Epoch [185/1500], Training Loss: 4283.514598573925, Validation Loss: 4390.55419921875\n",
      "Epoch [186/1500], Training Loss: 4246.888958801288, Validation Loss: 4354.52587890625\n",
      "Epoch [187/1500], Training Loss: 4210.580194479287, Validation Loss: 4318.814453125\n",
      "Epoch [188/1500], Training Loss: 4174.570528772357, Validation Loss: 4283.41455078125\n",
      "Epoch [189/1500], Training Loss: 4138.852420717644, Validation Loss: 4248.30859375\n",
      "Epoch [190/1500], Training Loss: 4103.423017639755, Validation Loss: 4213.49658203125\n",
      "Epoch [191/1500], Training Loss: 4068.275851727175, Validation Loss: 4178.970703125\n",
      "Epoch [192/1500], Training Loss: 4033.401505849087, Validation Loss: 4144.7158203125\n",
      "Epoch [193/1500], Training Loss: 3998.7972249830973, Validation Loss: 4110.7373046875\n",
      "Epoch [194/1500], Training Loss: 3964.4749796186948, Validation Loss: 4077.028076171875\n",
      "Epoch [195/1500], Training Loss: 3930.4107392586106, Validation Loss: 4043.583984375\n",
      "Epoch [196/1500], Training Loss: 3896.6147812469, Validation Loss: 4010.40234375\n",
      "Epoch [197/1500], Training Loss: 3863.0857413026406, Validation Loss: 3977.485107421875\n",
      "Epoch [198/1500], Training Loss: 3829.8164552223107, Validation Loss: 3944.8251953125\n",
      "Epoch [199/1500], Training Loss: 3796.810599174741, Validation Loss: 3912.427734375\n",
      "Epoch [200/1500], Training Loss: 3764.0581988365425, Validation Loss: 3880.279541015625\n",
      "Epoch [201/1500], Training Loss: 3731.562457178738, Validation Loss: 3848.38330078125\n",
      "Epoch [202/1500], Training Loss: 3699.31337024241, Validation Loss: 3816.73681640625\n",
      "Epoch [203/1500], Training Loss: 3667.326142567357, Validation Loss: 3785.345703125\n",
      "Epoch [204/1500], Training Loss: 3635.608365073351, Validation Loss: 3754.23974609375\n",
      "Epoch [205/1500], Training Loss: 3604.149723393604, Validation Loss: 3723.38671875\n",
      "Epoch [206/1500], Training Loss: 3572.942900842899, Validation Loss: 3692.784912109375\n",
      "Epoch [207/1500], Training Loss: 3541.996328237045, Validation Loss: 3662.445556640625\n",
      "Epoch [208/1500], Training Loss: 3511.3028844077608, Validation Loss: 3632.37060546875\n",
      "Epoch [209/1500], Training Loss: 3480.879117287703, Validation Loss: 3602.56591796875\n",
      "Epoch [210/1500], Training Loss: 3450.7185171813403, Validation Loss: 3573.03173828125\n",
      "Epoch [211/1500], Training Loss: 3420.8240361824683, Validation Loss: 3543.768798828125\n",
      "Epoch [212/1500], Training Loss: 3391.195065186736, Validation Loss: 3514.77587890625\n",
      "Epoch [213/1500], Training Loss: 3361.8445811727993, Validation Loss: 3486.064208984375\n",
      "Epoch [214/1500], Training Loss: 3332.758896285807, Validation Loss: 3457.62939453125\n",
      "Epoch [215/1500], Training Loss: 3303.9496525901045, Validation Loss: 3429.48095703125\n",
      "Epoch [216/1500], Training Loss: 3275.418748825515, Validation Loss: 3401.617431640625\n",
      "Epoch [217/1500], Training Loss: 3247.1637354644936, Validation Loss: 3374.0283203125\n",
      "Epoch [218/1500], Training Loss: 3219.1892892586916, Validation Loss: 3346.73193359375\n",
      "Epoch [219/1500], Training Loss: 3191.497356467728, Validation Loss: 3319.72412109375\n",
      "Epoch [220/1500], Training Loss: 3164.091986949582, Validation Loss: 3293.0146484375\n",
      "Epoch [221/1500], Training Loss: 3136.974445430631, Validation Loss: 3266.58642578125\n",
      "Epoch [222/1500], Training Loss: 3110.1453265008035, Validation Loss: 3240.466552734375\n",
      "Epoch [223/1500], Training Loss: 3083.6058510513235, Validation Loss: 3214.622314453125\n",
      "Epoch [224/1500], Training Loss: 3057.3419509761243, Validation Loss: 3189.0556640625\n",
      "Epoch [225/1500], Training Loss: 3031.354942722287, Validation Loss: 3163.771484375\n",
      "Epoch [226/1500], Training Loss: 3005.6444106415643, Validation Loss: 3138.762939453125\n",
      "Epoch [227/1500], Training Loss: 2980.2035688710766, Validation Loss: 3114.02685546875\n",
      "Epoch [228/1500], Training Loss: 2955.016005763216, Validation Loss: 3089.5634765625\n",
      "Epoch [229/1500], Training Loss: 2930.106637607072, Validation Loss: 3065.376953125\n",
      "Epoch [230/1500], Training Loss: 2905.47034770721, Validation Loss: 3041.461669921875\n",
      "Epoch [231/1500], Training Loss: 2881.0961427301295, Validation Loss: 3017.806396484375\n",
      "Epoch [232/1500], Training Loss: 2856.9747500963026, Validation Loss: 2994.40283203125\n",
      "Epoch [233/1500], Training Loss: 2833.1055188215514, Validation Loss: 2971.2607421875\n",
      "Epoch [234/1500], Training Loss: 2809.487875781391, Validation Loss: 2948.361572265625\n",
      "Epoch [235/1500], Training Loss: 2786.1127064358293, Validation Loss: 2925.7041015625\n",
      "Epoch [236/1500], Training Loss: 2762.979697802947, Validation Loss: 2903.28515625\n",
      "Epoch [237/1500], Training Loss: 2740.0936428031846, Validation Loss: 2881.116943359375\n",
      "Epoch [238/1500], Training Loss: 2717.449818319679, Validation Loss: 2859.18603515625\n",
      "Epoch [239/1500], Training Loss: 2695.0367751136423, Validation Loss: 2837.483154296875\n",
      "Epoch [240/1500], Training Loss: 2672.8555384769015, Validation Loss: 2816.01171875\n",
      "Epoch [241/1500], Training Loss: 2650.9129287393853, Validation Loss: 2794.776611328125\n",
      "Epoch [242/1500], Training Loss: 2629.2027641858226, Validation Loss: 2773.7666015625\n",
      "Epoch [243/1500], Training Loss: 2607.717357563156, Validation Loss: 2752.984619140625\n",
      "Epoch [244/1500], Training Loss: 2586.461494870879, Validation Loss: 2732.4287109375\n",
      "Epoch [245/1500], Training Loss: 2565.4374058011117, Validation Loss: 2712.10009765625\n",
      "Epoch [246/1500], Training Loss: 2544.6411412873267, Validation Loss: 2692.002197265625\n",
      "Epoch [247/1500], Training Loss: 2524.067025833059, Validation Loss: 2672.121826171875\n",
      "Epoch [248/1500], Training Loss: 2503.7208969782264, Validation Loss: 2652.4736328125\n",
      "Epoch [249/1500], Training Loss: 2483.6067311161173, Validation Loss: 2633.04931640625\n",
      "Epoch [250/1500], Training Loss: 2463.717763537779, Validation Loss: 2613.847412109375\n",
      "Epoch [251/1500], Training Loss: 2444.0518211573963, Validation Loss: 2594.8642578125\n",
      "Epoch [252/1500], Training Loss: 2424.599579381173, Validation Loss: 2576.0888671875\n",
      "Epoch [253/1500], Training Loss: 2405.370147426533, Validation Loss: 2557.531982421875\n",
      "Epoch [254/1500], Training Loss: 2386.355800238402, Validation Loss: 2539.193359375\n",
      "Epoch [255/1500], Training Loss: 2367.5558491333427, Validation Loss: 2521.058837890625\n",
      "Epoch [256/1500], Training Loss: 2348.9579821476623, Validation Loss: 2503.1162109375\n",
      "Epoch [257/1500], Training Loss: 2330.564675572116, Validation Loss: 2485.37744140625\n",
      "Epoch [258/1500], Training Loss: 2312.379926879792, Validation Loss: 2467.8349609375\n",
      "Epoch [259/1500], Training Loss: 2294.3868696596105, Validation Loss: 2450.47509765625\n",
      "Epoch [260/1500], Training Loss: 2276.5826741290703, Validation Loss: 2433.296630859375\n",
      "Epoch [261/1500], Training Loss: 2258.9569097996987, Validation Loss: 2416.287109375\n",
      "Epoch [262/1500], Training Loss: 2241.504354179375, Validation Loss: 2399.44140625\n",
      "Epoch [263/1500], Training Loss: 2224.2244361276785, Validation Loss: 2382.76025390625\n",
      "Epoch [264/1500], Training Loss: 2207.1086509846477, Validation Loss: 2366.24072265625\n",
      "Epoch [265/1500], Training Loss: 2190.157355143257, Validation Loss: 2349.874267578125\n",
      "Epoch [266/1500], Training Loss: 2173.3539605396036, Validation Loss: 2333.654052734375\n",
      "Epoch [267/1500], Training Loss: 2156.697899140438, Validation Loss: 2317.57470703125\n",
      "Epoch [268/1500], Training Loss: 2140.1871841502734, Validation Loss: 2301.6337890625\n",
      "Epoch [269/1500], Training Loss: 2123.8139333517297, Validation Loss: 2285.826904296875\n",
      "Epoch [270/1500], Training Loss: 2107.571446040769, Validation Loss: 2270.14404296875\n",
      "Epoch [271/1500], Training Loss: 2091.463788742036, Validation Loss: 2254.59619140625\n",
      "Epoch [272/1500], Training Loss: 2075.481664849599, Validation Loss: 2239.162109375\n",
      "Epoch [273/1500], Training Loss: 2059.620474474216, Validation Loss: 2223.846435546875\n",
      "Epoch [274/1500], Training Loss: 2043.8717760917923, Validation Loss: 2208.63916015625\n",
      "Epoch [275/1500], Training Loss: 2028.241376688995, Validation Loss: 2193.54541015625\n",
      "Epoch [276/1500], Training Loss: 2012.736933593128, Validation Loss: 2178.5673828125\n",
      "Epoch [277/1500], Training Loss: 1997.3443451373498, Validation Loss: 2163.70068359375\n",
      "Epoch [278/1500], Training Loss: 1982.0676722891492, Validation Loss: 2148.9404296875\n",
      "Epoch [279/1500], Training Loss: 1966.9043952224029, Validation Loss: 2134.29296875\n",
      "Epoch [280/1500], Training Loss: 1951.8569256627404, Validation Loss: 2119.75439453125\n",
      "Epoch [281/1500], Training Loss: 1936.926318357713, Validation Loss: 2105.328369140625\n",
      "Epoch [282/1500], Training Loss: 1922.111433587211, Validation Loss: 2091.008056640625\n",
      "Epoch [283/1500], Training Loss: 1907.409284673915, Validation Loss: 2076.80322265625\n",
      "Epoch [284/1500], Training Loss: 1892.823764984252, Validation Loss: 2062.712646484375\n",
      "Epoch [285/1500], Training Loss: 1878.3533868749935, Validation Loss: 2048.7275390625\n",
      "Epoch [286/1500], Training Loss: 1863.9924404547528, Validation Loss: 2034.84912109375\n",
      "Epoch [287/1500], Training Loss: 1849.7422344540998, Validation Loss: 2021.0810546875\n",
      "Epoch [288/1500], Training Loss: 1835.615184138988, Validation Loss: 2007.4285888671875\n",
      "Epoch [289/1500], Training Loss: 1821.607111860514, Validation Loss: 1993.8885498046875\n",
      "Epoch [290/1500], Training Loss: 1807.7192861522997, Validation Loss: 1980.4608154296875\n",
      "Epoch [291/1500], Training Loss: 1793.947333464027, Validation Loss: 1967.1534423828125\n",
      "Epoch [292/1500], Training Loss: 1780.301186261365, Validation Loss: 1953.954833984375\n",
      "Epoch [293/1500], Training Loss: 1766.7761165096786, Validation Loss: 1940.8741455078125\n",
      "Epoch [294/1500], Training Loss: 1753.376872352555, Validation Loss: 1927.9056396484375\n",
      "Epoch [295/1500], Training Loss: 1740.0917316443943, Validation Loss: 1915.044189453125\n",
      "Epoch [296/1500], Training Loss: 1726.9227735020522, Validation Loss: 1902.2906494140625\n",
      "Epoch [297/1500], Training Loss: 1713.8716325373823, Validation Loss: 1889.6495361328125\n",
      "Epoch [298/1500], Training Loss: 1700.9446935796225, Validation Loss: 1877.1226806640625\n",
      "Epoch [299/1500], Training Loss: 1688.1478478120569, Validation Loss: 1864.7186279296875\n",
      "Epoch [300/1500], Training Loss: 1675.4785996294772, Validation Loss: 1852.4285888671875\n",
      "Epoch [301/1500], Training Loss: 1662.925967517651, Validation Loss: 1840.2470703125\n",
      "Epoch [302/1500], Training Loss: 1650.4996789630989, Validation Loss: 1828.1839599609375\n",
      "Epoch [303/1500], Training Loss: 1638.195055597424, Validation Loss: 1816.2294921875\n",
      "Epoch [304/1500], Training Loss: 1626.0056424138472, Validation Loss: 1804.384765625\n",
      "Epoch [305/1500], Training Loss: 1613.9361260796554, Validation Loss: 1792.6552734375\n",
      "Epoch [306/1500], Training Loss: 1601.9897994062087, Validation Loss: 1781.040283203125\n",
      "Epoch [307/1500], Training Loss: 1590.1695172594673, Validation Loss: 1769.5391845703125\n",
      "Epoch [308/1500], Training Loss: 1578.4691659856182, Validation Loss: 1758.1488037109375\n",
      "Epoch [309/1500], Training Loss: 1566.8867019589024, Validation Loss: 1746.8731689453125\n",
      "Epoch [310/1500], Training Loss: 1555.4247834069154, Validation Loss: 1735.706787109375\n",
      "Epoch [311/1500], Training Loss: 1544.0815406285296, Validation Loss: 1724.64990234375\n",
      "Epoch [312/1500], Training Loss: 1532.8585228381369, Validation Loss: 1713.708740234375\n",
      "Epoch [313/1500], Training Loss: 1521.7585446712326, Validation Loss: 1702.875244140625\n",
      "Epoch [314/1500], Training Loss: 1510.7706854834541, Validation Loss: 1692.1431884765625\n",
      "Epoch [315/1500], Training Loss: 1499.8966303183606, Validation Loss: 1681.520263671875\n",
      "Epoch [316/1500], Training Loss: 1489.1411932257733, Validation Loss: 1671.0029296875\n",
      "Epoch [317/1500], Training Loss: 1478.4995536450385, Validation Loss: 1660.59130859375\n",
      "Epoch [318/1500], Training Loss: 1467.9699498583404, Validation Loss: 1650.2796630859375\n",
      "Epoch [319/1500], Training Loss: 1457.5459304491576, Validation Loss: 1640.0699462890625\n",
      "Epoch [320/1500], Training Loss: 1447.2323085140856, Validation Loss: 1629.9569091796875\n",
      "Epoch [321/1500], Training Loss: 1437.0262220616419, Validation Loss: 1619.949462890625\n",
      "Epoch [322/1500], Training Loss: 1426.9266239151655, Validation Loss: 1610.0343017578125\n",
      "Epoch [323/1500], Training Loss: 1416.9277555038605, Validation Loss: 1600.2159423828125\n",
      "Epoch [324/1500], Training Loss: 1407.0304717838355, Validation Loss: 1590.493896484375\n",
      "Epoch [325/1500], Training Loss: 1397.2316732620286, Validation Loss: 1580.8590087890625\n",
      "Epoch [326/1500], Training Loss: 1387.5293100852568, Validation Loss: 1571.315673828125\n",
      "Epoch [327/1500], Training Loss: 1377.9256589612312, Validation Loss: 1561.870849609375\n",
      "Epoch [328/1500], Training Loss: 1368.4209573426106, Validation Loss: 1552.5096435546875\n",
      "Epoch [329/1500], Training Loss: 1359.01113692463, Validation Loss: 1543.2391357421875\n",
      "Epoch [330/1500], Training Loss: 1349.6866742817742, Validation Loss: 1534.0455322265625\n",
      "Epoch [331/1500], Training Loss: 1340.4473014639395, Validation Loss: 1524.935546875\n",
      "Epoch [332/1500], Training Loss: 1331.29869724543, Validation Loss: 1515.907470703125\n",
      "Epoch [333/1500], Training Loss: 1322.2340863292025, Validation Loss: 1506.9556884765625\n",
      "Epoch [334/1500], Training Loss: 1313.2490429674344, Validation Loss: 1498.0775146484375\n",
      "Epoch [335/1500], Training Loss: 1304.3484901504398, Validation Loss: 1489.2763671875\n",
      "Epoch [336/1500], Training Loss: 1295.5300464091101, Validation Loss: 1480.5546875\n",
      "Epoch [337/1500], Training Loss: 1286.7894330550128, Validation Loss: 1471.9046630859375\n",
      "Epoch [338/1500], Training Loss: 1278.1264620868094, Validation Loss: 1463.321044921875\n",
      "Epoch [339/1500], Training Loss: 1269.5379124375902, Validation Loss: 1454.8094482421875\n",
      "Epoch [340/1500], Training Loss: 1261.0277225787454, Validation Loss: 1446.3665771484375\n",
      "Epoch [341/1500], Training Loss: 1252.5903716638736, Validation Loss: 1437.9925537109375\n",
      "Epoch [342/1500], Training Loss: 1244.2247178721957, Validation Loss: 1429.6827392578125\n",
      "Epoch [343/1500], Training Loss: 1235.9296616586105, Validation Loss: 1421.4327392578125\n",
      "Epoch [344/1500], Training Loss: 1227.7009308714096, Validation Loss: 1413.245361328125\n",
      "Epoch [345/1500], Training Loss: 1219.540767759658, Validation Loss: 1405.1177978515625\n",
      "Epoch [346/1500], Training Loss: 1211.4474455978411, Validation Loss: 1397.048095703125\n",
      "Epoch [347/1500], Training Loss: 1203.416276753887, Validation Loss: 1389.0345458984375\n",
      "Epoch [348/1500], Training Loss: 1195.4477622295708, Validation Loss: 1381.0718994140625\n",
      "Epoch [349/1500], Training Loss: 1187.538358491029, Validation Loss: 1373.15673828125\n",
      "Epoch [350/1500], Training Loss: 1179.6894727417175, Validation Loss: 1365.294921875\n",
      "Epoch [351/1500], Training Loss: 1171.8948332783286, Validation Loss: 1357.4736328125\n",
      "Epoch [352/1500], Training Loss: 1164.15395429536, Validation Loss: 1349.69091796875\n",
      "Epoch [353/1500], Training Loss: 1156.4669216162324, Validation Loss: 1341.95166015625\n",
      "Epoch [354/1500], Training Loss: 1148.8344601017657, Validation Loss: 1334.24462890625\n",
      "Epoch [355/1500], Training Loss: 1141.2541212750925, Validation Loss: 1326.5703125\n",
      "Epoch [356/1500], Training Loss: 1133.725148453842, Validation Loss: 1318.926513671875\n",
      "Epoch [357/1500], Training Loss: 1126.2453678650468, Validation Loss: 1311.314208984375\n",
      "Epoch [358/1500], Training Loss: 1118.8152504455982, Validation Loss: 1303.72021484375\n",
      "Epoch [359/1500], Training Loss: 1111.4321601671536, Validation Loss: 1296.15087890625\n",
      "Epoch [360/1500], Training Loss: 1104.0949686430101, Validation Loss: 1288.5987548828125\n",
      "Epoch [361/1500], Training Loss: 1096.802659501682, Validation Loss: 1281.0675048828125\n",
      "Epoch [362/1500], Training Loss: 1089.5541501596492, Validation Loss: 1273.5533447265625\n",
      "Epoch [363/1500], Training Loss: 1082.350610506699, Validation Loss: 1266.055419921875\n",
      "Epoch [364/1500], Training Loss: 1075.191642186225, Validation Loss: 1258.5712890625\n",
      "Epoch [365/1500], Training Loss: 1068.0754672782007, Validation Loss: 1251.106201171875\n",
      "Epoch [366/1500], Training Loss: 1061.0041425285922, Validation Loss: 1243.657958984375\n",
      "Epoch [367/1500], Training Loss: 1053.9754456217393, Validation Loss: 1236.2237548828125\n",
      "Epoch [368/1500], Training Loss: 1046.9860513077203, Validation Loss: 1228.8038330078125\n",
      "Epoch [369/1500], Training Loss: 1040.0399027737124, Validation Loss: 1221.40283203125\n",
      "Epoch [370/1500], Training Loss: 1033.1355818249808, Validation Loss: 1214.016357421875\n",
      "Epoch [371/1500], Training Loss: 1026.2737702365116, Validation Loss: 1206.648193359375\n",
      "Epoch [372/1500], Training Loss: 1019.4519076950335, Validation Loss: 1199.2919921875\n",
      "Epoch [373/1500], Training Loss: 1012.6693969223986, Validation Loss: 1191.9525146484375\n",
      "Epoch [374/1500], Training Loss: 1005.9222685130459, Validation Loss: 1184.6263427734375\n",
      "Epoch [375/1500], Training Loss: 999.2157500989321, Validation Loss: 1177.31689453125\n",
      "Epoch [376/1500], Training Loss: 992.5464821867483, Validation Loss: 1170.0238037109375\n",
      "Epoch [377/1500], Training Loss: 985.9149645186874, Validation Loss: 1162.7484130859375\n",
      "Epoch [378/1500], Training Loss: 979.3179879331811, Validation Loss: 1155.489013671875\n",
      "Epoch [379/1500], Training Loss: 972.7563884292186, Validation Loss: 1148.245849609375\n",
      "Epoch [380/1500], Training Loss: 966.2307513825136, Validation Loss: 1141.0211181640625\n",
      "Epoch [381/1500], Training Loss: 959.7425272151354, Validation Loss: 1133.8170166015625\n",
      "Epoch [382/1500], Training Loss: 953.290389032263, Validation Loss: 1126.635498046875\n",
      "Epoch [383/1500], Training Loss: 946.8753701559893, Validation Loss: 1119.47412109375\n",
      "Epoch [384/1500], Training Loss: 940.4954662542739, Validation Loss: 1112.33349609375\n",
      "Epoch [385/1500], Training Loss: 934.1514433173849, Validation Loss: 1105.2186279296875\n",
      "Epoch [386/1500], Training Loss: 927.8435228260053, Validation Loss: 1098.124755859375\n",
      "Epoch [387/1500], Training Loss: 921.5727745787276, Validation Loss: 1091.0531005859375\n",
      "Epoch [388/1500], Training Loss: 915.3395739752756, Validation Loss: 1084.005859375\n",
      "Epoch [389/1500], Training Loss: 909.1422054765674, Validation Loss: 1076.9832763671875\n",
      "Epoch [390/1500], Training Loss: 902.9800350911294, Validation Loss: 1069.9779052734375\n",
      "Epoch [391/1500], Training Loss: 896.8513102708825, Validation Loss: 1062.9915771484375\n",
      "Epoch [392/1500], Training Loss: 890.7576083307678, Validation Loss: 1056.025634765625\n",
      "Epoch [393/1500], Training Loss: 884.7040376992767, Validation Loss: 1049.079345703125\n",
      "Epoch [394/1500], Training Loss: 878.687917234446, Validation Loss: 1042.15087890625\n",
      "Epoch [395/1500], Training Loss: 872.7073629585028, Validation Loss: 1035.2327880859375\n",
      "Epoch [396/1500], Training Loss: 866.7583545165395, Validation Loss: 1028.3204345703125\n",
      "Epoch [397/1500], Training Loss: 860.8396471148267, Validation Loss: 1021.4110717773438\n",
      "Epoch [398/1500], Training Loss: 854.9528119374829, Validation Loss: 1014.5079956054688\n",
      "Epoch [399/1500], Training Loss: 849.0987320703953, Validation Loss: 1007.608154296875\n",
      "Epoch [400/1500], Training Loss: 843.2777897966823, Validation Loss: 1000.7061767578125\n",
      "Epoch [401/1500], Training Loss: 837.4893695601265, Validation Loss: 993.8067626953125\n",
      "Epoch [402/1500], Training Loss: 831.7310058162864, Validation Loss: 986.9039916992188\n",
      "Epoch [403/1500], Training Loss: 826.0017246274626, Validation Loss: 980.0022583007812\n",
      "Epoch [404/1500], Training Loss: 820.300595729345, Validation Loss: 973.1007690429688\n",
      "Epoch [405/1500], Training Loss: 814.6269446340813, Validation Loss: 966.2000122070312\n",
      "Epoch [406/1500], Training Loss: 808.9799198706748, Validation Loss: 959.3057250976562\n",
      "Epoch [407/1500], Training Loss: 803.3618410815594, Validation Loss: 952.4207763671875\n",
      "Epoch [408/1500], Training Loss: 797.7735102599304, Validation Loss: 945.5517578125\n",
      "Epoch [409/1500], Training Loss: 792.2136755268778, Validation Loss: 938.7022705078125\n",
      "Epoch [410/1500], Training Loss: 786.6800911333576, Validation Loss: 931.873046875\n",
      "Epoch [411/1500], Training Loss: 781.1772724598015, Validation Loss: 925.0731811523438\n",
      "Epoch [412/1500], Training Loss: 775.701577989131, Validation Loss: 918.3018188476562\n",
      "Epoch [413/1500], Training Loss: 770.2500002992819, Validation Loss: 911.5560913085938\n",
      "Epoch [414/1500], Training Loss: 764.8267348828019, Validation Loss: 904.8484497070312\n",
      "Epoch [415/1500], Training Loss: 759.4328277110836, Validation Loss: 898.1718139648438\n",
      "Epoch [416/1500], Training Loss: 754.0673954385721, Validation Loss: 891.5382080078125\n",
      "Epoch [417/1500], Training Loss: 748.7299411381028, Validation Loss: 884.944091796875\n",
      "Epoch [418/1500], Training Loss: 743.4206332439793, Validation Loss: 878.3876953125\n",
      "Epoch [419/1500], Training Loss: 738.1412452516636, Validation Loss: 871.8798217773438\n",
      "Epoch [420/1500], Training Loss: 732.889732513131, Validation Loss: 865.416015625\n",
      "Epoch [421/1500], Training Loss: 727.6666352912741, Validation Loss: 859.0\n",
      "Epoch [422/1500], Training Loss: 722.472958911283, Validation Loss: 852.6337890625\n",
      "Epoch [423/1500], Training Loss: 717.3075153752142, Validation Loss: 846.3195190429688\n",
      "Epoch [424/1500], Training Loss: 712.1730085332431, Validation Loss: 840.0618896484375\n",
      "Epoch [425/1500], Training Loss: 707.0680132482863, Validation Loss: 833.8566284179688\n",
      "Epoch [426/1500], Training Loss: 701.9930587183603, Validation Loss: 827.7088012695312\n",
      "Epoch [427/1500], Training Loss: 696.9501297663624, Validation Loss: 821.6227416992188\n",
      "Epoch [428/1500], Training Loss: 691.9410261828604, Validation Loss: 815.5975952148438\n",
      "Epoch [429/1500], Training Loss: 686.9673126807204, Validation Loss: 809.6338500976562\n",
      "Epoch [430/1500], Training Loss: 682.0274263725012, Validation Loss: 803.72998046875\n",
      "Epoch [431/1500], Training Loss: 677.1206393290771, Validation Loss: 797.8838500976562\n",
      "Epoch [432/1500], Training Loss: 672.251290175116, Validation Loss: 792.1028442382812\n",
      "Epoch [433/1500], Training Loss: 667.4151097898224, Validation Loss: 786.382080078125\n",
      "Epoch [434/1500], Training Loss: 662.6149013778578, Validation Loss: 780.7205200195312\n",
      "Epoch [435/1500], Training Loss: 657.8502382306816, Validation Loss: 775.1183471679688\n",
      "Epoch [436/1500], Training Loss: 653.1190076006252, Validation Loss: 769.5726928710938\n",
      "Epoch [437/1500], Training Loss: 648.4214546099769, Validation Loss: 764.0825805664062\n",
      "Epoch [438/1500], Training Loss: 643.7589840668924, Validation Loss: 758.6483764648438\n",
      "Epoch [439/1500], Training Loss: 639.130614285768, Validation Loss: 753.26611328125\n",
      "Epoch [440/1500], Training Loss: 634.5342314282599, Validation Loss: 747.9322509765625\n",
      "Epoch [441/1500], Training Loss: 629.9700580037348, Validation Loss: 742.6467895507812\n",
      "Epoch [442/1500], Training Loss: 625.437070244343, Validation Loss: 737.4105224609375\n",
      "Epoch [443/1500], Training Loss: 620.938465386412, Validation Loss: 732.2189331054688\n",
      "Epoch [444/1500], Training Loss: 616.4712526643154, Validation Loss: 727.0703125\n",
      "Epoch [445/1500], Training Loss: 612.033601434145, Validation Loss: 721.96142578125\n",
      "Epoch [446/1500], Training Loss: 607.6249312348577, Validation Loss: 716.8914184570312\n",
      "Epoch [447/1500], Training Loss: 603.2465780457172, Validation Loss: 711.8577270507812\n",
      "Epoch [448/1500], Training Loss: 598.8964745534961, Validation Loss: 706.856201171875\n",
      "Epoch [449/1500], Training Loss: 594.5740294122451, Validation Loss: 701.8875732421875\n",
      "Epoch [450/1500], Training Loss: 590.279367113194, Validation Loss: 696.947998046875\n",
      "Epoch [451/1500], Training Loss: 586.0116462398197, Validation Loss: 692.0352172851562\n",
      "Epoch [452/1500], Training Loss: 581.7731725435249, Validation Loss: 687.1499633789062\n",
      "Epoch [453/1500], Training Loss: 577.560302565376, Validation Loss: 682.2858276367188\n",
      "Epoch [454/1500], Training Loss: 573.3749274810024, Validation Loss: 677.444091796875\n",
      "Epoch [455/1500], Training Loss: 569.215616725199, Validation Loss: 672.62158203125\n",
      "Epoch [456/1500], Training Loss: 565.0819844859908, Validation Loss: 667.814697265625\n",
      "Epoch [457/1500], Training Loss: 560.973715357788, Validation Loss: 663.0253295898438\n",
      "Epoch [458/1500], Training Loss: 556.8905333618677, Validation Loss: 658.2492065429688\n",
      "Epoch [459/1500], Training Loss: 552.8333117754864, Validation Loss: 653.4868774414062\n",
      "Epoch [460/1500], Training Loss: 548.7982586055417, Validation Loss: 648.7332153320312\n",
      "Epoch [461/1500], Training Loss: 544.7861299742153, Validation Loss: 643.9916381835938\n",
      "Epoch [462/1500], Training Loss: 540.7976359246682, Validation Loss: 639.2593383789062\n",
      "Epoch [463/1500], Training Loss: 536.833167900187, Validation Loss: 634.5409545898438\n",
      "Epoch [464/1500], Training Loss: 532.8901007536126, Validation Loss: 629.82958984375\n",
      "Epoch [465/1500], Training Loss: 528.9682319695952, Validation Loss: 625.1278076171875\n",
      "Epoch [466/1500], Training Loss: 525.068678274667, Validation Loss: 620.439453125\n",
      "Epoch [467/1500], Training Loss: 521.1917961218467, Validation Loss: 615.7656860351562\n",
      "Epoch [468/1500], Training Loss: 517.3382386373761, Validation Loss: 611.10498046875\n",
      "Epoch [469/1500], Training Loss: 513.5056994533401, Validation Loss: 606.4615478515625\n",
      "Epoch [470/1500], Training Loss: 509.6954756147842, Validation Loss: 601.8338623046875\n",
      "Epoch [471/1500], Training Loss: 505.90534635787157, Validation Loss: 597.2228393554688\n",
      "Epoch [472/1500], Training Loss: 502.13588621760914, Validation Loss: 592.6316528320312\n",
      "Epoch [473/1500], Training Loss: 498.3891610121668, Validation Loss: 588.0640258789062\n",
      "Epoch [474/1500], Training Loss: 494.66593599802286, Validation Loss: 583.5231323242188\n",
      "Epoch [475/1500], Training Loss: 490.96652525038695, Validation Loss: 579.0119018554688\n",
      "Epoch [476/1500], Training Loss: 487.2903279987234, Validation Loss: 574.52978515625\n",
      "Epoch [477/1500], Training Loss: 483.6369347197997, Validation Loss: 570.0786743164062\n",
      "Epoch [478/1500], Training Loss: 480.00688974321326, Validation Loss: 565.661376953125\n",
      "Epoch [479/1500], Training Loss: 476.40198006750666, Validation Loss: 561.2782592773438\n",
      "Epoch [480/1500], Training Loss: 472.8203868025736, Validation Loss: 556.9327392578125\n",
      "Epoch [481/1500], Training Loss: 469.2627881776612, Validation Loss: 552.6237182617188\n",
      "Epoch [482/1500], Training Loss: 465.7302504590477, Validation Loss: 548.3514404296875\n",
      "Epoch [483/1500], Training Loss: 462.22170307623196, Validation Loss: 544.1168823242188\n",
      "Epoch [484/1500], Training Loss: 458.73575563328325, Validation Loss: 539.9231567382812\n",
      "Epoch [485/1500], Training Loss: 455.2731383514565, Validation Loss: 535.7676391601562\n",
      "Epoch [486/1500], Training Loss: 451.83663951931555, Validation Loss: 531.6537475585938\n",
      "Epoch [487/1500], Training Loss: 448.4250995053425, Validation Loss: 527.5801391601562\n",
      "Epoch [488/1500], Training Loss: 445.03764449478734, Validation Loss: 523.5473022460938\n",
      "Epoch [489/1500], Training Loss: 441.6758984237365, Validation Loss: 519.5567016601562\n",
      "Epoch [490/1500], Training Loss: 438.33952019823124, Validation Loss: 515.6063842773438\n",
      "Epoch [491/1500], Training Loss: 435.0278741953443, Validation Loss: 511.69598388671875\n",
      "Epoch [492/1500], Training Loss: 431.74109926791823, Validation Loss: 507.8251647949219\n",
      "Epoch [493/1500], Training Loss: 428.4794230785787, Validation Loss: 503.9929504394531\n",
      "Epoch [494/1500], Training Loss: 425.2412691931963, Validation Loss: 500.1990661621094\n",
      "Epoch [495/1500], Training Loss: 422.0276152965333, Validation Loss: 496.4438781738281\n",
      "Epoch [496/1500], Training Loss: 418.83781616109917, Validation Loss: 492.7237243652344\n",
      "Epoch [497/1500], Training Loss: 415.6711051867331, Validation Loss: 489.0404052734375\n",
      "Epoch [498/1500], Training Loss: 412.5272706429762, Validation Loss: 485.3896484375\n",
      "Epoch [499/1500], Training Loss: 409.4055815964912, Validation Loss: 481.7733459472656\n",
      "Epoch [500/1500], Training Loss: 406.30614828571004, Validation Loss: 478.1905212402344\n",
      "Epoch [501/1500], Training Loss: 403.23039040397487, Validation Loss: 474.6404724121094\n",
      "Epoch [502/1500], Training Loss: 400.17823743761227, Validation Loss: 471.12298583984375\n",
      "Epoch [503/1500], Training Loss: 397.14774406061105, Validation Loss: 467.63568115234375\n",
      "Epoch [504/1500], Training Loss: 394.13994985529337, Validation Loss: 464.17901611328125\n",
      "Epoch [505/1500], Training Loss: 391.1541915687255, Validation Loss: 460.7507019042969\n",
      "Epoch [506/1500], Training Loss: 388.1919292122176, Validation Loss: 457.3528137207031\n",
      "Epoch [507/1500], Training Loss: 385.25212080510596, Validation Loss: 453.9817199707031\n",
      "Epoch [508/1500], Training Loss: 382.33374745460105, Validation Loss: 450.6368408203125\n",
      "Epoch [509/1500], Training Loss: 379.4361296989461, Validation Loss: 447.3187561035156\n",
      "Epoch [510/1500], Training Loss: 376.5600625595301, Validation Loss: 444.0261535644531\n",
      "Epoch [511/1500], Training Loss: 373.704595802659, Validation Loss: 440.75848388671875\n",
      "Epoch [512/1500], Training Loss: 370.86988464577064, Validation Loss: 437.51544189453125\n",
      "Epoch [513/1500], Training Loss: 368.0561041878856, Validation Loss: 434.2952575683594\n",
      "Epoch [514/1500], Training Loss: 365.26133335795214, Validation Loss: 431.0982971191406\n",
      "Epoch [515/1500], Training Loss: 362.4865853886794, Validation Loss: 427.92510986328125\n",
      "Epoch [516/1500], Training Loss: 359.73243045327206, Validation Loss: 424.77520751953125\n",
      "Epoch [517/1500], Training Loss: 356.99844442977525, Validation Loss: 421.6491394042969\n",
      "Epoch [518/1500], Training Loss: 354.2859335020352, Validation Loss: 418.5477294921875\n",
      "Epoch [519/1500], Training Loss: 351.5937836112457, Validation Loss: 415.4703063964844\n",
      "Epoch [520/1500], Training Loss: 348.9215475310563, Validation Loss: 412.4159240722656\n",
      "Epoch [521/1500], Training Loss: 346.2700253853259, Validation Loss: 409.38531494140625\n",
      "Epoch [522/1500], Training Loss: 343.6374831753013, Validation Loss: 406.3768310546875\n",
      "Epoch [523/1500], Training Loss: 341.02548912267775, Validation Loss: 403.3940124511719\n",
      "Epoch [524/1500], Training Loss: 338.43204951771713, Validation Loss: 400.4339599609375\n",
      "Epoch [525/1500], Training Loss: 335.8562035456042, Validation Loss: 397.49530029296875\n",
      "Epoch [526/1500], Training Loss: 333.29988025544765, Validation Loss: 394.58099365234375\n",
      "Epoch [527/1500], Training Loss: 330.7632348344532, Validation Loss: 391.69146728515625\n",
      "Epoch [528/1500], Training Loss: 328.2465517140538, Validation Loss: 388.8267517089844\n",
      "Epoch [529/1500], Training Loss: 325.7482231745936, Validation Loss: 385.9855041503906\n",
      "Epoch [530/1500], Training Loss: 323.27020445820585, Validation Loss: 383.1705017089844\n",
      "Epoch [531/1500], Training Loss: 320.8122763651405, Validation Loss: 380.3804626464844\n",
      "Epoch [532/1500], Training Loss: 318.37278360954497, Validation Loss: 377.6134948730469\n",
      "Epoch [533/1500], Training Loss: 315.9517915233291, Validation Loss: 374.8713073730469\n",
      "Epoch [534/1500], Training Loss: 313.5494682454148, Validation Loss: 372.1525573730469\n",
      "Epoch [535/1500], Training Loss: 311.165658673683, Validation Loss: 369.458251953125\n",
      "Epoch [536/1500], Training Loss: 308.80120872429416, Validation Loss: 366.7896728515625\n",
      "Epoch [537/1500], Training Loss: 306.4564366972854, Validation Loss: 364.14434814453125\n",
      "Epoch [538/1500], Training Loss: 304.1289230150071, Validation Loss: 361.5211181640625\n",
      "Epoch [539/1500], Training Loss: 301.8195339355633, Validation Loss: 358.92193603515625\n",
      "Epoch [540/1500], Training Loss: 299.5263712323145, Validation Loss: 356.3442077636719\n",
      "Epoch [541/1500], Training Loss: 297.25163914053184, Validation Loss: 353.79083251953125\n",
      "Epoch [542/1500], Training Loss: 294.99567022997564, Validation Loss: 351.2616882324219\n",
      "Epoch [543/1500], Training Loss: 292.7575450187639, Validation Loss: 348.7533874511719\n",
      "Epoch [544/1500], Training Loss: 290.53569847773673, Validation Loss: 346.26763916015625\n",
      "Epoch [545/1500], Training Loss: 288.3333234855063, Validation Loss: 343.8053283691406\n",
      "Epoch [546/1500], Training Loss: 286.1480108015647, Validation Loss: 341.36480712890625\n",
      "Epoch [547/1500], Training Loss: 283.98049104083344, Validation Loss: 338.9468994140625\n",
      "Epoch [548/1500], Training Loss: 281.83045356839943, Validation Loss: 336.55133056640625\n",
      "Epoch [549/1500], Training Loss: 279.6977944908514, Validation Loss: 334.17578125\n",
      "Epoch [550/1500], Training Loss: 277.58164933167353, Validation Loss: 331.8223571777344\n",
      "Epoch [551/1500], Training Loss: 275.48260196644554, Validation Loss: 329.489990234375\n",
      "Epoch [552/1500], Training Loss: 273.4011257759517, Validation Loss: 327.1789245605469\n",
      "Epoch [553/1500], Training Loss: 271.3360611325345, Validation Loss: 324.887939453125\n",
      "Epoch [554/1500], Training Loss: 269.2872719632653, Validation Loss: 322.61785888671875\n",
      "Epoch [555/1500], Training Loss: 267.2563756751628, Validation Loss: 320.3696594238281\n",
      "Epoch [556/1500], Training Loss: 265.24212279536556, Validation Loss: 318.1414489746094\n",
      "Epoch [557/1500], Training Loss: 263.2446136957598, Validation Loss: 315.93310546875\n",
      "Epoch [558/1500], Training Loss: 261.2631281116156, Validation Loss: 313.7440490722656\n",
      "Epoch [559/1500], Training Loss: 259.2963055007743, Validation Loss: 311.5725402832031\n",
      "Epoch [560/1500], Training Loss: 257.3440682399984, Validation Loss: 309.42047119140625\n",
      "Epoch [561/1500], Training Loss: 255.40762670627223, Validation Loss: 307.2872009277344\n",
      "Epoch [562/1500], Training Loss: 253.48714653070684, Validation Loss: 305.1730651855469\n",
      "Epoch [563/1500], Training Loss: 251.58304740327316, Validation Loss: 303.07861328125\n",
      "Epoch [564/1500], Training Loss: 249.69384016804068, Validation Loss: 301.00262451171875\n",
      "Epoch [565/1500], Training Loss: 247.82001091395972, Validation Loss: 298.9449462890625\n",
      "Epoch [566/1500], Training Loss: 245.96164789032028, Validation Loss: 296.9058532714844\n",
      "Epoch [567/1500], Training Loss: 244.11757178803109, Validation Loss: 294.8839111328125\n",
      "Epoch [568/1500], Training Loss: 242.28846425624866, Validation Loss: 292.88006591796875\n",
      "Epoch [569/1500], Training Loss: 240.47412427962186, Validation Loss: 290.8937683105469\n",
      "Epoch [570/1500], Training Loss: 238.67427160773371, Validation Loss: 288.9245300292969\n",
      "Epoch [571/1500], Training Loss: 236.88835565387708, Validation Loss: 286.97283935546875\n",
      "Epoch [572/1500], Training Loss: 235.1167974724678, Validation Loss: 285.0386047363281\n",
      "Epoch [573/1500], Training Loss: 233.35981748784428, Validation Loss: 283.12164306640625\n",
      "Epoch [574/1500], Training Loss: 231.61744918907223, Validation Loss: 281.22216796875\n",
      "Epoch [575/1500], Training Loss: 229.88912696454938, Validation Loss: 279.3391418457031\n",
      "Epoch [576/1500], Training Loss: 228.17462770073752, Validation Loss: 277.4722595214844\n",
      "Epoch [577/1500], Training Loss: 226.47383914874897, Validation Loss: 275.6209716796875\n",
      "Epoch [578/1500], Training Loss: 224.78546047028, Validation Loss: 273.78619384765625\n",
      "Epoch [579/1500], Training Loss: 223.11177594191642, Validation Loss: 271.9682922363281\n",
      "Epoch [580/1500], Training Loss: 221.45114953500834, Validation Loss: 270.1655578613281\n",
      "Epoch [581/1500], Training Loss: 219.80469556870955, Validation Loss: 268.380859375\n",
      "Epoch [582/1500], Training Loss: 218.17234056342392, Validation Loss: 266.6111755371094\n",
      "Epoch [583/1500], Training Loss: 216.5539412513425, Validation Loss: 264.85833740234375\n",
      "Epoch [584/1500], Training Loss: 214.94994454621298, Validation Loss: 263.1212158203125\n",
      "Epoch [585/1500], Training Loss: 213.35855418199887, Validation Loss: 261.40057373046875\n",
      "Epoch [586/1500], Training Loss: 211.78094129115615, Validation Loss: 259.6941223144531\n",
      "Epoch [587/1500], Training Loss: 210.21588376090128, Validation Loss: 258.001953125\n",
      "Epoch [588/1500], Training Loss: 208.662522078909, Validation Loss: 256.3241882324219\n",
      "Epoch [589/1500], Training Loss: 207.1215928321369, Validation Loss: 254.66098022460938\n",
      "Epoch [590/1500], Training Loss: 205.59359365278456, Validation Loss: 253.0127410888672\n",
      "Epoch [591/1500], Training Loss: 204.07939121858678, Validation Loss: 251.3804931640625\n",
      "Epoch [592/1500], Training Loss: 202.57852018995416, Validation Loss: 249.7623748779297\n",
      "Epoch [593/1500], Training Loss: 201.08990544164627, Validation Loss: 248.1577911376953\n",
      "Epoch [594/1500], Training Loss: 199.61262044315595, Validation Loss: 246.56732177734375\n",
      "Epoch [595/1500], Training Loss: 198.1485545316026, Validation Loss: 244.99050903320312\n",
      "Epoch [596/1500], Training Loss: 196.69645675407043, Validation Loss: 243.42649841308594\n",
      "Epoch [597/1500], Training Loss: 195.25512446473658, Validation Loss: 241.87429809570312\n",
      "Epoch [598/1500], Training Loss: 193.8256266833396, Validation Loss: 240.33499145507812\n",
      "Epoch [599/1500], Training Loss: 192.40730022292533, Validation Loss: 238.8072967529297\n",
      "Epoch [600/1500], Training Loss: 190.99980356444226, Validation Loss: 237.29190063476562\n",
      "Epoch [601/1500], Training Loss: 189.60321247362043, Validation Loss: 235.78866577148438\n",
      "Epoch [602/1500], Training Loss: 188.21919504601954, Validation Loss: 234.29843139648438\n",
      "Epoch [603/1500], Training Loss: 186.84739444850794, Validation Loss: 232.821044921875\n",
      "Epoch [604/1500], Training Loss: 185.48717014495384, Validation Loss: 231.3550567626953\n",
      "Epoch [605/1500], Training Loss: 184.13817590749932, Validation Loss: 229.89935302734375\n",
      "Epoch [606/1500], Training Loss: 182.79934026827289, Validation Loss: 228.4547882080078\n",
      "Epoch [607/1500], Training Loss: 181.47200648483613, Validation Loss: 227.02197265625\n",
      "Epoch [608/1500], Training Loss: 180.15531470293416, Validation Loss: 225.5992889404297\n",
      "Epoch [609/1500], Training Loss: 178.8486098797948, Validation Loss: 224.18673706054688\n",
      "Epoch [610/1500], Training Loss: 177.55289941156872, Validation Loss: 222.78562927246094\n",
      "Epoch [611/1500], Training Loss: 176.2684173159314, Validation Loss: 221.3959197998047\n",
      "Epoch [612/1500], Training Loss: 174.99485310971542, Validation Loss: 220.01670837402344\n",
      "Epoch [613/1500], Training Loss: 173.73173596664688, Validation Loss: 218.64947509765625\n",
      "Epoch [614/1500], Training Loss: 172.4786313375063, Validation Loss: 217.29159545898438\n",
      "Epoch [615/1500], Training Loss: 171.235548902097, Validation Loss: 215.94407653808594\n",
      "Epoch [616/1500], Training Loss: 170.00160300458353, Validation Loss: 214.60525512695312\n",
      "Epoch [617/1500], Training Loss: 168.77706108592332, Validation Loss: 213.2771453857422\n",
      "Epoch [618/1500], Training Loss: 167.5623418072352, Validation Loss: 211.95941162109375\n",
      "Epoch [619/1500], Training Loss: 166.357481423586, Validation Loss: 210.65199279785156\n",
      "Epoch [620/1500], Training Loss: 165.16266716011674, Validation Loss: 209.3546600341797\n",
      "Epoch [621/1500], Training Loss: 163.97682419309922, Validation Loss: 208.06704711914062\n",
      "Epoch [622/1500], Training Loss: 162.80056163247104, Validation Loss: 206.7877960205078\n",
      "Epoch [623/1500], Training Loss: 161.63280631764346, Validation Loss: 205.51658630371094\n",
      "Epoch [624/1500], Training Loss: 160.4733065034774, Validation Loss: 204.2529296875\n",
      "Epoch [625/1500], Training Loss: 159.32259995248052, Validation Loss: 202.99757385253906\n",
      "Epoch [626/1500], Training Loss: 158.1800458458365, Validation Loss: 201.7500762939453\n",
      "Epoch [627/1500], Training Loss: 157.04597818353034, Validation Loss: 200.51046752929688\n",
      "Epoch [628/1500], Training Loss: 155.9206390465951, Validation Loss: 199.27908325195312\n",
      "Epoch [629/1500], Training Loss: 154.80313682253612, Validation Loss: 198.054443359375\n",
      "Epoch [630/1500], Training Loss: 153.69378028287417, Validation Loss: 196.83738708496094\n",
      "Epoch [631/1500], Training Loss: 152.59235618697315, Validation Loss: 195.62693786621094\n",
      "Epoch [632/1500], Training Loss: 151.49878141642822, Validation Loss: 194.4240264892578\n",
      "Epoch [633/1500], Training Loss: 150.413591430985, Validation Loss: 193.22744750976562\n",
      "Epoch [634/1500], Training Loss: 149.33570476262224, Validation Loss: 192.03689575195312\n",
      "Epoch [635/1500], Training Loss: 148.26553553075448, Validation Loss: 190.85179138183594\n",
      "Epoch [636/1500], Training Loss: 147.20200985988137, Validation Loss: 189.67164611816406\n",
      "Epoch [637/1500], Training Loss: 146.14544506460538, Validation Loss: 188.4970245361328\n",
      "Epoch [638/1500], Training Loss: 145.09693042456942, Validation Loss: 187.32830810546875\n",
      "Epoch [639/1500], Training Loss: 144.05563805529738, Validation Loss: 186.16375732421875\n",
      "Epoch [640/1500], Training Loss: 143.02096409597874, Validation Loss: 185.00326538085938\n",
      "Epoch [641/1500], Training Loss: 141.99362775750353, Validation Loss: 183.8480224609375\n",
      "Epoch [642/1500], Training Loss: 140.97325787063977, Validation Loss: 182.69668579101562\n",
      "Epoch [643/1500], Training Loss: 139.95998568170054, Validation Loss: 181.55050659179688\n",
      "Epoch [644/1500], Training Loss: 138.9536463485184, Validation Loss: 180.40794372558594\n",
      "Epoch [645/1500], Training Loss: 137.95411906189105, Validation Loss: 179.26866149902344\n",
      "Epoch [646/1500], Training Loss: 136.96082582675865, Validation Loss: 178.13316345214844\n",
      "Epoch [647/1500], Training Loss: 135.9741549738404, Validation Loss: 177.00120544433594\n",
      "Epoch [648/1500], Training Loss: 134.99431559952058, Validation Loss: 175.87222290039062\n",
      "Epoch [649/1500], Training Loss: 134.01973262192882, Validation Loss: 174.74560546875\n",
      "Epoch [650/1500], Training Loss: 133.05117226454558, Validation Loss: 173.6215057373047\n",
      "Epoch [651/1500], Training Loss: 132.0880427163156, Validation Loss: 172.49932861328125\n",
      "Epoch [652/1500], Training Loss: 131.12998695157108, Validation Loss: 171.3793182373047\n",
      "Epoch [653/1500], Training Loss: 130.17778863154342, Validation Loss: 170.26174926757812\n",
      "Epoch [654/1500], Training Loss: 129.23128366919158, Validation Loss: 169.14620971679688\n",
      "Epoch [655/1500], Training Loss: 128.29005934618502, Validation Loss: 168.03260803222656\n",
      "Epoch [656/1500], Training Loss: 127.355234881683, Validation Loss: 166.9215545654297\n",
      "Epoch [657/1500], Training Loss: 126.42586877390066, Validation Loss: 165.8118896484375\n",
      "Epoch [658/1500], Training Loss: 125.5018266738467, Validation Loss: 164.70448303222656\n",
      "Epoch [659/1500], Training Loss: 124.58355183350488, Validation Loss: 163.59934997558594\n",
      "Epoch [660/1500], Training Loss: 123.67046166534286, Validation Loss: 162.49554443359375\n",
      "Epoch [661/1500], Training Loss: 122.76350832223302, Validation Loss: 161.3944091796875\n",
      "Epoch [662/1500], Training Loss: 121.86162706944205, Validation Loss: 160.2943878173828\n",
      "Epoch [663/1500], Training Loss: 120.96516830265956, Validation Loss: 159.19619750976562\n",
      "Epoch [664/1500], Training Loss: 120.07368786448866, Validation Loss: 158.09996032714844\n",
      "Epoch [665/1500], Training Loss: 119.18705823622564, Validation Loss: 157.00453186035156\n",
      "Epoch [666/1500], Training Loss: 118.30502589854161, Validation Loss: 155.91090393066406\n",
      "Epoch [667/1500], Training Loss: 117.42819510227697, Validation Loss: 154.8187713623047\n",
      "Epoch [668/1500], Training Loss: 116.55573968172042, Validation Loss: 153.72850036621094\n",
      "Epoch [669/1500], Training Loss: 115.68828189638624, Validation Loss: 152.6402587890625\n",
      "Epoch [670/1500], Training Loss: 114.82637909114133, Validation Loss: 151.5551300048828\n",
      "Epoch [671/1500], Training Loss: 113.97012567442923, Validation Loss: 150.47276306152344\n",
      "Epoch [672/1500], Training Loss: 113.11846443068295, Validation Loss: 149.3927764892578\n",
      "Epoch [673/1500], Training Loss: 112.2720327471616, Validation Loss: 148.31553649902344\n",
      "Epoch [674/1500], Training Loss: 111.43073542455222, Validation Loss: 147.24154663085938\n",
      "Epoch [675/1500], Training Loss: 110.59495633902354, Validation Loss: 146.17080688476562\n",
      "Epoch [676/1500], Training Loss: 109.76373433395433, Validation Loss: 145.10292053222656\n",
      "Epoch [677/1500], Training Loss: 108.9366217505126, Validation Loss: 144.03744506835938\n",
      "Epoch [678/1500], Training Loss: 108.11359546653802, Validation Loss: 142.9750213623047\n",
      "Epoch [679/1500], Training Loss: 107.29547332623896, Validation Loss: 141.9166717529297\n",
      "Epoch [680/1500], Training Loss: 106.48244515590159, Validation Loss: 140.862060546875\n",
      "Epoch [681/1500], Training Loss: 105.67430741452802, Validation Loss: 139.81259155273438\n",
      "Epoch [682/1500], Training Loss: 104.87220360122407, Validation Loss: 138.76806640625\n",
      "Epoch [683/1500], Training Loss: 104.07495749373894, Validation Loss: 137.72842407226562\n",
      "Epoch [684/1500], Training Loss: 103.28294333813297, Validation Loss: 136.69357299804688\n",
      "Epoch [685/1500], Training Loss: 102.49628666468567, Validation Loss: 135.66421508789062\n",
      "Epoch [686/1500], Training Loss: 101.71509209695192, Validation Loss: 134.64060974121094\n",
      "Epoch [687/1500], Training Loss: 100.93886487227437, Validation Loss: 133.6226806640625\n",
      "Epoch [688/1500], Training Loss: 100.1682083585565, Validation Loss: 132.6107635498047\n",
      "Epoch [689/1500], Training Loss: 99.4025746469834, Validation Loss: 131.60568237304688\n",
      "Epoch [690/1500], Training Loss: 98.64237547008673, Validation Loss: 130.60646057128906\n",
      "Epoch [691/1500], Training Loss: 97.88750969725645, Validation Loss: 129.61373901367188\n",
      "Epoch [692/1500], Training Loss: 97.13786372360775, Validation Loss: 128.6279754638672\n",
      "Epoch [693/1500], Training Loss: 96.39362248043668, Validation Loss: 127.64874267578125\n",
      "Epoch [694/1500], Training Loss: 95.6542193873687, Validation Loss: 126.6762924194336\n",
      "Epoch [695/1500], Training Loss: 94.92000422252225, Validation Loss: 125.71134185791016\n",
      "Epoch [696/1500], Training Loss: 94.19105252840994, Validation Loss: 124.75393676757812\n",
      "Epoch [697/1500], Training Loss: 93.4677556528628, Validation Loss: 123.80370330810547\n",
      "Epoch [698/1500], Training Loss: 92.74975383684827, Validation Loss: 122.86162567138672\n",
      "Epoch [699/1500], Training Loss: 92.03754996952048, Validation Loss: 121.92752838134766\n",
      "Epoch [700/1500], Training Loss: 91.33052551488655, Validation Loss: 121.00104522705078\n",
      "Epoch [701/1500], Training Loss: 90.62918507638847, Validation Loss: 120.0823745727539\n",
      "Epoch [702/1500], Training Loss: 89.93279694966898, Validation Loss: 119.17147827148438\n",
      "Epoch [703/1500], Training Loss: 89.24187513258116, Validation Loss: 118.26838684082031\n",
      "Epoch [704/1500], Training Loss: 88.55599160489209, Validation Loss: 117.37321472167969\n",
      "Epoch [705/1500], Training Loss: 87.8755558987158, Validation Loss: 116.4861831665039\n",
      "Epoch [706/1500], Training Loss: 87.2004113011772, Validation Loss: 115.60717010498047\n",
      "Epoch [707/1500], Training Loss: 86.53055310863671, Validation Loss: 114.73660278320312\n",
      "Epoch [708/1500], Training Loss: 85.86630847095442, Validation Loss: 113.87384796142578\n",
      "Epoch [709/1500], Training Loss: 85.20760836328677, Validation Loss: 113.01960754394531\n",
      "Epoch [710/1500], Training Loss: 84.55434295979298, Validation Loss: 112.17328643798828\n",
      "Epoch [711/1500], Training Loss: 83.90659706123698, Validation Loss: 111.33515167236328\n",
      "Epoch [712/1500], Training Loss: 83.26395923209834, Validation Loss: 110.50531005859375\n",
      "Epoch [713/1500], Training Loss: 82.62709302122634, Validation Loss: 109.68363189697266\n",
      "Epoch [714/1500], Training Loss: 81.99540980849633, Validation Loss: 108.87020111083984\n",
      "Epoch [715/1500], Training Loss: 81.36886351625449, Validation Loss: 108.0643081665039\n",
      "Epoch [716/1500], Training Loss: 80.74757115330418, Validation Loss: 107.26615142822266\n",
      "Epoch [717/1500], Training Loss: 80.1315885816518, Validation Loss: 106.47610473632812\n",
      "Epoch [718/1500], Training Loss: 79.52085638864277, Validation Loss: 105.69395446777344\n",
      "Epoch [719/1500], Training Loss: 78.9156177332276, Validation Loss: 104.92012786865234\n",
      "Epoch [720/1500], Training Loss: 78.31553813478199, Validation Loss: 104.15357208251953\n",
      "Epoch [721/1500], Training Loss: 77.7204307869925, Validation Loss: 103.39453887939453\n",
      "Epoch [722/1500], Training Loss: 77.13021507370239, Validation Loss: 102.64301300048828\n",
      "Epoch [723/1500], Training Loss: 76.54514986357204, Validation Loss: 101.89897918701172\n",
      "Epoch [724/1500], Training Loss: 75.96492511254466, Validation Loss: 101.16170501708984\n",
      "Epoch [725/1500], Training Loss: 75.38954028368185, Validation Loss: 100.43191528320312\n",
      "Epoch [726/1500], Training Loss: 74.8193156160554, Validation Loss: 99.70982360839844\n",
      "Epoch [727/1500], Training Loss: 74.2538054327315, Validation Loss: 98.9946060180664\n",
      "Epoch [728/1500], Training Loss: 73.6933569396116, Validation Loss: 98.28668212890625\n",
      "Epoch [729/1500], Training Loss: 73.13784570326398, Validation Loss: 97.58607482910156\n",
      "Epoch [730/1500], Training Loss: 72.58724447590473, Validation Loss: 96.8929443359375\n",
      "Epoch [731/1500], Training Loss: 72.04166849508128, Validation Loss: 96.20661163330078\n",
      "Epoch [732/1500], Training Loss: 71.5008437077213, Validation Loss: 95.52719116210938\n",
      "Epoch [733/1500], Training Loss: 70.96462747490126, Validation Loss: 94.85437774658203\n",
      "Epoch [734/1500], Training Loss: 70.43347233181547, Validation Loss: 94.18848419189453\n",
      "Epoch [735/1500], Training Loss: 69.90711089874415, Validation Loss: 93.5291519165039\n",
      "Epoch [736/1500], Training Loss: 69.38551752282535, Validation Loss: 92.87680053710938\n",
      "Epoch [737/1500], Training Loss: 68.86867944408932, Validation Loss: 92.2307357788086\n",
      "Epoch [738/1500], Training Loss: 68.35631832351427, Validation Loss: 91.5908432006836\n",
      "Epoch [739/1500], Training Loss: 67.84818914338489, Validation Loss: 90.9570083618164\n",
      "Epoch [740/1500], Training Loss: 67.34410530089704, Validation Loss: 90.3292236328125\n",
      "Epoch [741/1500], Training Loss: 66.84472954226119, Validation Loss: 89.70774841308594\n",
      "Epoch [742/1500], Training Loss: 66.34985630392848, Validation Loss: 89.09205627441406\n",
      "Epoch [743/1500], Training Loss: 65.85952562903812, Validation Loss: 88.48247528076172\n",
      "Epoch [744/1500], Training Loss: 65.37349545693479, Validation Loss: 87.87918090820312\n",
      "Epoch [745/1500], Training Loss: 64.89200870054664, Validation Loss: 87.28160095214844\n",
      "Epoch [746/1500], Training Loss: 64.41482121645495, Validation Loss: 86.69017028808594\n",
      "Epoch [747/1500], Training Loss: 63.94190058283156, Validation Loss: 86.1043930053711\n",
      "Epoch [748/1500], Training Loss: 63.47311982569987, Validation Loss: 85.52401733398438\n",
      "Epoch [749/1500], Training Loss: 63.00835088083348, Validation Loss: 84.94921875\n",
      "Epoch [750/1500], Training Loss: 62.54797371992745, Validation Loss: 84.3801498413086\n",
      "Epoch [751/1500], Training Loss: 62.091630024752774, Validation Loss: 83.81640625\n",
      "Epoch [752/1500], Training Loss: 61.63899066442702, Validation Loss: 83.25749969482422\n",
      "Epoch [753/1500], Training Loss: 61.19043284270075, Validation Loss: 82.7042236328125\n",
      "Epoch [754/1500], Training Loss: 60.74607629246092, Validation Loss: 82.15648651123047\n",
      "Epoch [755/1500], Training Loss: 60.30574844757338, Validation Loss: 81.61371612548828\n",
      "Epoch [756/1500], Training Loss: 59.869239535648376, Validation Loss: 81.0760498046875\n",
      "Epoch [757/1500], Training Loss: 59.43689484809142, Validation Loss: 80.54365539550781\n",
      "Epoch [758/1500], Training Loss: 59.00821625567983, Validation Loss: 80.01641082763672\n",
      "Epoch [759/1500], Training Loss: 58.58346548247576, Validation Loss: 79.49414825439453\n",
      "Epoch [760/1500], Training Loss: 58.16286905703005, Validation Loss: 78.97697448730469\n",
      "Epoch [761/1500], Training Loss: 57.7461831809399, Validation Loss: 78.4650650024414\n",
      "Epoch [762/1500], Training Loss: 57.333275953105066, Validation Loss: 77.95821380615234\n",
      "Epoch [763/1500], Training Loss: 56.924060804229676, Validation Loss: 77.45601654052734\n",
      "Epoch [764/1500], Training Loss: 56.51853405729489, Validation Loss: 76.95849609375\n",
      "Epoch [765/1500], Training Loss: 56.116731099145134, Validation Loss: 76.46588134765625\n",
      "Epoch [766/1500], Training Loss: 55.718359932673195, Validation Loss: 75.9780502319336\n",
      "Epoch [767/1500], Training Loss: 55.32388911492397, Validation Loss: 75.49474334716797\n",
      "Epoch [768/1500], Training Loss: 54.93274936893381, Validation Loss: 75.01597595214844\n",
      "Epoch [769/1500], Training Loss: 54.544795736138845, Validation Loss: 74.54167938232422\n",
      "Epoch [770/1500], Training Loss: 54.16030807891983, Validation Loss: 74.07201385498047\n",
      "Epoch [771/1500], Training Loss: 53.7793602967251, Validation Loss: 73.60699462890625\n",
      "Epoch [772/1500], Training Loss: 53.40181600393475, Validation Loss: 73.14656829833984\n",
      "Epoch [773/1500], Training Loss: 53.02766598178855, Validation Loss: 72.69039916992188\n",
      "Epoch [774/1500], Training Loss: 52.65678629853222, Validation Loss: 72.23889923095703\n",
      "Epoch [775/1500], Training Loss: 52.28914519304543, Validation Loss: 71.79170989990234\n",
      "Epoch [776/1500], Training Loss: 51.92508659461457, Validation Loss: 71.34910583496094\n",
      "Epoch [777/1500], Training Loss: 51.5647527813151, Validation Loss: 70.91124725341797\n",
      "Epoch [778/1500], Training Loss: 51.20763624746525, Validation Loss: 70.47782135009766\n",
      "Epoch [779/1500], Training Loss: 50.85414627519687, Validation Loss: 70.04889678955078\n",
      "Epoch [780/1500], Training Loss: 50.503806151796766, Validation Loss: 69.62425231933594\n",
      "Epoch [781/1500], Training Loss: 50.156940840746074, Validation Loss: 69.2043228149414\n",
      "Epoch [782/1500], Training Loss: 49.81343105146666, Validation Loss: 68.78843688964844\n",
      "Epoch [783/1500], Training Loss: 49.47288629871779, Validation Loss: 68.37665557861328\n",
      "Epoch [784/1500], Training Loss: 49.13568645820135, Validation Loss: 67.96928405761719\n",
      "Epoch [785/1500], Training Loss: 48.80171451558783, Validation Loss: 67.56615447998047\n",
      "Epoch [786/1500], Training Loss: 48.47059175350582, Validation Loss: 67.16677856445312\n",
      "Epoch [787/1500], Training Loss: 48.14210582298567, Validation Loss: 66.77113342285156\n",
      "Epoch [788/1500], Training Loss: 47.816445278803194, Validation Loss: 66.37936401367188\n",
      "Epoch [789/1500], Training Loss: 47.493901663716585, Validation Loss: 65.99175262451172\n",
      "Epoch [790/1500], Training Loss: 47.17408575801484, Validation Loss: 65.6080093383789\n",
      "Epoch [791/1500], Training Loss: 46.857190689721435, Validation Loss: 65.22835540771484\n",
      "Epoch [792/1500], Training Loss: 46.54319148065824, Validation Loss: 64.85234832763672\n",
      "Epoch [793/1500], Training Loss: 46.232220642583634, Validation Loss: 64.48043060302734\n",
      "Epoch [794/1500], Training Loss: 45.92400341731021, Validation Loss: 64.11227416992188\n",
      "Epoch [795/1500], Training Loss: 45.61840441463404, Validation Loss: 63.74766540527344\n",
      "Epoch [796/1500], Training Loss: 45.315542047428, Validation Loss: 63.38666915893555\n",
      "Epoch [797/1500], Training Loss: 45.01530631160714, Validation Loss: 63.02947235107422\n",
      "Epoch [798/1500], Training Loss: 44.71763496782881, Validation Loss: 62.67555236816406\n",
      "Epoch [799/1500], Training Loss: 44.42260557929784, Validation Loss: 62.325286865234375\n",
      "Epoch [800/1500], Training Loss: 44.13043407006338, Validation Loss: 61.97880554199219\n",
      "Epoch [801/1500], Training Loss: 43.840774780883976, Validation Loss: 61.635887145996094\n",
      "Epoch [802/1500], Training Loss: 43.55377408203352, Validation Loss: 61.29650115966797\n",
      "Epoch [803/1500], Training Loss: 43.269443610709516, Validation Loss: 60.96052551269531\n",
      "Epoch [804/1500], Training Loss: 42.987598526753345, Validation Loss: 60.62803649902344\n",
      "Epoch [805/1500], Training Loss: 42.70836825410835, Validation Loss: 60.29910659790039\n",
      "Epoch [806/1500], Training Loss: 42.43192771393407, Validation Loss: 59.97368621826172\n",
      "Epoch [807/1500], Training Loss: 42.15763819999765, Validation Loss: 59.6513671875\n",
      "Epoch [808/1500], Training Loss: 41.885932403546995, Validation Loss: 59.332489013671875\n",
      "Epoch [809/1500], Training Loss: 41.616410434900786, Validation Loss: 59.01642990112305\n",
      "Epoch [810/1500], Training Loss: 41.3490899515317, Validation Loss: 58.70341110229492\n",
      "Epoch [811/1500], Training Loss: 41.084286473695855, Validation Loss: 58.393741607666016\n",
      "Epoch [812/1500], Training Loss: 40.821688206415914, Validation Loss: 58.08698654174805\n",
      "Epoch [813/1500], Training Loss: 40.561386041984335, Validation Loss: 57.7833366394043\n",
      "Epoch [814/1500], Training Loss: 40.30344776247863, Validation Loss: 57.483192443847656\n",
      "Epoch [815/1500], Training Loss: 40.04761028829462, Validation Loss: 57.18547439575195\n",
      "Epoch [816/1500], Training Loss: 39.79398992027078, Validation Loss: 56.89092254638672\n",
      "Epoch [817/1500], Training Loss: 39.542685666157325, Validation Loss: 56.59915542602539\n",
      "Epoch [818/1500], Training Loss: 39.29366242729171, Validation Loss: 56.31064224243164\n",
      "Epoch [819/1500], Training Loss: 39.0468576616975, Validation Loss: 56.02479934692383\n",
      "Epoch [820/1500], Training Loss: 38.802356087581764, Validation Loss: 55.74208450317383\n",
      "Epoch [821/1500], Training Loss: 38.559854099492306, Validation Loss: 55.461978912353516\n",
      "Epoch [822/1500], Training Loss: 38.31939562079208, Validation Loss: 55.18451690673828\n",
      "Epoch [823/1500], Training Loss: 38.081098219448876, Validation Loss: 54.91002655029297\n",
      "Epoch [824/1500], Training Loss: 37.844942932948825, Validation Loss: 54.63825225830078\n",
      "Epoch [825/1500], Training Loss: 37.610896431241066, Validation Loss: 54.36908721923828\n",
      "Epoch [826/1500], Training Loss: 37.378853952785136, Validation Loss: 54.10254669189453\n",
      "Epoch [827/1500], Training Loss: 37.14871785457966, Validation Loss: 53.838321685791016\n",
      "Epoch [828/1500], Training Loss: 36.92049855374746, Validation Loss: 53.576873779296875\n",
      "Epoch [829/1500], Training Loss: 36.69442332619761, Validation Loss: 53.318023681640625\n",
      "Epoch [830/1500], Training Loss: 36.47036303138218, Validation Loss: 53.06157302856445\n",
      "Epoch [831/1500], Training Loss: 36.24827050788361, Validation Loss: 52.807701110839844\n",
      "Epoch [832/1500], Training Loss: 36.02788211758393, Validation Loss: 52.55613708496094\n",
      "Epoch [833/1500], Training Loss: 35.809446332440025, Validation Loss: 52.30691909790039\n",
      "Epoch [834/1500], Training Loss: 35.59275355925768, Validation Loss: 52.059844970703125\n",
      "Epoch [835/1500], Training Loss: 35.37788908657136, Validation Loss: 51.815486907958984\n",
      "Epoch [836/1500], Training Loss: 35.164884108805964, Validation Loss: 51.5734748840332\n",
      "Epoch [837/1500], Training Loss: 34.953891801699136, Validation Loss: 51.33363723754883\n",
      "Epoch [838/1500], Training Loss: 34.74488308979715, Validation Loss: 51.09630584716797\n",
      "Epoch [839/1500], Training Loss: 34.53762684151789, Validation Loss: 50.86093521118164\n",
      "Epoch [840/1500], Training Loss: 34.332221485137694, Validation Loss: 50.628273010253906\n",
      "Epoch [841/1500], Training Loss: 34.128507276605255, Validation Loss: 50.39732360839844\n",
      "Epoch [842/1500], Training Loss: 33.92659398141305, Validation Loss: 50.16878890991211\n",
      "Epoch [843/1500], Training Loss: 33.72615716922116, Validation Loss: 49.94184494018555\n",
      "Epoch [844/1500], Training Loss: 33.52736577555832, Validation Loss: 49.71699142456055\n",
      "Epoch [845/1500], Training Loss: 33.3302835467861, Validation Loss: 49.49425506591797\n",
      "Epoch [846/1500], Training Loss: 33.134878370879925, Validation Loss: 49.27362823486328\n",
      "Epoch [847/1500], Training Loss: 32.94112494161399, Validation Loss: 49.055076599121094\n",
      "Epoch [848/1500], Training Loss: 32.74911402466657, Validation Loss: 48.83831024169922\n",
      "Epoch [849/1500], Training Loss: 32.55849226998196, Validation Loss: 48.623355865478516\n",
      "Epoch [850/1500], Training Loss: 32.369511441488505, Validation Loss: 48.41058349609375\n",
      "Epoch [851/1500], Training Loss: 32.18217696204295, Validation Loss: 48.19939041137695\n",
      "Epoch [852/1500], Training Loss: 31.996363851234236, Validation Loss: 47.99013900756836\n",
      "Epoch [853/1500], Training Loss: 31.812227343617554, Validation Loss: 47.7830696105957\n",
      "Epoch [854/1500], Training Loss: 31.629700086388794, Validation Loss: 47.57773971557617\n",
      "Epoch [855/1500], Training Loss: 31.448715788107677, Validation Loss: 47.37430191040039\n",
      "Epoch [856/1500], Training Loss: 31.269269842863995, Validation Loss: 47.17280578613281\n",
      "Epoch [857/1500], Training Loss: 31.09124655127196, Validation Loss: 46.97273635864258\n",
      "Epoch [858/1500], Training Loss: 30.91457677554779, Validation Loss: 46.77473068237305\n",
      "Epoch [859/1500], Training Loss: 30.739316098670642, Validation Loss: 46.57822799682617\n",
      "Epoch [860/1500], Training Loss: 30.565610529456915, Validation Loss: 46.3836784362793\n",
      "Epoch [861/1500], Training Loss: 30.39328408184587, Validation Loss: 46.19060516357422\n",
      "Epoch [862/1500], Training Loss: 30.22232171914091, Validation Loss: 45.9987678527832\n",
      "Epoch [863/1500], Training Loss: 30.052835827085396, Validation Loss: 45.8089485168457\n",
      "Epoch [864/1500], Training Loss: 29.88492181672705, Validation Loss: 45.62087631225586\n",
      "Epoch [865/1500], Training Loss: 29.718383352619902, Validation Loss: 45.434547424316406\n",
      "Epoch [866/1500], Training Loss: 29.5531047659686, Validation Loss: 45.2496223449707\n",
      "Epoch [867/1500], Training Loss: 29.389154159833378, Validation Loss: 45.06642150878906\n",
      "Epoch [868/1500], Training Loss: 29.226690844367653, Validation Loss: 44.88495635986328\n",
      "Epoch [869/1500], Training Loss: 29.065556381897895, Validation Loss: 44.705108642578125\n",
      "Epoch [870/1500], Training Loss: 28.90568820962014, Validation Loss: 44.52687454223633\n",
      "Epoch [871/1500], Training Loss: 28.747139194922415, Validation Loss: 44.35015106201172\n",
      "Epoch [872/1500], Training Loss: 28.589860888793925, Validation Loss: 44.1750373840332\n",
      "Epoch [873/1500], Training Loss: 28.433953383743013, Validation Loss: 44.001617431640625\n",
      "Epoch [874/1500], Training Loss: 28.27921056959378, Validation Loss: 43.82947540283203\n",
      "Epoch [875/1500], Training Loss: 28.12575643085514, Validation Loss: 43.65878677368164\n",
      "Epoch [876/1500], Training Loss: 27.97357676083486, Validation Loss: 43.48942184448242\n",
      "Epoch [877/1500], Training Loss: 27.822450676562607, Validation Loss: 43.321537017822266\n",
      "Epoch [878/1500], Training Loss: 27.672595991676946, Validation Loss: 43.15505599975586\n",
      "Epoch [879/1500], Training Loss: 27.523829257538935, Validation Loss: 42.98968505859375\n",
      "Epoch [880/1500], Training Loss: 27.376173971500204, Validation Loss: 42.82612228393555\n",
      "Epoch [881/1500], Training Loss: 27.229612229914437, Validation Loss: 42.66367721557617\n",
      "Epoch [882/1500], Training Loss: 27.084199666880966, Validation Loss: 42.502540588378906\n",
      "Epoch [883/1500], Training Loss: 26.940038022596795, Validation Loss: 42.34284591674805\n",
      "Epoch [884/1500], Training Loss: 26.79708829904744, Validation Loss: 42.18431854248047\n",
      "Epoch [885/1500], Training Loss: 26.655346446300452, Validation Loss: 42.02732467651367\n",
      "Epoch [886/1500], Training Loss: 26.514674568470717, Validation Loss: 41.87154769897461\n",
      "Epoch [887/1500], Training Loss: 26.37511358228691, Validation Loss: 41.71741485595703\n",
      "Epoch [888/1500], Training Loss: 26.236665717498315, Validation Loss: 41.564395904541016\n",
      "Epoch [889/1500], Training Loss: 26.099345192148444, Validation Loss: 41.41270065307617\n",
      "Epoch [890/1500], Training Loss: 25.963185688782804, Validation Loss: 41.262203216552734\n",
      "Epoch [891/1500], Training Loss: 25.828160175829044, Validation Loss: 41.112953186035156\n",
      "Epoch [892/1500], Training Loss: 25.694176566057003, Validation Loss: 40.964927673339844\n",
      "Epoch [893/1500], Training Loss: 25.56125654983663, Validation Loss: 40.81841278076172\n",
      "Epoch [894/1500], Training Loss: 25.42936616008305, Validation Loss: 40.6729850769043\n",
      "Epoch [895/1500], Training Loss: 25.298400277572092, Validation Loss: 40.528709411621094\n",
      "Epoch [896/1500], Training Loss: 25.16848355887863, Validation Loss: 40.385704040527344\n",
      "Epoch [897/1500], Training Loss: 25.039603632032907, Validation Loss: 40.243831634521484\n",
      "Epoch [898/1500], Training Loss: 24.911841151736382, Validation Loss: 40.103179931640625\n",
      "Epoch [899/1500], Training Loss: 24.78510307034187, Validation Loss: 39.96355438232422\n",
      "Epoch [900/1500], Training Loss: 24.659344953125263, Validation Loss: 39.82564163208008\n",
      "Epoch [901/1500], Training Loss: 24.534547653346962, Validation Loss: 39.688331604003906\n",
      "Epoch [902/1500], Training Loss: 24.410757720586698, Validation Loss: 39.552154541015625\n",
      "Epoch [903/1500], Training Loss: 24.28791700729047, Validation Loss: 39.41731643676758\n",
      "Epoch [904/1500], Training Loss: 24.165922814900682, Validation Loss: 39.282894134521484\n",
      "Epoch [905/1500], Training Loss: 24.044902365346033, Validation Loss: 39.150081634521484\n",
      "Epoch [906/1500], Training Loss: 23.92481414492067, Validation Loss: 39.01783752441406\n",
      "Epoch [907/1500], Training Loss: 23.805728662373685, Validation Loss: 38.88715744018555\n",
      "Epoch [908/1500], Training Loss: 23.68749169998108, Validation Loss: 38.757022857666016\n",
      "Epoch [909/1500], Training Loss: 23.57007659288807, Validation Loss: 38.62793731689453\n",
      "Epoch [910/1500], Training Loss: 23.453651282841854, Validation Loss: 38.50016403198242\n",
      "Epoch [911/1500], Training Loss: 23.338168210782154, Validation Loss: 38.373294830322266\n",
      "Epoch [912/1500], Training Loss: 23.22347855600949, Validation Loss: 38.24746322631836\n",
      "Epoch [913/1500], Training Loss: 23.10963834086755, Validation Loss: 38.12245559692383\n",
      "Epoch [914/1500], Training Loss: 22.996750364500073, Validation Loss: 37.998470306396484\n",
      "Epoch [915/1500], Training Loss: 22.884670506014732, Validation Loss: 37.87525939941406\n",
      "Epoch [916/1500], Training Loss: 22.7735523538385, Validation Loss: 37.753257751464844\n",
      "Epoch [917/1500], Training Loss: 22.66326032390859, Validation Loss: 37.632118225097656\n",
      "Epoch [918/1500], Training Loss: 22.553823544308067, Validation Loss: 37.51152038574219\n",
      "Epoch [919/1500], Training Loss: 22.445141460476847, Validation Loss: 37.39201736450195\n",
      "Epoch [920/1500], Training Loss: 22.33723464395842, Validation Loss: 37.27338409423828\n",
      "Epoch [921/1500], Training Loss: 22.229998095318695, Validation Loss: 37.155494689941406\n",
      "Epoch [922/1500], Training Loss: 22.12357221255416, Validation Loss: 37.03840255737305\n",
      "Epoch [923/1500], Training Loss: 22.017862394093214, Validation Loss: 36.92233657836914\n",
      "Epoch [924/1500], Training Loss: 21.91296752326274, Validation Loss: 36.80732727050781\n",
      "Epoch [925/1500], Training Loss: 21.80895668181446, Validation Loss: 36.69276428222656\n",
      "Epoch [926/1500], Training Loss: 21.70577517400329, Validation Loss: 36.57929992675781\n",
      "Epoch [927/1500], Training Loss: 21.60332659050695, Validation Loss: 36.466773986816406\n",
      "Epoch [928/1500], Training Loss: 21.501739002968804, Validation Loss: 36.35500717163086\n",
      "Epoch [929/1500], Training Loss: 21.400963252361628, Validation Loss: 36.24386978149414\n",
      "Epoch [930/1500], Training Loss: 21.300905298197396, Validation Loss: 36.133758544921875\n",
      "Epoch [931/1500], Training Loss: 21.201701362384494, Validation Loss: 36.024532318115234\n",
      "Epoch [932/1500], Training Loss: 21.103338661074325, Validation Loss: 35.91618728637695\n",
      "Epoch [933/1500], Training Loss: 21.005690100710048, Validation Loss: 35.80860900878906\n",
      "Epoch [934/1500], Training Loss: 20.908709686575595, Validation Loss: 35.701454162597656\n",
      "Epoch [935/1500], Training Loss: 20.812566553379835, Validation Loss: 35.595394134521484\n",
      "Epoch [936/1500], Training Loss: 20.71699212864632, Validation Loss: 35.49017333984375\n",
      "Epoch [937/1500], Training Loss: 20.622254177398823, Validation Loss: 35.38562774658203\n",
      "Epoch [938/1500], Training Loss: 20.528155583957787, Validation Loss: 35.28169250488281\n",
      "Epoch [939/1500], Training Loss: 20.434721327051253, Validation Loss: 35.17853546142578\n",
      "Epoch [940/1500], Training Loss: 20.34188434866097, Validation Loss: 35.07609558105469\n",
      "Epoch [941/1500], Training Loss: 20.24979382678308, Validation Loss: 34.974178314208984\n",
      "Epoch [942/1500], Training Loss: 20.15829574944489, Validation Loss: 34.87330627441406\n",
      "Epoch [943/1500], Training Loss: 20.06752006892613, Validation Loss: 34.77306365966797\n",
      "Epoch [944/1500], Training Loss: 19.97739519292308, Validation Loss: 34.67365646362305\n",
      "Epoch [945/1500], Training Loss: 19.8880158323312, Validation Loss: 34.5749626159668\n",
      "Epoch [946/1500], Training Loss: 19.79929430355213, Validation Loss: 34.476844787597656\n",
      "Epoch [947/1500], Training Loss: 19.71122118683933, Validation Loss: 34.37961196899414\n",
      "Epoch [948/1500], Training Loss: 19.62391940806076, Validation Loss: 34.2830696105957\n",
      "Epoch [949/1500], Training Loss: 19.537128426440937, Validation Loss: 34.18708038330078\n",
      "Epoch [950/1500], Training Loss: 19.45106940480907, Validation Loss: 34.091915130615234\n",
      "Epoch [951/1500], Training Loss: 19.36567445997174, Validation Loss: 33.99727249145508\n",
      "Epoch [952/1500], Training Loss: 19.280792776478116, Validation Loss: 33.903480529785156\n",
      "Epoch [953/1500], Training Loss: 19.196468733792916, Validation Loss: 33.81013870239258\n",
      "Epoch [954/1500], Training Loss: 19.11276387898145, Validation Loss: 33.717376708984375\n",
      "Epoch [955/1500], Training Loss: 19.02969381252643, Validation Loss: 33.62532424926758\n",
      "Epoch [956/1500], Training Loss: 18.947217065098584, Validation Loss: 33.53380584716797\n",
      "Epoch [957/1500], Training Loss: 18.86526414430456, Validation Loss: 33.442955017089844\n",
      "Epoch [958/1500], Training Loss: 18.783918896615244, Validation Loss: 33.3528938293457\n",
      "Epoch [959/1500], Training Loss: 18.7032022832286, Validation Loss: 33.263389587402344\n",
      "Epoch [960/1500], Training Loss: 18.622985100673798, Validation Loss: 33.17448043823242\n",
      "Epoch [961/1500], Training Loss: 18.543315684170523, Validation Loss: 33.086082458496094\n",
      "Epoch [962/1500], Training Loss: 18.464202340031605, Validation Loss: 32.99837875366211\n",
      "Epoch [963/1500], Training Loss: 18.385711261017622, Validation Loss: 32.91097640991211\n",
      "Epoch [964/1500], Training Loss: 18.30777698941006, Validation Loss: 32.824256896972656\n",
      "Epoch [965/1500], Training Loss: 18.230416040552647, Validation Loss: 32.73836135864258\n",
      "Epoch [966/1500], Training Loss: 18.15363127205118, Validation Loss: 32.65286636352539\n",
      "Epoch [967/1500], Training Loss: 18.077489285892977, Validation Loss: 32.56796646118164\n",
      "Epoch [968/1500], Training Loss: 18.00180526928041, Validation Loss: 32.483795166015625\n",
      "Epoch [969/1500], Training Loss: 17.926662325779812, Validation Loss: 32.40010452270508\n",
      "Epoch [970/1500], Training Loss: 17.851971741851436, Validation Loss: 32.31674575805664\n",
      "Epoch [971/1500], Training Loss: 17.77772450178267, Validation Loss: 32.234073638916016\n",
      "Epoch [972/1500], Training Loss: 17.703912342598024, Validation Loss: 32.1517333984375\n",
      "Epoch [973/1500], Training Loss: 17.630715160980273, Validation Loss: 32.070098876953125\n",
      "Epoch [974/1500], Training Loss: 17.5580345970362, Validation Loss: 31.989181518554688\n",
      "Epoch [975/1500], Training Loss: 17.48584427907209, Validation Loss: 31.908348083496094\n",
      "Epoch [976/1500], Training Loss: 17.414188651170708, Validation Loss: 31.82845687866211\n",
      "Epoch [977/1500], Training Loss: 17.342911788791323, Validation Loss: 31.748828887939453\n",
      "Epoch [978/1500], Training Loss: 17.272112868774215, Validation Loss: 31.66963768005371\n",
      "Epoch [979/1500], Training Loss: 17.20177685613667, Validation Loss: 31.591079711914062\n",
      "Epoch [980/1500], Training Loss: 17.13190581497058, Validation Loss: 31.512954711914062\n",
      "Epoch [981/1500], Training Loss: 17.062554276963116, Validation Loss: 31.435373306274414\n",
      "Epoch [982/1500], Training Loss: 16.993622035228857, Validation Loss: 31.35832977294922\n",
      "Epoch [983/1500], Training Loss: 16.925151591343184, Validation Loss: 31.281757354736328\n",
      "Epoch [984/1500], Training Loss: 16.857091577687736, Validation Loss: 31.205638885498047\n",
      "Epoch [985/1500], Training Loss: 16.789439630407404, Validation Loss: 31.129989624023438\n",
      "Epoch [986/1500], Training Loss: 16.722206108944267, Validation Loss: 31.05475425720215\n",
      "Epoch [987/1500], Training Loss: 16.655446094456263, Validation Loss: 30.979900360107422\n",
      "Epoch [988/1500], Training Loss: 16.589105090817117, Validation Loss: 30.905620574951172\n",
      "Epoch [989/1500], Training Loss: 16.52321345382062, Validation Loss: 30.83179473876953\n",
      "Epoch [990/1500], Training Loss: 16.457671282151857, Validation Loss: 30.758420944213867\n",
      "Epoch [991/1500], Training Loss: 16.39258676100544, Validation Loss: 30.68553924560547\n",
      "Epoch [992/1500], Training Loss: 16.327899830710326, Validation Loss: 30.6129093170166\n",
      "Epoch [993/1500], Training Loss: 16.263640878384223, Validation Loss: 30.54098129272461\n",
      "Epoch [994/1500], Training Loss: 16.199834700884008, Validation Loss: 30.46924591064453\n",
      "Epoch [995/1500], Training Loss: 16.136351000924435, Validation Loss: 30.398250579833984\n",
      "Epoch [996/1500], Training Loss: 16.073219375693768, Validation Loss: 30.32744026184082\n",
      "Epoch [997/1500], Training Loss: 16.01047051584319, Validation Loss: 30.25706672668457\n",
      "Epoch [998/1500], Training Loss: 15.948215566832062, Validation Loss: 30.187334060668945\n",
      "Epoch [999/1500], Training Loss: 15.886407213442139, Validation Loss: 30.11819839477539\n",
      "Epoch [1000/1500], Training Loss: 15.825013516751971, Validation Loss: 30.04922866821289\n",
      "Epoch [1001/1500], Training Loss: 15.763988996914705, Validation Loss: 29.980730056762695\n",
      "Epoch [1002/1500], Training Loss: 15.703335278094954, Validation Loss: 29.912708282470703\n",
      "Epoch [1003/1500], Training Loss: 15.643058095801134, Validation Loss: 29.845008850097656\n",
      "Epoch [1004/1500], Training Loss: 15.58312811355894, Validation Loss: 29.777780532836914\n",
      "Epoch [1005/1500], Training Loss: 15.523601585987024, Validation Loss: 29.711074829101562\n",
      "Epoch [1006/1500], Training Loss: 15.464438932779975, Validation Loss: 29.644590377807617\n",
      "Epoch [1007/1500], Training Loss: 15.40565268292133, Validation Loss: 29.57853889465332\n",
      "Epoch [1008/1500], Training Loss: 15.347225991154305, Validation Loss: 29.512868881225586\n",
      "Epoch [1009/1500], Training Loss: 15.28927248166888, Validation Loss: 29.447677612304688\n",
      "Epoch [1010/1500], Training Loss: 15.231714430743281, Validation Loss: 29.383081436157227\n",
      "Epoch [1011/1500], Training Loss: 15.174493146087398, Validation Loss: 29.31878089904785\n",
      "Epoch [1012/1500], Training Loss: 15.117611952355297, Validation Loss: 29.254846572875977\n",
      "Epoch [1013/1500], Training Loss: 15.061046569689456, Validation Loss: 29.191329956054688\n",
      "Epoch [1014/1500], Training Loss: 15.004781378901155, Validation Loss: 29.128253936767578\n",
      "Epoch [1015/1500], Training Loss: 14.94894382482607, Validation Loss: 29.065534591674805\n",
      "Epoch [1016/1500], Training Loss: 14.893426478023605, Validation Loss: 29.003110885620117\n",
      "Epoch [1017/1500], Training Loss: 14.83824645070369, Validation Loss: 28.941205978393555\n",
      "Epoch [1018/1500], Training Loss: 14.783401433103329, Validation Loss: 28.87967300415039\n",
      "Epoch [1019/1500], Training Loss: 14.72885731337516, Validation Loss: 28.818401336669922\n",
      "Epoch [1020/1500], Training Loss: 14.674574892961646, Validation Loss: 28.757564544677734\n",
      "Epoch [1021/1500], Training Loss: 14.620607327248898, Validation Loss: 28.697086334228516\n",
      "Epoch [1022/1500], Training Loss: 14.566990169610026, Validation Loss: 28.637027740478516\n",
      "Epoch [1023/1500], Training Loss: 14.513757086785196, Validation Loss: 28.57735252380371\n",
      "Epoch [1024/1500], Training Loss: 14.460778961021788, Validation Loss: 28.518054962158203\n",
      "Epoch [1025/1500], Training Loss: 14.408123189180085, Validation Loss: 28.45901870727539\n",
      "Epoch [1026/1500], Training Loss: 14.35579899581533, Validation Loss: 28.400400161743164\n",
      "Epoch [1027/1500], Training Loss: 14.303760711698049, Validation Loss: 28.342063903808594\n",
      "Epoch [1028/1500], Training Loss: 14.252057433109414, Validation Loss: 28.284151077270508\n",
      "Epoch [1029/1500], Training Loss: 14.200671077897686, Validation Loss: 28.22666358947754\n",
      "Epoch [1030/1500], Training Loss: 14.14954038669681, Validation Loss: 28.16936683654785\n",
      "Epoch [1031/1500], Training Loss: 14.098670102705588, Validation Loss: 28.112491607666016\n",
      "Epoch [1032/1500], Training Loss: 14.048075454450206, Validation Loss: 28.056045532226562\n",
      "Epoch [1033/1500], Training Loss: 13.99780752702666, Validation Loss: 28.000028610229492\n",
      "Epoch [1034/1500], Training Loss: 13.947767446552263, Validation Loss: 27.944211959838867\n",
      "Epoch [1035/1500], Training Loss: 13.898019155930447, Validation Loss: 27.88878059387207\n",
      "Epoch [1036/1500], Training Loss: 13.848609383754349, Validation Loss: 27.833721160888672\n",
      "Epoch [1037/1500], Training Loss: 13.799567218980224, Validation Loss: 27.779041290283203\n",
      "Epoch [1038/1500], Training Loss: 13.750800016699955, Validation Loss: 27.72470474243164\n",
      "Epoch [1039/1500], Training Loss: 13.702326896562978, Validation Loss: 27.670719146728516\n",
      "Epoch [1040/1500], Training Loss: 13.654147718413054, Validation Loss: 27.617000579833984\n",
      "Epoch [1041/1500], Training Loss: 13.60623723776677, Validation Loss: 27.563589096069336\n",
      "Epoch [1042/1500], Training Loss: 13.558596800303716, Validation Loss: 27.510526657104492\n",
      "Epoch [1043/1500], Training Loss: 13.511224997243703, Validation Loss: 27.457826614379883\n",
      "Epoch [1044/1500], Training Loss: 13.464099976707352, Validation Loss: 27.405378341674805\n",
      "Epoch [1045/1500], Training Loss: 13.417236616853431, Validation Loss: 27.353296279907227\n",
      "Epoch [1046/1500], Training Loss: 13.370595092021098, Validation Loss: 27.301504135131836\n",
      "Epoch [1047/1500], Training Loss: 13.324160150825913, Validation Loss: 27.249910354614258\n",
      "Epoch [1048/1500], Training Loss: 13.278004446457926, Validation Loss: 27.19877815246582\n",
      "Epoch [1049/1500], Training Loss: 13.232147562851633, Validation Loss: 27.147869110107422\n",
      "Epoch [1050/1500], Training Loss: 13.186529729599407, Validation Loss: 27.09715461730957\n",
      "Epoch [1051/1500], Training Loss: 13.141152143118697, Validation Loss: 27.046655654907227\n",
      "Epoch [1052/1500], Training Loss: 13.096016928303388, Validation Loss: 26.996545791625977\n",
      "Epoch [1053/1500], Training Loss: 13.051181904328178, Validation Loss: 26.946765899658203\n",
      "Epoch [1054/1500], Training Loss: 13.0065924671283, Validation Loss: 26.897342681884766\n",
      "Epoch [1055/1500], Training Loss: 12.962297482238343, Validation Loss: 26.848140716552734\n",
      "Epoch [1056/1500], Training Loss: 12.91820273400745, Validation Loss: 26.799287796020508\n",
      "Epoch [1057/1500], Training Loss: 12.87437973759604, Validation Loss: 26.75068473815918\n",
      "Epoch [1058/1500], Training Loss: 12.830759211825976, Validation Loss: 26.702417373657227\n",
      "Epoch [1059/1500], Training Loss: 12.787447211276065, Validation Loss: 26.654571533203125\n",
      "Epoch [1060/1500], Training Loss: 12.744391126592562, Validation Loss: 26.606969833374023\n",
      "Epoch [1061/1500], Training Loss: 12.701573023611152, Validation Loss: 26.5595645904541\n",
      "Epoch [1062/1500], Training Loss: 12.658988848648265, Validation Loss: 26.512313842773438\n",
      "Epoch [1063/1500], Training Loss: 12.616600123289057, Validation Loss: 26.465425491333008\n",
      "Epoch [1064/1500], Training Loss: 12.574394371115115, Validation Loss: 26.418807983398438\n",
      "Epoch [1065/1500], Training Loss: 12.532365122032399, Validation Loss: 26.37227439880371\n",
      "Epoch [1066/1500], Training Loss: 12.490569241839076, Validation Loss: 26.32611846923828\n",
      "Epoch [1067/1500], Training Loss: 12.449033801101766, Validation Loss: 26.280282974243164\n",
      "Epoch [1068/1500], Training Loss: 12.407719659868897, Validation Loss: 26.234628677368164\n",
      "Epoch [1069/1500], Training Loss: 12.366643752293317, Validation Loss: 26.189245223999023\n",
      "Epoch [1070/1500], Training Loss: 12.325785767473283, Validation Loss: 26.14417839050293\n",
      "Epoch [1071/1500], Training Loss: 12.285101883203106, Validation Loss: 26.099225997924805\n",
      "Epoch [1072/1500], Training Loss: 12.244628599644825, Validation Loss: 26.054542541503906\n",
      "Epoch [1073/1500], Training Loss: 12.204337581593354, Validation Loss: 26.009990692138672\n",
      "Epoch [1074/1500], Training Loss: 12.164267151013448, Validation Loss: 25.965700149536133\n",
      "Epoch [1075/1500], Training Loss: 12.124423177032906, Validation Loss: 25.921724319458008\n",
      "Epoch [1076/1500], Training Loss: 12.084792539237348, Validation Loss: 25.877880096435547\n",
      "Epoch [1077/1500], Training Loss: 12.04540989186965, Validation Loss: 25.8343505859375\n",
      "Epoch [1078/1500], Training Loss: 12.006257467750393, Validation Loss: 25.791141510009766\n",
      "Epoch [1079/1500], Training Loss: 11.967289628964696, Validation Loss: 25.748106002807617\n",
      "Epoch [1080/1500], Training Loss: 11.928487447735987, Validation Loss: 25.70526885986328\n",
      "Epoch [1081/1500], Training Loss: 11.889904331264512, Validation Loss: 25.662569046020508\n",
      "Epoch [1082/1500], Training Loss: 11.85152146978864, Validation Loss: 25.620098114013672\n",
      "Epoch [1083/1500], Training Loss: 11.813344958532625, Validation Loss: 25.577838897705078\n",
      "Epoch [1084/1500], Training Loss: 11.775370197346513, Validation Loss: 25.535808563232422\n",
      "Epoch [1085/1500], Training Loss: 11.737620560152813, Validation Loss: 25.49403953552246\n",
      "Epoch [1086/1500], Training Loss: 11.700051182618276, Validation Loss: 25.45256233215332\n",
      "Epoch [1087/1500], Training Loss: 11.66264145720682, Validation Loss: 25.411108016967773\n",
      "Epoch [1088/1500], Training Loss: 11.625445406681056, Validation Loss: 25.369993209838867\n",
      "Epoch [1089/1500], Training Loss: 11.58840668734527, Validation Loss: 25.32904052734375\n",
      "Epoch [1090/1500], Training Loss: 11.551546167714452, Validation Loss: 25.28811264038086\n",
      "Epoch [1091/1500], Training Loss: 11.514886188768264, Validation Loss: 25.247468948364258\n",
      "Epoch [1092/1500], Training Loss: 11.478400363344337, Validation Loss: 25.207082748413086\n",
      "Epoch [1093/1500], Training Loss: 11.442075800923012, Validation Loss: 25.166746139526367\n",
      "Epoch [1094/1500], Training Loss: 11.405953265400633, Validation Loss: 25.126689910888672\n",
      "Epoch [1095/1500], Training Loss: 11.369966051961505, Validation Loss: 25.08674430847168\n",
      "Epoch [1096/1500], Training Loss: 11.334156781870929, Validation Loss: 25.04695701599121\n",
      "Epoch [1097/1500], Training Loss: 11.298561712195973, Validation Loss: 25.007381439208984\n",
      "Epoch [1098/1500], Training Loss: 11.263131980279347, Validation Loss: 24.967971801757812\n",
      "Epoch [1099/1500], Training Loss: 11.227866565847918, Validation Loss: 24.928667068481445\n",
      "Epoch [1100/1500], Training Loss: 11.192785757869032, Validation Loss: 24.889476776123047\n",
      "Epoch [1101/1500], Training Loss: 11.157889187351929, Validation Loss: 24.850536346435547\n",
      "Epoch [1102/1500], Training Loss: 11.123185146046902, Validation Loss: 24.811870574951172\n",
      "Epoch [1103/1500], Training Loss: 11.088630228188318, Validation Loss: 24.773359298706055\n",
      "Epoch [1104/1500], Training Loss: 11.05427425495216, Validation Loss: 24.73508644104004\n",
      "Epoch [1105/1500], Training Loss: 11.020052691385507, Validation Loss: 24.69679832458496\n",
      "Epoch [1106/1500], Training Loss: 10.986008514048319, Validation Loss: 24.658620834350586\n",
      "Epoch [1107/1500], Training Loss: 10.952138406043112, Validation Loss: 24.62061309814453\n",
      "Epoch [1108/1500], Training Loss: 10.918431016370496, Validation Loss: 24.58292579650879\n",
      "Epoch [1109/1500], Training Loss: 10.884941751306833, Validation Loss: 24.54535484313965\n",
      "Epoch [1110/1500], Training Loss: 10.851607822246008, Validation Loss: 24.507858276367188\n",
      "Epoch [1111/1500], Training Loss: 10.818417943194365, Validation Loss: 24.470550537109375\n",
      "Epoch [1112/1500], Training Loss: 10.78538947161936, Validation Loss: 24.433454513549805\n",
      "Epoch [1113/1500], Training Loss: 10.752516704226192, Validation Loss: 24.39641761779785\n",
      "Epoch [1114/1500], Training Loss: 10.719796490559926, Validation Loss: 24.359603881835938\n",
      "Epoch [1115/1500], Training Loss: 10.687253502294714, Validation Loss: 24.322933197021484\n",
      "Epoch [1116/1500], Training Loss: 10.65488562763829, Validation Loss: 24.286571502685547\n",
      "Epoch [1117/1500], Training Loss: 10.622691727103525, Validation Loss: 24.250261306762695\n",
      "Epoch [1118/1500], Training Loss: 10.59065661440521, Validation Loss: 24.214096069335938\n",
      "Epoch [1119/1500], Training Loss: 10.55877219581624, Validation Loss: 24.17807388305664\n",
      "Epoch [1120/1500], Training Loss: 10.527049098928833, Validation Loss: 24.1422176361084\n",
      "Epoch [1121/1500], Training Loss: 10.495476680489936, Validation Loss: 24.10647201538086\n",
      "Epoch [1122/1500], Training Loss: 10.464071741406858, Validation Loss: 24.070764541625977\n",
      "Epoch [1123/1500], Training Loss: 10.432830683599942, Validation Loss: 24.035350799560547\n",
      "Epoch [1124/1500], Training Loss: 10.401703704747083, Validation Loss: 24.000051498413086\n",
      "Epoch [1125/1500], Training Loss: 10.370677844584161, Validation Loss: 23.9648494720459\n",
      "Epoch [1126/1500], Training Loss: 10.3398002750594, Validation Loss: 23.92978858947754\n",
      "Epoch [1127/1500], Training Loss: 10.309081108420942, Validation Loss: 23.89487648010254\n",
      "Epoch [1128/1500], Training Loss: 10.278487949875617, Validation Loss: 23.860122680664062\n",
      "Epoch [1129/1500], Training Loss: 10.248027967297121, Validation Loss: 23.82549476623535\n",
      "Epoch [1130/1500], Training Loss: 10.217712123250445, Validation Loss: 23.791006088256836\n",
      "Epoch [1131/1500], Training Loss: 10.187533328039137, Validation Loss: 23.75650978088379\n",
      "Epoch [1132/1500], Training Loss: 10.15748448832188, Validation Loss: 23.722232818603516\n",
      "Epoch [1133/1500], Training Loss: 10.127602493308922, Validation Loss: 23.688186645507812\n",
      "Epoch [1134/1500], Training Loss: 10.0978551935868, Validation Loss: 23.654081344604492\n",
      "Epoch [1135/1500], Training Loss: 10.068235965455875, Validation Loss: 23.62015151977539\n",
      "Epoch [1136/1500], Training Loss: 10.038711656484244, Validation Loss: 23.58631134033203\n",
      "Epoch [1137/1500], Training Loss: 10.009322648604048, Validation Loss: 23.552595138549805\n",
      "Epoch [1138/1500], Training Loss: 9.98007550320696, Validation Loss: 23.518991470336914\n",
      "Epoch [1139/1500], Training Loss: 9.950966502869523, Validation Loss: 23.48552703857422\n",
      "Epoch [1140/1500], Training Loss: 9.922012238347232, Validation Loss: 23.45211410522461\n",
      "Epoch [1141/1500], Training Loss: 9.893185547958016, Validation Loss: 23.418895721435547\n",
      "Epoch [1142/1500], Training Loss: 9.864488532228059, Validation Loss: 23.385883331298828\n",
      "Epoch [1143/1500], Training Loss: 9.835899783877645, Validation Loss: 23.352930068969727\n",
      "Epoch [1144/1500], Training Loss: 9.80745794611574, Validation Loss: 23.32007598876953\n",
      "Epoch [1145/1500], Training Loss: 9.779157889138975, Validation Loss: 23.287464141845703\n",
      "Epoch [1146/1500], Training Loss: 9.75099785308261, Validation Loss: 23.254901885986328\n",
      "Epoch [1147/1500], Training Loss: 9.722949930264003, Validation Loss: 23.222593307495117\n",
      "Epoch [1148/1500], Training Loss: 9.695067050348172, Validation Loss: 23.1904239654541\n",
      "Epoch [1149/1500], Training Loss: 9.667305741701993, Validation Loss: 23.158309936523438\n",
      "Epoch [1150/1500], Training Loss: 9.639689772795677, Validation Loss: 23.12640380859375\n",
      "Epoch [1151/1500], Training Loss: 9.612203297996176, Validation Loss: 23.09440803527832\n",
      "Epoch [1152/1500], Training Loss: 9.584861318747226, Validation Loss: 23.062702178955078\n",
      "Epoch [1153/1500], Training Loss: 9.557666045131691, Validation Loss: 23.031110763549805\n",
      "Epoch [1154/1500], Training Loss: 9.530555941988244, Validation Loss: 22.999473571777344\n",
      "Epoch [1155/1500], Training Loss: 9.503526829240105, Validation Loss: 22.967994689941406\n",
      "Epoch [1156/1500], Training Loss: 9.476640609047239, Validation Loss: 22.936695098876953\n",
      "Epoch [1157/1500], Training Loss: 9.4499005607065, Validation Loss: 22.90552520751953\n",
      "Epoch [1158/1500], Training Loss: 9.423285889373245, Validation Loss: 22.874568939208984\n",
      "Epoch [1159/1500], Training Loss: 9.39676743582969, Validation Loss: 22.843708038330078\n",
      "Epoch [1160/1500], Training Loss: 9.37034755634632, Validation Loss: 22.812889099121094\n",
      "Epoch [1161/1500], Training Loss: 9.34404918351534, Validation Loss: 22.782180786132812\n",
      "Epoch [1162/1500], Training Loss: 9.317892991034798, Validation Loss: 22.751678466796875\n",
      "Epoch [1163/1500], Training Loss: 9.291868434316251, Validation Loss: 22.721315383911133\n",
      "Epoch [1164/1500], Training Loss: 9.26592448382836, Validation Loss: 22.69111442565918\n",
      "Epoch [1165/1500], Training Loss: 9.240089935127923, Validation Loss: 22.660785675048828\n",
      "Epoch [1166/1500], Training Loss: 9.214328427878689, Validation Loss: 22.63068389892578\n",
      "Epoch [1167/1500], Training Loss: 9.188681797293501, Validation Loss: 22.60062599182129\n",
      "Epoch [1168/1500], Training Loss: 9.16312633996007, Validation Loss: 22.57064437866211\n",
      "Epoch [1169/1500], Training Loss: 9.137652017308458, Validation Loss: 22.54082489013672\n",
      "Epoch [1170/1500], Training Loss: 9.112264330177679, Validation Loss: 22.511043548583984\n",
      "Epoch [1171/1500], Training Loss: 9.087016369251659, Validation Loss: 22.48145866394043\n",
      "Epoch [1172/1500], Training Loss: 9.061895430412852, Validation Loss: 22.451984405517578\n",
      "Epoch [1173/1500], Training Loss: 9.036920987201201, Validation Loss: 22.42270278930664\n",
      "Epoch [1174/1500], Training Loss: 9.012012178031352, Validation Loss: 22.393455505371094\n",
      "Epoch [1175/1500], Training Loss: 8.987209786841369, Validation Loss: 22.3643798828125\n",
      "Epoch [1176/1500], Training Loss: 8.962499943723746, Validation Loss: 22.33536720275879\n",
      "Epoch [1177/1500], Training Loss: 8.937950866577294, Validation Loss: 22.306425094604492\n",
      "Epoch [1178/1500], Training Loss: 8.913513408060377, Validation Loss: 22.277734756469727\n",
      "Epoch [1179/1500], Training Loss: 8.889186861837317, Validation Loss: 22.249176025390625\n",
      "Epoch [1180/1500], Training Loss: 8.864916004634445, Validation Loss: 22.22072982788086\n",
      "Epoch [1181/1500], Training Loss: 8.840754680406564, Validation Loss: 22.192378997802734\n",
      "Epoch [1182/1500], Training Loss: 8.816686595980931, Validation Loss: 22.164051055908203\n",
      "Epoch [1183/1500], Training Loss: 8.792699702029791, Validation Loss: 22.135799407958984\n",
      "Epoch [1184/1500], Training Loss: 8.768810953747373, Validation Loss: 22.107561111450195\n",
      "Epoch [1185/1500], Training Loss: 8.745027865800969, Validation Loss: 22.07964324951172\n",
      "Epoch [1186/1500], Training Loss: 8.721355355280396, Validation Loss: 22.051794052124023\n",
      "Epoch [1187/1500], Training Loss: 8.697776490709868, Validation Loss: 22.02400016784668\n",
      "Epoch [1188/1500], Training Loss: 8.674287897049926, Validation Loss: 21.996387481689453\n",
      "Epoch [1189/1500], Training Loss: 8.650885872276746, Validation Loss: 21.968795776367188\n",
      "Epoch [1190/1500], Training Loss: 8.627583912490854, Validation Loss: 21.941360473632812\n",
      "Epoch [1191/1500], Training Loss: 8.604346388965688, Validation Loss: 21.913875579833984\n",
      "Epoch [1192/1500], Training Loss: 8.581269587134706, Validation Loss: 21.886499404907227\n",
      "Epoch [1193/1500], Training Loss: 8.558258860894, Validation Loss: 21.859420776367188\n",
      "Epoch [1194/1500], Training Loss: 8.535300098073654, Validation Loss: 21.83225440979004\n",
      "Epoch [1195/1500], Training Loss: 8.51245514353065, Validation Loss: 21.805282592773438\n",
      "Epoch [1196/1500], Training Loss: 8.48967789308714, Validation Loss: 21.778345108032227\n",
      "Epoch [1197/1500], Training Loss: 8.466980489584056, Validation Loss: 21.751461029052734\n",
      "Epoch [1198/1500], Training Loss: 8.444369908183774, Validation Loss: 21.72475242614746\n",
      "Epoch [1199/1500], Training Loss: 8.421806534906596, Validation Loss: 21.698020935058594\n",
      "Epoch [1200/1500], Training Loss: 8.399307711306397, Validation Loss: 21.671329498291016\n",
      "Epoch [1201/1500], Training Loss: 8.37692122257762, Validation Loss: 21.644739151000977\n",
      "Epoch [1202/1500], Training Loss: 8.354612776887414, Validation Loss: 21.618276596069336\n",
      "Epoch [1203/1500], Training Loss: 8.33234930482087, Validation Loss: 21.59197235107422\n",
      "Epoch [1204/1500], Training Loss: 8.310161610069077, Validation Loss: 21.565763473510742\n",
      "Epoch [1205/1500], Training Loss: 8.288079401743648, Validation Loss: 21.53964614868164\n",
      "Epoch [1206/1500], Training Loss: 8.266087761818376, Validation Loss: 21.513580322265625\n",
      "Epoch [1207/1500], Training Loss: 8.244168067337599, Validation Loss: 21.48772430419922\n",
      "Epoch [1208/1500], Training Loss: 8.222312386828628, Validation Loss: 21.461729049682617\n",
      "Epoch [1209/1500], Training Loss: 8.20052614088445, Validation Loss: 21.435890197753906\n",
      "Epoch [1210/1500], Training Loss: 8.17880255599707, Validation Loss: 21.410045623779297\n",
      "Epoch [1211/1500], Training Loss: 8.15712688941249, Validation Loss: 21.384214401245117\n",
      "Epoch [1212/1500], Training Loss: 8.13552792555072, Validation Loss: 21.358613967895508\n",
      "Epoch [1213/1500], Training Loss: 8.114011827636865, Validation Loss: 21.333036422729492\n",
      "Epoch [1214/1500], Training Loss: 8.092571862136504, Validation Loss: 21.307621002197266\n",
      "Epoch [1215/1500], Training Loss: 8.071208181954894, Validation Loss: 21.28243637084961\n",
      "Epoch [1216/1500], Training Loss: 8.049900514001184, Validation Loss: 21.257266998291016\n",
      "Epoch [1217/1500], Training Loss: 8.028640192169307, Validation Loss: 21.232067108154297\n",
      "Epoch [1218/1500], Training Loss: 8.00745133649378, Validation Loss: 21.207000732421875\n",
      "Epoch [1219/1500], Training Loss: 7.986336399237676, Validation Loss: 21.181840896606445\n",
      "Epoch [1220/1500], Training Loss: 7.96527882292548, Validation Loss: 21.156877517700195\n",
      "Epoch [1221/1500], Training Loss: 7.944304069301167, Validation Loss: 21.13198471069336\n",
      "Epoch [1222/1500], Training Loss: 7.923432301576752, Validation Loss: 21.107357025146484\n",
      "Epoch [1223/1500], Training Loss: 7.902610215306609, Validation Loss: 21.082632064819336\n",
      "Epoch [1224/1500], Training Loss: 7.881880353763735, Validation Loss: 21.05824089050293\n",
      "Epoch [1225/1500], Training Loss: 7.861201861806539, Validation Loss: 21.0335636138916\n",
      "Epoch [1226/1500], Training Loss: 7.840658746104188, Validation Loss: 21.009580612182617\n",
      "Epoch [1227/1500], Training Loss: 7.820175102183458, Validation Loss: 20.984752655029297\n",
      "Epoch [1228/1500], Training Loss: 7.799777612768, Validation Loss: 20.961463928222656\n",
      "Epoch [1229/1500], Training Loss: 7.7794063507791416, Validation Loss: 20.936511993408203\n",
      "Epoch [1230/1500], Training Loss: 7.75915374110018, Validation Loss: 20.913942337036133\n",
      "Epoch [1231/1500], Training Loss: 7.738928711983901, Validation Loss: 20.888288497924805\n",
      "Epoch [1232/1500], Training Loss: 7.7188747122365715, Validation Loss: 20.867090225219727\n",
      "Epoch [1233/1500], Training Loss: 7.69874717421392, Validation Loss: 20.84035873413086\n",
      "Epoch [1234/1500], Training Loss: 7.6789017726157915, Validation Loss: 20.820907592773438\n",
      "Epoch [1235/1500], Training Loss: 7.658897676213715, Validation Loss: 20.792762756347656\n",
      "Epoch [1236/1500], Training Loss: 7.639262212743117, Validation Loss: 20.775300979614258\n",
      "Epoch [1237/1500], Training Loss: 7.6193891055081835, Validation Loss: 20.74622344970703\n",
      "Epoch [1238/1500], Training Loss: 7.599866604194705, Validation Loss: 20.729684829711914\n",
      "Epoch [1239/1500], Training Loss: 7.5800925459931605, Validation Loss: 20.70014190673828\n",
      "Epoch [1240/1500], Training Loss: 7.560692829993514, Validation Loss: 20.684158325195312\n",
      "Epoch [1241/1500], Training Loss: 7.541012052722662, Validation Loss: 20.65457534790039\n",
      "Epoch [1242/1500], Training Loss: 7.521771253516409, Validation Loss: 20.638948440551758\n",
      "Epoch [1243/1500], Training Loss: 7.502273406974442, Validation Loss: 20.609575271606445\n",
      "Epoch [1244/1500], Training Loss: 7.483168893690584, Validation Loss: 20.5941219329834\n",
      "Epoch [1245/1500], Training Loss: 7.463821217723027, Validation Loss: 20.5648250579834\n",
      "Epoch [1246/1500], Training Loss: 7.444862540238936, Validation Loss: 20.549604415893555\n",
      "Epoch [1247/1500], Training Loss: 7.425624694776321, Validation Loss: 20.52033805847168\n",
      "Epoch [1248/1500], Training Loss: 7.406811216396549, Validation Loss: 20.50531005859375\n",
      "Epoch [1249/1500], Training Loss: 7.387686849455382, Validation Loss: 20.47636604309082\n",
      "Epoch [1250/1500], Training Loss: 7.3689919654554865, Validation Loss: 20.461271286010742\n",
      "Epoch [1251/1500], Training Loss: 7.34996716058488, Validation Loss: 20.432308197021484\n",
      "Epoch [1252/1500], Training Loss: 7.331392861717845, Validation Loss: 20.41773796081543\n",
      "Epoch [1253/1500], Training Loss: 7.31254384630444, Validation Loss: 20.38899803161621\n",
      "Epoch [1254/1500], Training Loss: 7.294115821938954, Validation Loss: 20.37469482421875\n",
      "Epoch [1255/1500], Training Loss: 7.27538094459263, Validation Loss: 20.34624671936035\n",
      "Epoch [1256/1500], Training Loss: 7.257058715662425, Validation Loss: 20.33199119567871\n",
      "Epoch [1257/1500], Training Loss: 7.238441474276362, Validation Loss: 20.303674697875977\n",
      "Epoch [1258/1500], Training Loss: 7.22027893434016, Validation Loss: 20.289859771728516\n",
      "Epoch [1259/1500], Training Loss: 7.201795678165422, Validation Loss: 20.261764526367188\n",
      "Epoch [1260/1500], Training Loss: 7.183741551295187, Validation Loss: 20.24803352355957\n",
      "Epoch [1261/1500], Training Loss: 7.165363931912966, Validation Loss: 20.220205307006836\n",
      "Epoch [1262/1500], Training Loss: 7.14741840621278, Validation Loss: 20.206615447998047\n",
      "Epoch [1263/1500], Training Loss: 7.129147945379862, Validation Loss: 20.179052352905273\n",
      "Epoch [1264/1500], Training Loss: 7.111320098502987, Validation Loss: 20.16555404663086\n",
      "Epoch [1265/1500], Training Loss: 7.093165199191852, Validation Loss: 20.13810157775879\n",
      "Epoch [1266/1500], Training Loss: 7.075485477838197, Validation Loss: 20.124897003173828\n",
      "Epoch [1267/1500], Training Loss: 7.0574531771772016, Validation Loss: 20.097881317138672\n",
      "Epoch [1268/1500], Training Loss: 7.039903820362995, Validation Loss: 20.085002899169922\n",
      "Epoch [1269/1500], Training Loss: 7.022209475176074, Validation Loss: 20.057941436767578\n",
      "Epoch [1270/1500], Training Loss: 7.005008656661326, Validation Loss: 20.045108795166016\n",
      "Epoch [1271/1500], Training Loss: 6.987445370753746, Validation Loss: 20.01856803894043\n",
      "Epoch [1272/1500], Training Loss: 6.970378477906402, Validation Loss: 20.005979537963867\n",
      "Epoch [1273/1500], Training Loss: 6.952927505984451, Validation Loss: 19.97954559326172\n",
      "Epoch [1274/1500], Training Loss: 6.935968755085376, Validation Loss: 19.967050552368164\n",
      "Epoch [1275/1500], Training Loss: 6.918647530285652, Validation Loss: 19.941057205200195\n",
      "Epoch [1276/1500], Training Loss: 6.901805564564603, Validation Loss: 19.928699493408203\n",
      "Epoch [1277/1500], Training Loss: 6.884597130477788, Validation Loss: 19.902820587158203\n",
      "Epoch [1278/1500], Training Loss: 6.8678634264964105, Validation Loss: 19.89049530029297\n",
      "Epoch [1279/1500], Training Loss: 6.850741362361076, Validation Loss: 19.864830017089844\n",
      "Epoch [1280/1500], Training Loss: 6.834135689454046, Validation Loss: 19.85284423828125\n",
      "Epoch [1281/1500], Training Loss: 6.817156023552405, Validation Loss: 19.827463150024414\n",
      "Epoch [1282/1500], Training Loss: 6.800694287844275, Validation Loss: 19.815649032592773\n",
      "Epoch [1283/1500], Training Loss: 6.783844446993051, Validation Loss: 19.790414810180664\n",
      "Epoch [1284/1500], Training Loss: 6.767461047063656, Validation Loss: 19.778701782226562\n",
      "Epoch [1285/1500], Training Loss: 6.750704039380512, Validation Loss: 19.753816604614258\n",
      "Epoch [1286/1500], Training Loss: 6.734418327173224, Validation Loss: 19.74251365661621\n",
      "Epoch [1287/1500], Training Loss: 6.717753533246254, Validation Loss: 19.7178897857666\n",
      "Epoch [1288/1500], Training Loss: 6.701591710460244, Validation Loss: 19.70638084411621\n",
      "Epoch [1289/1500], Training Loss: 6.68502575411968, Validation Loss: 19.682167053222656\n",
      "Epoch [1290/1500], Training Loss: 6.668936150042774, Validation Loss: 19.67099380493164\n",
      "Epoch [1291/1500], Training Loss: 6.652434764618564, Validation Loss: 19.646690368652344\n",
      "Epoch [1292/1500], Training Loss: 6.636451446170929, Validation Loss: 19.635787963867188\n",
      "Epoch [1293/1500], Training Loss: 6.62005319887131, Validation Loss: 19.611867904663086\n",
      "Epoch [1294/1500], Training Loss: 6.604203254561661, Validation Loss: 19.601036071777344\n",
      "Epoch [1295/1500], Training Loss: 6.5879359698458035, Validation Loss: 19.57746124267578\n",
      "Epoch [1296/1500], Training Loss: 6.572191832789775, Validation Loss: 19.566789627075195\n",
      "Epoch [1297/1500], Training Loss: 6.556046435264323, Validation Loss: 19.543481826782227\n",
      "Epoch [1298/1500], Training Loss: 6.54043265095277, Validation Loss: 19.532882690429688\n",
      "Epoch [1299/1500], Training Loss: 6.524429129633347, Validation Loss: 19.509756088256836\n",
      "Epoch [1300/1500], Training Loss: 6.508970828409102, Validation Loss: 19.49940299987793\n",
      "Epoch [1301/1500], Training Loss: 6.49315146750819, Validation Loss: 19.476743698120117\n",
      "Epoch [1302/1500], Training Loss: 6.4778369961912725, Validation Loss: 19.46637535095215\n",
      "Epoch [1303/1500], Training Loss: 6.462152198363613, Validation Loss: 19.4439754486084\n",
      "Epoch [1304/1500], Training Loss: 6.446959441413102, Validation Loss: 19.433753967285156\n",
      "Epoch [1305/1500], Training Loss: 6.431347771053888, Validation Loss: 19.411592483520508\n",
      "Epoch [1306/1500], Training Loss: 6.416249023061927, Validation Loss: 19.40144157409668\n",
      "Epoch [1307/1500], Training Loss: 6.400708425152112, Validation Loss: 19.379528045654297\n",
      "Epoch [1308/1500], Training Loss: 6.385703268079662, Validation Loss: 19.36945152282715\n",
      "Epoch [1309/1500], Training Loss: 6.370221674796963, Validation Loss: 19.34796142578125\n",
      "Epoch [1310/1500], Training Loss: 6.355288399328901, Validation Loss: 19.337936401367188\n",
      "Epoch [1311/1500], Training Loss: 6.339929913457954, Validation Loss: 19.316652297973633\n",
      "Epoch [1312/1500], Training Loss: 6.325114372957998, Validation Loss: 19.306921005249023\n",
      "Epoch [1313/1500], Training Loss: 6.309879373422791, Validation Loss: 19.285921096801758\n",
      "Epoch [1314/1500], Training Loss: 6.295181782523222, Validation Loss: 19.2763671875\n",
      "Epoch [1315/1500], Training Loss: 6.280058911189617, Validation Loss: 19.25553321838379\n",
      "Epoch [1316/1500], Training Loss: 6.2654542212695485, Validation Loss: 19.24604606628418\n",
      "Epoch [1317/1500], Training Loss: 6.2504170208347505, Validation Loss: 19.225601196289062\n",
      "Epoch [1318/1500], Training Loss: 6.235930966414962, Validation Loss: 19.216394424438477\n",
      "Epoch [1319/1500], Training Loss: 6.221033840671109, Validation Loss: 19.19622802734375\n",
      "Epoch [1320/1500], Training Loss: 6.206669867360401, Validation Loss: 19.187196731567383\n",
      "Epoch [1321/1500], Training Loss: 6.191899190745255, Validation Loss: 19.167451858520508\n",
      "Epoch [1322/1500], Training Loss: 6.17767122785708, Validation Loss: 19.158458709716797\n",
      "Epoch [1323/1500], Training Loss: 6.163050721566653, Validation Loss: 19.1389217376709\n",
      "Epoch [1324/1500], Training Loss: 6.148956778442921, Validation Loss: 19.13003158569336\n",
      "Epoch [1325/1500], Training Loss: 6.134433434787897, Validation Loss: 19.110769271850586\n",
      "Epoch [1326/1500], Training Loss: 6.120478639050964, Validation Loss: 19.10200309753418\n",
      "Epoch [1327/1500], Training Loss: 6.106101076429954, Validation Loss: 19.083110809326172\n",
      "Epoch [1328/1500], Training Loss: 6.092259593955689, Validation Loss: 19.074634552001953\n",
      "Epoch [1329/1500], Training Loss: 6.0779734949192035, Validation Loss: 19.05598258972168\n",
      "Epoch [1330/1500], Training Loss: 6.064237740324482, Validation Loss: 19.04750633239746\n",
      "Epoch [1331/1500], Training Loss: 6.050040109625242, Validation Loss: 19.028976440429688\n",
      "Epoch [1332/1500], Training Loss: 6.036370033549375, Validation Loss: 19.020551681518555\n",
      "Epoch [1333/1500], Training Loss: 6.022262488522086, Validation Loss: 19.00246810913086\n",
      "Epoch [1334/1500], Training Loss: 6.008688517141565, Validation Loss: 18.994108200073242\n",
      "Epoch [1335/1500], Training Loss: 5.994694681200895, Validation Loss: 18.976293563842773\n",
      "Epoch [1336/1500], Training Loss: 5.981208194201633, Validation Loss: 18.968050003051758\n",
      "Epoch [1337/1500], Training Loss: 5.96733049059916, Validation Loss: 18.950538635253906\n",
      "Epoch [1338/1500], Training Loss: 5.953935899512563, Validation Loss: 18.942485809326172\n",
      "Epoch [1339/1500], Training Loss: 5.9401540085776725, Validation Loss: 18.925241470336914\n",
      "Epoch [1340/1500], Training Loss: 5.926872966715624, Validation Loss: 18.916906356811523\n",
      "Epoch [1341/1500], Training Loss: 5.913165970856504, Validation Loss: 18.89996910095215\n",
      "Epoch [1342/1500], Training Loss: 5.899982922472547, Validation Loss: 18.89181137084961\n",
      "Epoch [1343/1500], Training Loss: 5.886400621260246, Validation Loss: 18.87513542175293\n",
      "Epoch [1344/1500], Training Loss: 5.873314503439788, Validation Loss: 18.867088317871094\n",
      "Epoch [1345/1500], Training Loss: 5.859803721399072, Validation Loss: 18.85048484802246\n",
      "Epoch [1346/1500], Training Loss: 5.846818818730006, Validation Loss: 18.842483520507812\n",
      "Epoch [1347/1500], Training Loss: 5.833433941853782, Validation Loss: 18.826162338256836\n",
      "Epoch [1348/1500], Training Loss: 5.82057032568517, Validation Loss: 18.818288803100586\n",
      "Epoch [1349/1500], Training Loss: 5.807293421125769, Validation Loss: 18.80211067199707\n",
      "Epoch [1350/1500], Training Loss: 5.794562286300745, Validation Loss: 18.794404983520508\n",
      "Epoch [1351/1500], Training Loss: 5.781400794810487, Validation Loss: 18.77843475341797\n",
      "Epoch [1352/1500], Training Loss: 5.768777069835336, Validation Loss: 18.77063751220703\n",
      "Epoch [1353/1500], Training Loss: 5.755760556073858, Validation Loss: 18.75493049621582\n",
      "Epoch [1354/1500], Training Loss: 5.743225300326973, Validation Loss: 18.747385025024414\n",
      "Epoch [1355/1500], Training Loss: 5.730296800492872, Validation Loss: 18.731914520263672\n",
      "Epoch [1356/1500], Training Loss: 5.7178680555750505, Validation Loss: 18.724205017089844\n",
      "Epoch [1357/1500], Training Loss: 5.705049476000413, Validation Loss: 18.70907211303711\n",
      "Epoch [1358/1500], Training Loss: 5.6927540171370685, Validation Loss: 18.70159339904785\n",
      "Epoch [1359/1500], Training Loss: 5.680091257741677, Validation Loss: 18.686677932739258\n",
      "Epoch [1360/1500], Training Loss: 5.667917823600572, Validation Loss: 18.679183959960938\n",
      "Epoch [1361/1500], Training Loss: 5.655328822986495, Validation Loss: 18.664588928222656\n",
      "Epoch [1362/1500], Training Loss: 5.643225752895372, Validation Loss: 18.657127380371094\n",
      "Epoch [1363/1500], Training Loss: 5.630707494299073, Validation Loss: 18.642709732055664\n",
      "Epoch [1364/1500], Training Loss: 5.618674934264137, Validation Loss: 18.635255813598633\n",
      "Epoch [1365/1500], Training Loss: 5.6062636862093695, Validation Loss: 18.621068954467773\n",
      "Epoch [1366/1500], Training Loss: 5.594331497793338, Validation Loss: 18.61358642578125\n",
      "Epoch [1367/1500], Training Loss: 5.582024434062666, Validation Loss: 18.59967041015625\n",
      "Epoch [1368/1500], Training Loss: 5.570207216034816, Validation Loss: 18.592391967773438\n",
      "Epoch [1369/1500], Training Loss: 5.55802310168994, Validation Loss: 18.578723907470703\n",
      "Epoch [1370/1500], Training Loss: 5.546296236578914, Validation Loss: 18.57136344909668\n",
      "Epoch [1371/1500], Training Loss: 5.534230395275281, Validation Loss: 18.557777404785156\n",
      "Epoch [1372/1500], Training Loss: 5.522573420960741, Validation Loss: 18.550514221191406\n",
      "Epoch [1373/1500], Training Loss: 5.510583068246262, Validation Loss: 18.537057876586914\n",
      "Epoch [1374/1500], Training Loss: 5.499029196508605, Validation Loss: 18.529565811157227\n",
      "Epoch [1375/1500], Training Loss: 5.48717847643952, Validation Loss: 18.51630973815918\n",
      "Epoch [1376/1500], Training Loss: 5.475751474380379, Validation Loss: 18.50891876220703\n",
      "Epoch [1377/1500], Training Loss: 5.464022340044399, Validation Loss: 18.495878219604492\n",
      "Epoch [1378/1500], Training Loss: 5.45270247095846, Validation Loss: 18.48860740661621\n",
      "Epoch [1379/1500], Training Loss: 5.441081228584052, Validation Loss: 18.47564125061035\n",
      "Epoch [1380/1500], Training Loss: 5.429873758899604, Validation Loss: 18.468320846557617\n",
      "Epoch [1381/1500], Training Loss: 5.418353401557468, Validation Loss: 18.455669403076172\n",
      "Epoch [1382/1500], Training Loss: 5.4072206185863525, Validation Loss: 18.448383331298828\n",
      "Epoch [1383/1500], Training Loss: 5.3957752697478725, Validation Loss: 18.435867309570312\n",
      "Epoch [1384/1500], Training Loss: 5.384750025855718, Validation Loss: 18.42845344543457\n",
      "Epoch [1385/1500], Training Loss: 5.373448472182348, Validation Loss: 18.416290283203125\n",
      "Epoch [1386/1500], Training Loss: 5.3625274402294805, Validation Loss: 18.408721923828125\n",
      "Epoch [1387/1500], Training Loss: 5.351336999333632, Validation Loss: 18.396459579467773\n",
      "Epoch [1388/1500], Training Loss: 5.340537770752142, Validation Loss: 18.389001846313477\n",
      "Epoch [1389/1500], Training Loss: 5.329476250649446, Validation Loss: 18.377098083496094\n",
      "Epoch [1390/1500], Training Loss: 5.318805210671179, Validation Loss: 18.36977195739746\n",
      "Epoch [1391/1500], Training Loss: 5.307851598626413, Validation Loss: 18.357946395874023\n",
      "Epoch [1392/1500], Training Loss: 5.297230220525183, Validation Loss: 18.350635528564453\n",
      "Epoch [1393/1500], Training Loss: 5.286360713785081, Validation Loss: 18.338972091674805\n",
      "Epoch [1394/1500], Training Loss: 5.275860602926976, Validation Loss: 18.3316650390625\n",
      "Epoch [1395/1500], Training Loss: 5.265089212337589, Validation Loss: 18.320215225219727\n",
      "Epoch [1396/1500], Training Loss: 5.254667932521203, Validation Loss: 18.31290054321289\n",
      "Epoch [1397/1500], Training Loss: 5.244011054157347, Validation Loss: 18.30160903930664\n",
      "Epoch [1398/1500], Training Loss: 5.233735997266321, Validation Loss: 18.29427719116211\n",
      "Epoch [1399/1500], Training Loss: 5.223235587511076, Validation Loss: 18.283008575439453\n",
      "Epoch [1400/1500], Training Loss: 5.213065229521423, Validation Loss: 18.275705337524414\n",
      "Epoch [1401/1500], Training Loss: 5.202678505086712, Validation Loss: 18.2645320892334\n",
      "Epoch [1402/1500], Training Loss: 5.192631186070652, Validation Loss: 18.25724220275879\n",
      "Epoch [1403/1500], Training Loss: 5.182337716473431, Validation Loss: 18.246217727661133\n",
      "Epoch [1404/1500], Training Loss: 5.172367827505539, Validation Loss: 18.23883819580078\n",
      "Epoch [1405/1500], Training Loss: 5.162204961343513, Validation Loss: 18.228212356567383\n",
      "Epoch [1406/1500], Training Loss: 5.152364953543395, Validation Loss: 18.220876693725586\n",
      "Epoch [1407/1500], Training Loss: 5.142307877387455, Validation Loss: 18.210073471069336\n",
      "Epoch [1408/1500], Training Loss: 5.132567827315732, Validation Loss: 18.20274543762207\n",
      "Epoch [1409/1500], Training Loss: 5.122621385031807, Validation Loss: 18.191946029663086\n",
      "Epoch [1410/1500], Training Loss: 5.112981498898031, Validation Loss: 18.184436798095703\n",
      "Epoch [1411/1500], Training Loss: 5.103137505324401, Validation Loss: 18.173954010009766\n",
      "Epoch [1412/1500], Training Loss: 5.0936292764434175, Validation Loss: 18.166574478149414\n",
      "Epoch [1413/1500], Training Loss: 5.083910330259575, Validation Loss: 18.15595817565918\n",
      "Epoch [1414/1500], Training Loss: 5.0745151780613, Validation Loss: 18.1488037109375\n",
      "Epoch [1415/1500], Training Loss: 5.0648824608577625, Validation Loss: 18.138147354125977\n",
      "Epoch [1416/1500], Training Loss: 5.055555787642901, Validation Loss: 18.130918502807617\n",
      "Epoch [1417/1500], Training Loss: 5.046009595252823, Validation Loss: 18.120338439941406\n",
      "Epoch [1418/1500], Training Loss: 5.036797177139535, Validation Loss: 18.113033294677734\n",
      "Epoch [1419/1500], Training Loss: 5.027353772088366, Validation Loss: 18.102506637573242\n",
      "Epoch [1420/1500], Training Loss: 5.018243152843942, Validation Loss: 18.095455169677734\n",
      "Epoch [1421/1500], Training Loss: 5.008910487569658, Validation Loss: 18.085115432739258\n",
      "Epoch [1422/1500], Training Loss: 4.999931162582962, Validation Loss: 18.077974319458008\n",
      "Epoch [1423/1500], Training Loss: 4.990710777740263, Validation Loss: 18.067543029785156\n",
      "Epoch [1424/1500], Training Loss: 4.981861518501791, Validation Loss: 18.06061553955078\n",
      "Epoch [1425/1500], Training Loss: 4.9727376922923705, Validation Loss: 18.050256729125977\n",
      "Epoch [1426/1500], Training Loss: 4.963940009869899, Validation Loss: 18.043251037597656\n",
      "Epoch [1427/1500], Training Loss: 4.954866265014258, Validation Loss: 18.032773971557617\n",
      "Epoch [1428/1500], Training Loss: 4.946148750206148, Validation Loss: 18.02580451965332\n",
      "Epoch [1429/1500], Training Loss: 4.937155846950122, Validation Loss: 18.015209197998047\n",
      "Epoch [1430/1500], Training Loss: 4.92853795866655, Validation Loss: 18.008419036865234\n",
      "Epoch [1431/1500], Training Loss: 4.919650871601256, Validation Loss: 17.99789047241211\n",
      "Epoch [1432/1500], Training Loss: 4.91113627183019, Validation Loss: 17.99103546142578\n",
      "Epoch [1433/1500], Training Loss: 4.902339724665718, Validation Loss: 17.9805850982666\n",
      "Epoch [1434/1500], Training Loss: 4.893957398250904, Validation Loss: 17.97382164001465\n",
      "Epoch [1435/1500], Training Loss: 4.885286887788477, Validation Loss: 17.963306427001953\n",
      "Epoch [1436/1500], Training Loss: 4.877028453750498, Validation Loss: 17.956592559814453\n",
      "Epoch [1437/1500], Training Loss: 4.868483741709927, Validation Loss: 17.946090698242188\n",
      "Epoch [1438/1500], Training Loss: 4.860305222040794, Validation Loss: 17.939308166503906\n",
      "Epoch [1439/1500], Training Loss: 4.851846032111506, Validation Loss: 17.92900848388672\n",
      "Epoch [1440/1500], Training Loss: 4.843763409059772, Validation Loss: 17.92211151123047\n",
      "Epoch [1441/1500], Training Loss: 4.835362294551482, Validation Loss: 17.911909103393555\n",
      "Epoch [1442/1500], Training Loss: 4.827360555206081, Validation Loss: 17.905038833618164\n",
      "Epoch [1443/1500], Training Loss: 4.819080627123238, Validation Loss: 17.89513397216797\n",
      "Epoch [1444/1500], Training Loss: 4.811187415135907, Validation Loss: 17.887948989868164\n",
      "Epoch [1445/1500], Training Loss: 4.803023490126706, Validation Loss: 17.87810516357422\n",
      "Epoch [1446/1500], Training Loss: 4.7951973194174125, Validation Loss: 17.87101173400879\n",
      "Epoch [1447/1500], Training Loss: 4.7871691993155805, Validation Loss: 17.861251831054688\n",
      "Epoch [1448/1500], Training Loss: 4.779430237148332, Validation Loss: 17.854061126708984\n",
      "Epoch [1449/1500], Training Loss: 4.771503295680836, Validation Loss: 17.844646453857422\n",
      "Epoch [1450/1500], Training Loss: 4.763871999998925, Validation Loss: 17.837474822998047\n",
      "Epoch [1451/1500], Training Loss: 4.756078697388514, Validation Loss: 17.82844352722168\n",
      "Epoch [1452/1500], Training Loss: 4.748515416157689, Validation Loss: 17.8209171295166\n",
      "Epoch [1453/1500], Training Loss: 4.740839993531253, Validation Loss: 17.812057495117188\n",
      "Epoch [1454/1500], Training Loss: 4.733383568175857, Validation Loss: 17.804574966430664\n",
      "Epoch [1455/1500], Training Loss: 4.725841106845832, Validation Loss: 17.79606819152832\n",
      "Epoch [1456/1500], Training Loss: 4.718441316796779, Validation Loss: 17.788433074951172\n",
      "Epoch [1457/1500], Training Loss: 4.711012858235544, Validation Loss: 17.780254364013672\n",
      "Epoch [1458/1500], Training Loss: 4.703670064785333, Validation Loss: 17.772388458251953\n",
      "Epoch [1459/1500], Training Loss: 4.696313518189674, Validation Loss: 17.764402389526367\n",
      "Epoch [1460/1500], Training Loss: 4.689008511402805, Validation Loss: 17.7565860748291\n",
      "Epoch [1461/1500], Training Loss: 4.681761996479836, Validation Loss: 17.748706817626953\n",
      "Epoch [1462/1500], Training Loss: 4.6745423160909345, Validation Loss: 17.740951538085938\n",
      "Epoch [1463/1500], Training Loss: 4.667375835692757, Validation Loss: 17.732975006103516\n",
      "Epoch [1464/1500], Training Loss: 4.660253352113987, Validation Loss: 17.72508430480957\n",
      "Epoch [1465/1500], Training Loss: 4.653158525732578, Validation Loss: 17.717082977294922\n",
      "Epoch [1466/1500], Training Loss: 4.646116993068838, Validation Loss: 17.709081649780273\n",
      "Epoch [1467/1500], Training Loss: 4.639103642773557, Validation Loss: 17.701173782348633\n",
      "Epoch [1468/1500], Training Loss: 4.6321289905811325, Validation Loss: 17.693078994750977\n",
      "Epoch [1469/1500], Training Loss: 4.625197386861724, Validation Loss: 17.68509292602539\n",
      "Epoch [1470/1500], Training Loss: 4.618287181433853, Validation Loss: 17.677125930786133\n",
      "Epoch [1471/1500], Training Loss: 4.611396835314207, Validation Loss: 17.669116973876953\n",
      "Epoch [1472/1500], Training Loss: 4.60455785255814, Validation Loss: 17.66095542907715\n",
      "Epoch [1473/1500], Training Loss: 4.597734867141796, Validation Loss: 17.65300941467285\n",
      "Epoch [1474/1500], Training Loss: 4.590939750135878, Validation Loss: 17.645071029663086\n",
      "Epoch [1475/1500], Training Loss: 4.584188675318203, Validation Loss: 17.637069702148438\n",
      "Epoch [1476/1500], Training Loss: 4.5774738012781, Validation Loss: 17.62901496887207\n",
      "Epoch [1477/1500], Training Loss: 4.5707770914160974, Validation Loss: 17.621068954467773\n",
      "Epoch [1478/1500], Training Loss: 4.564088813286073, Validation Loss: 17.61330223083496\n",
      "Epoch [1479/1500], Training Loss: 4.5574649108962895, Validation Loss: 17.605365753173828\n",
      "Epoch [1480/1500], Training Loss: 4.5508699260676675, Validation Loss: 17.597732543945312\n",
      "Epoch [1481/1500], Training Loss: 4.544308243115272, Validation Loss: 17.590129852294922\n",
      "Epoch [1482/1500], Training Loss: 4.537767032073166, Validation Loss: 17.582496643066406\n",
      "Epoch [1483/1500], Training Loss: 4.531268790121459, Validation Loss: 17.575183868408203\n",
      "Epoch [1484/1500], Training Loss: 4.524815385649891, Validation Loss: 17.567928314208984\n",
      "Epoch [1485/1500], Training Loss: 4.518392384529338, Validation Loss: 17.56083869934082\n",
      "Epoch [1486/1500], Training Loss: 4.51198630231488, Validation Loss: 17.554174423217773\n",
      "Epoch [1487/1500], Training Loss: 4.505603422098554, Validation Loss: 17.547664642333984\n",
      "Epoch [1488/1500], Training Loss: 4.499275644091302, Validation Loss: 17.541215896606445\n",
      "Epoch [1489/1500], Training Loss: 4.4929825898132, Validation Loss: 17.535221099853516\n",
      "Epoch [1490/1500], Training Loss: 4.486719615859219, Validation Loss: 17.529369354248047\n",
      "Epoch [1491/1500], Training Loss: 4.480482363526474, Validation Loss: 17.52366828918457\n",
      "Epoch [1492/1500], Training Loss: 4.474262297466465, Validation Loss: 17.51825714111328\n",
      "Epoch [1493/1500], Training Loss: 4.46811323169285, Validation Loss: 17.513254165649414\n",
      "Epoch [1494/1500], Training Loss: 4.461972800530864, Validation Loss: 17.508197784423828\n",
      "Epoch [1495/1500], Training Loss: 4.455885036799977, Validation Loss: 17.50334930419922\n",
      "Epoch [1496/1500], Training Loss: 4.449828742434838, Validation Loss: 17.498714447021484\n",
      "Epoch [1497/1500], Training Loss: 4.443777040062674, Validation Loss: 17.494152069091797\n",
      "Epoch [1498/1500], Training Loss: 4.437769587824717, Validation Loss: 17.489871978759766\n",
      "Epoch [1499/1500], Training Loss: 4.431776745875453, Validation Loss: 17.485492706298828\n",
      "Epoch [1500/1500], Training Loss: 4.425837704811646, Validation Loss: 17.481334686279297\n",
      "Final Test Loss: 19.464984893798828\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.0001]\n",
    "window_sizes = [5]\n",
    "\n",
    "num_epochs = 1500\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "       # Calculate test loss after training is complete\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Final Test Loss: {test_loss / len(x_test_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0005, window_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Training Loss: 26554.435056082148, Validation Loss: 25769.255859375\n",
      "Epoch [2/1500], Training Loss: 24113.030602345916, Validation Loss: 23648.970703125\n",
      "Epoch [3/1500], Training Loss: 22248.190985589346, Validation Loss: 21938.279296875\n",
      "Epoch [4/1500], Training Loss: 20633.63550542519, Validation Loss: 20363.251953125\n",
      "Epoch [5/1500], Training Loss: 19226.18263190864, Validation Loss: 19010.71875\n",
      "Epoch [6/1500], Training Loss: 17970.956462144102, Validation Loss: 17823.931640625\n",
      "Epoch [7/1500], Training Loss: 16842.529355600494, Validation Loss: 16732.078125\n",
      "Epoch [8/1500], Training Loss: 15824.234414015571, Validation Loss: 15738.044921875\n",
      "Epoch [9/1500], Training Loss: 14907.033740300398, Validation Loss: 14790.0\n",
      "Epoch [10/1500], Training Loss: 14076.092330914087, Validation Loss: 13944.2060546875\n",
      "Epoch [11/1500], Training Loss: 13315.717384574731, Validation Loss: 13203.388671875\n",
      "Epoch [12/1500], Training Loss: 12618.84352988923, Validation Loss: 12527.7255859375\n",
      "Epoch [13/1500], Training Loss: 11980.865693351403, Validation Loss: 11930.6455078125\n",
      "Epoch [14/1500], Training Loss: 11395.197188524773, Validation Loss: 11467.1943359375\n",
      "Epoch [15/1500], Training Loss: 10856.730932154484, Validation Loss: 10988.6142578125\n",
      "Epoch [16/1500], Training Loss: 10361.99742892361, Validation Loss: 10459.3330078125\n",
      "Epoch [17/1500], Training Loss: 9908.072218988387, Validation Loss: 9987.330078125\n",
      "Epoch [18/1500], Training Loss: 9492.863107837215, Validation Loss: 9608.3828125\n",
      "Epoch [19/1500], Training Loss: 9054.791707041672, Validation Loss: 9753.537109375\n",
      "Epoch [20/1500], Training Loss: 8694.088077086239, Validation Loss: 9355.7861328125\n",
      "Epoch [21/1500], Training Loss: 8297.797548141438, Validation Loss: 8723.98046875\n",
      "Epoch [22/1500], Training Loss: 8377.59204859906, Validation Loss: 8165.36767578125\n",
      "Epoch [23/1500], Training Loss: 7914.046068572399, Validation Loss: 7960.30859375\n",
      "Epoch [24/1500], Training Loss: 7542.741550181171, Validation Loss: 8035.6962890625\n",
      "Epoch [25/1500], Training Loss: 7242.494056130285, Validation Loss: 7344.20947265625\n",
      "Epoch [26/1500], Training Loss: 6906.306060928073, Validation Loss: 6966.29541015625\n",
      "Epoch [27/1500], Training Loss: 6597.188615766497, Validation Loss: 6657.275390625\n",
      "Epoch [28/1500], Training Loss: 6306.4193124594485, Validation Loss: 6374.13427734375\n",
      "Epoch [29/1500], Training Loss: 6033.108635869936, Validation Loss: 6081.02734375\n",
      "Epoch [30/1500], Training Loss: 5779.197255456371, Validation Loss: 5831.4599609375\n",
      "Epoch [31/1500], Training Loss: 5521.3965223316145, Validation Loss: 5614.45654296875\n",
      "Epoch [32/1500], Training Loss: 5265.712791630946, Validation Loss: 5449.39501953125\n",
      "Epoch [33/1500], Training Loss: 5014.461874880893, Validation Loss: 5224.4169921875\n",
      "Epoch [34/1500], Training Loss: 4783.238299696901, Validation Loss: 4967.7080078125\n",
      "Epoch [35/1500], Training Loss: 4570.838550245974, Validation Loss: 4724.8916015625\n",
      "Epoch [36/1500], Training Loss: 4373.581604950221, Validation Loss: 4517.005859375\n",
      "Epoch [37/1500], Training Loss: 4184.571993575329, Validation Loss: 4336.5302734375\n",
      "Epoch [38/1500], Training Loss: 4003.1255067136062, Validation Loss: 4161.5888671875\n",
      "Epoch [39/1500], Training Loss: 3828.120419225137, Validation Loss: 3984.511474609375\n",
      "Epoch [40/1500], Training Loss: 3658.1752295822357, Validation Loss: 3805.237060546875\n",
      "Epoch [41/1500], Training Loss: 3491.3774869098793, Validation Loss: 3624.43896484375\n",
      "Epoch [42/1500], Training Loss: 3285.5979825356576, Validation Loss: 3488.046142578125\n",
      "Epoch [43/1500], Training Loss: 3136.4679374643065, Validation Loss: 3327.804931640625\n",
      "Epoch [44/1500], Training Loss: 2997.133051601683, Validation Loss: 3180.475830078125\n",
      "Epoch [45/1500], Training Loss: 2865.7313434880352, Validation Loss: 3046.7802734375\n",
      "Epoch [46/1500], Training Loss: 2733.6260166328243, Validation Loss: 2971.19482421875\n",
      "Epoch [47/1500], Training Loss: 2615.3944212631027, Validation Loss: 2763.472900390625\n",
      "Epoch [48/1500], Training Loss: 2508.404508827718, Validation Loss: 2641.521484375\n",
      "Epoch [49/1500], Training Loss: 2408.011152598292, Validation Loss: 2549.65185546875\n",
      "Epoch [50/1500], Training Loss: 2314.107446755856, Validation Loss: 2469.439697265625\n",
      "Epoch [51/1500], Training Loss: 2224.7368981831055, Validation Loss: 2379.70068359375\n",
      "Epoch [52/1500], Training Loss: 2139.156827764477, Validation Loss: 2285.825439453125\n",
      "Epoch [53/1500], Training Loss: 2057.0500180273075, Validation Loss: 2197.25537109375\n",
      "Epoch [54/1500], Training Loss: 1978.4537479883284, Validation Loss: 2115.2275390625\n",
      "Epoch [55/1500], Training Loss: 1903.15874503903, Validation Loss: 2041.4468994140625\n",
      "Epoch [56/1500], Training Loss: 1831.0891230079255, Validation Loss: 1975.3695068359375\n",
      "Epoch [57/1500], Training Loss: 1762.4592771385062, Validation Loss: 1915.8436279296875\n",
      "Epoch [58/1500], Training Loss: 1697.1872638215089, Validation Loss: 1862.0516357421875\n",
      "Epoch [59/1500], Training Loss: 1634.768083821337, Validation Loss: 1811.80126953125\n",
      "Epoch [60/1500], Training Loss: 1575.350858394002, Validation Loss: 1754.836669921875\n",
      "Epoch [61/1500], Training Loss: 1518.7271443644074, Validation Loss: 1695.4876708984375\n",
      "Epoch [62/1500], Training Loss: 1465.1231426335476, Validation Loss: 1639.4412841796875\n",
      "Epoch [63/1500], Training Loss: 1414.523734535909, Validation Loss: 1587.26953125\n",
      "Epoch [64/1500], Training Loss: 1366.500155383509, Validation Loss: 1538.4083251953125\n",
      "Epoch [65/1500], Training Loss: 1320.0668508471178, Validation Loss: 1490.6295166015625\n",
      "Epoch [66/1500], Training Loss: 1275.3396378675518, Validation Loss: 1443.87255859375\n",
      "Epoch [67/1500], Training Loss: 1232.380187450922, Validation Loss: 1400.6385498046875\n",
      "Epoch [68/1500], Training Loss: 1191.3030710465841, Validation Loss: 1360.220458984375\n",
      "Epoch [69/1500], Training Loss: 1152.1379211431356, Validation Loss: 1324.2335205078125\n",
      "Epoch [70/1500], Training Loss: 1114.7848999994883, Validation Loss: 1289.814453125\n",
      "Epoch [71/1500], Training Loss: 1079.0248280465737, Validation Loss: 1252.1488037109375\n",
      "Epoch [72/1500], Training Loss: 1044.378761758699, Validation Loss: 1213.3900146484375\n",
      "Epoch [73/1500], Training Loss: 1010.7747830487799, Validation Loss: 1177.9827880859375\n",
      "Epoch [74/1500], Training Loss: 978.2089994116529, Validation Loss: 1144.4407958984375\n",
      "Epoch [75/1500], Training Loss: 946.5272198784592, Validation Loss: 1113.0953369140625\n",
      "Epoch [76/1500], Training Loss: 915.566598529195, Validation Loss: 1083.6607666015625\n",
      "Epoch [77/1500], Training Loss: 885.4977526067663, Validation Loss: 1056.18994140625\n",
      "Epoch [78/1500], Training Loss: 856.2279106509408, Validation Loss: 1021.654052734375\n",
      "Epoch [79/1500], Training Loss: 827.4642612622457, Validation Loss: 978.7351684570312\n",
      "Epoch [80/1500], Training Loss: 799.5325599087582, Validation Loss: 942.7779541015625\n",
      "Epoch [81/1500], Training Loss: 772.319709913263, Validation Loss: 911.51611328125\n",
      "Epoch [82/1500], Training Loss: 746.2727585463673, Validation Loss: 882.6239013671875\n",
      "Epoch [83/1500], Training Loss: 721.1139667177696, Validation Loss: 853.9685668945312\n",
      "Epoch [84/1500], Training Loss: 696.8569118905629, Validation Loss: 826.2048950195312\n",
      "Epoch [85/1500], Training Loss: 673.4582943783477, Validation Loss: 800.0111694335938\n",
      "Epoch [86/1500], Training Loss: 650.871363319694, Validation Loss: 775.4228515625\n",
      "Epoch [87/1500], Training Loss: 629.0284917134718, Validation Loss: 751.74755859375\n",
      "Epoch [88/1500], Training Loss: 607.8393696737941, Validation Loss: 729.0541381835938\n",
      "Epoch [89/1500], Training Loss: 587.266011780486, Validation Loss: 707.1812133789062\n",
      "Epoch [90/1500], Training Loss: 567.282941415744, Validation Loss: 685.5923461914062\n",
      "Epoch [91/1500], Training Loss: 547.8905479081166, Validation Loss: 664.4959716796875\n",
      "Epoch [92/1500], Training Loss: 529.0650989424059, Validation Loss: 644.0348510742188\n",
      "Epoch [93/1500], Training Loss: 510.80255816051084, Validation Loss: 624.2203369140625\n",
      "Epoch [94/1500], Training Loss: 493.0839201685493, Validation Loss: 605.0541381835938\n",
      "Epoch [95/1500], Training Loss: 475.8948859935555, Validation Loss: 586.5750732421875\n",
      "Epoch [96/1500], Training Loss: 459.21337352518054, Validation Loss: 568.80615234375\n",
      "Epoch [97/1500], Training Loss: 443.02388087063673, Validation Loss: 551.7130126953125\n",
      "Epoch [98/1500], Training Loss: 427.3052434114063, Validation Loss: 535.1254272460938\n",
      "Epoch [99/1500], Training Loss: 412.0200950894169, Validation Loss: 518.7656860351562\n",
      "Epoch [100/1500], Training Loss: 397.12919114952587, Validation Loss: 502.36669921875\n",
      "Epoch [101/1500], Training Loss: 382.6585763292366, Validation Loss: 486.2273864746094\n",
      "Epoch [102/1500], Training Loss: 368.74056669887864, Validation Loss: 471.2926025390625\n",
      "Epoch [103/1500], Training Loss: 355.3481035095389, Validation Loss: 457.7420349121094\n",
      "Epoch [104/1500], Training Loss: 342.4281241296178, Validation Loss: 444.91082763671875\n",
      "Epoch [105/1500], Training Loss: 329.95723395077897, Validation Loss: 432.39453125\n",
      "Epoch [106/1500], Training Loss: 317.92319588163053, Validation Loss: 420.07965087890625\n",
      "Epoch [107/1500], Training Loss: 306.3200441725806, Validation Loss: 407.95233154296875\n",
      "Epoch [108/1500], Training Loss: 295.13506464015745, Validation Loss: 395.9970703125\n",
      "Epoch [109/1500], Training Loss: 284.3561780083108, Validation Loss: 384.1963195800781\n",
      "Epoch [110/1500], Training Loss: 273.9692411690217, Validation Loss: 372.5155029296875\n",
      "Epoch [111/1500], Training Loss: 263.9574861133786, Validation Loss: 360.8999938964844\n",
      "Epoch [112/1500], Training Loss: 254.30162918009145, Validation Loss: 349.3055725097656\n",
      "Epoch [113/1500], Training Loss: 244.98427946086537, Validation Loss: 337.6917419433594\n",
      "Epoch [114/1500], Training Loss: 235.9913479739457, Validation Loss: 326.029296875\n",
      "Epoch [115/1500], Training Loss: 227.31648613888936, Validation Loss: 314.3405456542969\n",
      "Epoch [116/1500], Training Loss: 218.9500841019856, Validation Loss: 302.76629638671875\n",
      "Epoch [117/1500], Training Loss: 210.87322180918815, Validation Loss: 291.4384460449219\n",
      "Epoch [118/1500], Training Loss: 203.06886375386787, Validation Loss: 280.4181213378906\n",
      "Epoch [119/1500], Training Loss: 195.52465076554319, Validation Loss: 269.75811767578125\n",
      "Epoch [120/1500], Training Loss: 188.2515097440057, Validation Loss: 259.4727783203125\n",
      "Epoch [121/1500], Training Loss: 181.25017546972296, Validation Loss: 249.5348358154297\n",
      "Epoch [122/1500], Training Loss: 174.5099167822456, Validation Loss: 239.84939575195312\n",
      "Epoch [123/1500], Training Loss: 168.0211058624847, Validation Loss: 230.30564880371094\n",
      "Epoch [124/1500], Training Loss: 161.76648577745715, Validation Loss: 220.93496704101562\n",
      "Epoch [125/1500], Training Loss: 155.73770377395357, Validation Loss: 211.87155151367188\n",
      "Epoch [126/1500], Training Loss: 149.93540938735842, Validation Loss: 203.213134765625\n",
      "Epoch [127/1500], Training Loss: 144.3640332892764, Validation Loss: 195.05184936523438\n",
      "Epoch [128/1500], Training Loss: 139.0150364663, Validation Loss: 187.47097778320312\n",
      "Epoch [129/1500], Training Loss: 133.84470599324746, Validation Loss: 180.28497314453125\n",
      "Epoch [130/1500], Training Loss: 128.8006297143736, Validation Loss: 173.294921875\n",
      "Epoch [131/1500], Training Loss: 123.84340875867196, Validation Loss: 166.3621063232422\n",
      "Epoch [132/1500], Training Loss: 119.00444025086253, Validation Loss: 159.5597381591797\n",
      "Epoch [133/1500], Training Loss: 114.45198266079275, Validation Loss: 153.28384399414062\n",
      "Epoch [134/1500], Training Loss: 110.26517115328961, Validation Loss: 147.65524291992188\n",
      "Epoch [135/1500], Training Loss: 106.33751654973847, Validation Loss: 142.51766967773438\n",
      "Epoch [136/1500], Training Loss: 102.58211619604705, Validation Loss: 137.60826110839844\n",
      "Epoch [137/1500], Training Loss: 98.96933475889978, Validation Loss: 132.78318786621094\n",
      "Epoch [138/1500], Training Loss: 95.48897742619567, Validation Loss: 128.04019165039062\n",
      "Epoch [139/1500], Training Loss: 92.13565320993398, Validation Loss: 123.37821960449219\n",
      "Epoch [140/1500], Training Loss: 88.9064199496688, Validation Loss: 118.77110290527344\n",
      "Epoch [141/1500], Training Loss: 85.79479640072759, Validation Loss: 114.17980194091797\n",
      "Epoch [142/1500], Training Loss: 82.79511663181286, Validation Loss: 109.56192779541016\n",
      "Epoch [143/1500], Training Loss: 79.90172301320415, Validation Loss: 104.84814453125\n",
      "Epoch [144/1500], Training Loss: 77.10212163535635, Validation Loss: 100.17341613769531\n",
      "Epoch [145/1500], Training Loss: 74.38504449052917, Validation Loss: 95.76058197021484\n",
      "Epoch [146/1500], Training Loss: 71.74111520215445, Validation Loss: 91.52091217041016\n",
      "Epoch [147/1500], Training Loss: 69.17238310194163, Validation Loss: 87.7358627319336\n",
      "Epoch [148/1500], Training Loss: 66.69064047416717, Validation Loss: 84.16863250732422\n",
      "Epoch [149/1500], Training Loss: 64.31173900675161, Validation Loss: 81.06009674072266\n",
      "Epoch [150/1500], Training Loss: 62.038976553636076, Validation Loss: 78.16979217529297\n",
      "Epoch [151/1500], Training Loss: 59.87067738451552, Validation Loss: 75.51292419433594\n",
      "Epoch [152/1500], Training Loss: 57.801270134276706, Validation Loss: 72.97613525390625\n",
      "Epoch [153/1500], Training Loss: 55.828348181647776, Validation Loss: 70.59178161621094\n",
      "Epoch [154/1500], Training Loss: 53.9450419276957, Validation Loss: 68.3450698852539\n",
      "Epoch [155/1500], Training Loss: 52.146953081526846, Validation Loss: 66.23062133789062\n",
      "Epoch [156/1500], Training Loss: 50.42884838085404, Validation Loss: 64.23399353027344\n",
      "Epoch [157/1500], Training Loss: 48.786600881106075, Validation Loss: 62.337371826171875\n",
      "Epoch [158/1500], Training Loss: 47.21450365117757, Validation Loss: 60.52513122558594\n",
      "Epoch [159/1500], Training Loss: 45.70722492872618, Validation Loss: 58.78249740600586\n",
      "Epoch [160/1500], Training Loss: 44.26178913230932, Validation Loss: 57.1017951965332\n",
      "Epoch [161/1500], Training Loss: 42.87393285738358, Validation Loss: 55.4739875793457\n",
      "Epoch [162/1500], Training Loss: 41.540485292698605, Validation Loss: 53.895992279052734\n",
      "Epoch [163/1500], Training Loss: 40.25809483662859, Validation Loss: 52.36359786987305\n",
      "Epoch [164/1500], Training Loss: 39.02372946710093, Validation Loss: 50.87826919555664\n",
      "Epoch [165/1500], Training Loss: 37.83597972082956, Validation Loss: 49.437477111816406\n",
      "Epoch [166/1500], Training Loss: 36.69213821300882, Validation Loss: 48.03725814819336\n",
      "Epoch [167/1500], Training Loss: 35.59075157536375, Validation Loss: 46.677242279052734\n",
      "Epoch [168/1500], Training Loss: 34.531505568385164, Validation Loss: 45.35759353637695\n",
      "Epoch [169/1500], Training Loss: 33.51247072917448, Validation Loss: 44.07936096191406\n",
      "Epoch [170/1500], Training Loss: 32.53212542934696, Validation Loss: 42.84482192993164\n",
      "Epoch [171/1500], Training Loss: 31.590243375540844, Validation Loss: 41.65522003173828\n",
      "Epoch [172/1500], Training Loss: 30.684755648202575, Validation Loss: 40.51045227050781\n",
      "Epoch [173/1500], Training Loss: 29.81475820262599, Validation Loss: 39.41292190551758\n",
      "Epoch [174/1500], Training Loss: 28.979262445837985, Validation Loss: 38.3619499206543\n",
      "Epoch [175/1500], Training Loss: 28.17663786126702, Validation Loss: 37.35670852661133\n",
      "Epoch [176/1500], Training Loss: 27.405735223037983, Validation Loss: 36.39535140991211\n",
      "Epoch [177/1500], Training Loss: 26.66448905225775, Validation Loss: 35.4738655090332\n",
      "Epoch [178/1500], Training Loss: 25.9522186565074, Validation Loss: 34.58925247192383\n",
      "Epoch [179/1500], Training Loss: 25.26691238624093, Validation Loss: 33.73667907714844\n",
      "Epoch [180/1500], Training Loss: 24.606903118585063, Validation Loss: 32.9115104675293\n",
      "Epoch [181/1500], Training Loss: 23.97063121033713, Validation Loss: 32.109107971191406\n",
      "Epoch [182/1500], Training Loss: 23.35676218831511, Validation Loss: 31.32437515258789\n",
      "Epoch [183/1500], Training Loss: 22.763276679178773, Validation Loss: 30.551788330078125\n",
      "Epoch [184/1500], Training Loss: 22.189016650053812, Validation Loss: 29.788719177246094\n",
      "Epoch [185/1500], Training Loss: 21.633122445521803, Validation Loss: 29.032041549682617\n",
      "Epoch [186/1500], Training Loss: 21.094261737791783, Validation Loss: 28.28035545349121\n",
      "Epoch [187/1500], Training Loss: 20.571002813371543, Validation Loss: 27.533653259277344\n",
      "Epoch [188/1500], Training Loss: 20.063149941333815, Validation Loss: 26.7939453125\n",
      "Epoch [189/1500], Training Loss: 19.570375682407, Validation Loss: 26.064350128173828\n",
      "Epoch [190/1500], Training Loss: 19.092542140012814, Validation Loss: 25.34990882873535\n",
      "Epoch [191/1500], Training Loss: 18.629913876536126, Validation Loss: 24.655437469482422\n",
      "Epoch [192/1500], Training Loss: 18.182356926734446, Validation Loss: 23.984729766845703\n",
      "Epoch [193/1500], Training Loss: 17.750376425845108, Validation Loss: 23.339696884155273\n",
      "Epoch [194/1500], Training Loss: 17.333401720045384, Validation Loss: 22.720279693603516\n",
      "Epoch [195/1500], Training Loss: 16.93135232410043, Validation Loss: 22.128080368041992\n",
      "Epoch [196/1500], Training Loss: 16.54403076017733, Validation Loss: 21.562145233154297\n",
      "Epoch [197/1500], Training Loss: 16.171025393046953, Validation Loss: 21.02446937561035\n",
      "Epoch [198/1500], Training Loss: 15.81182679669272, Validation Loss: 20.51551055908203\n",
      "Epoch [199/1500], Training Loss: 15.466202625200062, Validation Loss: 20.038217544555664\n",
      "Epoch [200/1500], Training Loss: 15.133751936443689, Validation Loss: 19.59372901916504\n",
      "Epoch [201/1500], Training Loss: 14.813382893255145, Validation Loss: 19.18357276916504\n",
      "Epoch [202/1500], Training Loss: 14.50448795094847, Validation Loss: 18.80670738220215\n",
      "Epoch [203/1500], Training Loss: 14.206214394666404, Validation Loss: 18.460124969482422\n",
      "Epoch [204/1500], Training Loss: 13.917880041362977, Validation Loss: 18.1396484375\n",
      "Epoch [205/1500], Training Loss: 13.638711794965124, Validation Loss: 17.8409366607666\n",
      "Epoch [206/1500], Training Loss: 13.368121255147857, Validation Loss: 17.560367584228516\n",
      "Epoch [207/1500], Training Loss: 13.105845787415664, Validation Loss: 17.294097900390625\n",
      "Epoch [208/1500], Training Loss: 12.85158513076369, Validation Loss: 17.04022979736328\n",
      "Epoch [209/1500], Training Loss: 12.604628381834647, Validation Loss: 16.79656410217285\n",
      "Epoch [210/1500], Training Loss: 12.364252063665454, Validation Loss: 16.561595916748047\n",
      "Epoch [211/1500], Training Loss: 12.130447409657412, Validation Loss: 16.334569931030273\n",
      "Epoch [212/1500], Training Loss: 11.90277794552328, Validation Loss: 16.11486053466797\n",
      "Epoch [213/1500], Training Loss: 11.681022404613135, Validation Loss: 15.901134490966797\n",
      "Epoch [214/1500], Training Loss: 11.465123282579631, Validation Loss: 15.693832397460938\n",
      "Epoch [215/1500], Training Loss: 11.254867262517163, Validation Loss: 15.492046356201172\n",
      "Epoch [216/1500], Training Loss: 11.049692592821511, Validation Loss: 15.294841766357422\n",
      "Epoch [217/1500], Training Loss: 10.849682900218614, Validation Loss: 15.102018356323242\n",
      "Epoch [218/1500], Training Loss: 10.654577686255784, Validation Loss: 14.9133939743042\n",
      "Epoch [219/1500], Training Loss: 10.464044531418182, Validation Loss: 14.728607177734375\n",
      "Epoch [220/1500], Training Loss: 10.277953725435328, Validation Loss: 14.546953201293945\n",
      "Epoch [221/1500], Training Loss: 10.096305016937388, Validation Loss: 14.368974685668945\n",
      "Epoch [222/1500], Training Loss: 9.918888376947175, Validation Loss: 14.193881034851074\n",
      "Epoch [223/1500], Training Loss: 9.745334077796217, Validation Loss: 14.02134895324707\n",
      "Epoch [224/1500], Training Loss: 9.575721669725683, Validation Loss: 13.850908279418945\n",
      "Epoch [225/1500], Training Loss: 9.409870604640057, Validation Loss: 13.682409286499023\n",
      "Epoch [226/1500], Training Loss: 9.247627193879076, Validation Loss: 13.516287803649902\n",
      "Epoch [227/1500], Training Loss: 9.089075587335557, Validation Loss: 13.351588249206543\n",
      "Epoch [228/1500], Training Loss: 8.934028602921673, Validation Loss: 13.188749313354492\n",
      "Epoch [229/1500], Training Loss: 8.78226828967162, Validation Loss: 13.02762508392334\n",
      "Epoch [230/1500], Training Loss: 8.633691042920226, Validation Loss: 12.867566108703613\n",
      "Epoch [231/1500], Training Loss: 8.488314763632191, Validation Loss: 12.708951950073242\n",
      "Epoch [232/1500], Training Loss: 8.3458820842731, Validation Loss: 12.551726341247559\n",
      "Epoch [233/1500], Training Loss: 8.206179413578012, Validation Loss: 12.395709991455078\n",
      "Epoch [234/1500], Training Loss: 8.069305123014, Validation Loss: 12.240808486938477\n",
      "Epoch [235/1500], Training Loss: 7.935262288319222, Validation Loss: 12.088000297546387\n",
      "Epoch [236/1500], Training Loss: 7.803823154643174, Validation Loss: 11.936793327331543\n",
      "Epoch [237/1500], Training Loss: 7.675220265959107, Validation Loss: 11.78767204284668\n",
      "Epoch [238/1500], Training Loss: 7.548929032369189, Validation Loss: 11.64047622680664\n",
      "Epoch [239/1500], Training Loss: 7.424947060345077, Validation Loss: 11.494745254516602\n",
      "Epoch [240/1500], Training Loss: 7.303281680137468, Validation Loss: 11.351662635803223\n",
      "Epoch [241/1500], Training Loss: 7.1841517791079985, Validation Loss: 11.211276054382324\n",
      "Epoch [242/1500], Training Loss: 7.067550717488955, Validation Loss: 11.073797225952148\n",
      "Epoch [243/1500], Training Loss: 6.953272913640128, Validation Loss: 10.938911437988281\n",
      "Epoch [244/1500], Training Loss: 6.841361546735993, Validation Loss: 10.806578636169434\n",
      "Epoch [245/1500], Training Loss: 6.731683591014647, Validation Loss: 10.677224159240723\n",
      "Epoch [246/1500], Training Loss: 6.624255482205359, Validation Loss: 10.550764083862305\n",
      "Epoch [247/1500], Training Loss: 6.519033209173498, Validation Loss: 10.427360534667969\n",
      "Epoch [248/1500], Training Loss: 6.4159696761414535, Validation Loss: 10.307281494140625\n",
      "Epoch [249/1500], Training Loss: 6.314943060559837, Validation Loss: 10.189615249633789\n",
      "Epoch [250/1500], Training Loss: 6.2160761706635626, Validation Loss: 10.075003623962402\n",
      "Epoch [251/1500], Training Loss: 6.1193374188631395, Validation Loss: 9.963348388671875\n",
      "Epoch [252/1500], Training Loss: 6.02470483032608, Validation Loss: 9.85415267944336\n",
      "Epoch [253/1500], Training Loss: 5.932318606153393, Validation Loss: 9.747618675231934\n",
      "Epoch [254/1500], Training Loss: 5.8420869616373405, Validation Loss: 9.643150329589844\n",
      "Epoch [255/1500], Training Loss: 5.753874798727618, Validation Loss: 9.540926933288574\n",
      "Epoch [256/1500], Training Loss: 5.667709409052461, Validation Loss: 9.440718650817871\n",
      "Epoch [257/1500], Training Loss: 5.583666814703754, Validation Loss: 9.342057228088379\n",
      "Epoch [258/1500], Training Loss: 5.501444755075688, Validation Loss: 9.245084762573242\n",
      "Epoch [259/1500], Training Loss: 5.421120167413029, Validation Loss: 9.149585723876953\n",
      "Epoch [260/1500], Training Loss: 5.342584655928014, Validation Loss: 9.055432319641113\n",
      "Epoch [261/1500], Training Loss: 5.265738049009165, Validation Loss: 8.96289348602295\n",
      "Epoch [262/1500], Training Loss: 5.190676535634846, Validation Loss: 8.871941566467285\n",
      "Epoch [263/1500], Training Loss: 5.1172308967480715, Validation Loss: 8.782365798950195\n",
      "Epoch [264/1500], Training Loss: 5.045365091737155, Validation Loss: 8.694766998291016\n",
      "Epoch [265/1500], Training Loss: 4.975065856778062, Validation Loss: 8.608612060546875\n",
      "Epoch [266/1500], Training Loss: 4.906138221587638, Validation Loss: 8.524421691894531\n",
      "Epoch [267/1500], Training Loss: 4.838713641417014, Validation Loss: 8.442057609558105\n",
      "Epoch [268/1500], Training Loss: 4.772832863374006, Validation Loss: 8.36251449584961\n",
      "Epoch [269/1500], Training Loss: 4.708385178939457, Validation Loss: 8.285709381103516\n",
      "Epoch [270/1500], Training Loss: 4.645310718927051, Validation Loss: 8.212624549865723\n",
      "Epoch [271/1500], Training Loss: 4.58354616057205, Validation Loss: 8.14289665222168\n",
      "Epoch [272/1500], Training Loss: 4.5231214420243555, Validation Loss: 8.077902793884277\n",
      "Epoch [273/1500], Training Loss: 4.463924186929483, Validation Loss: 8.01882553100586\n",
      "Epoch [274/1500], Training Loss: 4.406010674662125, Validation Loss: 7.964096546173096\n",
      "Epoch [275/1500], Training Loss: 4.349376386998538, Validation Loss: 7.914572238922119\n",
      "Epoch [276/1500], Training Loss: 4.293915758930227, Validation Loss: 7.866837024688721\n",
      "Epoch [277/1500], Training Loss: 4.239559187697776, Validation Loss: 7.817585468292236\n",
      "Epoch [278/1500], Training Loss: 4.186470156401413, Validation Loss: 7.761697769165039\n",
      "Epoch [279/1500], Training Loss: 4.134462906734727, Validation Loss: 7.698681831359863\n",
      "Epoch [280/1500], Training Loss: 4.08353034115056, Validation Loss: 7.628694534301758\n",
      "Epoch [281/1500], Training Loss: 4.033482913717577, Validation Loss: 7.557453155517578\n",
      "Epoch [282/1500], Training Loss: 3.984338567910822, Validation Loss: 7.490510940551758\n",
      "Epoch [283/1500], Training Loss: 3.936162177807779, Validation Loss: 7.429937839508057\n",
      "Epoch [284/1500], Training Loss: 3.888965695452284, Validation Loss: 7.3788161277771\n",
      "Epoch [285/1500], Training Loss: 3.8427837912141003, Validation Loss: 7.33491849899292\n",
      "Epoch [286/1500], Training Loss: 3.79747420357732, Validation Loss: 7.297552108764648\n",
      "Epoch [287/1500], Training Loss: 3.7530763382783157, Validation Loss: 7.265288352966309\n",
      "Epoch [288/1500], Training Loss: 3.7095146558897363, Validation Loss: 7.237778186798096\n",
      "Epoch [289/1500], Training Loss: 3.666920227931235, Validation Loss: 7.212052345275879\n",
      "Epoch [290/1500], Training Loss: 3.6251859424802064, Validation Loss: 7.188801288604736\n",
      "Epoch [291/1500], Training Loss: 3.5843562281234482, Validation Loss: 7.166182994842529\n",
      "Epoch [292/1500], Training Loss: 3.5443279985154144, Validation Loss: 7.145063400268555\n",
      "Epoch [293/1500], Training Loss: 3.505177863368764, Validation Loss: 7.124381065368652\n",
      "Epoch [294/1500], Training Loss: 3.466863065346218, Validation Loss: 7.103669166564941\n",
      "Epoch [295/1500], Training Loss: 3.429261268683825, Validation Loss: 7.083622932434082\n",
      "Epoch [296/1500], Training Loss: 3.392542822070741, Validation Loss: 7.063279151916504\n",
      "Epoch [297/1500], Training Loss: 3.356681576610326, Validation Loss: 7.043198108673096\n",
      "Epoch [298/1500], Training Loss: 3.3215760077615055, Validation Loss: 7.023536682128906\n",
      "Epoch [299/1500], Training Loss: 3.2872600127168443, Validation Loss: 7.004123210906982\n",
      "Epoch [300/1500], Training Loss: 3.2536893413051655, Validation Loss: 6.984974384307861\n",
      "Epoch [301/1500], Training Loss: 3.2208797950469816, Validation Loss: 6.966294288635254\n",
      "Epoch [302/1500], Training Loss: 3.1887583886812885, Validation Loss: 6.947769641876221\n",
      "Epoch [303/1500], Training Loss: 3.1572981391173536, Validation Loss: 6.9298601150512695\n",
      "Epoch [304/1500], Training Loss: 3.1265144057281264, Validation Loss: 6.911784648895264\n",
      "Epoch [305/1500], Training Loss: 3.096335631452485, Validation Loss: 6.894197940826416\n",
      "Epoch [306/1500], Training Loss: 3.0668319387231078, Validation Loss: 6.877049446105957\n",
      "Epoch [307/1500], Training Loss: 3.0379922995268767, Validation Loss: 6.860324382781982\n",
      "Epoch [308/1500], Training Loss: 3.009840953153949, Validation Loss: 6.844182014465332\n",
      "Epoch [309/1500], Training Loss: 2.9822985197699237, Validation Loss: 6.827561855316162\n",
      "Epoch [310/1500], Training Loss: 2.9554069297538135, Validation Loss: 6.812130928039551\n",
      "Epoch [311/1500], Training Loss: 2.9291006017608523, Validation Loss: 6.79592227935791\n",
      "Epoch [312/1500], Training Loss: 2.903325407579932, Validation Loss: 6.780120372772217\n",
      "Epoch [313/1500], Training Loss: 2.8781717018061532, Validation Loss: 6.764613628387451\n",
      "Epoch [314/1500], Training Loss: 2.853616177611065, Validation Loss: 6.7493205070495605\n",
      "Epoch [315/1500], Training Loss: 2.829632234939674, Validation Loss: 6.7343926429748535\n",
      "Epoch [316/1500], Training Loss: 2.8062961937112485, Validation Loss: 6.719128608703613\n",
      "Epoch [317/1500], Training Loss: 2.783539360543764, Validation Loss: 6.703948020935059\n",
      "Epoch [318/1500], Training Loss: 2.7613540456010517, Validation Loss: 6.6892194747924805\n",
      "Epoch [319/1500], Training Loss: 2.7396412558122325, Validation Loss: 6.674002170562744\n",
      "Epoch [320/1500], Training Loss: 2.7184169740386106, Validation Loss: 6.65891695022583\n",
      "Epoch [321/1500], Training Loss: 2.697703753064343, Validation Loss: 6.64306116104126\n",
      "Epoch [322/1500], Training Loss: 2.6775149976671386, Validation Loss: 6.627362251281738\n",
      "Epoch [323/1500], Training Loss: 2.6578635695819717, Validation Loss: 6.6112470626831055\n",
      "Epoch [324/1500], Training Loss: 2.6386585006412653, Validation Loss: 6.594915866851807\n",
      "Epoch [325/1500], Training Loss: 2.6199629705078356, Validation Loss: 6.578425884246826\n",
      "Epoch [326/1500], Training Loss: 2.6016380267526604, Validation Loss: 6.561074733734131\n",
      "Epoch [327/1500], Training Loss: 2.583735491350965, Validation Loss: 6.543531894683838\n",
      "Epoch [328/1500], Training Loss: 2.5663321580020235, Validation Loss: 6.52597188949585\n",
      "Epoch [329/1500], Training Loss: 2.5492731336243986, Validation Loss: 6.507392883300781\n",
      "Epoch [330/1500], Training Loss: 2.532602415798097, Validation Loss: 6.489457130432129\n",
      "Epoch [331/1500], Training Loss: 2.516338308782952, Validation Loss: 6.473637104034424\n",
      "Epoch [332/1500], Training Loss: 2.50047337117717, Validation Loss: 6.45602560043335\n",
      "Epoch [333/1500], Training Loss: 2.484896509754999, Validation Loss: 6.433186054229736\n",
      "Epoch [334/1500], Training Loss: 2.46962049099892, Validation Loss: 6.414562702178955\n",
      "Epoch [335/1500], Training Loss: 2.4548012049133843, Validation Loss: 6.414952278137207\n",
      "Epoch [336/1500], Training Loss: 2.440407437388911, Validation Loss: 6.415160655975342\n",
      "Epoch [337/1500], Training Loss: 2.4259190617915154, Validation Loss: 6.334204196929932\n",
      "Epoch [338/1500], Training Loss: 2.4114806488775575, Validation Loss: 6.265420436859131\n",
      "Epoch [339/1500], Training Loss: 2.3979173379928973, Validation Loss: 6.349376201629639\n",
      "Epoch [340/1500], Training Loss: 2.3857986066656425, Validation Loss: 6.471783638000488\n",
      "Epoch [341/1500], Training Loss: 2.3722794787107206, Validation Loss: 6.263611316680908\n",
      "Epoch [342/1500], Training Loss: 2.3582220246902135, Validation Loss: 6.056758403778076\n",
      "Epoch [343/1500], Training Loss: 2.3450013472437203, Validation Loss: 6.201587200164795\n",
      "Epoch [344/1500], Training Loss: 2.3343847232642245, Validation Loss: 6.401165962219238\n",
      "Epoch [345/1500], Training Loss: 2.322565245178106, Validation Loss: 6.27101993560791\n",
      "Epoch [346/1500], Training Loss: 2.309716205247679, Validation Loss: 5.994960784912109\n",
      "Epoch [347/1500], Training Loss: 2.29678066921914, Validation Loss: 6.113150119781494\n",
      "Epoch [348/1500], Training Loss: 2.286753126029626, Validation Loss: 6.267345905303955\n",
      "Epoch [349/1500], Training Loss: 2.2765322042704415, Validation Loss: 6.335951328277588\n",
      "Epoch [350/1500], Training Loss: 2.2651705233732375, Validation Loss: 5.990564823150635\n",
      "Epoch [351/1500], Training Loss: 2.2528597785841167, Validation Loss: 6.032057762145996\n",
      "Epoch [352/1500], Training Loss: 2.243141661198962, Validation Loss: 6.174107074737549\n",
      "Epoch [353/1500], Training Loss: 2.233977462498249, Validation Loss: 6.286559581756592\n",
      "Epoch [354/1500], Training Loss: 2.223712900300476, Validation Loss: 6.053603649139404\n",
      "Epoch [355/1500], Training Loss: 2.212924226462474, Validation Loss: 5.958019733428955\n",
      "Epoch [356/1500], Training Loss: 2.2029797525193384, Validation Loss: 6.111777305603027\n",
      "Epoch [357/1500], Training Loss: 2.1948973752698597, Validation Loss: 6.245959758758545\n",
      "Epoch [358/1500], Training Loss: 2.1858276408967425, Validation Loss: 6.102484703063965\n",
      "Epoch [359/1500], Training Loss: 2.176247698851458, Validation Loss: 5.930239677429199\n",
      "Epoch [360/1500], Training Loss: 2.1665340540581566, Validation Loss: 6.077328205108643\n",
      "Epoch [361/1500], Training Loss: 2.1592856324298295, Validation Loss: 6.193884372711182\n",
      "Epoch [362/1500], Training Loss: 2.150936311808364, Validation Loss: 6.032034873962402\n",
      "Epoch [363/1500], Training Loss: 2.141973155267659, Validation Loss: 5.957597732543945\n",
      "Epoch [364/1500], Training Loss: 2.133720821848324, Validation Loss: 6.102678298950195\n",
      "Epoch [365/1500], Training Loss: 2.126931092583997, Validation Loss: 6.19336462020874\n",
      "Epoch [366/1500], Training Loss: 2.1192187158809124, Validation Loss: 5.984277725219727\n",
      "Epoch [367/1500], Training Loss: 2.110879014440959, Validation Loss: 5.9949631690979\n",
      "Epoch [368/1500], Training Loss: 2.103825510826361, Validation Loss: 6.14142370223999\n",
      "Early stopping at epoch 368\n",
      "Final Test Loss: 6.519478797912598\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.0005]\n",
    "window_sizes = [5]\n",
    "\n",
    "num_epochs = 1500\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "       # Calculate test loss after training is complete\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Final Test Loss: {test_loss / len(x_test_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: input_size=7, hidden_size=4, num_layers=1, learning_rate=0.0003, window_size=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VisualStudioCode\\Repositories\\Projectarbeit-Dow-Jones-Index\\Projectarbeit-Dow-Jones-Index\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Training Loss: 27787.00012770133, Validation Loss: 27317.6796875\n",
      "Epoch [2/1500], Training Loss: 26060.76742158675, Validation Loss: 25744.07421875\n",
      "Epoch [3/1500], Training Loss: 24657.189663854282, Validation Loss: 24423.05859375\n",
      "Epoch [4/1500], Training Loss: 23502.63231663645, Validation Loss: 23337.021484375\n",
      "Epoch [5/1500], Training Loss: 22523.0760909937, Validation Loss: 22371.47265625\n",
      "Epoch [6/1500], Training Loss: 21527.508691539293, Validation Loss: 21407.619140625\n",
      "Epoch [7/1500], Training Loss: 20594.548296384204, Validation Loss: 20486.337890625\n",
      "Epoch [8/1500], Training Loss: 19730.691050508074, Validation Loss: 19634.634765625\n",
      "Epoch [9/1500], Training Loss: 18928.973166281237, Validation Loss: 18843.11328125\n",
      "Epoch [10/1500], Training Loss: 18180.33244681142, Validation Loss: 18105.09375\n",
      "Epoch [11/1500], Training Loss: 17478.64647459988, Validation Loss: 17415.5078125\n",
      "Epoch [12/1500], Training Loss: 16818.69883499127, Validation Loss: 16770.716796875\n",
      "Epoch [13/1500], Training Loss: 16197.996777534105, Validation Loss: 16164.56640625\n",
      "Epoch [14/1500], Training Loss: 15614.964771887991, Validation Loss: 15585.21875\n",
      "Epoch [15/1500], Training Loss: 15066.312907974028, Validation Loss: 15031.6865234375\n",
      "Epoch [16/1500], Training Loss: 14548.667429552377, Validation Loss: 14514.4443359375\n",
      "Epoch [17/1500], Training Loss: 14059.645450952372, Validation Loss: 14051.919921875\n",
      "Epoch [18/1500], Training Loss: 13594.905589748128, Validation Loss: 13605.765625\n",
      "Epoch [19/1500], Training Loss: 13153.920593077695, Validation Loss: 13168.9033203125\n",
      "Epoch [20/1500], Training Loss: 12735.617257405025, Validation Loss: 12751.306640625\n",
      "Epoch [21/1500], Training Loss: 12338.8767630311, Validation Loss: 12354.8017578125\n",
      "Epoch [22/1500], Training Loss: 11962.294066797127, Validation Loss: 11979.7548828125\n",
      "Epoch [23/1500], Training Loss: 11604.447612639386, Validation Loss: 11625.525390625\n",
      "Epoch [24/1500], Training Loss: 11264.133399641763, Validation Loss: 11291.20703125\n",
      "Epoch [25/1500], Training Loss: 10940.470697584231, Validation Loss: 10975.7646484375\n",
      "Epoch [26/1500], Training Loss: 10632.889776778102, Validation Loss: 10677.927734375\n",
      "Epoch [27/1500], Training Loss: 10340.913185624147, Validation Loss: 10397.3486328125\n",
      "Epoch [28/1500], Training Loss: 10064.092881012048, Validation Loss: 10135.0029296875\n",
      "Epoch [29/1500], Training Loss: 9801.889551203505, Validation Loss: 9893.708984375\n",
      "Epoch [30/1500], Training Loss: 9553.673967532195, Validation Loss: 9691.607421875\n",
      "Epoch [31/1500], Training Loss: 9312.467648124542, Validation Loss: 9779.2666015625\n",
      "Epoch [32/1500], Training Loss: 9073.863685085715, Validation Loss: 9475.73828125\n",
      "Epoch [33/1500], Training Loss: 8849.16932049576, Validation Loss: 9164.74609375\n",
      "Epoch [34/1500], Training Loss: 8589.107841941957, Validation Loss: 9029.0546875\n",
      "Epoch [35/1500], Training Loss: 8348.754733431777, Validation Loss: 8864.671875\n",
      "Epoch [36/1500], Training Loss: 8117.344028808252, Validation Loss: 8578.4580078125\n",
      "Epoch [37/1500], Training Loss: 7894.794756512204, Validation Loss: 8274.296875\n",
      "Epoch [38/1500], Training Loss: 7683.340059635369, Validation Loss: 7987.01513671875\n",
      "Epoch [39/1500], Training Loss: 7482.981707859004, Validation Loss: 7722.60791015625\n",
      "Epoch [40/1500], Training Loss: 7290.630465544371, Validation Loss: 7482.4853515625\n",
      "Epoch [41/1500], Training Loss: 7104.132721169367, Validation Loss: 7264.08349609375\n",
      "Epoch [42/1500], Training Loss: 6922.667423905902, Validation Loss: 7064.89501953125\n",
      "Epoch [43/1500], Training Loss: 6746.219106243936, Validation Loss: 6883.81201171875\n",
      "Epoch [44/1500], Training Loss: 6574.697168722953, Validation Loss: 6715.498046875\n",
      "Epoch [45/1500], Training Loss: 6407.70507868481, Validation Loss: 6554.6708984375\n",
      "Epoch [46/1500], Training Loss: 6244.7379109528865, Validation Loss: 6395.65673828125\n",
      "Epoch [47/1500], Training Loss: 6085.502378643878, Validation Loss: 6229.25634765625\n",
      "Epoch [48/1500], Training Loss: 5930.717794450132, Validation Loss: 6063.0849609375\n",
      "Epoch [49/1500], Training Loss: 5780.528938714536, Validation Loss: 5910.29345703125\n",
      "Epoch [50/1500], Training Loss: 5634.692235026093, Validation Loss: 5765.6455078125\n",
      "Epoch [51/1500], Training Loss: 5493.131198598155, Validation Loss: 5625.57177734375\n",
      "Epoch [52/1500], Training Loss: 5355.845341362778, Validation Loss: 5489.65185546875\n",
      "Epoch [53/1500], Training Loss: 5222.8186362657325, Validation Loss: 5358.18505859375\n",
      "Epoch [54/1500], Training Loss: 5093.99243946049, Validation Loss: 5231.21044921875\n",
      "Epoch [55/1500], Training Loss: 4969.133548539709, Validation Loss: 5108.29736328125\n",
      "Epoch [56/1500], Training Loss: 4847.832691322062, Validation Loss: 4988.3984375\n",
      "Epoch [57/1500], Training Loss: 4729.615361564027, Validation Loss: 4870.1708984375\n",
      "Epoch [58/1500], Training Loss: 4613.994434675548, Validation Loss: 4752.87109375\n",
      "Epoch [59/1500], Training Loss: 4500.671854751497, Validation Loss: 4636.5341796875\n",
      "Epoch [60/1500], Training Loss: 4389.491259762442, Validation Loss: 4521.62646484375\n",
      "Epoch [61/1500], Training Loss: 4280.370846837402, Validation Loss: 4408.6650390625\n",
      "Epoch [62/1500], Training Loss: 4173.288843586268, Validation Loss: 4298.03857421875\n",
      "Epoch [63/1500], Training Loss: 4068.2302307693967, Validation Loss: 4190.12109375\n",
      "Epoch [64/1500], Training Loss: 3965.2871364887037, Validation Loss: 4085.189453125\n",
      "Epoch [65/1500], Training Loss: 3864.5692471239495, Validation Loss: 3983.3310546875\n",
      "Epoch [66/1500], Training Loss: 3766.1674106933206, Validation Loss: 3884.609130859375\n",
      "Epoch [67/1500], Training Loss: 3670.1538744658587, Validation Loss: 3789.052978515625\n",
      "Epoch [68/1500], Training Loss: 3576.5848763648664, Validation Loss: 3696.658935546875\n",
      "Epoch [69/1500], Training Loss: 3485.4856779330044, Validation Loss: 3607.364013671875\n",
      "Epoch [70/1500], Training Loss: 3396.8499360497144, Validation Loss: 3521.01416015625\n",
      "Epoch [71/1500], Training Loss: 3310.6711668218536, Validation Loss: 3437.4853515625\n",
      "Epoch [72/1500], Training Loss: 3226.9397857190124, Validation Loss: 3356.625244140625\n",
      "Epoch [73/1500], Training Loss: 3145.6523416495384, Validation Loss: 3278.35986328125\n",
      "Epoch [74/1500], Training Loss: 3066.811559270704, Validation Loss: 3202.61962890625\n",
      "Epoch [75/1500], Training Loss: 2990.3886834658024, Validation Loss: 3129.306884765625\n",
      "Epoch [76/1500], Training Loss: 2916.3260811515493, Validation Loss: 3058.291015625\n",
      "Epoch [77/1500], Training Loss: 2844.4619832398876, Validation Loss: 2989.406494140625\n",
      "Epoch [78/1500], Training Loss: 2774.6132021791773, Validation Loss: 2922.499267578125\n",
      "Epoch [79/1500], Training Loss: 2706.6184299133906, Validation Loss: 2857.462646484375\n",
      "Epoch [80/1500], Training Loss: 2640.4425381944443, Validation Loss: 2794.308837890625\n",
      "Epoch [81/1500], Training Loss: 2576.171388612522, Validation Loss: 2733.1064453125\n",
      "Epoch [82/1500], Training Loss: 2513.935485963705, Validation Loss: 2673.930908203125\n",
      "Epoch [83/1500], Training Loss: 2453.8337982156936, Validation Loss: 2616.8671875\n",
      "Epoch [84/1500], Training Loss: 2395.910011241739, Validation Loss: 2561.927001953125\n",
      "Epoch [85/1500], Training Loss: 2340.055536679494, Validation Loss: 2508.983154296875\n",
      "Epoch [86/1500], Training Loss: 2286.0587207580825, Validation Loss: 2457.845458984375\n",
      "Epoch [87/1500], Training Loss: 2233.696133879464, Validation Loss: 2408.274169921875\n",
      "Epoch [88/1500], Training Loss: 2182.7780020835125, Validation Loss: 2360.0830078125\n",
      "Epoch [89/1500], Training Loss: 2133.1951368560904, Validation Loss: 2313.115478515625\n",
      "Epoch [90/1500], Training Loss: 2084.8675056608813, Validation Loss: 2267.2802734375\n",
      "Epoch [91/1500], Training Loss: 2037.716217876747, Validation Loss: 2222.461669921875\n",
      "Epoch [92/1500], Training Loss: 1991.6879944441243, Validation Loss: 2178.576416015625\n",
      "Epoch [93/1500], Training Loss: 1946.7486178400286, Validation Loss: 2135.57177734375\n",
      "Epoch [94/1500], Training Loss: 1902.8655424806077, Validation Loss: 2093.396240234375\n",
      "Epoch [95/1500], Training Loss: 1860.0359340511307, Validation Loss: 2052.03271484375\n",
      "Epoch [96/1500], Training Loss: 1818.2717373478158, Validation Loss: 2011.489990234375\n",
      "Epoch [97/1500], Training Loss: 1777.611242515806, Validation Loss: 1971.74853515625\n",
      "Epoch [98/1500], Training Loss: 1738.074528722723, Validation Loss: 1932.80224609375\n",
      "Epoch [99/1500], Training Loss: 1699.660974384514, Validation Loss: 1894.6070556640625\n",
      "Epoch [100/1500], Training Loss: 1662.3616843118648, Validation Loss: 1857.1085205078125\n",
      "Epoch [101/1500], Training Loss: 1626.150027436642, Validation Loss: 1820.220947265625\n",
      "Epoch [102/1500], Training Loss: 1591.006769542502, Validation Loss: 1783.8558349609375\n",
      "Epoch [103/1500], Training Loss: 1556.9134219493296, Validation Loss: 1747.92333984375\n",
      "Epoch [104/1500], Training Loss: 1523.8361313944397, Validation Loss: 1712.3148193359375\n",
      "Epoch [105/1500], Training Loss: 1491.7269304807162, Validation Loss: 1676.9483642578125\n",
      "Epoch [106/1500], Training Loss: 1460.543698411475, Validation Loss: 1641.820068359375\n",
      "Epoch [107/1500], Training Loss: 1430.2271735798313, Validation Loss: 1606.98046875\n",
      "Epoch [108/1500], Training Loss: 1400.7090236701147, Validation Loss: 1572.492919921875\n",
      "Epoch [109/1500], Training Loss: 1371.9018824207665, Validation Loss: 1538.4541015625\n",
      "Epoch [110/1500], Training Loss: 1343.7247350669109, Validation Loss: 1504.94775390625\n",
      "Epoch [111/1500], Training Loss: 1316.0985704758673, Validation Loss: 1471.9833984375\n",
      "Epoch [112/1500], Training Loss: 1288.996196316238, Validation Loss: 1439.5045166015625\n",
      "Epoch [113/1500], Training Loss: 1262.382811325473, Validation Loss: 1407.3221435546875\n",
      "Epoch [114/1500], Training Loss: 1236.236817670511, Validation Loss: 1374.97119140625\n",
      "Epoch [115/1500], Training Loss: 1210.5497722684424, Validation Loss: 1341.944580078125\n",
      "Epoch [116/1500], Training Loss: 1185.2974337947728, Validation Loss: 1308.1497802734375\n",
      "Epoch [117/1500], Training Loss: 1160.4629996327978, Validation Loss: 1274.0589599609375\n",
      "Epoch [118/1500], Training Loss: 1136.0465713291778, Validation Loss: 1240.3179931640625\n",
      "Epoch [119/1500], Training Loss: 1112.0765801966352, Validation Loss: 1207.4111328125\n",
      "Epoch [120/1500], Training Loss: 1088.5666259915597, Validation Loss: 1175.907958984375\n",
      "Epoch [121/1500], Training Loss: 1065.5776978129659, Validation Loss: 1146.27880859375\n",
      "Epoch [122/1500], Training Loss: 1043.1602422336034, Validation Loss: 1118.7242431640625\n",
      "Epoch [123/1500], Training Loss: 1021.3395530007292, Validation Loss: 1093.198974609375\n",
      "Epoch [124/1500], Training Loss: 1000.1405956159898, Validation Loss: 1069.427734375\n",
      "Epoch [125/1500], Training Loss: 979.5266146642872, Validation Loss: 1047.123291015625\n",
      "Epoch [126/1500], Training Loss: 959.4666853533129, Validation Loss: 1026.021240234375\n",
      "Epoch [127/1500], Training Loss: 939.9127068420499, Validation Loss: 1005.8758544921875\n",
      "Epoch [128/1500], Training Loss: 920.8271777254829, Validation Loss: 986.484130859375\n",
      "Epoch [129/1500], Training Loss: 902.1739025369301, Validation Loss: 967.7001342773438\n",
      "Epoch [130/1500], Training Loss: 883.9327815993468, Validation Loss: 949.427001953125\n",
      "Epoch [131/1500], Training Loss: 866.0803866906638, Validation Loss: 931.61083984375\n",
      "Epoch [132/1500], Training Loss: 848.6083713142094, Validation Loss: 914.2033081054688\n",
      "Epoch [133/1500], Training Loss: 831.4950636216679, Validation Loss: 897.1787109375\n",
      "Epoch [134/1500], Training Loss: 814.7255239087619, Validation Loss: 880.5211791992188\n",
      "Epoch [135/1500], Training Loss: 798.2895159774014, Validation Loss: 864.2296752929688\n",
      "Epoch [136/1500], Training Loss: 782.1810464490583, Validation Loss: 848.3074340820312\n",
      "Epoch [137/1500], Training Loss: 766.3895420318368, Validation Loss: 832.7650146484375\n",
      "Epoch [138/1500], Training Loss: 750.9103683905005, Validation Loss: 817.58837890625\n",
      "Epoch [139/1500], Training Loss: 735.7350224016186, Validation Loss: 802.79248046875\n",
      "Epoch [140/1500], Training Loss: 720.8592591700674, Validation Loss: 788.3597412109375\n",
      "Epoch [141/1500], Training Loss: 706.2730673576684, Validation Loss: 774.2714233398438\n",
      "Epoch [142/1500], Training Loss: 691.974460399076, Validation Loss: 760.51513671875\n",
      "Epoch [143/1500], Training Loss: 677.9484518536314, Validation Loss: 747.054931640625\n",
      "Epoch [144/1500], Training Loss: 664.1927160739041, Validation Loss: 733.8744506835938\n",
      "Epoch [145/1500], Training Loss: 650.7009454612913, Validation Loss: 720.9461059570312\n",
      "Epoch [146/1500], Training Loss: 637.4646014980881, Validation Loss: 708.2522583007812\n",
      "Epoch [147/1500], Training Loss: 624.4815603117827, Validation Loss: 695.7839965820312\n",
      "Epoch [148/1500], Training Loss: 611.7443732716736, Validation Loss: 683.5291748046875\n",
      "Epoch [149/1500], Training Loss: 599.2484692583637, Validation Loss: 671.4849243164062\n",
      "Epoch [150/1500], Training Loss: 586.9857654843722, Validation Loss: 659.658203125\n",
      "Epoch [151/1500], Training Loss: 574.9515501818964, Validation Loss: 648.0509643554688\n",
      "Epoch [152/1500], Training Loss: 563.1494567512342, Validation Loss: 636.6719970703125\n",
      "Epoch [153/1500], Training Loss: 551.5700748167407, Validation Loss: 625.5226440429688\n",
      "Epoch [154/1500], Training Loss: 540.2117784948806, Validation Loss: 614.6128540039062\n",
      "Epoch [155/1500], Training Loss: 529.0719597949195, Validation Loss: 603.9298706054688\n",
      "Epoch [156/1500], Training Loss: 518.1449118976126, Validation Loss: 593.4835815429688\n",
      "Epoch [157/1500], Training Loss: 507.43376165434717, Validation Loss: 583.2657470703125\n",
      "Epoch [158/1500], Training Loss: 496.92814781731227, Validation Loss: 573.27197265625\n",
      "Epoch [159/1500], Training Loss: 486.63060543766056, Validation Loss: 563.4989013671875\n",
      "Epoch [160/1500], Training Loss: 476.5360273659232, Validation Loss: 553.9486694335938\n",
      "Epoch [161/1500], Training Loss: 466.6457347558176, Validation Loss: 544.6094970703125\n",
      "Epoch [162/1500], Training Loss: 456.9483310069054, Validation Loss: 535.4796142578125\n",
      "Epoch [163/1500], Training Loss: 447.4473006176485, Validation Loss: 526.5573120117188\n",
      "Epoch [164/1500], Training Loss: 438.13646457219113, Validation Loss: 517.8360595703125\n",
      "Epoch [165/1500], Training Loss: 429.01186671256676, Validation Loss: 509.312255859375\n",
      "Epoch [166/1500], Training Loss: 420.07423533063326, Validation Loss: 500.9849548339844\n",
      "Epoch [167/1500], Training Loss: 411.3208747601832, Validation Loss: 492.85455322265625\n",
      "Epoch [168/1500], Training Loss: 402.7511939612241, Validation Loss: 484.91009521484375\n",
      "Epoch [169/1500], Training Loss: 394.35630168121395, Validation Loss: 477.15118408203125\n",
      "Epoch [170/1500], Training Loss: 386.1384223992429, Validation Loss: 469.5755920410156\n",
      "Epoch [171/1500], Training Loss: 378.0948211086049, Validation Loss: 462.17706298828125\n",
      "Epoch [172/1500], Training Loss: 370.2217478445594, Validation Loss: 454.9544677734375\n",
      "Epoch [173/1500], Training Loss: 362.5185522131524, Validation Loss: 447.90478515625\n",
      "Epoch [174/1500], Training Loss: 354.97893617086004, Validation Loss: 441.017333984375\n",
      "Epoch [175/1500], Training Loss: 347.60443629766763, Validation Loss: 434.29913330078125\n",
      "Epoch [176/1500], Training Loss: 340.38957697122254, Validation Loss: 427.7389221191406\n",
      "Epoch [177/1500], Training Loss: 333.33233498292543, Validation Loss: 421.3370666503906\n",
      "Epoch [178/1500], Training Loss: 326.43235700643385, Validation Loss: 415.0902404785156\n",
      "Epoch [179/1500], Training Loss: 319.68407104821046, Validation Loss: 408.9914245605469\n",
      "Epoch [180/1500], Training Loss: 313.08385583307626, Validation Loss: 403.0396728515625\n",
      "Epoch [181/1500], Training Loss: 306.6299620804077, Validation Loss: 397.2349853515625\n",
      "Epoch [182/1500], Training Loss: 300.32256418788467, Validation Loss: 391.57269287109375\n",
      "Epoch [183/1500], Training Loss: 294.1566914380412, Validation Loss: 386.05047607421875\n",
      "Epoch [184/1500], Training Loss: 288.1305692593774, Validation Loss: 380.6650085449219\n",
      "Epoch [185/1500], Training Loss: 282.24094487828165, Validation Loss: 375.4150695800781\n",
      "Epoch [186/1500], Training Loss: 276.48460463612815, Validation Loss: 370.29541015625\n",
      "Epoch [187/1500], Training Loss: 270.8608265472709, Validation Loss: 365.3030090332031\n",
      "Epoch [188/1500], Training Loss: 265.36639649272735, Validation Loss: 360.4388427734375\n",
      "Epoch [189/1500], Training Loss: 259.99908055785505, Validation Loss: 355.69708251953125\n",
      "Epoch [190/1500], Training Loss: 254.7579361577991, Validation Loss: 351.07635498046875\n",
      "Epoch [191/1500], Training Loss: 249.63805337781022, Validation Loss: 346.5719299316406\n",
      "Epoch [192/1500], Training Loss: 244.63898676597185, Validation Loss: 342.1838073730469\n",
      "Epoch [193/1500], Training Loss: 239.76048025297024, Validation Loss: 337.9106750488281\n",
      "Epoch [194/1500], Training Loss: 234.99772317521283, Validation Loss: 333.7485656738281\n",
      "Epoch [195/1500], Training Loss: 230.34903714214582, Validation Loss: 329.69403076171875\n",
      "Epoch [196/1500], Training Loss: 225.81090252933564, Validation Loss: 325.74462890625\n",
      "Epoch [197/1500], Training Loss: 221.3835717340763, Validation Loss: 321.8998718261719\n",
      "Epoch [198/1500], Training Loss: 217.0657539107981, Validation Loss: 318.1573791503906\n",
      "Epoch [199/1500], Training Loss: 212.8516335991023, Validation Loss: 314.51275634765625\n",
      "Epoch [200/1500], Training Loss: 208.7403903499447, Validation Loss: 310.9631652832031\n",
      "Epoch [201/1500], Training Loss: 204.72949264734154, Validation Loss: 307.5075988769531\n",
      "Epoch [202/1500], Training Loss: 200.8162612482817, Validation Loss: 304.1394958496094\n",
      "Epoch [203/1500], Training Loss: 196.99843997801977, Validation Loss: 300.85906982421875\n",
      "Epoch [204/1500], Training Loss: 193.27379719104528, Validation Loss: 297.6639709472656\n",
      "Epoch [205/1500], Training Loss: 189.63918006109546, Validation Loss: 294.5597839355469\n",
      "Epoch [206/1500], Training Loss: 186.0924487010277, Validation Loss: 291.5399475097656\n",
      "Epoch [207/1500], Training Loss: 182.63198549495485, Validation Loss: 288.5972900390625\n",
      "Epoch [208/1500], Training Loss: 179.2535419293457, Validation Loss: 285.72698974609375\n",
      "Epoch [209/1500], Training Loss: 175.95285336337548, Validation Loss: 282.9301452636719\n",
      "Epoch [210/1500], Training Loss: 172.72934103186867, Validation Loss: 280.2056884765625\n",
      "Epoch [211/1500], Training Loss: 169.57898547579228, Validation Loss: 277.5528259277344\n",
      "Epoch [212/1500], Training Loss: 166.49970440299973, Validation Loss: 274.97357177734375\n",
      "Epoch [213/1500], Training Loss: 163.48935520017108, Validation Loss: 272.4669189453125\n",
      "Epoch [214/1500], Training Loss: 160.5445461178027, Validation Loss: 270.03521728515625\n",
      "Epoch [215/1500], Training Loss: 157.66376565907308, Validation Loss: 267.6768493652344\n",
      "Epoch [216/1500], Training Loss: 154.8455338360046, Validation Loss: 265.39093017578125\n",
      "Epoch [217/1500], Training Loss: 152.08860042645003, Validation Loss: 263.1756896972656\n",
      "Epoch [218/1500], Training Loss: 149.3909142344222, Validation Loss: 261.0267639160156\n",
      "Epoch [219/1500], Training Loss: 146.75251210121462, Validation Loss: 258.943603515625\n",
      "Epoch [220/1500], Training Loss: 144.1710967661903, Validation Loss: 256.92120361328125\n",
      "Epoch [221/1500], Training Loss: 141.64534389630913, Validation Loss: 254.95700073242188\n",
      "Epoch [222/1500], Training Loss: 139.17387920938108, Validation Loss: 253.04763793945312\n",
      "Epoch [223/1500], Training Loss: 136.75684840798013, Validation Loss: 251.1904754638672\n",
      "Epoch [224/1500], Training Loss: 134.39129801419583, Validation Loss: 249.38357543945312\n",
      "Epoch [225/1500], Training Loss: 132.0759840158961, Validation Loss: 247.62278747558594\n",
      "Epoch [226/1500], Training Loss: 129.80954887412565, Validation Loss: 245.9080047607422\n",
      "Epoch [227/1500], Training Loss: 127.59218656555286, Validation Loss: 244.236572265625\n",
      "Epoch [228/1500], Training Loss: 125.42084836050746, Validation Loss: 242.60511779785156\n",
      "Epoch [229/1500], Training Loss: 123.29560711784049, Validation Loss: 241.01458740234375\n",
      "Epoch [230/1500], Training Loss: 121.21600593598852, Validation Loss: 239.46087646484375\n",
      "Epoch [231/1500], Training Loss: 119.18047328869997, Validation Loss: 237.94314575195312\n",
      "Epoch [232/1500], Training Loss: 117.18727691082964, Validation Loss: 236.45916748046875\n",
      "Epoch [233/1500], Training Loss: 115.23580010049945, Validation Loss: 235.00559997558594\n",
      "Epoch [234/1500], Training Loss: 113.32569548519041, Validation Loss: 233.58346557617188\n",
      "Epoch [235/1500], Training Loss: 111.4563466690117, Validation Loss: 232.1884307861328\n",
      "Epoch [236/1500], Training Loss: 109.62653835573362, Validation Loss: 230.81993103027344\n",
      "Epoch [237/1500], Training Loss: 107.83440586906403, Validation Loss: 229.4738311767578\n",
      "Epoch [238/1500], Training Loss: 106.0808919404225, Validation Loss: 228.15032958984375\n",
      "Epoch [239/1500], Training Loss: 104.36516156946114, Validation Loss: 226.8466796875\n",
      "Epoch [240/1500], Training Loss: 102.68623513443656, Validation Loss: 225.56149291992188\n",
      "Epoch [241/1500], Training Loss: 101.04321456658893, Validation Loss: 224.2923126220703\n",
      "Epoch [242/1500], Training Loss: 99.43560439531316, Validation Loss: 223.0368194580078\n",
      "Epoch [243/1500], Training Loss: 97.86262249259177, Validation Loss: 221.7946319580078\n",
      "Epoch [244/1500], Training Loss: 96.32403234257096, Validation Loss: 220.5636444091797\n",
      "Epoch [245/1500], Training Loss: 94.81949691932732, Validation Loss: 219.341796875\n",
      "Epoch [246/1500], Training Loss: 93.34746847276199, Validation Loss: 218.12620544433594\n",
      "Epoch [247/1500], Training Loss: 91.90798171582442, Validation Loss: 216.9163818359375\n",
      "Epoch [248/1500], Training Loss: 90.50010938489807, Validation Loss: 215.70973205566406\n",
      "Epoch [249/1500], Training Loss: 89.12232518348297, Validation Loss: 214.50416564941406\n",
      "Epoch [250/1500], Training Loss: 87.7740178508099, Validation Loss: 213.29893493652344\n",
      "Epoch [251/1500], Training Loss: 86.45511412117663, Validation Loss: 212.0907440185547\n",
      "Epoch [252/1500], Training Loss: 85.16388715870504, Validation Loss: 210.8775177001953\n",
      "Epoch [253/1500], Training Loss: 83.89996486438457, Validation Loss: 209.65797424316406\n",
      "Epoch [254/1500], Training Loss: 82.66239144031208, Validation Loss: 208.43006896972656\n",
      "Epoch [255/1500], Training Loss: 81.4499065108963, Validation Loss: 207.18984985351562\n",
      "Epoch [256/1500], Training Loss: 80.26049923332738, Validation Loss: 205.93458557128906\n",
      "Epoch [257/1500], Training Loss: 79.09459249043948, Validation Loss: 204.66326904296875\n",
      "Epoch [258/1500], Training Loss: 77.95027546822597, Validation Loss: 203.37168884277344\n",
      "Epoch [259/1500], Training Loss: 76.8263420246044, Validation Loss: 202.05906677246094\n",
      "Epoch [260/1500], Training Loss: 75.72153312796132, Validation Loss: 200.72769165039062\n",
      "Epoch [261/1500], Training Loss: 74.63444270755669, Validation Loss: 199.3811492919922\n",
      "Epoch [262/1500], Training Loss: 73.56393272879784, Validation Loss: 198.0246124267578\n",
      "Epoch [263/1500], Training Loss: 72.51130956035183, Validation Loss: 196.65524291992188\n",
      "Epoch [264/1500], Training Loss: 71.47607913589836, Validation Loss: 195.258544921875\n",
      "Epoch [265/1500], Training Loss: 70.45682113440175, Validation Loss: 193.82371520996094\n",
      "Epoch [266/1500], Training Loss: 69.45172297250201, Validation Loss: 192.34982299804688\n",
      "Epoch [267/1500], Training Loss: 68.46106351982053, Validation Loss: 190.8358612060547\n",
      "Epoch [268/1500], Training Loss: 67.4836701534799, Validation Loss: 189.28369140625\n",
      "Epoch [269/1500], Training Loss: 66.51943540606682, Validation Loss: 187.6924591064453\n",
      "Epoch [270/1500], Training Loss: 65.5672957627884, Validation Loss: 186.0612335205078\n",
      "Epoch [271/1500], Training Loss: 64.6260749066827, Validation Loss: 184.39134216308594\n",
      "Epoch [272/1500], Training Loss: 63.69563652191617, Validation Loss: 182.6825714111328\n",
      "Epoch [273/1500], Training Loss: 62.77606048255413, Validation Loss: 180.9371337890625\n",
      "Epoch [274/1500], Training Loss: 61.86618677898142, Validation Loss: 179.1543731689453\n",
      "Epoch [275/1500], Training Loss: 60.964647227916544, Validation Loss: 177.33644104003906\n",
      "Epoch [276/1500], Training Loss: 60.07183682176047, Validation Loss: 175.48301696777344\n",
      "Epoch [277/1500], Training Loss: 59.18688368522752, Validation Loss: 173.59515380859375\n",
      "Epoch [278/1500], Training Loss: 58.30804011687399, Validation Loss: 171.6712646484375\n",
      "Epoch [279/1500], Training Loss: 57.43592303534139, Validation Loss: 169.7109832763672\n",
      "Epoch [280/1500], Training Loss: 56.56931299772745, Validation Loss: 167.70925903320312\n",
      "Epoch [281/1500], Training Loss: 55.70810915502626, Validation Loss: 165.66580200195312\n",
      "Epoch [282/1500], Training Loss: 54.85030894295026, Validation Loss: 163.5729522705078\n",
      "Epoch [283/1500], Training Loss: 53.99530350161592, Validation Loss: 161.42811584472656\n",
      "Epoch [284/1500], Training Loss: 53.14295754530287, Validation Loss: 159.225341796875\n",
      "Epoch [285/1500], Training Loss: 52.292270761323586, Validation Loss: 156.95701599121094\n",
      "Epoch [286/1500], Training Loss: 51.44140486259591, Validation Loss: 154.6159210205078\n",
      "Epoch [287/1500], Training Loss: 50.59041165051746, Validation Loss: 152.19570922851562\n",
      "Epoch [288/1500], Training Loss: 49.738833762676926, Validation Loss: 149.6869659423828\n",
      "Epoch [289/1500], Training Loss: 48.88543617439272, Validation Loss: 147.0804901123047\n",
      "Epoch [290/1500], Training Loss: 48.030482937947404, Validation Loss: 144.3653106689453\n",
      "Epoch [291/1500], Training Loss: 47.17427335486626, Validation Loss: 141.53225708007812\n",
      "Epoch [292/1500], Training Loss: 46.317621427212735, Validation Loss: 138.578857421875\n",
      "Epoch [293/1500], Training Loss: 45.46010142074805, Validation Loss: 135.50624084472656\n",
      "Epoch [294/1500], Training Loss: 44.60194058202143, Validation Loss: 132.3191375732422\n",
      "Epoch [295/1500], Training Loss: 43.75672888954963, Validation Loss: 129.122802734375\n",
      "Epoch [296/1500], Training Loss: 42.96552039684819, Validation Loss: 126.17230987548828\n",
      "Epoch [297/1500], Training Loss: 42.259064011998696, Validation Loss: 123.67558288574219\n",
      "Epoch [298/1500], Training Loss: 41.61315177494838, Validation Loss: 121.58073425292969\n",
      "Epoch [299/1500], Training Loss: 40.99631101377891, Validation Loss: 119.73113250732422\n",
      "Epoch [300/1500], Training Loss: 40.39690924412122, Validation Loss: 118.02375793457031\n",
      "Epoch [301/1500], Training Loss: 39.81075954607954, Validation Loss: 116.40941619873047\n",
      "Epoch [302/1500], Training Loss: 39.23743817582278, Validation Loss: 114.86355590820312\n",
      "Epoch [303/1500], Training Loss: 38.676155182369754, Validation Loss: 113.37318420410156\n",
      "Epoch [304/1500], Training Loss: 38.12623460790666, Validation Loss: 111.92920684814453\n",
      "Epoch [305/1500], Training Loss: 37.587295281902854, Validation Loss: 110.52245330810547\n",
      "Epoch [306/1500], Training Loss: 37.05786604011142, Validation Loss: 109.14403533935547\n",
      "Epoch [307/1500], Training Loss: 36.53686904521143, Validation Loss: 107.78565216064453\n",
      "Epoch [308/1500], Training Loss: 36.02423378866968, Validation Loss: 106.44171905517578\n",
      "Epoch [309/1500], Training Loss: 35.51921532257141, Validation Loss: 105.10661315917969\n",
      "Epoch [310/1500], Training Loss: 35.0214342624067, Validation Loss: 103.77670288085938\n",
      "Epoch [311/1500], Training Loss: 34.5302967335239, Validation Loss: 102.4484634399414\n",
      "Epoch [312/1500], Training Loss: 34.0455585184249, Validation Loss: 101.11924743652344\n",
      "Epoch [313/1500], Training Loss: 33.566733429396464, Validation Loss: 99.78619384765625\n",
      "Epoch [314/1500], Training Loss: 33.09281949915871, Validation Loss: 98.44757843017578\n",
      "Epoch [315/1500], Training Loss: 32.62396097714746, Validation Loss: 97.10161590576172\n",
      "Epoch [316/1500], Training Loss: 32.15932652057175, Validation Loss: 95.7474365234375\n",
      "Epoch [317/1500], Training Loss: 31.699138899845394, Validation Loss: 94.38427734375\n",
      "Epoch [318/1500], Training Loss: 31.242458436770914, Validation Loss: 93.01173400878906\n",
      "Epoch [319/1500], Training Loss: 30.789733309531957, Validation Loss: 91.63082885742188\n",
      "Epoch [320/1500], Training Loss: 30.34074113727672, Validation Loss: 90.2412338256836\n",
      "Epoch [321/1500], Training Loss: 29.89479464879235, Validation Loss: 88.8439712524414\n",
      "Epoch [322/1500], Training Loss: 29.452223499305084, Validation Loss: 87.43896484375\n",
      "Epoch [323/1500], Training Loss: 29.012647478155255, Validation Loss: 86.02759552001953\n",
      "Epoch [324/1500], Training Loss: 28.57658223659491, Validation Loss: 84.61046600341797\n",
      "Epoch [325/1500], Training Loss: 28.143641439786567, Validation Loss: 83.18737030029297\n",
      "Epoch [326/1500], Training Loss: 27.714126839004546, Validation Loss: 81.75942993164062\n",
      "Epoch [327/1500], Training Loss: 27.287695497987027, Validation Loss: 80.32653045654297\n",
      "Epoch [328/1500], Training Loss: 26.864770105625556, Validation Loss: 78.89076232910156\n",
      "Epoch [329/1500], Training Loss: 26.44574070245276, Validation Loss: 77.45287322998047\n",
      "Epoch [330/1500], Training Loss: 26.030454624075357, Validation Loss: 76.01508331298828\n",
      "Epoch [331/1500], Training Loss: 25.619649413683952, Validation Loss: 74.57957458496094\n",
      "Epoch [332/1500], Training Loss: 25.21380091627903, Validation Loss: 73.14888000488281\n",
      "Epoch [333/1500], Training Loss: 24.812992542995417, Validation Loss: 71.7249526977539\n",
      "Epoch [334/1500], Training Loss: 24.417703609832984, Validation Loss: 70.31023406982422\n",
      "Epoch [335/1500], Training Loss: 24.02814694544151, Validation Loss: 68.90718078613281\n",
      "Epoch [336/1500], Training Loss: 23.644175022975265, Validation Loss: 67.51744079589844\n",
      "Epoch [337/1500], Training Loss: 23.266156282455682, Validation Loss: 66.14412689208984\n",
      "Epoch [338/1500], Training Loss: 22.894437484055253, Validation Loss: 64.78836059570312\n",
      "Epoch [339/1500], Training Loss: 22.529190158627596, Validation Loss: 63.453697204589844\n",
      "Epoch [340/1500], Training Loss: 22.170847789445144, Validation Loss: 62.1410026550293\n",
      "Epoch [341/1500], Training Loss: 21.819299701469784, Validation Loss: 60.8535270690918\n",
      "Epoch [342/1500], Training Loss: 21.47470400170777, Validation Loss: 59.59248733520508\n",
      "Epoch [343/1500], Training Loss: 21.13768052626036, Validation Loss: 58.35829162597656\n",
      "Epoch [344/1500], Training Loss: 20.80783779813969, Validation Loss: 57.15272521972656\n",
      "Epoch [345/1500], Training Loss: 20.485156453087587, Validation Loss: 55.97478485107422\n",
      "Epoch [346/1500], Training Loss: 20.169669413348586, Validation Loss: 54.82542037963867\n",
      "Epoch [347/1500], Training Loss: 19.861463225828803, Validation Loss: 53.70320129394531\n",
      "Epoch [348/1500], Training Loss: 19.560236024388995, Validation Loss: 52.60737228393555\n",
      "Epoch [349/1500], Training Loss: 19.265702468370968, Validation Loss: 51.536556243896484\n",
      "Epoch [350/1500], Training Loss: 18.97786871842358, Validation Loss: 50.49056625366211\n",
      "Epoch [351/1500], Training Loss: 18.696470898312874, Validation Loss: 49.46772003173828\n",
      "Epoch [352/1500], Training Loss: 18.421157717843666, Validation Loss: 48.46821212768555\n",
      "Epoch [353/1500], Training Loss: 18.15181123237917, Validation Loss: 47.48991012573242\n",
      "Epoch [354/1500], Training Loss: 17.888065348021417, Validation Loss: 46.53335189819336\n",
      "Epoch [355/1500], Training Loss: 17.629816090037394, Validation Loss: 45.596336364746094\n",
      "Epoch [356/1500], Training Loss: 17.376659007419825, Validation Loss: 44.67848205566406\n",
      "Epoch [357/1500], Training Loss: 17.128490635962724, Validation Loss: 43.77967834472656\n",
      "Epoch [358/1500], Training Loss: 16.885299019806467, Validation Loss: 42.90054702758789\n",
      "Epoch [359/1500], Training Loss: 16.646833693429, Validation Loss: 42.042781829833984\n",
      "Epoch [360/1500], Training Loss: 16.412900478471794, Validation Loss: 41.20844268798828\n",
      "Epoch [361/1500], Training Loss: 16.18372398716396, Validation Loss: 40.399051666259766\n",
      "Epoch [362/1500], Training Loss: 15.959129843838847, Validation Loss: 39.61393356323242\n",
      "Epoch [363/1500], Training Loss: 15.739254878989838, Validation Loss: 38.85161590576172\n",
      "Epoch [364/1500], Training Loss: 15.524019835818239, Validation Loss: 38.111114501953125\n",
      "Epoch [365/1500], Training Loss: 15.313078894270816, Validation Loss: 37.391334533691406\n",
      "Epoch [366/1500], Training Loss: 15.106663403755102, Validation Loss: 36.69091033935547\n",
      "Epoch [367/1500], Training Loss: 14.904470849981069, Validation Loss: 36.009185791015625\n",
      "Epoch [368/1500], Training Loss: 14.706465806691993, Validation Loss: 35.34534454345703\n",
      "Epoch [369/1500], Training Loss: 14.512349440100234, Validation Loss: 34.6987419128418\n",
      "Epoch [370/1500], Training Loss: 14.322352981108303, Validation Loss: 34.06864929199219\n",
      "Epoch [371/1500], Training Loss: 14.136207469549548, Validation Loss: 33.454803466796875\n",
      "Epoch [372/1500], Training Loss: 13.954056470747734, Validation Loss: 32.85664367675781\n",
      "Epoch [373/1500], Training Loss: 13.775236047227454, Validation Loss: 32.27399826049805\n",
      "Epoch [374/1500], Training Loss: 13.599862866673003, Validation Loss: 31.706300735473633\n",
      "Epoch [375/1500], Training Loss: 13.42751966876187, Validation Loss: 31.15374183654785\n",
      "Epoch [376/1500], Training Loss: 13.257877258307378, Validation Loss: 30.6162109375\n",
      "Epoch [377/1500], Training Loss: 13.090773771014007, Validation Loss: 30.093887329101562\n",
      "Epoch [378/1500], Training Loss: 12.925939412336195, Validation Loss: 29.587047576904297\n",
      "Epoch [379/1500], Training Loss: 12.763245882047832, Validation Loss: 29.0960636138916\n",
      "Epoch [380/1500], Training Loss: 12.60260754967867, Validation Loss: 28.62118148803711\n",
      "Epoch [381/1500], Training Loss: 12.443883734695275, Validation Loss: 28.16269302368164\n",
      "Epoch [382/1500], Training Loss: 12.287585909204527, Validation Loss: 27.72093963623047\n",
      "Epoch [383/1500], Training Loss: 12.133738918626385, Validation Loss: 27.29657554626465\n",
      "Epoch [384/1500], Training Loss: 11.982527436578446, Validation Loss: 26.89033317565918\n",
      "Epoch [385/1500], Training Loss: 11.834064362429864, Validation Loss: 26.503376007080078\n",
      "Epoch [386/1500], Training Loss: 11.68866095719187, Validation Loss: 26.137548446655273\n",
      "Epoch [387/1500], Training Loss: 11.546569589798853, Validation Loss: 25.794448852539062\n",
      "Epoch [388/1500], Training Loss: 11.407973877373529, Validation Loss: 25.473888397216797\n",
      "Epoch [389/1500], Training Loss: 11.273295836058676, Validation Loss: 25.17547035217285\n",
      "Epoch [390/1500], Training Loss: 11.142613700566882, Validation Loss: 24.897586822509766\n",
      "Epoch [391/1500], Training Loss: 11.016139521831153, Validation Loss: 24.636808395385742\n",
      "Epoch [392/1500], Training Loss: 10.893380928604223, Validation Loss: 24.391435623168945\n",
      "Epoch [393/1500], Training Loss: 10.774012966784948, Validation Loss: 24.16019058227539\n",
      "Epoch [394/1500], Training Loss: 10.657767744427423, Validation Loss: 23.93996810913086\n",
      "Epoch [395/1500], Training Loss: 10.544309224183392, Validation Loss: 23.727903366088867\n",
      "Epoch [396/1500], Training Loss: 10.433505675153485, Validation Loss: 23.521413803100586\n",
      "Epoch [397/1500], Training Loss: 10.325276974258507, Validation Loss: 23.318824768066406\n",
      "Epoch [398/1500], Training Loss: 10.219344992964281, Validation Loss: 23.1182804107666\n",
      "Epoch [399/1500], Training Loss: 10.1155398004593, Validation Loss: 22.918895721435547\n",
      "Epoch [400/1500], Training Loss: 10.01385425277883, Validation Loss: 22.71970558166504\n",
      "Epoch [401/1500], Training Loss: 9.914027854086827, Validation Loss: 22.51999282836914\n",
      "Epoch [402/1500], Training Loss: 9.815983006798563, Validation Loss: 22.319000244140625\n",
      "Epoch [403/1500], Training Loss: 9.719378387182202, Validation Loss: 22.117399215698242\n",
      "Epoch [404/1500], Training Loss: 9.624209873522515, Validation Loss: 21.914175033569336\n",
      "Epoch [405/1500], Training Loss: 9.53048724828131, Validation Loss: 21.710315704345703\n",
      "Epoch [406/1500], Training Loss: 9.437950075148711, Validation Loss: 21.506675720214844\n",
      "Epoch [407/1500], Training Loss: 9.346726402951264, Validation Loss: 21.305086135864258\n",
      "Epoch [408/1500], Training Loss: 9.256679778155478, Validation Loss: 21.108064651489258\n",
      "Epoch [409/1500], Training Loss: 9.168002825929253, Validation Loss: 20.918325424194336\n",
      "Epoch [410/1500], Training Loss: 9.081086388035741, Validation Loss: 20.737829208374023\n",
      "Epoch [411/1500], Training Loss: 8.995885605771978, Validation Loss: 20.567852020263672\n",
      "Epoch [412/1500], Training Loss: 8.912681925443643, Validation Loss: 20.40849494934082\n",
      "Epoch [413/1500], Training Loss: 8.831406868000467, Validation Loss: 20.25882911682129\n",
      "Epoch [414/1500], Training Loss: 8.751973264581766, Validation Loss: 20.117042541503906\n",
      "Epoch [415/1500], Training Loss: 8.674379593080115, Validation Loss: 19.981718063354492\n",
      "Epoch [416/1500], Training Loss: 8.598466516472365, Validation Loss: 19.85108184814453\n",
      "Epoch [417/1500], Training Loss: 8.524147033666823, Validation Loss: 19.72398567199707\n",
      "Epoch [418/1500], Training Loss: 8.451449150597348, Validation Loss: 19.599584579467773\n",
      "Epoch [419/1500], Training Loss: 8.380012620384692, Validation Loss: 19.47751235961914\n",
      "Epoch [420/1500], Training Loss: 8.309937773030029, Validation Loss: 19.35762596130371\n",
      "Epoch [421/1500], Training Loss: 8.24122202107362, Validation Loss: 19.239513397216797\n",
      "Epoch [422/1500], Training Loss: 8.173687164319464, Validation Loss: 19.123476028442383\n",
      "Epoch [423/1500], Training Loss: 8.107278258118907, Validation Loss: 19.009490966796875\n",
      "Epoch [424/1500], Training Loss: 8.042185057902389, Validation Loss: 18.897815704345703\n",
      "Epoch [425/1500], Training Loss: 7.978339957534395, Validation Loss: 18.78850746154785\n",
      "Epoch [426/1500], Training Loss: 7.915580290714166, Validation Loss: 18.681671142578125\n",
      "Epoch [427/1500], Training Loss: 7.85383930220379, Validation Loss: 18.57741928100586\n",
      "Epoch [428/1500], Training Loss: 7.793172511633841, Validation Loss: 18.475740432739258\n",
      "Epoch [429/1500], Training Loss: 7.73364584016705, Validation Loss: 18.37683868408203\n",
      "Epoch [430/1500], Training Loss: 7.675028046832321, Validation Loss: 18.280447006225586\n",
      "Epoch [431/1500], Training Loss: 7.6174397571349175, Validation Loss: 18.18668556213379\n",
      "Epoch [432/1500], Training Loss: 7.56080515444666, Validation Loss: 18.09565544128418\n",
      "Epoch [433/1500], Training Loss: 7.505012251455022, Validation Loss: 18.006757736206055\n",
      "Epoch [434/1500], Training Loss: 7.450138720366564, Validation Loss: 17.92042350769043\n",
      "Epoch [435/1500], Training Loss: 7.396150210919571, Validation Loss: 17.83656120300293\n",
      "Epoch [436/1500], Training Loss: 7.342968599596201, Validation Loss: 17.75498390197754\n",
      "Epoch [437/1500], Training Loss: 7.290589935585579, Validation Loss: 17.67580223083496\n",
      "Epoch [438/1500], Training Loss: 7.238941999761904, Validation Loss: 17.59857749938965\n",
      "Epoch [439/1500], Training Loss: 7.188146732883538, Validation Loss: 17.523439407348633\n",
      "Epoch [440/1500], Training Loss: 7.138134258384019, Validation Loss: 17.450374603271484\n",
      "Epoch [441/1500], Training Loss: 7.088805080393846, Validation Loss: 17.379220962524414\n",
      "Epoch [442/1500], Training Loss: 7.040223343403602, Validation Loss: 17.309890747070312\n",
      "Epoch [443/1500], Training Loss: 6.992408905607503, Validation Loss: 17.24227523803711\n",
      "Epoch [444/1500], Training Loss: 6.945306018487522, Validation Loss: 17.17641258239746\n",
      "Epoch [445/1500], Training Loss: 6.898867564971495, Validation Loss: 17.11224365234375\n",
      "Epoch [446/1500], Training Loss: 6.853133971080715, Validation Loss: 17.049467086791992\n",
      "Epoch [447/1500], Training Loss: 6.808138938345019, Validation Loss: 16.988327026367188\n",
      "Epoch [448/1500], Training Loss: 6.763719302485233, Validation Loss: 16.928348541259766\n",
      "Epoch [449/1500], Training Loss: 6.719927228656804, Validation Loss: 16.86969757080078\n",
      "Epoch [450/1500], Training Loss: 6.676720311671051, Validation Loss: 16.8123722076416\n",
      "Epoch [451/1500], Training Loss: 6.633985733141233, Validation Loss: 16.75611686706543\n",
      "Epoch [452/1500], Training Loss: 6.591814594558061, Validation Loss: 16.700868606567383\n",
      "Epoch [453/1500], Training Loss: 6.550161376499258, Validation Loss: 16.64672088623047\n",
      "Epoch [454/1500], Training Loss: 6.5089578253723515, Validation Loss: 16.59343719482422\n",
      "Epoch [455/1500], Training Loss: 6.468313040695466, Validation Loss: 16.54148292541504\n",
      "Epoch [456/1500], Training Loss: 6.428257601389037, Validation Loss: 16.49022102355957\n",
      "Epoch [457/1500], Training Loss: 6.388722674233282, Validation Loss: 16.43985939025879\n",
      "Epoch [458/1500], Training Loss: 6.349754746751747, Validation Loss: 16.390342712402344\n",
      "Epoch [459/1500], Training Loss: 6.311389220447685, Validation Loss: 16.341751098632812\n",
      "Epoch [460/1500], Training Loss: 6.2734500517203635, Validation Loss: 16.29389762878418\n",
      "Epoch [461/1500], Training Loss: 6.236002579133533, Validation Loss: 16.246540069580078\n",
      "Epoch [462/1500], Training Loss: 6.1990667288742065, Validation Loss: 16.20005989074707\n",
      "Epoch [463/1500], Training Loss: 6.1626882640926794, Validation Loss: 16.154293060302734\n",
      "Epoch [464/1500], Training Loss: 6.126766160584473, Validation Loss: 16.109045028686523\n",
      "Epoch [465/1500], Training Loss: 6.091374103195121, Validation Loss: 16.064647674560547\n",
      "Epoch [466/1500], Training Loss: 6.0563679343335295, Validation Loss: 16.02064323425293\n",
      "Epoch [467/1500], Training Loss: 6.021817511589915, Validation Loss: 15.977126121520996\n",
      "Epoch [468/1500], Training Loss: 5.987675154367787, Validation Loss: 15.934146881103516\n",
      "Epoch [469/1500], Training Loss: 5.954056848007583, Validation Loss: 15.891636848449707\n",
      "Epoch [470/1500], Training Loss: 5.920857678559657, Validation Loss: 15.849607467651367\n",
      "Epoch [471/1500], Training Loss: 5.888102937638392, Validation Loss: 15.807886123657227\n",
      "Epoch [472/1500], Training Loss: 5.855677321820379, Validation Loss: 15.766666412353516\n",
      "Epoch [473/1500], Training Loss: 5.823601041373114, Validation Loss: 15.7260103225708\n",
      "Epoch [474/1500], Training Loss: 5.791889445314152, Validation Loss: 15.685750961303711\n",
      "Epoch [475/1500], Training Loss: 5.760613173297467, Validation Loss: 15.646023750305176\n",
      "Epoch [476/1500], Training Loss: 5.729762521912957, Validation Loss: 15.606827735900879\n",
      "Epoch [477/1500], Training Loss: 5.699233670107104, Validation Loss: 15.568136215209961\n",
      "Epoch [478/1500], Training Loss: 5.6691940813557755, Validation Loss: 15.529852867126465\n",
      "Epoch [479/1500], Training Loss: 5.639553954403748, Validation Loss: 15.491958618164062\n",
      "Epoch [480/1500], Training Loss: 5.610248433602533, Validation Loss: 15.45427131652832\n",
      "Epoch [481/1500], Training Loss: 5.581242538870075, Validation Loss: 15.416882514953613\n",
      "Epoch [482/1500], Training Loss: 5.552575769626602, Validation Loss: 15.37989330291748\n",
      "Epoch [483/1500], Training Loss: 5.5242706161898125, Validation Loss: 15.343120574951172\n",
      "Epoch [484/1500], Training Loss: 5.496347778603478, Validation Loss: 15.306905746459961\n",
      "Epoch [485/1500], Training Loss: 5.468783508394048, Validation Loss: 15.271081924438477\n",
      "Epoch [486/1500], Training Loss: 5.441532861515881, Validation Loss: 15.235734939575195\n",
      "Epoch [487/1500], Training Loss: 5.414618380696835, Validation Loss: 15.200729370117188\n",
      "Epoch [488/1500], Training Loss: 5.388038042230566, Validation Loss: 15.165999412536621\n",
      "Epoch [489/1500], Training Loss: 5.361778594860007, Validation Loss: 15.131732940673828\n",
      "Epoch [490/1500], Training Loss: 5.335869103764856, Validation Loss: 15.09785270690918\n",
      "Epoch [491/1500], Training Loss: 5.310247256866904, Validation Loss: 15.064353942871094\n",
      "Epoch [492/1500], Training Loss: 5.28494781272148, Validation Loss: 15.031150817871094\n",
      "Epoch [493/1500], Training Loss: 5.259914030158699, Validation Loss: 14.998063087463379\n",
      "Epoch [494/1500], Training Loss: 5.23515528155789, Validation Loss: 14.965206146240234\n",
      "Epoch [495/1500], Training Loss: 5.210676077296081, Validation Loss: 14.93274211883545\n",
      "Epoch [496/1500], Training Loss: 5.186512142076672, Validation Loss: 14.900632858276367\n",
      "Epoch [497/1500], Training Loss: 5.16266295756075, Validation Loss: 14.868925094604492\n",
      "Epoch [498/1500], Training Loss: 5.139075771382407, Validation Loss: 14.83735466003418\n",
      "Epoch [499/1500], Training Loss: 5.115748326320023, Validation Loss: 14.806217193603516\n",
      "Epoch [500/1500], Training Loss: 5.0926902951625825, Validation Loss: 14.775230407714844\n",
      "Epoch [501/1500], Training Loss: 5.069924126357112, Validation Loss: 14.744434356689453\n",
      "Epoch [502/1500], Training Loss: 5.047431537709507, Validation Loss: 14.71402359008789\n",
      "Epoch [503/1500], Training Loss: 5.025270700704918, Validation Loss: 14.683928489685059\n",
      "Epoch [504/1500], Training Loss: 5.003318830739183, Validation Loss: 14.65381145477295\n",
      "Epoch [505/1500], Training Loss: 4.9816371141322735, Validation Loss: 14.624188423156738\n",
      "Epoch [506/1500], Training Loss: 4.960178357408741, Validation Loss: 14.594712257385254\n",
      "Epoch [507/1500], Training Loss: 4.93903086114707, Validation Loss: 14.565613746643066\n",
      "Epoch [508/1500], Training Loss: 4.918093719566119, Validation Loss: 14.536821365356445\n",
      "Epoch [509/1500], Training Loss: 4.897429201913968, Validation Loss: 14.508224487304688\n",
      "Epoch [510/1500], Training Loss: 4.877033763484414, Validation Loss: 14.479708671569824\n",
      "Epoch [511/1500], Training Loss: 4.856869560694213, Validation Loss: 14.45145320892334\n",
      "Epoch [512/1500], Training Loss: 4.83696781201963, Validation Loss: 14.423551559448242\n",
      "Epoch [513/1500], Training Loss: 4.817278309740452, Validation Loss: 14.395853996276855\n",
      "Epoch [514/1500], Training Loss: 4.797796357537939, Validation Loss: 14.368229866027832\n",
      "Epoch [515/1500], Training Loss: 4.778553846454671, Validation Loss: 14.340805053710938\n",
      "Epoch [516/1500], Training Loss: 4.759543629821074, Validation Loss: 14.313672065734863\n",
      "Epoch [517/1500], Training Loss: 4.740756156626015, Validation Loss: 14.286417961120605\n",
      "Epoch [518/1500], Training Loss: 4.722156538633027, Validation Loss: 14.25948429107666\n",
      "Epoch [519/1500], Training Loss: 4.703764279862789, Validation Loss: 14.232972145080566\n",
      "Epoch [520/1500], Training Loss: 4.685565926890519, Validation Loss: 14.206623077392578\n",
      "Epoch [521/1500], Training Loss: 4.667576204899732, Validation Loss: 14.180399894714355\n",
      "Epoch [522/1500], Training Loss: 4.649786325615722, Validation Loss: 14.154346466064453\n",
      "Epoch [523/1500], Training Loss: 4.632175809301078, Validation Loss: 14.128605842590332\n",
      "Epoch [524/1500], Training Loss: 4.614860873599647, Validation Loss: 14.103184700012207\n",
      "Epoch [525/1500], Training Loss: 4.597740980992269, Validation Loss: 14.077879905700684\n",
      "Epoch [526/1500], Training Loss: 4.580771467718151, Validation Loss: 14.052674293518066\n",
      "Epoch [527/1500], Training Loss: 4.56400718286681, Validation Loss: 14.027668952941895\n",
      "Epoch [528/1500], Training Loss: 4.547442435424029, Validation Loss: 14.002867698669434\n",
      "Epoch [529/1500], Training Loss: 4.53105712597646, Validation Loss: 13.978466033935547\n",
      "Epoch [530/1500], Training Loss: 4.514840848638598, Validation Loss: 13.954133033752441\n",
      "Epoch [531/1500], Training Loss: 4.498782324454855, Validation Loss: 13.9301176071167\n",
      "Epoch [532/1500], Training Loss: 4.48291177798011, Validation Loss: 13.906181335449219\n",
      "Epoch [533/1500], Training Loss: 4.467211553687533, Validation Loss: 13.882718086242676\n",
      "Epoch [534/1500], Training Loss: 4.451681001582207, Validation Loss: 13.859453201293945\n",
      "Epoch [535/1500], Training Loss: 4.436346704072525, Validation Loss: 13.836475372314453\n",
      "Epoch [536/1500], Training Loss: 4.421190509594929, Validation Loss: 13.813746452331543\n",
      "Epoch [537/1500], Training Loss: 4.406199779478598, Validation Loss: 13.79116439819336\n",
      "Epoch [538/1500], Training Loss: 4.391354257565478, Validation Loss: 13.768917083740234\n",
      "Epoch [539/1500], Training Loss: 4.376725608915094, Validation Loss: 13.747018814086914\n",
      "Epoch [540/1500], Training Loss: 4.362267171659907, Validation Loss: 13.725390434265137\n",
      "Epoch [541/1500], Training Loss: 4.3479650276560395, Validation Loss: 13.70400619506836\n",
      "Epoch [542/1500], Training Loss: 4.333813284763318, Validation Loss: 13.682757377624512\n",
      "Epoch [543/1500], Training Loss: 4.319827277519737, Validation Loss: 13.662001609802246\n",
      "Epoch [544/1500], Training Loss: 4.3059889995934615, Validation Loss: 13.64138412475586\n",
      "Epoch [545/1500], Training Loss: 4.2922772685817305, Validation Loss: 13.621073722839355\n",
      "Epoch [546/1500], Training Loss: 4.278715471591638, Validation Loss: 13.601032257080078\n",
      "Epoch [547/1500], Training Loss: 4.265302279587514, Validation Loss: 13.58140754699707\n",
      "Epoch [548/1500], Training Loss: 4.252030085699333, Validation Loss: 13.562049865722656\n",
      "Epoch [549/1500], Training Loss: 4.238925975645588, Validation Loss: 13.543010711669922\n",
      "Epoch [550/1500], Training Loss: 4.22594182289487, Validation Loss: 13.524211883544922\n",
      "Epoch [551/1500], Training Loss: 4.213076375712583, Validation Loss: 13.505563735961914\n",
      "Epoch [552/1500], Training Loss: 4.200347119181302, Validation Loss: 13.487336158752441\n",
      "Epoch [553/1500], Training Loss: 4.187758196874626, Validation Loss: 13.469325065612793\n",
      "Epoch [554/1500], Training Loss: 4.175283325777199, Validation Loss: 13.45144271850586\n",
      "Epoch [555/1500], Training Loss: 4.162983741234133, Validation Loss: 13.43380355834961\n",
      "Epoch [556/1500], Training Loss: 4.150814484130753, Validation Loss: 13.416424751281738\n",
      "Epoch [557/1500], Training Loss: 4.138743026903128, Validation Loss: 13.399235725402832\n",
      "Epoch [558/1500], Training Loss: 4.126777793939266, Validation Loss: 13.382072448730469\n",
      "Epoch [559/1500], Training Loss: 4.114933122174003, Validation Loss: 13.365254402160645\n",
      "Epoch [560/1500], Training Loss: 4.103208556820872, Validation Loss: 13.348518371582031\n",
      "Epoch [561/1500], Training Loss: 4.091592736588127, Validation Loss: 13.332025527954102\n",
      "Epoch [562/1500], Training Loss: 4.080093082695533, Validation Loss: 13.315752029418945\n",
      "Epoch [563/1500], Training Loss: 4.0686921508103, Validation Loss: 13.299599647521973\n",
      "Epoch [564/1500], Training Loss: 4.057399789681682, Validation Loss: 13.283546447753906\n",
      "Epoch [565/1500], Training Loss: 4.046210390675494, Validation Loss: 13.267674446105957\n",
      "Epoch [566/1500], Training Loss: 4.035141245046667, Validation Loss: 13.251936912536621\n",
      "Epoch [567/1500], Training Loss: 4.024178323560696, Validation Loss: 13.236433982849121\n",
      "Epoch [568/1500], Training Loss: 4.013303822806471, Validation Loss: 13.221004486083984\n",
      "Epoch [569/1500], Training Loss: 4.0025070735441854, Validation Loss: 13.20561695098877\n",
      "Epoch [570/1500], Training Loss: 3.991801174788608, Validation Loss: 13.190459251403809\n",
      "Epoch [571/1500], Training Loss: 3.9811810375332857, Validation Loss: 13.175408363342285\n",
      "Epoch [572/1500], Training Loss: 3.9706510486274724, Validation Loss: 13.160606384277344\n",
      "Epoch [573/1500], Training Loss: 3.9602354828254343, Validation Loss: 13.146097183227539\n",
      "Epoch [574/1500], Training Loss: 3.9499089836703027, Validation Loss: 13.13154125213623\n",
      "Epoch [575/1500], Training Loss: 3.939659737152557, Validation Loss: 13.11711597442627\n",
      "Epoch [576/1500], Training Loss: 3.9294754109897045, Validation Loss: 13.102861404418945\n",
      "Epoch [577/1500], Training Loss: 3.919408708989969, Validation Loss: 13.088772773742676\n",
      "Epoch [578/1500], Training Loss: 3.9094281799615658, Validation Loss: 13.074766159057617\n",
      "Epoch [579/1500], Training Loss: 3.899518106679344, Validation Loss: 13.06075668334961\n",
      "Epoch [580/1500], Training Loss: 3.889661477514978, Validation Loss: 13.046879768371582\n",
      "Epoch [581/1500], Training Loss: 3.8798766317804447, Validation Loss: 13.033169746398926\n",
      "Epoch [582/1500], Training Loss: 3.870169058038992, Validation Loss: 13.01939582824707\n",
      "Epoch [583/1500], Training Loss: 3.8605455774091237, Validation Loss: 13.005751609802246\n",
      "Epoch [584/1500], Training Loss: 3.850983980179156, Validation Loss: 12.992362022399902\n",
      "Epoch [585/1500], Training Loss: 3.841476955799107, Validation Loss: 12.978952407836914\n",
      "Epoch [586/1500], Training Loss: 3.832041779611935, Validation Loss: 12.965760231018066\n",
      "Epoch [587/1500], Training Loss: 3.822670196959386, Validation Loss: 12.952672004699707\n",
      "Epoch [588/1500], Training Loss: 3.8133293828405357, Validation Loss: 12.939671516418457\n",
      "Epoch [589/1500], Training Loss: 3.8040636657267193, Validation Loss: 12.92685317993164\n",
      "Epoch [590/1500], Training Loss: 3.7948342850415044, Validation Loss: 12.9142427444458\n",
      "Epoch [591/1500], Training Loss: 3.7856610931386805, Validation Loss: 12.901697158813477\n",
      "Epoch [592/1500], Training Loss: 3.7765619805376427, Validation Loss: 12.88913345336914\n",
      "Epoch [593/1500], Training Loss: 3.7674888862327713, Validation Loss: 12.876856803894043\n",
      "Epoch [594/1500], Training Loss: 3.758474845713241, Validation Loss: 12.864866256713867\n",
      "Epoch [595/1500], Training Loss: 3.749509685646405, Validation Loss: 12.852973937988281\n",
      "Epoch [596/1500], Training Loss: 3.740587771022002, Validation Loss: 12.841100692749023\n",
      "Epoch [597/1500], Training Loss: 3.7317052556900254, Validation Loss: 12.82898998260498\n",
      "Epoch [598/1500], Training Loss: 3.7228884862162492, Validation Loss: 12.817058563232422\n",
      "Epoch [599/1500], Training Loss: 3.7140731833650356, Validation Loss: 12.804734230041504\n",
      "Epoch [600/1500], Training Loss: 3.7052700505310994, Validation Loss: 12.792023658752441\n",
      "Epoch [601/1500], Training Loss: 3.696530682547479, Validation Loss: 12.779126167297363\n",
      "Epoch [602/1500], Training Loss: 3.6878342664040225, Validation Loss: 12.765569686889648\n",
      "Epoch [603/1500], Training Loss: 3.6791927581416126, Validation Loss: 12.751699447631836\n",
      "Epoch [604/1500], Training Loss: 3.670605688412236, Validation Loss: 12.737308502197266\n",
      "Epoch [605/1500], Training Loss: 3.662069091176814, Validation Loss: 12.72203540802002\n",
      "Epoch [606/1500], Training Loss: 3.653621761516001, Validation Loss: 12.706390380859375\n",
      "Epoch [607/1500], Training Loss: 3.6452270809125604, Validation Loss: 12.689958572387695\n",
      "Epoch [608/1500], Training Loss: 3.6369136138781766, Validation Loss: 12.672924041748047\n",
      "Epoch [609/1500], Training Loss: 3.628658584670962, Validation Loss: 12.65565013885498\n",
      "Epoch [610/1500], Training Loss: 3.620416276724986, Validation Loss: 12.63793659210205\n",
      "Epoch [611/1500], Training Loss: 3.6122610169770963, Validation Loss: 12.619941711425781\n",
      "Epoch [612/1500], Training Loss: 3.6041817746883953, Validation Loss: 12.601855278015137\n",
      "Epoch [613/1500], Training Loss: 3.596148209830359, Validation Loss: 12.583636283874512\n",
      "Epoch [614/1500], Training Loss: 3.5882040094576095, Validation Loss: 12.565532684326172\n",
      "Epoch [615/1500], Training Loss: 3.580315947226703, Validation Loss: 12.547388076782227\n",
      "Epoch [616/1500], Training Loss: 3.572520351721851, Validation Loss: 12.529435157775879\n",
      "Epoch [617/1500], Training Loss: 3.5647787102964665, Validation Loss: 12.51176643371582\n",
      "Epoch [618/1500], Training Loss: 3.557089864152135, Validation Loss: 12.49427604675293\n",
      "Epoch [619/1500], Training Loss: 3.549490084873168, Validation Loss: 12.477072715759277\n",
      "Epoch [620/1500], Training Loss: 3.5419371062669254, Validation Loss: 12.460346221923828\n",
      "Epoch [621/1500], Training Loss: 3.5344352971782587, Validation Loss: 12.443862915039062\n",
      "Epoch [622/1500], Training Loss: 3.5269766403571507, Validation Loss: 12.427736282348633\n",
      "Epoch [623/1500], Training Loss: 3.519596638985225, Validation Loss: 12.412141799926758\n",
      "Epoch [624/1500], Training Loss: 3.5122449079596585, Validation Loss: 12.396883010864258\n",
      "Epoch [625/1500], Training Loss: 3.5049197080604158, Validation Loss: 12.382019996643066\n",
      "Epoch [626/1500], Training Loss: 3.497644398103188, Validation Loss: 12.367486000061035\n",
      "Epoch [627/1500], Training Loss: 3.490431515811433, Validation Loss: 12.353302955627441\n",
      "Epoch [628/1500], Training Loss: 3.483276243008652, Validation Loss: 12.339515686035156\n",
      "Epoch [629/1500], Training Loss: 3.4761572529994207, Validation Loss: 12.326127052307129\n",
      "Epoch [630/1500], Training Loss: 3.4690727876075904, Validation Loss: 12.313003540039062\n",
      "Epoch [631/1500], Training Loss: 3.462019623683435, Validation Loss: 12.300297737121582\n",
      "Epoch [632/1500], Training Loss: 3.4550173411030043, Validation Loss: 12.287883758544922\n",
      "Epoch [633/1500], Training Loss: 3.448066235282447, Validation Loss: 12.275854110717773\n",
      "Epoch [634/1500], Training Loss: 3.4411312613669827, Validation Loss: 12.264025688171387\n",
      "Epoch [635/1500], Training Loss: 3.434223231812206, Validation Loss: 12.252381324768066\n",
      "Epoch [636/1500], Training Loss: 3.4273753094169135, Validation Loss: 12.240944862365723\n",
      "Epoch [637/1500], Training Loss: 3.4205672932304383, Validation Loss: 12.229718208312988\n",
      "Epoch [638/1500], Training Loss: 3.413784235242098, Validation Loss: 12.21889877319336\n",
      "Epoch [639/1500], Training Loss: 3.407044876290382, Validation Loss: 12.208224296569824\n",
      "Epoch [640/1500], Training Loss: 3.4003455796602213, Validation Loss: 12.197806358337402\n",
      "Epoch [641/1500], Training Loss: 3.3936422948924587, Validation Loss: 12.187347412109375\n",
      "Epoch [642/1500], Training Loss: 3.38697164940554, Validation Loss: 12.17715072631836\n",
      "Epoch [643/1500], Training Loss: 3.380317628758701, Validation Loss: 12.166966438293457\n",
      "Epoch [644/1500], Training Loss: 3.3736847633199414, Validation Loss: 12.156904220581055\n",
      "Epoch [645/1500], Training Loss: 3.367076942361359, Validation Loss: 12.146892547607422\n",
      "Epoch [646/1500], Training Loss: 3.360509519852651, Validation Loss: 12.136666297912598\n",
      "Epoch [647/1500], Training Loss: 3.3539876159157815, Validation Loss: 12.126577377319336\n",
      "Epoch [648/1500], Training Loss: 3.3474745461770814, Validation Loss: 12.11639404296875\n",
      "Epoch [649/1500], Training Loss: 3.341001702195362, Validation Loss: 12.106040954589844\n",
      "Epoch [650/1500], Training Loss: 3.3345433500673574, Validation Loss: 12.095680236816406\n",
      "Epoch [651/1500], Training Loss: 3.328097374816688, Validation Loss: 12.085051536560059\n",
      "Epoch [652/1500], Training Loss: 3.321676999699963, Validation Loss: 12.074461936950684\n",
      "Epoch [653/1500], Training Loss: 3.3152853721866853, Validation Loss: 12.063721656799316\n",
      "Epoch [654/1500], Training Loss: 3.3088982611563003, Validation Loss: 12.052845001220703\n",
      "Epoch [655/1500], Training Loss: 3.3025382334844813, Validation Loss: 12.04176139831543\n",
      "Epoch [656/1500], Training Loss: 3.2961933358021707, Validation Loss: 12.030339241027832\n",
      "Epoch [657/1500], Training Loss: 3.2898645937453304, Validation Loss: 12.018780708312988\n",
      "Epoch [658/1500], Training Loss: 3.2835478328181336, Validation Loss: 12.007102966308594\n",
      "Epoch [659/1500], Training Loss: 3.2772677404605632, Validation Loss: 11.995250701904297\n",
      "Epoch [660/1500], Training Loss: 3.271005133408199, Validation Loss: 11.98332691192627\n",
      "Epoch [661/1500], Training Loss: 3.2647730119645537, Validation Loss: 11.971111297607422\n",
      "Epoch [662/1500], Training Loss: 3.258568512561204, Validation Loss: 11.95881462097168\n",
      "Epoch [663/1500], Training Loss: 3.2523921129526845, Validation Loss: 11.946324348449707\n",
      "Epoch [664/1500], Training Loss: 3.2462436322391066, Validation Loss: 11.933648109436035\n",
      "Epoch [665/1500], Training Loss: 3.2401196412319577, Validation Loss: 11.920844078063965\n",
      "Epoch [666/1500], Training Loss: 3.234017041509012, Validation Loss: 11.907805442810059\n",
      "Epoch [667/1500], Training Loss: 3.2279404219226446, Validation Loss: 11.894671440124512\n",
      "Epoch [668/1500], Training Loss: 3.221886786337245, Validation Loss: 11.881507873535156\n",
      "Epoch [669/1500], Training Loss: 3.215864843015181, Validation Loss: 11.868117332458496\n",
      "Epoch [670/1500], Training Loss: 3.2098615470660192, Validation Loss: 11.854785919189453\n",
      "Epoch [671/1500], Training Loss: 3.203893205645094, Validation Loss: 11.841449737548828\n",
      "Epoch [672/1500], Training Loss: 3.197960372152233, Validation Loss: 11.828072547912598\n",
      "Epoch [673/1500], Training Loss: 3.1920448181127545, Validation Loss: 11.814398765563965\n",
      "Epoch [674/1500], Training Loss: 3.186154997854811, Validation Loss: 11.800775527954102\n",
      "Epoch [675/1500], Training Loss: 3.1802917148927348, Validation Loss: 11.786911964416504\n",
      "Epoch [676/1500], Training Loss: 3.174446564414363, Validation Loss: 11.773059844970703\n",
      "Epoch [677/1500], Training Loss: 3.168635253776629, Validation Loss: 11.759130477905273\n",
      "Epoch [678/1500], Training Loss: 3.1628355913399453, Validation Loss: 11.74506950378418\n",
      "Epoch [679/1500], Training Loss: 3.157044839548852, Validation Loss: 11.730931282043457\n",
      "Epoch [680/1500], Training Loss: 3.151261630569962, Validation Loss: 11.716582298278809\n",
      "Epoch [681/1500], Training Loss: 3.1454822506565416, Validation Loss: 11.702275276184082\n",
      "Epoch [682/1500], Training Loss: 3.139718960482571, Validation Loss: 11.687870025634766\n",
      "Epoch [683/1500], Training Loss: 3.1339825160662493, Validation Loss: 11.673346519470215\n",
      "Epoch [684/1500], Training Loss: 3.128268654245373, Validation Loss: 11.658835411071777\n",
      "Epoch [685/1500], Training Loss: 3.122572919298222, Validation Loss: 11.644185066223145\n",
      "Epoch [686/1500], Training Loss: 3.1168714800022768, Validation Loss: 11.629406929016113\n",
      "Epoch [687/1500], Training Loss: 3.1111651736370476, Validation Loss: 11.614522933959961\n",
      "Epoch [688/1500], Training Loss: 3.1054662785289273, Validation Loss: 11.599699020385742\n",
      "Epoch [689/1500], Training Loss: 3.099778318815885, Validation Loss: 11.584565162658691\n",
      "Epoch [690/1500], Training Loss: 3.094098323780652, Validation Loss: 11.569235801696777\n",
      "Epoch [691/1500], Training Loss: 3.0884261501595627, Validation Loss: 11.553881645202637\n",
      "Epoch [692/1500], Training Loss: 3.082757345547418, Validation Loss: 11.538296699523926\n",
      "Epoch [693/1500], Training Loss: 3.0771058371018962, Validation Loss: 11.522631645202637\n",
      "Epoch [694/1500], Training Loss: 3.0714591516522534, Validation Loss: 11.506743431091309\n",
      "Epoch [695/1500], Training Loss: 3.06582424232532, Validation Loss: 11.490679740905762\n",
      "Epoch [696/1500], Training Loss: 3.0602046300665773, Validation Loss: 11.474441528320312\n",
      "Epoch [697/1500], Training Loss: 3.0545711171950334, Validation Loss: 11.457969665527344\n",
      "Epoch [698/1500], Training Loss: 3.0489385624660765, Validation Loss: 11.441130638122559\n",
      "Epoch [699/1500], Training Loss: 3.043312489577916, Validation Loss: 11.424041748046875\n",
      "Epoch [700/1500], Training Loss: 3.037689139304676, Validation Loss: 11.40664291381836\n",
      "Epoch [701/1500], Training Loss: 3.0320580002497897, Validation Loss: 11.388895988464355\n",
      "Epoch [702/1500], Training Loss: 3.0264517717141164, Validation Loss: 11.370850563049316\n",
      "Epoch [703/1500], Training Loss: 3.020821679473294, Validation Loss: 11.352458000183105\n",
      "Epoch [704/1500], Training Loss: 3.0151942131286265, Validation Loss: 11.333857536315918\n",
      "Epoch [705/1500], Training Loss: 3.0095498353086674, Validation Loss: 11.314974784851074\n",
      "Epoch [706/1500], Training Loss: 3.003898203845545, Validation Loss: 11.295681953430176\n",
      "Epoch [707/1500], Training Loss: 2.9982496253278517, Validation Loss: 11.276166915893555\n",
      "Epoch [708/1500], Training Loss: 2.9926013280288455, Validation Loss: 11.256197929382324\n",
      "Epoch [709/1500], Training Loss: 2.986966541566997, Validation Loss: 11.235952377319336\n",
      "Epoch [710/1500], Training Loss: 2.9813279465736318, Validation Loss: 11.215399742126465\n",
      "Epoch [711/1500], Training Loss: 2.9756936422668874, Validation Loss: 11.194472312927246\n",
      "Epoch [712/1500], Training Loss: 2.9700667935406573, Validation Loss: 11.173169136047363\n",
      "Epoch [713/1500], Training Loss: 2.964439938604502, Validation Loss: 11.151378631591797\n",
      "Epoch [714/1500], Training Loss: 2.958805356337419, Validation Loss: 11.12918758392334\n",
      "Epoch [715/1500], Training Loss: 2.9531440719333517, Validation Loss: 11.106661796569824\n",
      "Epoch [716/1500], Training Loss: 2.9474879537086833, Validation Loss: 11.083725929260254\n",
      "Epoch [717/1500], Training Loss: 2.941810883202101, Validation Loss: 11.060368537902832\n",
      "Epoch [718/1500], Training Loss: 2.9361480898347803, Validation Loss: 11.036620140075684\n",
      "Epoch [719/1500], Training Loss: 2.9304679242976155, Validation Loss: 11.012407302856445\n",
      "Epoch [720/1500], Training Loss: 2.924790787581139, Validation Loss: 10.987850189208984\n",
      "Epoch [721/1500], Training Loss: 2.9191037374405364, Validation Loss: 10.962959289550781\n",
      "Epoch [722/1500], Training Loss: 2.9134231467974927, Validation Loss: 10.937610626220703\n",
      "Epoch [723/1500], Training Loss: 2.9077370931646938, Validation Loss: 10.911748886108398\n",
      "Epoch [724/1500], Training Loss: 2.9020718539888337, Validation Loss: 10.88558292388916\n",
      "Epoch [725/1500], Training Loss: 2.896400322624817, Validation Loss: 10.85886001586914\n",
      "Epoch [726/1500], Training Loss: 2.8907323202172472, Validation Loss: 10.83169174194336\n",
      "Epoch [727/1500], Training Loss: 2.885073396254682, Validation Loss: 10.804189682006836\n",
      "Epoch [728/1500], Training Loss: 2.8794087610039854, Validation Loss: 10.776123046875\n",
      "Epoch [729/1500], Training Loss: 2.8737677102307027, Validation Loss: 10.747761726379395\n",
      "Epoch [730/1500], Training Loss: 2.868118139187763, Validation Loss: 10.718775749206543\n",
      "Epoch [731/1500], Training Loss: 2.862476804421715, Validation Loss: 10.689380645751953\n",
      "Epoch [732/1500], Training Loss: 2.8568630289956887, Validation Loss: 10.65958023071289\n",
      "Epoch [733/1500], Training Loss: 2.851245677654538, Validation Loss: 10.62938117980957\n",
      "Epoch [734/1500], Training Loss: 2.8456449089285427, Validation Loss: 10.598628044128418\n",
      "Epoch [735/1500], Training Loss: 2.840051220101513, Validation Loss: 10.567200660705566\n",
      "Epoch [736/1500], Training Loss: 2.83445844484358, Validation Loss: 10.535416603088379\n",
      "Epoch [737/1500], Training Loss: 2.828860128138808, Validation Loss: 10.503104209899902\n",
      "Epoch [738/1500], Training Loss: 2.8232788254696186, Validation Loss: 10.470329284667969\n",
      "Epoch [739/1500], Training Loss: 2.8177008277506217, Validation Loss: 10.437082290649414\n",
      "Epoch [740/1500], Training Loss: 2.8121399716402786, Validation Loss: 10.403291702270508\n",
      "Epoch [741/1500], Training Loss: 2.8065712867074892, Validation Loss: 10.369026184082031\n",
      "Epoch [742/1500], Training Loss: 2.8010260577514243, Validation Loss: 10.334259986877441\n",
      "Epoch [743/1500], Training Loss: 2.795485159290476, Validation Loss: 10.299101829528809\n",
      "Epoch [744/1500], Training Loss: 2.7899579011326114, Validation Loss: 10.263386726379395\n",
      "Epoch [745/1500], Training Loss: 2.784435226695425, Validation Loss: 10.227202415466309\n",
      "Epoch [746/1500], Training Loss: 2.778943583182917, Validation Loss: 10.190601348876953\n",
      "Epoch [747/1500], Training Loss: 2.773477233471203, Validation Loss: 10.153460502624512\n",
      "Epoch [748/1500], Training Loss: 2.768039845598751, Validation Loss: 10.115880012512207\n",
      "Epoch [749/1500], Training Loss: 2.7626111781475737, Validation Loss: 10.077742576599121\n",
      "Epoch [750/1500], Training Loss: 2.7572025784299794, Validation Loss: 10.039278030395508\n",
      "Epoch [751/1500], Training Loss: 2.7518268451652173, Validation Loss: 10.000387191772461\n",
      "Epoch [752/1500], Training Loss: 2.746480327491286, Validation Loss: 9.960951805114746\n",
      "Epoch [753/1500], Training Loss: 2.741150604625123, Validation Loss: 9.921196937561035\n",
      "Epoch [754/1500], Training Loss: 2.735853642860882, Validation Loss: 9.881257057189941\n",
      "Epoch [755/1500], Training Loss: 2.730588145953575, Validation Loss: 9.84095287322998\n",
      "Epoch [756/1500], Training Loss: 2.725357992533073, Validation Loss: 9.800363540649414\n",
      "Epoch [757/1500], Training Loss: 2.720166652305324, Validation Loss: 9.759617805480957\n",
      "Epoch [758/1500], Training Loss: 2.715006798392379, Validation Loss: 9.718814849853516\n",
      "Epoch [759/1500], Training Loss: 2.7098945323340238, Validation Loss: 9.678000450134277\n",
      "Epoch [760/1500], Training Loss: 2.7048294583662846, Validation Loss: 9.637392044067383\n",
      "Epoch [761/1500], Training Loss: 2.6998360793755216, Validation Loss: 9.596892356872559\n",
      "Epoch [762/1500], Training Loss: 2.6948984753100445, Validation Loss: 9.556583404541016\n",
      "Epoch [763/1500], Training Loss: 2.690010149584857, Validation Loss: 9.516510009765625\n",
      "Epoch [764/1500], Training Loss: 2.6851825914584864, Validation Loss: 9.476726531982422\n",
      "Epoch [765/1500], Training Loss: 2.68040886212047, Validation Loss: 9.437414169311523\n",
      "Epoch [766/1500], Training Loss: 2.675687074920322, Validation Loss: 9.398633003234863\n",
      "Epoch [767/1500], Training Loss: 2.671027535910775, Validation Loss: 9.360567092895508\n",
      "Epoch [768/1500], Training Loss: 2.666416330847742, Validation Loss: 9.323205947875977\n",
      "Epoch [769/1500], Training Loss: 2.661865027495195, Validation Loss: 9.286758422851562\n",
      "Epoch [770/1500], Training Loss: 2.65737266255796, Validation Loss: 9.251179695129395\n",
      "Epoch [771/1500], Training Loss: 2.6529343438520003, Validation Loss: 9.216753959655762\n",
      "Epoch [772/1500], Training Loss: 2.648553824742566, Validation Loss: 9.183636665344238\n",
      "Epoch [773/1500], Training Loss: 2.644242265713129, Validation Loss: 9.151763916015625\n",
      "Epoch [774/1500], Training Loss: 2.639988003794306, Validation Loss: 9.1212797164917\n",
      "Epoch [775/1500], Training Loss: 2.635811861963309, Validation Loss: 9.09223747253418\n",
      "Epoch [776/1500], Training Loss: 2.6316792161865514, Validation Loss: 9.064491271972656\n",
      "Epoch [777/1500], Training Loss: 2.627604458832918, Validation Loss: 9.038263320922852\n",
      "Epoch [778/1500], Training Loss: 2.6235676417009595, Validation Loss: 9.013381004333496\n",
      "Epoch [779/1500], Training Loss: 2.6195854880004914, Validation Loss: 8.9899263381958\n",
      "Epoch [780/1500], Training Loss: 2.61565505041627, Validation Loss: 8.967855453491211\n",
      "Epoch [781/1500], Training Loss: 2.6117664819366957, Validation Loss: 8.946986198425293\n",
      "Epoch [782/1500], Training Loss: 2.6079331186803074, Validation Loss: 8.927252769470215\n",
      "Epoch [783/1500], Training Loss: 2.6041264583794743, Validation Loss: 8.908529281616211\n",
      "Epoch [784/1500], Training Loss: 2.600343938037434, Validation Loss: 8.890759468078613\n",
      "Epoch [785/1500], Training Loss: 2.596583475008874, Validation Loss: 8.873710632324219\n",
      "Epoch [786/1500], Training Loss: 2.592831922983317, Validation Loss: 8.857368469238281\n",
      "Epoch [787/1500], Training Loss: 2.5891018421474725, Validation Loss: 8.841474533081055\n",
      "Epoch [788/1500], Training Loss: 2.5853657707534214, Validation Loss: 8.826086044311523\n",
      "Epoch [789/1500], Training Loss: 2.581645717978501, Validation Loss: 8.810964584350586\n",
      "Epoch [790/1500], Training Loss: 2.5779403691076075, Validation Loss: 8.796120643615723\n",
      "Epoch [791/1500], Training Loss: 2.574220882505485, Validation Loss: 8.781344413757324\n",
      "Epoch [792/1500], Training Loss: 2.57048700133661, Validation Loss: 8.766593933105469\n",
      "Epoch [793/1500], Training Loss: 2.566731888044516, Validation Loss: 8.75190544128418\n",
      "Epoch [794/1500], Training Loss: 2.562971978972651, Validation Loss: 8.737197875976562\n",
      "Epoch [795/1500], Training Loss: 2.559194202072655, Validation Loss: 8.722405433654785\n",
      "Epoch [796/1500], Training Loss: 2.5554075817809165, Validation Loss: 8.707374572753906\n",
      "Epoch [797/1500], Training Loss: 2.551603915592799, Validation Loss: 8.692313194274902\n",
      "Epoch [798/1500], Training Loss: 2.5477650627150656, Validation Loss: 8.67702579498291\n",
      "Epoch [799/1500], Training Loss: 2.5439054163993244, Validation Loss: 8.66140079498291\n",
      "Epoch [800/1500], Training Loss: 2.5400134844561135, Validation Loss: 8.645529747009277\n",
      "Epoch [801/1500], Training Loss: 2.5361108719735617, Validation Loss: 8.629400253295898\n",
      "Epoch [802/1500], Training Loss: 2.5321836779846185, Validation Loss: 8.613037109375\n",
      "Epoch [803/1500], Training Loss: 2.528226288567, Validation Loss: 8.596358299255371\n",
      "Epoch [804/1500], Training Loss: 2.524237480732483, Validation Loss: 8.579418182373047\n",
      "Epoch [805/1500], Training Loss: 2.5202225323164757, Validation Loss: 8.562117576599121\n",
      "Epoch [806/1500], Training Loss: 2.5161888470917413, Validation Loss: 8.544560432434082\n",
      "Epoch [807/1500], Training Loss: 2.512120640826041, Validation Loss: 8.526630401611328\n",
      "Epoch [808/1500], Training Loss: 2.5080354976196575, Validation Loss: 8.508334159851074\n",
      "Epoch [809/1500], Training Loss: 2.5039127632134233, Validation Loss: 8.489644050598145\n",
      "Epoch [810/1500], Training Loss: 2.4997637262609564, Validation Loss: 8.470569610595703\n",
      "Epoch [811/1500], Training Loss: 2.4955935039381423, Validation Loss: 8.45109748840332\n",
      "Epoch [812/1500], Training Loss: 2.4913913436948327, Validation Loss: 8.431318283081055\n",
      "Epoch [813/1500], Training Loss: 2.4871677591148, Validation Loss: 8.4112548828125\n",
      "Epoch [814/1500], Training Loss: 2.482930086124941, Validation Loss: 8.390827178955078\n",
      "Epoch [815/1500], Training Loss: 2.4786780627082106, Validation Loss: 8.370070457458496\n",
      "Epoch [816/1500], Training Loss: 2.4744088153653454, Validation Loss: 8.348875045776367\n",
      "Epoch [817/1500], Training Loss: 2.470122518055829, Validation Loss: 8.327371597290039\n",
      "Epoch [818/1500], Training Loss: 2.4658245077062975, Validation Loss: 8.305486679077148\n",
      "Epoch [819/1500], Training Loss: 2.4614999427805326, Validation Loss: 8.283202171325684\n",
      "Epoch [820/1500], Training Loss: 2.4571666580689295, Validation Loss: 8.26064682006836\n",
      "Epoch [821/1500], Training Loss: 2.452819626928272, Validation Loss: 8.237839698791504\n",
      "Epoch [822/1500], Training Loss: 2.4484776556116894, Validation Loss: 8.214780807495117\n",
      "Epoch [823/1500], Training Loss: 2.4441210041650416, Validation Loss: 8.191352844238281\n",
      "Epoch [824/1500], Training Loss: 2.439754587165855, Validation Loss: 8.167688369750977\n",
      "Epoch [825/1500], Training Loss: 2.4353950965724254, Validation Loss: 8.1437406539917\n",
      "Epoch [826/1500], Training Loss: 2.4310242278713576, Validation Loss: 8.11959457397461\n",
      "Epoch [827/1500], Training Loss: 2.4266449469754288, Validation Loss: 8.095240592956543\n",
      "Epoch [828/1500], Training Loss: 2.4222654224146263, Validation Loss: 8.070560455322266\n",
      "Epoch [829/1500], Training Loss: 2.417869270153923, Validation Loss: 8.045737266540527\n",
      "Epoch [830/1500], Training Loss: 2.4134782946434505, Validation Loss: 8.020660400390625\n",
      "Epoch [831/1500], Training Loss: 2.4090778747228296, Validation Loss: 7.995302200317383\n",
      "Epoch [832/1500], Training Loss: 2.4046600763690913, Validation Loss: 7.969762325286865\n",
      "Epoch [833/1500], Training Loss: 2.400227515156309, Validation Loss: 7.944003105163574\n",
      "Epoch [834/1500], Training Loss: 2.395793379833924, Validation Loss: 7.918088912963867\n",
      "Epoch [835/1500], Training Loss: 2.391325511898289, Validation Loss: 7.8920392990112305\n",
      "Epoch [836/1500], Training Loss: 2.386849841550004, Validation Loss: 7.865864276885986\n",
      "Epoch [837/1500], Training Loss: 2.382375110504372, Validation Loss: 7.8395094871521\n",
      "Epoch [838/1500], Training Loss: 2.377865770618956, Validation Loss: 7.81284236907959\n",
      "Epoch [839/1500], Training Loss: 2.373349249375973, Validation Loss: 7.785932540893555\n",
      "Epoch [840/1500], Training Loss: 2.368820058817248, Validation Loss: 7.758915901184082\n",
      "Epoch [841/1500], Training Loss: 2.3642599841428296, Validation Loss: 7.731553077697754\n",
      "Epoch [842/1500], Training Loss: 2.359672997268808, Validation Loss: 7.703984260559082\n",
      "Epoch [843/1500], Training Loss: 2.355061105061536, Validation Loss: 7.676021575927734\n",
      "Epoch [844/1500], Training Loss: 2.3503988598846473, Validation Loss: 7.647609710693359\n",
      "Epoch [845/1500], Training Loss: 2.34571334695709, Validation Loss: 7.619095325469971\n",
      "Epoch [846/1500], Training Loss: 2.3409842346424847, Validation Loss: 7.590203285217285\n",
      "Epoch [847/1500], Training Loss: 2.336224743056074, Validation Loss: 7.56104850769043\n",
      "Epoch [848/1500], Training Loss: 2.3313990031060445, Validation Loss: 7.531403064727783\n",
      "Epoch [849/1500], Training Loss: 2.3265273648549423, Validation Loss: 7.501503944396973\n",
      "Epoch [850/1500], Training Loss: 2.3216018277623505, Validation Loss: 7.47117280960083\n",
      "Epoch [851/1500], Training Loss: 2.316629220366991, Validation Loss: 7.440303802490234\n",
      "Epoch [852/1500], Training Loss: 2.311607285735207, Validation Loss: 7.409112453460693\n",
      "Epoch [853/1500], Training Loss: 2.3065109068544563, Validation Loss: 7.377476215362549\n",
      "Epoch [854/1500], Training Loss: 2.3013389484712565, Validation Loss: 7.345417022705078\n",
      "Epoch [855/1500], Training Loss: 2.296070304579523, Validation Loss: 7.312654495239258\n",
      "Epoch [856/1500], Training Loss: 2.2907288814290485, Validation Loss: 7.279401779174805\n",
      "Epoch [857/1500], Training Loss: 2.2853113755467707, Validation Loss: 7.245690822601318\n",
      "Epoch [858/1500], Training Loss: 2.2797991838853893, Validation Loss: 7.211359024047852\n",
      "Epoch [859/1500], Training Loss: 2.274180172804532, Validation Loss: 7.1762871742248535\n",
      "Epoch [860/1500], Training Loss: 2.268460706902025, Validation Loss: 7.140463352203369\n",
      "Epoch [861/1500], Training Loss: 2.2626430840375944, Validation Loss: 7.103912353515625\n",
      "Epoch [862/1500], Training Loss: 2.256711506269472, Validation Loss: 7.066518306732178\n",
      "Epoch [863/1500], Training Loss: 2.2506997031229323, Validation Loss: 7.0284013748168945\n",
      "Epoch [864/1500], Training Loss: 2.2445723482447764, Validation Loss: 6.989363193511963\n",
      "Epoch [865/1500], Training Loss: 2.2383441134819466, Validation Loss: 6.949710845947266\n",
      "Epoch [866/1500], Training Loss: 2.2320127593563543, Validation Loss: 6.909643650054932\n",
      "Epoch [867/1500], Training Loss: 2.2255782479893433, Validation Loss: 6.868936538696289\n",
      "Epoch [868/1500], Training Loss: 2.2190417689878053, Validation Loss: 6.828149795532227\n",
      "Epoch [869/1500], Training Loss: 2.212436269439793, Validation Loss: 6.786978244781494\n",
      "Epoch [870/1500], Training Loss: 2.2057369648320595, Validation Loss: 6.745664119720459\n",
      "Epoch [871/1500], Training Loss: 2.1989843041828934, Validation Loss: 6.704486846923828\n",
      "Epoch [872/1500], Training Loss: 2.192163833332255, Validation Loss: 6.66335391998291\n",
      "Epoch [873/1500], Training Loss: 2.1852810813329486, Validation Loss: 6.622326850891113\n",
      "Epoch [874/1500], Training Loss: 2.1783293590056716, Validation Loss: 6.581735610961914\n",
      "Epoch [875/1500], Training Loss: 2.1713361225727237, Validation Loss: 6.541455268859863\n",
      "Epoch [876/1500], Training Loss: 2.1643176684830685, Validation Loss: 6.50152587890625\n",
      "Epoch [877/1500], Training Loss: 2.157246240617097, Validation Loss: 6.4620137214660645\n",
      "Epoch [878/1500], Training Loss: 2.1501430460940028, Validation Loss: 6.422774314880371\n",
      "Epoch [879/1500], Training Loss: 2.1429942910312114, Validation Loss: 6.383882999420166\n",
      "Epoch [880/1500], Training Loss: 2.13580318442848, Validation Loss: 6.34552526473999\n",
      "Epoch [881/1500], Training Loss: 2.128588055042373, Validation Loss: 6.307346820831299\n",
      "Epoch [882/1500], Training Loss: 2.1213304244269233, Validation Loss: 6.269655704498291\n",
      "Epoch [883/1500], Training Loss: 2.1140624327924695, Validation Loss: 6.2322587966918945\n",
      "Epoch [884/1500], Training Loss: 2.10676997056163, Validation Loss: 6.195000171661377\n",
      "Epoch [885/1500], Training Loss: 2.0994535234550225, Validation Loss: 6.1580281257629395\n",
      "Epoch [886/1500], Training Loss: 2.0920975129001382, Validation Loss: 6.121412754058838\n",
      "Epoch [887/1500], Training Loss: 2.084713237758917, Validation Loss: 6.084995269775391\n",
      "Epoch [888/1500], Training Loss: 2.0773006311955746, Validation Loss: 6.048681735992432\n",
      "Epoch [889/1500], Training Loss: 2.06987270705576, Validation Loss: 6.01250696182251\n",
      "Epoch [890/1500], Training Loss: 2.0624124493851634, Validation Loss: 5.976747989654541\n",
      "Epoch [891/1500], Training Loss: 2.0549219886286814, Validation Loss: 5.940995693206787\n",
      "Epoch [892/1500], Training Loss: 2.047396688898697, Validation Loss: 5.905489921569824\n",
      "Epoch [893/1500], Training Loss: 2.039854993512829, Validation Loss: 5.870125770568848\n",
      "Epoch [894/1500], Training Loss: 2.032286560040368, Validation Loss: 5.8349199295043945\n",
      "Epoch [895/1500], Training Loss: 2.0247081718380247, Validation Loss: 5.7998247146606445\n",
      "Epoch [896/1500], Training Loss: 2.0171194132120687, Validation Loss: 5.765057563781738\n",
      "Epoch [897/1500], Training Loss: 2.0095125213236535, Validation Loss: 5.730377197265625\n",
      "Epoch [898/1500], Training Loss: 2.0018861832710404, Validation Loss: 5.695836544036865\n",
      "Epoch [899/1500], Training Loss: 1.9942495562465001, Validation Loss: 5.661798477172852\n",
      "Epoch [900/1500], Training Loss: 1.986605332250394, Validation Loss: 5.627818584442139\n",
      "Epoch [901/1500], Training Loss: 1.9789473746481732, Validation Loss: 5.59437894821167\n",
      "Epoch [902/1500], Training Loss: 1.9712987728501463, Validation Loss: 5.561251163482666\n",
      "Epoch [903/1500], Training Loss: 1.9636557707617617, Validation Loss: 5.528223514556885\n",
      "Epoch [904/1500], Training Loss: 1.95600373001445, Validation Loss: 5.495661735534668\n",
      "Epoch [905/1500], Training Loss: 1.9483787137867932, Validation Loss: 5.463244915008545\n",
      "Epoch [906/1500], Training Loss: 1.9407534712312964, Validation Loss: 5.431222438812256\n",
      "Epoch [907/1500], Training Loss: 1.9331482371006348, Validation Loss: 5.399615287780762\n",
      "Epoch [908/1500], Training Loss: 1.9255489345110555, Validation Loss: 5.368196964263916\n",
      "Epoch [909/1500], Training Loss: 1.9179747388669504, Validation Loss: 5.337436199188232\n",
      "Epoch [910/1500], Training Loss: 1.9104159566689998, Validation Loss: 5.30695104598999\n",
      "Epoch [911/1500], Training Loss: 1.902884496990659, Validation Loss: 5.276939392089844\n",
      "Epoch [912/1500], Training Loss: 1.8953803715292499, Validation Loss: 5.247387886047363\n",
      "Epoch [913/1500], Training Loss: 1.887918350595198, Validation Loss: 5.21851921081543\n",
      "Epoch [914/1500], Training Loss: 1.8804903244294418, Validation Loss: 5.190017223358154\n",
      "Epoch [915/1500], Training Loss: 1.8730930116188005, Validation Loss: 5.1621012687683105\n",
      "Epoch [916/1500], Training Loss: 1.865741261173465, Validation Loss: 5.134642601013184\n",
      "Epoch [917/1500], Training Loss: 1.858430855821446, Validation Loss: 5.107967853546143\n",
      "Epoch [918/1500], Training Loss: 1.8511779849894805, Validation Loss: 5.0818610191345215\n",
      "Epoch [919/1500], Training Loss: 1.843979589214775, Validation Loss: 5.0560688972473145\n",
      "Epoch [920/1500], Training Loss: 1.8368269659026297, Validation Loss: 5.031063079833984\n",
      "Epoch [921/1500], Training Loss: 1.8297310440297854, Validation Loss: 5.006661415100098\n",
      "Epoch [922/1500], Training Loss: 1.8227178433053886, Validation Loss: 4.983061790466309\n",
      "Epoch [923/1500], Training Loss: 1.8157691233072624, Validation Loss: 4.9601850509643555\n",
      "Epoch [924/1500], Training Loss: 1.8089019469198757, Validation Loss: 4.937992572784424\n",
      "Epoch [925/1500], Training Loss: 1.8021064429916054, Validation Loss: 4.916298866271973\n",
      "Epoch [926/1500], Training Loss: 1.795388318533738, Validation Loss: 4.895533084869385\n",
      "Epoch [927/1500], Training Loss: 1.7887497223181759, Validation Loss: 4.875460624694824\n",
      "Epoch [928/1500], Training Loss: 1.7821941454412138, Validation Loss: 4.855962753295898\n",
      "Epoch [929/1500], Training Loss: 1.7757376337310875, Validation Loss: 4.837327480316162\n",
      "Epoch [930/1500], Training Loss: 1.769351464019698, Validation Loss: 4.8192901611328125\n",
      "Epoch [931/1500], Training Loss: 1.7630656359789127, Validation Loss: 4.80171537399292\n",
      "Epoch [932/1500], Training Loss: 1.7568824726804426, Validation Loss: 4.78492546081543\n",
      "Epoch [933/1500], Training Loss: 1.7507931543316664, Validation Loss: 4.768767833709717\n",
      "Epoch [934/1500], Training Loss: 1.7448047836758478, Validation Loss: 4.753292083740234\n",
      "Epoch [935/1500], Training Loss: 1.7389225542188642, Validation Loss: 4.738391399383545\n",
      "Epoch [936/1500], Training Loss: 1.7331319311459832, Validation Loss: 4.724367141723633\n",
      "Epoch [937/1500], Training Loss: 1.727470743423296, Validation Loss: 4.7109174728393555\n",
      "Epoch [938/1500], Training Loss: 1.7219042794528616, Validation Loss: 4.698084831237793\n",
      "Epoch [939/1500], Training Loss: 1.7164386154911901, Validation Loss: 4.686016082763672\n",
      "Epoch [940/1500], Training Loss: 1.7110816419197732, Validation Loss: 4.674405574798584\n",
      "Epoch [941/1500], Training Loss: 1.7058185176299718, Validation Loss: 4.6636528968811035\n",
      "Epoch [942/1500], Training Loss: 1.700674100593906, Validation Loss: 4.653419017791748\n",
      "Epoch [943/1500], Training Loss: 1.6956467936294473, Validation Loss: 4.643566131591797\n",
      "Epoch [944/1500], Training Loss: 1.690706996170958, Validation Loss: 4.634466171264648\n",
      "Epoch [945/1500], Training Loss: 1.6858825094801728, Validation Loss: 4.62570858001709\n",
      "Epoch [946/1500], Training Loss: 1.6811693722677805, Validation Loss: 4.617271900177002\n",
      "Epoch [947/1500], Training Loss: 1.676555217770103, Validation Loss: 4.609396934509277\n",
      "Epoch [948/1500], Training Loss: 1.6720392655483194, Validation Loss: 4.602094650268555\n",
      "Epoch [949/1500], Training Loss: 1.6676359317320837, Validation Loss: 4.594996452331543\n",
      "Epoch [950/1500], Training Loss: 1.6633332539783878, Validation Loss: 4.588148593902588\n",
      "Epoch [951/1500], Training Loss: 1.6591245286731109, Validation Loss: 4.581851482391357\n",
      "Epoch [952/1500], Training Loss: 1.6550114641661569, Validation Loss: 4.5757646560668945\n",
      "Epoch [953/1500], Training Loss: 1.6510013721107761, Validation Loss: 4.57012414932251\n",
      "Epoch [954/1500], Training Loss: 1.6470983695531762, Validation Loss: 4.5649003982543945\n",
      "Epoch [955/1500], Training Loss: 1.643289464385152, Validation Loss: 4.560029983520508\n",
      "Epoch [956/1500], Training Loss: 1.639565754627156, Validation Loss: 4.555274963378906\n",
      "Epoch [957/1500], Training Loss: 1.6359326219892376, Validation Loss: 4.550582408905029\n",
      "Epoch [958/1500], Training Loss: 1.6323792337948402, Validation Loss: 4.546115875244141\n",
      "Epoch [959/1500], Training Loss: 1.628910832006444, Validation Loss: 4.541884422302246\n",
      "Epoch [960/1500], Training Loss: 1.6255265307216502, Validation Loss: 4.537693500518799\n",
      "Epoch [961/1500], Training Loss: 1.6222171963253011, Validation Loss: 4.533886909484863\n",
      "Epoch [962/1500], Training Loss: 1.6189902553838436, Validation Loss: 4.530067443847656\n",
      "Epoch [963/1500], Training Loss: 1.6158575800203565, Validation Loss: 4.526098251342773\n",
      "Epoch [964/1500], Training Loss: 1.6127895591546706, Validation Loss: 4.522533893585205\n",
      "Epoch [965/1500], Training Loss: 1.6097953071776152, Validation Loss: 4.519049167633057\n",
      "Epoch [966/1500], Training Loss: 1.6068679389828624, Validation Loss: 4.515313148498535\n",
      "Epoch [967/1500], Training Loss: 1.6040161917434799, Validation Loss: 4.5120158195495605\n",
      "Epoch [968/1500], Training Loss: 1.6012343889158738, Validation Loss: 4.5085577964782715\n",
      "Epoch [969/1500], Training Loss: 1.5985041314057928, Validation Loss: 4.505139350891113\n",
      "Epoch [970/1500], Training Loss: 1.5958495064109974, Validation Loss: 4.501422882080078\n",
      "Epoch [971/1500], Training Loss: 1.5932640498380688, Validation Loss: 4.497983455657959\n",
      "Epoch [972/1500], Training Loss: 1.5907231878601757, Validation Loss: 4.494641304016113\n",
      "Epoch [973/1500], Training Loss: 1.5882394363849321, Validation Loss: 4.490954875946045\n",
      "Epoch [974/1500], Training Loss: 1.5858049177857847, Validation Loss: 4.487283229827881\n",
      "Epoch [975/1500], Training Loss: 1.5834298779842264, Validation Loss: 4.483550071716309\n",
      "Epoch [976/1500], Training Loss: 1.5810994582784925, Validation Loss: 4.480095386505127\n",
      "Epoch [977/1500], Training Loss: 1.5788263347314888, Validation Loss: 4.476533889770508\n",
      "Epoch [978/1500], Training Loss: 1.5765953349678614, Validation Loss: 4.473030090332031\n",
      "Epoch [979/1500], Training Loss: 1.5744092987915486, Validation Loss: 4.4690937995910645\n",
      "Epoch [980/1500], Training Loss: 1.5722659428324133, Validation Loss: 4.464980602264404\n",
      "Epoch [981/1500], Training Loss: 1.570168828714879, Validation Loss: 4.460989475250244\n",
      "Epoch [982/1500], Training Loss: 1.5681216497168673, Validation Loss: 4.4570183753967285\n",
      "Epoch [983/1500], Training Loss: 1.5661162442657761, Validation Loss: 4.452945709228516\n",
      "Epoch [984/1500], Training Loss: 1.564147721781314, Validation Loss: 4.448877811431885\n",
      "Epoch [985/1500], Training Loss: 1.562215017757454, Validation Loss: 4.4446940422058105\n",
      "Epoch [986/1500], Training Loss: 1.560317100860237, Validation Loss: 4.4405083656311035\n",
      "Epoch [987/1500], Training Loss: 1.558448556210634, Validation Loss: 4.43621301651001\n",
      "Epoch [988/1500], Training Loss: 1.5566309311826947, Validation Loss: 4.431935787200928\n",
      "Epoch [989/1500], Training Loss: 1.5548534503855027, Validation Loss: 4.427605152130127\n",
      "Epoch [990/1500], Training Loss: 1.5531099487267483, Validation Loss: 4.423061847686768\n",
      "Epoch [991/1500], Training Loss: 1.5513942007453954, Validation Loss: 4.418613433837891\n",
      "Epoch [992/1500], Training Loss: 1.5497078292471684, Validation Loss: 4.4140496253967285\n",
      "Epoch [993/1500], Training Loss: 1.5480549307219347, Validation Loss: 4.409549713134766\n",
      "Epoch [994/1500], Training Loss: 1.5464339611277482, Validation Loss: 4.4049763679504395\n",
      "Epoch [995/1500], Training Loss: 1.5448398062519524, Validation Loss: 4.400332927703857\n",
      "Epoch [996/1500], Training Loss: 1.54326495500317, Validation Loss: 4.395660877227783\n",
      "Epoch [997/1500], Training Loss: 1.541729914511291, Validation Loss: 4.390964508056641\n",
      "Epoch [998/1500], Training Loss: 1.5402173330071094, Validation Loss: 4.3862175941467285\n",
      "Epoch [999/1500], Training Loss: 1.5387266545077591, Validation Loss: 4.38167142868042\n",
      "Epoch [1000/1500], Training Loss: 1.53727306739758, Validation Loss: 4.377025604248047\n",
      "Epoch [1001/1500], Training Loss: 1.5358480074753436, Validation Loss: 4.372495651245117\n",
      "Epoch [1002/1500], Training Loss: 1.5344331094890657, Validation Loss: 4.367763042449951\n",
      "Epoch [1003/1500], Training Loss: 1.5330413580628242, Validation Loss: 4.363202095031738\n",
      "Epoch [1004/1500], Training Loss: 1.5316739847467777, Validation Loss: 4.358551502227783\n",
      "Epoch [1005/1500], Training Loss: 1.5303299493021247, Validation Loss: 4.3541951179504395\n",
      "Epoch [1006/1500], Training Loss: 1.5290086554192035, Validation Loss: 4.34978723526001\n",
      "Epoch [1007/1500], Training Loss: 1.5277132090522838, Validation Loss: 4.345324993133545\n",
      "Epoch [1008/1500], Training Loss: 1.5264377082618168, Validation Loss: 4.341063022613525\n",
      "Epoch [1009/1500], Training Loss: 1.525185877822915, Validation Loss: 4.337091445922852\n",
      "Epoch [1010/1500], Training Loss: 1.5239441965103044, Validation Loss: 4.332913398742676\n",
      "Epoch [1011/1500], Training Loss: 1.5227229929839157, Validation Loss: 4.329076290130615\n",
      "Epoch [1012/1500], Training Loss: 1.5215257206275614, Validation Loss: 4.3253374099731445\n",
      "Epoch [1013/1500], Training Loss: 1.520351624309231, Validation Loss: 4.321734428405762\n",
      "Epoch [1014/1500], Training Loss: 1.5192058833370334, Validation Loss: 4.318314552307129\n",
      "Epoch [1015/1500], Training Loss: 1.5180836336670769, Validation Loss: 4.314957618713379\n",
      "Epoch [1016/1500], Training Loss: 1.5169850023285758, Validation Loss: 4.311573505401611\n",
      "Epoch [1017/1500], Training Loss: 1.515913961973689, Validation Loss: 4.308350563049316\n",
      "Epoch [1018/1500], Training Loss: 1.5148641637890163, Validation Loss: 4.305225849151611\n",
      "Epoch [1019/1500], Training Loss: 1.5138416488750355, Validation Loss: 4.302433967590332\n",
      "Epoch [1020/1500], Training Loss: 1.5128433715974607, Validation Loss: 4.299532413482666\n",
      "Epoch [1021/1500], Training Loss: 1.5118623247717704, Validation Loss: 4.296753406524658\n",
      "Epoch [1022/1500], Training Loss: 1.5109008653670912, Validation Loss: 4.2942795753479\n",
      "Epoch [1023/1500], Training Loss: 1.5099622261121761, Validation Loss: 4.291651248931885\n",
      "Epoch [1024/1500], Training Loss: 1.5090505606613123, Validation Loss: 4.289353847503662\n",
      "Epoch [1025/1500], Training Loss: 1.5081561489341486, Validation Loss: 4.287032604217529\n",
      "Epoch [1026/1500], Training Loss: 1.5072833710103712, Validation Loss: 4.284892559051514\n",
      "Epoch [1027/1500], Training Loss: 1.5064245736061532, Validation Loss: 4.282597064971924\n",
      "Epoch [1028/1500], Training Loss: 1.5055987106587077, Validation Loss: 4.280717372894287\n",
      "Epoch [1029/1500], Training Loss: 1.5048004925610012, Validation Loss: 4.279017448425293\n",
      "Epoch [1030/1500], Training Loss: 1.504020317601262, Validation Loss: 4.277410984039307\n",
      "Epoch [1031/1500], Training Loss: 1.5032719863847108, Validation Loss: 4.275875568389893\n",
      "Epoch [1032/1500], Training Loss: 1.5025581688341876, Validation Loss: 4.274655342102051\n",
      "Epoch [1033/1500], Training Loss: 1.501866469680033, Validation Loss: 4.273399353027344\n",
      "Epoch [1034/1500], Training Loss: 1.5011976569410548, Validation Loss: 4.27219820022583\n",
      "Epoch [1035/1500], Training Loss: 1.5005569640603085, Validation Loss: 4.271226406097412\n",
      "Epoch [1036/1500], Training Loss: 1.4999477089668098, Validation Loss: 4.270297527313232\n",
      "Epoch [1037/1500], Training Loss: 1.4993577405878677, Validation Loss: 4.269476413726807\n",
      "Epoch [1038/1500], Training Loss: 1.4987982804473972, Validation Loss: 4.268871307373047\n",
      "Epoch [1039/1500], Training Loss: 1.4982702518388251, Validation Loss: 4.2682013511657715\n",
      "Epoch [1040/1500], Training Loss: 1.497754375400879, Validation Loss: 4.2677154541015625\n",
      "Epoch [1041/1500], Training Loss: 1.4972664415242392, Validation Loss: 4.2672553062438965\n",
      "Epoch [1042/1500], Training Loss: 1.4968037227865263, Validation Loss: 4.2670817375183105\n",
      "Epoch [1043/1500], Training Loss: 1.4963724727748224, Validation Loss: 4.267079830169678\n",
      "Epoch [1044/1500], Training Loss: 1.4959577316907995, Validation Loss: 4.2673444747924805\n",
      "Epoch [1045/1500], Training Loss: 1.4955628598717334, Validation Loss: 4.26753568649292\n",
      "Epoch [1046/1500], Training Loss: 1.4951973196423052, Validation Loss: 4.2679972648620605\n",
      "Epoch [1047/1500], Training Loss: 1.4948518771003811, Validation Loss: 4.268592834472656\n",
      "Epoch [1048/1500], Training Loss: 1.4945384484055826, Validation Loss: 4.269099235534668\n",
      "Epoch [1049/1500], Training Loss: 1.4942521918639313, Validation Loss: 4.269753932952881\n",
      "Epoch [1050/1500], Training Loss: 1.4939775589954525, Validation Loss: 4.270659923553467\n",
      "Epoch [1051/1500], Training Loss: 1.4937413656452383, Validation Loss: 4.271617412567139\n",
      "Epoch [1052/1500], Training Loss: 1.4935169181005978, Validation Loss: 4.272566318511963\n",
      "Early stopping at epoch 1052\n",
      "Final Test Loss: 4.418914318084717\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "input_sizes = [7]\n",
    "hidden_sizes = [4]\n",
    "num_layers_list = [1]\n",
    "learning_rates = [0.0003]\n",
    "window_sizes = [5]\n",
    "\n",
    "num_epochs = 1500\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility to achieve reproducibility in your PyTorch script\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Additional steps to ensure determinism if needed !!!!!!!!!!!!!!\n",
    "\n",
    "# ensures that cuDNN (CUDA Deep Neural Network library) will always produce the same results given the same input\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# select the best algorithm for your input data. which can lead to faster execution times.\n",
    "# different algorithms may be chosen even with the same input\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine hyperparameters into a list of tuples\n",
    "hyperparameter_combinations = list(itertools.product(input_sizes, hidden_sizes, num_layers_list, learning_rates, window_sizes))\n",
    "\n",
    "# Walk-forward validation training with sliding window for each hyperparameter combination\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    input_size, hidden_size, num_layers, learning_rate, window_size = hyperparams\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, window_size={window_size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, learning_rate, window_size)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(len(x_train_tensor)):\n",
    "            window_end = min(i + window_size, len(x_train_tensor))\n",
    "            inputs = x_train_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_train_tensor[window_end - 1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(x_val_tensor)):\n",
    "                window_end = min(i + window_size, len(x_val_tensor))\n",
    "                inputs = x_val_tensor[i:window_end].unsqueeze(0)\n",
    "                labels = y_val_tensor[window_end - 1]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(x_train_tensor)}, Validation Loss: {val_loss / len(x_val_tensor)}')\n",
    "\n",
    "       # Calculate test loss after training is complete\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            window_end = min(i + window_size, len(x_test_tensor))\n",
    "            inputs = x_test_tensor[i:window_end].unsqueeze(0)\n",
    "            labels = y_test_tensor[window_end - 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "    print(f'Final Test Loss: {test_loss / len(x_test_tensor)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
