{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# x_train_tensor inverse\\n\\nx_test_original = scaler.inverse_transform(x_train_tensor.numpy())\\nprint(\"\\nFirst row of x_test_original:\")\\nprint(x_test_original[0])\\n\\nprint(\"\\nFirst row of x_train:\")\\nprint(x_train.head(1))\\n\\n\\n\\nprint(\"\\nLast row of x_test_original:\")\\nprint(x_test_original[-1])\\n\\nprint(\"\\nLast row of x_train:\")\\nprint(x_train.tail(1))\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "import numpy as npw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"../data/data/aapl_raw_data.csv\")\n",
    "\n",
    "data = data.drop(\"date\", axis=1)\n",
    "\n",
    "data.isnull().sum()\n",
    "data=data.fillna(0)  # Filling null values with zero\n",
    "data.isnull().sum()\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "# Keep data until 31.08.2023\n",
    "data = data.iloc[:10731]\n",
    "\n",
    "#print(data['open'].dtype)\n",
    "#print(data.shape)\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame\n",
    "x_data = data[['open', 'high', 'low', 'volume', 'adjusted_close', 'change_percent', 'avg_vol_20d']]\n",
    "y_data = data[\"close\"]\n",
    "\n",
    "# Now x_data and y_data are pandas DataFrames/Series, respectively\n",
    "\n",
    "x_data.tail(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the window size for training\n",
    "train_window_size = 20\n",
    "\n",
    "# Initialize lists to store training and temporary sets\n",
    "x_train_list, y_train_list, x_temp_list, y_temp_list = [], [], [], []\n",
    "\n",
    "# Iterate through the data with the specified window size\n",
    "for i in range(0, len(x_data) - train_window_size, train_window_size + 1):\n",
    "    x_train_temp = x_data.iloc[i:i+train_window_size+1]\n",
    "    y_train_temp = y_data.iloc[i:i+train_window_size+1]\n",
    "\n",
    "    # Separate the last row for the temporary set\n",
    "    x_train = x_train_temp.iloc[:-1]\n",
    "    y_train = y_train_temp.iloc[:-1]\n",
    "\n",
    "    x_temp = x_train_temp.iloc[-1:]\n",
    "    y_temp = y_train_temp.iloc[-1:]\n",
    "\n",
    "    x_train_list.append(x_train)\n",
    "    y_train_list.append(y_train)\n",
    "    x_temp_list.append(x_temp)\n",
    "    y_temp_list.append(y_temp)\n",
    "\n",
    "# Concatenate the lists into pandas DataFrames\n",
    "x_train = pd.concat(x_train_list)\n",
    "y_train = pd.concat(y_train_list)\n",
    "x_temp = pd.concat(x_temp_list)\n",
    "y_temp = pd.concat(y_temp_list)\n",
    "\n",
    "# print(y_train.head(50))\n",
    "x_temp_train, x_temp_val, y_temp_train, y_temp_val = train_test_split(x_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Split x_temp and y_temp into validation and test sets\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Print the last 5 rows of x_data\n",
    "print(\"Last 5 rows of x_data:\")\n",
    "print(x_data.tail(5))\n",
    "\n",
    "# Print the last 5 rows of x_train\n",
    "print(\"\\nLast 25 rows of x_train:\")\n",
    "print(x_train.tail(25))\n",
    "\n",
    "print(\"\\nLast 3 rows of y_train:\")\n",
    "print(y_temp.tail(3))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_val_normalized = scaler.transform(x_val)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# x_train_tensor inverse\n",
    "\n",
    "x_test_original = scaler.inverse_transform(x_train_tensor.numpy())\n",
    "print(\"\\nFirst row of x_test_original:\")\n",
    "print(x_test_original[0])\n",
    "\n",
    "print(\"\\nFirst row of x_train:\")\n",
    "print(x_train.head(1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLast row of x_test_original:\")\n",
    "print(x_test_original[-1])\n",
    "\n",
    "print(\"\\nLast row of x_train:\")\n",
    "print(x_train.tail(1))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Iteration [1000/10211], Loss: 501.533543\n",
      "Epoch [1/3], Iteration [2000/10211], Loss: 729.443687\n",
      "Epoch [1/3], Iteration [3000/10211], Loss: 823.493972\n",
      "Epoch [1/3], Iteration [4000/10211], Loss: 109.409308\n",
      "Epoch [1/3], Iteration [5000/10211], Loss: 1203.029473\n",
      "Epoch [1/3], Iteration [6000/10211], Loss: 261.584988\n",
      "Epoch [1/3], Iteration [7000/10211], Loss: 9343.494938\n",
      "Epoch [1/3], Iteration [8000/10211], Loss: 159777.455990\n",
      "Epoch [1/3], Iteration [9000/10211], Loss: 16581.715630\n",
      "Epoch [1/3], Iteration [10000/10211], Loss: 115690.540478\n",
      "Epoch [2/3], Iteration [1000/10211], Loss: 1225.690309\n",
      "Epoch [2/3], Iteration [2000/10211], Loss: 681.830103\n",
      "Epoch [2/3], Iteration [3000/10211], Loss: 177.644553\n",
      "Epoch [2/3], Iteration [4000/10211], Loss: 671.220928\n",
      "Epoch [2/3], Iteration [5000/10211], Loss: 874.370415\n",
      "Epoch [2/3], Iteration [6000/10211], Loss: 198.423201\n",
      "Epoch [2/3], Iteration [7000/10211], Loss: 7207.519338\n",
      "Epoch [2/3], Iteration [8000/10211], Loss: 146655.841758\n",
      "Epoch [2/3], Iteration [9000/10211], Loss: 13612.344253\n",
      "Epoch [2/3], Iteration [10000/10211], Loss: 93184.870208\n",
      "Epoch [3/3], Iteration [1000/10211], Loss: 2141.467930\n",
      "Epoch [3/3], Iteration [2000/10211], Loss: 76.731782\n",
      "Epoch [3/3], Iteration [3000/10211], Loss: 58.771702\n",
      "Epoch [3/3], Iteration [4000/10211], Loss: 55.504275\n",
      "Epoch [3/3], Iteration [5000/10211], Loss: 359.776918\n",
      "Epoch [3/3], Iteration [6000/10211], Loss: 45.059692\n",
      "Epoch [3/3], Iteration [7000/10211], Loss: 3471.253392\n",
      "Epoch [3/3], Iteration [8000/10211], Loss: 126056.310201\n",
      "Epoch [3/3], Iteration [9000/10211], Loss: 10364.568735\n",
      "Epoch [3/3], Iteration [10000/10211], Loss: 65897.239041\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hx, cx):\n",
    "        out, (hn, cn) = self.lstm(x, (hx, cx))\n",
    "        out = self.fc(out[:, -1, :])  # Taking the output from the last time step\n",
    "        return out, (hn, cn)\n",
    "\n",
    "# Initialize model and other hyperparameters\n",
    "input_size = 7  # Number of features\n",
    "hidden_size = 64  # Number of hidden units\n",
    "output_size = 1  # Number of output units\n",
    "num_layers = 16  # Number of LSTM layers\n",
    "learning_rate = 0.0001\n",
    "window_size = 10\n",
    "stride = 1\n",
    "num_epochs = 1\n",
    "print_interval = 1000\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initial hidden and cell states\n",
    "hx = torch.zeros(num_layers, 1, hidden_size)  # Assuming batch size is 1 for simplicity\n",
    "cx = torch.zeros(num_layers, 1, hidden_size)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Iterate over the training data with a sliding window\n",
    "    for i in range(0, len(x_train_tensor) - window_size + 1, stride):\n",
    "        # Extract a segment of input and target sequences based on the window\n",
    "        x_window = x_train_tensor[i:i+window_size].unsqueeze(0)  # Add batch dimension\n",
    "        y_window = y_train_tensor[i:i+window_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, (hn, cn) = model(x_window, hx, cx)  # Include initial hidden states\n",
    "        loss = criterion(outputs.squeeze(0), y_window)  # Squeeze to match target dimensions\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Print the loss every print_interval iterations\n",
    "        if (i + 1) % print_interval == 0:\n",
    "            avg_loss = train_loss / min(print_interval, len(x_train_tensor) - window_size + 1 - i)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Iteration [{i+1}/{len(x_train_tensor) - window_size + 1}], Loss: {avg_loss:.6f}')\n",
    "            train_loss = 0.0  # Reset the loss\n",
    "\n",
    "    # Average the loss over all windows\n",
    "    train_loss /= len(x_train_tensor) - window_size + 1\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation and printing omitted for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hx, cx):\n",
    "        out, (hn, cn) = self.lstm(x, (hx, cx))\n",
    "        out = self.fc(out[:, -1, :])  # Taking the output from the last time step\n",
    "        return out, (hn, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Iteration [1000/10211], Loss: 23.289828\n",
      "Epoch [1/1], Iteration [2000/10211], Loss: 7.852781\n",
      "Epoch [1/1], Iteration [3000/10211], Loss: 6.227853\n",
      "Epoch [1/1], Iteration [4000/10211], Loss: 4.554815\n",
      "Epoch [1/1], Iteration [5000/10211], Loss: 24.954045\n",
      "Epoch [1/1], Iteration [6000/10211], Loss: 7.500409\n",
      "Epoch [1/1], Iteration [7000/10211], Loss: 34.939914\n",
      "Epoch [1/1], Iteration [8000/10211], Loss: 624.957047\n",
      "Epoch [1/1], Iteration [9000/10211], Loss: 1074.647610\n",
      "Epoch [1/1], Iteration [10000/10211], Loss: 2639.636837\n"
     ]
    }
   ],
   "source": [
    "input_size = 7  # Number of features\n",
    "hidden_size = 64  # Number of hidden units\n",
    "output_size = 1  # Number of output units\n",
    "num_layers = 128  # Number of LSTM layers\n",
    "\n",
    "learning_rate = 0.0001  # Change this to your desired learning rate\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Iterate over the training data with a sliding window\n",
    "    for i in range(0, len(x_train_tensor) - window_size + 1, stride):\n",
    "        # Extract a segment of input and target sequences based on the window\n",
    "        x_window = x_train_tensor[i:i+window_size]\n",
    "        y_window = y_train_tensor[i:i+window_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_window.unsqueeze(0))  # Add batch dimension\n",
    "        loss = criterion(outputs.squeeze(0), y_window)  # Squeeze to match target dimensions\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Print the loss every 100 iterations\n",
    "        if (i + 1) % print_interval == 0:\n",
    "            avg_loss = train_loss / min(print_interval, len(x_train_tensor) - window_size + 1 - i)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Iteration [{i+1}/{len(x_train_tensor) - window_size + 1}], Loss: {avg_loss:.6f}')\n",
    "            train_loss = 0.0  # Reset the loss\n",
    "\n",
    "    # Average the loss over all windows\n",
    "    train_loss /= len(x_train_tensor) - window_size + 1\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation and printing omitted for brevity\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
