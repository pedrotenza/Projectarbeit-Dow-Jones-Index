{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgaben zu Vertiefung AI-Engineer Modul 5 - Time-Series-Data\n",
    "\n",
    "Aufgabe 1: Time-Series-Data\n",
    "\n",
    "Identifizieren Sie im UCI Repository (oder von anderen Stellen) einen Datensatz mit temporaler \n",
    "Dynamik. Implementieren Sie ein Neuronales Netz mit dem “naiven Ansatz”, mehrere Instanzen \n",
    "nachrutschend in die Input Schicht zu geben. Evaluieren Sie diesen naiven Ansatz gegen eine \n",
    "Implementierung mittels rekurrenter Layer.\n",
    "\n",
    "https://archive.ics.uci.edu/datasets.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dow Jones Index\n",
    "Donated on 10/22/2014\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/312/dow+jones+index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/data/dow_jones_index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data [\"close\"] = data[\"close\"] .apply(lambda x: x.replace(\"$\",\"\"))\n",
    "data [\"open\"] = data[\"open\"] .apply(lambda x: x.replace(\"$\",\"\"))\n",
    "data [\"high\"] = data[\"high\"] .apply(lambda x: x.replace(\"$\",\"\"))\n",
    "data [\"low\"] = data[\"low\"] .apply(lambda x: x.replace(\"$\",\"\"))\n",
    "data [\"next_weeks_open\"] = data[\"next_weeks_open\"] .apply(lambda x: x.replace(\"$\",\"\"))\n",
    "data [\"next_weeks_close\"] = data[\"next_weeks_close\"] .apply(lambda x: x.replace(\"$\",\"\"))\n",
    "\n",
    "\n",
    "#df['DataFrame Column'] = df['DataFrame Column'].astype(float)\n",
    "\n",
    "data [\"close\"] = data[\"close\"].astype(float) \n",
    "data [\"open\"] = data[\"open\"].astype(float)\n",
    "data [\"high\"] = data[\"high\"].astype(float)\n",
    "data [\"low\"] = data[\"low\"].astype(float)\n",
    "data [\"volume\"] = data[\"volume\"].astype(float)\n",
    "data [\"percent_change_price\"] = data[\"percent_change_price\"].astype(float)\n",
    "data [\"next_weeks_open\"] = data[\"next_weeks_open\"].astype(float)\n",
    "data [\"next_weeks_close\"] = data[\"next_weeks_close\"].astype(float)\n",
    "data [\"percent_change_next_weeks_price\"] = data[\"percent_change_next_weeks_price\"].astype(float)\n",
    "data [\"days_to_next_dividend\"] = data[\"days_to_next_dividend\"].astype(float)\n",
    "data [\"percent_return_next_dividend\"] = data[\"percent_return_next_dividend\"].astype(float)\n",
    "\n",
    "data['close'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quarter</th>\n",
       "      <th>stock</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>percent_change_price</th>\n",
       "      <th>percent_change_volume_over_last_wk</th>\n",
       "      <th>previous_weeks_volume</th>\n",
       "      <th>next_weeks_open</th>\n",
       "      <th>next_weeks_close</th>\n",
       "      <th>percent_change_next_weeks_price</th>\n",
       "      <th>days_to_next_dividend</th>\n",
       "      <th>percent_return_next_dividend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/7/2011</td>\n",
       "      <td>15.82</td>\n",
       "      <td>16.72</td>\n",
       "      <td>15.78</td>\n",
       "      <td>16.42</td>\n",
       "      <td>239655616.0</td>\n",
       "      <td>3.79267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.71</td>\n",
       "      <td>15.97</td>\n",
       "      <td>-4.428490</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.182704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/14/2011</td>\n",
       "      <td>16.71</td>\n",
       "      <td>16.71</td>\n",
       "      <td>15.64</td>\n",
       "      <td>15.97</td>\n",
       "      <td>242963398.0</td>\n",
       "      <td>-4.42849</td>\n",
       "      <td>1.380223</td>\n",
       "      <td>239655616.0</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.79</td>\n",
       "      <td>-2.470660</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.187852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/21/2011</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.38</td>\n",
       "      <td>15.60</td>\n",
       "      <td>15.79</td>\n",
       "      <td>138428495.0</td>\n",
       "      <td>-2.47066</td>\n",
       "      <td>-43.024959</td>\n",
       "      <td>242963398.0</td>\n",
       "      <td>15.87</td>\n",
       "      <td>16.13</td>\n",
       "      <td>1.638310</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.189994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/28/2011</td>\n",
       "      <td>15.87</td>\n",
       "      <td>16.63</td>\n",
       "      <td>15.82</td>\n",
       "      <td>16.13</td>\n",
       "      <td>151379173.0</td>\n",
       "      <td>1.63831</td>\n",
       "      <td>9.355500</td>\n",
       "      <td>138428495.0</td>\n",
       "      <td>16.18</td>\n",
       "      <td>17.14</td>\n",
       "      <td>5.933250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.185989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>2/4/2011</td>\n",
       "      <td>16.18</td>\n",
       "      <td>17.39</td>\n",
       "      <td>16.18</td>\n",
       "      <td>17.14</td>\n",
       "      <td>154387761.0</td>\n",
       "      <td>5.93325</td>\n",
       "      <td>1.987452</td>\n",
       "      <td>151379173.0</td>\n",
       "      <td>17.33</td>\n",
       "      <td>17.37</td>\n",
       "      <td>0.230814</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.175029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quarter stock       date   open   high    low  close       volume  \\\n",
       "0        1    AA   1/7/2011  15.82  16.72  15.78  16.42  239655616.0   \n",
       "1        1    AA  1/14/2011  16.71  16.71  15.64  15.97  242963398.0   \n",
       "2        1    AA  1/21/2011  16.19  16.38  15.60  15.79  138428495.0   \n",
       "3        1    AA  1/28/2011  15.87  16.63  15.82  16.13  151379173.0   \n",
       "4        1    AA   2/4/2011  16.18  17.39  16.18  17.14  154387761.0   \n",
       "\n",
       "   percent_change_price  percent_change_volume_over_last_wk  \\\n",
       "0               3.79267                                 NaN   \n",
       "1              -4.42849                            1.380223   \n",
       "2              -2.47066                          -43.024959   \n",
       "3               1.63831                            9.355500   \n",
       "4               5.93325                            1.987452   \n",
       "\n",
       "   previous_weeks_volume  next_weeks_open  next_weeks_close  \\\n",
       "0                    NaN            16.71             15.97   \n",
       "1            239655616.0            16.19             15.79   \n",
       "2            242963398.0            15.87             16.13   \n",
       "3            138428495.0            16.18             17.14   \n",
       "4            151379173.0            17.33             17.37   \n",
       "\n",
       "   percent_change_next_weeks_price  days_to_next_dividend  \\\n",
       "0                        -4.428490                   26.0   \n",
       "1                        -2.470660                   19.0   \n",
       "2                         1.638310                   12.0   \n",
       "3                         5.933250                    5.0   \n",
       "4                         0.230814                   97.0   \n",
       "\n",
       "   percent_return_next_dividend  \n",
       "0                      0.182704  \n",
       "1                      0.187852  \n",
       "2                      0.189994  \n",
       "3                      0.185989  \n",
       "4                      0.175029  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "x = data[['open', 'high', 'low', 'volume', 'percent_change_price', 'next_weeks_open', 'next_weeks_close', 'percent_change_next_weeks_price', 'days_to_next_dividend', 'percent_return_next_dividend']].to_numpy()\n",
    "y = data[\"close\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=45)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([502, 10])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2000], Loss: 1195.2109\n",
      "Epoch [200/2000], Loss: 896.3358\n",
      "Epoch [300/2000], Loss: 880.3021\n",
      "Epoch [400/2000], Loss: 880.0225\n",
      "Epoch [500/2000], Loss: 880.0092\n",
      "Epoch [600/2000], Loss: 880.0000\n",
      "Epoch [700/2000], Loss: 879.9922\n",
      "Epoch [800/2000], Loss: 879.9852\n",
      "Epoch [900/2000], Loss: 879.9783\n",
      "Epoch [1000/2000], Loss: 879.9715\n",
      "Epoch [1100/2000], Loss: 879.9645\n",
      "Epoch [1200/2000], Loss: 879.9573\n",
      "Epoch [1300/2000], Loss: 879.9498\n",
      "Epoch [1400/2000], Loss: 879.9419\n",
      "Epoch [1500/2000], Loss: 879.9337\n",
      "Epoch [1600/2000], Loss: 879.9250\n",
      "Epoch [1700/2000], Loss: 879.9159\n",
      "Epoch [1800/2000], Loss: 879.9064\n",
      "Epoch [1900/2000], Loss: 879.8961\n",
      "Epoch [2000/2000], Loss: 879.8854\n",
      "Predicted Value: 38.22936248779297\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define the LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        out, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        return out, (hidden, cell)\n",
    "\n",
    "# Assuming you have defined device somewhere\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 10\n",
    "output_size = 1  # Assuming it's a regression problem\n",
    "hidden_dim = 32\n",
    "n_layers = 2\n",
    "sequence_length = 20\n",
    "batch_size = 8\n",
    "\n",
    "# Hidden Dim = 16, Layers = 2, Learning Rate = 0.01, Batch Size = 2\n",
    "# Hidden Dim = 64, Layers = 2, Learning Rate = 0.01, Batch Size = 2\n",
    "# Create an instance of the LSTM model\n",
    "model = LSTMModel(input_size, output_size, hidden_dim, n_layers).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()  # Use Mean Squared Error for regression\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Generate random input and target data\n",
    "x_train = np.random.rand(batch_size, sequence_length, input_size)\n",
    "y_train = np.random.rand(batch_size, sequence_length, output_size) * 100  # Generating continuous target values\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs, _ = model(x_train)\n",
    "    outputs = outputs.view(-1, output_size)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_function(outputs, y_train.view(-1, output_size))\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Code for making predictions\n",
    "input_data = np.array([15.82, 16.72, 15.78, 239655616.0, 3.79267, 16.71, 15.97, -4.428490, 26.0, 0.182704])\n",
    "input_data = input_data.reshape(1, -1, input_size)\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction_tuple = model(input_tensor)\n",
    "predicted_value = prediction_tuple[0].item()\n",
    "\n",
    "print(\"Predicted Value:\", predicted_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Hyperparameters: Hidden Dim = 8, Layers = 2, Learning Rate = 0.01, Batch Size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/6000], Loss: 1243.7919\n",
      "Epoch [800/6000], Loss: 961.5900\n",
      "Epoch [1200/6000], Loss: 947.6854\n",
      "Epoch [1600/6000], Loss: 947.4916\n",
      "Epoch [2000/6000], Loss: 947.4724\n",
      "Epoch [2400/6000], Loss: 947.4508\n",
      "Epoch [2800/6000], Loss: 926.8208\n",
      "Epoch [3200/6000], Loss: 926.6218\n",
      "Epoch [3600/6000], Loss: 926.4611\n",
      "Epoch [4000/6000], Loss: 926.2775\n",
      "Epoch [4400/6000], Loss: 926.0581\n",
      "Epoch [4800/6000], Loss: 925.7931\n",
      "Epoch [5200/6000], Loss: 925.4725\n",
      "Epoch [5600/6000], Loss: 925.0854\n",
      "Epoch [6000/6000], Loss: 924.6201\n",
      "\n",
      "Training with Hyperparameters: Hidden Dim = 16, Layers = 2, Learning Rate = 0.01, Batch Size = 2\n",
      "Epoch [400/6000], Loss: 864.3794\n",
      "Epoch [800/6000], Loss: 835.9557\n",
      "Epoch [1200/6000], Loss: 835.7936\n",
      "Epoch [1600/6000], Loss: 835.6956\n",
      "Epoch [2000/6000], Loss: 835.5961\n",
      "Epoch [2400/6000], Loss: 835.4831\n",
      "Epoch [2800/6000], Loss: 66.0741\n",
      "Epoch [3200/6000], Loss: 3.1234\n",
      "Epoch [3600/6000], Loss: 0.8146\n",
      "Epoch [4000/6000], Loss: 0.4528\n",
      "Epoch [4400/6000], Loss: 0.3143\n",
      "Epoch [4800/6000], Loss: 0.2216\n",
      "Epoch [5200/6000], Loss: 0.1491\n",
      "Epoch [5600/6000], Loss: 0.3666\n",
      "Epoch [6000/6000], Loss: 0.0601\n",
      "\n",
      "Training with Hyperparameters: Hidden Dim = 32, Layers = 2, Learning Rate = 0.01, Batch Size = 2\n",
      "Epoch [400/6000], Loss: 839.4896\n",
      "Epoch [800/6000], Loss: 839.2933\n",
      "Epoch [1200/6000], Loss: 839.2321\n",
      "Epoch [1600/6000], Loss: 839.1655\n",
      "Epoch [2000/6000], Loss: 839.0840\n",
      "Epoch [2400/6000], Loss: 838.9831\n",
      "Epoch [2800/6000], Loss: 838.8591\n",
      "Epoch [3200/6000], Loss: 838.7077\n",
      "Epoch [3600/6000], Loss: 838.5256\n",
      "Epoch [4000/6000], Loss: 838.3143\n",
      "Epoch [4400/6000], Loss: 838.0829\n",
      "Epoch [4800/6000], Loss: 837.8447\n",
      "Epoch [5200/6000], Loss: 837.6116\n",
      "Epoch [5600/6000], Loss: 837.3952\n",
      "Epoch [6000/6000], Loss: 837.2059\n",
      "\n",
      "Training with Hyperparameters: Hidden Dim = 64, Layers = 2, Learning Rate = 0.01, Batch Size = 2\n",
      "Epoch [400/6000], Loss: 873.7518\n",
      "Epoch [800/6000], Loss: 873.1514\n",
      "Epoch [1200/6000], Loss: 10.4990\n",
      "Epoch [1600/6000], Loss: 0.0223\n",
      "Epoch [2000/6000], Loss: 0.0008\n",
      "Epoch [2400/6000], Loss: 0.0004\n",
      "Epoch [2800/6000], Loss: 0.0003\n",
      "Epoch [3200/6000], Loss: 0.0002\n",
      "Epoch [3600/6000], Loss: 0.0008\n",
      "Epoch [4000/6000], Loss: 0.0003\n",
      "Epoch [4400/6000], Loss: 0.0001\n",
      "Epoch [4800/6000], Loss: 0.0001\n",
      "Epoch [5200/6000], Loss: 0.0001\n",
      "Epoch [5600/6000], Loss: 0.0000\n",
      "Epoch [6000/6000], Loss: 0.0000\n",
      "\n",
      "\n",
      "Best Parameters Combination:\n",
      "Hidden Dim = 64, Layers = 2, Learning Rate = 0.01, Batch Size = 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\"\"\"\n",
    "# Define hyperparameter grid\n",
    "hidden_dim_values = [32]\n",
    "n_layers_values = [2]\n",
    "learning_rate_values = [0.01]\n",
    "batch_size_values = [4, 8, 16]\n",
    "\"\"\"\"\"\n",
    "# Define hyperparameter grid\n",
    "hidden_dim_values = [16]\n",
    "n_layers_values = [2]\n",
    "learning_rate_values = [0.01]\n",
    "batch_size_values = [2]\n",
    "\n",
    "\"\"\"\"\n",
    "# Define hyperparameter grid\n",
    "hidden_dim_values = [2,4,8,16,32,64,128,512,1024,2048]\n",
    "n_layers_values = [2,4,8,16,32,64,128,512,1024,2048]\n",
    "learning_rate_values = [0,1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "batch_size_values = [1,2,3,4,5,6,7,8,9,10,11,13,14,15,16]\n",
    "\"\"\"\n",
    "\n",
    "# Create all possible combinations of hyperparameters\n",
    "hyperparameter_combinations = list(itertools.product(hidden_dim_values, n_layers_values, learning_rate_values, batch_size_values))\n",
    "\n",
    "best_loss = float('inf')  # Initialize with a high value\n",
    "best_parameters = None\n",
    "\n",
    "# Training loop with grid search\n",
    "for idx, (hidden_dim, n_layers, learning_rate, batch_size) in enumerate(hyperparameter_combinations):\n",
    "    print(f\"Training with Hyperparameters: Hidden Dim = {hidden_dim}, Layers = {n_layers}, Learning Rate = {learning_rate}, Batch Size = {batch_size}\")\n",
    "\n",
    "    # Create an instance of the LSTM model with the current hyperparameters\n",
    "    model = LSTMModel(input_size, output_size, hidden_dim, n_layers).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Generate random input and target data for the current batch size\n",
    "    x_train = np.random.rand(batch_size, sequence_length, input_size)\n",
    "    y_train = np.random.rand(batch_size, sequence_length, output_size) * 100\n",
    "\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 6000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = model(x_train)\n",
    "        outputs = outputs.view(-1, output_size)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, y_train.view(-1, output_size))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 400 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    #print(f\"Finished Training with Hyperparameters: Hidden Dim = {hidden_dim}, Layers = {n_layers}, Learning Rate = {learning_rate}, Batch Size = {batch_size}\")\n",
    "    print (         )\n",
    "    # Update the best parameters if this run has a lower loss\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_parameters = (hidden_dim, n_layers, learning_rate, batch_size)\n",
    "\n",
    "print(\"\\nBest Parameters Combination:\")\n",
    "print(f\"Hidden Dim = {best_parameters[0]}, Layers = {best_parameters[1]}, Learning Rate = {best_parameters[2]}, Batch Size = {best_parameters[3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llllllllllllll"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
